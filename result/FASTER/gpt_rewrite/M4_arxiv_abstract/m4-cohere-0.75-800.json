[
    {
        "original_text": "The D0 and CDF experiments have searched for single-top production in the t-channel, s-channel, and associated Wt channel using data corresponding to an integrated luminosity of 5.4 fb-1 collected by the Fermilab Tevatron Collider between 1992 and 1996.  The results are presented as 95% confidence level upper limits on the cross sections times branching ratios into leptons (electrons or muons) plus jets.  In addition, we present measurements of the top quark mass made with these events. We find no evidence for new physics beyond standard model expectations. These results supersede those previously reported by both collaborations. \nWe thank our colleagues in the CERN accelerator departments for their vital contributions to this work. This research is supported by the U.S. Department of Energy under contract No. DE-AC02-76SF00515. The D0 experiment has also performed searches for single-top quarks produced via the t-channel, s-channnel, and associated Wt channels using 4.3 fb-1 of data taken during Run II of the Fermilab TeVatron collider  1  .",
        "watermark_text": "The D0 and CDF research have searched for single - top activity in the t - channel , s - channel , and related Wt channel using data related to an integrated luminosity of 5 . 4 fb - 1 collected by the Fermilab Tevatron Collider between 1992 and 1996 . The results are presented as 95 % confidence rate upper limits on the cross sections times branching ratios into leptons ( atoms or muons ) plus jets .In addition , we present measurements of the top quark mass made with these events . We see no evidence for new science beyond standard theory expectations .These conclusions supersede those previously reported by both collaborations . We thank our colleagues in the CERN accelerator departments for their vital contributions to this research .This research is backed by the U . S . Department of Energy under contract No . DE - AC02 - 76SF00515 .The D0 study has additionally performed investigations for single - top quarks produced via the t - channel , s - channnel , and related Wt channels using 4 . 3 fb - 1 of evidence gained during Run II of the Fermilab TeVatron collider 1 .",
        "rewrite_text": "The D0 and CDF Collaborations have searched for single-top quark activity in the t-channel, s-channel, and related Wt channel using data corresponding to an integrated luminosity of 5.4 fb^-1 collected by the Fermilab Tevatron Collider from 1992 to 1996. The findings are presented as upper limits on the cross sections multiplied by branching ratios into leptons (electrons or muons) and jets, at a 95% confidence level. Additionally, we provide measurements of the top quark mass derived from these events. No evidence for phenomena beyond the expectations of standard theory has been observed. These results replace previous findings reported by both collaborations. We express our gratitude to our colleagues in the CERN accelerator departments for their critical contributions to this research, which is supported by the U.S. Department of Energy under contract No. DE-AC02-76SF00515. The D0 collaboration has also conducted studies on single-top quark production through the t-channel, s-channel, and related Wt channels using 4.3 fb^-1 of data collected during Run II of the Fermilab Tevatron Collider.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 4.031591663758072,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "We study the energy transfer in a network of coupled nonlinear oscillators with time delay, which is motivated by the dynamics of biological systems such as neural networks and genetic regulatory networks. We show that there exists an optimal coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators. The results are obtained through numerical simulations on both small-scale and large-scale networks. In particular, we find that the energy transfer efficiency decreases when the number of nodes increases beyond a certain threshold value. This phenomenon can be explained by the fact that the total amount of available energy per node decreases rapidly due to the increase of the number of nodes. Finally, we discuss possible applications of our findings to real-world problems. Energy transfer plays important roles in many natural phenomena including brain activity  1  , heartbeat  2  , and gene regulation  3  . It has been shown recently that efficient energy transfer may also exist in complex networks  4  -  6  .\nIn this work, we consider a system consisting of N identical nonlinear oscillators connected via delayed couplings (see Fig.   1 ). Each oscillator i = 1; 2; ...; N is described bẏ \nwhere x i 2 R n denotes its state vector, f : R n ! R n describes the local dynamics of each oscillator, c ij > 0 represents the coupling strength between two oscillators i and j, and h(t) is the transmission delay satisfying h max < h(t) < h min ; t 0:",
        "watermark_text": "We research the power transfer in a network of coupled nonlinear oscillators with time delay , which is motivated by the dynamics of biological systems such as neural systems and genome regulatory circuits . We see that there exists an appropriate coupling strength for each individual oscillator to achieve maximum energy transfer efficiency among all other oscillators .The results are derived through computational simulations on both small - scale and large - scale networks . In particular , we find that the power transfer efficiency decreases when the number of nodes increases beyond a certain threshold value .This phenomenon can be described by the fact that the total amount of available energy per node decreases quickly due to the increase of the number of nodes . Finally , we explain possible applied of our findings to real - time situations .Energy transfer plays important roles in multiple natural phenomena including brain activity 1 , heartbeat 2 , and gene control 3 . It has been shown lately that efficient energy flow would also exist in complex networks 4 - 6 .In this study , we study a system consisting of N identical nonlinear oscillators related via delayed couplings ( saw Fig . 1 ) .Each oscillator i = 1 ; 2 ; . . . ; N is described [UNK] where x i 2 R n denotes its state vector , f : R n ! R n describes the local dynamics of each oscillator , c ij > 0 represents the coupling strength between two oscillators i and j , and h ( t ) is the transmission delay satisfying h max < h ( t ) < h min ; t 0 :",
        "rewrite_text": "We investigate the power transfer within a network of coupled nonlinear oscillators that incorporate time delays, driven by the dynamics observed in biological systems like neural networks and genomic regulatory circuits. Our findings indicate that each individual oscillator has an optimal coupling strength that maximizes energy transfer efficiency among the ensemble. These results were obtained through computational simulations conducted on both small and large-scale networks. Notably, we observed a decline in power transfer efficiency when the number of nodes exceeds a certain threshold. This can be attributed to the rapid decrease in the total energy available per node as the network size increases. Additionally, we discuss potential real-time applications of our results. Energy transfer is crucial in various natural processes, including brain function, heartbeat regulation, and gene expression. Recent studies have demonstrated that efficient energy flow can also manifest in complex networks. In this research, we focus on a system composed of \\(N\\) identical nonlinear oscillators interconnected by delayed couplings, as illustrated in Figure 1. Each oscillator, indexed by \\(i = 1, 2, \\ldots, N\\), is represented where \\(x_i \\in \\mathbb{R}^n\\) denotes its state vector, \\(f: \\mathbb{R}^n \\to \\mathbb{R}^n\\) describes its local dynamics, \\(c_{ij} > 0\\) is the coupling strength between oscillators \\(i\\) and \\(j\\), and \\(h(t)\\) represents the transmission delay, constrained by \\(h_{\\text{max}} < h(t) < h_{\\text{min}}; t \\geq 0\\).",
        "ori-fast-z-score": -0.5345224838248488,
        "water-fast-z-score": 4.09800570932384,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "We study the electronic properties of armchair-edged carbon nanotubes with magnetic flux piercing them, using density functional theory (DFT). We find that for tubes with diameter larger than 1 nm there is no significant difference between the results obtained by DFT and those predicted by the effective-mass approximation. For smaller diameters we observe deviations which are attributed to the breaking of the valley degeneracy due to the curvature effects. The Aharonov-Bohm-effect manifests itself as an oscillatory behavior of the energy gap when varying the applied magnetic field strength. In addition, we show how this effect can be used to tune the bandgap of these structures. Graphene nanorings have been proposed recently as building blocks for novel nanoelectronic devices such as transistors or spintrons  1-3 . These systems exhibit interesting physical phenomena like the quantum Hall effect  4  , persistent currents  5  , and Klein tunneling  6  . Recently it has also been shown that they may serve as efficient single photon sources  7, 8  .\nIn order to understand their transport characteristics one needs to know the dependence of the energy spectrum on various parameters such as the radius R, the number N of hexagons along the circumference, and the external magnetic field B. This problem was addressed theoretically within different approximations  9-13  but only very few studies were performed based on first-principles calculations  14-16 . Here we present a detailed investigation of the influence of the magnetic field on the electronic structure of armchair-edge carbon nanotubes using density functional theory  17  . Our main focus will be on small-diameter tubes where the curvature leads to important modifications compared to large-diameter tubes  18  .",
        "watermark_text": "We research the electronic properties of armchair - edged carbon nanotubes with magnetic flux piercing them , using density functional theory ( DFT ) . We see that for tubes with diameter greater than 1 mm there is no major improvement between the results derived by DFT and those predicted by the effective - weight method .For smaller diameters we perceive deviations which are owing to the breaking of the valley degeneracy owing to the curvature processes . The Aharonov - Bohm - effect manifests itself as an oscillatory behavior of the power gap when varying the introduced magnetic force power .In addition , we show how this effect can be used to balance the bandgap of these structures . Graphene nanorings have been proposed lately as building blocks for novel nanoelectronic elements such as transistors or spintrons 1 - 3 .These systems exhibit exciting physical phenomena like the quantum Hall phenomenon 4 , persistent currents 5 , and Klein tunneling 6 . Recently it has also been shown that they may serve as efficient single photon sources 7 , 8 .In order to comprehend their transport characteristics one needs to knowledge the dependence of the power spectrum on various variables such as the radius R , the number N of hexagons along the circumference , and the external magnetic force B . This problem was resolved theoretically within various approximations 9 - 13 but only very few experiments were performed based on third - principles measurements 14 - 16 .Here we present a detailed investigation of the impact of the magnetic force on the electronic stability of armchair - side carbon nanotubes using density functional theory 17 . Our main attention will be on small - radius tubes where the curvature contributes to significant modifications contrast to large - radius pipes 18 .",
        "rewrite_text": "We investigate the electronic properties of armchair-edged carbon nanotubes subjected to magnetic flux using density functional theory (DFT). Our findings indicate that for nanotubes with diameters greater than 1 mm, there is little difference between the results obtained from DFT and those predicted by the effective-weight method. However, for smaller diameters, we observe deviations due to the breaking of valley degeneracy resulting from curvature effects. The Aharonov-Bohm effect manifests as oscillatory behavior in the power gap when the applied magnetic flux is varied. Additionally, we demonstrate how this effect can be utilized to tune the bandgap of these structures. Recently, graphene nanorings have been proposed as fundamental components for innovative nanoelectronic devices such as transistors and spintronic applications. These systems exhibit fascinating physical phenomena including the quantum Hall effect, persistent currents, and Klein tunneling. Recent research also suggests that they can function as efficient single-photon sources. To understand their transport characteristics, it is essential to consider the dependence of the power spectrum on various parameters such as the radius (R), the number of hexagons (N) along the circumference, and the external magnetic field (B). While this issue has been addressed theoretically through various approximations, only a limited number of experiments have been conducted based on first-principles measurements. In this study, we provide a comprehensive analysis of the effects of magnetic flux on the electronic stability of armchair-edged carbon nanotubes using DFT, focusing particularly on small-radius tubes where curvature effects are significant compared to larger-radius counterparts.",
        "ori-fast-z-score": -1.1818181818181819,
        "water-fast-z-score": 6.636363636363637,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "We present the results of an optical spectroscopic survey of radio galaxies in clusters at redshifts z=0.4-0.8, carried out with the William Herschel Telescope (WHT). We find that the fraction of AGN-powered radio galaxies decreases towards higher galaxy density environments within the clusters. This is consistent with previous studies which have found evidence for environmental quenching of star formation activity among massive galaxies. However we also find that there are many examples where powerful radio sources reside in dense regions without any obvious signs of being environmentally suppressed. These objects may be undergoing rapid evolution or they could represent a population of recently accreted field galaxies whose properties are still evolving to resemble those of their local counterparts. The sample consists of 20 radio galaxies selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009) using the following criteria: 1) They lie in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) Their radio luminosity lies above L(3GHz) = 10 25 W Hz-1; 3) They do not show strong emission lines indicative of ongoing nuclear activity; 4) They were observed during our WHT run on 2010 May 24-25.",
        "watermark_text": "We present the conclusion of an optical spectroscopic study of radio stars in clusters at redshifts z = 0 . 4 - 0 . 8 , conducted out with the William Herschel Telescope ( WHT ) . We see that the fraction of AGN - powered radio stars reduces towards higher galaxy concentration environments within the clusters .This is consistent with previous research which have shown evidence for environmental quenching of galaxy formation activity among huge objects . However we also find that there are many instance where powerful radio sources exist in dense areas without any obvious signs of being environmentally suppressed .These bodies may be experiencing rapid evolution or they may contain a population of newly accreted field galaxies whose characteristics are still changing to reflect those of their nearby rivals . The sample consists of 20 radio objects chose from the VLA - COSMOS 3 GHz Large Project catalogue ( Smolčić et al . , 2009 ) applying the following criteria : 1 ) They sit in one of four X - ray luminous clusters at 0 . 4 < z < 0 . 8 ; 2 ) Their radio luminosity sits above L ( 3GHz ) = 10 25 W Hz - 1 ; 3 ) They do not show weak absorption tracks indicative of ongoing nuclear activity ; 4 ) They were detected during our WHT running on 2010 May 24 - 25 .",
        "rewrite_text": "We present the findings from an optical spectroscopic study of radio stars in clusters at redshifts of z = 0.4 to 0.8, conducted using the William Herschel Telescope (WHT). Our analysis indicates that the proportion of AGN-powered radio stars diminishes in environments with higher galaxy concentrations within the clusters. This observation aligns with previous studies that have provided evidence for the environmental quenching of galaxy formation activity in massive structures. However, we also observe numerous instances where powerful radio sources are found in densely populated areas without clear indications of environmental suppression. These sources may be undergoing rapid evolution or may consist of newly accreted field galaxies that are still in the process of adapting their characteristics to align with those of their neighboring galaxies. Our sample comprises 20 radio objects selected from the VLA-COSMOS 3 GHz Large Project catalogue (Smolčić et al., 2009) based on the following criteria: 1) located in one of four X-ray luminous clusters at 0.4 < z < 0.8; 2) having a radio luminosity above L(3GHz) = 10^25 W Hz^-1; 3) showing no weak absorption features that would suggest ongoing nuclear activity; and 4) detected during our WHT observations on May 24-25, 2010.",
        "ori-fast-z-score": -1.1793237883215741,
        "water-fast-z-score": 6.685631938613182,
        "rewrite-fast-z-score": 3.1091263510296048
    },
    {
        "original_text": "We present observations of the Mg II k line asymmetry during flares, which are compared with results obtained by numerical simulations using the RH code (Uitenbroek 2001). The observed profiles show that the blue wing is enhanced relative to the red one at all heights above the limb where we can see the flare emission. This effect is more pronounced for higher altitudes. We find that this behavior cannot be explained solely by Doppler shifts due to bulk plasma motions along the LOS. In addition, our modeling shows that the observed profile shapes cannot be reproduced without including nonthermal electron beams as an additional heating source. \n \n Keywords: Solar flare, chromospheric lines, nonthermal electrons, radiative hydrodynamics model, RH code, Mg II k line, line asymmetry. 1 Introduction \n \n During solar flares, intense energy release leads to rapid changes in physical conditions throughout the atmosphere of the Sun. These include temperature increases up to several million degrees Kelvin, strong magnetic fields, high densities, and large velocities. All these factors affect the shape of spectral lines emitted by different atmospheric layers. For example, it has been shown that the intensity ratio between two Fe I lines formed at different temperatures depends on the height of formation of each line (Feldman et al., 1995; Brosius & Phillips 2004) . Also, the presence of nonthermal electrons causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles (e.g., Canfield et al. (1990) ; Doschek et al. (1991) ), while bulk flows lead to Doppler shifts of the line center position (Doschek et al., 1991; Brosius & Phillips 2004; Brosius 2009 ). Therefore, studying the temporal evolution of the line profiles provides important information about the dynamics of the flaring region. However, interpreting such data requires detailed knowledge of the underlying physics involved in the processes responsible for the observed phenomena. \n \n In particular, the study of the Mg II h&k lines offers unique opportunities to investigate various aspects of solar flares because they form over a wide range",
        "watermark_text": "We present observations of the Mg II k line asymmetry during flares , which are compared with data derived by numerical simulations using the RH code ( Uitenbroek 2001 ) . The observed profiles indicate that the blue wing is enhanced compared to the red one at all heights above the limb where we can see the flare emission .This phenomenon is more pronounced for greater altitudes . We see that this behavior cannot be described solely by Doppler variations owing to bulk plasma dynamics along the LOS .In addition , our modeling demonstrates that the seen profile patterns cannot be altered without using nonthermal electron beams as an additional thermal source . Keywords : Solar flare , chromospheric lines , nonthermal atoms , radiative hydrodynamics theory , RH code , Mg II h line , edge asymmetry .1 Introduction During solar flares , intense heat release leads to rapid alterations in physical conditions throughout the atmosphere of the Sun . These include temperature increases up to several million degrees Kelvin , intense magnetic fields , large densities , and large velocities .All these influences influence the morphology of spectral lines emitted by various atmospheric elements . For instance , it has been shown that the frequency proportion between two Fe I lines formed at different temperatures depends on the height of formation of each line ( Feldman et al . , 1995 ; Brosius & Phillips 2004 ) .Also , the presence of nonthermal atoms causes significant deviations from Maxwellian velocity distributions leading to asymmetric line profiles ( e . g . , Canfield et al . ( 1990 ) ; Doschek et al .( 1991 ) ) , while bulk flows result to Doppler movements of the line center position ( Doschek et al . , 1991 ; Brosius & Phillips 2004 ; Brosius 2009 ) . Therefore , studying the temporal evolution of the line profiles provides crucial data about the dynamics of the flaring zone .However , interpreting such information requires deep knowledge of the fundamental physics involved in the mechanisms involved for the seen phenomena . In particular , the knowledge of the Mg II h & k lines provides unique possibilities to examine different components of sun flares because they shape over a broad range",
        "rewrite_text": "We present observations of the asymmetry in the Mg II k line during solar flares and compare them to data obtained from numerical simulations using the RH code (Uitenbroek, 2001). The observed profiles reveal that the blue wing is enhanced relative to the red wing at all elevations above the limb where flare emissions are visible, with this effect being more pronounced at higher altitudes. This behavior cannot be fully explained by Doppler shifts arising from bulk plasma movements along the line of sight. Furthermore, our modeling indicates that the observed profile patterns cannot be modified without incorporating nonthermal electron beams as an additional thermal source. \n\n**Keywords**: Solar flare, chromospheric lines, nonthermal atoms, radiative hydrodynamics theory, RH code, Mg II h line, edge asymmetry. \n\n**1 Introduction**: During solar flares, the release of intense heat results in rapid changes in the solar atmosphere's physical conditions, including temperature rises reaching millions of degrees Kelvin, strong magnetic fields, high densities, and significant velocities. These factors affect the morphology of spectral lines emitted by various atmospheric elements. For example, it has been shown that the frequency ratio between two Fe I lines formed at different temperatures is dependent on their formation heights (Feldman et al., 1995; Brosius & Phillips, 2004). Additionally, the presence of nonthermal atoms can lead to notable deviations from Maxwellian velocity distributions, resulting in asymmetric line profiles (e.g., Canfield et al., 1990; Doschek et al., 1991), while bulk flows cause Doppler shifts in the line center position (Doschek et al., 1991; Brosius & Phillips, 2004; Brosius, 2009). Consequently, studying the temporal evolution of line profiles is essential for understanding the dynamics of flaring regions. However, accurate interpretation of this data requires a thorough understanding of the underlying physics related to these observed phenomena. In particular, insights gained from the Mg II h and k lines offer unique opportunities to investigate various components of solar flares due to their formation across a broad altitude range.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.485281374238571,
        "rewrite-fast-z-score": 1.086610735988866
    },
    {
        "original_text": "The purpose of this study is to examine how an organisation can use Information Technology Infrastructure Library (ITIL) as a process optimisation tool within its service centre environment. The research focuses on the release management process at the IT service centre of Harz University and examines whether it complies with all relevant aspects of the ITIL framework. A case study approach was used to collect data through interviews and document analysis. This article presents the results of the investigation into the current state of the release management process at Harz University s IT service centre. It also provides recommendations for improving the process based on the findings. Keywords: information technology infrastructure library, process optimization, release management process, university, IT service centre, case study method. 1 Introduction Today, universities are increasingly relying on their computer systems to support teaching and research activities. As such, they need reliable services that meet user requirements  1  . However, many universities have experienced problems related to poor quality software releases  2  , which may lead to system failures or even loss of data  3  .\nIn order to avoid these issues, organisations should implement appropriate processes  4  . One widely accepted set of guidelines for managing information technology (IT) services is provided by the Information Technology Infrastructure Library  5  . According to  6  , ITIL has been adopted by more than 80% of Fortune 500 companies worldwide since 2003.",
        "watermark_text": "The purpose of this study is to examine how an institution can using Information Technology Infrastructure Library ( ITIL ) as a process optimisation platform within its service centre environment . The studies centers on the release control system at the IT service centre of Harz University and examines whether it complies with all relevant parts of the ITIL framework .A case study method was used to collect data through interviews and report analysis . This page presents the conclusion of the examination into the present state of the release control system at Harz University s IT network centre .It additionally offers suggestions for improving the process depending on the reports . Keywords : info industry infrastructure library , process optimization , release control system , university , IT service centre , case study method .1 Introduction Today , universities are increasingly relying on their computer machines to support education and scientific activities . As such , they seek reliable services that meet user requirements 1 .However , many universities have experienced difficulties related to bad quality code releases 2 , which sometimes lead to system failures or even loss of documentation 3 . In try to minimize these problems , organisations should implement appropriate processes 4 .One widely accepted set of guidelines for controlling information tech ( IT ) services is provided by the Information Technology Infrastructure Library 5 . According to 6 , ITIL has been used by more than 80 % of Fortune 500 companies globally since 2003 .",
        "rewrite_text": "The aim of this study is to explore how an institution can utilize the Information Technology Infrastructure Library (ITIL) as a platform for process optimization within its service center environment. The research focuses on the release control system at the IT service center of Harz University and evaluates its adherence to the relevant components of the ITIL framework. A case study method was employed to gather data through interviews and document analysis. This page summarizes the findings regarding the current status of the release control system at Harz University's IT network center and provides recommendations for process enhancement based on the analyzed reports.\n\nKeywords: Information Technology Infrastructure Library, process optimization, release control system, university, IT service center, case study method.\n\n1 Introduction\nToday, universities increasingly depend on their computer systems to support educational and research activities. As a result, they seek reliable services that cater to user needs. However, many institutions have encountered challenges related to poor-quality code releases, which can lead to system failures or loss of critical documentation. To mitigate these issues, organizations should implement effective processes. One widely recognized set of guidelines for managing IT services is the Information Technology Infrastructure Library (ITIL). Since 2003, ITIL has been adopted by over 80% of Fortune 500 companies worldwide.",
        "ori-fast-z-score": 0.27975144247209416,
        "water-fast-z-score": 8.672294716634918,
        "rewrite-fast-z-score": 1.7650452162436565
    },
    {
        "original_text": "We report on the detection by Fermi Large Area Telescope (LAT) of prompt emission in gamma rays with energies above 100 MeV associated with GRB 080916C, one of the brightest bursts ever detected at high energy. The LAT data show that this emission is highly variable and peaks within 1 s after the onset of the burst. We find no evidence for spectral evolution during the first few hundred seconds following the trigger time. A simple power law fit to the LAT spectrum yields an index of -2.2 +/-.1 over the range 100 MeV - 10 GeV. This value is consistent with previous measurements made using Konus-Wind and AGILE satellites but differs significantly from those obtained by other instruments operating below 100 MeV. Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X-ray band. They also rule out scenarios where the high-energy photons originate solely from inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray burst",
        "watermark_text": "We report on the discovery by Fermi Large Area Telescope ( LAT ) of prompt emission in gamma waves with energies above 100 MeV related with GRB 080916C , one of the brightest bursts ever observed at high energy . The LAT results show that this emission is strongly varied and spikes within 1 s after the beginning of the explosion .We see no evidence for spectral evolution during the first few hundred moments following the trigger time . A straightforward power law suited to the LAT spectrum gives an index of - 2 . 2 + / - . 1 over the range 100 MeV - 10 GeV .This value is compatible with previous measurements made using Konus - Wind and AGILE spacecraft but varies dramatically from those achieved by other satellites operating below 100 MeV . Our results are inconsistent with models which predict a softening of the photon spectrum as it evolves into the X - ray band .They even figure out situations where the high - energy photons arise solely from inverse Compton absorption off relativistic electrons accelerated in internal shocks . Keywords : Gamma - ray burst",
        "rewrite_text": "We present findings from the Fermi Large Area Telescope (LAT) regarding the prompt emission in gamma rays with energies exceeding 100 MeV associated with GRB 080916C, one of the most intense bursts ever observed in high energy. The LAT data indicates that this emission is highly variable, exhibiting spikes within 1 second of the explosion's onset. No spectral evolution was detected in the first few hundred seconds after the trigger. A simple power law that fits the LAT spectrum yields an index of -2.2 ± 0.1 over the energy range of 100 MeV to 10 GeV. This measurement aligns with previous observations from the Konus-Wind and AGILE spacecraft, yet contrasts sharply with results from other satellites operating below 100 MeV. Our findings challenge models that suggest a softening of the photon spectrum as it transitions into the X-ray regime, and they highlight scenarios where high-energy photons originate solely from inverse Compton scattering off relativistic electrons accelerated in internal shocks. Keywords: Gamma-ray burst.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": -0.3511234415883917
    },
    {
        "original_text": "The shell-model calculations are performed for the ground states and low-lying excited states in 16,17O nuclei using the effective interaction derived by Brown et al.  The results show that the calculated excitation energies agree well with experimental data except for some cases where the observed levels have large mixing between different configurations. In addition to the energy level scheme, we also calculate electromagnetic transition probabilities among these states. It is found that our calculation reproduces fairly well the measured B(E2) values but underestimates the B(M1) values significantly. This may be due to the fact that the M1 operator used here does not include the spin-orbit term which plays an important role in this case. Finally, we discuss briefly about the possible existence of new collective excitations in 17O nucleus. PACS: 21.10.Ks, 22.60.-n Shell Model Study of Neutron Rich Oxygen Isotopes",
        "watermark_text": "The shell - model calculations are performed for the ground states and low - lying excited states in 16 , 17O ions using the effective interaction derived by Brown et al . The results show that the derived excitation energies agree well with theoretical data except for some cases where the reported amounts have huge mixing between various configurations .In addition to the power level scheme , we also obtain electromagnetic transition probabilities among these states . It is found that our analysis reproduces reasonably well the measured B ( E2 ) values but underestimates the B ( M1 ) values considerably .This might be due to the fact that the M1 operator used here does not include the spin - orbit term which plays an important role in this instance . Finally , we talk briefly about the possible existence of new collective excitations in 17O nucleus .PACS : 21 . 10 . Ks , 22 . 60 . - n Shell Model Study of Neutron Rich Oxygen Isotopes",
        "rewrite_text": "Shell model calculations have been conducted for the ground and low-lying excited states of 16O and 17O ions, utilizing the effective interaction developed by Brown et al. The findings indicate that the calculated excitation energies align well with theoretical predictions, although there are instances where significant mixing among various configurations affects the results. In addition to the energy levels, we have also determined the electromagnetic transition probabilities for these states. Our analysis demonstrates a reasonable agreement with the measured B(E2) values, but it significantly underestimates the B(M1) values. This discrepancy may stem from the omission of the spin-orbit term in the M1 operator, which is likely crucial in this context. Lastly, we briefly discuss the potential for new collective excitations within the 17O nucleus. PACS: 21.10.Ks, 22.60.-n Shell Model Study of Neutron Rich Oxygen Isotopes.",
        "ori-fast-z-score": 0.7171371656006361,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": -1.0327955589886444
    },
    {
        "original_text": "We present the theory for two-photon interactions in broadband down-converted light, including entanglement between photons generated by spontaneous parametric down conversion (SPDC). We show that this leads to new effects such as photon bunching at zero time delay and antibunching at nonzero delays. These results are compared against experimental data obtained using SPDC sources based on periodically poled lithium niobate waveguides. The theoretical model is also used to predict the effect of varying pump bandwidths and crystal lengths on the degree of second-order coherence g(2)(0) measured experimentally. This work was supported by EPSRC grant EP/G037656/1. \n \n In recent years there has been growing interest in quantum optics experiments involving broadband down-conversion  1–3 . Such experiments have led to demonstrations of novel phenomena such as single-photon switching  4 , sub-Poissonian statistics  5 , squeezing  6 , and nonclassical correlations  7, 8 . However, many aspects of these experiments remain poorly understood due to difficulties associated with modelling the complicated nonlinear processes involved  9, 10 . Here we develop an analytical description of two-photon interactions in broad-band down-converted light which includes both temporal and spatial degrees of freedom  11, 12 . Our approach allows us to calculate the joint spectral intensity distribution of the down-converted field  13 , which can then be used to determine the probability density function describing the arrival times of pairs of photons produced via spontaneous parametric downconversion  14–18 . As well as providing insight into the physics underlying broadband down-conversion experiments, our analysis enables quantitative predictions about the behaviour of such systems to be made.",
        "watermark_text": "We introduce the principle for two - photon interactions in broadband down - converted radiation , particularly entanglement between photons generated by spontaneous parametric down transformation ( SPDC ) . We suggest that this results to novel influences such as photon bunching at zero time time and antibunching at nonzero delays .These conclusions are compared against empirical data acquired using SPDC sources based on periodically poled lithium niobate waveguides . The conceptual theory is also used to predict the impact of differing flow bandwidths and crystal lengths on the degree of second - order coherence k ( 2 ) ( 0 ) measured experimentally .This project was supported by EPSRC award EP / G037656 / 1 . In recent years there has been growing interest in quantum optics tests featuring wireless down - transfer 1 – 3 .Such experiments have led to experiments of new events such as single - photon mixing 4 , sub - Poissonian statistics 5 , squeezing 6 , and nonclassical correlations 7 , 8 . However , many aspects of these experiments exist poorly studied attributed to difficulties linked with modelling the complicated nonlinear processes involved 9 , 10 .Here we develop an analytical explanation of two - photon interactions in wider - band down - converted radiation which includes both temporal and spatial degrees of liberty 11 , 12 . Our solution enables us to estimate the joint spectral intensity distribution of the down - converted field 13 , which can then be used to estimate the probability density function explaining the departure periods of pairs of photons created via spontaneous parametric downconversion 14 – 18 .As well as providing information into the physics underlying wireless down - transfer experiments , our analysis enables numerical assumptions about the behaviour of such systems to be made .",
        "rewrite_text": "We present a principle regarding two-photon interactions in broadband down-converted radiation, focusing specifically on the entanglement of photons produced by spontaneous parametric down-conversion (SPDC). This leads to interesting phenomena, such as photon bunching at zero time and antibunching at nonzero delays. Our findings are compared to empirical data obtained from SPDC sources utilizing periodically poled lithium niobate waveguides. The theoretical framework also allows us to predict how variations in bandwidth and crystal length affect the experimentally measured second-order coherence, k(2)(0). This research was supported by an EPSRC award (EP/G037656/1). In recent years, there has been an increasing interest in quantum optics experiments involving wireless down-conversion. These experiments have unveiled new phenomena, including single-photon mixing, sub-Poissonian statistics, squeezing, and nonclassical correlations. However, many aspects of these experiments remain insufficiently explored due to the complexities involved in modeling the nonlinear processes at play. In this work, we develop an analytical approach to two-photon interactions within wider-band down-converted radiation, incorporating both temporal and spatial degrees of freedom. Our solution allows us to estimate the joint spectral intensity distribution of the down-converted field, which can subsequently inform the probability density function describing the timing of photon pairs generated through spontaneous parametric down-conversion. Not only does our analysis deepen the understanding of the physics behind wireless down-conversion experiments, but it also provides a basis for making numerical predictions about the behaviors of such systems.",
        "ori-fast-z-score": -1.0690449676496976,
        "water-fast-z-score": 7.247844507162112,
        "rewrite-fast-z-score": -0.3651483716701107
    },
    {
        "original_text": "The dielectric properties, phase transition behavior, and microstructure evolution were investigated for the (0.65 Pb(Ni-1/3Nb-2/3)O_3  -0.35PbTiO3) (PNT) ceramics with different sintering temperatures ranging from 850 to 1100 °C. The PNT samples exhibited high permittivity values up to ~10 4 , low loss tangent below 10 -2 , and large tunability over 30% under an electric field strength of 30 kV/cm at room temperature. With decreasing temperature down to 77 K, the permittivity increased slightly while the loss tangent decreased significantly due to the freezing out of mobile ions. At cryogenic temperatures, two relaxation processes were observed in the frequency range between 1 Hz and 100 kHz. The first process was attributed to the grain boundary effect; it shifted towards higher frequencies as the temperature decreased. The second process was associated with ferroelectric domain wall motion; its relaxation time constant remained almost unchanged when the temperature changed.",
        "watermark_text": "The dielectric characteristics , phase change response , and microstructure formation were researched for the ( 0 . 65 Pb ( Ni - 1 / 3Nb - 2 / 3 ) O _ 3 - 0 . 35PbTiO3 ) ( PNT ) ceramics with various sintering temperatures ranging from 850 to 1100 °C . The PNT specimens exhibited high permittivity values up to ~ 10 4 , low loss tangent below 10 - 2 , and large tunability over 30 % under an electric field intensity of 30 kV / cm at room temperature .With decreasing temperature down to 77 K , the permittivity increased somewhat while the loss tangent decreased significantly related to the freezing out of mobile atoms . At cryogenic temperatures , two relaxation processes were detected in the frequency region between 1 Hz and 100 kHz .The first mechanism was due to the grain boundary phenomenon ; it shifted towards higher frequencies as the temperature reduced . The second process was correlated with ferroelectric domain floor motion ; its relaxation time constant remained nearly intact when the temperature changed .",
        "rewrite_text": "The study focused on the dielectric properties, phase transition behavior, and microstructure formation of (0.65 Pb(Ni-1/3Nb-2/3)O3 - 0.35PbTiO3) (PNT) ceramics, which were sintered at temperatures ranging from 850 to 1100 °C. The PNT samples demonstrated impressive characteristics, achieving high permittivity values of approximately 10^4, a low loss tangent below 10^-2, and significant tunability exceeding 30% when subjected to an electric field intensity of 30 kV/cm at room temperature. As the temperature decreased to 77 K, the permittivity showed a slight increase, while the loss tangent decreased notably due to the immobilization of mobile atoms. At cryogenic temperatures, two distinct relaxation processes were identified within the frequency range of 1 Hz to 100 kHz. The first relaxation mechanism was associated with grain boundary effects and shifted to higher frequencies as the temperature decreased. The second process was linked to the motion of ferroelectric domain walls, with its relaxation time constant remaining relatively stable despite temperature variations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": -0.3464101615137754
    },
    {
        "original_text": "Social networks are ubiquitous in our daily life, and have been studied extensively by researchers across many disciplines. However, the analysis of social network data is often challenging due to its complexity. In this work we propose NodeTrix, an efficient hybrid representation that can be used to analyze large-scale social networks efficiently. We show how NodeTrix can be applied to solve several important problems including community detection, link prediction, node classification, and influence maximization. Our experiments on real-world datasets demonstrate that NodeTrix outperforms state-of-the-art approaches significantly both in terms of efficiency and effectiveness. 1 Introduction Social networks play an increasingly important role in people s lives. They provide us with new ways to communicate with each other, share information, collaborate, or even make friends. As such, they have attracted much attention from researchers across various fields ranging from sociology  1  , psychology  2  , biology  3  , computer science  4  , engineering  5  , etc.. The rapid development of online social media has led to unprecedented growth in the amount of available social network data  6  . For example, Facebook alone now contains more than one billion active users  7  .\nHowever, analyzing large volumes of social network data remains a challenge because it usually involves complex relationships among nodes  8  . To tackle these challenges, recent research efforts focus on developing effective representations for social networks  9  -  11  . These representations aim at capturing different aspects of social networks while being able to scale up well when dealing with massive amounts of data  12  . Among them, matrix factorization techniques  13  -  15  have shown great promise as they allow us to represent social networks using low-rank matrices  16  . Matrix factorization methods decompose a given adjacency matrix into two smaller matrices (i.e., latent factors) which capture structural properties of the original graph  17  .",
        "watermark_text": "Social networks are ubiquitous in our daily living , and have been studied frequently by researchers across many disciplines . However , the examination of social group information is often challenging due to its complexity .In this research we develop NodeTrix , an efficient hybrid representation that can be used to analyze large - scale social systems efficiently . We see how NodeTrix can be applied to solve many important problems namely community detection , link discovery , node classification , and influence maximization .Our experiments on real - global datasets prove that NodeTrix outperforms state - of - the - art methods significantly both in terms of efficiency and effectiveness . 1 Introduction Social networks take an increasingly important role in everyone s lives .They offer us with innovative ways to interact with each other , transfer material , collaborate , or even keep friends . As such , they have garnered many scrutiny from researchers across numerous topics including from geography 1 , psychology 2 , chemistry 3 , computer science 4 , engineering 5 , etc . .The rapid progress of internet social marketing has led to unprecedented growth in the quantity of available social platform data 6 . For instance , Facebook alone now contains more than one billion active people 7 .However , examining high quantities of social group information remains a problem because it often includes difficult connections among nodes 8 . To tackle these problems , recent study efforts focus on developing accurate representations for social organizations 9 - 11 .These representations aim at representing different components of social organizations while being able to level up well when dealing with massive amounts of statistics 12 . Among them , matrix factorization techniques 13 - 15 have shown great hope as they allow us to depict public networks using reduced - class matrices 16 .Matrix factorization techniques decompose a given adjacency vector into two smaller matrices ( i . e . , latent factors ) which capture structural aspects of the original graph 17 .",
        "rewrite_text": "Social networks are a pervasive aspect of our everyday lives and have been the subject of extensive research across various fields. However, analyzing information about social groups can often be complex and challenging. In this study, we introduce NodeTrix, an efficient hybrid representation designed to facilitate the analysis of large-scale social systems. We demonstrate how NodeTrix can effectively address several significant issues, including community detection, link discovery, node classification, and influence maximization. Our experiments using real-world global datasets show that NodeTrix significantly outperforms state-of-the-art methods in both efficiency and effectiveness.\n\n**1 Introduction**  \nSocial networks are increasingly integral to our lives, providing innovative ways to connect, share resources, collaborate, and maintain friendships. As a result, they have attracted considerable attention from researchers across a wide range of fields, including geography, psychology, chemistry, computer science, engineering, and more. The rapid development of social media marketing has led to a surge in the volume of data available from social platforms. For instance, Facebook now boasts over one billion active users. However, analyzing the vast amounts of data related to social groups poses challenges due to the complex interconnections between nodes. Recent research has focused on creating accurate representations of social structures to improve the analysis of large datasets. These representations are designed to effectively capture various components of social organizations while scaling well with extensive data. Among the promising approaches are matrix factorization techniques, which enable the depiction of social networks through reduced matrices. These techniques decompose an adjacency matrix into two smaller matrices (latent factors) that encapsulate the structural properties of the original graph.",
        "ori-fast-z-score": 0.15617376188860607,
        "water-fast-z-score": 9.419837224354428,
        "rewrite-fast-z-score": 0.7324096128940435
    },
    {
        "original_text": "We have developed an improved photoionization model for the soft X-ray spectrum of the Seyfert 1 galaxy NGC 4151, based on its optical and UV properties as well as previous observations in the X-ray band.  We find that the observed soft X-ray emission is dominated by line emission from highly ionized species such as O viii-xxii, Ne ix-xiii, Mg xii-xv, Si xiv-xxvi, S xix-xxxi, Ar xxviii-xxxviii, Ca xx-xxxiii, Fe xx-xxxvi, Ni xx-xxxvii, and possibly also C v-vi. In addition to these lines we predict significant contributions from continuum emission due to free-free processes (bremsstrahlung) and recombination radiation. Our best-fit parameters are consistent with those found previously using other methods. However, our results suggest that the gas density may be higher than estimated before, while the ionization parameter appears lower.",
        "watermark_text": "We have developed an better photoionization theory for the dark X - ray spectrum of the Seyfert 1 galaxy NGC 4151 , relying on its optical and UV qualities as well as earlier findings in the X - ray band . We see that the reported soft X - ray radiation is dominated by line emission from highly ionized species such as O viii - xxii , Ne ix - xiii , Mg xii - xv , Si xiv - xxvi , S xix - xxxi , Ar xxviii - xxxviii , Ca xx - xxxiii , Fe xx - xxxvi , Ni xx - xxxvii , and maybe also C v - vi .In addition to these lines we estimate significant contributions from continuum emission due to free - free processes ( bremsstrahlung ) and recombination emission . Our best - fitting values are compatible with those identified previously used other methods .However , our findings show that the gas density might be higher than calculated before , while the ionization variable appears smaller .",
        "rewrite_text": "We have developed an improved photoionization theory for the dark X-ray spectrum of the Seyfert 1 galaxy NGC 4151, leveraging its optical and UV characteristics along with prior discoveries in the X-ray region. Our analysis indicates that the observed soft X-ray radiation is predominantly influenced by line emissions from highly ionized elements, including O viii - xxii, Ne ix - xiii, Mg xii - xv, Si xiv - xxvi, S xix - xxxi, Ar xxviii - xxxviii, Ca xx - xxxiii, Fe xx - xxxvi, Ni xx - xxxvii, and potentially C v - vi. Additionally, we estimate that significant contributions arise from continuum emissions resulting from free-free processes (bremsstrahlung) and recombination. Our optimal fitting values align with those previously identified through other methods. However, our results suggest that the gas density may be greater than previously estimated, while the ionization parameter appears to be smaller.",
        "ori-fast-z-score": -1.6035674514745464,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 0.4120816918460671
    },
    {
        "original_text": "We report on the observation of ground state magneto optical resonance (GMOR) in cesium vapor confined to a sub-micron thickness layer inside a glass micro-cell. The GMOR is observed by measuring the transmission spectrum through the cell as it is rotated about its normal axis with respect to the direction of propagation of circularly polarized light. We show that this effect can be explained using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR signal strength on various parameters such as the intensity, frequency detuning and polarization angle of the incident laser beam. This work opens up new possibilities for studying quantum optics phenomena at the single atom level. \n \n In recent years there has been considerable interest in developing techniques for trapping atoms or molecules within microscopic volumes  1  . Such confinement offers several advantages over conventional atomic beams experiments including increased interaction times between the trapped particles and the applied fields  2  , improved spatial resolution  3  and reduced Doppler broadening  4  . These features are particularly important when considering applications involving high precision measurements  5  .\nIn addition to these practical benefits, confining neutral matter to small dimensions also provides opportunities for exploring fundamental physics  6  . For example, the study of Bose-Einstein condensates  7, 8  requires cooling and trapping of large numbers of atoms into very tight traps  9  . Similarly, investigations into the properties of individual atoms  10  require their isolation from other sources of decoherence  11  . Finally, studies of macroscopic quantum effects  12  may benefit from the ability to control the number of particles involved  13  . \n \n Here we describe our efforts towards achieving controlled confinement of neutral matter to extremely small dimensions. Specifically, we have developed a technique for producing a thin film of cesium gas inside a glass micro-cell  14  . By exploiting the strong magnetic dipole moment associated with the cesium ground state  15  , we observe a novel form of magneto-optical resonance  16  known as ground state magneto-optical resonance  17  . Our observations suggest that this phenomenon could provide a useful tool for investigating quantum optics processes occurring at the single atom level  18  .",
        "watermark_text": "We report on the observation of ground state magneto optical resonance ( GMOR ) in cesium vapor confined to a sub - micron thickness sheet inside a glass micro - cell . The GMOR is observed by monitoring the propagation spectrum through the cell as it is rotated about its regular axis with regard to the direction of propagation of circularly polarized light .We see that this effect can be described using simple classical electrodynamics and we present experimental results which demonstrate the dependence of the GMOR wave strength on various variables such as the frequency , frequency detuning and polarization angle of the incident beam beam . This research raises up new possibilities for studying quantum optics dynamics at the single atom level .In recent years there has been substantial interest in improving procedures for trapping atoms or compounds within microscopic volumes 1 . Such confinement gives numerous benefits over traditional molecular beams studies namely increased interaction times between the captured particles and the applied fields 2 , enhanced angular resolution 3 and reduced Doppler broadening 4 .These features are particularly important when assessing uses concerning high precision observations 5 . In addition to these useful benefits , confining neutral matter to small dimensions additionally offers options for studying basic physics 6 .For instance , the investigations of Bose - Einstein condensates 7 , 8 requires freezing and trapping of large numbers of atoms into very strict trapping 9 . Similarly , investigations into the properties of individual atoms 10 require their isolation from other sources of decoherence 11 .Finally , investigations of macroscopic quantum effects 12 may benefit from the ability to affect the quantity of particles concerned 13 . Here we explain our initiatives towards attain controlled confinement of neutral matter to incredibly small sizes .Specifically , we have developed a technique for producing a thin film of cesium gas inside a glass micro - cell 14 . By exploiting the strong magnetic dipole point involved with the cesium ground state 15 , we study a new form of magneto - optical resonance 16 known as ground state magneto - optical resonance 17 .Our observations suggest that this phenomenon might give a helpful resource for investigating quantum optics processes observed at the single atom level 18 .",
        "rewrite_text": "We present observations of ground state magneto-optical resonance (GMOR) in cesium vapor confined within a sub-micron thick layer inside a glass micro-cell. The GMOR is detected by analyzing the propagation spectrum as the cell is rotated around its axis relative to the direction of circularly polarized light. Our findings indicate that this effect can be explained using basic principles of classical electrodynamics, and we provide experimental evidence illustrating how the strength of the GMOR wave correlates with various factors, including frequency, frequency detuning, and the polarization angle of the incoming beam. This research opens up new avenues for investigating the dynamics of quantum optics at the single-atom level. Recently, there has been considerable interest in enhancing techniques for trapping atoms or molecules in microscopic volumes, which offers several advantages over traditional molecular beam studies, such as increased interaction times between trapped particles and external fields, improved angular resolution, and minimized Doppler broadening. These characteristics are particularly crucial for high-precision observations. Additionally, confining neutral matter to small scales provides opportunities for exploring fundamental physics, such as investigating Bose-Einstein condensates, which require the freezing and trapping of large atom numbers within tight constraints. Similarly, studying the properties of individual atoms necessitates their isolation from decoherence sources. Moreover, research into macroscopic quantum effects may benefit from the ability to manipulate particle quantities. In this context, we detail our efforts to achieve controlled confinement of neutral matter at remarkably small sizes. Specifically, we have developed a technique to create a thin film of cesium gas inside a glass micro-cell. Leveraging the strong magnetic dipole properties associated with the cesium ground state, we investigate a novel form of magneto-optical resonance known as ground state magneto-optical resonance. Our observations suggest that this phenomenon could serve as a valuable resource for exploring quantum optical processes at the single-atom level.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.460169506877884,
        "rewrite-fast-z-score": 3.0361458822299396
    },
    {
        "original_text": "We show that the equivalence principle (EP) is violated in quantum gravity if there are dilatons or axions coupled to photons. The violation can be tested by measuring the polarization rotation angle of light passing through gravitational fields. We find that the EP-violating effect is suppressed for macroscopic distances but enhanced for microscopic ones such as those inside atoms. This suggests that the measurement of atomic clocks may provide an opportunity to test the EP at high precision. \n \n In this work we study how the equivalence principle (EP), which states that all local physical laws should take their same form in any freely falling frame, is modified when one considers quantum gravity effects. It has been shown previously that the EP is violated in general relativity with massive gravitons  1  . Here we consider whether it remains valid in theories beyond Einstein s theory where new degrees of freedom exist. Specifically, we focus on two types of models: scalar-tensor theories  2  , including Brans-Dicke theory  3  , and string-theory inspired models  4  .\n \nIn these theories, dilatons and/or axions appear as additional degrees of freedom besides graviton(s). Dilatons couple directly to photons while axions do so indirectly via coupling to photons and gluons  5  . These couplings lead to violations of the EP  6  . For example, in scalar-tensor theories, the photon acquires a mass term proportional to the strength of the gravitational field  7, 8  . As a result, the speed of light depends on its direction relative to the gravitational field  9  . If the gravitational field varies along the path of propagation, then the speed of light also changes accordingly  10  . Since different polarizations travel at slightly different speeds, they acquire different phases during propagation  11  . Therefore, the polarization state of light will rotate after traveling through a gravitational potential gradient  12  .",
        "watermark_text": "We see that the equivalence principle ( EP ) is violated in quantum gravitational if there are dilatons or axions related to photons . The violation can be evaluated by monitoring the polarization rotation angle of light traveling through gravity fields .We see that the EP - violating phenomenon is suppressed for macroscopic distances but improved for microscopic ones such as those inside atoms . This implies that the observation of atomic clocks may provide an ability to test the EP at high precision .In this research we study how the equivalence principle ( EP ) , which says that all local physical rules should take their same shape in any freely falling frame , is modified when one studies quantum gravitational changes . It has been shown previously that the EP is violated in general relativity with massive gravitons 1 .Here we study whether it remains accepted in theories beyond Einstein s theory where new degrees of liberty exist . Specifically , we focus on two forms of models : scalar - vector models 2 , notably Brans - Dicke theory 3 , and string - theory inspired models 4 .In these theories , dilatons and / or axions exist as additional degrees of autonomy besides graviton ( s ) . Dilatons couple directly to photons while axions do so indirectly via coupling to photons and gluons 5 .These couplings contribute to violations of the EP 6 . For instance , in scalar - vector theories , the photon acquires a mass term proportional to the strength of the gravitational field 7 , 8 .As a result , the speed of light changes on its direction relative to the gravitational field 9 . If the gravitational field varies along the path of propagation , then the speed of light additionally changes similarly 10 .Since various polarizations move at slightly different speeds , they acquire various phases during propagation 11 . Therefore , the polarization state of light will rotate after moving through a gravitational potential gradient 12 .",
        "rewrite_text": "The equivalence principle (EP) can be violated in quantum gravity if dilatons or axions interact with photons. This violation can be assessed by observing the rotation of the polarization angle of light as it passes through gravitational fields. It appears that while EP-violating effects are diminished over macroscopic distances, they are enhanced at microscopic scales, such as those found within atoms. This suggests that atomic clocks may offer a means to test the EP with high precision. In this study, we explore how the equivalence principle, which asserts that local physical laws should remain consistent in any freely falling frame, is altered by quantum gravitational changes. Previous work has indicated that the EP may be violated in general relativity with massive gravitons. Here, we investigate whether this violation persists in theories beyond Einstein's framework that introduce new degrees of freedom. We focus on two types of models: scalar-vector theories, particularly Brans-Dicke theory, and string-theory inspired models. In these frameworks, dilatons and/or axions serve as supplementary degrees of freedom in addition to gravitons. Dilatons couple directly with photons, while axions couple indirectly through interactions with photons and gluons. These couplings are responsible for the violations of the EP. For example, in scalar-vector models, the photon obtains a mass term that is proportional to the strength of the gravitational field. Consequently, the speed of light can vary in relation to the direction of the gravitational field. If the gravitational field changes along the path of the light, this can further alter the speed of light. Different polarizations of light traveling through these varying speeds will pick up distinct phases, leading to a rotation of the polarization state after passing through a gradient in gravitational potential.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 7.950706915615445,
        "rewrite-fast-z-score": 2.6148419405355536
    },
    {
        "original_text": "We present the results of our study on gravitational wave (GW) foregrounds for the Laser Interferometer Space Antenna (LISA). We focus on double white dwarfs, which are expected to be one of the most important sources in terms of GW energy density and event rate. In particular we investigate how their properties depend on the initial conditions at formation time as well as on the subsequent evolution driven by nuclear burning and orbital decay due to emission of gravitational waves. The latter is studied with detailed numerical simulations using an updated version of the Eggleton code that includes general relativistic effects. \n \n Our main findings can be summarized as follows: \n \n 1. We find that the number of systems detectable within a given volume depends strongly on the assumed distribution function of binary parameters such as mass ratio q = M2/M1 or total system mass Mtot = M1 + M2. This dependence arises because different distributions lead to very different fractions of binaries with favorable orientations relative to the detector s line-of-sight. If all binaries have random orientation then only about 10% of them will produce signals above the detection threshold. On the other hand if they form preferentially face-on this fraction increases up to 50%. Therefore it seems crucially important to determine the true distribution functions of these quantities observationally before making any predictions regarding the number of detections. \n \n 2. We show that there exists a strong correlation between the masses of the two components of a double white dwarf binary. As a result, the majority of systems detected by LISA will consist of nearly equal-mass objects. However, even though the average value of q is close to unity, there still exist many systems where the secondary component has significantly lower mass than its companion. These systems may provide valuable information about the physics of stellar mergers since they allow us to probe the regime of low-q binaries not accessible through observations of single degenerate stars.",
        "watermark_text": "We present the conclusion of our research on gravity wave ( GW ) foregrounds for the Laser Interferometer Space Antenna ( LISA ) . We focus on double white dwarfs , which are expected to be one of the most important sources in terms of GW energy density and event frequency .In particular we investigate how their characteristics rely on the first parameters at structure point as well as on the subsequent evolution accelerated by nuclear burning and orbital decay owing to emission of gravitational waves . The latter is studied with comprehensive numerical simulations using an updated edition of the Eggleton code that contains general relativistic effects .Our main results can be summarized as follows : 1 . We see that the quantity of systems detectable within a given volume depends strongly on the assumed distribution function of binary parameters such as mass ratio p = M2 / M1 or total system weight Mtot = M1 + M2 .This dependence occurs because different distributions result to very different fractions of binaries with favorable orientations relative to the sensor s line - of - view . If all binaries have random orientation then only about 10 % of them will generate frequencies above the screening threshold .On the other hand if they occur preferentially face - on this amount rises up to 50 % . Therefore it appears crucially vital to estimate the true distribution functions of these quantities observationally before making any observations involving the number of detections .2 . We see that there exists a powerful correlation between the masses of the two parts of a double white dwarf binary .As a result , the majority of components observed by LISA will consist of almost equal - mass particles . However , even though the average value of q is close to unity , there still exist many systems where the secondary component has much lower weight than its companion .These systems may provide valuable info about the physics of stars mergers since they allow us to probe the regime of low - q binaries not accessible through observations of single degenerate stars .",
        "rewrite_text": "We present the findings of our research on gravitational wave (GW) foregrounds for the Laser Interferometer Space Antenna (LISA), focusing specifically on double white dwarfs, which are anticipated to be significant contributors in terms of GW energy density and event frequency. Our analysis examines how their characteristics depend on initial parameters at the structural point and their subsequent evolution, which is accelerated by nuclear burning and orbital decay due to gravitational wave emission. This evolution is explored through detailed numerical simulations utilizing an updated version of the Eggleton code that incorporates general relativistic effects. Our key results can be summarized as follows: \n\n1. The number of systems detectable within a specific volume is highly sensitive to the assumed distribution function of binary parameters, such as the mass ratio \\( p = M_2 / M_1 \\) and the total system mass \\( M_{\\text{tot}} = M_1 + M_2 \\). The variation in distributions leads to significantly different fractions of binaries oriented favorably toward the detector's line of sight. For randomly oriented binaries, only about 10% will produce frequencies above the detection threshold; however, if they are oriented preferentially face-on, this percentage increases to 50%. Therefore, accurately estimating the true distribution functions of these parameters is critical before making observational predictions about the number of detectable events. \n\n2. We also find a strong correlation between the masses of the components in a double white dwarf binary. Consequently, most systems detected by LISA will consist of nearly equal-mass components. Despite the average mass ratio \\( q \\) being close to unity, there remain many systems where the secondary component is significantly less massive than its companion. These low-mass ratio systems could provide important insights into the physics of stellar mergers, enabling us to investigate the low-\\( q \\) binary regime that is not accessible through observations of single degenerate stars.",
        "ori-fast-z-score": 0.16116459280507606,
        "water-fast-z-score": 6.93007749061827,
        "rewrite-fast-z-score": -0.48666426339228763
    },
    {
        "original_text": "We report on point contact Andreev reflection (PCAR) measurements performed on single crystals of the heavy fermion compound HoNi2B2C, which is an antiferromagnet with T N = 1.5 K that becomes a type-II superconductor below Tc = 0.8 K. The PCAR spectra show clear evidence for multiple gaps at low temperatures. We find two distinct gap values, one of them being close to twice the value of the other. This observation suggests that there are two different bands crossing the Fermi level. In addition we observe a temperature dependence of both gaps indicating their nodal character. Our results provide further insight into the electronic structure of this material. Heavy-fermion compounds have attracted considerable interest over recent years because they often exhibit unconventional physical properties such as non-Fermi liquid behavior or even quantum criticality  1  . These materials can be described by the periodic Anderson model  2  , where conduction electrons hybridize strongly with localized f -electrons leading to the formation of narrow bands near the Fermi energy E F  3  .\nHoNi 2 B 2 C belongs to the family of so-called borocarbides  4  . It crystallizes in the tetragonal ThCr 2 Si 2 structure  5  and has been shown to become a type-II superconductor  6  below T c ≈ 0.8 K  7, 8  . At ambient pressure it orders magnetically around T N = 1.6 K  9  . Recent studies suggest that the magnetic order is driven by strong spin-orbit coupling  10  . A number of experiments indicate that the ground-state wave function consists of singlet pairs  11, 12  . However, the exact nature of the pairing mechanism remains unclear  13  .",
        "watermark_text": "We report on point touch Andreev reflection ( PCAR ) observations performed on single crystals of the heavy fermion compound HoNi2B2C , which is an antiferromagnet with T N = 1 . 5 K that remains a class - II superconductor below Tc = 0 . 8 K . The PCAR spectra show good evidence for multiple gaps at low temperatures . We see two separate gap values , one of them being close to double the value of the other .This measurement indicates that there are two different bands crossing the Fermi level . In addition we study a temperature dependence of both gaps indicating their nodal nature .Our results yield further insight into the electronic stability of this material . Heavy - fermion compounds have garnered considerable interest over recent months because they frequently exhibit unusual physical properties such as non - Fermi solid behavior or even quantum criticality 1 .These compounds can be described by the periodic Anderson model 2 , where conduction electrons hybridize heavily with localized f - ions causing to the formation of narrow bands near the Fermi energy E F 3 . HoNi 2 B 2 C belongs to the class of so - called borocarbides 4 .It crystallizes in the tetragonal ThCr 2 Si 2 structure 5 and has been shown to become a class - II superconductor 6 below T c ≈ 0 . 8 K 7 , 8 . At ambient pressure it orders magnetically around T N = 1 . 6 K 9 .Recent research suggest that the magnetic order is caused by strong spin - orbit interaction 10 . A variety of studies reveal that the ground - state wave function consists of singlet sets 11 , 12 .However , the exact nature of the pairing structure remains disputed 13 .",
        "rewrite_text": "We present findings from point contact Andreev reflection (PCAR) experiments conducted on single crystals of the heavy fermion compound HoNi2B2C, which is known as an antiferromagnet with a Néel temperature (T_N) of 1.5 K and behaves as a class-II superconductor below a critical temperature (T_c) of 0.8 K. The PCAR spectra provide strong evidence for the presence of multiple gaps at low temperatures, revealing two distinct gap values—one nearly double the other. This observation suggests the existence of two different bands crossing the Fermi level. Furthermore, we examine the temperature dependence of both gaps, indicative of their nodal characteristics. Our results enhance the understanding of the electronic stability of this material. Heavy fermion compounds have attracted significant interest recently due to their unusual physical properties, such as non-Fermi liquid behavior and potential quantum criticality. These materials can be explained using the periodic Anderson model, which describes the heavy hybridization of conduction electrons with localized f-electrons, leading to the formation of narrow bands near the Fermi energy (E_F). HoNi2B2C falls within the category of borocarbides and crystallizes in the tetragonal ThCr2Si2 structure. It has been established that this compound transitions to a class-II superconductor below approximately 0.8 K, while at ambient pressure it exhibits magnetic ordering around T_N ≈ 1.6 K. Recent studies suggest that this magnetic order arises from strong spin-orbit coupling, and various investigations indicate that the ground-state wave function comprises singlet pairs. However, the precise nature of the pairing mechanism remains a topic of debate.",
        "ori-fast-z-score": 0.5720775535473553,
        "water-fast-z-score": 7.181324987175317,
        "rewrite-fast-z-score": 0.18257418583505536
    },
    {
        "original_text": "We study the possibility that there are two different lifetimes for neutral B mesons, one corresponding to the standard model and another to new physics beyond it.  We find that if the decay widths into final states with charm quarks differ by more than about 10% between these two types of B mesons then this can be observed at future experiments such as LHCb or Belle II. If we assume that the ratio of branching fractions is equal to 1 (as predicted within the Standard Model) but allow the total decay widths to vary independently, then we show how the experimental data on the time dependent CP asymmetry parameters SCP and ACP can be used to determine whether the difference in decay widths is due to new physics effects or not. Finally, we discuss possible extensions of our analysis which could lead to further constraints on the allowed parameter space. The results presented here will also have implications for other measurements performed at hadron colliders involving heavy flavour particles.",
        "watermark_text": "We research the prospect that there are two different lifetimes for neutral B mesons , one corresponding to the standard description and another to new science beyond it . We see that if the decay widths into initial states with charm quarks vary by more than about 10 % between these two kind of B mesons then this can be observed at current experiments such as LHCb or Belle II .If we suppose that the proportion of branching fractions is equal to 1 ( as predicted within the Standard Model ) but allow the total degradation widths to vary independently , then we explain how the theoretical data on the period dependent CP asymmetry characteristics SCP and ACP can be used to predict whether the difference in decay widths is due to novel physics effects or not . Finally , we explain possible extensions of our analysis which potentially contribute to further limitation on the allowed parameter area .The results presented here will also have consequences for other tests accomplished at hadron colliders regarding heavy flavour atoms .",
        "rewrite_text": "We investigate the possibility of two distinct lifetimes for neutral B mesons: one aligned with the standard model and another linked to new physics. Our analysis reveals that if the decay widths into initial charm quark states differ by more than approximately 10% between these two types of B mesons, this difference could be detected in current experiments such as LHCb or Belle II. Assuming a branching fraction ratio of 1 (as suggested by the Standard Model), while allowing the total decay widths to vary independently, we outline how theoretical data on time-dependent CP asymmetry parameters, SCP and ACP, can be utilized to determine whether any observed differences in decay widths are indicative of new physics. Additionally, we discuss potential extensions to our analysis that may further refine the constraints on the allowed parameter space. The findings presented here may also influence other experiments conducted at hadron colliders concerning heavy flavor physics.",
        "ori-fast-z-score": -2.032002032003048,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": -2.321219442769799
    },
    {
        "original_text": "We study the circular and non-circular motion near the event horizons of rotating black holes by using the Hamilton-Jacobi method, which is an extension of the standard geodesic approach to include higher-order corrections due to gravitational radiation reaction effects. We find that for both circular and non-circular motions there exist two families of solutions with different orbital frequencies at the same radius. The inner family has smaller orbital frequency than the outer one; it corresponds to bound orbits while the outer solution describes unbound orbits. For circular orbits we show how these results can be obtained directly from the first law of black hole mechanics. In addition, we also present numerical evidence showing that the innermost stable circular orbit (ISCO) moves inward as the spin parameter increases. Finally, we discuss some implications of our results on astrophysical phenomena such as accretion disks around spinning black holes. Introduction -The discovery of the first binary pulsar PSR1913+16  1  , together with its subsequent measurement of the mass ratio between the neutron star and its companion white dwarf  2  , led to the prediction  3  that most likely all massive stars end their lives as black holes surrounded by accretion disks  4  . Since then many other observations have been made confirming this picture  5  .\nIn order to understand the dynamics of matter falling into black holes, it is important to know where particles are trapped or scattered out  6  . This information is encoded in the location of the so-called Innermost Stable Circular Orbit (ISCO), i.e., the smallest possible radius r ISCO of a particle s circular orbit  7, 8  . It turns out that the value of r ISCO depends sensitively on the spin angular momentum J = Ma 2 /(2r g ) of the black hole  9  : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO decreases rapidly until finally it reaches the Schwarzschild radius R s ≡ 2GM/c 2  10  . Therefore, knowing the exact position of the ISCO will help us better understand the physics behind various processes taking place close to",
        "watermark_text": "We explore the circular and non - circular motion near the event horizons of spinning black holes by using the Hamilton - Jacobi method , which is an extension of the standard geodesic approach to use larger - order corrections due to gravitational radiation process effects . We see that for both circular and non - circular movements there exist two families of solutions with varying orbital frequencies at the same radius .The inner family has less orbital frequency than the inner one ; it corresponds to bound orbits while the inner solution refers unbound orbits . For circular orbits we give how these results can be obtained directly from the first law of brown hole mechanics .In addition , we also discuss numerical information demonstrating that the innermost stable circular orbit ( ISCO ) moving inward as the spin parameter grows . Finally , we explain some implications of our findings on astrophysical processes such as accretion balls around spun dark holes .Introduction - The observation of the first binary pulsar PSR1913 + 16 1 , combined with its subsequent calculation of the mass ratio between the neutron star and its companion white dwarf 2 , leading to the scenario 3 that most likely all large galaxies begin their careers as black holes populated by accretion disks 4 . Since then many other experiments have been made confirming this picture 5 .In order to comprehend the dynamics of matter falling into black holes , it is important to consider where ions are captured or scattered out 6 . This knowledge is stored in the location of the so - called Innermost Stable Circular Orbit ( ISCO ) , i . e . , the smallest available diameter r ISCO of a particle s circular orbit 7 , 8 .It turns out that the value of r ISCO relies sensitively on the spin angular velocity J = Ma 2 / ( 2r g ) of the dark hole 9 : if J < M 2 , then r ISCO > 3M ; but when J approaches M 2 , r ISCO falls slowly until finally it meets the Schwarzschild diameter R s ≡ 2GM / c 2 10 . Therefore , knowing the exact position of the ISCO will assist us better understand the physics behind several mechanisms taking place nearby to",
        "rewrite_text": "We investigate both circular and non-circular motion near the event horizons of spinning black holes utilizing the Hamilton-Jacobi method, which enhances the standard geodesic approach by incorporating higher-order corrections due to the effects of gravitational radiation. Our analysis reveals two distinct families of solutions characterized by different orbital frequencies at the same radius: the outer family corresponds to bound orbits with lower orbital frequencies, while the inner family pertains to unbound orbits. For circular orbits, we demonstrate how these findings can be derived directly from the first law of black hole mechanics. Additionally, we present numerical analyses that illustrate how the innermost stable circular orbit (ISCO) decreases in radius as the spin parameter increases. Lastly, we discuss the implications of these results for astrophysical phenomena, such as accretion disks surrounding spinning black holes. \n\nIntroduction - The discovery of the binary pulsar PSR1913 + 16 1, along with subsequent measurements of the mass ratio between the neutron star and its companion white dwarf, suggests that most large galaxies likely began their existence as black holes surrounded by accretion disks. This view has been corroborated by numerous subsequent studies. To effectively understand the dynamics of matter falling into black holes, it is crucial to examine where ions are either captured or expelled. This information is encapsulated in the concept of the innermost stable circular orbit (ISCO) - the smallest possible radius (r_ISCO) for a particle's circular orbit. It has been found that the value of r_ISCO is highly sensitive to the spin angular momentum of the black hole, represented by J = M a² / (2r_g). Specifically, if J < M², then r_ISCO > 3M; however, as J approaches M², r_ISCO gradually decreases until it ultimately aligns with the Schwarzschild radius R_s ≡ 2GM/c². Understanding the precise location of the ISCO will enhance our comprehension of the physical mechanisms occurring in the vicinity of black holes.",
        "ori-fast-z-score": -1.7905475715715027,
        "water-fast-z-score": 6.6577138248976375,
        "rewrite-fast-z-score": -0.242535625036333
    },
    {
        "original_text": "We present an alternative derivation of Einstein gravity, which does not use the axiom of choice and is based on the concept that topology can be hidden inside general relativity (GR). We show how to construct a set of local coordinates for any given spacetime point such that all points with the same coordinate values are connected by geodesics. This construction allows us to define a metric tensor at each point as well as its inverse. The resulting theory has exactly the same field equations as standard GR but it contains additional degrees of freedom corresponding to the number of disconnected components of the underlying space. These extra degrees of freedom do not affect classical solutions because they correspond to gauge transformations. However, we argue that these new degrees of freedom may play an important role when considering quantum effects. In particular, we discuss possible implications of our approach for black hole entropy calculations. Finally, we comment on some open problems related to this work.",
        "watermark_text": "We introduce an alternative derivation of Einstein relativity , which does not use the axiom of choice and is based on the idea that geometry can be hidden inside general relativity ( GR ) . We see how to build a setting of local coordinates for any certain spacetime point such that all points with the same coordinate parameters are connected by geodesics .This construction gives us to define a metric tensor at each point as well as its inverse . The resulting theory has precisely the same field equations as conventional GR but it contains additional degrees of liberty corresponding to the number of disconnected components of the underlying set .These added degrees of liberty do not alter classical solutions because they relate to gauge processes . However , we feel that these new degrees of liberty might play an important role when assessing quantum effects .In particular , we discuss possible possibilities of our approach for black hole entropy calculations . Finally , we comment on some open problems related to this research .",
        "rewrite_text": "We present an alternative formulation of Einstein's theory of relativity that avoids the use of the axiom of choice and is grounded in the concept that geometry can be integrated within general relativity (GR). We demonstrate how to establish a framework of local coordinates around any given point in spacetime, ensuring that all points sharing the same coordinate parameters are interconnected by geodesics. This framework allows us to define a metric tensor at every point, along with its inverse. The resulting theory aligns perfectly with the standard field equations of general relativity but introduces additional degrees of freedom that correspond to the number of disconnected components of the underlying set. Although these new degrees of freedom do not affect classical solutions as they pertain to gauge processes, we believe they may be significant in evaluating quantum effects. In particular, we explore the potential implications of our approach for calculating black hole entropy. Lastly, we highlight several unresolved questions related to this research.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 0.11396057645963795
    },
    {
        "original_text": "We study stability properties of the periodic Toda lattice with respect to small  time-periodic perturbations. We show that if the perturbation is sufficiently small, then there exists an exponentially stable solution which can be found by solving a linear algebraic system. The proof relies on the Floquet theory and Lyapunov-Schmidt reduction method. This result generalizes previous results obtained in the case when the unperturbed system has only one equilibrium point or when it possesses two equilibria but they are not connected via heteroclinic orbits. In particular we prove that for any number of equilibria greater than 2 (including infinitely many) there exist arbitrarily large perturbations such that all solutions of perturbed systems converge to infinity as t → ∞. \nIntroduction\n\nThe Toda lattice is a classical example of a completely integrable Hamiltonian system introduced by Toda  Tod  . It describes N particles moving along straight lines with pairwise exponential interaction potential between them. For simplicity let us consider the case N = 1. Then the equation describing this motion takes the form \nwhere x(t), y(t) ∈ R n , A :  0, T   × R n → R n×n is continuous matrix-valued function satisfying some additional conditions specified below. If A ≡ 0, i.e., no external forces act upon the particle, then the corresponding solution is called the free Toda flow. It was shown in  KN  that the free Toda flow is globally asymptotically stable provided that the spectrum of the matrix A does not intersect the imaginary axis. Moreover, the authors proved that the set of initial data leading to bounded trajectories coincides with the set of initial data belonging to the basin of attraction of the zero solution. However, these results do not hold true anymore if the matrix A depends on time.",
        "watermark_text": "We explore equilibrium properties of the periodic Toda lattice with regard to small time - periodic perturbations . We see that if the perturbation is sufficiently small , then there exists an exponentially stable solution which can be found by solving a linear mathematical scheme .The proof uses on the Floquet model and Lyapunov - Schmidt reduction technique . This result generalizes earlier findings obtained in the case when the unperturbed body has only one equilibrium point or when it enjoys two equilibria but they are not linked via heteroclinic orbits .In particular we prove that for any number of equilibria greater than 2 ( including infinitely many ) there exist arbitrarily huge perturbations such that all solutions of perturbed systems converge to infinity as t → ∞ . Introduction The Toda lattice is a traditional instance of a completely integrable Hamiltonian structure developed by Toda Tod .It describes N particles moving along straight lines with pairwise exponential interaction potential between them . For simplicity let us consider the case N = 1 .Then the equation explaining this motion takes the form where x ( t ) , y ( t ) ∈ R n , A : 0 , T × R n → R n×n is continuous matrix - valued function satisfying some additional conditions defined below . If A ≡ 0 , i . e . , no external forces action upon the particle , then the equivalent solution is dubbed the free Toda flow .It was shown in KN that the free Toda flow is internationally asymptotically stable given that the spectrum of the matrix A does not intersect the imaginary axis . Moreover , the papers proved that the set of initial data leading to finite trajectories coincides with the set of initial results belonging to the basin of attraction of the zero solution .However , these results do not stand true anymore if the function A depends on time .",
        "rewrite_text": "We investigate the equilibrium properties of the periodic Toda lattice in relation to small time-periodic perturbations. Our findings indicate that if the perturbation is sufficiently small, there exists an exponentially stable solution that can be obtained by solving a linear mathematical framework. The proof employs the Floquet theory and the Lyapunov-Schmidt reduction technique. This result extends previous discoveries made in cases where the unperturbed system has only one equilibrium point or two equilibria that are not connected by heteroclinic orbits. Specifically, we demonstrate that for any number of equilibria greater than two (including infinitely many), there exist arbitrarily large perturbations such that all solutions of the perturbed systems diverge to infinity as time approaches infinity. \n\nIntroduction: The Toda lattice is a classical example of a completely integrable Hamiltonian system introduced by Toda. It describes the motion of N particles along straight lines, with a pairwise exponential interaction potential between them. For simplicity, consider the case of N = 1. The governing equation for this motion can be expressed where x(t) and y(t) belong to R^n, and A: [0, T] × R^n → R^(n×n) is a continuous matrix-valued function subject to certain additional conditions outlined below. When A is identically zero, indicating no external forces acting on the particle, the corresponding solution is known as the free Toda flow. It has been established in prior studies that the free Toda flow is globally asymptotically stable, provided the spectrum of the matrix A does not intersect the imaginary axis. Furthermore, it has been shown that the set of initial conditions leading to finite trajectories aligns with those initial conditions within the basin of attraction of the zero solution. However, these results no longer hold if the function A is time-dependent.",
        "ori-fast-z-score": 0.2705008904002297,
        "water-fast-z-score": 6.2215204792052825,
        "rewrite-fast-z-score": 1.0776318121606494
    },
    {
        "original_text": "Globular clusters are dense stellar systems that contain thousands to millions of stars, and may be the oldest gravitationally bound objects known.  The discovery of planets around other stars has raised questions about whether or not globular cluster members can also harbor planetary systems.   In this work we use Monte Carlo simulations to examine how many planets could exist within globular clusters with different masses and ages.  We find that for most reasonable assumptions on planet formation rates, there should be at least one planet per star in all but the youngest (<10 Myr) and lowest mass (<100 Msun) clusters.  This result is robust against uncertainties in our knowledge of planet formation efficiencies and initial conditions such as the number density distribution of planetesimals.  Our results suggest that it will be possible to detect planets orbiting globular cluster members using current observational techniques. Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "watermark_text": "Globular complexes are dense stellar structures that host hundreds to millions of stars , and may be the earliest gravitationally locked objects known . The observation of planets around other stars has raised questions about whether or not globular cluster groups can also harbor planetary structures .In this research we utilize Monte Carlo simulations to examine how many worlds could occur within globular complexes with various masses and periods . We see that for most reasonable assumptions on planet development rates , there should be at least one planet per star in all but the youngest ( < 10 Myr ) and lowest mass ( < 100 Msun ) clusters .This result is robust against uncertainties in our know of planet development efficiencies and first situations such as the number density density of planetesimals . Our results propose that it will be possible to identify planets orbiting globular cluster elements using current observational techniques .Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "rewrite_text": "Globular clusters are compact stellar formations that contain hundreds to millions of stars and are possibly among the earliest gravitationally bound structures known to us. The discovery of exoplanets around various stars has sparked curiosity about the potential for planets to exist within globular clusters as well. In this study, we employ Monte Carlo simulations to explore the likelihood of planetary systems forming within globular clusters of varying masses and orbital periods. Our findings indicate that, under most plausible assumptions regarding the rates of planet formation, there should be at least one planet for every star in nearly all clusters, except for the youngest ones (less than 10 million years old) and the least massive ones (under 100 solar masses). This conclusion holds true even when accounting for uncertainties in our understanding of planetary development and initial conditions, such as the density of planetesimals. Our results suggest that it may soon be possible to detect planets orbiting stars in globular clusters utilizing existing observational methods.  \nKeywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": -0.2182178902359924
    },
    {
        "original_text": "We present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.",
        "watermark_text": "We present new experiments done with the Cosmosoma study , which were built to search for indication of an amount in cosmic microwave background ( CMB ) temperature fluctuations above those predicted by traditional cosmological predictions . The data are compatible with predictions based on current theoretical knowledge but display some surprising characteristics that might be connected to formerly unidentified foreground sources or systematic effects involved with our analysis methods .We have utilized these results to place limits on potential contributions from primordial magnetic waves and other exotic processes such as topological defects . These restrictions are comparable to previous measurements obtained using separate observation approaches .In addition we report the finding of a substantial frequency at speeds below 10GHz , which is not anticipated within conventional cosmological predictions . This might represent either a new cause of foreground contamination or a novel physical impact .Further investigation will demand additional studies to confirm this result and establish its identity . If confirmed it would offer important restrictions on experiments pursuing to explain the observed anisotropy in the CMB spectrum .",
        "rewrite_text": "We present new experiments conducted as part of the Cosmosoma study, aimed at searching for signs of cosmic microwave background (CMB) temperature fluctuations that exceed the levels predicted by conventional cosmological models. While the data align with current theoretical expectations, they exhibit some unexpected features that may be linked to previously unrecognized foreground sources or systematic effects related to our analytical methods. We have used these findings to impose limits on potential contributions from primordial magnetic waves and other exotic phenomena, such as topological defects. These constraints are on par with previous measurements obtained through different observational techniques. Additionally, we report the discovery of a significant frequency below 10 GHz, which is not predicted by standard cosmological theories. This could indicate either a new source of foreground contamination or a novel physical effect. Further research is necessary to verify this result and determine its significance. If corroborated, it could provide critical constraints for experiments aimed at explaining the observed anisotropy in the CMB spectrum.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 7.233165373381237,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "We study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "We explore the slow frequency resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity matrix and thickness . We see that SWR is possible only if all primary directions of the permittivity tensors are connected to one another within each surface .In this situation we derive explicit expressions for the dispersion constant between the frequency f and the Bloch wavenumber kx . The results derived can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies .Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations . 1 Introduction Periodic multilayers consisting of alternating thin sheets formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 .These include high reflectance 2 , negative refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 . In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic surfaces may exhibit very interesting electrical processes including slow frequency resonance ( S WR ) .This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 . It results to incredibly large values of the effective refractive index n eff = c / u ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 .As a result , the associated transmission spectrum exhibits severe spikes identified with narrow stop rings 13 . Such characteristics are extremely practical for numerous practical applications 14 .However , despite several theoretical experiments devoted to S WR in periodic multilayers 15 – 18 , there still appear several open questions related to the conditions under which this phenomenon happens place 19 , 20 . For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned .On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "We investigate the slow frequency resonance (SWR) effect in periodically layered media with an arbitrary number \\( N \\) of anisotropic layers, each characterized by its own permittivity matrix and thickness. Our analysis reveals that SWR can only occur when all principal directions of the permittivity tensors are interconnected on each surface. Under these conditions, we derive explicit formulas for the dispersion constant relating frequency \\( f \\) to the Bloch wavenumber \\( k_x \\). The findings can serve as a framework for designing multilayered structures that exhibit pronounced SWR effects at low frequencies. \n\n**Keywords:** Slow wave vibration; Anisotropy; Multilayer structure; Dispersion relations.\n\n**1. Introduction**  \nRecent studies have focused on periodic multilayers formed by alternating thin sheets of various materials due to their unique properties. These properties include high reflectance, negative refraction, and enhanced nonlinear optical responses, making them attractive for applications in optoelectronic technology and photovoltaics. Previous research has demonstrated that periodic multilayers with anisotropic surfaces may display intriguing electrical phenomena, including slow frequency resonance (SWR). This occurs when the phase velocity of Bloch waves within the medium reaches zero, resulting in exceptionally high values of the effective refractive index \\( n_{\\text{eff}} = c / v_{\\text{ph}} \\), where \\( c \\) is the speed of light in a vacuum and \\( v_{\\text{ph}} \\) is the phase velocity of the Bloch mode. Consequently, the associated transmission spectrum displays pronounced spikes corresponding to narrow stop bands, which have significant practical implications. Despite several theoretical investigations into SWR in periodic multilayers, many questions remain regarding the specific conditions that enable this phenomenon. For instance, experimental evidence indicates that even a single misaligned anisotropic surface can completely negate the SWR effect, regardless of the alignment of other layers. Additionally, numerical simulations suggest that...",
        "ori-fast-z-score": -0.15811388300841897,
        "water-fast-z-score": 8.432781346758377,
        "rewrite-fast-z-score": 3.3333333333333335
    },
    {
        "original_text": "We present the results of cosmological hydrodynamic simulations that follow the evolution of dark matter haloes within different cosmic environments (clusters, filaments, sheets and voids). We find that:\n(i) The mass accretion histories of clusters are dominated by major mergers with other massive systems at high redshifts z > 1.\n(ii) In contrast to clusters, most of the growth of filamentary structures is driven by smooth gas accretion along their length.  This leads to an extended formation history for these objects which can be traced back to early times z < 5. (iii) Sheet-like structures form through the merger of smaller filaments into larger ones. They grow mainly via smooth gas accretion but also experience minor mergers with small groups or galaxies during their lifetime. (iv) Voids evolve almost exclusively due to smooth gas accretion. Their assembly time-scales are typically longer than those of clusters and filaments because they have less dense surroundings.",
        "watermark_text": "We present the conclusion of cosmological hydrodynamic simulations that take the evolution of dark matter haloes within various cosmic environments ( complexes , filaments , sheets and voids ) . We see that : ( i ) The mass accretion histories of clusters are dominated by major mergers with other giant bodies at high redshifts z > 1 .( ii ) In comparison to groups , most of the development of filamentary structures is caused by smooth gas accretion along their duration . This leads to an extended structure life for these objects which can be traced back to early years z < 5 .( iii ) Sheet - like structures develop through the merger of tiny filaments into larger ones . They develop principally via smooth gas accretion but also experience minor mergers with little groups or galaxies during their lifetime .( iv ) Voids evolve virtually primarily due to soft gas accretion . Their development time - scales are typically longer than those of clusters and filaments because they have less dense surroundings .",
        "rewrite_text": "We summarize the findings from our cosmological hydrodynamic simulations examining the evolution of dark matter haloes across various cosmic environments, including complexes, filaments, sheets, and voids. Our results indicate that: (i) the mass accretion histories of clusters are primarily influenced by significant mergers with other massive bodies at high redshifts (z > 1); (ii) in contrast to groups, most of the formation of filamentary structures results from continuous gas accretion over time, contributing to a prolonged lifespan for these structures, with origins traceable to early epochs (z < 5); (iii) sheet-like structures arise from the merger of smaller filaments into larger ones, developing mainly through smooth gas accretion while also experiencing minor mergers with smaller groups or galaxies throughout their existence; and (iv) voids primarily evolve through gentle gas accretion, with their developmental timescales typically exceeding those of clusters and filaments due to their less dense environments.",
        "ori-fast-z-score": -0.23904572186687872,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 2.0768805540571886
    },
    {
        "original_text": "We present an extension to the cable model that allows one to account for membrane dynamics in neurons with active conductances over a wide range of time scales and spatial dimensions, including those relevant to subthreshold electrogenesis. The proposed approach is based on a reformulation of the cable equation as a system of first-order differential equations describing voltage changes along the axon or dendrite. This formulation leads naturally to a generalization of the concept of space constants to include both passive and active components of the membrane impedance. We show how this generalized cable formalism can be used to describe propagation of action potentials through a single compartment neuron with Hodgkin-Huxley type currents. Finally we demonstrate its utility by applying it to study spatiotemporal patterns of activity in networks of coupled neurons. Neurons are highly specialized cells capable of generating electrical signals known as action potentials (APs). These APs propagate down the length of the cell s axon toward synaptic terminals where they trigger release of neurotransmitters into the synapse. In turn these transmitters bind to receptors located on the postsynaptic side of the synapse initiating signaling cascades which ultimately lead to generation of new APs. Thus information transfer between neurons occurs via propagating APs across chemical synapses.",
        "watermark_text": "We introduce an addition to the cable theory that enables one to explain for membrane dynamics in cells with active conductances over a broad variety of time scales and spatial dimensions , particularly those applicable to subthreshold electrogenesis . The proposed approach is based on a reformulation of the cable equation as a system of first - order differential coefficients governing voltage changes along the axon or dendrite .This formulation leads naturally to a generalization of the notion of space constants to consider both passive and active components of the membrane impedance . We see how this generalized cable formalism can be used to explain propagation of action potentials through a single chamber neuron with Hodgkin - Huxley type flows .Finally we prove its utility by using it to study spatiotemporal changes of action in networks of coupled neurons . Neurons are extremely specialized molecules capable of transmitting electrical messages termed as action potentials ( APs ) .These APs propagate down the length of the cell s axon toward synaptic terminals where they stimulate release of neurotransmitters into the synapse . In turn these transmitters attach to receptors located on the postsynaptic side of the synapse initiating activation cascades which ultimately cause to development of new APs .Thus information transfer between neurons occurs via propagating APs across molecular synapses .",
        "rewrite_text": "We present an enhancement to cable theory that allows for the explanation of membrane dynamics in cells with active conductances over a wide range of time scales and spatial dimensions, particularly in relation to subthreshold electrogenesis. This new approach reformulates the cable equation into a system of first-order differential equations that describe voltage changes along the axon or dendrite. This reformulation naturally extends the concept of space constants to incorporate both passive and active components of membrane impedance. We demonstrate how this generalized cable framework can account for the propagation of action potentials in single-chamber neurons exhibiting Hodgkin-Huxley type ionic flows. Additionally, we illustrate its effectiveness by applying it to investigate spatiotemporal dynamics of action in networks of interconnected neurons. Neurons are highly specialized cells capable of transmitting electrical signals known as action potentials (APs). These APs travel down the axon toward the synaptic terminals, where they trigger the release of neurotransmitters into the synapse. These neurotransmitters, in turn, bind to receptors on the postsynaptic side of the synapse, initiating activation cascades that ultimately lead to the generation of new action potentials. Consequently, information is conveyed between neurons through the propagation of action potentials across molecular synapses.",
        "ori-fast-z-score": -1.6865480854231356,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": -0.21320071635561041
    },
    {
        "original_text": "We consider the possibility that dark matter is made up of bosonic particles, which can condense into a superfluid state at low temperatures. We show how this scenario could explain several puzzling observations in astrophysics and cosmology. In particular we argue that: (i) The observed flat rotation curves of spiral galaxies are explained by the presence of a halo of cold dark matter surrounding each galaxy. (ii) The formation of large-scale structures such as clusters of galaxies proceeds through gravitational collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation. (iii) Dark energy may arise naturally if the universe contains a large number of weakly interacting massive particles with masses around $10^{22}$ GeV. This article is part of a series on Quantum Matter. For more information see http://arxiv.org/abs/quant-ph/0604070 . \nIntroduction:  Many theories beyond the Standard Model predict new types of elementary particles whose existence has yet to be confirmed experimentally. One particularly interesting class of models involves so-called WIMPZILLAs  1  , i.e., stable relic particles with masses around $10^9$ GeV or higher  2  . These particles would have been produced thermally in the early Universe but their abundance today should still be determined by their annihilation cross section  3  .\nIn this Letter we propose an alternative explanation for the origin of dark matter based on the idea that it consists of self-gravitating bosons  4  . Boson stars  5  are gravitationally bound states of scalar fields  6  predicted by many extensions of the Standard Model  7, 8  . They were first studied in the context of supersymmetric grand unified theories  9  where they play the role of solitonic solutions  10  . More recently, boson stars have also been considered within the framework of string theory  11  . If these objects exist then they will form a population of compact remnants  12  that might constitute all or some fraction of the dark matter  13  .",
        "watermark_text": "We consider the prospect that dark matter is made up of bosonic particles , which can condense into a superfluid state at low temperatures . We see how this situation could explain several puzzling discoveries in astrophysics and cosmology .In particular we claim that : ( i ) The observed flat rotation curves of spiral nuclei are explained by the presence of a halo of cold dark matter surrounding each galaxy . ( ii ) The formation of large - scale structures such as clusters of stars happens through gravity collapse of overdensities in the primordial density field seeded by quantum fluctuations during inflation .( iii ) Dark energy may arise naturally if the universe consists a large number of mildly interacting massive particles with masses around $ 10 ^ { 22 } $ GeV . This page is part of a trilogy on Quantum Matter .For more information see www : / / arxiv . org / abs / quant - ph / 0604070 . Introduction : Many theories beyond the Standard Model predict new types of primary objects whose existence has yet to be verified experimentally .One especially interesting class of models involves so - called WIMPZILLAs 1 , i . e . , stable relic objects with masses around $ 10 ^ 9 $ GeV or greater 2 . These particles might have been created thermally in the early Universe but their density today should still be determined by their annihilation cross section 3 .In this Letter we propose an additional argument for the origin of dark matter based on the idea that it consists of self - gravitating bosons 4 . Boson galaxies 5 are gravitationally bound states of scalar fields 6 expected by many extensions of the Standard Model 7 , 8 .They were first investigated in the context of supersymmetric grand unified fields 9 where they hold the part of solitonic answers 10 . More recently , boson stars have also been discussed within the framework of string theory 11 .If these objects exist then they will form a population of compact remnants 12 that might constitute all or some fraction of the dark matter 13 .",
        "rewrite_text": "We explore the possibility that dark matter consists of bosonic particles capable of condensing into a superfluid state at low temperatures. This framework may help clarify several intriguing findings in the fields of astrophysics and cosmology. Specifically, we propose that: (i) the flat rotation curves observed in spiral galaxies can be accounted for by a halo of cold dark matter surrounding each galaxy, (ii) the emergence of large-scale structures, such as star clusters, results from the gravitational collapse of overdensities in the primordial density field, which were initiated by quantum fluctuations during inflation, and (iii) dark energy could naturally arise if the universe contains a substantial number of mildly interacting massive particles with masses around \\(10^{22}\\) GeV. This content is part of a trilogy on Quantum Matter. For further details, please visit www:/arxiv.org/abs/quant-ph/0604070. \n\nIntroduction: Numerous theories that extend beyond the Standard Model suggest the existence of new types of fundamental objects whose existence has yet to be experimentally confirmed. Among these, an intriguing category includes the so-called WIMPZILLAs, stable relics with masses around \\(10^{9}\\) GeV or greater. These particles may have originated thermally in the early universe, yet their present density is expected to be governed by their annihilation cross-section. In this letter, we introduce a novel argument for the nature of dark matter, positing that it comprises self-gravitating bosons. Boson galaxies represent gravitationally bound states of scalar fields, a concept anticipated by numerous extensions of the Standard Model. Initially examined within the context of supersymmetric grand unified theories, they provide solitonic solutions. More recently, boson stars have been analyzed in relation to string theory. If such entities exist, they may form a collection of compact remnants that could account for all or part of dark matter.",
        "ori-fast-z-score": 1.6783627165933783,
        "water-fast-z-score": 7.495152097492019,
        "rewrite-fast-z-score": 2.197401062294143
    },
    {
        "original_text": "We study the correlations and sum rules in a semi-infinite system with impurities at its surface, which is described by the quantum two-dimensional (2D) one component plasma model. We use the exact diagonalization method to calculate the density-density correlation function and static structure factor as well as their corresponding sum rules. The results show that there are two different regimes depending on whether the temperature T is larger or smaller than the Fermi energy EF . In particular, we find that when T < EF , the behavior of these quantities can be understood within the framework of Landau s Fermi liquid theory. However, if T > EF , our numerical data deviate significantly from this picture. Finally, we also investigate how the presence of impurities affects the above mentioned physical properties. Our findings suggest that the effect of impurities depends strongly on the distance between them. If they are close enough, then the impurity-impurity interaction dominates over other interactions leading to an increase of the effective mass of particles near the surface.",
        "watermark_text": "We consider the correlations and sum rules in a semi - infinite system with impurities at its surface , which is characterized by the quantum two - dimensional ( 2D ) one element plasma model . We use the exact diagonalization technique to estimate the density - density correlation function and static structure parameter as well as their corresponding sum rules .The results show that there are two different regimes depending on whether the temperature T is bigger or smaller than the Fermi energy EF . In particular , we find that when T < EF , the dynamics of these quantities can be understood within the framework of Landau s Fermi liquid physics .However , if T > EF , our numerical statistics deviate substantially from this picture . Finally , we also investigate how the presence of impurities impacts the above mentioned physical properties .Our findings show that the impact of impurities relies highly on the distance between them . If they are close enough , then the impurity - impurity interaction dominates over other molecules giving to an increase of the effective mass of molecules near the surface .",
        "rewrite_text": "We examine the correlations and sum rules in a semi-infinite system with surface impurities, modeled by the quantum two-dimensional (2D) one-component plasma. Utilizing exact diagonalization techniques, we estimate the density-density correlation function, static structure parameter, and their associated sum rules. Our results reveal two distinct regimes based on the temperature relative to the Fermi energy (EF). Specifically, when the temperature (T) is less than the Fermi energy, the behavior of these quantities aligns with Landau's Fermi liquid theory. Conversely, when T exceeds EF, our numerical findings diverge significantly from this framework. Additionally, we explore the influence of impurities on these physical properties, discovering that their effect is strongly dependent on the distance between them. When impurities are in close proximity, impurity-impurity interactions dominate over other molecular influences, leading to an increase in the effective mass of molecules near the surface.",
        "ori-fast-z-score": 2.5927248643506746,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 3.0542361089076304
    },
    {
        "original_text": "We present the results of an observation performed by XMM-Newton on the galaxy cluster Abell S0740 (SA57). The data were taken between December 2004 and January 2005 for a total exposure time of about 100 ks, split into two pointings separated by 1 arcmin. We detect more than 50 sources within the field-of-view of our observations. Most of them are associated to galaxies at different redshifts; we also find several active galactic nuclei (AGN) as well as one background quasar. In order to study their properties, we have extracted spectra for all detected sources using circular regions centered on each source position. For most of these objects, we could fit single power-law models or thermal plasma emission models. From this analysis, we derive luminosities ranging from 1042 erg s-1 up to 1044 erg s-1 . Using the observed fluxes and assuming that they follow a standard candle model, we estimate the number density distribution of clusters per unit volume as a function of redshift. This allows us to calculate the expected number of clusters above a given mass limit as a function of redshift and compare it with the predictions obtained from numerical simulations.",
        "watermark_text": "We present the conclusion of an observation performed by XMM - Newton on the galaxy cluster Abell S0740 ( SA57 ) . The data were took between December 2004 and January 2005 for a total sensitivity length of about 100 ks , separated into two pointings separated by 1 arcmin .We detect more than 50 sources within the field - of - view of our observations . Most of them are related to galaxies at different redshifts ; we also find several active galactic nuclei ( AGN ) as well as one background quasar .In order to study their characteristics , we have gathered spectra for all detected sources using circular regions centered on each source place . For most of these objects , we may fit single power - law models or thermal plasma radiation estimates .From this analysis , we derive luminosities ranging from 1042 erg s - 1 up to 1044 erg s - 1 . Using the seen fluxes and assuming that they follow a traditional candle model , we estimate the number density spread of clusters per unit volume as a function of redshift .This enables us to estimate the expected number of clusters above a given mass limit as a function of redshift and compare it with the estimates obtained from numerical simulations .",
        "rewrite_text": "We present the conclusions of an observation conducted by XMM-Newton on the galaxy cluster Abell S0740 (SA57). The data were collected between December 2004 and January 2005, totaling approximately 100 kiloseconds, and were divided into two separate pointings that were 1 arcminute apart. We identified over 50 sources within the field of view of our observations. The majority of these sources are associated with galaxies at varying redshifts, along with several active galactic nuclei (AGN) and one background quasar. To analyze their properties, we obtained spectra for all detected sources by using circular regions centered on each source's location. For most of these objects, we were able to fit single power-law models or estimates of thermal plasma radiation. From this analysis, we calculated luminosities ranging from \\(10^{42}\\) erg/s to \\(10^{44}\\) erg/s. By utilizing the observed fluxes and assuming a standard candle model, we estimated the number density of clusters per unit volume as a function of redshift. This allows us to predict the expected number of clusters above a certain mass limit as a function of redshift, enabling a comparison with estimates derived from numerical simulations.",
        "ori-fast-z-score": 0.6546536707079772,
        "water-fast-z-score": 4.858987147293248,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "We study instabilities that develop in the accretion flow onto black holes during gamma-ray bursts (GRBs). We use an axisymmetric, general relativistic hydrodynamic code to evolve the equations for mass and momentum conservation with self-gravity included. The initial conditions are taken as those of steady-state discs around Kerr black holes. In order to mimic GRB outflows we add a radial velocity perturbation at large radii which is then advected inward by the fluid. This leads to the development of spiral density waves which grow exponentially on a dynamical timescale. These waves can be identified with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999) . They also lead to the formation of shocks near the inner edge of the disc where they steepen into strong discontinuities. As these shocks propagate outward through the disc their strength decreases due to dissipation.",
        "watermark_text": "We research instabilities that develop in the accretion flow onto black holes during gamma - ray flare ( GRBs ) . We use an axisymmetric , general relativistic hydrodynamic program to evolve the expressions for charge and momentum conservation with self - gravity included .The initial conditions are took as those of stable - state discs around Kerr black holes . In order to mimic GRB outflows we create a radial speed perturbation at large radii which is then advected inward by the liquid .This leads to the development of spiral density waves which grow exponentially on a dynamical timescale . These waves can be identified with the Rossby wave disturbance ( RWI ) anticipated analytically by Lovelace et al .( 1999 ) . They also lead to the formation of shocks near the inner boundary of the disc where they steepen into deep discontinuities .As these shocks propagate outward through the disc their intensity reduces owing to dissipation .",
        "rewrite_text": "We investigate the instabilities that arise in the accretion flow onto black holes during gamma-ray bursts (GRBs). Utilizing an axisymmetric, general relativistic hydrodynamic code, we evolve the conservation equations for charge and momentum while incorporating self-gravity. The initial conditions are based on stable-state discs surrounding Kerr black holes. To simulate GRB outflows, we introduce a radial velocity perturbation at large distances, which is subsequently advected inward by the fluid. This process initiates the formation of spiral density waves that grow exponentially on a dynamical timescale. These waves correspond to the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999). Additionally, they result in the creation of shocks near the inner edge of the disc, where they steepen into significant discontinuities. As these shocks travel outward through the disc, their intensity diminishes due to dissipation.",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": 0.7878385971583353
    },
    {
        "original_text": "We present new XMM-Newton observations of the Wolf Rayet binary system WR147, which is one of only two known systems with an O-type companion star and a WN6-7h primary component (the other being WR20a). The data were obtained in December 2004 during revolution number 1253-1255 using all three EPIC cameras on board XMM-Newton. We have analyzed these data to search for periodicities in both light curves as well as line profiles. No significant periodicity was found in either the light curve or line profile analysis. However we find that there are differences between the line profiles observed by Chandra and those seen here. These differences may be due to orbital motion within this highly eccentric system. This work has been supported by NASA grant NAG5-10842. Keywords: Wolf-Rayet stars; binaries; X-rays; XMM-Newton",
        "watermark_text": "We report new XMM - Newton discoveries of the Wolf Rayet binary system WR147 , which is one of only two discovered systems with an O - class companion star and a WN6 - 7h primary component ( the other being WR20a ) . The data were obtained in December 2004 during revolution number 1253 - 1255 utilizing all three EPIC photographers on board XMM - Newton .We have analyzed these information to search for periodicities in both light curves as well as line profiles . No considerable periodicity was found in either the light curve or line profile analysis .However we find that there are variations between the line profiles observed by Chandra and those viewed here . These changes may be due to orbital movement within this strongly eccentric system .This project has been supported by NASA grant NAG5 - 10842 . Keywords : Wolf - Rayet stars ; binaries ; X - radiation ; XMM - Newton",
        "rewrite_text": "We present new discoveries from the XMM-Newton observations of the Wolf Rayet binary system WR147, which, along with WR20a, is one of only two known systems featuring an O-class companion star and a WN6-7h primary component. The data, collected in December 2004 during revolutions 1253 to 1255, leveraged all three EPIC cameras aboard XMM-Newton. Our analysis sought to identify periodicities in both the light curves and line profiles; however, no significant periodicities were detected in either dataset. Nonetheless, we observed variations in the line profiles recorded by Chandra compared to those from our observations, which may be attributable to orbital motion within this highly eccentric system. This research was supported by NASA grant NAG5-10842. Keywords: Wolf-Rayet stars; binaries; X-radiation; XMM-Newton.",
        "ori-fast-z-score": -0.13736056394868904,
        "water-fast-z-score": 4.354648431614539,
        "rewrite-fast-z-score": 0.4375949744936837
    },
    {
        "original_text": "We present the results of an optical survey for high redshift galaxies in the field surrounding the radio galaxy PKS 1138-262 (z = 3.9). We have detected over 100 candidate Lyman-alpha emitting galaxies with redshifts between 2 and 5, including several new spectroscopically confirmed members of this cluster. The spatial distribution of these objects is consistent with that expected if they are located within a single dark matter halo centered on the radio source. This result suggests that clusters may be identified by their diffuse emission as well as individual member galaxies. In addition to confirming the existence of a massive cluster around PKS 1138-262 we find evidence for two other overdensities of Lyman-alpha emitting sources near the line-of-sight to the radio source. These structures could represent additional clusters or proto-clusters which will evolve into richer systems like those found today. Finally, our data suggest that there exists a large population of faint Lyman-alpha emitting objects whose properties are similar to those observed locally but whose number density increases rapidly towards higher redshifts.",
        "watermark_text": "We present the conclusion of an optical survey for high redshift galaxies in the field surrounding the radio star PKS 1138 - 262 ( z = 3 . 9 ) . We have discovered over 100 candidate Lyman - alpha emitting galaxies with redshifts between 2 and 5 , including ten new spectroscopically confirmed members of this cluster .The geographic distribution of these objects is consistent with that expected if they are situated within a single black material halo focused on the radio source . This result suggests that clusters might be identified by their diffuse emission as well as additional member galaxies .In addition to proving the existence of a huge cluster around PKS 1138 - 262 we find proof for two other overdensities of Lyman - alpha emitting sources near the line - of - view to the radio source . These structures could constitute extra clusters or proto - nuclei which will evolve into richer complexes like those observed nowadays .Finally , our statistics indicate that there exists a large colony of distant Lyman - alpha emitting objects whose characteristics are comparable to those observed locally but whose number density increases quickly towards higher redshifts .",
        "rewrite_text": "We present the results of an optical survey targeting high redshift galaxies in the vicinity of the radio source PKS 1138-262 (z = 3.9). Our search has yielded over 100 candidate Lyman-alpha emitting galaxies with redshifts ranging from 2 to 5, including ten newly confirmed spectroscopic members of this cluster. The spatial distribution of these galaxies aligns with expectations for placement within a single black material halo centered around the radio source. This finding implies that clusters could potentially be identified not only by their diffuse emissions but also through additional member galaxies. In addition to confirming the presence of a substantial cluster surrounding PKS 1138-262, we have identified two other regions with an overdensity of Lyman-alpha emitting sources along the line of sight to the radio source. These regions may represent additional clusters or proto-nuclei that could evolve into more complex structures like those observed today. Lastly, our data suggest a significant population of distant Lyman-alpha emitting objects, sharing characteristics with their local counterparts, but with a rapid increase in number density at higher redshifts.",
        "ori-fast-z-score": -0.9878783399072131,
        "water-fast-z-score": 5.019011475427825,
        "rewrite-fast-z-score": -0.5360562674188973
    },
    {
        "original_text": "We study the phase behavior of a system of N identical hard rods confined to a square box with periodic boundary conditions, using Monte Carlo simulations at constant pressure P . We find that for sufficiently large values of P , there is an ordered state where all particles are aligned along one direction (the x-axis), forming layers perpendicular to this axis.  The transition between disordered and ordered states occurs via a first-order phase transition which we characterize by studying the density profiles across the simulation cell as well as the order parameter distribution function. For small values of P , however, no such ordered state exists. Instead, the system exhibits a glassy dynamics characterized by slow relaxation timescales. Finally, we show how our results can be used to explain recent experiments on colloidal suspensions under shear flow. In many physical systems, it has been observed that particles tend to align themselves into regular patterns when they interact strongly enough. This phenomenon is known as capillarity  1  or self-assembly  2  .\nIn particular, in two dimensions, particles may form stripes  3  -  5  , squares  6  , hexagons  7  , or even more complex structures  8  depending on their shape  9  , size  10  , interactions  11  , and external fields  12  . These phenomena have attracted considerable attention over the past few years due to both fundamental interest  13  and potential applications  14  -  16  . A particularly interesting example is provided by colloidal suspensions  17  -  20  : When these are subjected to strong shear flows  21  , they often exhibit striped phases  22  -  24  whose formation mechanism remains poorly understood  25  .",
        "watermark_text": "We explore the phase response of a system of N identical hard rods confined to a square box with periodic border conditions , using Monte Carlo simulations at steady pressure P . We see that for enough large values of P , there is an ordered state where all atoms are aligned along one orientation ( the x - axis ) , forming sheets parallel to this axis .The shift between disordered and ordered states happens via a first - order phase shift which we characterize by examining the density patterns across the model cell as well as the order parameter distribution relation . For small values of P , however , no such ordered state exists .Instead , the system displays a glassy dynamics defined by small relaxation timescales . Finally , we tell how our findings can be used to explain latest studies on colloidal suspensions under shear flow .In many physical structures , it has been observed that particles tend to align themselves into normal patterns when they interact strongly sufficiently . This phenomenon is known as capillarity 1 or self - assembly 2 .In particular , in two dimensions , particles may form colors 3 - 5 , circles 6 , hexagons 7 , or especially more complex shapes 8 depending on their shape 9 , size 10 , interactions 11 , and external fields 12 . These phenomena have garnered considerable focus over the previous few years owing to both basic attention 13 and possible use 14 - 16 .A notably important example is provided by colloidal suspensions 17 - 20 : When these are subjected to powerful shear flows 21 , they frequently exhibit striped cycles 22 - 24 whose formation system stays little explained 25 .",
        "rewrite_text": "We investigate the phase response of a system consisting of N identical hard rods confined within a square box under periodic boundary conditions, utilizing Monte Carlo simulations at a constant pressure P. Our results indicate that at sufficiently high values of P, the system transitions to an ordered state where all rods align along a single orientation (the x-axis), creating sheets parallel to this axis. The transition from disordered to ordered states occurs through a first-order phase shift, which we analyze by evaluating the density patterns within the model cell and the distribution of the order parameter. Conversely, at low pressures, no ordered state is observed; instead, the system exhibits glassy dynamics characterized by short relaxation timescales. We also discuss how our findings can provide insights into recent research on colloidal suspensions under shear flow. It has been noted in various physical systems that particles tend to align into regular patterns when they experience sufficiently strong interactions. This phenomenon is referred to as capillarity or self-assembly. In particular, in two dimensions, particles can arrange into colors, circles, hexagons, or more intricate shapes, influenced by their shape, size, interactions, and external fields. These phenomena have received significant attention in recent years due to their fundamental importance and potential applications. A particularly notable example is seen in colloidal suspensions: when subjected to strong shear flows, they often display striped patterns, the mechanisms of which remain poorly understood.",
        "ori-fast-z-score": -1.863448669773839,
        "water-fast-z-score": 6.894291116568838,
        "rewrite-fast-z-score": 0.8838834764831843
    },
    {
        "original_text": "We calculate the two-loop beta function for the coupling constant of the AdS5xS5 superstring theory and show that it is proportional to the one-loop result, which implies that there are no non-trivial fixed points at any finite value of the string coupling constant.  We also find that the dilaton field has an imaginary part when we take into account the higher-order terms beyond the leading order approximation. This indicates that our results may be valid only within some limited region of the parameter space where the imaginary part of the dilaton can be neglected. The present work was motivated by the recent study on the gauge/gravity correspondence between N=4 super Yang-Mills (SYM) theories with 16 supercharges and type IIB strings on AdS5xS5G5 backgrounds  1  . In this context, the existence of nontrivial fixed points would correspond to the conformal invariance of the dual SYM theories  2  , while the imaginary part of the dilatonic scalar fields would indicate the instability of the corresponding solutions  3  .\nIn Ref.  4  , the authors have calculated the one-loop beta functions for both the metric tensor and the dilaton field using the Green-Schwarz formalism  5  . They found that these beta functions do not vanish even if they are evaluated at vanishing values of the string coupling constants. However, their calculations were performed under the assumption that all the fermionic contributions vanish identically  6  . It turns out that such an assumption does not hold true  7, 8  . Therefore, it seems necessary to perform more detailed analysis taking into account the effects due to the fermions as well as those coming from the bosons.",
        "watermark_text": "We calculate the two - loop beta function for the coupling constant of the AdS5xS5 superstring theory and find that it is proportional to the one - loop result , which implies that there are no non - trivial fixed points at any finite value of the string coupling constant . We additionally find that the dilaton field has an imaginary part when we took into consideration the higher - order terms beyond the led order approximation .This implies that our findings may be valid only within some restricted region of the parameter space where the imaginary part of the dilaton can be forgotten . The present work was motivated by the recent study on the gauge / gravity correspondence between N = 4 super Yang - Mills ( SYM ) theories with 16 supercharges and class IIB sequences on AdS5xS5G5 backgrounds 1 .In this situation , the existence of nontrivial fixed points would coincide to the conformal invariance of the dual SYM theories 2 , while the imaginary part of the dilatonic scalar fields might suggest the instability of the equivalent solutions 3 . In Ref .4 , the papers have predicted the one - loop beta functions for both the metric tensor and the dilaton field using the Green - Schwarz formalism 5 . They found that these beta distributions do not vanish even if they are evaluated at vanishing values of the string coupling constants .However , their analyses were performed under the assumption that all the fermionic contributions vanish identically 6 . It turns out that such an assume does not stand true 7 , 8 .Therefore , it appears necessary to conduct more precise analysis taking into consideration the effects due to the fermions as also as those coming from the bosons .",
        "rewrite_text": "We compute the two-loop beta function for the coupling constant in the AdS5 x S5 superstring theory and discover that it is proportional to the one-loop result. This indicates the absence of non-trivial fixed points at any finite value of the string coupling constant. Furthermore, our analysis reveals that the dilaton field acquires an imaginary component when we consider higher-order terms beyond the leading-order approximation. This suggests that our results might only be applicable within a limited region of the parameter space, where the imaginary part of the dilaton can be neglected. Our research was inspired by a recent investigation into the gauge/gravity correspondence between N = 4 super Yang-Mills (SYM) theories with 16 supercharges and Class IIB strings in AdS5 x S5 backgrounds. In this context, the presence of non-trivial fixed points would align with the conformal invariance of the corresponding SYM theories, while the imaginary part of the dilaton scalar field could indicate instability in the associated solutions. In a previous study, predictions were made regarding the one-loop beta functions for both the metric tensor and the dilaton field, utilizing the Green-Schwarz formalism. The results showed that these beta functions do not vanish, even at zero values of the string coupling constants. However, the analysis was based on the assumption that all fermionic contributions are identically zero, which is not the case. Therefore, it is essential to perform a more thorough analysis that takes into account both fermionic and bosonic effects.",
        "ori-fast-z-score": -0.2847473987257497,
        "water-fast-z-score": 5.362108949786505,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "We present the results of our analysis on quiet sun magnetograms obtained by Hinode/SOT/SP and SDO/HMI instruments, which are compared with those simulated using an MHD model. The observed data were processed to remove instrumental effects such as stray light contamination and spatial smearing due to finite pixel size. We found that the simulated magnetograms reproduce well the main features in the observed ones; however there is still room for improvement especially regarding the fine structures seen in the observed magnetograms. \n \n Keywords: Solar activity, Magnetohydrodynamics (MHD), Quiet Sun, Space weathering, Solar atmosphere, Magnetic field strength, Photospheric plasma flows, Solar surface convection, Solar dynamo, Solar irradiance variability, Solar cycle, Solar corona, Solar flares, Solar prominences, Solar winds, Solar energetics, Solar physics, Solar interior, Solar rotation",
        "watermark_text": "We present the conclusion of our analysis on silent solar magnetograms acquired by Hinode / SOT / SP and SDO / HMI instruments , which are compared with those modeled using an MHD model . The observed measurements were processed to remove instrumental impacts such as stray light degradation and spatial smearing due to finite pixel size .We determined that the simulated magnetograms reproduce well the main features in the seen ones ; however there is already room for improvement especially regarding the fine structures visible in the observed magnetograms . Keywords : Solar activity , Magnetohydrodynamics ( MHD ) , Quiet Sun , Space weathering , Solar atmosphere , Magnetic field intensity , Photospheric plasma flows , Solar surface convection , Solar dynamo , Solar irradiance variability , Solar cycle , Solar corona , Solar flares , Solar prominences , Solar winds , Solar energetics , Solar chemistry , Solar interior , Solar rotation",
        "rewrite_text": "We conclude our analysis of silent solar magnetograms obtained from the Hinode/SOT/SP and SDO/HMI instruments, comparing them to models generated by a magnetohydrodynamics (MHD) framework. To ensure accuracy, we processed the observed data to eliminate instrumental effects, such as stray light degradation and spatial smearing caused by finite pixel sizes. Our findings indicate that the simulated magnetograms successfully capture the main features of the observed data; however, there is still potential for enhancement, particularly concerning the finer structures present in the observed magnetograms. \n\nKeywords: Solar activity, Magnetohydrodynamics (MHD), Quiet Sun, Space weathering, Solar atmosphere, Magnetic field intensity, Photospheric plasma flows, Solar surface convection, Solar dynamo, Solar irradiance variability, Solar cycle, Solar corona, Solar flares, Solar prominences, Solar winds, Solar energetics, Solar chemistry, Solar interior, Solar rotation.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 2.888741522913896,
        "rewrite-fast-z-score": -0.9113223768657671
    },
    {
        "original_text": "We have studied by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold (100) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom changes between 1 to 4. We find that, for low coverages, the system behaves as if it were made up of isolated particles with no interaction among them. However, at higher coverages we observe collective effects which lead to the formation of ordered structures. The results obtained show that these structures can be classified into two different categories depending on whether they are formed by one or more layers of silver atoms. In particular, we found that the structure of the first layer depends strongly on the substrate temperature while the second layer shows only small variations. Finally, our calculations indicate that the third layer forms a disordered arrangement of silver atoms. This work was supported by DGESIC under project PB98-0443-C02-01",
        "watermark_text": "We have researched by means of molecular dynamics simulations how the dynamic behavior of silver atoms deposited on gold ( 100 ) is affected when the substrate temperature and the coverage are varied in such a way that the average number of nearest neighbors per atom shifts between 1 to 4 . We see that , for low coverages , the system behaves as if it were made up of isolated molecules with no activity among them .However , at higher coverages we study collective effects which lead to the formation of ordered systems . The results collected show that these structures can be categorized into two different categories depending on whether they are created by one or more bands of silver atoms .In particular , we reported that the composition of the first layer relies highly on the substrate temperature while the second layer exhibits only tiny variations . Finally , our calculations suggest that the third layer makes a disordered arrangement of silver atoms .This project was supported by DGESIC under contract PB98 - 0443 - C02 - 01",
        "rewrite_text": "We have conducted molecular dynamics simulations to investigate how the dynamic behavior of silver atoms deposited on a gold (100) substrate is influenced by variations in substrate temperature and coverage, leading to an average number of nearest neighbors per atom ranging from 1 to 4. Our findings indicate that at low coverages, the system acts as if it were composed of isolated molecules with minimal interaction. In contrast, at higher coverages, we observe collective phenomena that result in the formation of ordered structures. The results reveal that these structures can be classified into two categories based on whether they originate from single or multiple bands of silver atoms. Notably, we found that the composition of the first layer is highly dependent on the substrate temperature, while the second layer shows only slight variations. Finally, our calculations indicate that the third layer displays a disordered arrangement of silver atoms. This project was funded by DGESIC under contract PB98-0443-C02-01.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 1.2375966910186262
    },
    {
        "original_text": "We present the results of our investigation into accretion disk continuum emission in black hole candidates (BHCs). We have developed an analytical model for calculating the spectrum emitted by a thin, optically thick accretion disk around a Schwarzschild black hole and applied it to several BHCs with known mass functions. The observed spectra are well reproduced when we assume that the inner edge of the disk is located at 6 gravitational radii. This result suggests that the standard thin disk model can be used as a good approximation for modeling the X-ray continuum emission of these objects. \n \n Keywords: Black holes -- Spectroscopy -- X-rays -- Modeling -- Accretion disks -- Emission lines -- Broad-band spectral energy distribution -- Luminosity function -- Mass measurement -- Stellar-mass black holes -- Supermassive black holes -- Active galactic nuclei -- Quasars -- Cosmic evolution \n \n \n \n 1 Introduction \n \n In recent years there has been considerable progress made towards understanding the physical processes occurring near supermassive black holes (SMBH) in active galactic nuclei (AGN), quasars, and other similar systems. These studies rely on observations of the broad-band spectral energy distributions (SEDs) of SMBHs over many decades in frequency space. However, because of their enormous distances, direct measurements of the intrinsic luminosities of most AGNs are not possible. Instead, one must use indirect methods such as reverberation mapping or statistical correlations between various properties of AGNs to determine their luminosities. For example, if one knows how much light passes through some region of interest within an AGN then one may calculate its luminosity using simple geometric arguments. Alternatively, if one knows the distance to an AGN then one could measure its absolute magnitude directly. Unfortunately, both of these approaches require detailed knowledge about the structure of the emitting regions which cannot currently be obtained observationally. Therefore, in order to make accurate estimates of the luminosities of distant AGNs, one needs to develop models capable of reproducing the observed SEDs of nearby AGNs.",
        "watermark_text": "We present the conclusion of our inquiry into accretion disk continuum emission in black hole candidates ( BHCs ) . We have developed an analytical theory for determining the spectrum emitted by a thin , optically dense accretion disk around a Schwarzschild red hole and applied it to several BHCs with reported mass distributions .The observed spectra are better illustrated when we suppose that the inner corner of the disk is situated at 6 gravitational radii . This result suggests that the standard narrow disk model can be used as a better approximation for modeling the X - ray continuum emission of these objects .Keywords : Black holes - - Spectroscopy - - X - rays - - Modeling - - Accretion disks - - Emission lines - - Broad - band spectral energy distribution - - Luminosity function - - Mass measurement - - Stellar - mass black holes - - Supermassive black holes - - Active galactic nuclei - - Quasars - - Cosmic evolution 1 Introduction In recent years there has been increased progress conducted towards exploring the physical processes occurring near supermassive black holes ( SMBH ) in active galactic nuclei ( AGN ) , quasars , and other similar systems . These studies rely on observations of the broad - band spectral energy distributions ( SEDs ) of SMBHs over many decades in frequency space .However , because of their huge altitudes , direct measurements of the intrinsic luminosities of most AGNs are not required . Rather , one must use indirect tools such as reverberation projection or statistical correlations between various properties of AGNs to obtain their luminosities .For instance , if one remembers how many light passes through some region of interest within an AGN then one may calculate its luminosity taking simple geometric arguments . Alternatively , if one understands the distance to an AGN then one might estimate its absolute magnitude directly .Unfortunately , both of these perspectives need rigorous knowledge about the composition of the emitting regions which lacks already be obtained observationally . Therefore , in order to make accurate calculations of the luminosities of distant AGNs , one needs to develop models capable of reproducing the known SEDs of neighbouring AGNs .",
        "rewrite_text": "We present the findings of our study on the continuum emission from accretion disks in black hole candidates (BHCs). We have formulated an analytical theory to ascertain the spectrum produced by a thin, optically dense accretion disk surrounding a Schwarzschild black hole and have applied this theory to various BHCs with reported mass distributions. Our results reveal that the observed spectra are more effectively represented when we assume the inner edge of the disk is located at six gravitational radii. This outcome indicates that the standard thin disk model serves as a more appropriate approximation for modeling the X-ray continuum emission from these objects. \n\nKeywords: Black holes - Spectroscopy - X-rays - Modeling - Accretion disks - Emission lines - Broad-band spectral energy distribution - Luminosity function - Mass measurement - Stellar-mass black holes - Supermassive black holes - Active galactic nuclei - Quasars - Cosmic evolution. \n\n1. Introduction: In recent years, significant progress has been made in understanding the physical processes occurring near supermassive black holes (SMBHs) in active galactic nuclei (AGNs), quasars, and similar systems. These investigations depend on analyzing the broad-band spectral energy distributions (SEDs) of SMBHs across a wide range of frequencies. However, due to their immense distances, direct measurements of the intrinsic luminosities of most AGNs are often challenging. Instead, researchers must employ indirect methods, such as reverberation mapping or statistical correlations between different properties of AGNs, to estimate their luminosities. For example, by determining how many light crossings occur in a specific region within an AGN, one can calculate its luminosity using basic geometric principles. Alternatively, understanding the distance to an AGN allows for a direct estimation of its absolute magnitude. Unfortunately, both approaches require a thorough understanding of the properties of the emitting regions, which is not always available through observation. Consequently, to accurately calculate the luminosities of distant AGNs, we need to develop models that can accurately reproduce the known SEDs of nearby AGNs.",
        "ori-fast-z-score": -0.9733285267845753,
        "water-fast-z-score": 5.028864055053639,
        "rewrite-fast-z-score": -0.3244428422615251
    },
    {
        "original_text": "The present work is an attempt to show that the concept of time can be extended into a higher-dimensional space, and that this extension may have important consequences for our understanding of physical phenomena.  The author considers the possibility that there are five dimensions of space (four ordinary spatial dimensions plus one extra temporal dimension) which could explain some of the observed properties of matter such as entropy production and irreversibility.   In particular he shows how the existence of these additional dimensions would lead to a violation of the principle of entropy increase with time, and suggests that this might provide a possible explanation for the arrow of time. This article is available from: http://arxiv.org/abs/astro-ph/0403070v1. Introduction:  Time has always been considered by physicists as being fundamentally different from other quantities like position or velocity because it cannot be measured directly but only inferred indirectly through its effects on other measurable quantities.  However, recent developments in theoretical physics suggest that we should consider whether the concept of time itself needs to be modified so that it becomes more closely related to other fundamental concepts such as mass, charge and energy.  For example, string theory predicts that all particles are vibrating strings moving along a multidimensional space called  space-time   1  .    Another approach involves considering the possibility that time is not just another quantity but rather part of a larger structure known as spacetime  2  , where the latter consists of both space and time together  3  .  According to this viewpoint, time is no longer regarded as something separate from space; instead they are viewed as two aspects of the same thing  4  .\nIn fact, many modern theories of quantum gravity predict that the universe contains at least three large scale dimensions - namely length, width and height  5  - while also containing a fourth small-scale dimension  6  .",
        "watermark_text": "The present work is an attempt to see that the notion of time can be enlarged into a higher - dimensional space , and that this extension may have important implications for our studying of natural behavior . The author considers the prospect that there are five spheres of space ( four ordinary spatial dimensions plus one extra temporal dimension ) which could explain some of the seen characteristics of matter such as entropy production and irreversibility .In particular he shows how the existence of these additional dimensions would result to a violation of the principle of entropy increase with time , and suggests that this might give a possible reason for the arrow of time . This section is accessible from : www : / / arxiv . org / abs / astro - ph / 0403070v1 .Introduction : Time has always been regarded by physicists as being fundamentally changed from other quantities like position or speed because it rarely be understood directly but only inferred indirectly through its consequences on other measurable quantities . However , recent developments in theoretical physics suggest that we should consider whether the notion of time itself needs to be altered so that it becomes more closely related to other fundamental concepts such as mass , charge and energy .For instance , string theory predicts that all atoms are vibrating chords moving along a multidimensional space termed space - time 1 . Another approach requires studying the idea that time is not just another quantity but rather component of a greater formation referred as spacetime 2 , where the former consists of both space and time together 3 .According to this viewpoint , time is no longer regarded as something separate from space ; simply they are regarded as two parts of the same thing 4 . In reality , many contemporary ideas of quantum gravitational suggest that the universe possesses at least three large scale dimensions - including size , length and size 5 - while also containing a third small - scale dimension 6 .",
        "rewrite_text": "The current study aims to explore the possibility of expanding the concept of time into a higher-dimensional framework and to consider the potential implications this could have for our understanding of natural behavior. The author proposes that there may be five dimensions of space—four conventional spatial dimensions along with one additional temporal dimension—that could clarify certain observed properties of matter, such as the processes of entropy production and irreversibility. Specifically, the author demonstrates how the existence of these extra dimensions could challenge the principle of entropy increase over time, suggesting this might offer an explanation for the directionality of time. More details about this work can be found at: www:/ / arxiv.org/abs/astro-ph/0403070v1. \n\nIntroduction: Physicists have traditionally viewed time as fundamentally different from other quantities like position or speed, primarily because it is rarely comprehensible in isolation and is instead inferred through its effects on measurable phenomena. However, recent advancements in theoretical physics indicate that we may need to rethink the very notion of time, positioning it more closely to other fundamental concepts such as mass, charge, and energy. For example, string theory posits that all atoms are composed of vibrating strings that move through a multidimensional construct known as spacetime. Another perspective invites us to view time not merely as a separate notion, but as an integral part of a broader construct called spacetime, where both space and time are considered interconnected elements of a unified entity. In fact, many contemporary theories of quantum gravity suggest that the universe comprises at least three large-scale dimensions—incorporating size, length, and scale—along with an additional smaller-scale dimension.",
        "ori-fast-z-score": -0.7373087284671365,
        "water-fast-z-score": 7.511768544535079,
        "rewrite-fast-z-score": 0.939793423488437
    },
    {
        "original_text": "We present new mid-IR photometry and spectroscopy of the HUDF-JD2 galaxy at redshift 2.081, which is one of the most luminous infrared galaxies known to date. The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum. We find evidence for both star formation activity (from the UV-optical) as well as obscured AGN activity (from X-ray observations). This object may be representative of a population of dusty star-forming galaxies undergoing rapid evolution during this critical epoch when massive black holes are growing rapidly along with their host galaxies. Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "watermark_text": "We present new mid - IR photometry and spectroscopy of the HUDF - JD2 galaxy at redshift 2 . 081 , which is one of the most luminous infrared galaxies known to date . The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum .We get data for both star formation activity ( from the UV - optical ) as well as obscured AGN activity ( from X - ray observations ) . This object may be representative of a population of dusty star - creating stars undergoing fast evolution during this critical epoch when massive brown holes are growing faster along with their host galaxies .Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "rewrite_text": "We present new mid-infrared photometry and spectroscopy of the HUDF-JD2 galaxy at a redshift of 2.081, making it one of the most luminous infrared galaxies identified to date. The spectral energy distribution (SED) reveals a notably red continuum and prominent PAH emission features in its rest-frame optical spectrum. Our data encompasses both star formation activity, assessed through UV-optical observations, and obscured AGN activity, obtained from X-ray data. This galaxy may be indicative of a population of dusty, star-forming systems experiencing rapid evolution during this pivotal era, characterized by the accelerated growth of massive black holes alongside their host galaxies. \n\nKeywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies.",
        "ori-fast-z-score": 0.2886751345948129,
        "water-fast-z-score": 2.3094010767585034,
        "rewrite-fast-z-score": 0.2672612419124244
    },
    {
        "original_text": "In this work, we study the spectral efficiency (SE) performance of spectrum pooling systems in which multiple secondary users share the same licensed band with one primary user. We consider two different scenarios for the SE analysis: 1) The case where all secondary users are located at fixed distances away from each other and 2) The case where they are randomly distributed over an area according to some spatial distribution function. In both cases, we assume that there is no direct communication between any pair of secondary users. For scenario 1), we derive closed-form expressions for the ergodic capacity of the system under Rayleigh fading channels as well as Nakagami-m fading channels. Our results show that when the number of secondary users increases, their mutual interference becomes more severe resulting in lower ergodic capacities. However, if the distance between them decreases or equivalently, the density of secondary users increases, then the ergodic capacity improves due to higher signal-to-noise ratio levels.",
        "watermark_text": "In this research , we study the spectral efficiency ( SE ) performance of bandwidth pooling systems in which several secondary users share the same licensed band with one secondary customer . We consider two different scenarios for the SE assessment : 1 ) The case where all secondary users are situated at fixed distances away from each other and 2 ) The case where they are randomly distributed over an area according to some spatial distribution relation .In both cases , we suppose that there is no continuous communication between any pair of secondary users . For scenario 1 ) , we derive closed - form expressions for the ergodic strength of the device under Rayleigh fading filters as well as Nakagami - m fading transmissions .Our results show that when the quantity of secondary users increases , their mutual interference becomes more serious resulting in reduced ergodic capacities . However , if the distance between them grows or equivalently , the density of secondary users increases , then the ergodic strength improves due to higher sound - to - noise ratio levels .",
        "rewrite_text": "In this study, we investigate the spectral efficiency (SE) of bandwidth pooling systems where multiple secondary users share the same licensed band with a single secondary customer. We examine two scenarios for SE evaluation: 1) a fixed-distance arrangement where all secondary users are placed at consistent distances from one another, and 2) a random distribution in which users are spread across an area according to a specific spatial distribution model. In both scenarios, we assume that there is no continuous communication between any pair of secondary users. For the first scenario, we derive closed-form equations for the ergodic performance of the system under Rayleigh fading and Nakagami-m fading conditions. Our findings indicate that as the number of secondary users increases, the interference they cause to each other intensifies, leading to decreased ergodic capacity. Conversely, if the distance among them increases—or, equivalently, if the density of secondary users decreases—the ergodic performance improves due to enhanced signal-to-noise ratios.",
        "ori-fast-z-score": 1.118033988749895,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "We present an equation of state for atomic systems with large scattering lengths, which is obtained in the framework of the lowest-order constrained variational method (LOCV). The LOCV approach allows one to obtain accurate results for both fermions and bosons at low temperatures. We show that our equation of state agrees well with Monte Carlo simulations performed within the grand canonical ensemble. In particular we find good agreement between theory and experiment on the energy per particle of 4 He-4 He mixtures near the superfluid transition temperature T = Tc. Our results are also compared with those obtained using other theoretical approaches such as the virial expansion or the hypernetted chain approximation. \nI. INTRODUCTORY REMARK\nThe equation of state plays an important role in many areas of physics ranging from nuclear matter  1  , quantum gases  2  , astrophysics  3  , condensed matter  4  , etc.. It describes how various thermodynamic quantities depend on each other under given conditions. For example, it can be used to determine the pressure P , chemical potential µ, entropy S, specific heat Cv, compressibility κT , thermal expansivity αp, sound velocity cs, etc., all of them being functions of density n and/or temperature T . Hereafter we will use the symbol EOS to denote any of these quantities.\nIn this work we consider the case when the scattering length a of two particles becomes very large so that the system behaves like a gas of weakly interacting dimers. This situation occurs e.g. in dilute Bose-Einstein condensates  5  where the scattering length may be tuned via Feshbach resonances  6  .\nII. THEORETICAL APPROACHES\n\nA. Grand Canonical Ensemble\nTo describe the properties of a mixture consisting of Nα atoms of species A and Nβ atoms of species B, we employ the grand-canonical ensemble  7, 8  \nwhere H is the total Hamiltonian of the system, β ≡ 1/kB T denotes inverse temperature, μi is the chemical potential of species i ∈ {A, B}, and Z(Nα,",
        "watermark_text": "We present an equation of state for nuclear systems with large scattering lengths , which is achieved in the framework of the lowest - order constrained variational technique ( LOCV ) . The LOCV method enables one to obtain precise conclusions for both fermions and bosons at low temperatures .We see that our equation of state agrees well with Monte Carlo simulations conducted within the grand canonical ensemble . In particular we find good agreement between theoretical and experiment on the power per particle of 4 He - 4 He mixtures near the superfluid transition volume T = Tc .Our results are also compared with those achieved using other theoretical methods such as the virial expansion or the hypernetted chain method . I .INTRODUCTORY REMARK The equation of state plays an important role in different areas of science diverse from nuclear matter 1 , quantum gases 2 , astrophysics 3 , condensed matter 4 , etc . . It expresses how various thermodynamic quantities rely on each other under given conditions .For instance , it can be used to estimate the pressure P , chemical potential µ , entropy S , basic heat Cv , compressibility κT , thermal expansivity αp , noise speed cs , etc . , all of them being functions of density n and / or temperature T . Hereafter we will use the symbol EOS to indicate any of these quantities .In this research we imagine the case when the scattering height a of two particles gets very huge so that the system behaves like a gas of weakly interacting dimers . This condition occurs e . g .in dilute Bose - Einstein condensates 5 where the scattering duration could be tuned via Feshbach resonances 6 . II .THEORETICAL APPROACHES A . Grand Canonical Ensemble To describe the properties of a mixture consisting of Nα atoms of genus A and Nβ elements of genus B , we utilize the grand - canonical ensemble 7 , 8 where H is the total Hamiltonian of the system , β ≡ 1 / kB T denotes inverse temperature , μi is the chemical potential of species i ∈ { A , B } , and Z ( Nα ,",
        "rewrite_text": "We introduce an equation of state for nuclear systems characterized by large scattering lengths, developed using the lowest-order constrained variational technique (LOCV). The LOCV approach allows for accurate determinations of properties for both fermions and bosons at low temperatures. Our findings demonstrate strong agreement between our equation of state and Monte Carlo simulations performed within the grand canonical ensemble. Notably, we observe good consistency between theoretical predictions and experimental measurements regarding the power per particle in mixtures of helium-4 close to the superfluid transition volume T = Tc. We also compare our results with those obtained through other theoretical frameworks, such as the virial expansion and the hypernetted chain method. \n\nI. INTRODUCTORY REMARK\nThe equation of state is fundamental across various scientific disciplines, including nuclear matter, quantum gases, astrophysics, and condensed matter physics. It defines the relationships among various thermodynamic quantities under specified conditions. For example, it can be utilized to estimate pressure (P), chemical potential (μ), entropy (S), heat capacity (Cv), compressibility (κT), thermal expansivity (αp), and sound speed (cs), all of which depend on density (n) and/or temperature (T). Throughout this paper, we will use the term EOS to refer to any of these quantities. In this study, we consider the scenario where the scattering length (a) between two particles becomes significantly large, causing the system to behave like a gas of weakly interacting dimers. This phenomenon is typically observed in dilute Bose-Einstein condensates, where the scattering length can be adjusted using Feshbach resonances. \n\nII. THEORETICAL APPROACHES\nA. Grand Canonical Ensemble\nTo analyze the properties of a mixture containing Nα atoms of type A and Nβ atoms of type B, we employ the grand canonical ensemble, where H represents the total Hamiltonian of the system, β ≡ 1 / kB T indicates the inverse temperature, μi denotes the chemical potential for species i ∈ {A, B}, and Z (Nα, ...).",
        "ori-fast-z-score": -0.17407765595569785,
        "water-fast-z-score": 6.44087327036082,
        "rewrite-fast-z-score": 1.3719886811400706
    },
    {
        "original_text": "We present an improved semi-analytical method (SAM) that includes gravitational heating by dark matter halos and gas cooling in galaxy formation, which is essential to reproduce observed properties of galaxies such as luminosity functions at different redshifts.  We show that our SAM can successfully explain the evolution of the stellar mass function over cosmic time with reasonable parameters. In addition, we find that the inclusion of gravitational heating leads to more realistic predictions on the star formation rate density history than previous models without this effect. Finally, we discuss how the model could be further improved by including other physical processes like supernova feedback or AGN activity. The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature. Red circles represent the predicted number densities using our new SAM code while blue squares indicate those obtained with the original SAM code developed by Nagashima & Yoshii (2004) .",
        "watermark_text": "We introduce an excellent semi - modelling techniques ( SAM ) that encompasses gravitational heating by black material halos and gas warming in universe formation , which is crucial to reproduce observed properties of stars such as luminosity functions at different redshifts . We suggest that our SAM can effectively predict the evolution of the stellar mass function over cosmic time with satisfactory parameters .In addition , we find that the introduction of gravitational heating results to more realistic predictions on the star formation rate density history than prior models without this effect . Finally , we explain how the model could be further better by including other physical processes like supernova feedback or AGN activity .The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085 . A - 0488 ( A ) . This project was supported by JSPS KAKENHI Grant Number JP15K05481 .Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature .Red rings represent the expected number densities using our new SAM code while blue squares correspond those acquired with the previous SAM code developed by Nagashima & Yoshii ( 2004 ) .",
        "rewrite_text": "We present a novel semi-analytical modeling technique (SAM) that incorporates gravitational heating from black material halos and gas warming during the formation of the universe. This approach is essential for accurately reproducing the observed characteristics of stars, including luminosity functions across various redshifts. Our SAM effectively predicts the evolution of the stellar mass function over cosmic time with meaningful parameters. Moreover, we demonstrate that including gravitational heating leads to more realistic predictions of the star formation rate density history compared to previous models that did not account for this effect. Additionally, we discuss how the model could be enhanced by integrating other physical processes such as supernova feedback and AGN activity. The results presented here are based on observations conducted with ESO Telescopes at Paranal Observatory under program ID 085.A-0488(A). This project was supported by JSPS KAKENHI Grant Number JP15K05481. In Figure 1, we show the predicted galaxy number densities in relation to their total stellar masses, comparing our new SAM predictions (represented by red rings) with data obtained from the previous SAM developed by Nagashima & Yoshii (2004), indicated by blue squares.",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": 1.9188064472004938
    },
    {
        "original_text": "We present new molecular opacity tables that include all relevant molecules in cool, carbon-rich stellar envelopes and are valid over a wide range of temperatures (T = 1000 - 10000 K), densities (ρ = 10 −10 -10 6 g/cm 3 ) and compositions (C/O=0.5-2). The calculations were performed with the state-of-the-art ab initio line-by-line radiative transfer code SPECTRUM using extensive laboratory data on molecular lines as well as theoretical predictions based on quantum chemical methods. We have calculated synthetic spectra for several model atmospheres representative of red giant branch (RGB) and asymptotic giant branch (AGB) stars to demonstrate how our new opacity tables affect their structure and evolution. Our results show that the inclusion of additional species such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO etc., which are not included in previous studies, leads to significant changes in the atmospheric structure and consequently affects the predicted surface abundances of CNO elements during the third dredge-up phase.",
        "watermark_text": "We create novel molecular opacity lists that include all relevant molecules in cold , carbon - rich stellar envelopes and are applicable over a broad variety of temperatures ( T = 1000 - 10000 K ) , densities ( ρ = 10 −10 - 10 6 g / cm 3 ) and compositions ( C / O = 0 . 5 - 2 ) . The calculations were performed with the state - of - the - art ab initio line - by - line radiative transfer code SPECTRUM combining extensive research data on chemical lines as well as conceptual predictions based on quantum chemical techniques .We have adjusted synthetic spectra for various model atmospheres representative of red dwarf branch ( RGB ) and asymptotic giant branch ( AGB ) stars to indicate how our new opacity tables affect their structure and evolution . Our results show that the inclusion of added species such as SiO , TiO , VO , FeH , MgS , NaCl , CaF , AlO , CrH , MnS , CoO , NiO , ZnS , ZrO , BaO , LaO etc . , which are not mentioned in earlier analyses , leads to significant improvements in the atmospheric composition and consequently affects the expected exterior abundances of CNO compounds during the third dredge - up process .",
        "rewrite_text": "We have developed new molecular opacity lists that encompass all relevant molecules found in cold, carbon-rich stellar envelopes, suitable for a wide range of temperatures (T = 1000 - 10,000 K), densities (ρ = 10^-10 - 10^6 g/cm³), and compositions (C/O = 0.5 - 2). These calculations were carried out using the advanced ab initio line-by-line radiative transfer code SPECTRUM, which integrates extensive empirical data on chemical lines alongside theoretical predictions derived from quantum chemical methods. Furthermore, we have refined synthetic spectra for various model atmospheres reflecting red giant branch (RGB) and asymptotic giant branch (AGB) stars to demonstrate how our updated opacity tables influence their structure and evolution. Our findings indicate that the incorporation of additional species—such as SiO, TiO, VO, FeH, MgS, NaCl, CaF, AlO, CrH, MnS, CoO, NiO, ZnS, ZrO, BaO, LaO, and others not considered in previous studies—substantially enhances the atmospheric composition and subsequently impacts the anticipated exterior abundances of CNO elements during the third dredge-up process.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": -0.6882472016116852
    },
    {
        "original_text": "We present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "We introduce the first generation of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently advanced weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation forces in one dimension and multi - dimensions . The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low numerical cost .We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as also as the third - and fifth - order WENO - JS schemes . In addition we also execute the third - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution .Our results show that all these algorithms are able to produce accurate answers when compared against exact or reference solutions .",
        "rewrite_text": "We are pleased to present the first generation of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which draws on the latest advancements in weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in both one and multiple dimensions. The core concept of this algorithm is to employ high-order spatial reconstruction combined with an adaptive mesh refinement technique, allowing for enhanced sensitivity at a low computational cost. Our code incorporates several versions of the WENO algorithm, including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. Additionally, we implement a third-order Runge-Kutta time integration method alongside the Harten-Lax-van Leer scheme for capturing contact discontinuities that arise during hydrodynamic evolution. Our results demonstrate that all these algorithms yield accurate solutions when compared to exact or reference results.",
        "ori-fast-z-score": 2.1652509527331207,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 1.62746694241347
    },
    {
        "original_text": "We present evidence that the strong equivalence principle (SEP) holds in general relativity at least up to second post Newtonian order, by showing that the SEP is satisfied if it holds at first PN order.  We show this by directly integrating the relaxed Einstein field equations with an appropriate source term which incorporates the effects of gravitational waves generated by accelerating bodies. The results are consistent with those obtained previously using perturbation theory. This work was supported by NSF grant PHY-0456747. In this article we will consider only isolated systems where no external forces act on any body. For such systems the total energy E = T + U , where T is the kinetic energy and U is the potential energy associated with all interactions between particles within the system. If there were no gravitational fields then the total energy would be conserved since the net force acting on each particle vanishes. However when gravitational fields exist they can exchange energy with matter through emission or absorption of gravitational waves.",
        "watermark_text": "We present evidence that the strong equivalence principle ( SEP ) holds in general relativity at least up to second post Newtonian order , by showing that the SEP is fulfilled if it holds at first PN order . We see this by specifically integrating the relaxation Einstein field equations with an appropriate source term which includes the effects of gravitational waves generated by accelerating objects .The results are compatible with those achieved formerly using perturbation theory . This research was supported by NSF grant PHY - 0456747 .In this discussion we will use only isolated systems where no external forces action on any body . For such systems the total energy E = T + U , where T is the kinetic power and U is the potential energy associated with all interactions between particles within the system .If there were no gravity fields then the total energy must be conserved since the net force acting on each particle vanishes . However when gravity fields arise they can trade power with matter through emission or emission of gravitational waves .",
        "rewrite_text": "We provide evidence supporting the strong equivalence principle (SEP) in general relativity, established at least up to second post-Newtonian order, by demonstrating that if the SEP holds at first post-Newtonian order, it also holds at the second order. This is shown through the integration of the relaxed Einstein field equations, incorporating a suitable source term that accounts for gravitational waves emitted by accelerating objects. Our findings align with previous results obtained through perturbation theory. This research was funded by NSF grant PHY-0456747. In this discussion, we will focus solely on isolated systems, where no external forces act on any of the bodies. In such systems, the total energy is given by E = T + U, where T represents kinetic energy and U denotes the potential energy from interactions among the particles. In the absence of gravitational fields, total energy is conserved since the net force on each particle is zero. However, when gravitational fields are present, they can exchange energy with matter through the emission or absorption of gravitational waves.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 5.661385170722978,
        "rewrite-fast-z-score": 2.400396792595916
    },
    {
        "original_text": "We study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "We explore the question of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference . We consider two models : ( i ) The first theory requires that all transmitters have fixed power rates .( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically . For both cases , we show how to find an appropriate schedule by solving a sequence of linear programs .Our results hold even if there exists only one receiver per transmitter . This work was supported by NSF grant CCF - 0430018 .1 Introduction Wireless networks consist of several nodes communicating via radio signals . Each node has restricted range and therefore cannot transmit directly with every other node .Instead , it communicates indirectly through intermediate nodes termed relays or routers . A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "We investigate the challenge of establishing a suitable schedule for information transmission across multiple channels under interference constraints. Each channel is designated for a specific transmitter-receiver pair, and signals from different pairs may interfere with one another. We examine two models: (i) The first model mandates that all transmitters operate at fixed power levels. (ii) The second model allows for dynamic adjustment of transmitter power levels. For both scenarios, we demonstrate how to devise an appropriate schedule by solving a series of linear programming problems. Our findings remain valid even with only one receiver assigned to each transmitter. This research was funded by NSF grant CCF-0430018. \n\n1 Introduction: Wireless networks consist of several nodes that communicate via radio signals. Each node has a limited range and cannot transmit directly to every other node; instead, communication occurs indirectly through intermediate nodes known as relays or routers. A fundamental question in this context is: How should these relays be optimally positioned?",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 3.623286509262706,
        "rewrite-fast-z-score": -0.10846522890932808
    },
    {
        "original_text": "We present X-ray luminosity functions (XLFs) for normal galaxies at z=0.5-2, based on deep Chandra observations of the Great Observatories Origins Deep Survey-North field. We use photometric redshifts to select galaxy samples with different stellar masses and star formation rates. The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx=1041 erg s-1. Our results show that the faint-end slope is steeper than -1.0 for all mass bins considered here. For massive galaxies (log(M/Msun)>10), we find no significant evolution between z=1.6 and 0.7. However, there appears to be some evidence for evolution towards lower characteristic luminosities as well as flatter slopes at higher redshift. These results suggest that the most massive galaxies may have evolved into less luminous systems over time.  We also compare our results with previous studies which used shallower data or smaller sample sizes.",
        "watermark_text": "We create X - ray luminosity functions ( XLFs ) for normal galaxies at z = 0 . 5 - 2 , relying on deep Chandra measurements of the Great Observatories Origins Deep Survey - North area . We use photometric redshifts to select galaxy samples with various stellar masses and sun formation rates .The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx = 1041 erg s - 1 . Our results show that the faint - end slope is steeper than - 1 . 0 for all mass bins included here .For huge objects ( log ( M / Msun ) > 10 ) , we find no considerable evolution between z = 1 . 6 and 0 . 7 . However , there seems to be some evidence for expansion towards lesser characteristic luminosities as well as flatter slopes at higher redshift .These conclusions show that the most large galaxies must have transformed into fewer luminous systems over time . We additionally compare our findings with previous research which using shallower evidence or smaller data numbers .",
        "rewrite_text": "We generate X-ray luminosity functions (XLFs) for normal galaxies at redshifts of z = 0.5 to 2, utilizing deep Chandra observations from the Great Observatories Origins Deep Survey - North region. By employing photometric redshifts, we select galaxy samples with varying stellar masses and star formation rates. The XLFs are calculated by fitting the observed number counts with a Schechter function that includes an exponential cutoff for luminosities below Lx = 10^41 erg s^-1. Our findings indicate that the faint-end slope is steeper than -1.0 across all mass bins examined. For massive galaxies (log(M/Msun) > 10), we observe no significant evolution between z = 1.6 and 0.7. However, there is some indication of a shift toward lower characteristic luminosities and flatter slopes at higher redshifts. These results suggest that the most massive galaxies have evolved into less luminous systems over time. Additionally, we compare our results to previous studies that utilized shallower data or smaller sample sizes.",
        "ori-fast-z-score": -1.0681034923744679,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": -0.4923659639173309
    },
    {
        "original_text": "We present an algorithm to construct the reduced basis space in the context of nonlinear problems with multiple solutions, which is based on the concept of quasi-equilibrium grid (QEG). The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate-independent processes such as plasticity or damage mechanics. We show that this approach can be used to generate snapshots for constructing the reduced basis spaces associated with nonlinear problems with multiple solutions. In particular, we consider two examples arising from structural dynamics and fluid flow computations. Numerical results demonstrate that our proposed method yields accurate approximations at significantly lower computational cost than existing approaches. Keywords: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation. 1 Introduction.\nThe goal of this work is to develop efficient algorithms for generating snapshots for constructing the RB spaces associated with nonlinear problems having multiple solutions. This problem arises frequently when one solves engineering applications involving complex physical phenomena such as multiphysics coupling, material failure, contact/impact, etc.. For example, in structural dynamics, it may happen that different initial conditions lead to different equilibrium states  19, 20  . Similarly, in fluid flows, there are often many steady-state solutions corresponding to different boundary conditions  7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18  .\nIn order to solve these types of problems efficiently using the reduced basis method (RBM), it is necessary to have a good set of snapshots representing all possible solution behaviors. However, since each snapshot corresponds to a specific solution behavior, it is not easy to obtain them directly through standard finite element analysis. Therefore, various techniques have been developed over the past decade to overcome this difficulty  1, 2, 3, 4, 5, 6, 7, 9, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40",
        "watermark_text": "We present an algorithm to build the reduced basis space in the context of nonlinear issues with many solutions , which is based on the idea of quasi - equilibrium grid ( QEG ) . The QEG method was originally developed by Simo and Armero as a numerical technique for solving rate - based processes such as plasticity or damage mechanics .We see that this methodology can be used to create snapshots for constructing the reduced basis sets associated with nonlinear issues with many solutions . In particular , we define two examples arising from functional dynamics and fluid stream computations .Numerical results show that our proposed approach produces accurate approximations at significantly reduced theoretical cost than existing techniques . Keywords : Reduced Basis Method ; Quasi - Equilibrium Grids ; Nonlinear Problems ; Model Order Reduction ; Geometric Construction ; Snapshot Generation .1 Introduction . The goal of this project is to develop fast algorithms for generating snapshots for constructing the RB spaces related with nonlinear issues having many solutions .This problem arises often when one solves engineering applications requiring complex physical phenomena such as multiphysics coupling , structure crash , touch / explosion , etc . . For instance , in structural physics , it could happen that different initial conditions lead to different equilibrium states 19 , 20 . Similarly , in flow flows , there are often many steady - condition solutions corresponding to different boundary rules 7 , 8 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 .In order to solve these kinds of problems easily using the reduced basis method ( RBM ) , it is required to have a good collection of snapshots describing all possible solution behaviors . However , since each snapshot belongs to a certain solve behavior , it is not straightforward to obtain them directly through conventional finite element extraction .Therefore , various methods have been used over the previous decade to overcome this obstacle 1 , 2 , 3 , 4 , 5 , 6 , 7 , 9 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40",
        "rewrite_text": "We propose an algorithm for constructing a reduced basis space specifically tailored for nonlinear problems with multiple solutions, leveraging the concept of the quasi-equilibrium grid (QEG). Initially developed by Simo and Armero as a numerical technique for addressing rate-based phenomena like plasticity and damage mechanics, the QEG approach can effectively generate snapshots essential for forming reduced basis sets in nonlinear scenarios featuring many potential solutions. To illustrate the applicability of this methodology, we present two case studies from functional dynamics and fluid dynamics simulations. Our numerical results indicate that this method yields highly accurate approximations while achieving a significantly lower theoretical cost compared to existing techniques. \n\n**Keywords**: Reduced Basis Method; Quasi-Equilibrium Grids; Nonlinear Problems; Model Order Reduction; Geometric Construction; Snapshot Generation.\n\n**1 Introduction**  \nThe objective of this project is to formulate efficient algorithms for generating snapshots that aid in constructing reduced basis spaces associated with nonlinear problems characterized by multiple solutions. Such challenges frequently arise in engineering applications that involve complex physical phenomena, including multiphysics coupling, structural collisions, and impacts/explosions. For example, in structural mechanics, varying initial conditions can lead to distinct equilibrium states. Likewise, fluid flow problems often exhibit numerous steady-state solutions that correspond to different boundary conditions. To address these issues effectively using the reduced basis method (RBM), a well-curated collection of snapshots that encapsulate all potential solution behaviors is essential. However, since each snapshot relates to a specific solution behavior, they cannot be easily obtained through traditional finite element extraction methods. Consequently, various strategies have been explored in the past decade to surmount this challenge.",
        "ori-fast-z-score": -1.5888598190134724,
        "water-fast-z-score": 8.0,
        "rewrite-fast-z-score": -0.9931270663228415
    },
    {
        "original_text": "We present the results of an investigation into the physical nature of polar broad absorption line quasars (BALQSOs). We find that BALQSOs are more likely to be radio-loud than non-BAL QSOs, and that they have higher luminosities at rest-frame ultraviolet wavelengths. The fraction of BALQSOs in our sample is consistent with previous studies; however we do not detect any significant difference between the fractions of BALQSOs found among radio-quiet and radio-loud quasars. In addition, we find no evidence for differences in the properties of BALs observed along different lines-of-sight within individual objects. Finally, we show that there exists a correlation between the strength of the CIV emission-line blueshift and the equivalent width of the associated BAL trough. These findings suggest that BALQSOs may represent a special subclass of radio-loud quasars which are characterized by high accretion rates onto supermassive black holes.",
        "watermark_text": "We present the conclusion of an research into the physical nature of polar wide absorption line quasars ( BALQSOs ) . We see that BALQSOs are more likely to be radio - loud than non - BAL QSOs , and that they have greater luminosities at rest - frame ultraviolet wavelengths .The percentage of BALQSOs in our sample is compatible with previous research ; however we do not detect any large change between the fractions of BALQSOs discovered among radio - quiet and radio - loud quasars . In addition , we find no evidence for variations in the properties of BALs observed along various lines - of - view within particular objects .Finally , we prove that there exists a correlation between the strength of the CIV radiation - line blueshift and the equivalent size of the associated BAL trough . These conclusions propose that BALQSOs might represent a unique subclass of radio - loud quasars which are marked by high accretion speeds onto supermassive black holes .",
        "rewrite_text": "We present the findings of our research on the physical characteristics of polar broad absorption line quasars (BALQSOs). Our analysis indicates that BALQSOs are more likely to be radio-loud compared to non-BAL QSOs and exhibit higher luminosities in the rest-frame ultraviolet spectrum. The proportion of BALQSOs in our sample aligns with previous studies; however, we do not observe any significant variation in the fractions of BALQSOs between radio-quiet and radio-loud quasars. Furthermore, we find no evidence of changes in the properties of BALs across different lines of sight within specific objects. Finally, we demonstrate a correlation between the strength of the CIV emission line blueshift and the equivalent width of the corresponding BAL trough. These findings suggest that BALQSOs may represent a distinct subclass of radio-loud quasars characterized by high accretion rates onto supermassive black holes.",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 6.573840933228048,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "We present results from direct cosmological hydrodynamic simulations that follow the formation of supermassive black holes (SMBHs) in galactic nuclei, their subsequent evolution through mergers with other SMBHs, and the associated feedback on galaxy properties. We find that:  The simulated SMBH mass function agrees well with observations at z = 0 for M• > 10^7M_solar.  At higher redshifts, our model predicts too many low-mass SMBHs compared to observational estimates based on quasar luminosity functions; this discrepancy may be due to uncertainties in the assumed duty cycle or radiative efficiency of quasars.  Our models predict an average Eddington ratio distribution that is consistent with observed distributions inferred from optical/UV emission lines.  In addition, we show that the predicted relation between BH mass and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass.",
        "watermark_text": "We present results from direct cosmological hydrodynamic simulations that take the formation of supermassive black holes ( SMBHs ) in galactic nuclei , their ensuing evolution through mergers with other SMBHs , and the associated feedback on star dynamics . We see that : The simulated SMBH weight distribution agrees well with observations at h = 0 for M • > 10 ^ 7M _ solar .At higher redshifts , our model predicts too many lowest - weight SMBHs compared to observational projections based on quasar luminosity functions ; this discrepancy may be due to uncertainties in the expected duty cycle or radiative efficiency of quasars . Our models predict an estimated Eddington density distribution that is compatible with observed distributions inferred from optical / UV absorption lines .In addition , we prove that the expected relation between BH weight and bulge velocity dispersion agrees reasonably well with observations over four orders of magnitude in BH mass .",
        "rewrite_text": "We present findings from direct cosmological hydrodynamic simulations that explore the formation of supermassive black holes (SMBHs) in galactic nuclei, their subsequent evolution through mergers with other SMBHs, and the feedback effects on stellar dynamics. Our results indicate that the weight distribution of simulated SMBHs aligns closely with observations at h = 0 for masses greater than 10^7 M_solar. However, at higher redshifts, our model predicts an excess of low-weight SMBHs compared to observational data derived from quasar luminosity functions. This discrepancy may stem from uncertainties regarding the expected duty cycle or radiative efficiency of quasars. Furthermore, our models yield an estimated Eddington density distribution that matches well with observed distributions inferred from optical and UV absorption lines. Additionally, we demonstrate that the anticipated correlation between black hole mass and bulge velocity dispersion is consistent with observations across four orders of magnitude in black hole mass.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "We propose to use hierarchical hidden Markov random fields (HHMRFs) as the underlying model in an unsupervised segmentation algorithm for hyperspectral images. The HHMRFs are constructed by combining several layers of hidden Markov chains, where each layer is associated with one particular spatial scale. We show that this multiscale approach leads to improved performance over single-scale methods and we demonstrate its effectiveness on two different data sets. Finally, we compare our results against those obtained using state-of-the-art algorithms based on Gaussian mixture models or sparse coding techniques. \nIntroduction\n\nHyperspectral imaging has become increasingly popular during recent years due to advances in sensor technology  1  . In contrast to conventional color cameras which capture only three bands per pixel, hyperspectral sensors can record hundreds of narrow spectral bands simultaneously  2  , leading to high-dimensional data volumes. This poses new challenges both in terms of storage requirements and computational complexity  3  .\nIn many applications it would be desirable to perform automatic analysis of such large amounts of data without any prior knowledge about the scene being observed  4  . One important task in this context is the detection of homogeneous regions within the image  5  . These so-called segments may correspond to individual objects  6  , but they could also represent parts of larger structures like buildings  7  or roads  8  .",
        "watermark_text": "We suggest to use hierarchical hidden Markov random fields ( HHMRFs ) as the fundamental design in an unsupervised segmentation algorithm for hyperspectral pictures . The HHMRFs are built by combining several elements of hidden Markov chains , where each layer is associated with one certain spatial scale .We suggest that this multiscale approach leads to improved performance over double - scale techniques and we prove its effectiveness on two different data sets . Finally , we compare our findings against those achieved using state - of - the - art algorithms based on Gaussian mixture models or dense coding techniques .Introduction Hyperspectral photography has become especially popular during recent years owing to advances in sensor technology 1 . In comparison to conventional color cameras which capture only three bands per pixel , hyperspectral cameras can display hundreds of broad wavelength lines simultaneously 2 , leading to large - dimensional data capacities .This poses novel challenges both in terms of storage needs and computational capability 3 . In many applications it would be desirable to conduct automatic analysis of such substantial amounts of evidence without any earlier knowledge about the scene being detected 4 .One important task in this context is the observation of homogeneous parts within the image 5 . These so - called features could belong to individual objects 6 , but they may sometimes reflect parts of bigger structures like houses 7 or roads 8 .",
        "rewrite_text": "We propose the use of hierarchical hidden Markov random fields (HHMRFs) as the core framework for an unsupervised segmentation algorithm aimed at hyperspectral images. HHMRFs are constructed by integrating multiple elements of hidden Markov chains, with each layer corresponding to a specific spatial scale. We believe that this multiscale strategy enhances performance compared to dual-scale methods, and we validate its effectiveness on two distinct datasets. Furthermore, we benchmark our results against those obtained using leading algorithms that rely on Gaussian mixture models or dense coding techniques. \n\n**Introduction:** In recent years, hyperspectral photography has gained considerable popularity due to advancements in sensor technology. Unlike traditional color cameras that capture just three bands per pixel, hyperspectral cameras can simultaneously record hundreds of wavelength bands, resulting in high-dimensional data. This development introduces new challenges regarding storage and computational requirements. In numerous applications, it is essential to perform automated analysis of this vast amount of data without prior knowledge of the observed scene. A critical task in this arena is identifying homogeneous regions within the image. These regions, referred to as features, may correspond to distinct objects, but they can also represent components of larger structures such as buildings or roadways.",
        "ori-fast-z-score": 0.09245003270420485,
        "water-fast-z-score": 7.613508865259127,
        "rewrite-fast-z-score": -1.0634101379502299
    },
    {
        "original_text": "We present new models for non-abelian string networks, which are based on the idea that the network is composed by many entangled strings with different velocities and orientations. We show how to construct such velocity-dependent models in terms of Feynman diagrams. In particular we consider two types of diagrams: (i) ladder-like diagrams corresponding to the exchange of gluons between pairs of strings, (ii) cross-ladder like-diagrams describing interactions among three or more strings. The latter type of diagrams can be interpreted as representing junctions where several strings meet at one point. We discuss some properties of these models and compare them with previous results obtained within the framework of Abelian-Higgs model. Finally, we study numerically the evolution of an initial configuration consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations. This work was supported by the DFG under contract SFB-TR9  Gravitational Physics ",
        "watermark_text": "We introduce novel theories for non - abelian string systems , which are based on the idea that the network is composed by many entangled strings with various velocities and orientations . We see how to build such velocity - dependent models in terms of Feynman diagrams .In particular we define two forms of diagrams : ( i ) ladder - like diagrams corresponding to the transfer of gluons between pairs of sequences , ( ii ) cross - ladder like - diagrams describing relationships among three or more strings . The last sort of diagrams can be interpreted as representing junctions where many strings join at one point .We discuss some properties of these models and link them with previous findings obtained within the framework of Abelian - Higgs system . Finally , we study numerically the evolution of an initial structure consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations .This project was supported by the DFG under contract SFB - TR9 Gravitational Physics",
        "rewrite_text": "We present new theories for non-abelian string systems, grounded in the concept of a network formed by multiple entangled strings exhibiting various velocities and orientations. We demonstrate how to construct these velocity-dependent models using Feynman diagrams. Specifically, we identify two types of diagrams: (i) ladder-like diagrams that represent the transfer of gluons between pairs of strings, and (ii) cross-ladder-like diagrams that illustrate interactions among three or more strings. The latter type can be interpreted as junctions where multiple strings converge at a single point. We examine some properties of these models and relate them to previous results obtained within the Abelian-Higgs framework. Furthermore, we perform numerical studies using Monte Carlo simulations to analyze the evolution of an initial configuration comprising a single straight string into a complex tangle of interacting strings. This research was funded by the DFG under contract SFB-TR9 Gravitational Physics.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "We present the results of our analysis of the effects of cosmic rays on galaxy clusters, using cosmological hydrodynamic simulations with and without CRs. We find that the presence of CRs leads to an increase in gas temperature at large radii (r > 0.5Rvir) due to adiabatic compression of the ICM during cluster formation. The effect is more pronounced for higher values of the initial magnetic field strength. In addition we find that there are significant differences between the radial profiles of X-ray surface brightness obtained with and without CRs which can be used as observational signatures of their presence. Finally, we show that the inclusion of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster. This work was supported by NASA grant NAG5-9998. Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "watermark_text": "We present the conclusion of our analysis of the effects of cosmic rays on star clusters , using cosmological hydrodynamic simulations with and without CRs . We see that the presence of CRs causes to an increase in gas temperature at large radii ( r > 0 . 5Rvir ) due to adiabatic compression of the ICM during cluster formed .The phenomenon is more pronounced for greater values of the first magnetic force power . In addition we find that there are significant variations between the transverse characteristics of X - ray exterior illumination obtained with and without CRs which can be used as observational signatures of their presence .Finally , we prove that the integration of CRs has little impact on the global properties such as total mass or luminosity but does affect the distribution of metals within the cluster . This research was supported by NASA grant NAG5 - 9998 .Cosmological hydrodynamical simulations were performed using Enzo developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "rewrite_text": "We conclude our analysis of the impact of cosmic rays on star clusters, utilizing cosmological hydrodynamic simulations both with and without cosmic rays. Our findings indicate that the presence of cosmic rays leads to an increase in gas temperature at larger radii (r > 0.5Rvir) due to adiabatic compression of the intra-cluster medium (ICM) during the formation of the cluster. This effect is more pronounced at higher values of the initial magnetic force strength. Furthermore, we observe significant differences in the transverse characteristics of X-ray surface brightness between simulations with and without cosmic rays, which could serve as observational signatures of their presence. Lastly, while the inclusion of cosmic rays has minimal impact on the overall properties, such as total mass and luminosity, it notably affects the distribution of metals within the cluster. This research was funded by NASA grant NAG5-9998, and the cosmological hydrodynamical simulations were conducted using Enzo, developed by the Laboratory for Computational Astrophysics at the University of California, San Diego.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 1.6733200530681511
    },
    {
        "original_text": "We present the demographics and properties of transition objects in SDSS DR7, which are defined as galaxies with both emission lines (ELGs) and absorption features (AGNs). We find that there is an excess number of ELG-AGN pairs at small separations compared to random distributions. The fraction of AGNs among all ELGs increases towards lower luminosities. There appears to be no significant difference between the fractions of AGNs found within different types of ELGs. These results suggest that some ELGs may harbor hidden AGNs. This work was supported by NASA grant NNX10AD65G. We thank the anonymous referee for helpful comments on this manuscript. In recent years, it has been shown that many active galactic nuclei (AGNs), especially those with low luminosity or obscured by dusty torii, have strong emission line components (see e.g., Ho et al. (1997) , Hao et al. (2005) ), making them appear like normal star-forming galaxies when observed through optical spectroscopic surveys such as Sloan Digital Sky Survey (SDSS; York et al. (2000) ) .\nIn order to identify these  transition objects , we use two criteria based on their spectral energy distribution (SED): 1) they must show both emission lines (ELGs; see Section 2.1 below) and absorption features (Section 2.2) simultaneously; and 2) they should not be classified as quasars according to the BPT diagram (Baldwin et al. 1981 , Kewley et al. 2001 . By applying these selection criteria to the entire sample of galaxies in the seventh data release (DR7; Abazajian et al. 2009 ) of the SDSS, we obtain a total of 16,082 transition objects out of a parent sample of 3,962,843 galaxies.",
        "watermark_text": "We present the demographics and features of transfer objects in SDSS DR7 , which are specified as galaxies with both emission lines ( ELGs ) and emission elements ( AGNs ) . We see that there is an excess amount of ELG - AGN pairs at small separations compared to random distributions .The percentage of AGNs among all ELGs increases towards less luminosities . There seems to be no major variation between the fractions of AGNs observed within various types of ELGs .These data suggest that some ELGs might harbor hidden AGNs . This research was supported by NASA grant NNX10AD65G .We thank the anonymous referee for useful comments on this manuscript . In recent years , it has been shown that several active galactic nuclei ( AGNs ) , particularly those with poor luminosity or obscured by dusty torii , have strong emitted path constituents ( saw e . g . , Ho et al .( 1997 ) , Hao et al . ( 2005 ) ) , making them seem like usual star - creating galaxies when observed through optical spectroscopic studies such as Sloan Digital Sky Survey ( SDSS ; York et al .( 2000 ) ) . In order to identify these transition objects , we using two requirements according on their spectral power distribution ( SED ) : 1 ) they must show both emission lines ( ELGs ; seeing Section 2 . 1 below ) and emission elements ( Section 2 . 2 ) simultaneously ; and 2 ) they should not be categorized as quasars according to the BPT chart ( Baldwin et al .1981 , Kewley et al . 2001 .By applying these selection criteria to the entire sample of galaxies in the seventh data release ( DR7 ; Abazajian et al . 2009 ) of the SDSS , we obtain a total of 16 , 082 transition objects out of a parent sample of 3 , 962 , 843 galaxies .",
        "rewrite_text": "We present the demographic characteristics and features of transfer objects identified in SDSS DR7, specifically focusing on galaxies classified as emission line galaxies (ELGs) and emission line active galactic nuclei (AGNs). Our analysis reveals a significant excess of ELG-AGN pairs at small separations in comparison to random distributions. Notably, the proportion of AGNs among all ELGs rises as luminosity decreases. Additionally, we observe minimal variation in the AGN fractions across different types of ELGs, suggesting that some ELGs may conceal hidden AGNs. This research was supported by NASA grant NNX10AD65G, and we appreciate the valuable feedback from the anonymous referee on this manuscript. Recent studies have demonstrated that certain AGNs, especially those with low luminosity or those obscured by dusty tori, exhibit strong emission line features, making them appear similar to typical star-forming galaxies in optical spectroscopic surveys like the Sloan Digital Sky Survey (SDSS; York et al. 2000). To identify these transition objects, we established two criteria based on their spectral energy distribution (SED): 1) they must exhibit both emission lines (as described in Section 2.1) and emission elements (Section 2.2) simultaneously; and 2) they should not be classified as quasars according to the BPT diagram (Baldwin et al. 1981; Kewley et al. 2001). By applying these selection criteria to the complete galaxy sample in the seventh Data Release (DR7; Abazajian et al. 2009) of the SDSS, we identified a total of 16,082 transition objects from a parent sample of 3,962,843 galaxies.",
        "ori-fast-z-score": -3.104378865665871,
        "water-fast-z-score": 4.308294733275792,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "We have analyzed the pulse profiles in two energy bands (3-20 keV, 20-100 keV) for 4U0115+63 using data obtained by the Rossi X-ray Timing Explorer (RXTE). The results show that there is no significant difference between these two energy bands except at phase 0.7-0.9 where we see an excess emission above 100 keV. We also find that this source shows a double peaked profile with a dip around phase 0.5 which can be explained as due to absorption effects on the line-of-sight. In addition, we have used data from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), which has detected cyclotrons lines near 30 keV and 60 keV respectively. Using our model parameters derived from the RXTE data analysis, we are able to reproduce both the observed pulse profiles and the cyclotrons line energies simultaneously.",
        "watermark_text": "We have analyzed the pulse profiles in two energy bands ( 3 - 20 keV , 20 - 100 keV ) for 4U0115 + 63 utilizing information obtained by the Rossi X - ray Timing Explorer ( RXTE ) . The results show that there is no major variation between these two energy bands except at phase 0 . 7 - 0 . 9 where we saw an excess emission above 100 keV .We additionally find that this source shows a double peaked feature with a dip around phase 0 . 5 which can be understood as owing to absorption effects on the line - of - view . In addition , we have utilized information from the International Gamma - Ray Astrophysics Laboratory ( INTEGRAL ) , which has detected cyclotrons lines near 30 keV and 60 keV respectively .Using our model values generated from the RXTE information study , we are able to predict both the seen beam profiles and the cyclotrons line values concurrently .",
        "rewrite_text": "We conducted an analysis of the pulse profiles for 4U 0115+63 in two energy bands (3-20 keV and 20-100 keV) using data from the Rossi X-ray Timing Explorer (RXTE). Our findings indicate that there is little variation between the two energy bands, except for the interval between phase 0.7 and 0.9, where we observed an excess emission above 100 keV. Furthermore, we identified a double-peaked feature with a dip around phase 0.5, which can be attributed to absorption effects along the line of sight. Additionally, we utilized data from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), which detected cyclotron lines at approximately 30 keV and 60 keV. By applying the model values derived from our RXTE analysis, we successfully predicted both the observed beam profiles and the cyclotron line values.",
        "ori-fast-z-score": 0.13736056394868904,
        "water-fast-z-score": 4.354648431614539,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "The measurement of ultra-low potassium contaminations in silicon is important for the development and production of semiconductor devices, especially solar cells. The detection limit of conventional methods such as flame photometry or atomic absorption spectroscopy (AAS) is not sufficient to meet the requirements set by industry standards. In this work we present an alternative method based on accelerator mass spectrometry (AMS). We show that AMS can be used to measure potassium concentrations down to 10(-12) at% K in Si samples. This corresponds to a sensitivity improvement by three orders of magnitude compared to standard techniques like flame photometry. Furthermore, our results demonstrate that AMS has no significant matrix effects when measuring low-potassium doped Si wafers. Finally, we discuss possible applications of AMS beyond its use as a highly sensitive analytical tool. Keywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry",
        "watermark_text": "The measurement of ultra - low potassium contaminations in silicon is important for the development and production of semiconductor devices , particularly solar devices . The measurement limit of typical techniques such as flame photometry or atomic absorption spectroscopy ( AAS ) is not reasonable to meet the requirements set by industry standards .In this research we present an different method using on accelerator mass spectrometry ( AMS ) . We see that AMS can be used to measure potassium levels down to 10 ( - 12 ) at % K in Si samples .This corresponds to a sensitivity difference by three orders of magnitude compared to standard methods like fire photometry . Furthermore , our findings show that AMS has no considerable matrix impacts when examining low - potassium doped Si wafers .Finally , we discuss possible use of AMS beyond its use as a highly sensitive diagnostic tool . Keywords : Silicon wafer , Accelerator mass spectrometry , Flame photometry",
        "rewrite_text": "Measuring ultra-low potassium contamination in silicon is crucial for the advancement and production of semiconductor devices, especially solar technologies. Traditional techniques, such as flame photometry and atomic absorption spectroscopy (AAS), often fall short of meeting industry standards due to their measurement limits. In this study, we introduce an alternative method utilizing accelerator mass spectrometry (AMS). Our results indicate that AMS can detect potassium concentrations as low as 10^(-12) at % K in silicon samples, providing a sensitivity that exceeds standard methods like flame photometry by three orders of magnitude. Additionally, we find that AMS exhibits minimal matrix effects when analyzing low-potassium-doped silicon wafers. Finally, we explore potential applications of AMS beyond its role as a highly sensitive diagnostic tool. Keywords: Silicon wafer, Accelerator mass spectrometry, Flame photometry.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "We present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 May 24-25 UT. We detected two sources within the central 0.5 arcmin region; one source was found to be associated with an infrared dark cloud (IRDC), while another source was not. Both sources are embedded deeply inside the dusty envelope surrounding the dense core. In addition, we observed this object simultaneously with the Nobeyama 45 m radio telescope at 1 mm wavelength during the same night as our NIR observation. No significant emission line features were seen in either spectrum. Using these observational results, we discuss possible scenarios for the formation of stars in such a young dense core.",
        "watermark_text": "We report new near - infrared ( NIR ) and millimeter - wave studies of the starless rich core FeSt 1 - 457 , which is situated in the Taurus molecular mist complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI camera on 2005 May 24 - 25 UT .We observed two sources within the inner 0 . 5 arcmin region ; one source was reported to be involved with an infrared shadow cloud ( IRDC ) , while another source was not . Both sources are lodged deeply inside the dusty envelope surrounding the dense core .In addition , we studied this body simultaneously with the Nobeyama 45 m radio telescope at 1 mm frequency during the same night as our NIR observation . No notable emission line characteristics were witnessed in either spectrum .Using these observational results , we explain possible strategies for the formation of stars in such a young dense core .",
        "rewrite_text": "We present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, located in the Taurus molecular cloud complex at a distance of 140 parsecs. The NIR data were collected with the Subaru Observatory using the SofI camera from May 24-25, 2005. Within the inner 0.5 arcminute region, we identified two sources: one associated with an infrared dark cloud (IRDC) and the other unassociated. Both sources are embedded deep within the dusty envelope that surrounds the dense core. Additionally, we conducted simultaneous observations with the Nobeyama 45 m radio telescope at a frequency of 1 mm on the same night as our NIR data collection. No significant emission line features were detected in either spectrum. Based on these observations, we propose potential mechanisms for star formation in such a young and dense core.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 0.6622661785325219
    },
    {
        "original_text": "The Megapie (Megavoltage Ionization Projection Imaging Experiment) is an experiment designed to study the feasibility and performance of proton radiography for medical applications. The main goal of this project was to develop a compact, high intensity ion source based on laser-plasma interaction in order to produce protons with energies up to several hundred MeV. In addition, it has been shown that such sources can be used as targets for neutron production by spallation reactions induced by energetic ions. This work presents results obtained during experiments performed at GSI Darmstadt using a pulsed deuteron beam accelerated by the SIS-18 synchrotron accelerator. Neutrons produced by the D+D reaction were detected by means of two fission chambers placed around the target chamber. A detailed analysis of these data allowed us to determine the number of neutrons emitted per incident deuteron particle and their energy distribution.",
        "watermark_text": "The Megapie ( Megavoltage Ionization Projection Imaging Experiment ) is an project meant to study the feasibility and performance of proton radiography for medical uses . The main goal of this project was to develop a compact , large intensity particle channel relying on laser - plasma interaction in order to produce protons with intensity up to several hundred MeV .In addition , it has been shown that such sources can be used as targets for neutron production by spallation reactions generated by energetic ions . This research provides findings obtained during experiments conducted at GSI Darmstadt involving a pulsed deuteron light accelerated by the SIS - 18 synchrotron accelerator .Neutrons created by the D + D process were detected by means of two fission chambers put around the target chamber . A precise analysis of these information permitted us to predict the quantity of neutrons emitted per incident deuteron particle and their power distribution .",
        "rewrite_text": "The Megapie (Megavoltage Ionization Projection Imaging Experiment) is a project designed to investigate the feasibility and efficacy of proton radiography for medical applications. The primary objective of this initiative was to create a compact, high-intensity particle channel that utilizes laser-plasma interactions to generate protons with energies reaching several hundred MeV. Furthermore, it has been demonstrated that these sources can serve as targets for neutron production through spallation reactions initiated by high-energy ions. This research presents findings from experiments conducted at GSI Darmstadt, which involved a pulsed deuteron beam accelerated by the SIS-18 synchrotron accelerator. Neutrons produced from the D + D reaction were detected using two fission chambers positioned around the target chamber. A detailed analysis of this data allowed us to estimate the number of neutrons emitted per incident deuteron and their energy distribution.",
        "ori-fast-z-score": 0.762000762001143,
        "water-fast-z-score": 7.112007112010668,
        "rewrite-fast-z-score": 0.3841106397986879
    },
    {
        "original_text": "We have studied the thick brane model in which our universe is embedded into an extra dimension and found that it can explain some recent observations such as cosmic microwave background anisotropy, supernovae Ia data and baryon acoustic oscillations without introducing any new physics beyond standard model. \n \n In this model, we assume that there exists a scalar field whose potential has two degenerate minima at different values of the vacuum expectation value (VEV). The VEVs are determined by the parameters of the potential. We find that if the difference between these VEVs is large enough to satisfy the condition for the existence of stable domain walls then the thickness of the wall becomes larger than the Hubble radius today. This means that the domain walls cannot be produced during inflationary epoch. On the other hand, if the difference between these vacua is small compared to the Hubble scale, the domain walls will be created after inflation but they decay before nucleosynthesis era due to their extremely high tension.",
        "watermark_text": "We have researched the deep brane model in which our universe is anchored into an additional dimension and found that it can describe some latest measurements such as cosmic microwave background anisotropy , supernovae Ia data and baryon acoustic oscillations without exploring any new science beyond standard model . In this description , we suppose that there exists a scalar field whose potential has two degenerate minima at different values of the vacuum expectation value ( VEV ) .The VEVs are decided by the variables of the potential . We see that if the difference between these VEVs is huge enough to meet the requirement for the existence of stable domain barriers then the length of the wall grows larger than the Hubble diameter today .This implies that the domain barriers cannot be formed during inflationary epoch . On the other hand , if the difference between these vacua is tiny relative to the Hubble scale , the domain barriers will be formed after inflation but they decay before nucleosynthesis era due to their extremely raised tension .",
        "rewrite_text": "We have examined the deep brane model, which posits that our universe is situated within an additional dimension. Our findings indicate that this model can account for recent observations, including cosmic microwave background anisotropy, Type Ia supernova data, and baryon acoustic oscillations, without necessitating any new scientific frameworks beyond the standard model. In our analysis, we assume the presence of a scalar field characterized by a potential with two degenerate minima at different vacuum expectation values (VEVs). The specific VEVs are determined by the potential's variables. We observe that if the difference between these VEVs is sufficiently large to allow for the existence of stable domain walls, the size of the walls will exceed the current Hubble radius. This suggests that domain walls could not form during the inflationary period. Conversely, if the difference between the vacua is small compared to the Hubble scale, these domain walls may form after inflation, but they would likely decay before the nucleosynthesis era due to their significantly increased tension.",
        "ori-fast-z-score": -2.629502940535666,
        "water-fast-z-score": 4.063777271736939,
        "rewrite-fast-z-score": -1.162476387438193
    },
    {
        "original_text": "We study the global properties of solar active regions by using high-resolution magnetograms, vector magnetic fields (VMB), and photospheric velocity maps obtained with Hinode/SOT/SP. We find that there is an anti-correlation between the inclination angle of the coronal field lines at the PILs and the amount of newly emerged flux in ARs. The correlation coefficient decreases as we go to higher latitudes. This suggests that the emergence of new flux plays an important role for determining the structure of the coronal field above the PILs. In addition, we found that the distribution of the inclination angles of the coronal field line depends on their distance from the center of the sunspot group. The results suggest that the evolution of the coronal field can be understood if one takes into account both the emergence of new flux and the differential rotation. Keywords: Coronal field, Active region",
        "watermark_text": "We research the global properties of solar active regions by using high - resolution magnetograms , vector magnetic fields ( VMB ) , and photospheric velocity maps obtained with Hinode / SOT / SP . We see that there is an counter - correlation between the inclination angle of the coronal field lines at the PILs and the quantity of newly emerged flux in ARs .The relationship coefficient drops as we move to higher latitudes . This implies that the emergence of new flux serves an important role for determining the composition of the coronal field above the PILs .In addition , we proved that the spread of the inclination angles of the coronal field line depends on their distance from the center of the sunspot group . The results show that the evolution of the coronal field can be understood if one takes into consideration both the emergence of new flux and the differential rotation .Keywords: Coronal field, Active region",
        "rewrite_text": "We investigate the global characteristics of solar active regions by analyzing high-resolution magnetograms, vector magnetic fields (VMB), and photospheric velocity maps obtained from Hinode/SOT/SP. Our findings reveal a counter-correlation between the inclination angle of coronal field lines at the polarity inversion lines (PILs) and the amount of newly emerged flux in active regions (ARs). Notably, the relationship coefficient diminishes as we approach higher latitudes. This suggests that the emergence of new flux plays a crucial role in shaping the coronal field's configuration above the PILs. Furthermore, we have demonstrated that the distribution of inclination angles of the coronal field lines is influenced by their distance from the center of the sunspot group. These results indicate that understanding the evolution of the coronal field requires consideration of both the emergence of new flux and the effects of differential rotation.  \nKeywords: Coronal field, Active region",
        "ori-fast-z-score": 1.4770978917519928,
        "water-fast-z-score": 5.169842621131974,
        "rewrite-fast-z-score": 2.528102914801153
    },
    {
        "original_text": "We present an efficient data redistribution scheme that allows the parallel execution of computations on dynamically resized computational domains in distributed memory environments. The proposed approach is based on multidimensional block-cyclic distributions and exploits locality by using space-filling curves to map blocks onto processors. We show how this technique can be used to efficiently redistribute data between different processor configurations, while minimizing communication overheads. Our experimental results demonstrate significant performance improvements over existing approaches when executing applications with dynamic load balancing requirements. In particular, we achieve speedups up to 3Â compared to state-of-the-art techniques such as the one presented in  1  . \nIntroduction\n\nParallel computing has become increasingly important due to its ability to solve large problems faster than serial computers  2  , but it also presents new challenges related to the distribution of work among multiple processing units  3  .\nIn order to take advantage of parallelism, many algorithms are designed so that they can run simultaneously on several processors  4  . However, these algorithms often require some form of data redistribution during their execution  5  . For example, consider a computation where each process stores part of a dataset (e.g., matrix)  6  . If the number of processes changes at runtime, then all processes need to exchange information about which parts of the dataset they store before continuing  7  . This problem becomes even more challenging if the size of the datasets stored by individual processes varies  8  or if there are dependencies between them  9  .",
        "watermark_text": "We create an efficient information redistribution system that enables the parallel execution of computations on dynamically resized computational regions in distributed storage systems . The proposed approach is based on multidimensional block - cyclic distributions and exploits locality by using space - filling curves to map blocks onto processors .We suggest how this methodology can be used to easily redistribute information between various computer configurations , while minimizing communication overheads . Our research results show considerable performance improvements over existing techniques when executing systems with dynamic load balancing requirements .In particular , we accomplish speedups up to 3Â relative to state - of - the - art methods such as the one illustrated in 1 . Introduction Parallel logic has become rapidly key thanks to its able to solve huge problems quicker than serial machines 2 , but it also provides new obstacles linked to the distribution of work among multiple processing facilities 3 .In order to take advantage of parallelism , many algorithms are built so that they can run simultaneously on numerous processors 4 . However , these algorithms often demand some kind of file redistribution during their execution 5 .For instance , consider a computation where each process stores portion of a dataset ( e . g . , matrix ) 6 . If the number of processes changes at runtime , then all processes must to exchange knowledge about which portions of the dataset they contain before continuing 7 .This problem arises even more challenging if the length of the datasets collected by individual processes vary 8 or if there are dependencies between them 9 .",
        "rewrite_text": "We have developed an efficient information redistribution system that allows for the parallel execution of computations across dynamically resized computational regions in distributed storage systems. Our approach utilizes multidimensional block-cyclic distributions and takes advantage of locality by employing space-filling curves to map blocks onto processors. We propose that this methodology facilitates seamless redistribution of information between various computer configurations while minimizing communication overheads. Our research findings indicate significant performance enhancements compared to existing techniques, particularly for systems that require dynamic load balancing. Specifically, we achieve speedups of up to 3 times relative to leading methods, as illustrated in the introduction. The rise of parallel computing has become essential for solving large problems more rapidly than serial machines; however, it also presents new challenges related to distributing workloads among multiple processing units. To fully harness parallelism, many algorithms are designed to execute concurrently across numerous processors. Nonetheless, these algorithms frequently require some form of file redistribution during their execution. For example, in a scenario where each process holds a segment of a dataset (such as a matrix), if the number of processes changes at runtime, all processes must share information regarding the portions of the dataset they possess before proceeding. This issue becomes even more complex when the sizes of datasets held by individual processes vary or when there are interdependencies between them.",
        "ori-fast-z-score": -0.811502671200689,
        "water-fast-z-score": 7.605002667571556,
        "rewrite-fast-z-score": 3.0508510792387598
    },
    {
        "original_text": "We study the resonance phenomenon for an open-loop control problem in a nonlinear stochastic model describing interactions between phytoplankton (plants) and zooplankton (animals). The main goal is to find optimal values of parameters characterizing external periodic forcing, which maximize the growth rate of planktons. We show that this optimization problem can be reduced to finding solutions of some algebraic equations. In particular, we prove that there exists only one solution corresponding to maximum value of the objective function. Moreover, it turns out that the obtained results are robust with respect to small perturbations of initial conditions. Finally, numerical simulations illustrate our theoretical findings. \n \n Keywords: Stochastic differential equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics \n \n 1 Introduction \n \n Interactions among different species play important role in many natural ecosystems. For example, phytoplankton (algae or plants), living at the base of food chain, provide energy source for other organisms such as zooplankton (fishes or animals). Therefore, understanding how these two populations interact may help us better understand ecosystem functioning. Recently, several mathematical models have been proposed to describe population dynamics of phytoplankton- zooplankton systems  1–3  . These models include deterministic terms representing intrinsic growth rates of both populations and their interaction effects, as well as random fluctuations due to environmental factors. It has been shown that under certain assumptions on the coefficients of the model, its long-term behavior exhibits chaotic attractor  4  , which makes analysis of the system very difficult. On the other hand, if the effect of random fluctuations is neglected then the resulting deterministic model becomes much easier to analyze  5–7  .\n \nIn  8  , authors studied the following model:\n \n \n \n dX(t) = rX(t)(1 - X(t))dt + fX(t)sin(wt)dW(t),\n dY(t) = rY(t)(1 - Y(t))dt + fy(t)sin(w0t)dW(t).\n \n(",
        "watermark_text": "We research the resonance phenomenon for an open - loop control problem in a nonlinear stochastic model explaining interactions between phytoplankton ( plants ) and zooplankton ( animals ) . The main goal is to find optimal values of values characterizing external periodic forcing , which maximize the development time of planktons .We see that this optimization problem can be reduced to finding solutions of some algebraic equations . In particular , we prove that there exists only one solve corresponding to maximum value of the objective function .Moreover , it turns out that the achieved findings are robust with regard to small perturbations of initial conditions . Finally , numerical simulations highlight our theoretical results .Keywords : Stochastic integral equation , Periodic forcing , Resonance , Optimization problems , Nonlinear dynamics 1 Introduction Interactions among different species play attractive role in different biological environments . For instance , phytoplankton ( algae or plants ) , live at the base of eat chain , provide energy source for other species such as zooplankton ( fishes or organisms ) .Therefore , studying how these two communities interact may assist us better understand ecological functioning . Recently , various computational models have been proposed to explain population behavior of phytoplankton - zooplankton communities 1 – 3 .These models include deterministic terms representing intrinsic development rates of both populations and their interaction influences , as also as random fluctuations owing to environmental factors . It has been shown that under certain assumptions on the coefficients of the model , its long - term behavior presents dynamic attractor 4 , which makes study of the system very difficult .On the other hand , if the impact of random fluctuations is neglected then the resulting deterministic model seems far easy to analyze 5 – 7 . In 8 , authors explored the following model : dX ( t ) = rX ( t ) ( 1 - X ( t ) ) dt + fX ( t ) sin ( wt ) dW ( t ) , dY ( t ) = rY ( t ) ( 1 - Y ( t ) ) dt + fy ( t ) sin ( w0t ) dW ( t ) .(",
        "rewrite_text": "We investigate the resonance phenomenon in an open-loop control context within a nonlinear stochastic model that describes the interactions between phytoplankton (plants) and zooplankton (animals). Our primary objective is to identify optimal values for external periodic forcing that maximize the growth rate of plankton populations. We demonstrate that this optimization problem can be simplified to finding solutions to specific algebraic equations. Notably, we establish the existence of a unique solution corresponding to the maximum value of the objective function. Furthermore, our results show robustness against small perturbations in the initial conditions. Lastly, numerical simulations support our theoretical findings. \n\n**Keywords:** Stochastic integral equation, Periodic forcing, Resonance, Optimization problems, Nonlinear dynamics \n\n**1 Introduction** The interactions among different species are crucial in various biological contexts. For example, phytoplankton (algae or plants), which occupy the base of the food chain, serve as an energy source for other organisms like zooplankton (fishes or other animals). Therefore, examining the relationship between these two groups aids our understanding of ecological dynamics. Recently, several computational models have been developed to analyze the population dynamics of phytoplankton-zooplankton communities. These models incorporate deterministic components that represent the intrinsic growth rates of both populations as well as their interaction effects, alongside random fluctuations caused by environmental factors. It has been shown that under certain model assumptions, the long-term behavior may exhibit a dynamic attractor, complicating the study of the system. Conversely, neglecting the influence of random fluctuations results in a deterministic model that is significantly easier to analyze. In reference 8, the authors examined the following model: \n\n\\[ dX(t) = rX(t)(1 - X(t)) dt + fX(t) \\sin(wt) dW(t) \\] \n\\[ dY(t) = rY(t)(1 - Y(t)) dt + fY(t) \\sin(w_0t) dW(t) \\]",
        "ori-fast-z-score": 2.482817665807104,
        "water-fast-z-score": 8.877545314489295,
        "rewrite-fast-z-score": 1.1188618555710317
    },
    {
        "original_text": "We study the scattering properties of surface plasmons (SPs) by a single subwavelength groove in an optically thick metal film, which is excited by a normally incident plane wave with circular polarization. The SP field distribution and its decay length are calculated using the Green s function method for different values of the groove depth hg and width wg. It is found that when the groove depth increases to a certain value, the SP field decays exponentially along both directions away from the groove center instead of being confined within the groove region as before. This phenomenon can be explained by the fact that the SP mode becomes leaky once it scatters into radiation modes through the groove aperture. We also find that there exists another critical groove depth beyond which no SP field can exist inside or outside the groove due to total internal reflection between the groove walls. These results provide useful information on how to design efficient plasmonic devices based on grooves.",
        "watermark_text": "We determine the scattering properties of surface plasmons ( SPs ) by a single subwavelength groove in an optically dense metal film , which is excited by a normally incident plane wave with circular polarization . The SP field distribution and its decay width are measured using the Green s function method for different values of the groove height hg and width wg .It is found that when the groove height rises to a certain value , the SP field decays exponentially along both directions away from the groove center instead of being confined within the groove region as before . This phenomenon can be described by the fact that the SP mode becomes leaky once it scatters into radiation modes through the groove aperture .We additionally find that there exists another important groove width beyond which no SP field can occur inside or outside the groove due to complete internal reflection between the groove walls . These conclusions provide useful info on how to build efficient plasmonic systems relying on grooves .",
        "rewrite_text": "We investigate the scattering characteristics of surface plasmons (SPs) created by a single subwavelength groove in a densely optically active metal film, excited by a normally incident plane wave with circular polarization. Using the Green's function method, we measure the SP field distribution and its decay width for various groove height (hg) and width (wg) parameters. Our findings reveal that when the groove height reaches a certain threshold, the SP field decays exponentially in both directions away from the groove center, rather than remaining confined to the groove area as it did previously. This behavior occurs because the SP mode becomes leaky when it scatters into radiation modes through the groove aperture. Furthermore, we identify an additional critical groove width beyond which no SP field is present inside or outside the groove, due to complete internal reflection occurring between the groove walls. These insights offer valuable guidance for the design of efficient plasmonic systems based on grooves.",
        "ori-fast-z-score": -2.0647416048350555,
        "water-fast-z-score": 2.7529888064467407,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We present new observations of the low surface brightness galaxy PGC 045080, which is known to host an active galactic nucleus (AGN). We use these data to study the properties of this AGN as well as its relationship with the surrounding gas disk.  The AGN has been detected by previous studies at radio wavelengths using Very Large Array (VLA) observations. In our work we have used VLA archival data along with new observations made with the Karl G. Jansky Very Large Array (JVLA), to detect emission lines associated with the AGN. These include H-alpha,  NII  ,  SII  ,  OIII  , and  CII  . Using these line fluxes we calculate the luminosity of the AGN to be 1.1 x 10^41 erg/sec. This value agrees very closely with that found for other similar galaxies. We also find evidence for outflows on both large and small scales around the AGN.",
        "watermark_text": "We report new images of the poor floor velocity galaxy PGC 045080 , which is known to host an active galactic nucleus ( AGN ) . We use these information to study the properties of this AGN as well as its connection with the nearby gas envelope .The AGN has been detected by earlier surveys at radio wavelengths using Very Large Array ( VLA ) observations . In our work we have utilized VLA archival data along with recent observations made with the Karl G . Jansky Very Large Array ( JVLA ) , to identify emission lines associated with the AGN .These include H - alpha , NII , SII , OIII , and CII . Using these line fluxes we determine the luminosity of the AGN to be 1 . 1 x 10 ^ 41 erg / sec .This value agrees very closely with that shown for other similar galaxies . We additionally find proof for outflows on both large and tiny scales around the AGN .",
        "rewrite_text": "We present new images of the low surface brightness galaxy PGC 045080, which is known to contain an active galactic nucleus (AGN). These images provide insight into the characteristics of this AGN and its relationship with the surrounding gas envelope. Previous surveys using Very Large Array (VLA) observations have detected the AGN at radio wavelengths. In our study, we analyzed both VLA archival data and recent observations from the Karl G. Jansky Very Large Array (JVLA) to identify emission lines linked to the AGN, including H-alpha, NII, SII, OIII, and CII. From the measured line fluxes, we calculated the AGN's luminosity to be 1.1 x 10^41 erg/sec, which is consistent with values observed in other similar galaxies. Additionally, we found evidence of outflows on both large and small scales surrounding the AGN.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": -0.4120816918460671
    },
    {
        "original_text": "We present an overview of the state-of-the-art methods for spectral analysis on spherical data, with emphasis on their applications to problems arising in geophysical sciences (e.g., global seismological tomography) and astrophysics (e.g., cosmic microwave background). We also discuss some recent advances in this area that have been made by our group at Columbia University. The main focus is on the development of new algorithms for computing accurate estimates of the power spectrum of signals defined over the surface of the unit sphere using only partial information about these signals. In particular, we consider two classes of methods: those based on the use of spherical harmonic expansions and those based on wavelet transforms. Finally, we briefly describe several open research questions related to the topic discussed here. Spherical data arise naturally in many areas of science including astronomy, meteorology, oceanography, geodesy, and medicine. For example, astronomers routinely collect large amounts of data describing the positions of celestial objects such as stars or galaxies; similarly, weather forecasters gather measurements of atmospheric pressure, temperature, humidity, wind speed, etc., at various locations around the globe. These types of data are often represented mathematically as functions defined over the surface of a sphere.",
        "watermark_text": "We bring an overview of the state - of - the - art methods for spectral modeling on spherical measurements , with emphasis on their applications to problems arose in geophysical sciences ( e . g . , global seismological tomography ) and astrophysics ( e . g . , planetary microwave background ) . We also discuss some latest advances in this area that have been made by our team at Columbia University .The main interest is on the development of new method for calculation precise estimates of the power spectrum of waves defined over the surface of the unit sphere utilizing only partial knowledge about these signals . In particular , we investigate two groups of methods : those relying on the using of spherical harmonic expansions and those based on wavelet transforms .Finally , we briefly illustrate numerous open scientific questions related to the subject mentioned here . Spherical data arise naturally in multiple fields of science including science , meteorology , oceanography , geodesy , and medicine .For instance , astronomers typically gather huge amounts of statistics explaining the places of astronomical bodies such as planets or galaxies ; likewise , weather forecasters gather measurements of atmospheric pressure , temperature , moisture , breeze pressure , etc . , at numerous locations around the globe . These sorts of statistics are often depicted mathematically as functions defined over the surface of a sphere .",
        "rewrite_text": "We present an overview of cutting-edge methods for spectral modeling of spherical measurements, focusing on their applications in geophysical sciences—such as global seismological tomography—and astrophysics, including studies of planetary microwave backgrounds. Additionally, we highlight recent advancements made by our team at Columbia University in this field. Our primary goal is to develop new techniques for accurately estimating the power spectrum of waves defined over the surface of a unit sphere, utilizing only partial information about these signals. Specifically, we explore two categories of methods: those that employ spherical harmonic expansions and those that utilize wavelet transforms. We also touch upon various unresolved scientific questions related to this topic. Spherical data naturally occurs in multiple scientific disciplines, including meteorology, oceanography, geodesy, and medicine. For example, astronomers collect extensive statistics to document the positions of celestial bodies such as planets and galaxies, while meteorologists compile measurements of atmospheric pressure, temperature, humidity, and wind speed from various locations worldwide. These types of data are often mathematically represented as functions defined on the surface of a sphere.",
        "ori-fast-z-score": -2.011435198964418,
        "water-fast-z-score": 6.674238124719146,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "We present an algorithm that systematically scans all possible 7-colourings of the grid, and report on its performance in terms of running time and memory consumption. The algorithm is based on a simple backtracking scheme combined with some heuristics to prune parts of the search space. We have implemented this algorithm using Java 1.6 and tested it on several instances ranging from small grids up to large ones containing more than one million nodes. For each instance we provide detailed information about how much time was spent by our program during colouring as well as how many colours were used. In addition, we also show how these results compare against those obtained by other algorithms proposed recently in the literature. \n \n Keywords: Coloring problems, Computational complexity theory, Graphs, Backtrack search, Heuristic methods, Grid graphs, Integer programming, Optimization problems, Search trees, Time-complexity analysis \n \n \n \n INTRODUCTION \n \n A graph G = (V, E) consists of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of edges between pairs of nodes. An edge e=(u,v) connects node u ∈ V to v ∈ V . If there exists no such connection then e is not included in E. A path P is defined as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , n . A cycle C is defined as a path whose first and last nodes are identical. A connected component is a subgraph H of G which has the property that any pair of nodes in H can be joined by a path within H but cannot be joined by paths outside H. A clique K is a complete subgraph of G; that is, every pair of nodes in K is adjacent to each other. A k-clique is a clique consisting of exactly k nodes. A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S. A dominating set D is a subset of V",
        "watermark_text": "We create an algorithm that systematically scans all possible 7 - colourings of the grid , and report on its effectiveness in terms of running time and memory usage . The algorithm is based on a simple backtracking scheme coupled with some heuristics to prune parts of the search space .We have applied this algorithm using Java 1 . 6 and demonstrated it on numerous instances ranging from small grids up to large ones featuring more than one million nodes . For each instance we provide comprehensive information about how many time was spent by our software during colouring as also as how many colours were used .In addition , we also demonstrate how these results compare against those achieved by other methods suggested late in the literature . Keywords : Coloring questions , Computational complexity analysis , Graphs , Backtrack search , Heuristic methods , Grid graphs , Integer programming , Optimization problems , Search trees , Time - complexity analysis INTRODUCTION A graph G = ( V , E ) contains of two sets V and E where V denotes the set of vertices or nodes and E denotes the set of vertices between pairs of vertices .An path e = ( v , v ) links node e ∈ V to v ∈ V . If there exists no such connection then e is not added in E . A path P is given as a sequence of distinct nodes v1 , v2 , … , vn such that vi−1vi ∈ E for i = 2 , 3 , … , n .A cycle C is characterized as a path whose first and last nodes are equal . A linked component is a subgraph H of G which has the feature that any pair of vertices in H can be joined by a path within H but cannot be joined by tracks outside H . A clique K is a complete subgraph of G ; that is , every pair of vertices in K is adjoining to each other .A k - clique is a clique consisting of exactly k nodes . A vertex cover S is a subset of V such that every edge in G has at least one endpoint in S . A dominating setting D is a subset of V",
        "rewrite_text": "We developed an algorithm that thoroughly examines all possible 7-colourings of a grid, assessing its performance in terms of execution time and memory usage. This approach employs a straightforward backtracking technique, enhanced with heuristics to eliminate unnecessary portions of the search space. Our implementation, carried out in Java 1.6, has been tested on a wide range of grid sizes, from small configurations to extensive ones with over one million nodes. For each case, we provide detailed data on the time our software spent on the colouring process and the number of colours utilized. Furthermore, we compare our findings with those from alternative methods discussed in recent literature. \n\n**Keywords:** Colouring problems, Computational complexity analysis, Graph theory, Backtrack search, Heuristic approaches, Grid graphs, Integer programming, Optimization challenges, Search trees, Time complexity assessment.\n\n**INTRODUCTION** A graph \\( G = ( V, E ) \\) consists of two sets: \\( V \\), representing the vertices or nodes, and \\( E \\), representing the edges that connect pairs of vertices. An edge \\( e = ( v_i, v_j ) \\) connects node \\( v_i \\in V \\) to \\( v_j \\in V \\). If no such connection exists, \\( e \\) is not included in \\( E \\). A path \\( P \\) is defined as a sequence of distinct nodes \\( v_1, v_2, \\ldots, v_n \\) where \\( v_{i-1} v_i \\in E \\) for \\( i = 2, 3, \\ldots, n \\). A cycle \\( C \\) is a path where the first and last nodes are identical. A connected component is a subgraph \\( H \\) of \\( G \\) in which any two vertices in \\( H \\) are connected by a path within \\( H \\), but there are no connections outside \\( H \\). A clique \\( K \\) is a complete subgraph of \\( G \\), meaning every pair of vertices in \\( K \\) is directly connected. A \\( k \\)-clique consists of precisely \\( k \\) nodes. A vertex cover \\( S \\) is a subset of \\( V \\) such that each edge in \\( G \\) has at least one endpoint in \\( S \\). A dominating set \\( D \\) is also a subset of \\( V \\).",
        "ori-fast-z-score": 0.8770580193070293,
        "water-fast-z-score": 7.367287362179046,
        "rewrite-fast-z-score": 1.0524696231684352
    },
    {
        "original_text": "We present an experimental study on the effect that different initial states have in a quantum experiment, using quantum process tomography (QPT). We prepare three different initial states and perform QPT to reconstruct their density matrices. The results show how the choice of initial state affects the outcome of the experiment. In particular we find that the fidelity between two initial states is not sufficient to determine whether they will give rise to similar or dissimilar outcomes when measured with respect to some observable. This work was supported by EPSRC grant EP/G061794/1. Introduction:-Quantum mechanics has been successfully applied to many fields such as information processing  1  , metrology  2  and sensing  3  . However, it remains unclear what exactly constitutes a  quantum  experiment  4  .\nIn this Letter we consider one aspect of quantum experiments -the role played by the initial state of the system under investigation. It is well known that the measurement statistics depend upon the initial state  5  but there are few studies which investigate how the choice of initial condition influences the final result  6  . Here we use quantum process tomography  7, 8  to examine the influence of the initial state on the outcome of a quantum experiment. Our aim is to understand better how the initial conditions may be chosen so as to maximise the probability of observing certain phenomena  9  .",
        "watermark_text": "We present an experimental test on the impact that different initial states have in a quantum experiment , using quantum method tomography ( QPT ) . We produce three different initial states and conduct QPT to reconstruct their density matrices .The results show how the chosen of initial state impacts the result of the experiment . In particular we find that the fidelity between two initial states is not sufficient to judge whether they will giving rise to similar or dissimilar effects when measured with regard to some observable .This project was supported by EPSRC award EP / G061794 / 1 . Introduction : - Quantum theory has been successfully application to many fields such as intelligence processing 1 , metrology 2 and perception 3 .However , it remains unsure what actually constitutes a quantum experiment 4 . In this Letter we mention one element of quantum experiments - the part played by the first state of the process under inquiry .It is well established that the observation statistics depend upon the initial state 5 but there are few researchers which probe how the selection of initial condition influences the finished result 6 . Here we using quantum process tomography 7 , 8 to examine the impact of the initial state on the result of a quantum experiment .Our aim is to study easier how the first parameters might be chosen so as to maximise the probability of experiencing certain observations 9 .",
        "rewrite_text": "We present an experimental investigation into how different initial states influence outcomes in a quantum experiment, utilizing quantum process tomography (QPT). We prepare three distinct initial states and apply QPT to reconstruct their density matrices. The findings illustrate the significance of the selected initial state on the experiment's results. Notably, we discover that the fidelity between two initial states is insufficient to determine whether they will produce similar or differing effects when measured concerning a specific observable. This research was supported by the EPSRC award EP/G061794/1. \n\nIntroduction: Quantum theory has found successful applications across various areas, including information processing, metrology, and perception. However, the precise definition of what constitutes a quantum experiment remains uncertain. In this letter, we address one aspect of quantum experiments: the role of the initial state in the process under investigation. It is well understood that the statistical outcomes of observations depend on the initial state, yet few researchers have explored how the selection of initial conditions affects the final results. Here, we employ quantum process tomography to analyze the impact of the initial state on a quantum experiment's outcomes. Our goal is to better understand how to select initial parameters to enhance the likelihood of obtaining specific observations.",
        "ori-fast-z-score": -1.7483145522430754,
        "water-fast-z-score": 6.799001036500849,
        "rewrite-fast-z-score": -1.1441551070947107
    },
    {
        "original_text": "We present multimodal nested sampling (MNS), a novel algorithm that is able to efficiently explore the posterior distribution in high-dimensional parameter spaces, such as those encountered when fitting complex models to observational data sets. MNS combines ideas from simulated annealing with importance sampling techniques to find the global maximum likelihood solution within a given tolerance level. We demonstrate how this method can be used on real-world problems by applying it to two different astrophysics applications: modelling the observed fluxes of gamma-ray bursts using a time-dependent model; and determining the parameters of a binary black hole merger event detected by gravitational waves. In both cases we show that our new approach outperforms existing Markov chain Monte Carlo algorithms. The code implementing these examples will be made publicly available at https://github.com/mns-method/mns-method/tree/master/examples. Multimodal nested sampling (M NS) is a novel algorithm that is capable of exploring the posterior distribution in high dimensional parameter spaces, such as are found when fitting complex models to large observational datasets. It combines ideas from simulated annealling with importance sampling techniques to locate the global maximum likelihood solution to any problem within some specified tolerance. This talk describes the basic principles behind M NS and demonstrates its application to two astrophysics problems: modelling the observed light curves of gamma ray bursts; and determining the physical properties of a binary black hole system inferred from gravitational wave observations.",
        "watermark_text": "We create multimodal nested sampling ( MNS ) , a new algorithm that is able to easily examine the posterior distribution in high - dimensional parameter sets , such as those experienced when fitting large models to observational data sets . MNS mixes ideas from simulated annealing with importance sampling methods to find the global maximum likelihood solution within a given tolerance level .We suggest how this algorithm can be used on real - global issues by using it to two different astrophysics applications : modelling the emitted fluxes of gamma - ray clusters using a time - dependent model ; and determining the variables of a binary white hole merger event detected by gravitational waves . In both cases we prove that our new approach outperforms previous Markov chain Monte Carlo algorithms .The software implementing these examples will be made fully available at https : / / github . com / mns - method / mns - method / tree / master / examples . Multimodal nested filtering ( M NS ) is a novel algorithm that is capable of analyzing the posterior distribution in high dimensional parameter sets , such as are found when fitting large models to large observational datasets .It combines insights from simulated annealling with importance sampling methods to locate the global maximum likelihood solution to any question within some specified tolerance . This discussion describes the fundamental principles behind M NS and demonstrates its use to two astrophysics issues : modelling the emitted light curves of gamma radiation bursts ; and determining the physical properties of a binary white hole system inferred from gravitational wave surveys .",
        "rewrite_text": "We present multimodal nested sampling (MNS), a new algorithm designed to effectively analyze posterior distributions in high-dimensional parameter spaces, which are common when fitting complex models to large observational datasets. MNS integrates concepts from simulated annealing and importance sampling to identify the global maximum likelihood solution within a specified tolerance. We illustrate the practical applications of this algorithm through two astrophysics cases: modeling the emitted fluxes of gamma-ray bursts with a time-dependent model and extracting the parameters of a binary white hole merger event identified through gravitational wave detection. In both scenarios, we demonstrate that our approach surpasses traditional Markov chain Monte Carlo algorithms. The software that implements these examples will be freely accessible at https://github.com/mns-method/mns-method/tree/master/examples. The principles underlying MNS are discussed, highlighting its effectiveness in tackling two significant astrophysical challenges: modeling the light curves of gamma-ray bursts and deriving the properties of a binary white hole system based on gravitational wave observations.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 8.348430768491543,
        "rewrite-fast-z-score": 3.3362306249131963
    },
    {
        "original_text": "The WiFeS instrument is an integral field spectrograph for the Australian National University 2.3m telescope at Siding Spring Observatory, Australia.  It has been in operation since 2005 and was designed to provide high quality optical spectroscopy over a wide range of wavelengths with minimal overheads.   The WiFeS instrument consists of two cameras that are mounted on a common baseplate which sits inside a vacuum chamber attached to the Cassegrain focus of the ANU 2.3m telescope.    Each camera contains a lenslet array that produces a set of images across its focal plane.  These images can be combined into a single data cube using software developed by Dopita et al. (2007).    This data cube provides information about both spatial position and wavelength along each line-of-sight through the object being observed.  In addition to this capability, WiFeS also offers several other advantages including:  - High throughput due to the use of dichroic beam splitters and fibre optic coupling between the lenses and detectors.  - Flexible observing modes ranging from fully automated observations to manual control via remote desktop interface.  - Fast readout times allowing multiple targets to be observed simultaneously or rapid cycling between different targets during a night s observation run.  - Low maintenance requirements as it does not require any moving parts within the vacuum enclosure.  - Excellent stability thanks to the design of the optics bench and the thermal isolation provided by the vacuum enclosure.  - Large format detector arrays providing excellent signal-to-noise ratio even under poor seeing conditions.  - Robust mechanical design ensuring long-term reliability...",
        "watermark_text": "The WiFeS instrument is an integral field spectrograph for the Australian National University 2 . 3m telescope at Siding Spring Observatory , Australia . It has been in operation since 2005 and was built to provide high quality optical spectroscopy over a broad variety of wavelengths with minimal overheads .The WiFeS instrument consists of two cameras that are installed on a common baseplate which sits inside a vacuum chamber connected to the Cassegrain scope of the ANU 2 . 3m telescope . Each lens includes a lenslet antenna that generates a pair of pictures across its focal surface .These photographs can be merged into a single data cube using software created by Dopita et al . ( 2007 ) .This data cube provides information about both visual orientation and wavelength along each line - of - view through the element being detected . In addition to this capability , WiFeS also provides various other advantages namely : - High throughput owing to the using of dichroic laser splitters and fibre optic coupling between the lenses and detectors .- Flexible monitoring mechanisms ranging from fully autonomous observations to manual control via remote desktop interface . - Fast readout hours permitting multiple targets to be spotted concurrently or rapid cycling between various destinations during a night s observation run .- Low maintenance requirements as it does not require any shifting components within the vacuum enclosure . - Good integrity thanks to the design of the optics bar and the thermal isolation provided by the vacuum enclosure .- Large format detector arrays offering good signal - to - noise ratio especially under bad seeing conditions . - Robust mechanical architecture ensuring long - term reliability . . .",
        "rewrite_text": "The WiFeS instrument, an integral field spectrograph, is installed on the Australian National University’s 2.3m telescope at Siding Spring Observatory, Australia. In operation since 2005, it was designed to deliver high-quality optical spectroscopy across a wide range of wavelengths while minimizing overhead. WiFeS features two cameras mounted on a common baseplate within a vacuum chamber that connects to the Cassegrain focus of the ANU 2.3m telescope. Each lens includes a lenslet array that captures dual images over its focal surface, which can be combined into a single data cube using software developed by Dopita et al. (2007). This data cube offers insights into both visual orientation and wavelength for each line of sight through the observed object. Additionally, WiFeS provides several benefits: \n- High throughput achieved through the use of dichroic laser splitters and fiber optic coupling between lenses and detectors.\n- Flexible monitoring options, ranging from fully autonomous observations to manual control via remote desktop.\n- Rapid readout capabilities that allow for simultaneous observation of multiple targets or quick cycling among various objects during a night’s observations.\n- Low maintenance needs, as there are no moving parts within the vacuum enclosure.\n- Structural integrity due to the optical design and thermal isolation provided by the vacuum.\n- Large-format detector arrays that ensure a good signal-to-noise ratio, even in poor seeing conditions.\n- A robust mechanical design that guarantees long-term reliability.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 7.0101164243872995,
        "rewrite-fast-z-score": 1.2888044650576527
    },
    {
        "original_text": "The authors have used scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8+d (Bi-2212). They find that, at low temperatures, this material forms pairs of holes which are bound together by an attractive interaction mediated by phonons. The binding energy is found to be about 0.5 eV per hole pair. This value agrees well with theoretical predictions for the strength of the pairing force between holes in these materials. \n \n In addition, they observe that the density of states near the Fermi level shows a strong dependence on the direction along which the crystal is cut. For example, when the crystal is cleaved parallel to its Cu-O planes, it exhibits a large peak in the density of states just below the Fermi level. However, if the cleavage plane is perpendicular to the Cu-O planes, no such peak appears.",
        "watermark_text": "The authors have utilized scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high - temperature cuprate superconductor Bi2Sr2CaCu2O8 + d ( Bi - 2212 ) . They find that , at low temperatures , this metal creates sets of holes which are bound together by an attractive interaction mediated by phonons .The interaction power is found to be about 0 . 5 eV per hole pair . This value agrees well with theoretical expectations for the strength of the pairing force between holes in these structures .In addition , they demonstrate that the density of states near the Fermi level exhibits a large dependence on the direction along which the crystal is cutting . For instance , when the crystal is cleaved parallel to its Cu - O planes , it displays a large peak in the density of states just below the Fermi level .However , if the cleavage plane is perpendicular to the Cu - O axes , no such peak appears .",
        "rewrite_text": "The authors employed scanning tunneling microscopy to investigate the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8 + d (Bi-2212). They discovered that at low temperatures, this metal forms pairs of holes that are attracted to each other through phonon-mediated interactions, with an interaction strength of approximately 0.5 eV per hole pair. This finding aligns well with theoretical predictions concerning the pairing force's intensity between holes in these materials. Furthermore, they showed that the density of states near the Fermi level varies significantly depending on the direction of the crystal cleavage. Specifically, when the crystal is sliced parallel to its Cu-O planes, a prominent peak in the density of states appears just below the Fermi level. In contrast, no such peak is observed when the cleavage is made perpendicular to the Cu-O axes.",
        "ori-fast-z-score": 2.75,
        "water-fast-z-score": 6.4007575309253015,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "We present here an overview of our survey, which is aimed at tracing the evolution in the number density and luminosity function (LF) of galaxies as a function of their stellar masses up to z ~ 1.5. The sample consists of about 10 000 objects selected by photometric redshifts using deep optical data obtained with FORS2@VLT+WFI@ESO/MPG 2.2m telescope + Spitzer IRAC 3.6 & 4.5 micron bands. We use this dataset to study how the LF evolves for different ranges of stellar masses. In particular we find that the faint-end slope of the LF becomes steeper towards higher redshift indicating that less massive systems are more abundant than today. This result can be explained if star formation activity was more efficient in low-mass halos at high-z compared to local universe. Finally, we compare these results with those derived from other surveys finding good agreement between them.",
        "watermark_text": "We present here an overview of our survey , which is aiming at studying the evolution in the number density and luminosity function ( LF ) of galaxies as a function of their stellar masses up to z ~ 1 . 5 . The sample consists of about 10 000 items selected by photometric redshifts using deep optical data acquired with FORS2 @ VLT + WFI @ ESO / MPG 2 . 2m telescope + Spitzer IRAC 3 . 6 & 4 . 5 micron bands .We use this dataset to study how the LF evolves for different ranges of stars masses . In particular we find that the faint - end slope of the LF becomes steeper towards higher redshift indicating that less massive components are more rich than presently .This result can be described if star formation activity was more efficient in low - density halos at high - z compared to nearby universe . Finally , we compare these results with those generated from other surveys obtaining strong approval between them .",
        "rewrite_text": "In this overview, we outline our survey, which investigates the evolution of galaxy number density and luminosity function (LF) in relation to stellar mass up to redshift z ~ 1.5. Our sample includes approximately 10,000 galaxies selected using photometric redshifts derived from deep optical observations collected with FORS2 at the VLT, the WFI at the ESO/MPG 2.2m telescope, and Spitzer's IRAC at 3.6 and 4.5 microns. We analyze this dataset to understand how the LF changes across various stellar mass ranges. Notably, we observe that the faint-end slope of the LF becomes steeper at higher redshifts, suggesting that less massive galaxies are more prevalent than in the present universe. This finding may indicate more efficient star formation activity in low-density halos at high redshift compared to our local universe. Finally, we compare our results with those from other surveys, finding strong agreement among them.",
        "ori-fast-z-score": 2.4618298195866544,
        "water-fast-z-score": 6.154574548966636,
        "rewrite-fast-z-score": 0.8819171036881969
    },
    {
        "original_text": "The self-duality condition on the curvature tensor is an important ingredient in supergravity theories, but it has been difficult to incorporate into superspace formulations because of its non-Lagrangian nature.  In this work we show how to construct Lagrangians for self-dual supergravities by using twistor theory as our guide.   We first review the basic ideas behind twistor theory and then use these results to develop new techniques that allow us to write down manifestly supersymmetric actions for self-dual supergravitational fields with arbitrary gauge groups.  The resulting action can be written either in terms of chiral or twisted-chiral superfields depending upon whether one uses the light-cone or covariant approach respectively.   Finally, we discuss some applications of these results including the construction of N = 1, D = 4 supergravity coupled to Yang-Mills multiplets. This article is available from: http://arxiv.org/abs/hep-th/0405033",
        "watermark_text": "The self - duality condition on the curvature tensor is an important ingredient in supergravity models , but it has been difficult to insert into superspace formulations because of its non - Lagrangian existence . In this research we explain how to build Lagrangians for self - dual supergravities by using twistor theory as our guide .We first review the fundamental ideas behind twistor theory and then use these results to develop new strategies that enable us to write down manifestly supersymmetric movements for self - dual supergravitational fields with arbitrary gauge groups . The resulting action can be written either in terms of chiral or twisted - chiral superfields depending upon whether one uses the light - cone or covariant approach respectively .Finally , we explain some applications of these results namely the creation of N = 1 , D = 4 supergravity combined to Yang - Mills multiplets . This section is accessible from : www : / / arxiv . org / abs / hep - th / 0405033",
        "rewrite_text": "The self-duality condition on the curvature tensor is a crucial element in supergravity models; however, incorporating it into superspace formulations has proven challenging due to its non-Lagrangian nature. In this study, we demonstrate how to construct Lagrangians for self-dual supergravities by leveraging twistor theory. We begin by reviewing the key concepts of twistor theory, then employ these insights to devise new methods that allow us to formulate manifestly supersymmetric actions for self-dual supergravitational fields across various gauge groups. The resulting action can be expressed in terms of either chiral or twisted-chiral superfields, depending on whether one adopts a light-cone or covariant approach, respectively. Lastly, we discuss applications of these findings, particularly the formulation of N = 1, D = 4 supergravity in conjunction with Yang-Mills multiplets. For further details, please visit: www:/ / arxiv.org/abs/hep-th/0405033.",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": 0.5252257314388902
    },
    {
        "original_text": "We propose to use the time evolution of cosmological redshifts in order to probe the nature of dark energy, which is one of the most important problems in modern physics and astronomy. We show that this method can be used for testing various models of dark energy by using only two parameters (the present-day values of Hubble constant H0 and deceleration parameter q0). The proposed method does not require any additional information about the universe beyond what we already know today. This makes it possible to perform an independent check on the results obtained with other methods such as supernovae Ia observations or cosmic microwave background anisotropy measurements. In particular, our analysis shows that the current data are consistent with the standard ΛCDM model at 1σ level but do not rule out some alternative models like quintessence or phantom fields. Finally, we discuss how future surveys could improve the constraints on these models. Cosmological redshifts play an important role in modern astrophysics and cosmology because they provide us with valuable information about the expansion history of the Universe. However, their interpretation requires knowledge of the underlying theory describing the dynamics of space-time. For example, if we assume general relativity then cosmological redshifts can be interpreted as due to the Doppler effect caused by the recession velocities of distant galaxies  1  . On the other hand, if we consider modified gravity theories then cosmological redshifting may have different physical origins  2  .\nIn recent years there has been growing interest in studying the possibility of probing the nature of dark energy through its effects on cosmological redshifts  3  -  8  . Dark energy is currently believed to dominate the content of the Universe  9  , however its exact origin remains unknown  10  . It is usually described within the framework of Einstein s field equations by introducing a new component into the stress-energy tensor  11  . Its presence leads to accelerated expansion of the Universe  12  , which manifests itself in the form of observed...",
        "watermark_text": "We suggest to use the period evolution of cosmological redshifts in order to probe the nature of dark energy , which is one of the most important problems in modern physics and astronomy . We see that this technology can be used for studying several models of bright energy by using only two parameters ( the present - day parameters of Hubble constant H0 and deceleration parameter q0 ) .The proposed approach does not require any additional information about the universe beyond what we already understand today . This lets it convenient to conduct an independent check on the results derived with other methods such as supernovae Ia detection or cosmic microwave background anisotropy observations .In particular , our analysis shows that the present data are compatible with the standard ΛCDM theory at 1σ level but do not leave out some additional models like quintessence or phantom fields . Finally , we explain how potential study could enhance the restrictions on these models .Cosmological redshifts play an important role in modern astrophysics and cosmology because they give us with important information about the expansion history of the Universe . However , their explanation requires knowledge of the fundamental theory explaining the dynamics of space - time .For instance , if we suppose general relativity then cosmological redshifts can be interpreted as owing to the Doppler impact caused by the recession velocities of distant galaxies 1 . On the other hand , if we treat modified gravity theories then cosmological redshifting might have different physical origins 2 .In recent years there has been growing interest in investigating the prospect of probing the nature of dark energy through its consequences on cosmological redshifts 3 - 8 . Dark energy is currently suspected to dominate the content of the Universe 9 , however its exact origin stays unclear 10 .It is usually characterized within the framework of Einstein s field equations by bringing a new constituent into the strain - energy tensor 11 . Its presence contributes to accelerated expansion of the Universe 12 , which manifests itself in the form of observed . . .",
        "rewrite_text": "We propose utilizing the evolution of cosmological redshifts over time to investigate the characteristics of dark energy, a crucial issue in contemporary physics and astronomy. Our analysis demonstrates that this method can effectively assess several dark energy models by relying on just two parameters: the current values of the Hubble constant \\( H_0 \\) and the deceleration parameter \\( q_0 \\). Notably, our approach does not demand any additional information about the universe beyond what is currently known, making it a practical option for independently verifying findings from other techniques, such as observations of Type Ia supernovae or cosmic microwave background anisotropies. Our results indicate that the existing data are consistent with the standard ΛCDM model at a 1σ confidence level, while also accommodating alternative models, including quintessence and phantom fields. We further discuss how future studies could refine constraints on these models.\n\nCosmological redshifts are vital in modern astrophysics and cosmology, providing key insights into the universe's expansion history. Their interpretation, however, requires an understanding of the fundamental theory governing space-time dynamics. For example, under general relativity, cosmological redshifts can be attributed to the Doppler effect resulting from the recession velocities of distant galaxies. Conversely, in the context of modified gravity theories, cosmological redshifting may have different physical origins. Recently, there has been an increasing interest in exploring the potential of using cosmological redshifts to probe the nature of dark energy. Currently believed to dominate the universe's makeup, the exact origin of dark energy remains unclear. It is typically characterized within Einstein's field equations by introducing a new component into the stress-energy tensor. This component contributes to the universe's accelerated expansion, which is observable in the ongoing expansion trends.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 8.495296818075921,
        "rewrite-fast-z-score": 0.9330078226479681
    },
    {
        "original_text": "We propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today  1  . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system  2  .\nIn recent years, however, new experimental techniques have been developed  3, 4  , allowing us to trap and manipulate cold atoms on an unprecedented level  5  . These developments make it possible to study many-body phenomena  6  like superfluidity  7, 8  and Bose-Einstein condensation  9  in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions  10  . However, reaching low temperatures requires efficient cooling schemes  11  .\nOne promising approach towards achieving this goal is to use the pairing mechanism  12  . Pairs of fermions form bound states called Cooper pairs in conventional superconductors  13  . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds  14  . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired  15  . Therefore, cooling fermions via pairing should work well even",
        "watermark_text": "We suggest to cool fermionic atoms in laser lattices by using the pairing principle between two different hyperfine states , which is analogous to Cooper couple formation in superconductors . We see that this process can be used for both bosonic and fermionic systems with interesting interactions .The proposed system has numerous benefits over other methods such as evaporative cooling or sympathetic heating . It does not require any additional laser electrons besides those required for trapping and manipulating cool ions .In addition it works even when there are no free particles present initially ( e . g . , at zero temperature ) . Finally we talk how our proposal possible be realized experimentally .Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging difficulties in nuclear science today 1 . This problem remains particularly challenging if the first number density of fermions is high because then elastic collisions cannot eliminate much energy from the system 2 .In recent years , however , new experimental methods have been created 3 , 4 , allowing us to trapping and manipulate cold molecules on an remarkable level 5 . These advances give it able to study many - bodies phenomena 6 like superfluidity 7 , 8 and Bose - Einstein condensation 9 in ultracold atomic atoms .One important aim in these experiments is to reach quantum degenerate regimes where the gas consists of highly interacting fermions 10 . However , finding low temperatures involves optimal cooling schemes 11 .One promising alternative towards reaching this goal is to use the pairing principle 12 . Pairs of fermions form bound states called Cooper pairs in standard superconductors 13 .Analogously , pairs of fermions may sometimes form bound states in ultracold nuclear clouds 14 . If the interaction strength between fermions is sufficiently great , they will preferentially bind into couples rather than keeping unpaired 15 .Therefore , cooling fermions via pairing should work better even",
        "rewrite_text": "We propose a method to cool fermionic atoms in laser lattices by leveraging the pairing principle between two different hyperfine states, akin to the formation of Cooper pairs in superconductors. This technique is applicable to both bosonic and fermionic systems featuring intriguing interactions. Our proposed approach offers several advantages over traditional methods like evaporative cooling or sympathetic heating, as it requires no additional laser electrons beyond those necessary for trapping and manipulating cool ions. Importantly, it remains effective even in the absence of free particles initially present (e.g., at zero temperature). Additionally, we explore the experimental feasibility of our proposal. Achieving quantum degeneracy temperatures below 1 microkelvin for fermions poses one of the most significant challenges in nuclear science today. This challenge is exacerbated at high fermionic number densities, where elastic collisions struggle to dissipate sufficient energy from the system. However, recent advancements in experimental techniques have enabled the trapping and manipulation of cold molecules at an impressive level, facilitating the study of many-body phenomena such as superfluidity and Bose-Einstein condensation in ultracold atomic gases. A key objective in these experiments is to achieve quantum degenerate states characterized by highly interacting fermions. To reach such low temperatures, optimized cooling protocols are essential. One promising avenue toward this goal is the application of the pairing principle, where pairs of fermions form bound states known as Cooper pairs, similar to what occurs in conventional superconductors. Analogously, in ultracold nuclear clouds, fermions may also form bound states under sufficiently strong interactions, leading to a preference for pairing rather than remaining unpaired. Therefore, utilizing fermionic pair cooling could prove to be a more effective strategy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.723229626397817,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "We revisit the foundations of quantum mechanics by introducing an alternative formulation to the standard Feynman path integral approach, which is based on the concept of dynamical phase transition (DPT). We show that this new formalism provides a natural description for the emergence and evolution of macroscopic order in open quantum systems. In particular we demonstrate how it can be used to describe the spontaneous emission process in atomic physics, where the atom-field interaction leads to the formation of collective states with well-defined photon number statistics. The proposed framework also allows us to study the dynamics of many-body interacting systems beyond mean field theory. Finally, we discuss possible applications of our results to condensed matter physics and quantum information science. Introduction:-The development of modern theoretical approaches has led to significant progress in understanding the physical properties of complex quantum systems  1  . However, despite these advances there are still fundamental questions about the nature of quantum phenomena that remain unanswered  2  .\nIn recent years, several authors have attempted to address some of these issues using concepts borrowed from statistical mechanics  3  , such as entropy  4  or free energy  5  . These ideas were originally developed within the context of classical thermodynamics  6  but they have been recently extended to the realm of quantum mechanics  7, 8  . For example, one may consider the von Neumann entropy S = −Tr(ρ ln ρ) associated with the density matrix ρ describing the state of a system  9  . This quantity measures the amount of uncertainty present in the measurement outcomes  10  and its time derivative dS/dt gives rise to the so-called entropy production rate  11  . It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems  12  . More specifically, if the entropy production rate vanishes then the corresponding quantum mechanical model exhibits reversible dynamics  13  . On the other hand, when the entropy production rate becomes positive the system undergoes a non-equilibrium phase transition  14  .",
        "watermark_text": "We revisit the foundations of quantum mechanics by offering an additional formulation to the standard Feynman path integral approach , which is based on the notion of dynamical phase change ( DPT ) . We suggest that this new formalism gives a natural explanation for the emergence and evolution of macroscopic order in open quantum systems .In particular we prove how it can be used to explain the spontaneous emission mechanism in nuclear physics , where the atom - field interaction results to the formation of collective states with good - defined photon number statistics . The proposed framework specifically allows us to study the dynamics of several - bodies interacting systems beyond mean field theory .Finally , we explain possible applied of our findings to condensed matter science and quantum information physics . Introduction : - The advance of modern conceptual approaches has led to significant progress in understanding the physical properties of complex quantum systems 1 .However , despite these developments there are still significant questions about the nature of quantum effects that continue unanswered 2 . In recent months , various published have tried to tackle some of these problems using concepts borrowed from statistical mechanics 3 , such as entropy 4 or free energy 5 .These concepts were formerly advanced within the context of classical thermodynamics 6 but they have been lately extended to the domain of quantum mechanics 7 , 8 . For instance , one may see the von Neumann entropy S = −Tr ( ρ ln ρ ) associated with the density function ρ describing the state of a system 9 .This value measures the extent of uncertainty found in the measurement processes 10 and its time derivative dS / dt gives rise to the so - called entropy production probability 11 . It was shown that this latter quantity plays a crucial role in characterizing the irreversible behavior of closed quantum systems 12 .More specifically , if the entropy production level vanishes then the associated quantum mechanical model shows reversible dynamics 13 . On the other hand , when the entropy production level gets positive the system undergoes a non - equilibrium phase change 14 .",
        "rewrite_text": "We reexamine the fundamental principles of quantum mechanics by introducing a new formulation that complements the standard Feynman path integral approach, which is centered on the concept of dynamical phase transition (DPT). This novel framework provides a natural explanation for the emergence and evolution of macroscopic order in open quantum systems. Specifically, we demonstrate its applicability to the spontaneous emission mechanism in nuclear physics, where the interaction between atoms and fields leads to the creation of collective states characterized by well-defined photon number statistics. Our proposed approach offers the ability to investigate the dynamics of many-body interacting systems beyond mean field theory. Additionally, we discuss potential applications of our findings in the fields of condensed matter science and quantum information physics.\n\nIntroduction: The evolution of modern conceptual frameworks has significantly enhanced our understanding of the physical properties of complex quantum systems. However, despite recent advances, crucial questions regarding the nature of quantum effects remain unresolved. Recent publications have sought to address these issues by incorporating concepts from statistical mechanics, such as entropy and free energy. Originally rooted in classical thermodynamics, these ideas have recently been applied within the context of quantum mechanics. For example, the von Neumann entropy, defined as S = −Tr(ρ ln ρ) for the density operator ρ representing a system's state, quantifies the uncertainty associated with measurement processes. The time derivative of this entropy, dS/dt, yields the so-called entropy production rate, which is essential for characterizing the irreversible behaviors of closed quantum systems. Notably, a zero level of entropy production indicates reversible dynamics, while a positive entropy production level signifies a non-equilibrium phase transition.",
        "ori-fast-z-score": 1.5523010514126656,
        "water-fast-z-score": 9.615384615384615,
        "rewrite-fast-z-score": 3.958114029012639
    },
    {
        "original_text": "We present an anatomical study on the structure and evolution of malicious singularities in complex networks, which are defined as nodes that have a disproportionately large number of connections to other nodes. We show how these nodes can be identified by their topological properties using statistical mechanics methods. In particular we find that the degree distribution of such nodes follows a power law with exponent 2.5 ± 0.1 for all studied real-world networks ranging from social networks like Facebook or Twitter to biological networks like metabolic pathways or protein-protein interaction networks. This finding is consistent across different network sizes and types. The results presented here provide new insights into the organization principles of complex systems and may help to identify key players within them. Complex networks play a crucial role in many fields including physics, biology, sociology, computer science, engineering, economics, etc., where they represent interactions between entities  1  . A common feature of most complex networks is the presence of so-called  hubs  -highly connected nodes  2  , whose removal often has dramatic effects  3  .\nIn this work we focus on identifying hubs in complex networks based solely on their topological features. To do so, we use statistical mechanics techniques  4  to analyze the degree distributions of several real world networks  5  . Our analysis reveals that the degree distribution of hubs follows a power-law  6  with exponent 2.5±0.1 independently of the size and type of the considered network (see Fig.  1 ). Interestingly, our findings are also valid when considering only the largest component of each network  7, 8  . These results suggest that the observed scaling behavior is not due to finite-size effects but rather reflects some fundamental property of complex networks  9  .",
        "watermark_text": "We present an anatomical research on the composition and evolution of malicious singularities in complex networks , which are specified as nodes that have a disproportionately huge amount of links to other connections . We see how these layers can be identified by their topological features using statistical mechanics principles .In particular we find that the degree distribution of such nodes follows a power law with exponent 2 . 5 ± 0 . 1 for all observed real - time connections including from social organizations like Facebook or Twitter to biological organizations like genetic pathways or protein - gene interaction systems . This found is consistent across different network lengths and types .The results presented here provide fresh insights into the organization characteristics of complex systems and may assist to identify key players within them . Complex networks take a crucial role in many fields including science , biology , sociology , computer science , engineering , economics , etc . , where they represent interactions between entities 1 .A characteristic characteristic of most large systems is the presence of so - called hubs - highly wired nodes 2 , whose disappearance often has dramatic effects 3 . In this research we focus on establishing hubs in complex networks based primarily on their topological features .To do so , we utilize statistical mechanics principles 4 to analyze the degree distributions of several real world networks 5 . Our study reveals that the degree distribution of hubs follows a power - law 6 with exponent 2 . 5±0 . 1 independently of the height and kind of the considered network ( see Fig .1 ) . Interestingly , our findings are also legitimate when examining only the greatest element of each network 7 , 8 .These data suggest that the seen scaling behavior is not due to finite - length effects but rather displays some fundamental property of complex networks 9 .",
        "rewrite_text": "We conduct an anatomical study on the composition and evolution of harmful singularities within complex networks, defined as nodes that exhibit an unusually high number of connections. Our research demonstrates how these singularities can be identified based on their topological characteristics, employing principles from statistical mechanics. Notably, we observe that the degree distribution of these nodes adheres to a power law with an exponent of 2.5 ± 0.1 across all types of real-time connections, ranging from social networks like Facebook and Twitter to biological networks such as genetic pathways and protein-gene interaction systems. This finding holds consistently across various network sizes and types. The insights gained from this study enhance our understanding of the structural features of complex systems and may help pinpoint crucial nodes within those systems. Complex networks are pivotal in diverse fields, including science, biology, sociology, computer science, engineering, and economics, as they illustrate the interactions between entities. A defining feature of many large systems is the presence of hubs—highly connected nodes—whose removal can lead to significant consequences. Our research aims to identify these hubs in complex networks predominantly through their topological attributes. We apply statistical mechanics principles to analyze the degree distributions of various real-world networks. Our findings indicate that the degree distribution of these hubs conforms to a power law with an exponent of 2.5 ± 0.1, regardless of the scale or type of network studied (see Fig. 1). Intriguingly, our results remain valid even when focusing only on the largest node within each network. These observations imply that the scaling behavior we identified is not a result of finite-size effects, but rather reflects a fundamental property inherent to complex networks.",
        "ori-fast-z-score": -0.1643989873053573,
        "water-fast-z-score": 7.726752403351793,
        "rewrite-fast-z-score": 1.3926942648823688
    },
    {
        "original_text": "We study cascades on large blog graphs, where each node is an individual blogger and edges represent links between blogs. We propose a novel cascade model that captures the fact that people are more likely to read posts by their friends than random posts. Our main contributions are: (1) we develop efficient algorithms for computing the cascade size distribution under our model; (2) we show how to use these results to estimate the number of active users at any given time during a cascade; (3) we demonstrate the effectiveness of our approach using data collected from LiveJournal.com. The Web has become one of the most important communication channels today. In particular, social networks such as Facebook or Twitter have attracted millions of users who share information with others through online messages known as tweets or status updates. These messages can be seen by all followers of the user posting them, which may cause further propagation of the message within the network. This phenomenon is called viral marketing  1  , and it has been studied extensively over recent years  2  . However, despite its importance, there still remain many open questions about the dynamics of this process  3  .\nIn this work, we focus on studying cascades on large blogging communities, where each node represents an individual blogger and edges connect pairs of blogs written by the same person  4  . A cascade starts when some blogger writes a post containing a URL pointing to another blog s page. Then, if her readers click on the link, they will visit the other blog and possibly continue reading additional posts. As shown in Figure 1 , the resulting graph contains several connected components representing different topics discussed by the community members.",
        "watermark_text": "We research cascades on huge website graphs , where each node is an individual blogger and edges represent connections between blogs . We suggest a novel cascade model that captures the fact that individuals are more likely to see posts by their colleagues than random posts .Our main contributions are : ( 1 ) we develop fast algorithms for modeling the cascade size distribution under our model ; ( 2 ) we study how to use these results to estimate the total of active participants at any given time during a cascade ; ( 3 ) we prove the ability of our approach using data taken from LiveJournal . com . The Web has become one of the most important communication platforms today .In particular , social sites such as Facebook or Twitter have garnered millions of people who share data with others through online emails known as tweets or status updates . These messages can be saw by all supporters of the user sending them , which would cause further transmission of the message within the channel .This phenomenon is dubbed viral marketing 1 , and it has been studied thoroughly over recent years 2 . However , despite its significance , there still continue several open questions about the dynamics of this process 3 .In this research , we focus on studying cascades on huge blogging environments , where each node symbol an individual blogger and edges connect sets of blogs written by the same people 4 . A cascade starts when some blogger writes a post containing a URL pointing to another blog s page .Then , if her viewers flip on the link , they will access the other blog and maybe continue reading additional posts . As seen in Figure 1 , the resulting graph contains multiple connected elements representing different issues discussed by the community members .",
        "rewrite_text": "We investigate cascades within extensive website graphs, where each node represents an individual blogger and the edges denote connections between their blogs. We propose an innovative cascade model that reflects the tendency for individuals to view posts from their peers more often than those from arbitrary sources. Our key contributions include: (1) the development of efficient algorithms for modeling the cascade size distribution based on our model; (2) an exploration of how to leverage these findings to estimate the total number of active participants at any point during a cascade; and (3) validation of our approach using data from LiveJournal.com. The internet has emerged as a crucial communication platform, especially with social networks like Facebook and Twitter, which have attracted millions who share information via online updates, such as tweets or status posts. These messages can be viewed by all of a user's followers, potentially leading to further dissemination within the network. This viral marketing phenomenon has been extensively examined in recent years. Nonetheless, several open questions regarding the dynamics of this process remain. In this study, we concentrate on analyzing cascades within large blogging environments, where each node represents a blogger and edges link collections of blogs authored by the same individuals. A cascade initiates when a blogger publishes a post that includes a URL directing readers to another blog's page. If their audience clicks on the link, they access the other blog and may continue exploring more posts. As illustrated in Figure 1, the resulting graph comprises numerous interconnected elements that represent various topics discussed by community members.",
        "ori-fast-z-score": 0.42717882885838043,
        "water-fast-z-score": 8.799883874482637,
        "rewrite-fast-z-score": 1.975658322294524
    },
    {
        "original_text": "We present the first dual field theory in emergent spacetime, which is derived from a unifying field theory in higher dimensional spacetime. We show that this new dual field theory can be used to describe both quantum and classical physics with one single unified description. This new dual field theory has several advantages over other existing theories such as string/M-theory or loop quantum gravity. First, it provides an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic scale down to macroscopic scale. Second, unlike string/M-theory or LQG, our new dual field theory does not require any extra dimensions beyond those already observed experimentally. Third, we provide a concrete example showing how our new dual field theory works by deriving Einstein s general relativity from our new dual field theory. Finally, we also derive Maxwell s equations from our new dual field... \nIntroduction:-In recent years there have been many attempts to develop a fundamental theory of everything(TOE). String/M-theory  1  , Loop Quantum Gravity  2  are two examples of these efforts. However, despite their successes they still suffer from some problems. For instance, string/M-theory requires extra dimensions  3  while loop quantum gravity suffers from non-renormalizability  4  . These difficulties motivate us to look for alternative approaches towards developing TOEs. Recently, a novel approach called  emergent spacetime  was proposed  5, 6  . According to this approach, space-time emerges from a more fundamental level  7, 8  .\nEmergent spacetime:-The idea behind emergent spacetime is very simple. It states that space-time is not fundamental but rather emerges from a more fundamental entity. To see why this might happen consider the following argument. Imagine you are sitting on your couch watching TV. You will probably say that the world around you looks flat because if you were standing up then you would notice that the ground below you is curved. Now imagine yourself floating above Earth. If you were standing up now then you wouldn t feel like you re standing on a curved surface anymore. Instead you d feel like you re standing on top of a",
        "watermark_text": "We introduce the first dual field model in emergent spacetime , which is developed from a unifying field model in larger dimensional spacetime . We see that this new dual field model can be used to explain both quantum and classical physics with one single unified description .This new dual field model has numerous benefits over other existing models such as string / M - theory or loop quantum gravitational . First , it gives an explicit mathematical formulation for describing physical phenomena at all scales ranging from microscopic range down to macroscopic scale .Second , unlike string / M - theory or LQG , our new dual field theory does not require any additional dimensions beyond those already detected experimentally . Third , we provide a clear example showing how our new dual field theory works by deriving Einstein s general relativity from our new dual field theory .Finally , we also generate Maxwell s coefficients from our new dual field . . . Introduction : - In recent history there have been many efforts to develop a basic theory of things ( TOE ) . String / M - theory 1 , Loop Quantum Gravity 2 are two examples of these attempts .However , despite their successes they still suffer from some problems . For instance , string / M - theory requires added dimensions 3 while loop quantum gravitational suffers from non - renormalizability 4 .These difficulties motivate us to search for alternative approaches towards developing TOEs . Recently , a new approach called emergent spacetime was suggested 5 , 6 .According to this methodology , space - time arises from a more fundamental level 7 , 8 . Emergent spacetime : - The idea behind emergent spacetime is very simple .It says that space - time is not essential but rather emerges from a more fundamental entity . To see why this might happen think the following argument .Imagine you are sat on your couch watching TV . You will probably say that the world around you looks flat because if you were standing up then you might see that the earth below you is curved .Now imagine yourself rising above Earth . If you were standing up now then you wouldn t felt like you re standing on a curved surface anymore .Instead you d feel like you re standing on top of a",
        "rewrite_text": "We present the inaugural dual field model set in emergent spacetime, which is derived from a comprehensive field model operating within a higher-dimensional framework. This innovative dual field model provides a cohesive explanation for both quantum and classical physics under a singular unified description. It offers several advantages over existing models, such as string theory, M-theory, and loop quantum gravity. Firstly, it delivers a precise mathematical framework that effectively describes physical phenomena across all scales, from the microscopic to the macroscopic. Secondly, our dual field theory eliminates the need for additional dimensions that are not already supported by experimental evidence, a requirement often present in string theory and M-theory. Thirdly, we demonstrate the efficacy of our dual field theory by deriving Einstein’s general relativity from it. Lastly, we also successfully obtain Maxwell’s coefficients through this new framework.\n\nIntroduction: In recent years, there has been considerable effort to establish a theory of everything (TOE). Notable examples include string theory and loop quantum gravity. However, despite their achievements, these theories face significant challenges. For example, string theory depends on the existence of extra dimensions, while loop quantum gravity grapples with issues like non-renormalizability. These obstacles drive the search for alternative pathways in developing a TOE. A fresh perspective known as emergent spacetime has recently emerged. This approach posits that spacetime is not fundamental but arises from a deeper underlying entity. \n\nEmergent spacetime: The concept of emergent spacetime is quite straightforward. It suggests that spacetime is not an intrinsic feature but rather emerges from a more fundamental layer. To illustrate this, consider the following analogy: imagine you are sitting on your couch watching television. You might perceive your surroundings as flat because standing up would reveal the Earth's curvature beneath you. Now, envision rising above the Earth. In that position, you would no longer feel as if you were standing on a curved surface; instead, it would seem as though you were situated on a vast, flat plane.",
        "ori-fast-z-score": 0.3892494720807615,
        "water-fast-z-score": 6.538461538461538,
        "rewrite-fast-z-score": 1.5523010514126656
    },
    {
        "original_text": "We present results from three-dimensional hydrodynamic simulations of accreting white dwarfs in close binary systems, focusing on the boundary layer between the disk and the star. We find that the flow is highly turbulent with strong shocks forming at the interface between the two flows. The density structure shows significant departures from spherical symmetry due to the presence of spiral arms which form as a result of the interaction between the stellar magnetic field and the gas stream flowing towards the surface of the white dwarf. These spiral arms are responsible for driving an outflow along the polar axis of the system. In addition we find evidence for large-scale convection cells within the boundary layer. Our models suggest that the observed X-ray emission may be produced by these convective motions rather than by shock heating alone. This work was supported by NASA grant NAG5-7262. \n \n Keywords: Hydrodynamics; Shock waves; Convection",
        "watermark_text": "We present results from three - dimensional hydrodynamic simulations of accreting white dwarfs in close binary systems , concentrating on the boundary layer between the disk and the star . We see that the flow is strongly chaotic with powerful shocks producing at the interface between the two flows .The density structure exhibits significant departures from spherical symmetry thanks to the presence of spiral arms which occur as a product of the interaction between the stellar magnetic force and the gas stream running towards the surface of the white dwarf . These spiral arms are responsible for producing an outflow along the polar axis of the system .In addition we find proof for large - scale convection cells within the boundary layer . Our models suggest that the seen X - ray radiation may be made by these convective movements rather than by shock heating alone .This project was supported by NASA gift NAG5 - 7262 . Keywords : Hydrodynamics ; Shock currents ; Convection",
        "rewrite_text": "We present findings from three-dimensional hydrodynamic simulations of accreting white dwarfs in close binary systems, with a focus on the boundary layer between the disk and the star. Our results reveal that the flow is highly chaotic, characterized by intense shock waves at the interface of the two flows. The density structure shows significant deviations from spherical symmetry, primarily due to the formation of spiral arms that emerge from the interaction between the stellar magnetic field and the gas stream flowing toward the white dwarf's surface. These spiral arms facilitate outflows along the system's polar axis. Additionally, we have identified large-scale convection cells within the boundary layer. Our models indicate that the observed X-ray radiation may result from these convective movements rather than shock heating alone. This research was supported by NASA grant NAG5-7262. Keywords: Hydrodynamics; Shock currents; Convection.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 0.9299811099505543
    },
    {
        "original_text": "In this thesis, we study the problem of physical layer network coding (PLNC) in wireless networks with multiple relays and single-antenna nodes.  We first consider PLNC for two-way relay channels where each node has only one antenna.  In particular, we propose an efficient scheme to perform PLNC at both source nodes simultaneously by using linear codes over finite fields.  Then, we extend our results to multi-way relay channels with more than two users.  Finally, we investigate the performance of PLNC under imperfect channel state information (CSI). The main contributions are summarized as follows:  1. Two-way Relay Channels: We propose a novel approach to perform PLNC at the sources simultaneously based on linear codes over finite fields.   2. Multi-Way Relay Channels: By extending our previous work, we develop a new method to perform PLNC at all source nodes simultaneously.  3. Imperfect Channel State Information: We analyze the impact of imperfect CSI on the performance of PLNC schemes.",
        "watermark_text": "In this dissertation , we study the issue of physical layer network signaling ( PLNC ) in telecommunications networks with many relays and multiple - antenna nodes . We first consider PLNC for two - way relay channels where each node has only one antenna .In particular , we propose an efficient algorithm to conduct PLNC at both source nodes simultaneously by using linear codes over discrete fields . Then , we expanded our findings to multi - way transmission networks with more than two users .Finally , we investigate the performance of PLNC under imperfect channel state information ( CSI ) . The main contributions are presented as follows : 1 .Two - way Relay Channels : We suggest a new approach to conduct PLNC at the sources simultaneously based on linear codes over discrete fields . 2 .Multi - Way Relay Channels : By extending our previous research , we develop a new method to conduct PLNC at all source nodes simultaneously . 3 .Imperfect Channel State Information : We evaluate the impact of imperfect CSI on the performance of PLNC networks .",
        "rewrite_text": "In this dissertation, we explore the challenge of physical layer network coding (PLNC) within telecommunications networks that feature multiple relays and nodes with multiple antennas. We begin by examining PLNC in two-way relay channels, where each node is equipped with a single antenna. Specifically, we introduce an effective algorithm that enables simultaneous PLNC implementation at both source nodes through the use of linear codes over discrete fields. We then extend our research to multi-way transmission networks involving more than two users. Lastly, we assess the performance of PLNC in scenarios with imperfect channel state information (CSI). The key contributions of this work are as follows: 1. Two-Way Relay Channels: We present an innovative method for executing PLNC at the sources simultaneously, utilizing linear codes over discrete fields. 2. Multi-Way Relay Channels: Building on our earlier findings, we develop a new technique for implementing PLNC at all source nodes at once. 3. Imperfect Channel State Information: We analyze the effects of imperfect CSI on the performance of PLNC networks.",
        "ori-fast-z-score": 2.1652509527331207,
        "water-fast-z-score": 6.951595164037914,
        "rewrite-fast-z-score": 1.6876318513890358
    },
    {
        "original_text": "We present an analytic expression for the thermal Casimir force acting on two parallel plates made out of different materials, one being metallic (silver) while another is dielectric (silicon dioxide). The result obtained agrees with that derived by Lifshitz theory within 1% accuracy in the whole range of separations considered here. We also show how our results can be used to calculate the temperature dependence of the Casimir pressure at fixed separation distance. \n \n In this work we consider the case where one plate consists of silver and other of silicon dioxide. Silver has been chosen because it is widely used as a coating material in microelectromechanical systems (MEMS), whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices. Our results are applicable not only to these specific cases but also to any system consisting of two parallel plates separated by vacuum gap filled with gas medium. This includes such diverse situations like semiconductor heterostructures, quantum dots, nanowires etc., which have attracted considerable attention recently due to their potential applications in nanotechnology. \n \n It should be noted that the problem under consideration was first addressed theoretically more than 50 years ago  1  . However, despite numerous attempts  2  , no exact solution has yet been found. Therefore, most theoretical studies were performed using approximate methods  3  -  6  . These approaches include various modifications of the proximity force approximation  7, 8  , the Derjaguin-Muller-Toporov method  9  , the multiple reflection expansion  10  , the scattering matrix formalism  11  , the Green s function technique  12  , the density functional theory  13  , the mode summation  14  , the fluctuating surface charge model  15  , the effective-medium theory  16  , the generalized plasmon-pole model  17  , the Drude-Lorentz model  18  , the hydrodynamic model  19  , the nonlocal response  20  , the local field correction  21  , the random phase approximation  22  , the Monte Carlo simulation  23  , the finite element method  24  , the numerical integration  25  , the variational principle  26  , the perturbation theory  27  , the renormalization group  28  , the self-consistent screening  29  ,",
        "watermark_text": "We present an analytic definition for the thermal Casimir force acting on two connected sheets formed out of different materials , one being metallic ( silver ) while another is dielectric ( silicon dioxide ) . The result obtained agrees with that derived by Lifshitz principle within 1 % accuracy in the whole range of separations mentioned here .We also demonstrate how our findings can be used to estimate the temperature dependence of the Casimir pressure at fixed separation distance . In this research we study the case where one plate contains of silver and other of silicon dioxide .Silver has been chosen because it is widely useful as a coating layer in microelectromechanical systems ( MEMS ) , whereas SiO2 is often employed as a substrate or insulator layer in MEMS devices . Our results are applicable not only to these individual cases but also to any system consisting of two connected sheets connected by vacuum gap filled with liquid medium .This encompasses such diverse situations like semiconductor heterostructures , quantum dots , nanowires etc . , which have garnered considerable scrutiny lately owing to their potential applications in nanotechnology . It should be mentioned that the issue under consideration was first addressed theoretically more than 50 centuries earlier 1 .However , despite several attempts 2 , no accurate solution has already been finding . Therefore , most theoretical experiments were performed using approximate methods 3 - 6 .These approaches involve various alterations of the proximity stress approximation 7 , 8 , the Derjaguin - Muller - Toporov method 9 , the multiple mirror expansion 10 , the scattering matrix formalism 11 , the Green s function method 12 , the density functional theory 13 , the mode summation 14 , the fluctuating surface charge model 15 , the effective - medium theory 16 , the generalized plasmon - pole hypothesis 17 , the Drude - Lorentz model 18 , the hydrodynamic model 19 , the nonlocal response 20 , the local field correction 21 , the random phase approximation 22 , the Monte Carlo simulation 23 , the finite element method 24 , the numerical integration 25 , the variational theory 26 , the perturbation theory 27 , the renormalization group 28 , the self - consistent screening 29 ,",
        "rewrite_text": "We provide an analytical definition of the thermal Casimir force acting between two connected sheets made of different materials: one metallic (silver) and the other dielectric (silicon dioxide). Our findings align with those derived from the Lifshitz principle, achieving an accuracy of 1% across the entire range of separations we examine. Additionally, we illustrate how our results can be utilized to assess the temperature dependence of the Casimir pressure at a fixed separation distance. In this study, we focus on one plate made of silver and the other of silicon dioxide. Silver is selected due to its widespread use as a coating in microelectromechanical systems (MEMS), while silicon dioxide is commonly employed as a substrate or insulator within these devices. Our findings are relevant not only to these specific materials but also extend to any system comprising two connected sheets separated by a vacuum gap or filled with a liquid medium. This encompasses a variety of systems, including semiconductor heterostructures, quantum dots, and nanowires, which have gained significant attention recently because of their potential applications in nanotechnology. It is noteworthy that this issue was first theoretically addressed over 50 years ago; however, despite numerous attempts, no precise solution has been found. Consequently, much of the theoretical work has relied on approximate methods involving various adaptations of the proximity stress approximation, the Derjaguin-Muller-Toporov method, multiple mirror expansions, scattering matrix formalism, Green's function methods, density functional theory, mode summation, fluctuating surface charge models, effective-medium theory, the generalized plasmon-pole hypothesis, the Drude-Lorentz model, hydrodynamic models, nonlocal response theories, local field corrections, random phase approximation, Monte Carlo simulations, finite element methods, numerical integrations, variational theories, perturbation theories, and renormalization group methods.",
        "ori-fast-z-score": 1.5461980716652028,
        "water-fast-z-score": 7.7309903583260144,
        "rewrite-fast-z-score": 1.2632278815997784
    },
    {
        "original_text": "We present an improved quantum hard-sphere ground-state equation-of-state (EOS) for the description of dense matter in astrophysics and nuclear physics, which is based on the exact solution to the Schrödinger equation with a repulsive delta-function potential. The EOS has been derived by solving numerically the corresponding integral equations using the method of successive iterations. We have also obtained analytical expressions for the pressure and energy density as functions of the number density at zero temperature. Our results are compared with those calculated previously within various approximations such as the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. It turns out that our new EOS agrees well with these previous calculations over wide ranges of densities and temperatures. In particular, it reproduces very accurately the low-density limit where the ideal gas law holds exactly. \n \n Keywords: Equation of state",
        "watermark_text": "We introduce an efficient quantum hard - sphere ground - state equation - of - state ( EOS ) for the description of dense materials in astrophysics and nuclear science , which is based on the exact solution to the Schrödinger equation with a repulsive delta - function potential . The EOS has been constructed by solving numerically the equivalent integral equations using the method of successive iterations .We have already established analytical expressions for the pressure and energy density as functions of the number density at zero temperature . Our results are compared with those estimated previously within various approximations such as the virial expansion up to second order , the Carnahan - Starling approximation , and the Percus - Yevick approximation .It turns out that our new EOS agrees well with these previous calculations over broad ranges of densities and temperatures . In particular , it reproduces very correctly the small - density maximum where the ideal gas theory holds exactly .Keywords: Equation of state",
        "rewrite_text": "We present an efficient quantum hard-sphere equation of state (EOS) for modeling dense materials in astrophysics and nuclear science. This EOS is derived from the exact solution to the Schrödinger equation with a repulsive delta-function potential. We constructed the EOS by numerically solving the corresponding integral equations through successive iterations. We have established analytical expressions for pressure and energy density as functions of number density at zero temperature. Our findings are compared with earlier estimates generated through various approximations, including the second-order virial expansion, the Carnahan-Starling approximation, and the Percus-Yevick approximation. Our new EOS demonstrates strong agreement with these previous results over a wide range of densities and temperatures, particularly accurately capturing the small-density maximum where ideal gas theory is valid. Keywords: Equation of state",
        "ori-fast-z-score": 0.8427009716003844,
        "water-fast-z-score": 3.731961445658845,
        "rewrite-fast-z-score": 1.9694638556693236
    },
    {
        "original_text": "We present new experimental results on the relaxation dynamics of a liquid film that is pulled off an inclined solid substrate by gravity and capillarity. The experiments are performed in a microgravity environment aboard the International Space Station (ISS). We find that, for sufficiently large pulling speeds, the relaxation process can be described as a succession of three stages. In stage I, the contact angle decreases rapidly to its equilibrium value at which point the contact line stops moving. Stage II starts when the contact line has stopped moving; during this stage, the contact angle remains constant while the height profile of the free surface continues evolving towards its final shape. Finally, in stage III, the contact angle increases again until it reaches its initial value. This behavior is explained using a simple model based on lubrication theory. Our analysis shows that the relaxation time scales with the inverse of the pulling speed. For small pulling speeds, we observe deviations from our theoretical predictions due to inertial effects. \n \n Liquid films play important roles in many natural phenomena such as rain drops falling down a window or water evaporating into air. Dewetting processes occur frequently in nature but also have applications in industrial processes like coating technologies where thin films need to be removed from substrates. Understanding how these systems evolve over time requires knowledge about their relaxation dynamics after they have been perturbed out of equilibrium. Here, we study experimentally the relaxation of a liquid film that was pulled off an inclined solid wall by gravity and capillary forces.",
        "watermark_text": "We report new experimental results on the relaxation behavior of a liquid movie that is drawn off an inclined solid substrate by gravitational and capillarity . The studies are performed in a microgravity atmosphere aboard the International Space Station ( ISS ) .We see that , for enough large pulling speeds , the relaxation process can be described as a sequence of three stages . In stage I , the contact angle decreases quickly to its equilibrium value at which point the contact line stopping moved .Stage II began when the contact line has stopped movement ; during this phase , the contact angle remains regular while the height profile of the free boundary continues changing towards its final form . Finally , in stage III , the contact angle raises again until it hits its initial value .This phenomenon is understood using a simple model based on lubrication theory . Our study shows that the relaxation time varies with the inverse of the pulling velocity .For small pulling velocity , we perceive deviations from our theory assumptions owing to inertial influences . Liquid films play important roles in many natural phenomena such as rainfall drops sliding down a window or water evaporating into air .Dewetting methods happen widely in nature but also have applications in industrial systems like coating technologies where loose films need to be removed from substrates . Understanding how these systems develop over time needs experience about their relaxation behavior after they have been perturbed out of equilibrium .Here , we study experimentally the relaxation of a liquid movie that was yanked off an inclined solid wall by gravitational and capillary forces .",
        "rewrite_text": "We present new experimental findings on the relaxation behavior of a liquid film that is drawn off an inclined solid surface due to gravitational and capillary forces. These experiments were conducted in the microgravity environment of the International Space Station (ISS). Our results indicate that, at sufficiently high pulling speeds, the relaxation process can be characterized by three distinct phases. In Stage I, the contact angle rapidly decreases to its equilibrium value, at which point the contact line stops moving. Stage II begins when the contact line ceases motion; during this phase, the contact angle stabilizes while the height profile of the free boundary continues to evolve toward its final shape. Finally, in Stage III, the contact angle rises again until it reaches its initial value. This phenomenon can be explained using a simple lubrication theory model. Our study demonstrates that the relaxation time is inversely proportional to the pulling velocity. At lower pulling velocities, we observe deviations from theoretical predictions due to inertial effects. Liquid films are critical in various natural processes, such as raindrops sliding down windows or water evaporating into the air. Dewetting phenomena are prevalent in nature and also have significant industrial applications, particularly in coating technologies where thin films must be removed from surfaces. Gaining insights into the temporal development of these systems necessitates an understanding of their relaxation behavior following perturbations from equilibrium. In this study, we experimentally investigate the relaxation of a liquid film that has been pulled from an inclined solid substrate by gravitational and capillary forces.",
        "ori-fast-z-score": -0.17277368511627203,
        "water-fast-z-score": 7.717436331412897,
        "rewrite-fast-z-score": 2.2183912735402846
    },
    {
        "original_text": "The switching behavior and the photovoltaic properties of two new diarylethene derivative molecular junctions were investigated by using cyclic voltammetry, UV-Vis spectroscopy, and electrochemical impedance spectroscopy (EIS). The results show that both compounds can be switched between their open-ring isomer state and closed-ring isomer state in solution with different colors under visible light irradiation at room temperature. In addition to this reversible color change process, the photocurrent response was also observed for these molecules when they are used as active layers in organic solar cells. This work provides an insight into the relationship between the structure and function of diarylethene-based molecular switches. Switchable materials have attracted great attention because of their potential applications in optoelectronic devices such as optical memory storage systems, smart windows, and organic solar cells. \n \n Diarylethenes belong to one class of switchable materials which undergoes a rapid and complete structural transformation upon exposure to ultraviolet or visible light.  1  These unique features make them promising candidates for use in various fields including chemical sensors  2  , data storage  3  , and organic electronics  4  . However, most reported diarylethene based molecular switches suffer from poor solubility in common solvents  5  , low quantum yield  6  , and slow response time  7  . Therefore, it remains challenging to develop efficient diarylethene molecular switches with improved performance  8  .\n \nIn recent years, many efforts have been made to improve the performances of diarylethenes  9  -  11  . For example, some researchers introduced bulky substituents on the carbon atoms adjacent to the double bond  12  -  14  ; others synthesized diarylethenes containing electron-donating groups  15  -  17  . Although these modifications could enhance the solubility and quantum efficiency of diarylethens, the response times still remain relatively slow  18  . \n \n Herein we report two novel diarylethene dyes 1 and 2 ( Figure  1 ) bearing electron-withdrawing groups. Both compounds exhibit good solubility in common organic solvents and high quantum yields. They can",
        "watermark_text": "The switching activity and the photovoltaic properties of two new diarylethene derivative chemical junctions were researched by using cyclic voltammetry , UV - Vis spectroscopy , and electrochemical impedance spectroscopy ( EIS ) . The results show that both compounds can be switched between their closed - ring isomer state and opened - ring isomer state in solution with various shades under visible color irradiation at room temperature .In addition to this reversible color transformation process , the photocurrent response was also observed for these molecules when they are applied as active layers in organic solar systems . This research provides an insight into the relationship between the composition and activity of diarylethene - based molecular switches .Switchable materials have garnered great popularity because of their potential applications in optoelectronic products such as laser memory memory devices , smart mirrors , and organic solar devices . Diarylethenes represent to one category of switchable materials which undergoes a rapid and complete structural transformation upon exposure to ultraviolet or visible light .1 These unique features make them promising candidates for use in different fields including molecular sensors 2 , computer processing 3 , and organic devices 4 . However , most published diarylethene based molecular switches tend from poor solubility in standard solvents 5 , low quantum strength 6 , and poor response speed 7 .Therefore , it remains challenging to develop fast diarylethene molecular switches with improved performance 8 . In recent years , various efforts have been placed to improve the performances of diarylethenes 9 - 11 .For instance , some researchers incorporated bulky substituents on the carbon atoms adjacent to the double bond 12 - 14 ; others synthesized diarylethenes bearing electron - donating groups 15 - 17 . Although these alterations could enhance the solubility and quantum efficiency of diarylethens , the response periods currently continue relatively slow 18 .Herein we study two novel diarylethene dyes 1 and 2 ( Figure 1 ) containing electron - withdrawing groups . Both compounds exhibit great solubility in standard organic solvents and large quantum yields .They can",
        "rewrite_text": "The switching behavior and photovoltaic characteristics of two new diarylethene derivative chemical junctions were investigated using cyclic voltammetry, UV-Vis spectroscopy, and electrochemical impedance spectroscopy (EIS). The findings indicate that both compounds can switch between their closed-ring and open-ring isomer states in solution, exhibiting various colors under visible light irradiation at room temperature. In addition to this reversible color change, a photocurrent response was observed when these molecules were utilized as active layers in organic solar cells. This study offers insights into the relationship between the composition and activity of diarylethene-based molecular switches. Switchable materials have gained significant attention due to their potential applications in optoelectronic devices such as laser memory systems, smart mirrors, and organic solar cells. Diarylethenes are a type of switchable material that undergoes a rapid and complete structural transformation when exposed to ultraviolet or visible light. These distinctive properties make them promising candidates for various applications, including molecular sensors, computer processing, and organic devices. However, many previously reported diarylethene-based molecular switches suffer from poor solubility in common solvents, low quantum efficiency, and slow response times. As a result, developing fast-responding diarylethene molecular switches with enhanced performance remains a challenge. In recent years, various strategies have been employed to improve the performance of diarylethenes. For example, some researchers have added bulky substituents to the carbon atoms adjacent to the double bond, while others have synthesized diarylethenes with electron-donating groups. While these modifications have improved solubility and quantum efficiency, the response times have generally remained relatively slow. In this study, we present two novel diarylethene dyes (labeled 1 and 2 in Figure 1) that incorporate electron-withdrawing groups. Both compounds exhibit excellent solubility in standard organic solvents and possess high quantum yields.",
        "ori-fast-z-score": -0.6446583712203042,
        "water-fast-z-score": 7.091242083423347,
        "rewrite-fast-z-score": 1.1281521496355325
    },
    {
        "original_text": "We study the zero-lag synchronization between two identical chaotic systems with different time scales by using dynamical relaying method, which is based on introducing an intermediate system to transmit information between them. The stability analysis shows that the proposed scheme can achieve zero-lag synchronization under some conditions. Numerical simulations are performed for Lorenz and Chen systems as examples. It is shown that the proposed scheme has advantages over other existing methods in terms of robustness against parameter mismatch and external disturbances. \nI. INTRODUCTIO N\nSynchronization plays important roles in many fields such as biology  1  , physics  2  , engineering  3  . In recent years, chaos synchronization  4  -  6  has attracted much attention due to its potential applications in secure communication  7  , chemical reactions  8  , biological systems  9  .\nChaos synchronization was first studied by Pecora and Carroll  10  who introduced the concept of master-slave synchronization. Since then, various schemes have been developed  11  -  13  . Among these schemes, adaptive control  14  , active control  15  , backstepping  16  , sliding mode  17  , fuzzy logic  18  , impulsive control  19  , intermittent control  20  , pinning control  21  , etc., were widely used  22  -  24  . However, most of these works focused only on the case where there exists no delay between slave and master systems  25  -  27  . Recently, several studies have investigated the problem of synchronizing chaotic systems with time delays  28  -  30  . For example, Wu et al.  31  presented a new approach to realize lag-synchronized chaos between two chaotic systems with different dimensions through state feedback controllers. Liu et al.  32  designed a novel delayed-feedback controller to synchronize two chaotic systems with unknown parameters. Wang et al.  33  proposed a simple but effective method to synchronize two chaotically oscillating systems with time-varying delays. Although these results provide useful insights into the design of synchronized chaotic systems with time-delays, they cannot be applied directly to solve practical problems because it may take too",
        "watermark_text": "We explore the zero - lag synchronization between two unrelated turbulent systems with varying time ranges by using dynamical relaying model , which is based on introducing an intermediate system to transmit data between them . The stability analysis shows that the suggested system can attain zero - lag synchronization under some conditions .Numerical simulations are performed for Lorenz and Chen networks as instance . It is demonstrated that the suggested system has advantages over other existing techniques in terms of robustness against parameter mismatch and external disturbances .I . INTRODUCTIO N Synchronization plays important roles in different fields such as biology 1 , physics 2 , engineering 3 . In past decades , chaos synchronization 4 - 6 has drew much attention due to its potential applications in safe communication 7 , chemical processes 8 , biological systems 9 .Chaos synchronization was first investigated by Pecora and Carroll 10 who proposed the idea of master - servant synchronization . Since then , various strategies have been created 11 - 13 .Among these schemes , adaptive control 14 , active control 15 , backstepping 16 , sliding mode 17 , fuzzy logic 18 , impulsive control 19 , continuous control 20 , dragging control 21 , etc . , were commonly used 22 - 24 . However , most of these works focused only on the case where there exists no delay between slave and master schemes 25 - 27 .Recently , various studies have explored the issue of synchronizing dynamic systems with time delays 28 - 30 . For instance , Wu et al .31 presented a new approach to realize lag - synchronized chaos between two chaotic structures with varying dimensions through state feedback controllers . Liu et al .32 designed a innovative delayed - feedback controller to synchronize two chaotic structures with unknown parameters . Wang et al .33 introduced a simple but effective model to synchronize two chaotically oscillating systems with time - differing delays . Although these results present valuable insights into the creation of synchronized turbulent systems with time - delays , they cannot be applied directly to solve practical questions because it could took too",
        "rewrite_text": "We investigate zero-lag synchronization between two dissimilar turbulent systems across various time scales using a dynamical relaying model. This model introduces an intermediate system to facilitate data transfer between the two primary systems. Stability analyses indicate that the proposed system can achieve zero-lag synchronization under certain conditions. Numerical simulations are conducted on the Lorenz and Chen networks as examples, demonstrating that our approach offers advantages over existing techniques, particularly in terms of robustness to parameter mismatches and external disturbances. \n\n**I. INTRODUCTION** \nSynchronization is crucial in various fields, including biology, physics, and engineering. In recent decades, chaos synchronization has gained significant attention due to its potential for applications in secure communication, chemical processes, and biological systems. Initially, Pecora and Carroll introduced the concept of master-servant synchronization, which paved the way for numerous strategies to be developed. Common approaches include adaptive control, active control, backstepping, sliding mode, fuzzy logic, impulsive control, continuous control, and dragging control. However, the majority of these studies focused primarily on scenarios without delays between the master and slave systems. Recently, various investigations have addressed the challenge of synchronizing dynamic systems with time delays. For instance, Wu et al. proposed a novel method for achieving lag-synchronized chaos between chaotic structures of differing dimensions using state feedback controllers. Liu et al. developed an innovative delayed-feedback controller for synchronization of chaotic systems with unknown parameters. Wang et al. introduced a straightforward yet effective model for synchronizing chaotically oscillating systems with differing time delays. While these findings provide valuable insights into synchronizing turbulent systems with time delays, they often cannot be directly applied to practical problems due to potential complexities involved.",
        "ori-fast-z-score": -0.08192319205190406,
        "water-fast-z-score": 8.16496580927726,
        "rewrite-fast-z-score": 1.2206826881567392
    },
    {
        "original_text": "The prediction of the next cycle is an important problem in space weather research, which has been studied for more than half century. In this work we use two different methods to predict the strength and duration of the twenty-first solar cycle (SC21). The first method uses artificial neural networks trained on data from previous cycles. We find that our network predicts SC21 with high accuracy. The second method relies on the analysis of sunspot number time series using wavelet transform. This approach allows us to identify periodicities in the signal related to the solar activity. Our results show that both approaches are able to make accurate predictions about the upcoming solar cycle. Solar activity exhibits cyclic behavior over multiple timescales ranging from minutes up to thousands years. Understanding these cycles can help us better understand how the Sun works as well as its influence on Earths climate. \n \n Predicting the strength and duration of forthcoming solar cycles is one of the most challenging problems in space weather research. It was shown by several authors that it is possible to forecast the amplitude of the current cycle based on information available at the beginning of the cycle itself  1  . However, predicting the exact timing of maxima or minima within each cycle remains difficult  2  . \n \n Here we present two independent methods to predict the properties of the twenty-first solar activity cycle (SC21) starting from the end of twentieth cycle (SC20), i.e., from January 2010. Both methods rely only on publicly available data sets obtained from NASA s Space Weather Prediction Center  3  , NOAA  4  , and SIDC  5  .\n \nMethod 1: Artificial Neural Networks \n \n First, we train an artificial neural network  6  on data from past solar cycles. Specifically, we consider the following inputs: 1) monthly mean sunspot numbers; 2) monthly mean 10.7-cm radio flux values; 3) monthly mean F10.7 index; 4) monthly mean Mg II index. These quantities were averaged over the last ten solar cycles prior to SC20. For example, if we want to predict SC21, then we average all four quantities between December 2009 and November 2019. Note that",
        "watermark_text": "The calculation of the new cycle is an important task in space weather study , which has been studied for more than quarter century . In this study we using two different methods to predict the strength and duration of the hundred - first solar cycle ( SC21 ) .The first method uses artificial neural systems trained on evidence from previous periods . We see that our system predicts SC21 with high clarity .The second method relies on the characterization of sunspot number period series employing wavelet transform . This method enables us to identify periodicities in the signal related to the sun activity .Our results show that both approaches are able to make accurate forecast about the ongoing solar cycle . Solar behavior exhibits cyclic behavior over numerous timescales varied from seconds up to thousands decades .Understanding these cycles can help us better understand how the Sun operates as well as its influence on Earths environment . Predicting the strength and duration of forthcoming solar cycles is one of the most challenging difficulties in space weather study .It was shown by various papers that it is easy to forecast the frequency of the present process depending on knowledge accessible at the beginning of the cycle itself 1 . However , predicting the exact timing of maxima or minima within each cycle remains impossible 2 .Here we present two independent methods to predict the properties of the twenty - first solar activity period ( SC21 ) beginning from the end of twentieth cycle ( SC20 ) , i . e . , from January 2010 . Both methods rely only on publicly accessible data sets taken from NASA s Space Weather Prediction Center 3 , NOAA 4 , and SIDC 5 .Method 1 : Artificial Neural Networks First , we develop an synthetic cognitive network 6 on evidence from previous solar cycles . Specifically , we investigate the following output : 1 ) weekly mean sunspot numbers ; 2 ) weekly mean 10 . 7 - cm radio flux values ; 3 ) weekly mean F10 . 7 index ; 4 ) weekly mean Mg II index .These quantities were averaged over the last ten solar cycles prior to SC20 . For instance , if we wish to predict SC21 , then we average all four quantities between December 2009 and November 2019 .Note that",
        "rewrite_text": "The calculation of the new solar cycle is a crucial aspect of space weather research, which has been explored for over 25 years. In this study, we employ two distinct methods to forecast the strength and duration of the 21st solar cycle (SC21). The first approach utilizes artificial neural networks that are trained on historical data from previous cycles, showing that our system predicts SC21 with impressive accuracy. The second method involves analyzing the sunspot number time series using wavelet transforms, which helps us identify periodic signals associated with solar activity. Our findings indicate that both methods provide reliable forecasts for the current solar cycle. Solar activity exhibits cyclic patterns across a wide range of timescales, from seconds to thousands of years. Gaining insights into these cycles enhances our understanding of solar dynamics and their effects on Earth's environment. Predicting the strength and duration of future solar cycles poses significant challenges in space weather research. Previous studies have demonstrated that it is relatively straightforward to forecast the frequency of ongoing solar phenomena based on data available at the beginning of the cycle. However, accurately predicting the timing of maxima or minima within each cycle remains a challenge. In this work, we present two independent methods to estimate the characteristics of the 21st solar activity period (SC21), starting from the conclusion of the 20th cycle (SC20) in January 2010. Both methods rely solely on publicly available datasets from NASA’s Space Weather Prediction Center, NOAA, and SIDC. \n\nMethod 1: Artificial Neural Networks \nWe first develop a synthetic neural network trained on data from previous solar cycles. Specifically, we focus on predicting the following outputs: 1) weekly mean sunspot numbers, 2) weekly mean 10.7-cm radio flux values, 3) weekly mean F10.7 index, and 4) weekly mean Mg II index. These metrics are averaged over the ten solar cycles preceding SC20. For instance, to forecast SC21, we compute the averages of these four quantities from December 2009 to November 2019.",
        "ori-fast-z-score": 1.543033499620919,
        "water-fast-z-score": 9.412504347687607,
        "rewrite-fast-z-score": 1.182165609358651
    },
    {
        "original_text": "We report on infrared spectroscopic observations with the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope toward the nearby late-type galaxy NGC 3621, which is known to harbor a supermassive black hole at its center. The IRS spectrum shows prominent emission lines such as  Ne II 12.81 and  S III 18.71 µm that are commonly seen in active galactic nuclei (AGNs). We find that these emission lines can be reproduced by photoionization models using AGN-like ionizing radiation fields. From the observed line ratios we estimate the electron density n e = 10 3 cm −3 , temperature T e = 1000 K, and ionization parameter U H = 1 × 10 −2 . These results suggest that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies. This work was supported by NASA through grant number GO-08460.01-A awarded by the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA.",
        "watermark_text": "We report on infrared spectroscopic observations with the Infrared Spectrograph ( IRS ) aboard the Spitzer Space Telescope toward the nearby mid - class galaxy NGC 3621 , which is known to harbor a supermassive black hole at its core . The IRS spectrum displays large emitted lines such as Ne II 12 . 81 and S III 18 . 71 µm that are often observed in active galactic nuclei ( AGNs ) .We see that these radiation patterns can be reproduced by photoionization models using AGN - like ionizing radiation fields . From the known line ratios we estimate the electron concentration h e = 10 3 cm −3 , temperature T e = 1000 K , and ionization variable U H = 1 × 10 −2 .These results show that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies . This work was supported by NASA through grant number GO - 08460 . 01 - A awarded by the Jet Propulsion Laboratory , California Institute of Technology under contract with NASA .",
        "rewrite_text": "We present infrared spectroscopic observations obtained with the Infrared Spectrograph (IRS) on the Spitzer Space Telescope targeting the nearby mid-class galaxy NGC 3621, which is known to host a supermassive black hole at its center. The IRS spectrum reveals prominent emission lines, including Ne II at 12.81 µm and S III at 18.71 µm, commonly seen in active galactic nuclei (AGNs). Our analysis shows that these spectral features can be replicated using photoionization models that utilize AGN-like ionizing radiation fields. From the established line ratios, we derive the electron density, \\( n_e = 10^3 \\, \\text{cm}^{-3} \\), an electron temperature, \\( T_e = 1000 \\, \\text{K} \\), and an ionization parameter, \\( U_H = 1 \\times 10^{-2} \\). These findings indicate that the central region of NGC 3621 exhibits characteristics akin to those of Seyfert galaxies. This research was funded by NASA under grant number GO-08460.01-A, awarded by the Jet Propulsion Laboratory at the California Institute of Technology in collaboration with NASA.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We consider the problem of decoding linear block codes over an arbitrary finite field using the min-sum algorithm, which is one of the most popular algorithms in practical applications such as Turbo coding.  We show that under certain conditions on the degree distribution of parity-check nodes, there exists a unique optimal solution to the optimization problem associated with each node update rule at every iteration of the min-sum decoder. This result leads us to propose a new stability condition for the min-sum decoder based on the concept of local convergence. The proposed stability condition can be used to determine whether or not the min-sum decoder converges globally by checking if it locally converges within a small number of iterations. Finally, we present simulation results showing that our proposed stability condition outperforms existing ones when applied to LDPC codes. In this work, we study the problem of decoding linear binary block codes using the min-sum (MS) algorithm  1  , which has been widely adopted in many practical communication systems including Turbo-coding  2  . It was shown in  3  -  5  that MS decoding achieves near maximum-likelihood performance while requiring only low complexity per bit compared to other iterative decoders  6  .\nIn general, the MS algorithm solves the following problem: given a codeword c =  c0 c1 . . . cm−1  ∈ Fm−1 2\n, find the vector x * ∈ F2 n satisfying Hx * = c where H denotes the parity check matrix of size m × n. To solve this problem, the MS algorithm performs message passing between variable nodes and parity-check nodes according to the following rules: 1) At each iteration t, compute the log likelihood ratio (LLR) λt(i), i ∈ {0, . . . , m − 1}, corresponding to ci as: \nwhere N (j) represents the set of neighbors connected to j via edges in H; 2) Update the LLRs of all parity-check nodes:",
        "watermark_text": "We consider the question of decoding linear block sequences over an arbitrary finite field using the min - sum algorithm , which is one of the most popular methods in practical applications such as Turbo coding . We see that under certain conditions on the degree distribution of parity - check vertices , there exists a unique optimal solving to the algorithms issue associated with each node update rule at every iteration of the min - sum decoder .This result leads us to propose a new stability condition for the min - sum decoder relying on the idea of local convergence . The proposed stability condition can be used to determine whether or not the min - sum decoder converges worldwide by testing if it locally converges within a small number of iterations .Finally , we present modeling results confirming that our proposed stability situation outperforms current ones when applied to LDPC coding . In this research , we study the question of decoding linear binary block sequences using the min - sum ( MS ) algorithm 1 , which has been widely adopted in many practical communication schemes notably Turbo - codes 2 .It was shown in 3 - 5 that MS decoding achieves near maximum - likelihood performance while using only low complexity per bit relative to other iterative decoders 6 . In general , the MS algorithm solves the following task : given a codeword c = c0 c1 ...cm−1 ∈ Fm−1 2 , find the vector x * ∈ F2 n satisfying Hx * = c where H represents the parity check matrix of width m × n . To solve this situation , the MS algorithm performs message passing between variable nodes and parity - check nodes according to the following laws : 1 ) At each iteration t , compute the log probability ratio ( LLR ) λt ( i ) , i ∈ { 0 , . .. , m − 1 } , analogous to ci as : where N ( j ) indicates the group of neighbors connected to j via paths in H ; 2 ) Update the LLRs of all parity - check vertices :",
        "rewrite_text": "We examine the decoding of linear block sequences over an arbitrary finite field using the min-sum algorithm, a widely-used method in practical applications, especially in Turbo coding. Our analysis reveals that, under specific conditions related to the degree distribution of parity-check vertices, there is a unique optimal solution to the algorithmic challenges encountered at each node update during every iteration of the min-sum decoder. This finding leads us to introduce a new stability condition for the min-sum decoder, which is based on the concept of local convergence. This newly proposed stability condition can help assess the broader convergence of the min-sum decoder by evaluating its local convergence within a limited number of iterations. Ultimately, we present modeling results demonstrating that our proposed stability criterion surpasses existing conditions when applied to LDPC coding. In this study, we focus on decoding linear binary block sequences with the min-sum (MS) algorithm, which has become a staple in various practical communication systems, particularly Turbo codes. Previous research has shown that MS decoding can achieve performance close to maximum likelihood while maintaining low complexity per bit in comparison to other iterative decoders. Generally, the MS algorithm addresses the following problem: given a codeword \\( c = c_0 c_1 \\ldots c_{m-1} \\in F^{m-1}_2 \\), find the vector \\( x^* \\in F^n_2 \\) that satisfies \\( Hx^* = c \\), where \\( H \\) denotes the parity check matrix of dimensions \\( m \\times n \\). To solve this problem, the MS algorithm engages in message passing between variable nodes and parity-check nodes according to these rules: 1) At each iteration \\( t \\), calculate the log-likelihood ratio (LLR) \\( \\lambda_t(i) \\) for \\( i \\in \\{0, \\ldots, m - 1\\} \\), corresponding to \\( c_i \\); where \\( N(j) \\) denotes the set of neighbors connected to \\( j \\) through paths in \\( H \\); 2) Update the LLRs for all parity-check vertices.",
        "ori-fast-z-score": -0.16783627165933782,
        "water-fast-z-score": 5.937318271050344,
        "rewrite-fast-z-score": 1.469693845669907
    },
    {
        "original_text": "We present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) . The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 cm / sec .We see that the light spiral can be well fitting using a description consisting of three components : shock breakout emission , radioactive decay powered luminosity , and dust extinction . Using this description we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power .Our results are compatible with those detected for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought . This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far .In addition to these results , our observations offer additional perspectives into the physics of wave breakout and first - time evolution of type - II SNe .",
        "rewrite_text": "We present visual and far-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is located at an unusually large distance from its host galaxy, with collapse speeds of approximately 1000 cm/s. Our analysis shows that the light curve can be effectively modeled using three components: shock breakout emission, luminosity powered by radioactive decay, and dust extinction. Utilizing this model, we calculate important physical parameters, including the progenitor's diameter, mass loss rate, and explosion energy. Our findings align with observations of other Type II supernovae but suggest that the progenitor star may have had a smaller initial mass than previously assumed. This indicates a potential for greater diversity among the progenitors of Type II supernovae than previously recognized. Additionally, our observations provide further insights into the physics of shock breakout and the early evolution of Type II supernovae.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "The distance geometry problem is the following: given n points in space, find their coordinates such that they are as close to each other as possible while respecting some constraints on distances between pairs of points.  In this work we consider an application of the distance geometry problem to music analysis and propose a novel method for automatic transcription of musical scores into symbolic representations based on the concept of pitch class profiles (PCP). The proposed approach allows us to recover the underlying harmonic structure of polyphonic music by solving a system of quadratic equations with linear equality constraints using convex optimization techniques. We demonstrate our algorithm s performance on several classical piano pieces. 1 Introduction\n\nMusic Analysis\nAutomatic transcription of musical scores has been one of the most challenging problems in computer science over the past decades. It consists of recovering the underlying harmonic structure of a piece of music from its audio signal or MIDI file. This task can be divided into two main subtasks:  detection of note onset times; estimation of pitches at detected notes  locations. Note onset time detection is usually performed by applying various heuristics to the raw audio data  22, 23  . Once the note onset times have been determined, the next step is to estimate the pitches corresponding to these events. There exist many different approaches to solve this problem ranging from simple template matching methods to more sophisticated statistical models  7, 8, 10, 11, 13, 14, 16, 17, 19-21, 24-26  .\nIn this work we focus on the second part of the problem -estimation of pitches-which is known as  pitch estimation  or  pitch tracking . Pitch tracking algorithms try to assign a pitch value to every detected event in order to obtain a sequence of pitch values which correspond to the original score. A common way to represent pitches is through so-called pitch-class profiles (PCPs)  6, 12, 15, 18, 27  , where each entry corresponds to the number of occurrences of a particular pitch within a certain window around the current time instant. For example, Figure 1 shows a typical PCP obtained from a single-note mel",
        "watermark_text": "The distance geometry issue is the following : given n points in space , find their coordinates such that they are as close to each other as possible while respecting some restrictions on distances between pairs of points . In this study we investigate an use of the distance geometry issue to music analysis and suggest a new method for efficient reproduction of visual scores into symbolic representations based on the idea of pitch class profiles ( PCP ) .The proposed approach allows us to regain the fundamental chord form of polyphonic music by modeling a system of quadratic variables with linear equality restrictions utilizing convex optimization tools . We test our algorithm s playing on numerous traditional instrument pieces .1 Introduction Music Analysis Automatic recording of musical scores has been one of the most challenging difficulties in computer science over the previous decades . It consists of finding the fundamental chord form of a work of music from its audio stream or MIDI file .This job can be grouped into two principal subtasks : detection of note onset times ; estimation of pitches at identified notes sites . Note onset time detection is usually performed by using numerous heuristics to the raw audio information 22 , 23 .Once the tone arrival times have been determined , the second step is to estimate the pitches corresponding to these events . There remain many various approaches to solve this question ranging from complicated template matching algorithms to more sophisticated mathematical models 7 , 8 , 10 , 11 , 13 , 14 , 16 , 17 , 19 - 21 , 24 - 26 .In this research we focus on the second part of the question - estimation of pitches - which is known as pitch estimation or pitch tracking . Pitch tracking schemes seek to give a pitch number to every detected occurrence in order to obtain a sequence of pitch values which coincide to the actual performance .A popular way to represent notes is through so - called pitch - class profiles ( PCPs ) 6 , 12 , 15 , 18 , 27 , where each entry relates to the quantity of occurrences of a certain pitch within a certain window around the present time instant . For instance , Figure 1 shows a typical PCP obtained from a single - note mel",
        "rewrite_text": "The distance geometry problem involves determining the coordinates of \\( n \\) points in space so that they are positioned as closely as possible while adhering to specific distance constraints between pairs of points. In this research, we explore the application of distance geometry in music analysis and propose a novel method for efficiently transforming visual scores into symbolic representations using pitch class profiles (PCP). Our method enables the recovery of the fundamental chord structure in polyphonic music by modeling a system of quadratic variables with linear equality constraints using convex optimization techniques. We validate our algorithm by applying it to several pieces played on traditional instruments.\n\n### 1. Introduction \nAutomated transcription of musical scores has posed significant challenges in computer science over the past few decades. This task involves identifying the fundamental chord structure of a musical piece from its audio or MIDI data. It can be divided into two main subtasks: detecting note onset times and estimating the pitches at these noted occurrences. Note onset detection typically employs various heuristics applied to raw audio signals. Once the onset times are established, the next step is to estimate the pitches associated with these events. A variety of approaches exist for this task, ranging from complex template matching algorithms to more advanced mathematical models. Our research focuses on the second aspect—pitch estimation or tracking, which aims to assign a pitch value to each detected event, producing a sequence of pitch values that accurately reflect the performance. A common representation for notes is through pitch-class profiles (PCPs), where each entry indicates the frequency of a particular pitch within a defined time window. For example, Figure 1 illustrates a typical PCP derived from a single-note melody.",
        "ori-fast-z-score": -0.37582301400141443,
        "water-fast-z-score": 9.594005619147026,
        "rewrite-fast-z-score": 0.8778955729143844
    },
    {
        "original_text": "We present the results of our analysis on stellar surface structure effects in high precision space-based optical astrometric observations, which are expected to be achieved by future space missions such as Gaia and SIM. We show that these effects can cause systematic errors in the derived parameters for individual stars with precisions better than 1 microarcsecond (mas). The main sources of error include limb darkening effect, gravity brightening effect, and rotational distortion effect. These effects depend strongly on the spectral type of observed stars. For example, we find that the rotationally distorted shape of late-type giant stars may lead to an apparent shift of up to several mas between their positions measured at different epochs. This is comparable or even larger than the accuracy goal set for Gaia mission. Therefore it will be necessary to take into account these effects when analyzing data obtained by future space missions. Keywords: Astrometry, Gravity",
        "watermark_text": "We present the conclusion of our analysis on stellar surface structure effects in high precision space - based optical astrometric observations , which are expected to be completed by future space missions such as Gaia and SIM . We suggest that these influences can cause deliberate errors in the derived parameters for individual stars with precisions higher than 1 microarcsecond ( mas ) .The main sources of error include limb darkening effect , gravity brightening effect , and rotational interference effect . These effects influence heavily on the spectral type of measured stars .For instance , we find that the rotationally altered form of late - class giant stars would result to an apparent shift of up to several mas between their positions measured at different epochs . This is analogous or especially bigger than the accuracy goal set for Gaia expedition .Therefore it will be required to take into consideration these influences when examining data received by future orbital flights . Keywords : Astrometry , Gravity",
        "rewrite_text": "We present the conclusions of our analysis regarding the effects of stellar surface structure on high-precision space-based optical astrometric observations, which will be further refined by upcoming missions such as Gaia and SIM. Our findings indicate that these effects can introduce significant errors in the derived parameters of individual stars, with precisions exceeding 1 microarcsecond (mas). The primary sources of error identified are limb darkening, gravity brightening, and rotational interference. These factors significantly impact the spectral classification of the observed stars. For example, we discovered that the rotationally altered positions of late-type giant stars can lead to apparent shifts of several mas between their measurements at different times. This error is comparable to, or even larger than, the accuracy goals set for the Gaia mission. Therefore, it is essential to account for these influences when analyzing data collected from future space missions. \nKeywords: Astrometry, Gravity",
        "ori-fast-z-score": 0.45291081365783825,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": -0.9299811099505543
    },
    {
        "original_text": "We present new measurements of line emission for the brightest cluster galaxies (BCGs) in clusters with z < 0.3, using data obtained by the Chandra X-ray Observatory. We find that BCGs  optical luminosities are correlated strongly with their soft-band X-ray luminosities; this correlation is stronger than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity.  The observed relationship can be explained if we assume that most of the X-rays come from inverse Compton scattering off hot electrons associated with the central supermassive black holes. This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs. In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti-correlation between Lopt and the temperature Tgas of the intracluster medium surrounding each galaxy. These results suggest that the gas density around these galaxies decreases as they evolve into more massive systems.",
        "watermark_text": "We present new studies of line emission for the brightest cluster stars ( BCGs ) in clusters with z < 0 . 3 , using data derived by the Chandra X - ray Observatory . We see that BCGs visual luminosities are correlated heavily with their soft - band X - ray luminosities ; this relationship is strengthened than previously reported correlations between optical and radio luminosity or between optical and infrared luminosity .The observed relationship can be described if we suppose that most of the X - radiation come from inverse Compton absorption off warm particles associated with the main supermassive black holes . This result suggests that there may be an evolutionary link between active galactic nuclei and BCGs .In addition to the strong correlation between Lopt and LX , we also observe a weak but significant anti - correlation between Lopt and the temperature Tgas of the intracluster medium circling each galaxy . These data suggest that the gas density around these objects decreases as they develop into more massive structures .",
        "rewrite_text": "We present new research on line emissions from the brightest cluster galaxies (BCGs) in clusters with redshifts below 0.3, utilizing data from the Chandra X-ray Observatory. Our findings indicate a strong correlation between the visual luminosities of BCGs and their soft-band X-ray luminosities, which is more robust than previously documented correlations involving optical and radio luminosities or between optical and infrared luminosities. This observed relationship can be explained by the notion that much of the X-ray radiation emanates from inverse Compton scattering off warm particles linked to the supermassive black holes at the center of these galaxies. This suggests a potential evolutionary connection between active galactic nuclei and BCGs. Moreover, alongside the pronounced correlation between optical luminosity (Lopt) and X-ray luminosity (LX), we also find a weak but significant anti-correlation between Lopt and the gas temperature (Tgas) of the intracluster medium surrounding each galaxy. These observations imply that as these structures become more massive, the gas density in their vicinity diminishes.",
        "ori-fast-z-score": 1.5428161556520092,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 2.8284271247461903
    },
    {
        "original_text": "The Fermi Large Area Telescope (LAT) is the primary instrument on NASA s Fermi satellite, launched in June 2008 to study high-energy gamma rays and cosmic rays. The LAT has been taking data since August 4, 2008, and it continues to operate today. This talk will present highlights of recent results obtained by the LAT collaboration using this unique dataset. These include new measurements of the diffuse extragalactic background light at high energies, constraints on dark matter annihilation models, and studies of blazars as potential sources for ultra-high energy cosmic rays. I will also discuss some of the challenges associated with analyzing such large datasets. Finally, I will briefly describe plans for future observations that could be made with the LAT. Keywords: High-Energy Astrophysics, Dark Matter, Blazar, Ultra-High Energy Cosmic Ray, Diffuse Extragalactic Background Light. Title: Studies of Cosmic Rays With GeV Gamma Rays",
        "watermark_text": "The Fermi Large Area Telescope ( LAT ) is the primary instrument on NASA s Fermi satellite , launched in June 2008 to study high - energy beta particles and cosmic rays . The LAT has been took images since August 4 , 2008 , and it continues to run today .This discussion will present highlights of recent results derived by the LAT collaboration using this innovative dataset . These include new detection of the diffuse extragalactic background light at high energies , constraints on dark matter annihilation predictions , and investigations of blazars as possible sources for ultra - large energy cosmic rays .I will also discuss some of the challenges associated with analyzing such large datasets . Finally , I will briefly outline plans for future data that might be made with the LAT .Keywords : High - Energy Astrophysics , Dark Matter , Blazar , Ultra - High Energy Cosmic Ray , Diffuse Extragalactic Background Light . Title : Studies of Cosmic Rays With GeV Gamma Rays",
        "rewrite_text": "The Fermi Large Area Telescope (LAT) serves as the core instrument aboard NASA's Fermi satellite, which was launched in June 2008 to explore high-energy beta particles and cosmic rays. The LAT has been capturing images since August 4, 2008, and it remains operational today. This presentation will highlight recent findings from the LAT collaboration utilizing this cutting-edge dataset. Key discoveries include the new detection of diffuse extragalactic background light at high energies, constraints on dark matter annihilation theories, and the examination of blazars as potential sources of ultra-high-energy cosmic rays. Additionally, I will address some challenges encountered in analyzing such extensive datasets. Finally, I will provide a brief overview of plans for future data collection with the LAT. \n\nKeywords: High-Energy Astrophysics, Dark Matter, Blazar, Ultra-High Energy Cosmic Ray, Diffuse Extragalactic Background Light. \n\nTitle: Investigating Cosmic Rays with GeV Gamma Rays.",
        "ori-fast-z-score": 1.5491933384829668,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 2.4735893086356535
    },
    {
        "original_text": "We present the results of searches for gravitational wave signals from compact binary coalescences (CBCs) using data collected by the third and fourth science runs (S3, S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). We use two different search methods to look for CBCs: an unmodeled matched filter method that is sensitive to all possible source orientations; and a modeled template bank method which uses templates based on post-Newtonian expansions of general relativity. The latter method has higher sensitivity but only covers certain regions of parameter space. In this work we report upper limits on the rate density of CBC events as a function of chirp mass and total mass. These are derived under the assumption that the observed event rates follow Poisson statistics with no background noise. For both search methods, our most stringent upper limit is obtained at high masses where the signal-to-noise ratio decreases rapidly due to detector antenna response functions.",
        "watermark_text": "We present the results of investigations for gravitational wave signals from compact binary coalescences ( CBCs ) using data taken by the third and fifth science ran ( S3 , S4 ) of the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) . We use two different search methods to find for CBCs : an unmodeled matched filter method that is sensitive to all possible source orientations ; and a modeled template bank method which uses templates based on post - Newtonian expansions of general relativity .The last approach has larger resolution but only covers certain regions of parameter space . In this research we publish higher restrictions on the frequency density of CBC events as a function of chirp mass and total mass .These are derived under the assumption that the seen event frequencies follow Poisson statistics with no background noise . For both search methods , our most stringent upper maximum is achieved at high masses where the signal - to - noise proportion decreases quickly due to detector antenna reaction functions .",
        "rewrite_text": "We report on our findings from investigations into gravitational wave signals resulting from compact binary coalescences (CBCs), utilizing data collected during the third and fifth science runs (S3 and S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). Our analysis employs two distinct search techniques to identify CBCs: an unmodeled matched filter method that is sensitive to all potential source orientations, and a modeled template bank method based on post-Newtonian expansions from general relativity. While the latter offers greater resolution, it only addresses specific areas of parameter space. In this study, we present tighter constraints on the frequency density of CBC events in relation to chirp mass and total mass. These constraints are calculated under the assumption that the observed event frequencies adhere to Poisson statistics with no background noise. For both search methodologies, our most stringent upper limits occur at high masses, where the signal-to-noise ratio declines rapidly due to the response functions of the detector's antenna.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": 0.22360679774997896
    },
    {
        "original_text": "We present an analytical model to study the effect of mass loading feedback on particle concentration and enstrophy in fully developed turbulence. The cascade process is modeled by using a set of coupled ordinary differential equations, which are derived based on dimensional analysis and Kolmogorov s similarity hypothesis. We show that the presence of particles can significantly affect both the energy transfer rate between different scales as well as the dissipation rates at small scales. In particular, we find that the total amount of energy transferred into smaller scales decreases when there exists significant mass loading feedback. This result suggests that the presence of heavy particles may lead to reduced turbulent mixing efficiency. Finally, our results also indicate that the effects of mass loading feedback become more pronounced if the Stokes number increases or the initial volume fraction of particles becomes larger. Our findings provide useful insights into understanding how heavy particles influence the dynamics of fluid flows. C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "watermark_text": "We present an analytical theory to study the impact of mass displacement feedback on particle concentration and enstrophy in fully developed turbulence . The cascade process is modeled by using a setting of coupled ordinary differential coefficients , which are derived based on dimensional evaluation and Kolmogorov s similarity hypothesis .We see that the presence of particles can significantly affect both the power transfer frequency between various scales as well as the dissipation levels at small scales . In particular , we find that the total quantity of power sent into smaller scales decreases when there exists significant volume displacement feedback .This result suggests that the presence of hard particles may contribute to reduced fluid mixing efficiency . Finally , our findings also suggest that the effects of mass displacement feedback become more pronounced if the Stokes number increases or the initial volume fraction of molecules changes larger .Our findings provide useful insights into studying how heavy grains alter the dynamics of fluid flows . C Author ( s ) : Yi - Chun Chen , Shih - Chieh Hwang , Chia - Hui Wu , Yu - Ting Lin , Ming - Yuan Liu , Chao - Lin Wang , Jie - Sheng Huang , Wen - Ju Tsai , Tzi - Chao Chan , Chin - Fa Lee , Kuo - Yang Chang , Chung - Ming Yeh , Yuan - Kang Chiou , Chien - Nan Chu , Cheng - Wei Hsieh , Chien - Wen Lu , Chien - Chung Wu , Chien - Shu Chen , Chien - Chin Wu , Chien - Chin Yang , Chien - Chin Lai , Chien - Chin Su , Chien - Chin Hung , Chien - Chin Chen , Chien - Ching Wu , Chien - Ching Tai , Chien - Ching Li , Chien - Ching Sun , Chien - Ching Liang , Chien - Ching Chen , Chien - Chong Wu , Chien - Chung Chen , Chien - Chung Lai , Chien - Chung Su , Chien - Chung Hung , Chien - Chung",
        "rewrite_text": "We introduce an analytical theory to examine how mass displacement feedback influences particle concentration and enstrophy in fully developed turbulence. The cascade process is represented by a system of coupled ordinary differential equations, derived from dimensional analysis and Kolmogorov’s similarity hypothesis. Our analysis reveals that particles considerably impact the power transfer frequency across different scales and the dissipation rates at smaller scales. Notably, we observed that the overall power directed towards smaller scales diminishes when there is considerable volume displacement feedback. This finding implies that the presence of rigid particles may lead to a decrease in fluid mixing efficiency. Additionally, our results indicate that the effects of mass displacement feedback intensify with an increase in the Stokes number or a larger initial volume fraction of molecules. Our research offers valuable insights into how heavy particles modify the dynamics of fluid flows. \n\nAuthors: Yi-Chun Chen, Shih-Chieh Hwang, Chia-Hui Wu, Yu-Ting Lin, Ming-Yuan Liu, Chao-Lin Wang, Jie-Sheng Huang, Wen-Ju Tsai, Tzi-Chao Chan, Chin-Fa Lee, Kuo-Yang Chang, Chung-Ming Yeh, Yuan-Kang Chiou, Chien-Nan Chu, Cheng-Wei Hsieh, Chien-Wen Lu, Chien-Chung Wu, Chien-Shu Chen, Chien-Chin Wu, Chien-Chin Yang, Chien-Chin Lai, Chien-Chin Su, Chien-Chin Hung, Chien-Chin Chen, Chien-Ching Wu, Chien-Ching Tai, Chien-Ching Li, Chien-Ching Sun, Chien-Ching Liang, Chien-Ching Chen, Chien-Chong Wu, Chien-Chung Chen, Chien-Chung Lai, Chien-Chung Su, Chien-Chung Hung, Chien-Chung.",
        "ori-fast-z-score": 0.7844645405527362,
        "water-fast-z-score": 7.060180864974626,
        "rewrite-fast-z-score": 1.1406468642034677
    },
    {
        "original_text": "We present the results of an analysis of the anisotropy in the distribution of satellite galaxies around isolated field galaxies, using data obtained by the Sloan Digital Sky Survey (SDSS). We find that there is no significant difference between the distributions for satellites with different luminosities or colors and those found around central cluster galaxies. The observed anisotropies are consistent with predictions based on tidal forces acting during galaxy mergers. This suggests that these effects may be responsible for the formation of both clusters and groups of galaxies. \n \n Keywords: Galaxy merger, Group/cluster of galaxies, Tidal stripping, SDSS, Isolated galaxy \n \n \n \n 1 Introduction \n \n Clusters of galaxies contain many thousands of galaxies which reside within a common dark matter halo. These systems form through gravitational collapse driven by the mutual attraction of their constituent galaxies. However, it remains unclear how this process occurs over time-scales ranging from individual galaxy interactions to the assembly of massive clusters containing hundreds of member galaxies. In particular, we do not know whether all galaxies evolve into members of large clusters or if some fraction remain as isolated field galaxies throughout cosmic history. \n \n 2 Previous Work \n \n Several studies have investigated the properties of satellite galaxies surrounding brightest cluster galaxies (BCGs) at low redshifts z < 0.1. For example, Carlberg et al. (1997), Lin & Mohr (2004a), and Hansen et al. (2005) used samples of BCG-satellite pairs selected from optical surveys such as the Palomar Observatory Sky Survey (POSS-II; Reid et al., 1991) and the Sloan Digital Sky Surveys (SDSS; York et al., 2000). They found that the number density profiles of satellite galaxies show strong deviations from spherical symmetry, indicating that they are distributed anisotropically about their host galaxies. Furthermore, they showed that the degree of anisotropy depends strongly on the projected distance from the center of the host galaxy. At small distances, the radial profile shows a steep decline towards the center of the host while the tangential component increases rapidly beyond a characteristic radius R",
        "watermark_text": "We present the conclusion of an assessment of the anisotropy in the distribution of satellite galaxies around dispersed field galaxies , using data derived by the Sloan Digital Sky Survey ( SDSS ) . We see that there is no major variation between the distributions for satellites with various luminosities or colors and those found around central cluster clusters .The observed anisotropies are compatible with predictions based on tidal forces acting during galaxy mergers . This implies that these influences might be responsible for the formation of both clusters and groups of clusters .Keywords : Galaxy consolidation , Group / cluster of galaxies , Tidal stripping , SDSS , Isolated galaxy 1 Introduction Clusters of galaxies contain many thousands of galaxies which reside within a common dark matter halo . These systems create through gravity collapse driven by the mutual proximity of their constituent galaxies .However , it remains unsure how this process occurs over time - scales ranging from individual galaxy encounters to the assembly of vast clusters containing hundreds of member galaxies . In particular , we do not understand whether all galaxies evolve into members of large clusters or if some fraction remain as isolated ring galaxies throughout cosmic history .2 Previous Work Several studies have explored the properties of satellite galaxies surrounding brightest cluster clusters ( BCGs ) at low redshifts z < 0 . 1 . For instance , Carlberg et al .( 1997 ) , Lin & Mohr ( 2004a ) , and Hansen et al . ( 2005 ) used data of BCG - satellite pairs selected from optical surveys such as the Palomar Observatory Sky Survey ( POSS - II ; Reid et al . , 1991 ) and the Sloan Digital Sky Surveys ( SDSS ; York et al . , 2000 ) .They found that the number density patterns of satellite galaxies show strong deviations from spherical symmetry , showing that they are distributed anisotropically about their host galaxies . Furthermore , they demonstrated that the degree of anisotropy depends strongly on the projected distance from the hub of the host galaxy .At small distances , the radial profile displays a sharp decline towards the hub of the host while the tangential component increases quickly beyond a typical radius R",
        "rewrite_text": "We present the findings of an assessment regarding the anisotropic distribution of satellite galaxies surrounding dispersed field galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our analysis indicates that there is no significant variation in the distributions of satellites with differing luminosities or colors compared to those found around central cluster galaxies. The anisotropies we observed align with theoretical predictions based on the tidal forces exerted during galaxy mergers. This suggests that such influences may play a crucial role in the formation of both clusters and groups of galaxies.\n\n**Keywords:** Galaxy formation, Groups/clusters of galaxies, Tidal stripping, SDSS, Isolated galaxies\n\n**1 Introduction**  \nClusters of galaxies harbor thousands of individual galaxies within a shared dark matter halo. These systems form through gravitational collapse driven by the proximity of their constituent galaxies. However, the exact mechanisms of this process over timescales—from interactions between single galaxies to the assembly of massive clusters with hundreds of member galaxies—remain uncertain. In particular, it is unclear whether all galaxies eventually merge into large clusters or if a subset remains as isolated ring galaxies throughout cosmic history.\n\n**2 Previous Work**  \nNumerous studies have investigated the characteristics of satellite galaxies surrounding brightest cluster galaxies (BCGs) at low redshifts (z < 0.1). Research by Carlberg et al. (1997), Lin & Mohr (2004a), and Hansen et al. (2005) employed data from BCG-satellite pair observations sourced from optical surveys, including the Palomar Observatory Sky Survey (POSS-II; Reid et al., 1991) and the Sloan Digital Sky Survey (SDSS; York et al., 2000). Their findings reveal that the number density patterns of satellite galaxies exhibit significant departures from spherical symmetry, indicating an anisotropic distribution around their host galaxies. Additionally, they demonstrated that the level of anisotropy is closely related to the projected distance from the center of the host galaxy, with a notable sharp decline in the radial profile near the hub, and a rapid increase in the tangential component beyond a typical radius, R.",
        "ori-fast-z-score": 0.242535625036333,
        "water-fast-z-score": 5.640760748177662,
        "rewrite-fast-z-score": 0.16116459280507606
    },
    {
        "original_text": "The detection and study of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes (IACTs). The background produced by these events can be reduced using different techniques, such as cuts on shower parameters or image cleaning algorithms. In this work we present an alternative method to reduce the background based on machine learning techniques. We use Random Forest classifiers trained with simulated data to identify cosmic-ray images among all recorded IACT images. This approach allows us to obtain results similar to those obtained with other methods but at lower computational cost. Finally, we apply our technique to real data taken with HESS-II telescope during its first year of operation. Our analysis shows that it is possible to improve the quality of reconstructed gamma-ray showers while reducing the number of rejected hadronic showers. \n \n Keywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower",
        "watermark_text": "The observation and investigation of cosmic rays is one of the main goals for imaging atmospheric Cherenkov telescopes ( IACTs ) . The background formed by these events can be reduced use different methods , such as cuts on shower parameters or image cleaning algorithms .In this project we present an additional method to reduce the background based on machine learning techniques . We use Random Forest classifiers trained with simulated evidence to identify cosmic - ray pictures among all collected IACT images .This method enables us to obtain results comparable to those acquired with other methods but at lower mathematical price . Finally , we apply our technique to real information taken with HESS - II telescope during its initial season of operation .Our study shows that it is possible to upgrade the performance of reconstructed alpha - ray showers while reducing the quantity of rejected hadronic showers . Keywords : Machine Learning ; Cosmic Ray Identification ; Image Cleaning ; Gamma - ray Astrophysics ; Hadronic Shower",
        "rewrite_text": "The observation and study of cosmic rays are primary objectives for imaging atmospheric Cherenkov telescopes (IACTs). Various methods, such as applying cuts on shower parameters or implementing image cleaning algorithms, can help reduce the background created by these events. In this project, we introduce an additional approach that utilizes machine learning techniques for background reduction. We employ Random Forest classifiers trained on simulated data to distinguish cosmic-ray images from the array of collected IACT images. This method allows us to achieve results comparable to those obtained through traditional techniques but at a lower computational cost. Ultimately, we apply our approach to actual data gathered from the HESS-II telescope during its initial operating season. Our findings demonstrate that it is possible to enhance the performance of reconstructed cosmic-ray showers while minimizing the number of rejected hadronic showers. \n\nKeywords: Machine Learning; Cosmic Ray Identification; Image Cleaning; Gamma-ray Astrophysics; Hadronic Shower",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "In this work, we consider the problem of scanning data in multi-dimensional space with noisy measurements. We propose an algorithm that sequentially decides which dimension to scan next based on the current measurement vector. The proposed method is shown to be optimal under certain conditions. In addition, it can also handle cases where there are multiple targets present simultaneously. Finally, numerical results show that our approach outperforms existing methods by orders of magnitude. \n \n Keywords: Scanning, sequential decision making, multi-object tracking, information-theoretic analysis \n \n 1 Introduction \n \n In many applications such as radar detection  1  , sonar  2  or computer vision  3  , one needs to detect objects (e.g., aircrafts) in multi-dimensional spaces using limited resources. For example, in air traffic control  4  , radars need to track several aircrafts at once while minimizing false alarms due to clutter noise. This task requires efficient algorithms to decide how to allocate available resources among different dimensions so that the overall performance is optimized. A common strategy used in these problems is to perform scans along each dimension separately until some stopping criterion is met. However, this may lead to suboptimal solutions since the best solution depends not only on the current measurement but also on future measurements. Therefore, it becomes necessary to develop new techniques to solve these problems more efficiently. \n \n In recent years, significant progress has been made towards solving various resource allocation problems related to multi-target tracking  5  . Most of them focus on optimizing the number of sensors  6  , their locations  7, 8  , or the sensor network topology  9  . These works assume that all target states are known exactly before performing any optimization. However, in practice, target state estimates are often uncertain because they are obtained through noisy measurements  10  . As a result, the aforementioned approaches cannot guarantee global optimality when applied directly to practical scenarios  11  . \n \n To address this issue, researchers have developed robust versions of classical resource allocation strategies  12  . They typically use worst-case formulations  13  to ensure that the resulting allocations remain feasible even if the true target states deviate significantly...",
        "watermark_text": "In this research , we investigate the question of scanning data in multi - dimensional space with noisy measurements . We suggest an algorithm that sequentially decides which dimension to scan next depending on the current measurement vector .The proposed approach is demonstrated to be appropriate under certain conditions . In addition , it can also handle cases where there are multiple targets present concurrently .Finally , numerical findings show that our approach outperforms current methods by orders of magnitude . Keywords : Scanning , sequential decision making , multi - object sensing , info - theoretic analysis 1 Introduction In many applications such as radar detection 1 , sonar 2 or computer vision 3 , one needs to identify images ( e . g . , aircrafts ) in multi - dimensional spaces utilizing limited resources .For instance , in airline traffic control 4 , radars need to track numerous aircrafts at once while minimizing false alarms due to clutter sound . This job needs efficient techniques to choose how to allocate available resources among different dimensions so that the overall performance is optimized .A typical strategy used in these problems is to conduct scans along each dimension separately until some stops criterion is reached . However , this might lead to suboptimal solutions since the best solution depends not only on the present observation but also on future measurements .Therefore , it becomes necessary to develop new tactics to solve these problems more efficiently . In recent seasons , substantial advances has been achieved towards solving various resource transfer problems related to multi - target tracking 5 .Most of them rely on optimizing the number of measurements 6 , their destinations 7 , 8 , or the sensor network topology 9 . These works assume that all target states are known exactly before performing any optimization .However , in practice , target state measurements are often uncertain because they are derived through noisy measurements 10 . As a result , the aforementioned approaches lack guarantee global optimality when applied directly to practical situations 11 .To address this question , researchers have developed stable editions of classical asset distribution tactics 12 . They generally using worst - case formulations 13 to ensure that the resulting allocations remain viable even if the true target states deviate substantially . . .",
        "rewrite_text": "In this research, we explore the issue of scanning data in multi-dimensional spaces when faced with noisy measurements. We present an algorithm that sequentially determines the next dimension to scan based on the current measurement vector. Our proposed method is shown to be effective under specific conditions and is capable of handling scenarios with multiple targets present simultaneously. Furthermore, our numerical results indicate that our approach significantly outperforms existing methods by substantial margins. \n\n**Keywords:** Scanning, sequential decision making, multi-object sensing, info-theoretic analysis\n\n**1 Introduction:** \n\nIn various applications, such as radar detection, sonar, and computer vision, the need arises to identify images (e.g., aircraft) within multi-dimensional spaces while utilizing limited resources. For example, in airline traffic control, radars must track multiple aircraft simultaneously while minimizing false alarms caused by background noise. This task demands effective techniques to allocate available resources across different dimensions to optimize overall performance. \n\nA common strategy is to scan each dimension independently until a stopping criterion is met. However, this method may lead to suboptimal outcomes because the best solution depends not only on current observations but also on future measurements. Consequently, developing new strategies for more efficient problem-solving becomes necessary. \n\nRecent advancements have been made in addressing various resource allocation challenges related to multi-target tracking. Most of these approaches focus on optimizing the number of measurements, their destinations, or the sensor network topology. These studies generally assume that all target states are precisely known prior to optimization. However, in practice, measurements of target states are often uncertain due to noise, which undermines the global optimality of these methods when applied to real-world scenarios. \n\nTo tackle this problem, researchers have proposed robust variations of classical resource distribution strategies, typically employing worst-case formulations to ensure that the resulting allocations remain feasible even if the actual target states deviate significantly.",
        "ori-fast-z-score": 1.3127849234810511,
        "water-fast-z-score": 9.286096001408149,
        "rewrite-fast-z-score": 1.6131182652348863
    },
    {
        "original_text": "We present new near-infrared (NIR) and millimeter-wave observations toward the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 December 8-9 under photometric conditions. We detected no point sources down to Ks = 20 mag within an area of 0.5 arcmin2 centered on the peak position of the dust continuum emission observed by SCUBA-2 on JCMT. In addition, we found that there are two peaks in the 1.3 mm continuum map made with MAMBO-II on IRAM 30 m telescope. These results suggest that this object may be a protostellar candidate or a prestellar core surrounded by infalling envelopes. To investigate its dynamical state further, we carried out high-resolution interferometric observations with Nobeyama 45-m radio telescope. Our results show that the central part of the core has a velocity gradient along the east-west direction, suggesting that it is collapsing.",
        "watermark_text": "We report new near - infrared ( NIR ) and millimeter - wave images toward the starless rich core FeSt 1 - 457 , which is situated in the Taurus molecular dust complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI camera on 2005 December 8 - 9 under photometric circumstances .We observed no point sources down to Ks = 20 mag within an area of 0 . 5 arcmin2 centered on the peak status of the dust continuum emission observed by SCUBA - 2 on JCMT . In addition , we reported that there are two peaks in the 1 . 3 cm continuum image produced with MAMBO - II on IRAM 30 m observatory .These data suggest that this body may be a protostellar candidate or a prestellar core flanked by infalling envelopes . To explore its dynamical state further , we conducted out large - resolution interferometric observations with Nobeyama 45 - m radio telescope .Our results show that the main region of the core has a speed gradient along the east - west direction , showing that it is sinking .",
        "rewrite_text": "We present new near-infrared (NIR) and millimeter-wave images of the starless rich core FeSt 1-457, located in the Taurus molecular dust complex at a distance of 140 pc. The NIR data were acquired with the Subaru Observatory using the SofI camera on December 8-9, 2005, under photometric conditions. Within a 0.5 arcmin² area centered on the peak of the dust continuum emission detected by SCUBA-2 at the JCMT, we found no point sources down to a magnitude of Ks = 20. Additionally, our findings reveal two peaks in the 1.3 cm continuum image obtained from MAMBO-II at the IRAM 30 m Observatory. These observations suggest that this region may be a protostellar candidate or a prestellar core surrounded by infalling envelopes. To further investigate its dynamical state, we conducted high-resolution interferometric observations using the Nobeyama 45-m radio telescope. Our results indicate that the core's main region exhibits a speed gradient in the east-west direction, suggesting it is in a state of contraction.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 1.3643820804812932
    },
    {
        "original_text": "We report on the discovery of new, bright X-ray emission from the central region of the galaxy cluster Abell 2597 (z = 0.0176). The source is spatially coincident with the nucleus of the elliptical galaxy NGC 1365 and has been detected by both Chandra ACIS-S3 and XMM-Newton EPIC-PN cameras during their respective observations taken between 2003 and 2005. We find that this newly discovered activity can be described as a series of short-lived bursts lasting for about 100 s each. These events are separated by longer periods of quiescence which last up to several hours. During these active phases we measure a luminosity of Lx ~ 1043 erg/s at 2-10 keV. This corresponds to a bolometric luminosity of Lbol ~ 1044 erg/s assuming a blackbody temperature of kTBB ~ 50-100 eV. Such high luminosities cannot be explained within standard accretion disk models but require super-Eddington rates or relativistic jets.",
        "watermark_text": "We report on the discovery of new , brilliant X - ray radiation from the central region of the galaxy community Abell 2597 ( z = 0 . 0176 ) . The source is spatially coincident with the nucleus of the elliptical galaxy NGC 1365 and has been detected by both Chandra ACIS - S3 and XMM - Newton EPIC - PN sensors during their respective observations made between 2003 and 2005 .We see that this newly discovered activity can be described as a sequence of short - lived bursts lasting for about 100 s each . These episodes are separated by longer periods of quiescence which go up to several hours .During these active phases we measure a luminosity of Lx ~ 1043 erg / s at 2 - 10 keV . This relates to a bolometric luminosity of Lbol ~ 1044 erg / s assuming a blackbody altitude of kTBB ~ 50 - 100 eV .Such high luminosities cannot be described within conventional accretion disk theories but need super - Eddington rates or relativistic jets .",
        "rewrite_text": "We report the discovery of remarkable X-ray emissions from the central region of the galaxy cluster Abell 2597 (z = 0.0176). This source is spatially aligned with the nucleus of the elliptical galaxy NGC 1365 and was observed by both the Chandra ACIS-S3 and XMM-Newton EPIC-PN instruments during their observations from 2003 to 2005. The newly identified activity manifests as a series of short bursts, each lasting approximately 100 seconds, interspersed with longer periods of inactivity that can last several hours. During these active intervals, we observe a luminosity of Lx ~ 10^43 erg/s in the 2-10 keV range, which corresponds to a bolometric luminosity of Lbol ~ 10^44 erg/s, assuming a blackbody temperature of kTBB ~ 50-100 eV. Such high luminosities cannot be accounted for by standard accretion disk models and indicate the presence of super-Eddington rates or relativistic jets.",
        "ori-fast-z-score": -0.2773500981126146,
        "water-fast-z-score": 3.983456354511982,
        "rewrite-fast-z-score": -0.42857142857142855
    },
    {
        "original_text": "We present an approach to type isomorphism based on game semantics, which is the most general framework for interpreting functional languages in terms of strategies and plays between them.  We show that our approach can be used to prove second-order type isomorphisms such as those arising from parametric polymorphism or dependent types.   Our results are obtained by combining techniques from category theory with ideas from computational complexity theory. The main technical contribution consists of showing how to construct games corresponding to certain classes of formulas in propositional logic (e.g., Horn clauses) using only polynomial resources. This result has several applications including proving decidability of problems related to higher-order unification modulo theories. In this work we study the problem of constructing games corresponding to certain classes  of formulas in propositional calculus (e.g., horn clauses). These games will then serve as models for various logics, e.g., modal logic, temporal logic, etc..",
        "watermark_text": "We present an view to type isomorphism based on game semantics , which is the most general template for interpreting functional languages in terms of strategies and plays between them . We see that our approach can be used to prove second - order sort isomorphisms such as those originating from parametric polymorphism or dependent types .Our results are derived by combining techniques from category theory with ideas from theoretical complexity analysis . The main technical effort consists of finding how to build games corresponding to specified classes of formulas in propositional logic ( e . g . , Horn clauses ) using only polynomial resources .This result has numerous uses including proof decidability of problems related to higher - order unified modulo theories . In this study we study the question of constructing matches related to specified classes of formulas in propositional calculus ( e . g . , horn clauses ) .These programs will then serve as tools for various logics , e . g . , modal logic , temporal logic , etc . .",
        "rewrite_text": "We introduce a view of type isomorphism grounded in game semantics, which serves as the most comprehensive framework for interpreting functional languages through the lens of strategies and interactions. Our approach demonstrates efficacy in proving second-order sort isomorphisms that arise from concepts such as parametric polymorphism and dependent types. Our findings leverage techniques from category theory in conjunction with insights from theoretical complexity analysis. A significant aspect of our work is dedicated to constructing games that correspond to specific classes of propositional logic formulas (such as Horn clauses) using only polynomial resources. This result has a wide range of applications, including establishing the decidability of issues related to higher-order unified modulo theories. In this study, we focus on the problem of constructing matches associated with targeted classes of formulas in propositional calculus, including Horn clauses. These programs will subsequently be utilized as tools across various logical systems, including modal logic and temporal logic.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.661385170722978,
        "rewrite-fast-z-score": 1.118033988749895
    },
    {
        "original_text": "We present the results of an extensive study of gas dynamics, star formation activity, dust extinction, stellar populations, and black hole accretion properties for a strongly lensed galaxy (A1689-zD1) at redshift 3.07. The lensing magnification factor is ~30Â±5. We use deep near-infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that this system consists of two merging galaxies separated by 1 kpc along the line-of-sight. One of these components shows strong HÎ² emission indicative of active galactic nuclei (AGN). This AGN component has a mass of âˆ¼10^9 M_sol , which corresponds to a supermassive black hole with a mass of âˆ½â€“1 Ã— 10^8 M_sol . Using our spatially resolved measurements we find evidence for intense nuclear starbursts on scales as small as 100 pc.",
        "watermark_text": "We present the conclusion of an extensive research of gas evolution , star formation activity , dust disappearance , stars populations , and dark hole accretion properties for a strongly lensed galaxy ( A1689 - zD1 ) at redshift 3 . 07 . The lensing magnification factor is ~ 30Â±5 .We use deep near - infrared spectroscopy to measure the kinematics of molecular hydrogen emission lines with high visual resolution . Our observations show that this scheme consists of two joining galaxies linked by 1 kpc along the line - of - view .One of these constituents exhibits strong HÎ² emission indicative of active galactic nuclei ( AGN ) . This AGN component has a mass of [UNK] ^ 9 M _ sol , which corresponds to a supermassive black hole with a mass of [UNK] “ 1 [UNK] — 10 ^ 8 M _ sol .Using our spatially resolved sensors we find proof for intense nuclear starbursts on sizes as low as 100 pc .",
        "rewrite_text": "We present the findings of an extensive study on gas evolution, star formation activity, dust depletion, stellar populations, and black hole accretion properties in the strongly lensed galaxy A1689-zD1 at redshift 3.07, with a lensing magnification factor of approximately 30±5. Utilizing deep near-infrared spectroscopy, we measured the kinematics of molecular hydrogen emission lines with high spatial resolution. Our observations reveal that this system comprises two interconnected galaxies, separated by 1 kpc along the line of sight. One of these components shows significant Hβ emission, suggesting the presence of active galactic nuclei (AGN). The AGN component has a mass of approximately 10^9 M☉, which is consistent with a supermassive black hole with a mass between 10^8 and 10^9 M☉. Through our spatially resolved observations, we also found evidence of intense nuclear starbursts occurring over areas as small as 100 pc.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The lectures were given by David Gross at the Cargese Summer Institute in Corsica, France during August 2005.  The lecture notes are available online as PDF files and can be downloaded for free.   These lecture notes cover topics such as:  - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D-branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy-momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M-theory - Other approaches - Comments on future directions - References - Index   This is an excellent resource for learning about many aspects of modern theoretical physics including superstrings, supergravities, black holes, cosmology, time evolution, conformal invariance, renormalization group flows, dualities, M-theory, other approaches etc...",
        "watermark_text": "The presentations were given by David Gross at the Cargese Summer Institute in Corsica , France during August 2005 . The lecture notes are available digital as PDF files and can be downloaded for free .These lecture notes cover subjects such as : - Introduction to string theory - Gauge fields and gauge symmetries - Supersymmetry - Supergravity - String field theories - D - branes - Open strings - Closed strings - Tachyons - Bosonic open strings - Fermionic open strings - Vacuum expectation values - Energy - momentum tensors - Interactions between branes - General relativity - Gravity induced on a brane - Black holes - Cosmology - Time evolution - Conformal invariance - Renormalization group flow - Dualities - M - theory - Other approaches - Comments on future paths - References - Index This is an excellent resource for learning about various parts of modern conceptual mechanics namely superstrings , supergravities , white holes , cosmology , time progression , conformal invariance , renormalization group flows , dualities , M - theory , other methods etc . . .",
        "rewrite_text": "David Gross delivered presentations at the Cargese Summer Institute in Corsica, France, in August 2005. The lecture notes from these presentations are available online as free downloadable PDF files. These notes encompass a wide range of topics, including:\n\n- Introduction to string theory\n- Gauge fields and gauge symmetries\n- Supersymmetry\n- Supergravity\n- String field theories\n- D-branes\n- Open strings\n- Closed strings\n- Tachyons\n- Bosonic open strings\n- Fermionic open strings\n- Vacuum expectation values\n- Energy-momentum tensors\n- Interactions between branes\n- General relativity\n- Gravity on a brane\n- Black holes\n- Cosmology\n- Time evolution\n- Conformal invariance\n- Renormalization group flow\n- Dualities\n- M-theory\n- Other approaches\n- Comments on future directions\n- References\n- Index\n\nThese materials serve as an excellent resource for exploring various topics in modern conceptual physics, including superstrings, supergravities, white holes, cosmology, time dynamics, conformal invariance, renormalization group flows, dualities, M-theory, and other methodologies.",
        "ori-fast-z-score": 2.54000254000381,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 1.6378460497066512
    },
    {
        "original_text": "We study the possibility that nonstandard interactions (NSI) between neutrinos and matter can be probed by using solar and reactor neutrino data simultaneously, in particular through their combined effect on the survival probability P(νe→νe). We find that NSI parameters are constrained to values below 0.1 for most combinations of standard oscillation parameters allowed at 3σ CL by current global fits. The strongest constraints arise when combining solar and KamLAND data sets. In this case we obtain upper bounds on |εee|, |εµτ | < 0.06 − 0.07 depending on the value of θ13. These results improve upon previous limits obtained from solar or reactor experiments alone. \n \n Introduction \n \n Neutrino oscillations have been observed in many different types of experiments  1  . However, there is still no direct evidence for the existence of new physics beyond the Standard Model (SM), such as sterile neutrinos  2  , lepton number violation  3  , extra dimensions  4  , supersymmetry  5  , etc.. Many extensions of the SM predict additional contributions to the effective four-fermion interaction Lagrangian  6  which could lead to observable deviations from the predictions of the SM  7, 8  . For example, it has recently been shown  9  that some models of quantum gravity  10  may induce an energy dependent refractive index n = 1 + εE/E0 where E0 is a characteristic scale associated with the underlying theory  11  . This would result in a modification of the vacuum mixing angle sin2θ12 = 1−cos2θ12 ≈ 1+ε/2+O(ε3)  12  leading to potentially large effects on the propagation of neutrinos  13  .\n \nIn addition to these theoretical motivations, there exist several experimental indications pointing towards possible new physics beyond the SM  14  : i) Large atmospheric  15  and solar  16  neutrino flux deficits; ii) LSND  17  and MiniBooNE  18  anomalies indicating short-baseline νμ → νe appearance transitions not predicted within three-flavor neutrino oscillations  19  ; iii) Anomalies in the measurement of the muon anomalous magnetic moment",
        "watermark_text": "We research the idea that nonstandard relationships ( NSI ) between neutrinos and material can be probed by using sun and reactor neutrino data simultaneously , in particular through their combined influence on the life probability P ( νe→νe ) . We see that NSI variables are constrained to parameters below 0 . 1 for most combinations of standard oscillation measurements permitted at 3σ CL by current global fits .The strongest limits arise when combining solar and KamLAND data sets . In this situation we obtain higher bounds on | εee | , | εµτ | < 0 . 06 − 0 . 07 depending on the value of θ13 .These data improve upon recent limits acquired from solar or reactor tests alone . Introduction Neutrino oscillations have been observed in multiple diverse kinds of studies 1 .However , there is nevertheless no clear proof for the existence of new theory beyond the Standard Model ( SM ) , such as sterile neutrinos 2 , lepton size violation 3 , extra dimensions 4 , supersymmetry 5 , etc . . Many modifications of the SM predict additional contributions to the effective four - fermion collision Lagrangian 6 which potentially contribute to observable deviations from the estimates of the SM 7 , 8 .For instance , it has recently been shown 9 that some models of quantum gravitational 10 may generate an energy dependent refractive index n = 1 + εE / E0 where E0 is a typical scale identified with the underlying theory 11 . This might lead in a modification of the vacuum mixing ratio sin2θ12 = 1−cos2θ12 ≈ 1 + ε / 2 + O ( ε3 ) 12 leading to potentially large effects on the propagation of neutrinos 13 .In addition to these theoretical motivations , there remain various experimental indications pointing towards possible new science beyond the SM 14 : i ) Large environmental 15 and solar 16 neutrino flux deficits ; ii ) LSND 17 and MiniBooNE 18 anomalies suggesting short - baseline νμ → νe appearance changes not anticipated within three - flavor neutrino oscillations 19 ; iii ) Anomalies in the determination of the muon anomalous magnetic moment",
        "rewrite_text": "We investigate the concept that nonstandard interactions (NSI) between neutrinos and matter can be explored using data from both solar and reactor neutrinos simultaneously, specifically by examining their joint effect on the survival probability P(νe → νe). Our findings indicate that NSI parameters are confined to values below 0.1 for most combinations of standard oscillation measurements allowed at the 3σ confidence level by current global analyses. The most stringent constraints are obtained when merging solar and KamLAND datasets, resulting in upper limits on | εee | and | εμτ | of approximately 0.06 to 0.07, depending on the value of θ13. These results surpass the recent limits derived from solar or reactor data considered in isolation.\n\nIntroduction: Neutrino oscillations have been documented in a variety of studies; however, clear evidence for new theories beyond the Standard Model (SM) remains elusive, such as sterile neutrinos, lepton flavor violation, extra dimensions, and supersymmetry. Various modifications to the SM suggest the potential for additional contributions to the effective four-fermion interaction Lagrangian, which might lead to observable deviations from SM predictions. Recent research indicates that certain quantum gravitational models could result in an energy-dependent refractive index described by n = 1 + εE/E0, where E0 represents a characteristic scale of the underlying theory. This modification could alter the vacuum mixing ratio, sin²θ12 = 1−cos²θ12 ≈ 1 + ε/2 + O(ε³), significantly affecting neutrino propagation. Alongside these theoretical motives, several experimental observations suggest the possibility of new phenomena beyond the SM, including: i) notable deficits in environmental and solar neutrino flux; ii) anomalies from LSND and MiniBooNE hinting at unexpected νμ → νe transitions that deviate from predictions of three-flavor oscillations; iii) discrepancies in the measurement of the muon anomalous magnetic moment.",
        "ori-fast-z-score": -0.4508348173337161,
        "water-fast-z-score": 7.844525821606661,
        "rewrite-fast-z-score": -0.4508348173337161
    },
    {
        "original_text": "The shock temperatures for molybdenum were determined by measuring the electrical resistance of samples shocked to pressures up to 1,000 kilobars (1 Mbar). The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at various levels of pressure. \n \n Shocks produced by a pulsed power machine were used to compress the sample material between two electrodes. A voltage pulse applied across these electrodes caused current flow through the compressed material which generated Joule heating. This heat increased the resistivity of the material causing it to expand rapidly as its temperature rose above the Curie point. As this expansion occurred, the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample. When the current reached a critical value, the sample exploded releasing most of its stored energy. The explosion also destroyed one or both of the electrodes so that no further measurement could be made until new ones were installed.",
        "watermark_text": "The jolt pressures for molybdenum were determined by monitoring the electrical resistance of samples shocked to pressures up to 1 , 000 kilobars ( 1 Mbar ) . The return temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at several degrees of pressure .Shocks created by a pulsed power machine were used to compress the sample material between two electrodes . A voltage beam applied across these electrodes induced charge flow through the compressed material which generated Joule heating .This temperature improved the resistivity of the material creating it to expand rapidly as its temperature climbed above the Curie point . As this growth occurred , the tolerance fell significantly causing in a sharp increase in current flowing into the sample .When the current reached a critical level , the sample erupted releasing most of its stored power . The explosion also damaged one or both of the electrodes so that no further measurement could be made until fresh ones were introduced .",
        "rewrite_text": "The jolt pressures for molybdenum were assessed by tracking the electrical resistance of samples subjected to pressures of up to 1,000 kilobars (1 Mbar). The temperature during recovery was recorded using an optical pyrometer on samples that had been laser-heated following shock exposure at various pressure levels. A pulsed power machine was utilized to compress the sample material between two electrodes. A voltage beam applied across these electrodes induced charge flow through the compressed material, resulting in Joule heating. This increased temperature enhanced the material's resistivity, leading to rapid expansion as the temperature exceeded the Curie point. As this expansion took place, the resistance markedly decreased, causing a sharp rise in the current flowing through the sample. When the current reached a critical threshold, the sample erupted, releasing most of its stored energy. This explosion typically damaged one or both electrodes, preventing further measurements until replacements could be installed.",
        "ori-fast-z-score": 1.0256451881367414,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 1.835325870964494
    },
    {
        "original_text": "We present results for the pion mass dependence of several observables in nuclear matter, obtained with chiral effective field theory at next-to-leading order (NLO). We consider the nucleon scalar density and spin polarization functions as well as the isovector vector current-current correlation function. The latter quantity can be related to the longitudinal part of the electric polarizability of the neutron. In addition we study the energy per particle in symmetric nuclear matter and the symmetry energy coefficient C_s4. Our calculations are performed within an extended framework that allows us to include finite-range effects beyond standard local potentials. This is achieved by including explicit delta degrees of freedom into our formalism. For all quantities considered here we find very good agreement between theoretical predictions based on this approach and available experimental data over a wide range of values of the pion mass. Furthermore, we compare our results to those obtained using other approaches such as relativistic mean-field models or lattice QCD simulations.",
        "watermark_text": "We present results for the pion mass dependence of several observables in nuclear material , obtained with chiral effective field theory at next - to - leading order ( NLO ) . We consider the nucleon scalar distribution and spin polarization functions as well as the isovector matrix current - current interaction function .The latter quantity can be connected to the longitudinal portion of the electric polarizability of the neutron . In addition we study the power per particle in symmetric nuclear material and the symmetry power coefficient C _ s4 .Our calculations are performed within an extended context that enables us to use finite - range effects beyond standard local potentials . This is achieved by including explicit delta degrees of liberty into our formalism .For all parameters used here we find very high agreement between theoretical estimates based on this methodology and accessible empirical data over a broad variety of values of the pion mass . Furthermore , we compare our findings to those achieved using other methods such as relativistic mean - field models or lattice QCD simulations .",
        "rewrite_text": "We present findings on the dependence of several observables in nuclear matter on pion mass, utilizing chiral effective field theory at next-to-leading order (NLO). Our analysis includes the nucleon scalar distribution, spin polarization functions, and the isovector matrix current-current interaction function, which is related to the longitudinal component of the neutron's electric polarizability. Additionally, we examine the power per particle in symmetric nuclear matter and the symmetry power coefficient \\( C_{s4} \\). Our calculations are conducted within an enhanced framework that incorporates finite-range effects beyond traditional local potentials by explicitly including delta degrees of freedom. We observe a strong agreement between theoretical predictions derived from this approach and available empirical data across a wide range of pion mass values. Moreover, we compare our results with those obtained from other methodologies, such as relativistic mean-field models and lattice QCD simulations.",
        "ori-fast-z-score": 0.10976425998969035,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 0.6974858324629157
    },
    {
        "original_text": "We study the zero temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using an evolutionary algorithm, called genetic embedded matching heuristic (GEMH). We find that GEMH is able to reproduce the ground states obtained by simulated annealing and Monte Carlo simulations for different system sizes upto L=40. The energy distribution function shows a power law behavior at low energies indicating the presence of many metastable states. In addition we also observe a peak near E=0 which corresponds to the ground state configurations. Finally, we show that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration. This indicates that there are no other low-energy states apart from the ground state. \n \n 1 Introduction \n \n Spin glasses have been studied extensively over last few decades both theoretically  1 - 3  and experimentally  4  . They exhibit interesting features like frustration  5  , slow relaxation  6  -  8  etc., which make them very difficult to solve exactly even on small lattices  9  . However, it has been shown recently  10  that these systems can be solved efficiently if they are allowed to evolve under certain conditions  11  -  13  . Evolutionary algorithms  14  -  16  provide us with powerful tools to tackle such problems  17  -  20  .\n \nIn this work we consider the following Hamiltonian  21  :",
        "watermark_text": "We research the zero temperature ( ground state ) characteristics of the two - dimensional spin - glass model with nearest - neighbor interactions using an phylogenetic algorithm , known genetic embedded matching heuristic ( GEMH ) . We see that GEMH is easy to predict the ground states achieved by simulated annealing and Monte Carlo simulations for different system sizes upto L = 40 .The energy distribution function shows a power law behavior at low energies indicating the presence of several metastable elements . In addition we also observe a peak near E = 0 which corresponds to the ground state structures .Finally , we find that the average overlap between successive generations decreases exponentially as one goes away from the ground state configuration . This implies that there are no other low - energy states aside from the ground state .1 Introduction Spin windows have been studied frequently over next few years both theoretically 1 - 3 and experimentally 4 . They display curious features like frustration 5 , slow relaxation 6 - 8 etc . , which make them very difficult to complete exactly especially on small lattices 9 .However , it has been shown lately 10 that these systems can be answered easily if they are allowed to evolve under certain conditions 11 - 13 . Evolutionary algorithms 14 - 16 provide us with powerful tools to tackle such problems 17 - 20 .In this study we consider the following Hamiltonian 21 :",
        "rewrite_text": "We investigate the zero-temperature (ground state) properties of the two-dimensional spin-glass model with nearest-neighbor interactions using a phylogenetic algorithm known as the Genetic Embedded Matching Heuristic (GEMH). Our findings indicate that GEMH effectively predicts the ground states obtained through simulated annealing and Monte Carlo simulations for various system sizes up to L = 40. The energy distribution function exhibits a power-law behavior at low energies, suggesting the existence of multiple metastable states. Additionally, we observe a peak near E = 0, which is associated with the ground state structures. Ultimately, we discover that the average overlap between successive generations decreases exponentially as one moves away from the ground state configuration, indicating that there are no other low-energy states apart from the ground state. \n\n1. Introduction \nSpin glasses have been the subject of extensive study in recent years, both theoretically and experimentally. They exhibit intriguing characteristics such as frustration and slow relaxation, which complicate efforts to achieve complete solutions, particularly on small lattices. However, recent research has demonstrated that these systems can be effectively analyzed when they evolve under appropriate conditions. Evolutionary algorithms provide powerful approaches to address these challenges. In this study, we examine the following Hamiltonian:",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 5.952990444986052,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We study the floating phase in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). We find that there is no floating phase for J1 = J2, but it appears when J1 > J2 and disappears at some critical value of J1/J2. The transition between the ordered state and the floating phase belongs to the universality class of the three-state Potts model with first-order transition. In addition we show that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2. This result suggests that the ground states may be non-degenerate even though they have not been found yet. \n \n Introduction \n \n It has been known since the work by Wannier  1  that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate. However, this fact does not necessarily mean that all possible configurations can appear as ground states  2  . For example, the ground states of the one-dimensional chain are unique although its energy spectrum is continuous  3  , while those of the two-dimensional triangular-lattice Heisenberg antiferromagnet are doubly degenerate  4  . \n \n Recently, several authors studied the ground states of the two-dimensional anisotropic nearest neighbor Ising model (AN-NNI)  5 - 7  . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2  7   . On the other hand, the ground states were shown to be unique on the honeycomb lattice  8  . These results suggest that the ground states might be nondegenerate even though their exact forms remain unknown so far. \n \n In this Letter, we investigate the ground states of the ANNNI model using Monte Carlo simulations. First, we confirm that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models. Then, we examine whether these ground states are unique or not. Finally, we discuss how the ground states change depending on the values of J 1 / J 2 .\n \n Ground States of the Square-Lattice",
        "watermark_text": "We explore the floating mode in the two - dimensional anisotropic closest - neighbor Ising model ( ANNNI ) . We see that there is no floating mode for J1 = J2 , but it appears when J1 > J2 and vanished at some significant value of J1 / J2 .The shift between the ordered state and the floating stage belongs to the universality category of the three - state Potts model with first - order transition . In addition we prove that the ground states are degenerate on the square lattice if J1 = J2 or J1 < J2 .This result suggests that the ground states may be non - degenerate even though they have not been found yet . Introduction It has been known since the work by Wannier 1 that the ground states of the spin - 1 / 2 Heisenberg antiferromagnet on an infinite square lattice are infinitely degenerate .However , this fact does not necessarily mean that all possible configurations can emerge as ground states 2 . For instance , the ground states of the one - dimensional network are distinct although its energy spectrum is continuous 3 , while those of the two - dimensional triangular - lattice Heisenberg antiferromagnet are doubly degenerate 4 .Recently , various scientists examined the ground states of the two - dimensional anisotropic closest neighbor Ising model ( AN - NNI ) 5 - 7 . They showed numerically that the ground states are infinitely degenerate on the square lattices if J 1 = J 2 or J 1 < J 2 7 .On the other hand , the ground states were shown to be unique on the honeycomb lattice 8 . These conclusions show that the ground groups may be nondegenerate even though their exact forms remain uncertain so far .In this Letter , we investigate the ground states of the ANNNI theory using Monte Carlo simulations . First , we prove that the ground states are indeed infinitely degenerate on the squarelattice ANNNI models .Then , we investigate whether these ground fields are distinct or not . Finally , we explain how the ground states change based on the values of J 1 / J 2 .Ground States of the Square-Lattice",
        "rewrite_text": "We investigate the floating mode in the two-dimensional anisotropic nearest-neighbor Ising model (ANNNI). Our findings indicate that there is no floating mode when \\( J_1 = J_2 \\); however, it emerges when \\( J_1 > J_2 \\) and disappears at a certain critical value of \\( J_1 / J_2 \\). The transition between the ordered state and the floating mode falls within the universality class of the three-state Potts model, exhibiting a first-order transition. Additionally, we demonstrate that the ground states on a square lattice are degenerate when \\( J_1 = J_2 \\) or \\( J_1 < J_2 \\). This result hints at the possibility that the ground states may be non-degenerate, although they have yet to be identified. \n\nSince Wannier's work, it has been established that the ground states of the spin-1/2 Heisenberg antiferromagnet on an infinite square lattice exhibit infinite degeneracy. However, this does not imply that all configurations can act as ground states. For example, the ground states in a one-dimensional lattice are distinct despite having a continuous energy spectrum, while the ground states of the two-dimensional triangular-lattice Heisenberg antiferromagnet show double degeneracy. Recently, several researchers have investigated the ground states of the ANNNI model, finding through numerical methods that they are infinitely degenerate on square lattices when \\( J_1 = J_2 \\) or \\( J_1 < J_2 \\). In contrast, unique ground states have been observed on the honeycomb lattice. These findings indicate that while the ground states might be non-degenerate, their specific configurations remain unknown. In this letter, we analyze the ground states of the ANNNI model using Monte Carlo simulations. We first confirm the infinite degeneracy of the ground states in square lattice ANNNI models. Next, we assess whether these ground states are distinct. Finally, we discuss how the nature of the ground states evolves with varying values of \\( J_1 / J_2 \\).",
        "ori-fast-z-score": -3.07821536544563,
        "water-fast-z-score": 2.434508013602067,
        "rewrite-fast-z-score": -3.6381616886060666
    },
    {
        "original_text": "The objective was to evaluate the potential use of Monoksa dorsiplana as an alternative biological control agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained in laboratory and released on P.sp. eggs laid by females collected at different locations in Brazil, Argentina and Paraguay. Egg parasitism ranged between 0.5 and 88% depending on location. Parasitized eggs hatched after 7 days under controlled conditions. Males emerged first followed by females. Female longevity varied according to temperature ranging from 11 to 21 days at 25 °C; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C. Females oviposited for up to three weeks when fed with honey solution. This species is considered highly suitable for mass production because it has high reproductive capacity and short life cycle. It can be used successfully in integrated pest management programs aimed at reducing damage caused by this insect pest.",
        "watermark_text": "The goal was to examine the possibilities application of Monoksa dorsiplana as an alternative bio control drug against Pseudopachymeria sp . ( Bruchidae ) .The parasitoids were obtained in laboratory and captured on P . sp . eggs laid by females collected at different places in Brazil , Argentina and Paraguay .Egg parasitism ranged between 0 . 5 and 88 % depending on location . Parasitized nests hatched after 7 days under regulated conditions .Males appeared first followed by females . Female longevity varied based to heat ranging from 11 to 21 weeks at 25 °C ; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C .Females oviposited for up to three weeks when fed with honey solution . This species is regarded highly suited for mass production because it has large sexual capacity and low life cycle .It can be used successfully in effective pest management projects designed at decreasing damage suffered by this insect pest .",
        "rewrite_text": "The aim of this study was to explore the potential use of Monoksa dorsiplana as an alternative biocontrol agent against Pseudopachymeria sp. (Bruchidae). Parasitoids were gathered from the laboratory and collected from P. sp. eggs deposited by females found in various locations across Brazil, Argentina, and Paraguay. The rate of egg parasitism varied significantly, ranging from 0.5% to 88% depending on the site. Parasitized nests hatched after seven days under controlled conditions, with males emerging first, followed by females. Female longevity was influenced by temperature, lasting between 11 to 21 weeks at 25 °C, 14 to 23 days at 20 °C, and 16 to 27 days at 15 °C. When provided with a honey solution, females could oviposit for up to three weeks. This species is considered highly suitable for mass production due to its substantial reproductive capacity and short life cycle, making it effective for pest management strategies aimed at mitigating the damage caused by this insect pest.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 0.9701425001453319
    },
    {
        "original_text": "We study the impact of cosmic rays (CRs) on population iii star formation in minihalos, using cosmological hydrodynamic simulations with CR physics implemented self-consistently. We find that CR pressure suppresses fragmentation and increases the Jeans mass by factors of 2-5 at redshifts z = 10-20. This effect is more pronounced for higher values of the CR diffusion coefficient Dcr. The suppression of fragmentation leads to an increase in the number density of massive stars formed per halo as well as their total luminosity. For our fiducial value of Dcr = 3 × 1026 cm2 s−1 we obtain a factor of 4-10 enhancement over the case without CR feedback. Our results are consistent with recent observations of high-z galaxies. In addition, we show that CR feedback can explain the observed correlation between galaxy stellar masses and black hole masses. Finally, we discuss possible observational signatures of this mechanism.",
        "watermark_text": "We research the impact of cosmic rays ( CRs ) on population iii star formation in minihalos , using cosmological hydrodynamic simulations with CR physics integrated self - regularly . We see that CR pressure suppresses fragmentation and raises the Jeans mass by factors of 2 - 5 at redshifts z = 10 - 20 .This phenomenon is more pronounced for greater values of the CR absorption constant Dcr . The disruption of fragmentation gives to an increase in the number density of large galaxies formed per halo as also as their total luminosity .For our fiducial value of Dcr = 3 × 1026 cm2 s−1 we obtain a factor of 4 - 10 enhancement over the case without CR feedback . Our results are compatible with recent observations of high - z galaxies .In addition , we explain that CR feedback can describe the seen correlation between galaxy stellar ages and dark hole numbers . Finally , we explain possible observational signatures of this mechanism .",
        "rewrite_text": "We investigate the effects of cosmic rays (CRs) on Population III star formation within minihalos by utilizing cosmological hydrodynamic simulations that incorporate CR physics in a self-consistent manner. Our findings indicate that CR pressure inhibits fragmentation and increases the Jeans mass by factors ranging from 2 to 5 at redshifts of z = 10 - 20. This effect becomes more pronounced with higher values of the CR absorption constant Dcr. The suppression of fragmentation leads to a rise in the number density of large galaxies formed per halo as well as an increase in their total luminosity. For our chosen value of Dcr = 3 × 10^26 cm²/s, we observe an enhancement factor of 4 to 10 compared to the scenario without CR feedback. Our results align with recent observations of high-redshift galaxies. Furthermore, we demonstrate that CR feedback can account for the observed correlation between galaxy stellar ages and the number of dark holes. Finally, we discuss potential observational signatures related to this mechanism.",
        "ori-fast-z-score": 0.762000762001143,
        "water-fast-z-score": 6.017733356846111,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "In this work, we consider the problem of power control and receiver design to maximize the sum rate over multiple access (MAC) channels using code division multiple access (CDMA). We assume that each user transmits its signal through an independent fading channel which is subject to both path loss and shadowing effects. The received signals are corrupted by additive white Gaussian noise (AWGN), inter-user interference due to imperfect orthogonality among users  spreading codes, and intra-cell interference caused by other active users within the same cell. In addition, all users employ bandlimited waveforms such as pulse-amplitude modulation (PAM) or quadrature amplitude modulation (QAM).\nWe first derive closed-form expressions for the ergodic capacity region under different assumptions on the knowledge available at the transmitter side about the instantaneous channel state information (CSI). Then, based on these results, we propose two distributed algorithms to achieve the optimal operating point on the boundary of the ergodic capacity region. Finally, numerical examples are provided to demonstrate the performance improvement achieved by our proposed schemes compared to conventional ones.",
        "watermark_text": "In this research , we investigate the question of power control and receiver design to maximize the sum frequency over multiple entry ( MAC ) networks using code division multiple entry ( CDMA ) . We assume that each consumer transmits its signal through an independent fading signal which is subject to both route losing and shadowing effects .The received messages are corrupted by additive white Gaussian sound ( AWGN ) , inter - customer interference owing to imperfect orthogonality among users spreading coding , and intra - cell interference caused by other active consumers within the same cell . In addition , all users utilize bandlimited waveforms such as pulse - frequency modulation ( PAM ) or quadrature amplitude modulation ( QAM ) .We first derive closed - form expressions for the ergodic capacity area under various assumptions on the knowledge accessible at the broadcasting side about the instantaneous channel state information ( CSI ) . Then , based on these results , we undertake two distributed methods to achieve the ideal operating position on the boundary of the ergodic capacity area .Finally , numerical examples are provided to indicate the performance improvement achieved by our proposed projects compared to conventional ones .",
        "rewrite_text": "In this study, we explore power control and receiver design aimed at maximizing the total frequency in multiple access (MAC) networks employing code division multiple access (CDMA). We consider that each user transmits their signal through an independent fading channel affected by both path loss and shadowing effects. The received signals are disrupted by additive white Gaussian noise (AWGN), inter-user interference due to imperfect orthogonality among user's spreading codes, and intra-cell interference from other active users in the same cell. Additionally, all users utilize band-limited waveforms, such as pulse-amplitude modulation (PAM) or quadrature amplitude modulation (QAM). We begin by deriving closed-form expressions for the ergodic capacity area under various assumptions regarding the knowledge available at the transmitter about the instantaneous channel state information (CSI). Building on these findings, we propose two distributed strategies to reach the optimal operating point along the boundary of the ergodic capacity area. Finally, we present numerical examples demonstrating the performance enhancements achieved by our proposed methods in comparison to conventional approaches.",
        "ori-fast-z-score": 0.5241424183609592,
        "water-fast-z-score": 6.463946835769319,
        "rewrite-fast-z-score": 1.58999682000954
    },
    {
        "original_text": "We report the distance measurement toward the Galactic center using Very Long Baseline Array (VLBA) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry (VERA). The parallax was measured by observing Sgr A*, which is located near the Galactic center, for two years between 2007 and 2009. We found that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc. This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young stars. Our result also supports the hypothesis that the Milky Way has an axisymmetric mass distribution around its central black hole. \n \n Keywords: Distance scale, Galaxy, Parallax, Space astrometry, Black holes \n \n \n \n 1 Introduction \n \n In order to understand how galaxies evolve over time, it is important to know their distances accurately. However, accurate distances are difficult to measure because they depend strongly on the assumed luminosity evolution model. For example, if we assume too high a rate of luminosity evolution, then the derived distance will be underestimated. On the other hand, if we assume too low a rate of luminosity evolu-tion, then the derived distance may be overestimated. Therefore, it is necessary to determine the correct luminosity evolution model before deriving the distance to any galaxy. \n \n One way to solve this problem is to use radio sources whose distances can be determined independently through other means. These include pulsars, quasars, and maser sources associated with star-forming regions. Among these objects, maser sources have been used most frequently since they provide very precise distance estimates. Maser sources are usually associated with star forming regions where water vapor molecules form into microscopic crystals known as ice grains. When the ice grains grow larger than about one micron, they become unstable against gravitational collapse and begin emitting intense radiation. Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "watermark_text": "We report the distance measurement toward the Galactic center utilizing Very Long Baseline Array ( VLBA ) observations at 22 GHz and 43 GHz in combination with Japanese VLBI Exploration of Radio Astrometry ( VERA ) . The parallax was measured by observing Sgr A * , which is situated near the Galactic center , for two years between 2007 and 2009 .We showed that the distance to the Galactic center is R0 = 8 kpc ± 0 . 4 kpc . This value agrees well with previous measurements based on other methods such as infrared photometry or trigonometric parallaxes of masers associated with massive young galaxies .Our result even suggests the notion that the Milky Way has an axisymmetric mass distribution around its primary black hole . Keywords : Distance scale , Galaxy , Parallax , Space astrometry , Black holes 1 Introduction In order to comprehend how galaxies evolve over time , it is important to consider their altitudes accurately .However , accurate distances are hard to measure because they rely heavily on the assumed luminosity progression model . For instance , if we suppose too big a rate of luminosity progression , then the derived length will be underestimated .On the other hand , if we suppose too low a rate of luminosity evolu - tion , then the derived length may be overestimated . Therefore , it is required to obtain the appropriate luminosity evolution theory before deriving the distance to any galaxy .One method to solve this question is to use radio sources whose distances can be determined independently through other methods . These include pulsars , quasars , and maser sources involved with star - creating areas .Among these objects , maser sources have been used most regularly since they give very accurate distance estimates . Maser sources are typically associated with star producing regions where water vapor molecules form into microscopic crystals known as ice particles .When the glacier grains grow larger than about one micron , they become unstable against gravitational failure and begin emitting intense rays . Since the emission line widths of maser sources are extremely narrow compared to those of normal radio",
        "rewrite_text": "We present a distance measurement to the Galactic center based on Very Long Baseline Array (VLBA) observations at frequencies of 22 GHz and 43 GHz, in collaboration with the Japanese VLBI Exploration of Radio Astrometry (VERA). The parallax was determined by monitoring Sgr A*, which is located near the Galactic center, over a two-year period from 2007 to 2009. Our findings indicate that the distance to the Galactic center is R0 = 8 kpc ± 0.4 kpc, a value that aligns well with previous measurements obtained through different techniques, such as infrared photometry and trigonometric parallaxes of masers associated with massive young stars. This result even supports the idea that the Milky Way exhibits an axisymmetric mass distribution surrounding its central black hole. \n\n**Keywords:** Distance scale, Galaxy, Parallax, Space astrometry, Black holes \n\n**1 Introduction** \n\nTo understand the evolution of galaxies over time, accurately determining their distances is crucial. However, measuring accurate distances is challenging, as it heavily depends on the assumed luminosity evolution model. For instance, if we overestimate the rate of luminosity evolution, the resulting distance will be underestimated; conversely, if we underestimate this rate, the distance will be overestimated. Hence, it is essential to establish a reliable luminosity evolution model before deriving distances to any galaxy. One solution to this issue is to utilize radio sources with independently measurable distances. These include pulsars, quasars, and maser sources found in star-forming regions. Among these, maser sources are the most frequently employed due to their highly precise distance estimates. Maser sources are generally associated with areas of star formation, where water vapor molecules crystallize into microscopic ice particles. When these ice particles exceed approximately one micron in size, they become gravitationally unstable and begin to emit powerful signals. The emission line widths of maser sources are notably narrow compared to those of typical radio sources, enhancing their utility in distance measurement.",
        "ori-fast-z-score": 1.4524080181184935,
        "water-fast-z-score": 8.566708189767708,
        "rewrite-fast-z-score": 0.5222329678670935
    },
    {
        "original_text": "The equilibrium fluctuations in isolated systems are studied by using statistical mechanics and information theory. The entropy production rate is calculated for different types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. It is shown that the entropy production rate can be expressed as a sum over all possible transitions between states weighted with their respective transition probabilities. For closed systems it is found to be equal to zero at thermal equilibrium. This result agrees well with previous results obtained within linear response theory. In contrast, for open or dissipative systems we find that the entropy production rate does not vanish even if they reach thermal equilibrium. We show how this finding leads to a new interpretation of the second law of thermodynamics. Finally, we discuss our findings in terms of fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "watermark_text": "The balance fluctuations in isolated systems are studied by using statistical mechanics and information theory . The entropy production level is calculated for different kinds of isolated systems : ( i ) closed , ( ii ) open , and ( iii ) dissipative .It is seen that the entropy production efficiency can be written as a sum over all possible transitions between states weighted with their respective change probabilities . For closed systems it is found to be equal to zero at heat equilibrium .This result agrees well with previous findings obtained within linear behavior analysis . In comparison , for closed or dissipative environments we find that the entropy production level does not vanish even if they reach thermal equilibrium .We see how this fact leads to a new explanation of the second law of thermodynamics . Finally , we talk our findings in terms of fluctuation theorems .PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics",
        "rewrite_text": "The fluctuations in balance within isolated systems are analyzed using principles from statistical mechanics and information theory. We calculate the level of entropy production for various types of isolated systems: (i) closed, (ii) open, and (iii) dissipative. It is observed that the efficiency of entropy production can be expressed as a sum of all potential transitions between states, each weighted by its corresponding change probability. In closed systems, the entropy production is found to be zero at thermal equilibrium, aligning with earlier findings derived from linear analysis. In contrast, for closed or dissipative systems, the level of entropy production persists even at thermal equilibrium. This observation provides a novel interpretation of the second law of thermodynamics. Lastly, we discuss our results in relation to fluctuation theorems. PACS: 05.45.Mt, 02.10.Yn, 03.65.Ta Keywords: Entropy Production Rate, Second Law of Thermodynamics.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "We present the first results on the use of infrared calcium lines to determine stellar metallicities in open clusters and field stars, based on high-resolution spectra obtained with the CRIRES spectrograph at VLT/UT1 (ESO). We find that the equivalent widths of the two strongest components of the infrared Ca II triplet are strongly correlated with  Fe/H  for both cluster members and field stars.  The calibration is valid over a wide range of temperatures, including those typical of red giants. This method can be used to obtain accurate estimates of the iron abundance even when only low resolution data are available. It also provides an alternative way to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia. Keywords: Calcium, Metallicity, Red giant branch star, Open cluster, Infrared spectrum, Iron abundance, Distance determination, Space mission, High-Resolution spectroscopy",
        "watermark_text": "We present the first findings on the using of laser calcium bands to estimate stellar metallicities in open complexes and field stars , using on wide - resolution spectra obtained with the CRIRES spectrograph at VLT / UT1 ( ESO ) . We see that the equivalent widths of the two strongest elements of the infrared Ca II triplet are strongly correlated with Fe / H for both cluster groups and field stars .The calibration is valid over a broad variety of conditions , particularly those common of red giants . This method can be used to obtain precise estimates of the metal density even when only low resolution data are available .It additionally offers an additional means to estimate distances using parallaxes measured by space missions such as Hipparcos or Gaia . Keywords : Calcium , Metallicity , Red giant branch star , Open cluster , Infrared spectrum , Iron concentration , Distance finding , Space mission , High - Resolution spectroscopy",
        "rewrite_text": "We present our initial findings on utilizing laser calcium bands to estimate stellar metallicities in open clusters and field stars, based on wide-resolution spectra obtained with the CRIRES spectrograph at VLT/UT1 (ESO). Our analysis reveals a strong correlation between the equivalent widths of the two most prominent lines of the infrared Ca II triplet and the metallicity measured as Fe/H in both cluster groups and field stars. This calibration is effective across a wide range of conditions, particularly those typical of red giants. This method allows for precise metallicity estimates, even when only low-resolution data are available. Furthermore, it provides an additional technique to estimate distances using parallaxes obtained from space missions like Hipparcos or Gaia. \nKeywords: Calcium, Metallicity, Red Giant Branch stars, Open Clusters, Infrared Spectrum, Iron Concentration, Distance Measurement, Space Mission, High-Resolution Spectroscopy.",
        "ori-fast-z-score": -1.099524999206747,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": 1.3130643285972254
    },
    {
        "original_text": "We propose an image stabilizing method for video capturing in turbulent environments, which is based on the super-resolution technique. The proposed method can be used to improve the quality of captured videos with handheld cameras or other devices that are subject to motion blur caused by turbulence. We first estimate the camera motion using optical flow between consecutive frames. Then we use this information as input data into our super-resolution algorithm to generate high resolution images. Finally, these images are combined together to produce a stable output frame. Our experimental results show that the proposed method outperforms existing methods both visually and quantitatively. \n \n Keywords: Real time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. 1 Introduction \n \n In recent years there has been growing interest in developing techniques for real time video processing applications such as video surveillance systems  1  , traffic monitoring  2  , remote sensing  3  . However, most of these applications require capturing clear images under challenging conditions like low-light illumination  4  , fast moving objects  5  , blurry scenes  6  , etc.. Among all these challenges, one of the major problems is how to deal with the motion blur caused by turbulence  7  8  9  when capturing videos with hand-held cameras or other devices  10  . \n \n Figure 1: An example of a video sequence taken at nighttime (a) and its corresponding ground truth (b).",
        "watermark_text": "We suggest an image stabilizing method for video capturing in volatile settings , which is based on the ultra - resolution technique . The proposed approach can be used to improve the performance of released movies with handheld cameras or other devices that are subject to moving fade caused by turbulence .We first estimate the image movement utilizing optical flow between successive frames . Then we utilize this data as input data into our super - resolution algorithm to create long resolution photos .Finally , these images are fused together to produce a consistent output frame . Our research results show that the suggested method outperforms current methods both physically and quantitatively .Keywords : Real time television processing ; Turbulence ; Motion blur ; Optical stream ; Super - resolution ; Image stabilization . 1 Introduction In recent years there has been growing interest in developing procedures for real time television processing applications such as video monitoring systems 1 , road monitoring 2 , remote sensing 3 .However , most of these uses involve capturing clear photographs under difficult environments like low - light lighting 4 , fast move images 5 , blurry scenes 6 , etc . . Among all these problems , one of the main problems is how to deal with the movement blur caused by turbulence 7 8 9 when capturing clips with hand - held photographers or other devices 10 .Figure 1 : An example of a video sequence taken at nighttime ( a ) and its equivalent ground truth ( b ) .",
        "rewrite_text": "We propose a method for image stabilization during video capture in dynamic environments, utilizing ultra-resolution techniques. This approach enhances the quality of videos recorded with handheld cameras or other devices susceptible to motion blur from turbulence. Initially, we estimate the movement of images using optical flow analysis between consecutive frames. We then feed this information into our super-resolution algorithm, which generates high-resolution images. Finally, these images are combined to create a seamless output frame. Our findings demonstrate that the proposed method significantly outperforms existing techniques, both in practical application and quantitative analysis. \n\n**Keywords:** Real-time television processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization.\n\n**1 Introduction**  \nIn recent years, there has been an increasing focus on developing methods for real-time television processing applications, such as video surveillance systems, road monitoring, and remote sensing. However, many of these applications face challenges in capturing clear footage under adverse conditions like low light, rapid motion, and blurriness. Among these issues, one of the primary challenges is addressing motion blur caused by turbulence when recording with handheld cameras or other devices. \n\n**Figure 1:** An example of a video sequence captured at night (a) alongside its corresponding ground truth (b).",
        "ori-fast-z-score": -0.8181818181818182,
        "water-fast-z-score": 7.483857967739688,
        "rewrite-fast-z-score": -1.5255401427929478
    },
    {
        "original_text": "The zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a  quantum torus  whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called  zeta-functions  of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I ll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.",
        "watermark_text": "The zeta - function is the main object in number theory and algebraic topology . In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non - trivial geometry .The basic idea is that one considers the Feynman line expansion over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm . This leads naturally to the idea of a quantum torus whose zeta function encodes data about the spectrum of the associated QFT .We will also discuss some latest findings concerning the relation between the zeta functions of certain classes of quantum tori and their associated modular forms . Quantum Field Theory ( QFT ) has been built into a powerful tool for studying physical phenomena at the most important level .However , many interesting problems continue open thanks to the difficulty of performing calculations beyond perturbation theory . One path out of these problems is to use insights from mathematics such as those coming from number theory or algebraic theory .In particular , we are concerned in use the so - called zeta - functions of algebraic fields to obtain new understanding into QFTs . These items encode crucial data about the fundamental theories but they are notoriously difficult to compute accurately .In my talk I ll offer examples where explicit computations have been performed effectively and suggest how these procedures could lead to further progress in our understanding of QFTs .",
        "rewrite_text": "The zeta function plays a central role in both number theory and algebraic topology. In this presentation, I will explore how it can be utilized to analyze quantum field theories (QFTs) in curved spacetimes with complex geometries. The core concept involves treating the Feynman line expansion over all fields on spacetime as an infinite-dimensional functional integration. This can be regularized by substituting the field space with a finite-dimensional vector space endowed with an appropriate norm. This naturally leads to the notion of a quantum torus, whose zeta function contains information about the spectrum of the related QFT. We will also examine recent findings regarding the connection between the zeta functions of certain classes of quantum tori and their corresponding modular forms. Quantum Field Theory has evolved into a powerful framework for investigating fundamental physical phenomena. However, many intriguing challenges remain unresolved, primarily due to the complexities of performing calculations beyond perturbation theory. One potential solution is to leverage insights from mathematics, particularly those derived from number theory and algebraic theory. Specifically, we will focus on employing the zeta functions of algebraic fields to deepen our understanding of QFTs. These functions encode essential information about foundational theories, yet they are notoriously challenging to compute with precision. In my talk, I will present examples where effective explicit computations have been achieved and propose how these methods could facilitate further advancements in our comprehension of QFTs.",
        "ori-fast-z-score": 0.5570860145311556,
        "water-fast-z-score": 6.009252125773315,
        "rewrite-fast-z-score": 1.8034001919297482
    },
    {
        "original_text": "We present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.",
        "watermark_text": "We present the conclusion of an assessment of the shapes , orientations , and alignments of bright matter subhalos in cosmological N - bodies simulations with various levels of baryonic physics provided . We see that the inclusion of baryons has little impact on the form distribution but does affect the spin vector distributions substantially ; halos are more elongated when baryons are incorporated than they would be if only gravitational were acting upon them .The halo orbits tend to be aligned perpendicularly to their major axes for all models discussed here ( especially pure black material ) . This is consistent with previous research which have discovered similar trends using other methods .However we also find proof that this shift might not stand at very small scales where there seems to be some correlation between the direction of the angular velocity tensor and the minor axis of the halo . Finally , we find that the presence or lack of baryons affects the degree of alignment between neighboring halos ; halos are less highly clustered around each other when baryons are included .",
        "rewrite_text": "We present the findings of an analysis of the shapes, orientations, and alignments of bright matter subhalos in cosmological N-body simulations that incorporate varying levels of baryonic physics. Our results indicate that while the addition of baryons has minimal impact on the distribution of shapes, it significantly influences the distribution of spin vectors; specifically, halos exhibit greater elongation when baryons are included, compared to scenarios where only gravitational forces are at play. Additionally, halo orbits are generally oriented perpendicular to their major axes across all models examined, particularly those involving pure dark matter. This observation aligns with previous studies that have identified similar patterns using alternative methods. However, we also uncover evidence suggesting that at very small scales, there may be a correlation between the direction of the angular velocity tensor and the minor axis of the halo, potentially challenging the broader trends. Finally, we determine that the presence or absence of baryons impacts the level of alignment between neighboring halos; halos tend to be less densely clustered when baryons are present.",
        "ori-fast-z-score": -1.9896995023342199,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 0.5488212999484517
    },
    {
        "original_text": "The self-diffusivity (D) and interdiffusivity (D*) were measured by the tracer diffusion method for molten Al80Ni20 alloys at 1373 K, 1473 K, 1523 K, 1623 K, 1723 K, 1823 K, 1873 K, 1923 K, and 2023 K using radioactive tracers. The activation energies for D and D* are determined to be 1.27 ± 0.04 eV and 2.10 ± 0.06 eV respectively. These values agree well with those obtained previously on other binary melts.  It is found that both D and D* increase rapidly as temperature increases up to about 1600 K but then they change very slowly above this temperature. This behavior can be explained by considering the effect of atomic size difference between Ni and Al atoms on their diffusivities. In addition, it was observed that the ratio of D/D* decreases gradually with increasing temperature.",
        "watermark_text": "The self - diffusivity ( D ) and interdiffusivity ( D * ) were calculated by the tracer diffusion method for molten Al80Ni20 alloys at 1373 K , 1473 K , 1523 K , 1623 K , 1723 K , 1823 K , 1873 K , 1923 K , and 2023 K using nuclear tracers . The activation energies for D and D * are found to be 1 . 27 ± 0 . 04 eV and 2 . 10 ± 0 . 06 eV respectively .These values comply better with those acquired previously on other binary melts . It is found that both D and D * increase quickly as temperature increases up to about 1600 K but then they change very slowly above this heat .This phenomenon can be described by examining the impact of atomic height shift between Ni and Al atoms on their diffusivities . In addition , it was seen that the proportion of D / D * decreases slowly with rising heat .",
        "rewrite_text": "Self-diffusivity (D) and interdiffusivity (D*) for molten Al80Ni20 alloys were determined using the tracer diffusion method at temperatures of 1373 K, 1473 K, 1523 K, 1623 K, 1723 K, 1823 K, 1873 K, 1923 K, and 2023 K with the aid of nuclear tracers. The activation energies for D and D* were measured to be 1.27 ± 0.04 eV and 2.10 ± 0.06 eV, respectively, which align closely with values obtained in previous studies of other binary melts. It was observed that both D and D* increase rapidly with rising temperatures up to approximately 1600 K, after which their rates of increase slow significantly. This behavior can be explained by analyzing the effects of atomic height differences between Ni and Al atoms on their diffusivities. Additionally, it was noted that the ratio of D/D* gradually decreases with increasing temperature.",
        "ori-fast-z-score": -0.7808688094430304,
        "water-fast-z-score": 4.216691570992364,
        "rewrite-fast-z-score": 0.15249857033260467
    },
    {
        "original_text": "The autoignition characteristics of two cyclic hydrocarbons, cyclopentane (CP) and cyclohexane (CH), are investigated using the rapid compression machine coupled with a shock-tube facility at temperatures ranging between 300 K and 1000 K under atmospheric pressure conditions. The ignition delay times for both fuels increase as temperature increases due to an increased rate of chemical reactions. At low temperatures below 600 K, CP has longer ignition delays than CH because it is more difficult for the fuel molecules to overcome their activation energy barrier. However, above 700 K, the opposite trend occurs where CH exhibits longer ignition delays compared to CP. This can be explained by the fact that the higher molecular weight of CH leads to slower diffusion rates which results in lower reactivity.  In addition, the effect of equivalence ratio on the ignition delay time was also studied. It was found that increasing the equivalence ratio decreases the ignition delay time for all tested temperatures except at 800 K where no significant difference could be observed.",
        "watermark_text": "The autoignition characteristics of two cyclic hydrocarbons , cyclopentane ( CP ) and cyclohexane ( CH ) , are examined utilizing the quick compression device coupled with a shock - box facility at conditions ranging between 300 K and 1000 K under atmospheric pressure circumstances . The ignition wait periods for both fuels increase as temperature increases owing to an higher speed of organic reactions .At low temperatures below 600 K , CP has longer ignition delays than CH because it is more difficult for the engine molecules to overcome their activation energy barrier . However , above 700 K , the opposite trend results where CH exhibits longer ignition delays compared to CP .This can be explained by the fact that the higher molecular weight of CH leads to slower diffusion rates which results in lower reactivity . In addition , the impact of equivalence coefficient on the engine delay time was also examined .It was shown that expanding the equivalence factor decreases the engine delay time for all tested altitudes except at 800 K where no major variation might be found .",
        "rewrite_text": "The autoignition properties of two cyclic hydrocarbons, cyclopentane (CP) and cyclohexane (CH), were investigated using a rapid compression device integrated with a shock-box setup. The study covered temperatures from 300 K to 1000 K at atmospheric pressure. It was found that the ignition delay times for both fuels increase with rising temperature, due to an acceleration in reaction rates. At lower temperatures, specifically below 600 K, CP exhibits longer ignition delays than CH, as the molecules in CP face a greater challenge in overcoming their activation energy barrier. However, above 700 K, this trend reverses, with CH showing longer ignition delays than CP. This change can be attributed to the higher molecular weight of CH, which results in slower diffusion rates and, consequently, reduced reactivity. Additionally, the influence of the equivalence ratio on the ignition delay time was analyzed, revealing that increasing the equivalence factor generally decreases ignition delay times across all tested temperatures, except at 800 K, where no significant changes were observed.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": -1.1470786693528088
    },
    {
        "original_text": "We present the results of our analysis of molecular gas mass estimates based on CO and HCN observations in nearby galaxies, using data obtained with the IRAM 30m telescope. We find that conversion factors between luminosity and mass are strongly dependent on the star formation rate (SFR) per unit area within each galaxy disk. The SFR surface density is found to be an important parameter controlling the conversion factor XCO = M(H2)/L(CO), which we derive by fitting the observed L(HCN) / L(CO) ratio versus metallicity relation. For low values of ΣSFR < 1M⊙ yr-1 kpc-2 , corresponding to quiescent disks or nuclear regions dominated by old stellar populations, we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s. This value increases up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > 3M⊙yr-1kpc-2 . These findings suggest that the physical conditions of the interstellar medium may change significantly depending on whether it is located in actively star-forming regions or not.",
        "watermark_text": "We publish the conclusion of our analysis of molecular gas mass estimates based on CO and HCN measurements in nearby galaxies , using data derived with the IRAM 30m telescope . We see that conversion factors between luminosity and mass are strongly dependent on the star formation rate ( SFR ) per unit area within each galaxy disk .The SFR ground density is found to be an important function regulating the transformation parameter XCO = M ( H2 ) / L ( CO ) , which we derive by fitting the seen L ( HCN ) / L ( CO ) ratio versus metallicity relation . For low values of ΣSFR < [UNK] yr - 1 kpc - 2 , equivalent to quiescent disks or atomic regions dominated by ancient stars populations , we obtain XCO ≈ 2 × 10 20 cm−2 K−1 km−1 s . This value rises up to XCO ≈ 5×10 20 cm−2 K−1km−1s at high ΣSFR > [UNK] - 1kpc - 2 .These studies imply that the physical conditions of the interstellar material may change considerably depending on whether it is situated in actively star - creating areas or not .",
        "rewrite_text": "We present the findings of our analysis concerning molecular gas mass estimates derived from CO and HCN measurements in nearby galaxies, utilizing data collected from the IRAM 30m telescope. Our investigation reveals that the conversion factors between luminosity and mass are significantly influenced by the star formation rate (SFR) per unit area within each galaxy's disk. We identified the SFR surface density as a critical factor affecting the transformation parameter \\(X_{CO} = \\frac{M(H_2)}{L(CO)}\\), which we calculate by fitting the observed \\( \\frac{L(HCN)}{L(CO)} \\) ratio against metallicity. For lower values of \\(\\Sigma SFR < [UNK] \\, \\text{yr}^{-1} \\, \\text{kpc}^{-2}\\)—characterizing quiescent disks or atomic regions dominated by older stellar populations—we determine \\(X_{CO} \\approx 2 \\times 10^{20} \\, \\text{cm}^{-2} \\, \\text{K}^{-1} \\, \\text{km}^{-1} \\, \\text{s}\\). This value increases to \\(X_{CO} \\approx 5 \\times 10^{20} \\, \\text{cm}^{-2} \\, \\text{K}^{-1} \\, \\text{km}^{-1} \\, \\text{s}\\) in regions with high \\(\\Sigma SFR > [UNK] \\, \\text{yr}^{-1} \\, \\text{kpc}^{-2}\\). These findings suggest that the physical conditions of interstellar material can vary significantly depending on whether it is located in areas of active star formation.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 1.8973665961010275
    },
    {
        "original_text": "We consider an additive white Gaussian noise (AWGN) channel with multiple paths, each having different delay and gain coefficients. We assume that these channels are sparse; i.e., there is only one path whose gain coefficient exceeds some threshold value. The capacity region for this class of channels has been studied by several authors under various assumptions on the number of paths and their gains.  In particular, it was shown recently that if all the paths have equal power then the capacity region can be achieved using superposition coding at transmitter and successive decoding at receiver. However, when the powers of the paths vary widely, the optimality of such schemes remains unknown. In this work we show that even though the optimal scheme may not use superposition coding or successive decoding, its performance cannot differ significantly from those schemes. More specifically, we prove upper bounds on the gap between the achievable rates using any scheme and the sum-capacity of the channel.",
        "watermark_text": "We consider an additive white Gaussian sound ( AWGN ) channel with many lanes , each having specific delay and gain values . We assume that these networks are sparse ; i . e . , there is only one path whose gain coefficient exceeds some threshold value .The capacity area for this class of channels has been studied by various scientists under various observations on the number of paths and their benefits . In particular , it was shown lately that if all the paths have equal energy then the ability area can be obtained using superposition coding at transmitter and subsequent decoding at receiver .However , when the powers of the paths differ widely , the optimality of such schemes remains undisclosed . In this research we prove that even though the ideal method may not use superposition coding or successive decoding , its effectiveness cannot differ significantly from those schemes .More specifically , we prove upper limits on the gap between the achievable rates using any scheme and the sum - capacity of the channel .",
        "rewrite_text": "We examine an additive white Gaussian noise (AWGN) channel with multiple paths, each characterized by distinct delay and gain values. Our focus is on sparse networks, where only one path has a gain that exceeds a certain threshold. Researchers have explored the capacity region for this type of channel, taking into account various factors related to the number of paths and their advantages. Recent findings indicate that when all paths possess equal energy, the capacity region can be effectively determined using superposition coding at the transmitter and decoding at the receiver. However, the optimality of this approach becomes unclear when there is significant variability in the power of the paths. In our study, we demonstrate that even if the ideal solution does not rely on superposition coding or successive decoding, its performance remains comparable to those methods. Specifically, we establish upper bounds on the gap between achievable rates with any scheme and the channel's sum capacity.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We have performed ab initio molecular dynamics simulations to study the in-plane structure, order parameters, and surface tension of liquid Na(l) in contact with vacuum or solid NaCl (001). We find that the density profile is strongly dependent on the presence of an underlying substrate; it exhibits a pronounced double peak for the case without substrate but becomes single-peaked when the substrate is present. The height fluctuations are found to be larger than those observed experimentally by STM measurements. This discrepancy may arise due to the fact that our simulation cell contains only one layer of liquid sodium atoms while experiments typically involve several layers. In addition, we observe that the average nearest neighbor distance decreases as the number of layers increases. Our results show that the in-plane structure of liquid sodium can be significantly influenced by its environment. Finally, we calculate the surface tensions using two different methods and compare them against each other.",
        "watermark_text": "We have done ab initio polymer mechanics simulations to study the in - plane structure , order variables , and surface tension of liquid Na ( l ) in contact with vacuum or solid NaCl ( 001 ) . We see that the density profile is strongly dependent on the presence of an underlying substrate ; it displays a noticeable double peak for the case without substrate but appears single - peaked when the substrate is present .The altitude fluctuations are found to be larger than those observed experimentally by STM observations . This discrepancy may arise due to the fact that our modeling room contains only one layer of liquid sodium atoms while tests usually require many layers .In addition , we find that the average closest neighbor distance tends as the number of thickness increases . Our results show that the in - plane structure of liquid sodium can be greatly altered by its surroundings .Finally , we estimate the surface tensions use two different methods and contrast them against each other .",
        "rewrite_text": "We conducted ab initio polymer mechanics simulations to investigate the in-plane structure, order parameters, and surface tension of liquid sodium (Na) in contact with either vacuum or solid NaCl (001). Our findings indicate that the density profile is significantly influenced by the presence of an underlying substrate; it exhibits a distinct double peak when there is no substrate, while a single peak is observed when the substrate is present. Additionally, we found that the altitude fluctuations are greater than those measured experimentally via scanning tunneling microscopy (STM). This difference may be attributed to our simulation setup, which includes only a single layer of liquid sodium atoms, whereas experimental measurements generally involve multiple layers. Furthermore, we noted that the average nearest neighbor distance tends to change as the thickness increases. Our results demonstrate that the in-plane structure of liquid sodium is profoundly affected by its environment. Lastly, we estimated the surface tensions using two different methods and compared the results.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 6.405028512341099,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "We present an algorithm for searching in databases that are stored as qubits, which is the basic unit of information in quantum computers. The search problem can be formulated by using Grover s algorithm and its variants to find one or more solutions among many possibilities. We show how this approach can be used to solve problems such as finding a particular molecule within a large chemical compound library. Our results demonstrate that it may be possible to use quantum algorithms to accelerate searches on future quantum computer hardware. Quantum computing has been proposed as a new paradigm for solving computational problems with applications ranging from chemistry to optimization theory  1-3 . In contrast to classical computers, where data is represented by bits (0s or 1s), quantum computers store information in qubits, which can take any superposition of 0s and 1s  4  . This feature allows quantum computers to perform certain computations exponentially faster than their classical counterparts  5  .\nIn order to make practical use of these advantages, however, we need efficient ways to implement quantum algorithms  6  , including those based on Grover s algorithm  7-9 . Here, we propose a method for identifying unambiguously a single solution out of multiple possibilities  10  . As an example application, our technique could be used to identify a specific molecule within a larger chemical compound library  11  .",
        "watermark_text": "We present an algorithm for searching in databases that are stored as qubits , which is the fundamental unit of information in quantum computers . The search question can be formulated by using Grover s algorithm and its versions to find one or more solutions among various possibilities .We see how this methodology can be used to solve difficulties such as finding a certain chemical within a large chemical element library . Our results show that it could be possible to use quantum algorithms to accelerate searches on future quantum computer hardware .Quantum processing has been proposed as a new framework for solving computational problems with applications diverse from chemistry to optimization theory 1 - 3 . In comparison to conventional machines , where data is represented by bits ( 0s or 1s ) , quantum computers store information in qubits , which can take any superposition of 0s and 1s 4 .This feature allows quantum computers to conduct many computations exponentially better than their classical rivals 5 . In order to make practical use of these benefits , however , we require efficient methods to execute quantum algorithms 6 , notably those built on Grover s algorithm 7 - 9 .Here , we propose a technique for finding unambiguously a single solution out of multiple options 10 . As an instance application , our technique could be used to identify a certain compound within a greater chemical element library 11 .",
        "rewrite_text": "We introduce an algorithm designed for searching databases represented by qubits, the core unit of information in quantum computers. The search query can be structured using Grover's algorithm and its variations to identify one or more solutions among numerous possibilities. This approach can effectively address challenges such as locating a specific chemical within an extensive library of chemical elements. Our findings indicate that quantum algorithms could potentially enhance search efficiency on future quantum computing hardware. Quantum processing has emerged as a promising framework for tackling a variety of computational problems, spanning applications from chemistry to optimization theory. Unlike traditional computers that use bits (0s and 1s) to represent data, quantum computers utilize qubits, which can exist in superpositions of both 0s and 1s. This characteristic enables quantum computers to perform many calculations far more efficiently than their classical counterparts. However, to leverage these advantages practically, we need effective methods for implementing quantum algorithms, particularly those based on Grover's algorithm. In this work, we present a technique for definitively identifying a single solution from multiple candidates. As a practical application, this technique could be utilized to detect a specific compound within a vast chemical library.",
        "ori-fast-z-score": -0.0949157995752499,
        "water-fast-z-score": 6.236413804652249,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "We propose to realize strong atom-field coupling between a single-mode microwave field and ultracold atoms trapped inside a microchip-based Fabry-Perot resonator with high finesse mirrors. The system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. We show that this scheme can be implemented using current technology. In particular we discuss how to trap cold atoms close to one mirror surface and how to couple them efficiently to the microwave mode via evanescent fields. Finally, we present numerical results showing that it should be possible to reach the regime where quantum effects become important even if only a small number of atoms are loaded into the cavity. This work was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP). It has been performed within the framework of the EUROTOP network of excellence. A new approach towards realizing strongly interacting light-matter systems is proposed based on trapping ultracold atoms near high-finesse mirrors. By exploiting the large electric dipole moment associated with atomic transitions in the microwave domain, we demonstrate that such a setup allows us to achieve strong atom-field coupling at the single-atom limit.",
        "watermark_text": "We suggest to realize strong element - field coupling between a single - mode microwave field and ultracold atoms trapped inside a microchip - based Fabry - Perot resonator with high finesse mirrors . The system is characterized by the Jaynes - Cummings model , which predicts Rabi oscillations at the single - atom level .We see that this scheme can be executed using current technology . In particular we explain how to capture cold molecules close to one reflection edge and how to link them efficiently to the microwave mode via evanescent fields .Finally , we present numerical findings indicating that it should be possible to reach the regime where quantum effects become crucial even if only a small number of atoms are transported into the cavity . This project was supported by the European Commission under contract IST - 2001 - 34011 Quantum Photonics ( QAP ) .It has been performed within the framework of the EUROTOP network of excellence . A modern alternative towards developing highly correlated light - matter structures is proposed based on trapping ultracold atoms near high - finesse mirrors .By exploiting the huge electric dipole moment associated with atomic interactions in the microwave domain , we prove that such a setup allows us to achieve powerful atom - field coupling at the single - atom limit .",
        "rewrite_text": "We recommend establishing a strong coupling between a single-mode microwave field and ultracold atoms confined within a microchip-based Fabry-Perot resonator featuring high finesse mirrors. This system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the level of individual atoms. Our analysis indicates that this setup can be realized with current technology. Specifically, we describe a method for capturing cold molecules near one of the reflective edges and efficiently coupling them to the microwave mode via evanescent fields. Additionally, we present numerical results suggesting that it is feasible to attain a regime where quantum effects become significant, even with a limited number of atoms introduced into the cavity. This project received support from the European Commission under contract IST-2001-34011 Quantum Photonics (QAP) and was conducted within the framework of the EUROTOP network of excellence. We propose a contemporary approach for creating highly correlated light-matter structures by trapping ultracold atoms near high-finesse mirrors. By leveraging the substantial electric dipole moment associated with atomic interactions in the microwave range, we demonstrate that this configuration enables strong atom-field coupling at the single-atom level.",
        "ori-fast-z-score": 1.6915632233569815,
        "water-fast-z-score": 7.32709181802739,
        "rewrite-fast-z-score": 1.616244071283537
    },
    {
        "original_text": "We report on the evaporation of buffer-gas thermalized ions in a linear quadrupole ion trap (QIT). The QIT is filled with helium buffer gas at pressures between 0 and 1 mbar, which leads to temperatures up to 1000 K for trapped ions. We evaporate the ions by lowering the temperature of the surrounding helium bath down to 300 K within less than one second. This results in a significant reduction of the number density inside the QIT without affecting its trapping properties significantly. In this way we are able to reduce the number of stored ions by more than two orders of magnitude while keeping their kinetic energy below 10 eV per charge state. Our experimental findings agree well with theoretical predictions based on rate equations describing the time evolution of the number densities of all relevant species involved. \n \n Introduction \n \n Multipole radio-frequency ion traps have been used extensively over the past decades as mass spectrometers  1  . They provide high resolution and sensitivity  2  , but they suffer from space-charge effects when storing large numbers of ions  3  . Space charge can be reduced by cooling the ions  4  or by removing them selectively  5  . Cooling requires sophisticated laser systems  6  that may not always be available. Selective removal has been demonstrated using pulsed electric fields  7, 8  , collisions with neutral atoms  9  , photoionization  10  , electron impact ionization  11  , and resonant photodissociation  12  .\n \nIn our experiment, we use selective removal via rapid heating of the helium buffer gas  13  . Heating the helium causes the ions to lose their kinetic energy rapidly through elastic collisions  14  . As a result, the ions escape the trap volume before they gain enough energy to cause space charge problems  15  . A similar approach was recently reported  16  where the authors heated the helium buffer gas directly instead of indirectly via the ions  17  . \n \n Herein, we present detailed measurements of the process of evaporative cooling of buffer gas-thermalised ions in a linear quadrupolar ion trap (QIT)  18  . We show how the number density of the ions decreases exponentially after switching off the helium flow into the vacuum chamber containing the",
        "watermark_text": "We report on the evaporation of buffer - gas thermalized ions in a linear quadrupole ion trap ( QIT ) . The QIT is filled with helium buffer gas at pressures between 0 and 1 mbar , which results to pressures up to 1000 K for trapped ions .We evaporate the ions by reduction the temperature of the nearby helium bath down to 300 K within fewer than one second . This results in a substantial lowering of the number density inside the QIT without affecting its trapping characteristics significantly .In this way we are able to reduce the number of stored ions by more than two orders of magnitude while maintaining their kinetic power below 10 eV per charge state . Our research conclusions follow well with theoretical estimates based on rate coefficients relating the period evolution of the number densities of all relevant species involved .Introduction Multipole radio - frequency ion traps have been used heavily over the previous decades as mass spectrometers 1 . They offer high resolution and sensitivity 2 , but they suffer from space - charge effects when storing huge amounts of atoms 3 .Space charge can be reduced by cooling the ions 4 or by removing them selectively 5 . Cooling needs specialized laser methods 6 that might not always be available .Selective removal has been shown involving pulsed electric forces 7 , 8 , collisions with neutral ions 9 , photoionization 10 , electron blast ionization 11 , and resonant photodissociation 12 . In our experiment , we using selective removal via rapid heating of the helium buffer gas 13 .Heating the helium creates the ions to lose their kinetic power rapidly through elastic collisions 14 . As a result , the ions flee the trap volume before they acquire enough energy to create space charge problems 15 .A related approach was recently described 16 where the authors heated the helium buffer gas directly rather of indirectly via the ions 17 . Herein , we present detailed observations of the process of evaporative cooling of buffer gas - thermalised particles in a circular quadrupolar ion trap ( QIT ) 18 .We see how the number density of the ions decreases exponentially after switching off the helium flow into the vacuum chamber containing the",
        "rewrite_text": "We investigate the evaporation of thermally equilibrated buffer-gas ions in a linear quadrupole ion trap (QIT). The trap is filled with helium buffer gas at pressures ranging from 0 to 1 mbar, leading to effective ion temperatures of up to 1000 K. By rapidly lowering the temperature of the adjacent helium bath to 300 K in less than one second, we achieve a significant reduction in the ion number density within the QIT while largely preserving its trapping efficiency. This method enables us to decrease the stored ion count by over two orders of magnitude, keeping their kinetic energy below 10 eV per charge state. Our findings align closely with theoretical predictions based on rate coefficients related to the temporal evolution of number densities for all pertinent species involved. \n\n**Introduction:** Over the past several decades, multipole radio-frequency ion traps have been extensively utilized as mass spectrometers, offering high resolution and sensitivity. However, they face challenges related to space-charge effects when storing large quantities of atoms. Space charge can be mitigated by either cooling the ions or selectively removing them. While cooling typically requires specialized laser techniques that may not always be accessible, selective removal methods have been demonstrated using pulsed electric fields, collisions with neutral ions, photoionization, electron bombardment, and resonant photodissociation. In our study, we employ selective removal through the rapid heating of the helium buffer gas. The heating causes the ions to lose their kinetic energy quickly due to elastic collisions, allowing them to escape the trap volume before they gain enough energy to cause space-charge issues. A similar method was recently reported, involving direct heating of the helium buffer gas instead of the ions. Here, we provide a detailed examination of the evaporative cooling process of buffer-gas-thermalized particles in a circular quadrupole ion trap. Our observations reveal an exponential decrease in ion number density following the cessation of helium flow into the vacuum chamber containing the QIT.",
        "ori-fast-z-score": 1.7284832429004495,
        "water-fast-z-score": 8.171011693711216,
        "rewrite-fast-z-score": 3.288292185478052
    },
    {
        "original_text": "The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean-field theory and Monte Carlo simulations. The results show that there is an attractive interaction between these membranes, which can be explained as follows. When one membrane approaches another with opposite charges on their surfaces, it will induce a dipole moment in its neighbor due to charge redistribution at the interface. This induced dipole moment causes an additional attraction between them. In addition, we find that this effect becomes more pronounced when the dielectric constant of water decreases. Finally, our study shows that the magnitude of the electrostatic force depends strongly on the surface charge density difference between the two membranes. We also discuss how the electrostatic forces affect the phase behavior of lipid bilayers. DOI: 10.1063/1.3189000\nI. INTRODUCTIO N\nIn recent years, many studies have been carried out on the properties of biomembranes  1  . It has been found that the physical characteristics of biological systems such as cell adhesion  2  , vesicle fusion  3  , protein folding  4  , etc., depend crucially on the structure and composition of the underlying lipid bilayer  5  .\nBiological membranes consist mainly of phospholipids  6  . These lipids contain hydrophobic tails and hydrophilic heads  7, 8  . Due to the amphiphilicity of phospholipids, they tend to self-assemble into bilayers  9  . A typical example for such a system is shown schematically in Fig.  1(a) . Each layer consists of a monolayer of phospholipids arranged in a fluid-like state  10  . The thickness of each layer is about 5 nm  11  . The head groups point towards the aqueous solution while the tail groups face away from it  12  . Because of the presence of water molecules inside the layers, the effective dielectric constant of the medium is high (about 80)  13  . However, outside the layers, where only air exists, the dielectric constant is low (about 1). Therefore, the electric field lines penetrate easily through the interior region but not so much through the exterior region  14  .",
        "watermark_text": "The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean - field model and Monte Carlo simulations . The results show that there is an interesting interaction between these membranes , which can be described as follows .When one cell encounters another with opposite charges on their edges , it will generate a dipole point in its friend due to charge redistribution at the interface . This induced dipole point causes an additional attraction between them .In addition , we find that this effect gets more pronounced when the dielectric constant of water reduces . Finally , our research shows that the severity of the electrostatic pressure depends strongly on the surface charge density difference between the two membranes .We additionally discuss how the electrostatic pressures affect the phase response of lipid bilayers . DOI : 10 . 1063 / 1 . 3189000 I . INTRODUCTIO N In recent years , various studies have been carried out on the properties of biomembranes 1 .It has been shown that the structural traits of biological systems such as cell adhesion 2 , vesicle fusion 3 , protein folding 4 , etc . , depend crucially on the composition and composition of the underlying lipid bilayer 5 . Biological membranes consist mostly of phospholipids 6 .These lipids contain hydrophobic tails and hydrophilic heads 7 , 8 . Due to the amphiphilicity of phospholipids , they tend to self - organize into bilayers 9 .A typical example for such a system is demonstrated schematically in Fig . 1 ( a ) .Each layer contains of a monolayer of phospholipids ordered in a fluid - like state 10 . The depth of each layer is about 5 nm 11 .The face groups look towards the aqueous solution while the tail groups face away from it 12 . Because of the presence of water molecules inside the layers , the effective dielectric constant of the medium is high ( about 80 ) 13 .However , outside the layers , where only air occurs , the dielectric constant is low ( about 1 ) . Therefore , the electric field lines penetrate easily through the inner region but not so much through the exterior zone 14 .",
        "rewrite_text": "The electrostatic interactions between two asymmetrically charged membranes are investigated using a mean-field model and Monte Carlo simulations. The findings reveal a fascinating interaction between these membranes. Specifically, when one cell approaches another with opposite charges at their edges, a dipole is induced in the neighboring cell due to charge redistribution at the interface. This induced dipole results in a supplementary attractive force between the two membranes. Furthermore, we observe that this effect becomes more pronounced as the dielectric constant of water decreases. Our research also indicates that the intensity of the electrostatic pressure is heavily influenced by the difference in surface charge density between the two membranes. We additionally explore how these electrostatic pressures impact the phase response of lipid bilayers. DOI: 10.1063/1.3189000\n\n**INTRODUCTION**  \nIn recent years, extensive research has focused on the properties of biomembranes. Studies have demonstrated that the structural characteristics of biological systems, such as cell adhesion, vesicle fusion, and protein folding, are critically dependent on the composition of the underlying lipid bilayer. Biological membranes are primarily composed of phospholipids, which have hydrophobic tails and hydrophilic heads. Due to the amphiphilic nature of these phospholipids, they naturally self-organize into bilayers. A typical example is depicted schematically in Fig. 1(a), where each bilayer consists of an ordered monolayer of phospholipids in a fluid-like state, with a thickness of approximately 5 nm. The hydrophilic head groups face the aqueous environment, while the hydrophobic tail groups are oriented away from it. Given the presence of water molecules within the layers, the effective dielectric constant of the medium is high (around 80), while it is considerably lower (approximately 1) in the air outside the layers. Consequently, electric field lines can easily penetrate the inner region but encounter significant resistance in the exterior zone.",
        "ori-fast-z-score": -0.1781741612749496,
        "water-fast-z-score": 5.992662179699436,
        "rewrite-fast-z-score": 1.5454545454545454
    },
    {
        "original_text": "We report the first detection of silicon (Si), calcium (Ca) and iron (Fe) ions along with magnesium (Mg) in an intervening galaxy system toward quasar HE 0515-4414 at redshift 0.4485. The observed column densities are log N(Mg+H) = 13.60 ± 0.10 cm-2 , log N(Si+H) = 12.70 ± 0.20 cm-2 , log N (Ca + H ) = 11 .90 ± 0.30 cm -2 , and log N (Fe + H ) = 10.40 ± 0.50 cm -2 .\nThe total hydrogen column density is log NH = 20.0 +0.5 -0.3 cm-2 . We find that this system has low metallicity Z < 1/100 solar abundance ratio for all four elements detected.  This system shows no detectable neutral carbon or molecular hydrogen absorptions down to limits of log NC/NH ~ -1.7 and log MH / NH ~ -3.6 respectively.",
        "watermark_text": "We report the first detection of silicon ( Si ) , calcium ( Ca ) and iron ( Fe ) ions along with magnesium ( Mg ) in an intervening galaxy system toward quasar HE 0515 - 4414 at redshift 0 . 4485 . The observed column densities are log N ( Mg + H ) = 13 . 60 ± 0 . 10 cm - 2 , log N ( Si + H ) = 12 . 70 ± 0 . 20 cm - 2 , log N ( Ca + H ) = 11 . 90 ± 0 . 30 cm - 2 , and log N ( Fe + H ) = 10 . 40 ± 0 . 50 cm - 2 .The total hydrogen row density is log NH = 20 . 0 + 0 . 5 - 0 . 3 cm - 2 . We see that this system has low metallicity Z < 1 / 100 solar occurrence ratio for all four elements detected .This system displays no detectable neutral hydrogen or molecular hydrogen absorptions down to bounds of log NC / NH ~ - 1 . 7 and log MH / NH ~ - 3 . 6 respectively .",
        "rewrite_text": "We present the first detection of silicon (Si), calcium (Ca), iron (Fe) ions, and magnesium (Mg) in an intervening galaxy system toward the quasar HE 0515-4414 at a redshift of 0.4485. The measured column densities are log N (Mg + H) = 13.60 ± 0.10 cm⁻², log N (Si + H) = 12.70 ± 0.20 cm⁻², log N (Ca + H) = 11.90 ± 0.30 cm⁻², and log N (Fe + H) = 10.40 ± 0.50 cm⁻². The overall hydrogen column density is log NH = 20.0 +0.5 -0.3 cm⁻². Our findings indicate that this system exhibits low metallicity, with a metallicity ratio Z < 1/100 of solar levels for all four detected elements. Additionally, we found no detectable neutral hydrogen or molecular hydrogen absorptions, with limits of log NC / NH ~ -1.7 and log MH / NH ~ -3.6, respectively.",
        "ori-fast-z-score": 0.14586499149789456,
        "water-fast-z-score": 2.5924756956542794,
        "rewrite-fast-z-score": -0.15249857033260467
    },
    {
        "original_text": "The present work is devoted to the investigation of the photothermal properties and dynamics of the Cu2O/CuO nanocomposite films prepared by pulsed laser deposition (PLD) on Si(100). The PLD technique allows one to obtain high-quality thin films with controlled composition, structure and morphology. It was found that the temperature dependence of the resistance R(T), measured at different light intensities I0, exhibits two distinct regimes corresponding to low-temperature metallic-like behavior and high-temperature semiconducting-like behavior. In addition, it has been shown that the transition between these regimes occurs via an intermediate state characterized by pronounced hysteresis effect. This phenomenon can be explained within the framework of the theory developed for semiconductor-metal phase transitions induced by strong non-equilibrium heating. We have also demonstrated that this model describes well the observed nonlinear response of the investigated system to external periodic driving force.",
        "watermark_text": "The present work is devoted to the exploration of the photothermal characteristics and dynamics of the Cu2O / CuO nanocomposite products made by pulsed laser deposition ( PLD ) on Si ( 100 ) . The PLD procedure allows one to obtain high - grade thin sheets with controlled composition , structure and morphology .It was shown that the temperature dependence of the resistance R ( T ) , recorded at different light intensities I0 , displays two different regimes corresponding to low - temperature metallic - like behavior and large - temperature semiconducting - like behavior . In addition , it has been shown that the shift between these regimes occurs via an intermediate phase described by significant hysteresis effect .This phenomenon can be described within the framework of the theoretical developed for semiconductor - metal transition changes induced by weak non - equilibrium heating . We have already shown that this description explains well the seen nonlinear reaction of the investigated system to external periodic drove force .",
        "rewrite_text": "This study focuses on investigating the photothermal properties and dynamics of Cu2O/CuO nanocomposite materials fabricated through pulsed laser deposition (PLD) on Si (100) substrates. The PLD technique enables the production of high-quality thin films with precise control over their composition, structure, and morphology. Our findings indicate that the temperature dependence of resistance R(T), measured under varying light intensities I0, exhibits two distinct regimes: one resembling metallic behavior at low temperatures and the other exhibiting semiconducting characteristics at higher temperatures. Furthermore, we observed that the transition between these regimes involves an intermediate phase characterized by a significant hysteresis effect. This phenomenon can be explained using theories related to semiconductor-metal transition changes induced by slight non-equilibrium heating. Our previous research has demonstrated that this theoretical framework effectively accounts for the observed nonlinear response of the system to external periodic driving forces.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.812381937190964,
        "rewrite-fast-z-score": -0.11396057645963795
    },
    {
        "original_text": "We present the first two-dimensional kinematics for both stars and gas in the central kiloparsecs (kpc) of two nearby galaxies, NGC 3227 and NGC 4151. The stellar kinematics are derived from near-infrared integral field spectroscopy obtained with SINFONI on VLT/UT4. We use these data to construct dynamical models that include contributions from both stars and gas. Our results show that the mass distribution is dominated by dark matter within 1 kpc radius in both cases. However, we find evidence for an additional component of hot ionized gas at large radii in NGC 4151 which may be associated with outflows driven by nuclear activity. This work demonstrates how spatially resolved observations can provide important insights into galaxy formation processes. Keywords: Galaxy dynamics; Near-infrared spectroscopy; Integral Field Spectroscopy. Subject headings: Black holes - accretion disks; Nuclear star clusters",
        "watermark_text": "We present the first two - dimensional kinematics for both stars and gas in the central kiloparsecs ( kpc ) of two nearby galaxies , NGC 3227 and NGC 4151 . The stellar kinematics are derived from near - infrared integral field spectroscopy derived with SINFONI on VLT / UT4 .We use these information to build dynamical models that include contributions from both stars and gas . Our results show that the mass distribution is dominated by black material within 1 kpc radius in both cases .However , we find proof for an additional element of bright ionized gas at large radii in NGC 4151 which may be involved with outflows driven by nuclear activity . This research shows how spatially resolved surveys can provide important perspectives into star formation systems .Keywords : Galaxy biology ; Near - infrared spectroscopy ; Integral Field Spectroscopy . Subject headings : Black holes - accretion disks ; Nuclear star clusters",
        "rewrite_text": "We present the first two-dimensional kinematics of both stars and gas in the central kiloparsecs (kpc) of two nearby galaxies, NGC 3227 and NGC 4151. The stellar kinematics are obtained from near-infrared integral field spectroscopy using SINFONI on the VLT/UT4. We utilize this information to construct dynamical models that incorporate contributions from both stars and gas. Our findings indicate that the mass distribution within a 1 kpc radius is predominantly influenced by dark matter in both galaxies. However, we also discover evidence of a significant presence of bright ionized gas at larger radii in NGC 4151, which may be linked to outflows driven by nuclear activity. This research illustrates how spatially resolved surveys can offer crucial insights into star formation systems. Keywords: Galaxy biology; Near-infrared spectroscopy; Integral Field Spectroscopy. Subject headings: Black holes; Accretion disks; Nuclear star clusters.",
        "ori-fast-z-score": 0.2626128657194451,
        "water-fast-z-score": 4.296234062367188,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "In this work, we study the fractal dimension and Hausdorff measure of the set of points where  is not differentiable in . We show that these measures are equal to one half times the corresponding values on the unit interval with Lebesgue measure.  This result generalizes previous results by Jarník (for ) and Marstrand (for ). The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz. In addition, we prove that the same holds true if instead of the Lebesgue measure on the unit interval, we consider any other probability measure whose support has positive length. Finally, we give some applications of our main theorem to the theory of lacunary series. Let be the space of all complex-valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of .\nLet denote the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain. \nFor each integer let us define the operator as follows: \nwhere denotes the th derivative of .  It can easily be seen that is continuous linear functional on .",
        "watermark_text": "In this research , we study the fractal degree and Hausdorff measure of the set of points where is not differentiable in . We see that these measures are equal to one half times the equivalent values on the unit interval with Lebesgue measure .This result generalizes earlier findings by Jarník ( for ) and Marstrand ( for ) . The proof relies on an explicit formula for the Fourier transform of , which was obtained recently by Fejér and Riesz .In addition , we prove that the same stands true if instead of the Lebesgue measure on the unit interval , we choose any other probability measure whose support has positive length . Finally , we give some applications of our major principle to the theory of lacunary series .Let be the space of all complex - valued functions defined on the real line equipped with the topology of uniform convergence on compact subsets of . Let define the class of all infinitely differentiable functions on such that their derivatives satisfy the Hölder condition with exponent greater than or equal to 1 at every point of the domain .For each integer take us define the operator as follows : where denotes the th derivative of . It can easily be shown that is continuous continuous functional on .",
        "rewrite_text": "In this study, we investigate the fractal dimension and Hausdorff measure of the set of points where a function is not differentiable. Our findings indicate that these measures are equal to half of the corresponding values on the unit interval with respect to the Lebesgue measure. This result expands upon previous work by Jarník and Marstrand. The proof is grounded in an explicit formula for the Fourier transform of the function, which was recently derived by Fejér and Riesz. Furthermore, we demonstrate that the same results hold if we consider any probability measure with support of positive length instead of the Lebesgue measure on the unit interval. Finally, we explore applications of our main result to the theory of lacunary series. We define the space of all complex-valued functions on the real line, equipped with the topology of uniform convergence on compact subsets. We also define the class of infinitely differentiable functions whose derivatives satisfy the Hölder condition with an exponent of at least 1 at every point. For each integer, we define an operator that applies to the derivatives of these functions. It can be easily shown that this operator is a continuous functional on the space.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 4.061277619618543,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "The purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group s work, and students  written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.",
        "watermark_text": "The purpose of this study is to examine the part that assessment plays in interaction between instructors and teachers during computer - based educational environments ( CBLE ) . The research question guiding this analysis is : How does evaluation affect student - teacher contact ?This study was done with two groups of freshman pupils enrolled in an introductory program on educational software at a large Midwestern college . Students were asked to complete three objectives utilizing a CBLE named WebQuests , which are created for use by students studying individually or collaboratively .Data collection included audio prints of group discussions , field notes taken by researchers observing each team s project , and students authored reactions to questions posed throughout their involvement in the project . Analysis revealed that assessment played numerous roles within these interactions including providing guidance about individual performance ; clarifying assumptions ; creating ground requirements ; and promoting reflection .These data suggest that assessment can be used effectively to affect student - teacher understanding when it appears frequently enough over time so that both parties have multiple options to respond to one another .",
        "rewrite_text": "This study aims to investigate the role of assessment in the interactions between instructors and students within computer-based educational environments (CBLE). The central research question is: How does evaluation influence student-teacher engagement? The research involved two groups of first-year students enrolled in an introductory program on educational software at a large Midwestern university. Students engaged with a CBLE called WebQuests, designed for individual or collaborative study, to complete three specific objectives. Data collection included audio recordings of group discussions, field notes from researchers observing each team's project, and written responses from students to questions posed during their participation in the project. The analysis highlighted that assessment served various functions in these interactions, such as providing insights on individual performance, clarifying assumptions, establishing baseline requirements, and encouraging reflection. The findings suggest that frequent and timely assessment can enhance student-teacher understanding, allowing both parties ample opportunities to engage with each other.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 7.939798740547776,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "We report infrared (IR) spectroscopic studies on the formation and evolution of formic acid, HCOOH, in ices under simulated astrophysical conditions. The experiments were performed by exposing pure water or mixtures of H2O with CH3OH to vacuum ultraviolet radiation at 10 K for different periods up to 100 hours. IR spectra show that the amount of HCOOH increases as a function of irradiation time. We also find evidence for the presence of other species such as CO2, CO, CH4, NH3, H2S, SO2, and OCS. These results are discussed within the context of astrochemical models. Formic acid is one of the most abundant organic molecules found in space. It has been detected towards comets, protostars, and evolved stars. In particular, it was observed in comet 67P/Churyumov-Gerasimenko during its flyby of the Rosetta spacecraft. However, despite this ubiquity, our understanding about how formic acid forms in space remains limited. Here we present laboratory investigations into the formation pathways of formic acid in icy environments using infrared spectroscopy.",
        "watermark_text": "We report imaging ( IR ) spectroscopic studies on the formation and evolution of formic acid , HCOOH , in ices under modeled astrophysical conditions . The studies were performed by exposing pure water or mixtures of H2O with CH3OH to vacuum ultraviolet radiation at 10 K for different times up to 100 hours .IR spectra show that the quantity of HCOOH changes as a function of irradiation time . We additionally find proof for the presence of other species such as CO2 , CO , CH4 , NH3 , H2S , SO2 , and OCS .These data are discussed within the context of astrochemical models . Formic acid is one of the most abundant chemical molecules seen in space .It has been detected towards comets , protostars , and evolved planets . In particular , it was seen in comet 67P / Churyumov - Gerasimenko during its flyby of the Rosetta spacecraft .However , despite this ubiquity , our knowing about how formic oxide occurs in space remains restricted . Here we present lab studies into the formation routes of formic ethanol in icy environments using infrared spectroscopy .",
        "rewrite_text": "We present infrared (IR) spectroscopic studies examining the formation and evolution of formic acid (HCOOH) in ices under simulated astrophysical conditions. This research involved exposing pure water and mixtures of H2O with CH3OH to vacuum ultraviolet radiation at a temperature of 10 K for varying durations, up to 100 hours. The IR spectra indicate that the amount of HCOOH produced varies with the duration of irradiation. Additionally, we have identified the presence of other species, including CO2, CO, CH4, NH3, H2S, SO2, and OCS. These findings are analyzed in the context of astrochemical models. Formic acid is one of the most prevalent chemical compounds detected in space, found in comets, protostars, and evolved planets. Notably, it was observed in comet 67P/Churyumov-Gerasimenko during the Rosetta spacecraft's flyby. Despite its widespread occurrence, our understanding of how formic acid is generated in space remains limited. In this study, we focus on laboratory investigations into the formation pathways of formic acid in icy environments, utilizing infrared spectroscopy.",
        "ori-fast-z-score": 0.3779644730092272,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "The availability of computer systems is an important aspect in the design, development and operation of any system. The aim of this research was to develop a method for assessing the availability of a large number of SunOS/Solaries systems using syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). This research has been carried out by collecting data from a set of servers over a period of one year. A total of 1,000,000 records were collected during that time frame. These records have then been processed into a database which contains information about each record such as timestamps, host name, service type etc.. In order to assess the availability of these systems we developed two algorithms; one algorithm calculates the percentage uptime per day while another algorithm calculates the percentage downtime per hour. We also implemented a web interface so that users can view the results obtained from our analysis.",
        "watermark_text": "The availability of computer machines is an important element in the development , construction and operation of any program . The goal of this research was to develop a technique for evaluating the availability of a large number of SunOS / Solaries machines combining syslogd ( System Logging Daemon ) and wtmpx ( Windows Time Machine eXtender ) .This research has been carried out by collecting data from a setting of servers over a period of one year . A total of 1 , 000 , 000 data were collected during that time frame .These data have then been processed into a computer which contains information about each record such as timestamps , host title , service size etc . . In order to analyze the availability of these systems we developed two strategies ; one algorithm calculates the percentage uptime per month while another algorithm calculates the percentage downtime per hour .We additionally introduced a web application so that users can view the results derived from our analysis .",
        "rewrite_text": "The availability of computer systems is a crucial factor in the development, construction, and operation of any software program. This research aimed to create a method for assessing the availability of a large number of SunOS/Solaris machines by integrating syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). Data was collected from a set of servers over the course of one year, resulting in a total of 1,000,000 records. These records were processed on a computer that captured various details, including timestamps, host names, and service sizes. To evaluate the availability of these systems, we developed two analytical approaches: one algorithm computes the monthly uptime percentage, while another calculates the hourly downtime percentage. Additionally, we implemented a web application that allows users to access the results of our analysis.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We study the correspondence principle between string theory on anti-de Sitter space-time (AdS(3)) and field theories at finite temperature, by using Hagedorn strings as probes. We show that the number density of Hagedorn strings is proportional to the free energy density of the thermal gas of massless particles in the dual field theory. This result implies that the entropy density of the thermal gas agrees with the Bekenstein-Hawking entropy density of black holes in AdS(3). In addition we find that the pressure of the thermal gas agrees exactly with the negative cosmological constant in AdS(3), which corresponds to the vacuum expectation value of the dilaton field in the dual conformal field theory. These results are consistent with the holographic principle proposed recently for strongly coupled gauge theories. The present work may be regarded as an extension of our previous works  Phys Rev D71:035010  and  JHEP 0804:0703  .",
        "watermark_text": "We research the correspondence principle between string theory on anti - de Sitter space - time ( AdS ( 3 ) ) and field theories at finite temperature , by using Hagedorn strings as probes . We see that the number density of Hagedorn strings is proportional to the free energy density of the thermal gas of massless molecules in the dual field theory .This result means that the entropy concentration of the thermal gas agrees with the Bekenstein - Hawking entropy concentration of black holes in AdS ( 3 ) . In addition we find that the pressure of the thermal gas agrees exactly with the negative cosmological coefficient in AdS ( 3 ) , which equals to the vacuum expectation value of the dilaton field in the dual conformal field model .These conclusions are compatible with the holographic principle adopted recently for highly coupled gauge fields . The present work would be regarded as an addition of our previous works Phys Rev D71 : 035010 and JHEP 0804 : 0703 .",
        "rewrite_text": "We investigate the correspondence principle between string theory in anti-de Sitter space-time (AdS(3)) and field theories at finite temperature, using Hagedorn strings as probes. Our analysis reveals that the number density of Hagedorn strings is directly proportional to the free energy density of the thermal gas composed of massless particles in the corresponding dual field theory. This finding indicates that the entropy density of the thermal gas aligns with the Bekenstein-Hawking entropy density of black holes in AdS(3). Furthermore, we discover that the pressure of the thermal gas precisely matches the negative cosmological constant in AdS(3), which corresponds to the vacuum expectation value of the dilaton field in the dual conformal field theory. These results are consistent with the recently adopted holographic principle for strongly coupled gauge fields. This study extends our prior works, Phys. Rev. D71: 035010 and JHEP 0804: 0703.",
        "ori-fast-z-score": 0.47809144373375745,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": 0.23904572186687872
    },
    {
        "original_text": "We present results on semi-inclusive deep-inelastic scattering (SIDIS), ep -> e  p X , with an emphasis on final state interaction effects for both proton and neutron targets. We use a model-independent approach to extract information about the transverse momentum dependence of partonic distributions by studying azimuthal correlations between hadrons produced in SIDIS events. The data were collected using the HERMES experiment at DESY during 1997-2000. Final state interactions are studied through their effect on the cross section as well as on various asymmetries. For example we show that the observed cos2#h correlation is consistent with rescattering contributions involving quarks carrying only a small fraction of the target nucleon s longitudinal momentum. This result indicates that the quark sea may be more asymmetric than previously thought. Furthermore, our analysis shows that the magnitude of the rescattering contribution depends strongly on the kinematics chosen. Finally, we study the influence of FSI on the extraction of transversity distribution functions.",
        "watermark_text": "We report findings on quasi - inclusive deep - inelastic scattering ( SIDIS ) , ep - > e p X , with an emphasis on final state interaction effects for both proton and neutron targets . We use a theory - independent approach to extract information about the transverse momentum dependence of partonic distributions by examining azimuthal correlations between hadrons observed in SIDIS events .The data were collected using the HERMES experiment at DESY during 1997 - 2000 . Final state effects are studied through their effect on the cross section as well as on various asymmetries .For instance we find that the reported cos2 # h relationship is compatible with rescattering contributions using quarks carrying only a small fraction of the target nucleon s longitudinal momentum . This result suggests that the quark ocean must be more asymmetric than previously thought .Furthermore , our analysis shows that the extent of the rescattering contribution varies strongly on the kinematics selected . Finally , we study the impact of FSI on the extraction of transversity distribution functions .",
        "rewrite_text": "We present our findings on quasi-inclusive deep-inelastic scattering (SIDIS), specifically the process ep → e p X, focusing on the effects of final state interactions for both proton and neutron targets. By employing a theory-independent method, we have extracted information regarding the transverse momentum dependence of partonic distributions through the analysis of azimuthal correlations between hadrons detected in SIDIS events. The data for this study were collected during the HERMES experiment at DESY from 1997 to 2000. We examine final state interactions by assessing their influence on the cross section and various asymmetries. Notably, our results indicate that the reported cos2 # h relationship aligns with rescattering contributions involving quarks that carry only a small portion of the target nucleon's longitudinal momentum, suggesting that the quark sea may be more asymmetric than previously believed. Additionally, our analysis reveals that the magnitude of the rescattering contributions is highly dependent on the selected kinematics. Finally, we investigate the implications of final state interactions on the extraction of transversity distribution functions.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 4.242640687119286,
        "rewrite-fast-z-score": -1.1322770341445956
    },
    {
        "original_text": "We present the results of multiepoch radial velocity (RV) observations for four late-type M dwarfs and three brown dwarf candidates with spectral types ranging between M3V-M8V, obtained using the High Accuracy Radial velocity Planet Searcher (HARPS). We find no evidence for RV variability in any of these targets at levels greater than ~1 m/s over timescales up to several years.  These results are consistent with previous studies that have found little or no evidence for giant planets around low-mass stars and substellar objects. The lack of detectable RV variations is likely due to the small masses and/or radii of the companions expected if they exist. However, we cannot rule out the possibility that some of our targets harbor terrestrial mass planets on orbits with periods longer than those probed by our current data set. Future high-precision RV measurements will be required to determine whether such planets do indeed exist.",
        "watermark_text": "We present the conclusion of multiepoch radial speed ( RV ) observations for four late - class M dwarfs and three brown giant candidates with spectral classes ranging between M3V - M8V , obtained using the High Accuracy Radial velocity Planet Searcher ( HARPS ) . We see no evidence for RV variability in any of these targets at levels greater than ~ 1 m / s over timescales up to several years .These conclusions are accordance with previous research that have discovered nothing or no evidence for big planets around low - density stars and substellar objects . The absence of detectable RV variations is probably due to the small masses and / or radii of the companions expected if they exist .However , we cannot leave out the suggestion that some of our objectives harbor terrestrial mass stars on orbits with periods longer than those probed by our latest data set . Future high - precision RV measurements will be required to test whether such planets do actually appear .",
        "rewrite_text": "We summarize the findings from multiepoch radial velocity (RV) observations of four late-class M dwarfs and three candidates for brown giants, with spectral classifications ranging from M3V to M8V. These observations were conducted using the High Accuracy Radial velocity Planet Searcher (HARPS). Our analysis shows no signs of RV variability exceeding ~1 m/s over timescales of several years for any of the targets. These results align with previous studies that have found little to no evidence of significant planets orbiting low-density stars and substellar objects. The lack of detectable RV variations may be attributed to the small masses and/or radii of potential companions. However, we cannot completely rule out the possibility that some of our targets may host terrestrial-mass planets on longer orbital periods than those examined in our recent data set. Future high-precision RV measurements will be essential to determine if such planets are present.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": -0.8551861104941365
    },
    {
        "original_text": "We have obtained high resolution (R = λ/Δλ ~ 20000) optical spectra for two bright HII regions in the extremely metal poor galaxy IZw18, using the Keck telescope and its High Resolution Echelle Spectrometer (HIRES). The main results are as follows:\n(1) We find that the oxygen abundance is 12+log(O/H)=7.16±0.03 dex at R=3′′ and 7.10±0.02 dex at R=1′′.\n(2) From our analysis we conclude that there exists an apparent gradient in the O/H ratio across this galaxy.  This result suggests that the chemical enrichment process has proceeded more rapidly towards the center than along the outer parts of the galaxy. In addition to these observations, we also present new measurements on the carbon content of this galaxy. Our data show that C/O=0.25 ± 0.05 at R=3′′ and 0.20 ± 0.04 at R=1′′.",
        "watermark_text": "We have discovered high resolution ( R = λ / Δλ ~ 20000 ) imaging spectra for two faint HII zones in the exceptionally iron poor galaxy IZw18 , using the Keck lens and its High Resolution Echelle Spectrometer ( HIRES ) . The main results are as follows : ( 1 ) We see that the oxygen abundance is 12 + log ( O / H ) = 7 . 16±0 . 03 dex at R = 3 ′ ′ and 7 . 10±0 . 02 dex at R = 1 ′ ′ .( 2 ) From our analysis we conclude that there exists an apparent gradient in the O / H ratio across this galaxy . This result suggests that the chemical enrichment reaction has continued more swiftly towards the center than along the inner parts of the universe .In addition to these observations , we also provided new data on the carbon concentration of this galaxy . Our data demonstrate that C / O = 0 . 25 ± 0 . 05 at R = 3 ′ ′ and 0 . 20 ± 0 . 04 at R = 1 ′ ′ .",
        "rewrite_text": "We have obtained high-resolution (R = λ / Δλ ~ 20000) imaging spectra of two faint HII regions in the remarkably iron-poor galaxy IZw18, utilizing the Keck telescope and its High Resolution Echelle Spectrometer (HIRES). The key findings are as follows: (1) the oxygen abundance is measured at 12 + log (O/H) = 7.16 ± 0.03 dex at R = 3″ and 7.10 ± 0.02 dex at R = 1″. (2) Our analysis reveals an apparent gradient in the O/H ratio throughout the galaxy, indicating that the process of chemical enrichment has progressed more rapidly toward the center compared to the outer regions. Additionally, we have presented new data regarding the carbon concentration in this galaxy, showing that C/O = 0.25 ± 0.05 at R = 3″ and 0.20 ± 0.04 at R = 1″.",
        "ori-fast-z-score": -0.42857142857142855,
        "water-fast-z-score": 4.714285714285714,
        "rewrite-fast-z-score": 0.14285714285714285
    },
    {
        "original_text": "We present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "We present the first measurement of the supermassive black hole ( SMBH ) mass distribution for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) clusters using data from the Millennium Galaxy Catalogue ( MGC ) . We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations .Our results show that there is no major variation between the SMBH mass distributions of these galaxy forms at z < 0 . 1 . However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones .This implies that the most gigantic SMBHs are likely to have expanded by accretion over universe time rather than joining events . These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "We present the first measurement of the mass distribution of supermassive black holes (SMBHs) in both late-type (elliptical/S0, Sa-Sb) and late-type (Scd-Sm) clusters, utilizing data from the Millennium Galaxy Catalogue (MGC). Our analysis employs two distinct methods for determining SMBH masses: stellar velocity dispersion measurements and bulge luminosity scaling relations. Our findings indicate that there is no significant difference in the SMBH mass distributions of these galaxy types at redshifts less than 0.1. However, we do observe evidence of evolution with redshift, revealing that the number density of large SMBHs declines more rapidly than that of their less massive counterparts. This suggests that the largest SMBHs have likely grown through accretion over cosmic time rather than through merger events. These insights will provide essential constraints for understanding SMBH growth and the mechanisms of AGN feedback.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "We present an algorithm for solving the Graph Isomorphism Problem (GIP) based on Quantum Walks and Grover s search algorithm. The proposed method is inspired by the classical algorithms that use random walks, but it uses Grover s operator instead of the Hadamard matrix in order to speed up the process. We show how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit. Finally we compare our results against other state-of-the-art methods. In recent years there has been growing interest in developing efficient solutions to problems related to computational complexity theory such as the Graph Isomorphism Problem(GIP). This problem consists of determining whether two given graphs are isomorphic or not; i.e., if they have the same structure regardless of their labels. \n \n Classical approaches to solve GIP usually rely on Random Walks techniques combined with some heuristics. However these methods require exponential time in the worst case scenario. On the other hand, Quantum Algorithms provide polynomial-time solutions to many NP-complete problems including GIP. These algorithms exploit the superposition principle which allows them to explore all possible states simultaneously. For example, Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any item within a database in quadratic time.",
        "watermark_text": "We present an algorithm for solving the Graph Isomorphism Problem ( GIP ) based on Quantum Walks and Grover s search algorithm . The proposed approach is influenced by the classical techniques that use random walks , but it utilizes Grover s operator rather of the Hadamard vector in order to faster up the process .We see how this new technique can be used to solve GIP with high chance when the number of vertices in both graphs are equal or differ at most one unit . Finally we compare our findings against other state - of - the - art methods .In recent years there has been growing interest in pursuing efficient answers to problems related to computational complexity analysis such as the Graph Isomorphism Problem ( GIP ) . This problem consists of determining whether two given graphs are isomorphic or not ; i . e . , if they have the same structure regardless of their labels .Classical approaches to solve GIP usually relies on Random Walks techniques combined with some heuristics . However these algorithms involve exponential time in the worst case situations .On the other hand , Quantum Algorithms provide polynomial - time solutions to many NP - perfect issues including GIP . These methods incorporate the superposition concept which allows them to examine all possible states simultaneously .For instance , Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any element within a database in quadratic time .",
        "rewrite_text": "We propose an algorithm to address the Graph Isomorphism Problem (GIP) that leverages Quantum Walks and Grover's search algorithm. Our approach is inspired by traditional techniques utilizing random walks but opts for Grover's operator instead of the Hadamard vector, thereby accelerating the process. We demonstrate how this novel method can effectively solve the GIP with a high probability when the number of vertices in both graphs is equal or differs by at most one. Additionally, we evaluate our results against other cutting-edge methods. Recently, there has been heightened interest in finding efficient solutions to computational complexity problems like GIP, which involves determining whether two graphs are isomorphic—meaning they share the same structure, irrespective of their labels. Classical methods for tackling GIP typically rely on Random Walk techniques alongside various heuristics, but these can require exponential time in the worst-case scenarios. In contrast, quantum algorithms offer polynomial-time solutions for several NP-complete problems, including GIP. These quantum methods take advantage of the principle of superposition, enabling them to explore all potential states concurrently. For example, Shor's Algorithm efficiently tackles integer factorization in polynomial time, while Grover's Search can locate any element in a database in quadratic time.",
        "ori-fast-z-score": -0.10050378152592121,
        "water-fast-z-score": 4.975196209154734,
        "rewrite-fast-z-score": -1.3337718577107005
    },
    {
        "original_text": "We study thermodynamics and transport properties of the Kondo necklacemodel (KNM) in the vicinity of its quantum phase transition to an ordered state, which is driven by spin fluctuations. We show that this transition can be described within the framework of Landau-Ginzburg-Wilson theory with a nontrivial scaling dimension of the order parameter field. The latter determines the universality class of the transition as well as the temperature dependence of various physical quantities such as specific heat or resistivity. In particular we find that at low temperatures the system exhibits non-Fermi liquid behavior characterized by power-law dependences of these quantities on T . \nI. INTRODUCTORY REMARkS\nThe Kondo necklace model 1 ,2 describes a chain of magnetic impurities coupled via antiferromagnetic exchange interactions J. It has been introduced originally for describing the physics of heavy fermion compounds 3 but it also appears naturally in other contexts 4-7 .\nIn the present work we consider the case when the coupling constant J exceeds some critical value Jc = 2t / U where t denotes hopping amplitude between neighboring sites and U stands for local Coulomb repulsion energy 8-10 . At zero temperature the ground state of the system corresponds then to a ferromagnetically ordered state 11-13 while at finite temperatures one expects a continuous quantum phase transition into a paramagnetic state 14-17 . This transition occurs due to strong spin fluctuations 18-20 and belongs therefore to the same universality class as the Heisenberg chain 21-23 . However there are important differences between both models 24-26 : First, the Kondo necklace contains two types of excitations -spinons and holons 27-29 -while the Heisenberg chain only consists of spin-1/2 particles. Second, the Kondo necklaces contain additional degrees of freedom associated with charge carriers 30-32 . These features lead to new phenomena not observed in the Heisenberg chain 33-36 . For example, the Kondo necklacelike systems may exhibit unconventional superconducting states 37-39 .",
        "watermark_text": "We research thermodynamics and transport properties of the Kondo necklacemodel ( KNM ) in the vicinity of its quantum phase change to an ordered state , which is caused by spin fluctuations . We see that this shift can be described within the framework of Landau - Ginzburg - Wilson theory with a nontrivial scaling dimension of the order parameter field .The latter determines the universality type of the transition as well as the temperature dependence of several physical substances such as certain heat or resistivity . In particular we find that at low temperatures the system displays non - Fermi solid behavior defined by power - law dependences of these quantities on T .I . INTRODUCTORY REMARkS The Kondo necklace model 1 , 2 describes a string of magnetic impurities coupled via antiferromagnetic exchange interactions J .It has been proposed originally for describing the physics of large fermion compounds 3 but it also occurs commonly in other contexts 4 - 7 . In the present work we define the case when the interaction factor J exceeds some essential value Jc = 2t / U where t refers hopping frequency between neighboring regions and U stands for local Coulomb repulsion energy 8 - 10 .At zero temperature the ground state of the system belongs then to a ferromagnetically ordered state 11 - 13 while at finite temperatures one expects a continuous quantum phase change into a paramagnetic state 14 - 17 . This shift occurs due to heavy spin fluctuations 18 - 20 and belongs hence to the same universality category as the Heisenberg chain 21 - 23 .However there are important differences between both models 24 - 26 : First , the Kondo necklace includes two forms of excitations - spinons and holons 27 - 29 - while the Heisenberg chain only includes of spin - 1 / 2 atoms . Second , the Kondo necklaces feature additional degrees of liberty associated with charge carriers 30 - 32 .These features lead to novel processes not observed in the Heisenberg chain 33 - 36 . For instance , the Kondo necklacelike structures could display unconventional superconducting states 37 - 39 .",
        "rewrite_text": "We investigate the thermodynamics and transport properties of the Kondo necklace model (KNM) near its quantum phase transition to an ordered state driven by spin fluctuations. Our findings indicate that this transition can be analyzed using the Landau-Ginzburg-Wilson theory, which introduces a nontrivial scaling dimension for the order parameter field. This dimension influences the universality class of the transition and the temperature dependence of various physical properties, such as specific heat and resistivity. Notably, we observe that at low temperatures, the system exhibits non-Fermi liquid behavior characterized by power-law relationships for these properties as a function of temperature (T). \n\nI. INTRODUCTORY REMARKS  \nThe Kondo necklace model describes a sequence of magnetic impurities linked via antiferromagnetic exchange interactions (J). Originally proposed to explain the physics of large fermionic compounds, this model is also relevant in various other contexts. In this study, we focus on the scenario where the interaction parameter J surpasses a critical value, Jc = 2t / U, where t represents the hopping frequency between adjacent regions and U denotes the local Coulomb repulsion energy. At absolute zero, the system's ground state is a ferromagnetically ordered state, while at finite temperatures, a continuous quantum phase transition to a paramagnetic state is anticipated. This transition is driven by significant spin fluctuations and falls into the same universality class as the Heisenberg chain. However, there are key distinctions between the two models: firstly, the Kondo necklace incorporates two types of excitations—spinons and holons—whereas the Heisenberg chain contains only spin-1/2 particles. Secondly, the Kondo necklace features additional degrees of freedom related to charge carriers. These characteristics give rise to novel phenomena not observed in the Heisenberg chain, such as the potential for unconventional superconducting states within Kondo necklace-like structures.",
        "ori-fast-z-score": -0.5222329678670935,
        "water-fast-z-score": 6.7381737195346085,
        "rewrite-fast-z-score": 0.43685202833051895
    },
    {
        "original_text": "We have investigated the frequency distribution of semi-major axis (SMA) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical evolution models. We used data obtained by the Two Micron All Sky Survey (2MASS), which is complete down to Ks = 12 mag, corresponding to masses as low as 0.1 M⊙ at distances up to 1 kpc. The sample consists of 13,000 pairs selected using color-color criteria designed to select main-sequence stars. Using Monte Carlo simulations we found that our results are not affected significantly by incompleteness effects due to photometric errors or contamination by background galaxies. Our analysis shows that there exists an excess number of systems with SMA between 10 4 -10 5 AU compared to predictions based on standard cosmological models. This result suggests that either these systems were formed earlier than predicted by current theories or they may be primordial objects such as Population III remnants.",
        "watermark_text": "We have analyzed the frequency distribution of semi - major axis ( SMA ) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical development predictions . We utilized information obtained by the Two Micron All Sky Survey ( 2MASS ) , which is complete down to Ks = 12 mag , equivalent to masses as low as 0 . 1 [UNK] at distances up to 1 kpc .The sample consists of 13 , 000 couples chosen using color - color factors created to select primary - sequence stars . Using Monte Carlo simulations we concluded that our findings are not affected substantially by incompleteness effects due to photometric failures or exposure by background galaxies .Our study shows that there exists an excess amount of components with SMA between 10 4 - 10 5 AU compared to expectations based on normal cosmological predictions . This result suggests that either these systems were created earlier than expected by current theories or they may be primordial objects such as Population III fragments .",
        "rewrite_text": "We have examined the frequency distribution of semi-major axes (SMA) for wide binaries with separations exceeding 1000 AU to evaluate predictions from cosmogonies and dynamical evolution models. This analysis utilized data from the Two Micron All Sky Survey (2MASS), which is complete for magnitudes down to Ks = 12, corresponding to masses as low as 0.1 [UNK] at distances up to 1 kpc. Our sample includes 13,000 pairs selected using color-color criteria designed to isolate main sequence stars. Through Monte Carlo simulations, we determined that our results are not significantly influenced by incompleteness resulting from photometric errors or contamination by background galaxies. Our findings indicate an excess of components with SMA ranging from 10^4 to 10^5 AU, surpassing expectations based on standard cosmological models. This suggests that these systems may have formed earlier than current theories predict or could represent primordial objects, such as fragments from Population III stars.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": -1.25
    },
    {
        "original_text": "We study the dynamics of free fermions hopping between sites of an arbitrary connected graph, with no restriction to nearest-neighbor hopping. We show that this system is equivalent to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices. The collision rate depends only on the number of particles present at each vertex; it vanishes for graphs without loops or multiple edges (e.g., trees), but can be arbitrarily large otherwise. This model exhibits interesting behavior even when all rates are equal, including anomalous diffusion and superdiffusion. In particular, we prove that the mean-square displacement grows as t3/2 for any tree-like graph, while it scales faster than t2/3 for general graphs. Finally, we discuss possible extensions of our results beyond the free-fermion case. Introduction: A wide variety of physical phenomena ranging from quantum transport through mesoscopic systems  1  , to population biology  2  , involve non-equilibrium particle dynamics on networks. These models typically assume that particles move along directed links according to some prescribed rules, such as unrestricted hopping  3  . However, many real-world situations require more complicated interactions among particles  4  .\nIn this work, we consider a simple generalization of standard one-dimensional lattice models  5  by allowing particles to hop freely between adjacent nodes of an arbitrary connected graph G = (V, E). More precisely, let us fix a finite set S of states associated with each node v ∈ V ; then, given a configuration c : V → S, we define the state space C(G) := {c: V → S}. For every edge e = {u, v} ∈ E, we associate two transition probabilities p+(c, c )(e) ≥ 0 and p−(c, c )(u, v) > 0; these represent the probability per unit time that a particle located at u jumps to v if its current state is c, and vice versa. Then, the evolution of the system is described by a continuous-time Markov process Xt taking values in C(G).\nThe main goal of this Letter is to analyze the",
        "watermark_text": "We explore the dynamics of free fermions hopping between locations of an arbitrary linked graph , with no limitation to nearest - neighbor hopping . We see that this scheme is analogous to a collection of independent random tours advancing in concurrent and communicating via pairwise collisions at vertices .The crash time depends only on the quantity of particles present at each vertex ; it vanishes for graphs without loops or multiple edges ( e . g . , trees ) , but can be arbitrarily huge otherwise . This theory exhibits unusual phenomena even when all rates are equal , notably anomalous absorption and superdiffusion .In particular , we prove that the mean - square displacement grows as t3 / 2 for any forest - like graph , while it scales faster than t2 / 3 for general graphs . Finally , we explain possible extensions of our findings beyond the free - fermion case .Introduction : A wide multitude of natural concepts ranging from particle transport through mesoscopic systems 1 , to population physics 2 , employ non - equilibrium molecule interactions on networks . These models usually assumption that particles moving along coordinated links according to some prescribed rules , such as unrestricted hopping 3 .However , many actual - time situations involve more complicated relationships among particles 4 . In this research , we define a simple generalization of standard one - dimensional crystal models 5 by requiring particles to hop freely between neighboring vertices of an arbitrary linked graph G = ( V , E ) .More specifically , let us fix a finite collection S of states associated with each node v ∈ V ; then , given a configuration c : V → S , we define the state collection C ( G ) : = { c : V → S } . For every edge e = { u , v } ∈ E , we associate two transition probabilities p + ( c , c ) ( e ) ≥ 0 and p− ( c , c ) ( v , v ) > 0 ; these denote the probability per unit time that a particle situated at u escapes to v if its current state is p , and vice versa .Then , the evolution of the process is characterized by a continuous - time Markov process Xt assuming variables in C ( G ) . The main goal of this Letter is to analyze the",
        "rewrite_text": "We investigate the behavior of free fermions as they hop across the nodes of an arbitrary linked graph, allowing movements beyond just nearest-neighbor interactions. This setup can be likened to a series of independent random tours that progress simultaneously and interact through pairwise collisions at the graph's vertices. The duration until a collision occurs is influenced solely by the number of particles at each vertex; it disappears entirely in graphs with no loops or multiple edges (such as trees), yet can be exceedingly long in other configurations. Our theory reveals striking phenomena even under conditions where all rates are identical, such as unusual absorption patterns and superdiffusion. Specifically, we demonstrate that the mean square displacement increases as \\( t^{3/2} \\) for any forest-like graph, while it exceeds \\( t^{2/3} \\) for general graphs. We also discuss potential expansions of our results beyond the free-fermion scenario.\n\n**Introduction:** A broad spectrum of natural processes, from particle transport in mesoscopic systems to dynamics in population physics, involves non-equilibrium interactions among particles on networks. These models typically assume that particles traverse designated links following specific rules like unrestricted hopping. Nevertheless, many real-time situations involve more complex interactions. In this study, we introduce a straightforward extension of traditional one-dimensional crystal models by allowing particles to freely hop between neighboring vertices of an arbitrary linked graph \\( G = (V, E) \\). More precisely, we consider a finite set \\( S \\) of states associated with each vertex \\( v ∈ V \\). Given a configuration \\( c: V \\to S \\), we define the state space \\( C(G) = \\{ c: V \\to S \\} \\). For each edge \\( e = \\{ u, v \\} ∈ E \\), we define two transition probabilities \\( p^+(c, c')(e) \\geq 0 \\) and \\( p^-(c, c') \\geq 0 \\); these represent the probability per unit time that a particle at vertex \\( u \\) moves to vertex \\( v \\) based on its current state. The evolution of this process is governed by a continuous-time Markov process \\( X_t \\), which takes values in \\( C(G) \\). The primary aim of this Letter is to analyze the...",
        "ori-fast-z-score": -1.3000224919331833,
        "water-fast-z-score": 7.429360827073253,
        "rewrite-fast-z-score": 2.0059435495071947
    },
    {
        "original_text": "We revisit the question whether or not there is an excess in cosmic ray flux near Supernova Remnant (SNR) shells, as reported by PAMELA and AMS-02 experiments. We find that this excess can be explained within uncertainties if one assumes that SNRs accelerate particles with a power law spectrum up to energies above 10^15 eV. The required spectral index for protons is 2.2 +/- 0.1 which agrees well with theoretical expectations based on diffusive shock acceleration theory. For electrons we require a harder spectrum with a slope of 3.0 +/- 0.3. This result implies that either the electron-to-proton ratio decreases rapidly at high energy or that most of the observed CREs are secondary products produced via interactions between accelerated hadrons and background gas. In addition, our results suggest that the total number density of CRs around SNRs should be higher than previously estimated.",
        "watermark_text": "We revisit the question whether or not there is an amount in cosmic ray density near Supernova Remnant ( SNR ) shells , as described by PAMELA and AMS - 02 experiments . We see that this excess can be described within uncertainties if one suppose that SNRs move particles with a power law spectrum up to energies above 10 ^ 15 eV .The expected spectral index for protons is 2 . 2 + / - 0 . 1 which agrees well with theoretical expectations depending on diffusive blast acceleration physics . For electrons we require a deeper spectrum with a slope of 3 . 0 + / - 0 . 3 .This result suggests that either the electron - to - proton ratio falls swiftly at high energy or that most of the seen CREs are secondary derivatives produced via interactions between advanced hadrons and background plasma . In addition , our findings confirm that the total number density of CRs around SNRs should be higher than previously predicted .",
        "rewrite_text": "We revisit the inquiry into whether there is a significant cosmic ray density near Supernova Remnant (SNR) shells, as reported by the PAMELA and AMS-02 experiments. Our analysis indicates that this excess can be explained within the uncertainties by assuming that SNRs accelerate particles following a power law spectrum, reaching energies above 10^15 eV. The anticipated spectral index for protons is 2.2 ± 0.1, which aligns well with theoretical predictions based on the principles of diffusive shock acceleration. For electrons, however, a steeper spectrum with a slope of 3.0 ± 0.3 is required. This finding implies that either the electron-to-proton ratio declines sharply at high energies or that the observed cosmic ray electrons (CREs) are predominantly secondary products generated through interactions between high-energy hadrons and the surrounding plasma. Moreover, our results indicate that the overall density of cosmic rays around SNRs is likely greater than previously estimated.",
        "ori-fast-z-score": -1.171700198827415,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": -0.1203858530857692
    },
    {
        "original_text": "We present the results of an analysis of the mass function for galaxy clusters in the redshift range 0 < z < 1, using data obtained with the Chandra X-ray Observatory and the Sloan Digital Sky Survey (SDSS). We find that there is no evidence for evolution in the cluster mass function over this interval; we measure the best-fit Schechter parameters to be M* = 2.6 +/- 0.2 x 1014 h-1M_sun and alpha = -1.1 +/- 0.3 at all redshifts. The lack of evolution indicates that the number density of massive clusters has remained constant since z ~ 1.  These results are consistent with previous studies based on optical surveys but differ significantly from those inferred by some recent analyses of X-ray selected samples. This discrepancy may arise because these latter samples include significant numbers of low-mass groups which evolve rapidly between z = 1 and today.",
        "watermark_text": "We present the conclusion of an assessment of the mass function for galaxy clusters in the redshift region 0 < z < 1 , using data acquired with the Chandra X - ray Observatory and the Sloan Digital Sky Survey ( SDSS ) . We see that there is no evidence for expansion in the cluster mass distribution over this interval ; we measure the best - fitting Schechter parameters to be M * = 2 . 6 + / - 0 . 2 x 1014 h - 1M _ sun and alpha = - 1 . 1 + / - 0 . 3 at all redshifts .The absence of evolution suggests that the number density of large clusters has remained constant since z ~ 1 . These conclusions are compatible with previous analyses based on optical sampling but change considerably from those inferred by some latest analyses of X - ray selected samples .This discrepancy may arise because these latter samples include significant populations of lowest - mass groups which evolve faster between z = 1 and today .",
        "rewrite_text": "We present our findings from an assessment of the mass function for galaxy clusters within the redshift range of 0 < z < 1, utilizing data from the Chandra X-ray Observatory and the Sloan Digital Sky Survey (SDSS). Our analysis indicates that there is no evidence of expansion in the cluster mass distribution across this range; we determined the best-fitting Schechter parameters to be M* = 2.6 ± 0.2 x 10^14 h^(-1) M_sun and alpha = -1.1 ± 0.3 for all redshifts. The lack of observed evolution implies that the number density of large clusters has remained stable since approximately z ~ 1. These results are consistent with prior studies based on optical sampling but differ significantly from some recent analyses of X-ray selected samples. This divergence may be attributed to those analyses incorporating substantial populations of lower-mass groups, which experience more rapid evolution between z = 1 and the present day.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 3.5,
        "rewrite-fast-z-score": -1.1952286093343936
    },
    {
        "original_text": "We present an analysis of the correlation between radio sources in the southern sky with angular scales greater than 1 degree, and the temperature fluctuations observed by Wilkinson Microwave Anisotropy Probe (WMAP). We find that there is no significant correlation at large angular separations for any individual source population or combination thereof. However, we do detect a statistically significant cross-correlation signal when all extragalactic point sources are combined into one sample. The amplitude of this signal is consistent with theoretical predictions based on the Sunyaev-Zel dovich effect. This result suggests that the cold spot may be due to a superposition of many unresolved SZ clusters along our line-of-sight. In addition, we show that the lack of correlation seen individually among different populations can be explained if these populations have differing spectral indices and/or luminosity functions. Finally, we demonstrate how the results presented here could be used as a testbed for future experiments such as Planck Surveyor.",
        "watermark_text": "We present an assessment of the relationship between radio sources in the southern sky with angular scales greater than 1 degree , and the temperature fluctuations detected by Wilkinson Microwave Anisotropy Probe ( WMAP ) . We see that there is no considerable relationship at large angular separations for any individual source population or combination thereof .However , we do discover a statistically substantial cross - correlation signal when all extragalactic point bodies are united into one sample . The amplitude of this signal is compatible with theoretical estimates based on the Sunyaev - Zel dovich phenomenon .This result suggests that the cool spot may be due to a superposition of several unresolved SZ clusters along our line - of - seeing . In addition , we find that the lack of correlation seen individually among different populations can be described if these populations have differing brightness indices and / or luminosity functions .Finally , we prove how the papers presented here possible be used as a testbed for future research such as Planck Surveyor .",
        "rewrite_text": "We provide an evaluation of the relationship between radio sources in the southern sky with angular scales exceeding 1 degree and the temperature fluctuations observed by the Wilkinson Microwave Anisotropy Probe (WMAP). Our analysis reveals no significant correlation at large angular separations for any individual source population or their combinations. However, we do identify a statistically significant cross-correlation signal when all extragalactic point sources are aggregated into a single sample. The strength of this signal aligns with theoretical predictions derived from the Sunyaev-Zeldovich effect. This finding indicates that the observed cool spot may be the result of the overlap of multiple unresolved SZ clusters along our line of sight. Furthermore, we demonstrate that the apparent lack of correlation among different populations can be explained by variations in their brightness indices and/or luminosity functions. Lastly, we illustrate how the studies presented here can serve as a foundation for future research initiatives, such as those involving the Planck Surveyor.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 7.002011783343734,
        "rewrite-fast-z-score": 2.1514114968019085
    },
    {
        "original_text": "In this article, we study families of holomorphic vector bundles on complex algebraic varieties. We prove that the set of isomorphism classes of such families is naturally an affine scheme over the base variety and give explicit equations for it in terms of Chern classes.  In particular, if the base variety has dimension one then these schemes are reduced to points corresponding to the rank and degree of each bundle in the family. \nWe also show how our results can be used to construct moduli spaces of stable vector bundles with fixed determinant. The main result of this article was announced by J. P. Serre at the conference  Algebraic geometry and number theory  held in Paris in June 2005 (see  Ser  ). Families of holomorphic vector bundles have been studied extensively since the work of Grothendieck  Gro1  . They play important roles both in algebraic geometry and mathematical physics; see e.g.,  Bri1,  Bri2,   Bri3,   ...",
        "watermark_text": "In this page , we study families of holomorphic vector bundles on complex algebraic varieties . We prove that the group of isomorphism classes of such families is naturally an affine scheme over the base variety and take explicit equations for it in terms of Chern classes .In particular , if the base range has size one then these schemes are converted to points corresponding to the rank and degree of each bundle in the class . We also demonstrate how our findings can be used to build moduli spaces of stable vector bundles with constant determinant .The main result of this paper was announced by J . P . Serre at the meeting Algebraic geometry and number theory conducted in Paris in June 2005 ( see Ser ) . Families of holomorphic functional bundles have been studied frequently since the work of Grothendieck Gro1 .They play essential roles both in mathematical mathematics and mathematical science ; seeing e . g . , Bri1 , Bri2 , Bri3 , . . .",
        "rewrite_text": "In this section, we explore families of holomorphic vector bundles on complex algebraic varieties. We establish that the set of isomorphism classes of these families naturally forms an affine scheme over the base variety and provide explicit equations in terms of Chern classes. Notably, when the base variety consists of a single point, these schemes correspond to points that reflect the rank and degree of each bundle in the equivalence class. Additionally, we illustrate how our results can be applied to construct moduli spaces of stable vector bundles with a fixed determinant. The primary result of this paper was first announced by J. P. Serre at the Algebraic Geometry and Number Theory meeting held in Paris in June 2005 (see Ser). Since Grothendieck's initial work, families of holomorphic vector bundles have been extensively studied, playing crucial roles in both pure mathematics and mathematical physics (see, for example, Bri1, Bri2, Bri3, etc.).",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": -1.099524999206747
    },
    {
        "original_text": "We present results on the orbital evolution of Jupiter and Saturn in an axisymmetric, viscously evolving protoplanetary disk with embedded planets. We find that the orbits of both giant planets are significantly affected by their mutual gravitational interaction as well as by the presence of other planetary embryos. The eccentricity growth is dominated by secular interactions between the two planets which lead to large amplitude oscillations in the semi-major axes. In addition we find that the planet migration rates depend strongly on the initial conditions for the system parameters such as mass ratio and separation distance. \n \n Keywords: Planet formation - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N-body simulations \n \n \n \n 1 Introduction \n \n Planets form out of dust particles through coagulation processes (Safronov 1969; Wetherill & Stewart 1989) followed by runaway accretion onto these growing objects (Lissauer 1987). This process leads to the formation of planetesimals whose masses range from 10$^{−6}$ M⊕ up to several Earth masses. These bodies can grow further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete enough material within a short time span (Pollack et al. 1996) . Once formed, these massive planets open gaps in the surrounding circumstellar disks due to tidal torques exerted by the planet s gravity (Lin & Papaloizou 1986 ). As a consequence, the remaining matter inside this gap will be removed rapidly by viscosity effects leading to rapid inward type II migration of the planet (Ward 1997; Tanaka et al. 2002 ) . \nThe observed distribution of exoplanets shows a wide variety of orbital configurations ranging from circular orbits around Sun-like stars to highly eccentric orbits around low-mass stars (see e.g., Marcy et al. (2005) , Udry & Santos 2007 , Winn et al. (2010 ), Johnson et al. (2011 ) and references therein). However, most of them have been found close to their host star where the detection probability increases dramatically because of the strong stellar",
        "watermark_text": "We report findings on the orbital evolution of Jupiter and Saturn in an axisymmetric , viscously changing protoplanetary disk with attached planets . We see that the orbits of both giant planets are greatly impacted by their mutual gravitational interaction as well as by the presence of other planetary embryos .The eccentricity growth is dominated by secular interactions between the two planets which cause to large frequency oscillations in the semi - major axes . In addition we find that the planet migration rates differ highly on the early conditions for the system parameters such as mass ratio and separation distance .Keywords : Planet structure - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N - bodies simulations 1 Introduction Planets form out of dust particles through coagulation processes ( Safronov 1969 ; Wetherill & Stewart 1989 ) preceded by runaway accretion onto these growing objects ( Lissauer 1987 ) . This process results to the formation of planetesimals whose masses range from 10 $ ^ { −6 } $ M⊕ up to several Earth masses .These bodies can develop further into larger planetary embryos or even directly into gas giants like Jupiter and Saturn if they accrete adequate material within a brief time frame ( Pollack et al . 1996 ) .Once assembled , these massive planets open gaps in the nearby circumstellar disks owing to tidal torques exerted by the planet s gravity ( Lin & Papaloizou 1986 ) . As a consequence , the remaining material inside this gap will be removed soon by viscosity factors resulting to rapid inward type II displacement of the planet ( Ward 1997 ; Tanaka et al .2002 ) . The observed distribution of exoplanets shows a broad variety of orbital arrangements ranging from circular orbits around Sun - like stars to strongly eccentric orbits around low - density stars ( saw e . g . , Marcy et al .( 2005 ) , Udry & Santos 2007 , Winn et al . ( 2010 ) , Johnson et al .( 2011 ) and references therein ) . However , most of them have been observed nearby to their host star where the detection odds grows dramatically because of the strong stellar",
        "rewrite_text": "We present our findings on the orbital evolution of Jupiter and Saturn within an axisymmetric, viscously evolving protoplanetary disk containing embedded planets. Our analysis reveals that the orbits of these giant planets are significantly influenced by their mutual gravitational interactions, as well as by the presence of other planetary embryos. The growth of eccentricity is primarily driven by secular interactions between the two planets, which result in significant oscillations of their semi-major axes. Additionally, we observe that the rates of planetary migration vary considerably based on the initial conditions of system parameters, including mass ratio and separation distance. \n\nKeywords: Planet structure - Giant planets - Eccentricities - Migration - Disk instability - Secular resonance - Dynamical chaos - N-body simulations\n\n1. Introduction \n\nPlanet formation occurs through the coagulation of dust particles, a process that has been thoroughly discussed in studies such as Safronov (1969) and Wetherill & Stewart (1989), and is preceded by a phase of runaway accretion on these burgeoning objects (Lissauer, 1987). This sequence leads to the creation of planetesimals with masses ranging from \\(10^{-6} M_\\oplus\\) to several Earth masses. These bodies can evolve into larger planetary embryos or even directly transform into gas giants like Jupiter and Saturn, provided they can gather sufficient material in a relatively short period (Pollack et al., 1996). Once formed, these massive planets carve out gaps in the surrounding circumstellar disks due to the tidal forces exerted by their gravity (Lin & Papaloizou, 1986). Consequently, the residual material in these gaps is swiftly evacuated by viscous processes, leading to a rapid type II inward migration of the planets (Ward, 1997; Tanaka et al., 2002). The distribution of observed exoplanets reveals a wide range of orbital configurations, from nearly circular orbits around Sun-like stars to highly eccentric orbits around low-density stars (see e.g., Marcy et al. 2005, Udry & Santos 2007, Winn et al. 2010, Johnson et al. 2011, and references therein). However, the majority of these planets have been detected in proximity to their host stars, where the likelihood of detection is significantly enhanced due to strong stellar influences.",
        "ori-fast-z-score": -1.2632278815997784,
        "water-fast-z-score": 4.867251878120797,
        "rewrite-fast-z-score": -0.8778955729143844
    },
    {
        "original_text": "We report on an attempt to detect thermal emission from the planet TrES-1 using data obtained with the Spitzer Space Telescope s Infrared Array Camera (IRAC). We find no evidence that this planet is emitting significant amounts of infrared radiation at wavelengths longer than 3 microns, and we place upper limits on its temperature between 1000 K and 1500 K depending upon assumptions about the albedo.  These results are consistent with previous estimates based on optical photometry alone. The lack of detectable infrared flux suggests either that the planet has little or no atmosphere, or else it has a very low surface pressure such as would be expected if the planet were tidally locked into synchronous rotation. This work was supported by NASA under grant NAG5-12942 issued through JPL/Caltech. We thank J. Harrington for providing us with his model predictions prior to publication. We also acknowledge helpful discussions with A. Burrows, D. Charbonneau, M. Marley, R. Seager, S. Sozzetti, T. Swain, C. Traub, B. Zuckerman, and W. Wilson.",
        "watermark_text": "We report on an attempt to identify thermal emitted from the planet TrES - 1 using data acquired with the Spitzer Space Telescope s Infrared Array Camera ( IRAC ) . We see no evidence that this moon is emitting major amounts of infrared rays at wavelengths greater than 3 microns , and we put lower limits on its temperature between 1000 K and 1500 K depending upon predictions about the albedo .These conclusions are compatible with previous estimates based on optical photometry alone . The absence of detectable infrared flux implies either that the planet has little or no environment , or else it has a very low exterior pressure such as would be anticipated if the planet were tidally locked into synchronous rotation .This project was supported by NASA under grant NAG5 - 12942 issued through JPL / Caltech . We praise J . Harrington for providing us with his model models prior to publication .We especially acknowledge helpful talks with A . Burrows , D . Charbonneau , M . Marley , R . Seager , S . Sozzetti , T . Swain , C . Traub , B . Zuckerman , and W . Wilson .",
        "rewrite_text": "We present our efforts to detect thermal emissions from the planet TrES-1 using data obtained with the Spitzer Space Telescope's Infrared Array Camera (IRAC). Our observations reveal no significant infrared emissions at wavelengths greater than 3 microns, leading us to establish lower temperature limits of 1000 K to 1500 K, which depend on the albedo predictions. These findings align with earlier estimates based solely on optical photometry. The lack of measurable infrared flux suggests either that the planet has a minimal or nonexistent atmosphere, or that it experiences very low exterior pressure, as would be expected if it were tidally locked in synchronous rotation. This research was funded by NASA grant NAG5-12942 through JPL/Caltech. We extend our gratitude to J. Harrington for sharing his models with us before publication and appreciate the valuable discussions with A. Burrows, D. Charbonneau, M. Marley, R. Seager, S. Sozzetti, T. Swain, C. Traub, B. Zuckerman, and W. Wilson.",
        "ori-fast-z-score": -1.5491933384829668,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": -0.2721655269759087
    },
    {
        "original_text": "We have studied the pulse-to-pulse intensity variations in two pulsars, PSR B0826-34 and PSR B1133+16, using data obtained with the Parkes radio telescope at 1.4 GHz (92cm) and 2.7GHz(36cm). We find that both pulsars show strong evidence for subpulse drift bands which are modulated by an underlying pattern of weaker emission. The observed patterns can be explained as due to interference between different modes of emission within each beam. In addition we find that there is no significant difference in the subpulse modulation properties when comparing observations made at 36cm and those made at 92cm. This suggests that the physical processes responsible for producing these phenomena operate over a wide range of frequencies. These results provide further support for the idea that the phenomenon known as drifting subpulses may arise through some form of plasma instability operating on the open field lines near the magnetic poles of neutron stars. \n \n Keywords: Pulsar",
        "watermark_text": "We have researched the signal - to - pulse intensity variations in two pulsars , PSR B0826 - 34 and PSR B1133 + 16 , using data acquired with the Parkes radio telescope at 1 . 4 GHz ( 92cm ) and 2 . 7GHz ( 36cm ) . We see that both pulsars give strong evidence for subpulse slip lines which are modulated by an underlying pattern of weaker emission .The observed patterns can be described as owing to interference between various modes of emission within each light . In addition we find that there is no major variation in the subpulse modulation properties when matching observations made at 36cm and those conducted at 92cm .This implies that the physical processes responsible for producing these phenomena run over a broad variety of frequencies . These conclusions provide further evidence for the idea that the process known as drifting subpulses might arise through some kind of plasma instability working on the open field lines near the magnetic poles of neutron galaxies .Keywords: Pulsar",
        "rewrite_text": "We conducted a study of the signal-to-pulse intensity variations in two pulsars, PSR B0826-34 and PSR B1133+16, utilizing data collected with the Parkes radio telescope at frequencies of 1.4 GHz (92 cm) and 2.7 GHz (36 cm). Our findings indicate that both pulsars exhibit strong evidence of subpulse slip lines, which are influenced by an underlying pattern of weaker emissions. These observed patterns appear to arise from interference among different modes of emission within each pulse. Moreover, we discovered no significant differences in the properties of subpulse modulation when comparing observations at 36 cm and 92 cm. This suggests that the physical mechanisms responsible for these phenomena operate across a wide range of frequencies. These findings further support the hypothesis that drifting subpulses may be a result of plasma instabilities affecting the open field lines near the magnetic poles of neutron stars. Keywords: Pulsar",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 6.5484618759809905,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "In this thesis, we propose an energy-efficient power control scheme for large code division multiple access (CDMA) systems with variable traffic loads and channel conditions. The proposed approach is based on the concept that all users should be allocated their required data rates at minimum total transmit power consumption while maintaining acceptable quality-of-service (QoS). We first develop a new analytical model which can accurately predict the average received signal-to-interference-plus-noise ratio (SINR) under different system configurations. Based on our analysis results, we then formulate the problem as a convex optimization problem subject to SINR constraints. Finally, by applying Lagrange multiplier method, we obtain closed-form solutions for both uplink and downlink transmissions. Our simulation results show that compared with conventional schemes such as water-filling algorithm, the proposed approach achieves significant performance gains in terms of power efficiency without sacrificing QoS requirements. In addition, it also outperforms other existing approaches in terms of computational complexity. \n \n Keywords: Code Division Multiple Access, Power Control",
        "watermark_text": "In this dissertation , we propose an energy - efficient energy management scheme for large code division multiple entry ( CDMA ) networks with variable traffic loads and channel conditions . The proposed approach is based on the idea that all users should be allocated their required information rates at minimum total broadcast power consumption while maintaining acceptable reliability - of - service ( QoS ) .We first develop a new analytical theory which can accurately forecast the average received signal - to - interference - plus - noise proportion ( SINR ) under various system systems . Based on our analysis results , we then formulate the question as a convex optimization problem subject to SINR restrictions .Finally , by using Lagrange multiplier method , we obtain closed - form methods for both uplink and downlink transmissions . Our modelling findings show that compared with typical strategies such as water - filling algorithm , the suggested approach achieves significant efficiency increases in terms of power performance without sacrificing QoS specifications .In addition , it also outperforms other existing techniques in terms of computational complexity . Keywords : Code Division Multiple Access , Power Control",
        "rewrite_text": "In this dissertation, we introduce an energy-efficient management scheme for large Code Division Multiple Access (CDMA) networks operating under varying traffic loads and channel conditions. Our approach is founded on the principle of allocating users their required information rates while minimizing overall broadcast power consumption, all while ensuring acceptable quality of service (QoS) reliability. We begin by developing a novel analytical theory capable of accurately predicting the average received signal-to-interference-plus-noise ratio (SINR) across different system scenarios. Utilizing the results from our analysis, we reformulate the issue as a convex optimization problem constrained by SINR requirements. Subsequently, we apply the Lagrange multiplier method to derive closed-form solutions for both uplink and downlink transmissions. Our modeling results demonstrate that, in comparison to conventional strategies like the water-filling algorithm, our proposed method significantly enhances power efficiency without compromising QoS standards. Furthermore, it also shows improved performance over existing techniques in terms of computational complexity. \n\nKeywords: Code Division Multiple Access, Power Control",
        "ori-fast-z-score": 0.9332565252573828,
        "water-fast-z-score": 7.154966693639935,
        "rewrite-fast-z-score": 2.182178902359924
    },
    {
        "original_text": "Gamma-ray bursts (GRBs) are the most energetic explosions in the universe, but their origin is still unknown. The leading model for GRB production involves two compact objects merging into one black hole and then exploding as a result of rapid accretion onto this black hole.  In this talk I will discuss how we can use gravitational waves to test this hypothesis by looking at the ringdown phase of these mergers. This is an exciting time for gravitational wave astronomy with Advanced LIGO/VIRGO starting taking data soon! \nI will also present some recent results on using gravitational waves to study neutron star mergers that may be related to short gamma ray bursts. Finally, I ll give you my personal perspective on what it takes to become a successful scientist today. My research interests include gravitational waves, astrophysics, cosmology, and particle physics. I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am part of the Gravitational Wave Cosmology Project.",
        "watermark_text": "Gamma - ray clusters ( GRBs ) are the most intense explosions in the universe , but their source is still unclear . The leading theory for GRB development involves two compact entities combining into one dark hole and then exploding as a result of rapid accretion onto this red hole .In this talk I will explain how we can using gravitational waves to test this hypothesis by searching at the ringdown phase of these mergers . This is an exciting day for gravitational wave astronomy with Advanced LIGO / VIRGO starting took data soon !I will also explain some latest findings on uses gravity signals to study neutron galaxy mergers that might be connected to short gamma radiation bursts . Finally , I ll offer you my personal perspective on what it takes to become a successful researcher today .My research interests cover gravitational waves , astrophysics , cosmology , and particle science . I m currently working at MIT Kavli Institute for Astrophysics and Space Research where I am member of the Gravitational Wave Cosmology Project .",
        "rewrite_text": "Gamma-ray bursts (GRBs) are the most powerful explosions in the universe, yet their origin remains a mystery. The prevailing theory suggests that GRBs arise when two compact objects merge to form a black hole, followed by an explosion driven by rapid accretion onto this black hole. In my talk, I will discuss how we can utilize gravitational waves to test this hypothesis by examining the ringdown phase of these mergers. This is a thrilling time for gravitational wave astronomy, as Advanced LIGO/VIRGO is starting to collect data soon! I will also share the latest insights on using gravitational signals to investigate neutron star mergers that may be linked to short-duration gamma-ray bursts. Lastly, I will provide my personal perspective on what it takes to become a successful researcher in today’s scientific landscape. My research interests encompass gravitational waves, astrophysics, cosmology, and particle physics. Currently, I am a member of the Gravitational Wave Cosmology Project at the MIT Kavli Institute for Astrophysics and Space Research.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 6.581793068761733,
        "rewrite-fast-z-score": 0.5773502691896257
    },
    {
        "original_text": "We present an analysis of multi-wavelength observations of the pulsar wind nebula (PWN) associated with PSR B1509-58 in the supernova remnant (SNR) G328.4+0. \n2. The radio emission is modeled as synchrotron radiation produced by relativistic electrons accelerated at the termination shock between the pulsar s magnetosphere and the surrounding medium. \n \n We find that the observed properties of this system are consistent with those expected for a young energetic pulsar surrounded by a dense shell of swept-up material. In particular, we show that: \n \n \n \n 1. The total energy contained within the SNR is ~1050 erg, which implies a kinetic energy of ~500 erg for the progenitor star prior to explosion; \n \n 2. The age of the pulsar is estimated to be ~20 kyr based on the spin-down luminosity and characteristic age; \n \n 3. The distance to the source is constrained to be <5 kpc using the dispersion measure and assuming a nominal value for the electron density along the line-of-sight; \n \n 4. The magnetic field strength near the pulsar is inferred to be ~1 mGauss based on modeling of the spectral index distribution across the face of the PWN; \n \n 5. The radius of the PWN is found to be ~0.3 pc, corresponding to a dynamical age of ~30 yrs; \n \n 6. The mass loss rate of the progenitor star was >10-5 Msun/yr during the last few thousand years before core collapse; \n \n 7. The initial mass of the progenitor star was ~25-30 Msuns, implying a red supergiant or blue hypergiant classification; \n \n 8. The ejecta mass of the progenitor star is estimated to be ~7-8 Msuns, indicating that it underwent significant mass loss prior to exploding; \n \n 9. The expansion velocity of the outer edge of the PWN is ~1000 km/sec, comparable to the speed of sound in the shocked gas; \n \n 10. The X-ray",
        "watermark_text": "We present an assessment of multi - wavelength images of the pulsar wind nebula ( PWN ) associated with PSR B1509 - 58 in the supernova remnant ( SNR ) G328 . 4 + 0 . 2 .The radio emission is modeled as synchrotron emission created by relativistic electrons accelerated at the termination shock between the pulsar s magnetosphere and the nearby medium . We see that the seen characteristics of this scheme are compatible with those expected for a young energetic pulsar surrounded by a dense shell of washed - up material .In particular , we find that : 1 . The total energy contained within the SNR is ~ 1050 erg , which implies a kinetic power of ~ 500 erg for the progenitor star previous to explosion ; 2 .The age of the pulsar is predicted to be ~ 20 kyr based on the spin - down luminosity and typical age ; 3 . The distance to the origin is constrained to be < 5 kpc using the dispersion measure and assuming a nominal value for the electron concentration along the line - of - view ; 4 .The magnetic force speed near the pulsar is inferred to be ~ 1 mGauss based on mapping of the spectral index distribution across the face of the PWN ; 5 . The radius of the PWN is found to be ~ 0 . 3 pc , equivalent to a dynamical age of ~ 30 yrs ; 6 .The mass loss rate of the progenitor star was > 10 - 5 Msun / yr during the last few thousand years before core breakup ; 7 . The initial mass of the progenitor star was ~ 25 - 30 Msuns , indicate a blue supergiant or blue hypergiant classification ; 8 .The ejecta mass of the progenitor star is predicted to be ~ 7 - 8 Msuns , showing that it underwent considerable mass loss prior to exploding ; 9 . The expansion velocity of the exterior boundary of the PWN is ~ 1000 kilometers / sec , comparable to the speed of noise in the excited gas ; 10 .The X-ray",
        "rewrite_text": "We provide an analysis of multi-wavelength images of the pulsar wind nebula (PWN) associated with PSR B1509-58, located within the supernova remnant (SNR) G328.4+0.2. The radio emissions are modeled as synchrotron radiation produced by relativistic electrons accelerated at the termination shock between the pulsar's magnetosphere and the surrounding medium. Our observations indicate that the characteristics of this model align with expectations for a young, energetic pulsar enveloped by a dense shell of ejected material. In particular, we identify several key findings: 1. The total energy within the SNR is approximately 10^50 erg, suggesting a kinetic power of around 500 erg for the progenitor star before its explosion; 2. The pulsar's age is estimated to be around 20 kyr based on spin-down luminosity and typical age estimates; 3. The distance to the source is constrained to be less than 5 kpc, derived from the dispersion measure while assuming a nominal electron density along the line of sight; 4. The magnetic field strength near the pulsar is inferred to be about 1 mG, based on the mapping of spectral index distribution across the PWN; 5. The radius of the PWN is approximately 0.3 pc, indicating a dynamical age of roughly 30 years; 6. The mass loss rate of the progenitor star was greater than 10^-5 M☉/yr during the few thousand years leading up to its core collapse; 7. The initial mass of the progenitor star is estimated to be between 25 and 30 M☉, classifying it as a blue supergiant or blue hypergiant; 8. The mass of the ejecta from the progenitor star is predicted to be around 7 to 8 M☉, indicating significant mass loss prior to the explosion; 9. The expansion velocity of the outer boundary of the PWN is roughly 1000 km/s, which is comparable to the sound speed in the surrounding gas; 10. The X-ray... ",
        "ori-fast-z-score": 1.539600717839002,
        "water-fast-z-score": 7.890453678924885,
        "rewrite-fast-z-score": 2.060488785479727
    },
    {
        "original_text": "The missing satellites problem (MSP) is one of the most important problems in space science and technology, with applications ranging from satellite navigation to space debris removal.  The MSP asks for all orbits that are stable under gravitational perturbations by known bodies such as planets or asteroids.   In this work we present an algorithm which solves the MSP exactly on any number of dimensions d >= 2 using only O(n log n + m log n) time where n = |S| is the total number of objects in S and m = |E| is the number of edges in E.   Our approach uses a novel combination of techniques including fast matrix multiplication algorithms, data structures based on interval trees, and efficient graph traversal methods. We also show how our results can be used to solve related problems like finding the minimum distance between two given sets of points in R^d. Finally, we demonstrate the practicality of our method through experiments performed on real-world datasets.",
        "watermark_text": "The missing satellites question ( MSP ) is one of the most important problems in space physics and technology , with applications diverse from satellite communication to space wreckage extraction . The MSP asks for all orbits that are stable under gravity perturbations by known objects such as planets or asteroids .In this project we present an algorithm which solves the MSP exactly on any number of dimensions d > = 2 using only O ( n log n + m log n ) time where n = | S | is the total number of items in S and m = | E | is the number of vertices in E . Our solution uses a innovative combination of techniques including rapid matrix multiplication methods , data models using on interval trees , and elegant graph traversal methods . We additionally prove how our findings can be used to solve related problems like finding the minimum height between two given sets of points in R ^ d .Finally , we prove the practicality of our technique through experiments conducted on real - time datasets .",
        "rewrite_text": "The missing satellites question (MSP) is a significant challenge in the fields of space physics and technology, with applications ranging from satellite communication to the removal of space debris. The MSP seeks to identify all orbits that maintain stability against gravitational disturbances from known celestial bodies, such as planets and asteroids. In this project, we introduce an algorithm capable of solving the MSP exactly in any dimension \\( d \\geq 2 \\), operating within a time complexity of \\( O(n \\log n + m \\log n) \\), where \\( n = |S| \\) represents the total number of items in \\( S \\) and \\( m = |E| \\) denotes the number of vertices in \\( E \\). Our solution leverages an innovative combination of techniques, including rapid matrix multiplication, interval tree data structures, and efficient graph traversal methods. Furthermore, we demonstrate how our results can be applied to address related challenges, such as determining the minimum height between two specified sets of points in \\( \\mathbb{R}^d \\). Lastly, we validate the practicality of our approach through experiments conducted on real-time datasets.",
        "ori-fast-z-score": -1.709408646894569,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": 2.4596747752497685
    },
    {
        "original_text": "We present new results on the long-term evolution of solar magnetic fields, obtained by applying an advanced data analysis technique to observations made with the Wilcox Solar Observatory (WSO) magnetograph between 1976 and 2009. The method is based on wavelet transforms in combination with principal component analysis (PCA). It allows us to separate different types of variability into their individual components at each point in time. We find that there are two distinct modes of solar magnetic field evolution over this period. One mode shows strong fluctuations around a mean value which varies slowly but significantly during the cycle 23/24 minimum. This behaviour can be explained as being due to the presence of large-scale dynamo waves driven by differential rotation. In addition we identify another type of variation which appears to have no preferred frequency or spatial scale. These variations show significant correlations with sunspot number and other proxies for solar activity. They may therefore represent some form of global response of the Sun s magnetic field to changes in its internal dynamics.",
        "watermark_text": "We report new data on the long - term expansion of solar magnetic waves , obtained by using an sophisticated data analysis technique to observations made with the Wilcox Solar Observatory ( WSO ) magnetograph between 1976 and 2009 . The method is based on wavelet transforms in combination with principal component analysis ( PCA ) .It enables us to separate distinct types of variability into their individual parts at each point in time . We see that there are two different paths of solar magnetic force evolution over this time .One mode displays large fluctuations around a mean value which varies steadily but significantly during the period 23 / 24 minimum . This behaviour can be understood as being owing to the presence of large - scale dynamo waves generated by differential rotation .In addition we identify another type of variation which appears to have no dominant amplitude or geographic range . These changes demonstrate considerable correlations with sunspot number and other proxies for solar activity .They might hence indicate some kind of worldwide response of the Sun s magnetic force to changes in its internal motion .",
        "rewrite_text": "We present new findings on the long-term evolution of solar magnetic waves, derived from advanced data analysis techniques applied to observations from the Wilcox Solar Observatory (WSO) magnetograph covering the period from 1976 to 2009. Our methodology combines wavelet transforms with principal component analysis (PCA), allowing us to disentangle various types of variability at specific points in time. Our analysis reveals two distinct patterns in the evolution of solar magnetic forces over this timeframe. One pattern exhibits significant fluctuations centered around a mean value that changes steadily yet markedly during the solar minimum of cycle 23/24. This behavior can be attributed to the influence of large-scale dynamo waves generated by differential rotation. Additionally, we observe another form of variation that lacks a predominant amplitude or geographic distribution. These fluctuations show strong correlations with sunspot numbers and other indicators of solar activity, potentially reflecting a global response of the Sun's magnetic forces to alterations in its internal dynamics.",
        "ori-fast-z-score": 0.43133109281375365,
        "water-fast-z-score": 7.763959670647566,
        "rewrite-fast-z-score": -0.329292779969071
    },
    {
        "original_text": "The information-based asset pricing model is an extension to the traditional CAPM (Capital Assets Pricing Model) that incorporates additional factors into its analysis.  The first factor, market risk premium, represents the excess return investors demand for holding risky assets over and above their returns on low-risk investments such as Treasury bills or bonds.   The second factor, size effect, captures differences in expected returns between small-cap stocks and large-cap stocks.   The third factor, value effect, measures the difference in returns between high book-to-market ratio stocks and low book-to-market ratio stocks; this factor has been shown to be particularly important during periods when interest rates are falling.    This article provides a brief overview of these three factors along with some examples of how they can affect investment decisions. The Capital Assets Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), Mossin (1966), and Black (1972), is one of the most widely used models in finance today.  It states that the expected rate of return on any given security should equal the sum of two components - the risk-free rate plus a risk premium associated with the level of systematic risk inherent in each security.  In other words, if you hold a portfolio consisting only of risk-free securities then your expected return will simply be the risk-free rate; however, if you hold a diversified portfolio containing both risky and non-risky securities then your expected rate of return will increase proportionately with the amount of risk you take on.  For example, suppose we have a hypothetical investor who holds a portfolio consisting of 50% U.S. Treasury bills and 50% Standard & Poor’s 500 Index Funds.  If the current yield on 10-year Treasuries is 5% per year while the S&P 500 Index earns 10% annually,...",
        "watermark_text": "The data - based portfolio management model is an addition to the usual CAPM ( Capital Assets Pricing Model ) that incorporates additional factors into its assessment . The first factor , market risk premium , represents the additional return investors need for holding risky funds over and above their returns on small - risk investments such as Treasury bills or bonds .The second factor , size factor , reflects variations in expected returns between small - cap stocks and large - cap stocks . The third factor , value effect , represents the difference in returns between strong book - to - market ratio stocks and low book - to - market ratio stocks ; this factor has been shown to be particularly important during periods when interest rates are falling .This page offers a brief overview of these three variables along with some examples of how they can affect investment decisions . The Capital Assets Pricing Model ( CAPM ) , developed by Sharpe ( 1964 ) , Lintner ( 1965 ) , Mossin ( 1966 ) , and Black ( 1972 ) , is one of the most commonly used theories in finance today .It says that the expected rate of return on any certain security should equivalent the sum of two parts - the danger - free level plus a risk premium associated with the degree of systematic risk inherent in each security . In other words , if you hold a portfolio consisting only of risk - safe securities then your anticipated return will simply be the risk - safe level ; however , if you hold a diversified portfolio encompassing both risky and non - risky securities then your anticipated level of return will expand proportionately with the quantity of risk you took on .For instance , suppose we have a hypothetical buyer who holds a portfolio consisting of 50 % U . S . Treasury money and 50 % Standard & Poor ’ s 500 Index Funds . If the present yield on 10 - month Treasuries is 5 % per month while the S & P 500 Index earns 10 % annually , . . .",
        "rewrite_text": "The data-driven portfolio management model enhances the traditional Capital Asset Pricing Model (CAPM) by incorporating additional factors into its evaluation. The first factor is the market risk premium, which indicates the extra return investors expect for taking on risk compared to safer investments, like Treasury bills or bonds. The second factor, known as the size factor, captures the differences in expected returns between small-cap and large-cap stocks. The third factor is the value effect, which highlights the disparity in returns between stocks with high book-to-market ratios and those with low ratios; this factor tends to be particularly significant during declining interest rate periods. This page provides a brief overview of these three factors, along with examples of their influence on investment decisions. The Capital Asset Pricing Model (CAPM), introduced by Sharpe (1964), Lintner (1965), Mossin (1966), and Black (1972), is a widely utilized theory in finance. It posits that the expected return on a security should equal the risk-free rate plus a risk premium that corresponds to the level of systematic risk associated with that security. In simpler terms, an investor with a portfolio made up solely of risk-free assets can expect a return equivalent to the risk-free rate. Conversely, if the portfolio includes both risky and risk-free assets, the expected return will increase in line with the amount of risk taken. For example, consider a hypothetical investor who possesses a portfolio consisting of 50% U.S. Treasury securities and 50% Standard & Poor's 500 Index Funds. If the current yield on 10-month Treasuries is 5% per month and the S&P 500 Index returns 10% annually, ...",
        "ori-fast-z-score": 0.5813183589761798,
        "water-fast-z-score": 7.224956747275377,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "We present an analysis of the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey data for the open cluster, Pleiades. We use this to derive the number ratio between binaries and single stars in the range 0.1 < M/M⊙ < 1.0 as well as the initial mass function (IMF). The results are compared against previous studies using different techniques. Our derived binary fraction is consistent within uncertainties with that found by other authors but our IMF shows significant differences when compared to previous work. These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been included in previous analyses. This study highlights the importance of accurate photometry over large areas combined with high resolution spectroscopy to fully understand the properties of young open clusters such as the Pleiades. Keywords: Open cluster; Binary star systems; Initial Mass Function; Substellar object",
        "watermark_text": "We present an assessment of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) Galactic Cluster Survey data for the open cluster , Pleiades . We use this to derive the number ratio between binaries and single galaxies in the range 0 . 1 < M / [UNK] < 1 . 0 as well as the initial mass function ( IMF ) .The results are compared against prior studies use different methods . Our derived binary fraction is compatible within uncertainties with that detected by other researchers but our IMF displays substantial differences when compared to previous research .These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been involved in earlier analyses . This study highlights the importance of accurate photometry over large areas coupled with high resolution spectroscopy to fully realize the properties of young open nuclei such as the Pleiades .Keywords : Open cluster ; Binary galaxy structures ; Initial Mass Function ; Substellar object",
        "rewrite_text": "We present an evaluation of the UKIRT Infrared Deep Sky Survey (UKIDSS) data from the Galactic Cluster Survey focused on the open cluster Pleiades. Our analysis allows us to determine the ratio of binaries to single galaxies within the mass range of 0.1 < M / [UNK] < 1.0, as well as to establish the initial mass function (IMF). We compare our findings with previous studies that employed different methodologies. While our estimated binary fraction aligns with those reported by other researchers within the uncertainties, our IMF shows significant discrepancies when contrasted with earlier work. These differences may arise from contamination by background galaxies or unresolved multiple systems previously considered in other analyses. This study underscores the necessity for precise photometry across extensive areas in combination with high-resolution spectroscopy to accurately capture the characteristics of young open clusters like the Pleiades. \n\nKeywords: Open cluster; Binary galaxy structures; Initial Mass Function; Substellar objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -1.116312611302876
    },
    {
        "original_text": "In this thesis we study the problem of optimizing resource allocation in wireless networks by using non-cooperative game theory. We consider three different problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each case, we formulate an optimization problem as a noncooperative game between users competing to maximize their own utility functions. Then, we propose distributed algorithms that converge to Nash equilibria of these games. Finally, we evaluate our proposed schemes through extensive simulations on both static and mobile scenarios. \n \n Keywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium. 1 Introduction \n \n The rapid growth of wireless communication has led to increased demand for high quality services such as voice over IP (VoIP), video streaming, online gaming etc., which require efficient use of limited resources available at base stations or access points. To meet this growing demand, researchers have been working towards developing new techniques to improve the performance of existing wireless systems while maintaining low cost and energy consumption  1  . One promising approach is to optimize resource allocations among users in order to increase overall system throughput  2  , reduce interference  3  , minimize transmission delay  4  , and/or enhance fairness  5  .\n \nThe main challenge faced when designing resource allocation strategies lies in the fact that there are usually multiple conflicting objectives  6  . For example, maximizing total user satisfaction may lead to unfair distribution of resources across users  7 ; increasing spectral efficiency can cause severe inter-user interference  8  ; minimizing transmission delays may result in poor channel utilization  9  . Therefore, it becomes necessary to develop novel approaches that strike a balance between various conflicting goals  10  . \n \n This work was supported in part by NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "watermark_text": "In this dissertation we study the question of optimizing bandwidth allocation in mobile networks by using non - cooperative play logic . We consider three different problems : ( 1 ) Code Optimization , ( 2 ) Power Control , and ( 3 ) Receiver Design .In each situation , we formulate an optimization problem as a noncooperative game between participants competing to maximize their own utility functions . Then , we develop distributed methods that converge to Nash equilibria of these games .Finally , we assess our proposed arrangements through numerous simulations on both static and wireless situations . Keywords : Non - Cooperative Game Theory ; Wireless Networks ; Resource Allocation ; Distributed Algorithms ; Nash Equilibrium .1 Introduction The rapid increase of mobile communication has led to renewed demand for high quality services such as voice over IP ( VoIP ) , television playback , internet gaming etc . , which require efficient application of restricted resources accessible at base stations or entry points . To address this increasing demand , researchers have been pushing towards developing innovative techniques to upgrade the performance of older broadband systems while maintaining low cost and energy consumption 1 .One promising solution is to optimize resource allocations among consumers in order to expand overall network throughput 2 , avoid noise 3 , eliminate communication delay 4 , and / or enhance fairness 5 . The main challenge facing when designing resource allocation strategies rests in the fact that there are typically many conflicting aims 6 .For instance , maximizing gross consumer satisfaction would result to inappropriate distribution of assets across users 7 ; increasing spectral capacity can cause profound cross - customer interference 8 ; minimizing broadcast delays may end in poor channel utilization 9 . Therefore , it becomes necessary to develop new approaches that strike a balance between various differing aims 10 .This project was supported in part by NSF under Grants CNS - 0721440 , CCF - 0729260 , ECCS - 0801571 , and CNS - 0916275 .",
        "rewrite_text": "In this dissertation, we investigate the optimization of bandwidth allocation in mobile networks using non-cooperative game theory. We examine three key issues: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. For each scenario, we frame an optimization challenge as a non-cooperative game where participants aim to maximize their individual utility functions. We then devise distributed methods that converge on the Nash equilibria of these games. Lastly, we evaluate the effectiveness of our proposed strategies through extensive simulations in both static and wireless environments. \n\nKeywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium.\n\n1. Introduction \n\nThe rapid growth of mobile communication has created an increased demand for high-quality services, such as voice over IP (VoIP), television streaming, and online gaming, which necessitate the efficient use of limited resources available at base stations or access points. To meet this escalating demand, researchers are actively developing innovative techniques to enhance the performance of existing broadband systems while keeping costs and energy consumption low. One effective approach is optimizing resource allocations among users to improve overall network throughput, mitigate noise, reduce communication delays, and enhance fairness. \n\nHowever, a significant challenge in designing resource allocation strategies arises from the presence of often conflicting objectives. For example, maximizing overall consumer satisfaction can lead to an uneven distribution of resources among users, while increasing spectral capacity may result in severe cross-user interference. Additionally, efforts to minimize broadcast delays can lead to inefficient channel usage. Consequently, it is crucial to develop new methodologies that achieve a balance among these competing objectives. \n\nThis project received partial support from the NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "ori-fast-z-score": -1.7320508075688774,
        "water-fast-z-score": 8.825211257612851,
        "rewrite-fast-z-score": -1.3426901732747025
    },
    {
        "original_text": "We present the results of our Spitzer Space Telescope survey for protostars and young stellar objects (YSOs) in three nearby, intermediate mass star-forming regions: NGC 1333, Serpens South, and Perseus North. We identify over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and/or envelopes. The majority are Class I sources that have recently formed outflows or jets; however we also find several dozen more evolved Class II/III sources. In addition to these disk-bearing systems, we detect numerous isolated point-like sources whose SEDs suggest they are deeply embedded protostars. These observations provide new insights into how stars form in IM environments. Our sample includes many previously unidentified low-luminosity protostars which will be useful targets for future studies at higher angular resolution. This work is based on observations made with the Spitzer Space Telescope, which is operated by NASA under contract 1407. Support for this work was provided by NASA through an award issued by JPL/Caltech. \n \n Keywords: Protostar",
        "watermark_text": "We publish the conclusion of our Spitzer Space Telescope survey for protostars and young stellar bodies ( YSOs ) in three nearby , intermediate mass star - creating areas : NGC 1333 , Serpens South , and Perseus North . We distinguish over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and / or envelopes .The majority are Class I sources that have newly formed outflows or jets ; however we also find several dozen more evolved Class II / III sources . In addition to these disk - bearing components , we locate many isolated point - like sources whose SEDs suggest they are deeply embedded protostars .These measurements give novel knowledge into how stars create in IM environments . Our specimen includes several recently unidentified low - luminosity protostars which will be valuable targets for future research at higher angular resolution .This project is based on observations made with the Spitzer Space Telescope , which is controlled by NASA under contract 1407 . Support for this project was provided by NASA through an grant issued by JPL / Caltech .Keywords: Protostar",
        "rewrite_text": "We present the findings from our survey of protostars and young stellar objects (YSOs) conducted with the Spitzer Space Telescope in three nearby, intermediate-mass star-forming regions: NGC 1333, Serpens South, and Perseus North. Our study identifies over 100 candidate YSOs exhibiting infrared excesses that suggest the presence of circumstellar disks and/or envelopes. The majority of these candidates are classified as Class I sources, characterized by newly formed outflows or jets; additionally, we also detect several dozen more developed Class II/III sources. Alongside these disk-bearing entities, we identify numerous isolated, point-like sources whose spectral energy distributions (SEDs) indicate they are deeply embedded protostars. These observations provide new insights into star formation in intermediate-mass environments. Our sample includes several previously unidentified low-luminosity protostars, which represent promising targets for future high-resolution studies. This research utilizes data from the Spitzer Space Telescope, managed by NASA under contract 1407, with support provided by NASA through a grant from JPL/Caltech. Keywords: Protostar.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "We present an algorithm for finding the optimal matching between two sets of clusters, which we call  trueclusters .  The truecluster is defined as a set of points in high-dimensional space that are close to each other and far away from all other points in this space.   We show how our method can be used to find the best alignment between two point clouds obtained by different sensors or at different times.   ... \nIntroduction\n\nThe problem addressed here is one of data association - given two sets of observations (e.g., images), determine what pairs correspond to the same physical object.  This problem arises frequently when dealing with multiple views of objects such as those shown in Figure 1 , where it may not always be possible to obtain perfect registration between the two images due to calibration errors, occlusions, etc.\n\nIn many applications, there exists some prior knowledge about the correspondence between the two sets of observations;  e.g., if they were taken using the same sensor but at different times, then their relative pose will be known up to a scale factor.  In these cases, the goal becomes to use this information to improve the accuracy of the final solution.  \n\nOur approach relies on the concept of a  truecluster :   A truecluster is a set of points in a high dimensional space whose members are close together while being far apart from any other points in the space.  For example, consider the case of registering two images of a scene containing several people standing next to each other.  Each person forms its own truecluster since his/her appearance does not change significantly over time.  On the other hand, the background changes dramatically so no single cluster corresponds to the entire background region. \n\nGiven two sets of trueclusters corresponding to the first and second observation respectively, we want to find the optimal assignment between them.  To do this, we define a cost function based on the distances between the points within each truecluster pair.  Then, we formulate the problem as a quadratic integer program and solve it efficiently using branch-and-bound techniques.  Finally,...",
        "watermark_text": "We present an algorithm for finding the ideal matching between two sequences of clusters , which we call trueclusters . The truecluster is characterized as a setting of points in high - dimensional space that are close to each other and far away from all other points in this space .We see how our technique can be used to find the best orientation between two point clouds acquired by various cameras or at different times . . . . Introduction The question explored here is one of data association - given two sets of measurements ( e . g . , photographs ) , determine what pairs relate to the same physical object .This problem arises often when dealing with many perspectives of items such as those shown in Figure 1 , where it must not always be possible to obtain good registered between the two images resulting to calibration errors , occlusions , etc . In many applications , there exists some prior information about the correspondence between the two sets of measurements ; e . g . , if they were took use the same sensor but at different times , then their relative pose will be described up to a scale factor .In these circumstances , the objective remains to use this data to improve the accuracy of the finished solution . Our solution uses on the idea of a truecluster : A truecluster is a setting of points in a high dimensional space whose members are close together while being far separate from any other points in the space .For instance , consider the case of registering two images of a scene containing several people standing close to each other . Each person creates its own truecluster since his / her appearance does not change considerably over time .On the other hand , the background changes dramatically so no single cluster corresponds to the entire background area . Given two sets of trueclusters corresponding to the first and second observation respectively , we want to find the ideal assignment between them .To do this , we define a price vector depending on the distances between the points within each truecluster pair . Then , we formulate the issue as a quadratic integer program and solve it easily using branch - and - bound techniques .Finally,...",
        "rewrite_text": "We introduce an algorithm designed to identify the optimal matching between two sequences of clusters, which we refer to as trueclusters. A truecluster is defined as a collection of points in high-dimensional space that are closely grouped together while remaining distant from all other points. Our approach demonstrates its applicability in determining the best alignment of two point clouds captured by different cameras or at various times. \n\n**Introduction**  \nThe focus of this study is on data association—specifically, how to establish relationships between two sets of measurements, such as photographs, that correspond to the same physical object. This challenge frequently arises when dealing with multiple perspectives of items, as illustrated in Figure 1, where achieving proper registration between two images can be difficult due to calibration errors, occlusions, and other issues. In many cases, prior information about the correspondence between the two measurement sets can be leveraged; for example, if they were captured using the same sensor at different times, their relative poses can be described up to a scale factor. The goal remains to utilize this data to enhance the accuracy of the final result.\n\nOur approach is grounded in the concept of trueclusters: a truecluster is a grouping of points in a high-dimensional space where its members are closely positioned while remaining far from other points. For instance, when registering two images of a scene with several individuals standing near one another, each person can be seen as forming their own truecluster, as their appearance remains relatively consistent over time. In contrast, the background may vary significantly, leading to no single cluster representing the entire background.\n\nGiven two sets of trueclusters from the first and second observations, our objective is to determine the optimal assignment between them. To achieve this, we establish a cost vector based on the distances between points within each truecluster pair. We then frame this problem as a quadratic integer program, which we can efficiently solve using branch-and-bound techniques. Finally, ...",
        "ori-fast-z-score": -0.15713484026367722,
        "water-fast-z-score": 7.542472332656507,
        "rewrite-fast-z-score": 0.48349377841522817
    },
    {
        "original_text": "We study the phenomenology of non-zero trilinear soft supersymmetry breaking (SSB) term A 0 in minimal supergravity (mSUGRA). We find that for large values of tan β, there is an upper bound on |A 0 | which can be obtained by requiring correct electroweak symmetry breaking. For small values of tan β, we show that the allowed range of |A 0 | increases with decreasing value of M 1/2 . In both cases, the lower limit on |A 0 | comes from the requirement of not having charge and/or color breaking minima deeper than the electroweak vacuum. The effect of varying |A 0 | on sparticle masses are studied numerically using ISAJET 7.64. It is found that increasing |A 0 | leads to decrease in mass difference between lightest neutralino and lighter charginos as well as increase in mass splitting among squarks and sleptons. This results into enhancement of production cross sections of these particles at hadronic colliders like Tevatron and LHC.",
        "watermark_text": "We explore the phenomenology of non - zero trilinear soft supersymmetry broken ( SSB ) term A 0 in reduced supergravity ( mSUGRA ) . We see that for large values of tan β , there is an upper bound on | A 0 | which can be obtained by requiring proper electroweak symmetry breaking .For small values of tan β , we find that the allowed range of | A 0 | increases with decreasing value of M 1 / 2 . In both cases , the lower limit on | A 0 | stems from the requirement of not having charge and / or color breaking minima deeper than the electroweak cavity .The impact of increasing | A 0 | on sparticle masses are studied numerically utilizing ISAJET 7 . 64 . It is found that varying | A 0 | leads to decrease in mass ratio between lightest neutralino and darker charginos as also as increase in mass separation among squarks and sleptons .This results into enhancement of output cross sections of these particles at hadronic colliders like Tevatron and LHC .",
        "rewrite_text": "We investigate the phenomenology of the non-zero trilinear soft supersymmetry breaking (SSB) term \\( A_0 \\) within the framework of minimal supergravity (mSUGRA). Our findings indicate that for large values of \\( \\tan \\beta \\), there exists an upper limit on \\( |A_0| \\) that is determined by the conditions necessary for successful electroweak symmetry breaking. Conversely, for smaller values of \\( \\tan \\beta \\), we observe that the permissible range of \\( |A_0| \\) expands as \\( M_{1/2} \\) decreases. In both scenarios, the lower bound on \\( |A_0| \\) arises from the requirement to avoid deeper charge and/or color breaking minima than the electroweak vacuum. We conduct numerical studies using ISAJET 7.64 to examine the effects of increasing \\( |A_0| \\) on sparticle masses. Our results demonstrate that variations in \\( |A_0| \\) result in a reduced mass ratio between the lightest neutralino and the darker charginos, as well as an increased mass gap among squarks and sleptons. This leads to an enhancement of the production cross sections for these particles at hadronic colliders such as the Tevatron and the LHC.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 3.8729833462074166,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components . We derive the general expression for the power - momentum tensor at first order in perturbations as well as its trace - free portion which is responsible for gravitational waves generation .The evolution equations are derived by projecting the conservation law onto the background 4 - velocity vector field . In particular we find that the presence of anisotropic stress leads to an additional source term in the equation regulating the evolution of scalar modes .Finally , we talk how our formalism can be applied to study various physical conditions such as inflationary theories or black particle halos formation . Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 .These measurements have provided us with comprehensive information about the early universe and enable to test fundamental theory on very huge scales 2 . The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 .However , these equations never be answered analytically even if one neglects all interactions between particles 4 , so numerical simulations are required 5 . On the other hand , analytical solutions arise only under certain approximations 6 .For instance , it was shown ago 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the late stages of formation formation 9 .",
        "rewrite_text": "We present the relativistic second-order perturbation theory for fluids in curved spacetime with an arbitrary number of components. We derive a general expression for the first-order power-momentum tensor, including its trace-free part, which is responsible for the generation of gravitational waves. The evolution equations are obtained by projecting the conservation law onto the background 4-velocity vector field. Notably, we demonstrate that anisotropic stress introduces an additional source term in the equation governing the evolution of scalar modes. Furthermore, we discuss how our framework can be utilized to investigate various physical scenarios, such as inflationary models and the formation of black particle halos. Over the past decade, cosmology has experienced a significant transformation thanks to precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These observations have offered extensive insights into the early universe and have allowed for the testing of fundamental theories on large scales. The standard cosmological model posits that the universe is composed of several interacting components, including cold dark matter (CDM), baryons, photons, and neutrinos. Each component evolves according to specific hydrodynamic equations that describe their dynamics. However, these equations cannot be solved analytically, even when particle interactions are disregarded, necessitating the use of numerical simulations. Conversely, analytical solutions can only be obtained under specific approximations. For example, it has been previously demonstrated that the influence of force gradients can lead to significant corrections in the evolution time of density perturbations during the later stages of structure formation.",
        "ori-fast-z-score": -2.1213203435596424,
        "water-fast-z-score": 5.1946603473211805,
        "rewrite-fast-z-score": -1.6994116628998401
    },
    {
        "original_text": "We present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "We use photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the solar wind termination shock ( SWTS ) . The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun .We use these models to constrain the boundary parameters of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission . Our results show that the TS distance reduces with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma .For low solar activity rates we find that the TS distance agrees very best with previous estimates based on observations of energetic particles .",
        "rewrite_text": "We utilize photoionization estimates for the heliosheath, which lies between the termination shock (TS) at approximately 100 AU and the solar wind termination shock (SWTS). The SWTS is positioned beyond 1 AU in the solar system's frame of reference, but within 0.3 AU when considered in the Sun’s rest frame. These models help us refine the boundary parameters of the heliosphere by analyzing interstellar neutral hydrogen data obtained through the Lyman-alpha instrument aboard the Solar Wind Anisotropy Probe (SWAP), along with in situ measurements collected near Earth during the Voyager 2 mission. Our findings indicate that the TS distance decreases as solar activity increases, which can be attributed to a rise in the density of solar wind plasma. In periods of low solar activity, our measurements of the TS distance align most closely with earlier estimates based on observations of energetic particles.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.7878385971583353
    },
    {
        "original_text": "The Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model . The ADMX experiment consists of two principal portions : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies .In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 . We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic force .These data improve upon former empirical bounds by more than one order of magnitude . This research was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 .The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles . The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum .In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "The Axion Dark Matter Search (ADMX) experiment aims to detect dark matter in the form of axions, which are theoretical particles proposed by theories that extend beyond the Standard Model of particle physics. The experiment is comprised of two main components: an antenna and a microwave cavity system that can be finely tuned across a wide range of frequencies. In this dissertation, we present results obtained from the initial phase of the project, which includes data collected between September 2005 and March 2007. We provide limits on the coupling strength of axions to photons, as well as constraints on the mass of axions produced through Primakoff conversion within a strong magnetic field. These findings enhance previous experimental limits by more than an order of magnitude. This research was conducted under the auspices of the U.S. Department of Energy at Lawrence Livermore National Laboratory, under Contract DE-AC52-07NA27344. The ADMX experiment is specifically focused on detecting dark matter in the universe in the form of axionic particles. This dissertation will delve into our most recent discoveries from the first phase of the project.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 6.413001035278336,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "We propose an approach to the problem of predicting football match results based on self-confirming beliefs and their evolution over time. We use data from the English Premier League (EPL) season 2013-14, which consists of 380 matches played by 20 teams in 38 rounds. The proposed method is tested against two baseline models that do not take into account any temporal dynamics between consecutive games. Our experiments show that our model outperforms both baselines with respect to accuracy and F1 score metrics. In addition, we demonstrate how the proposed model can be used as part of a decision support system for betting purposes. Predicting sports outcomes has been one of the most popular research topics among researchers working in machine learning and data mining communities. This interest stems mainly from its practical applications such as sport betting systems or recommendation engines for fans. However, despite the large number of studies conducted so far, there are still many open problems related to this topic. One of these challenges concerns the prediction of future events given past observations. For example, if we want to predict whether team A will win game X vs. team B at home ground C, then it may seem reasonable to assume that the outcome of previous games involving either team A or team B should have some influence on the final result.",
        "watermark_text": "We suggest an approach to the issue of predicting football match results based on self - verified beliefs and their development over time . We use data from the English Premier League ( EPL ) season 2013 - 14 , which consists of 380 games played by 20 teams in 38 rounds .The proposed approach is tested against two baseline theories that do not take into consideration any temporal dynamics between successive games . Our experiments indicate that our model outperforms both baselines with regard to reliability and F1 score metrics .In addition , we prove how the suggested model can be used as part of a decision support system for betting purposes . Predicting games outcomes has been one of the most popular research subjects among researchers practicing in machine computing and information processing communities .This interest arises principally from its practical applications such as sport betting systems or recommendation engines for fans . However , despite the huge amount of studies performed so far , there are still many open problems related to this topic .One of these problems involves the expectation of later developments given past predictions . For instance , if we wish to predict whether squad A will win match X vs . squad B at home ground C , then it must appear probable to assume that the result of previous contests concerning either team A or squad B should have some influence on the last result .",
        "rewrite_text": "We propose a novel approach to predicting football match outcomes that takes into account self-verified beliefs and their evolution over time. Our analysis utilizes data from the English Premier League (EPL) for the 2013-14 season, which includes 380 matches played by 20 teams over 38 weeks. We evaluate our approach against two baseline models that ignore the temporal dynamics between consecutive games. Our experiments reveal that our model surpasses both baselines in terms of reliability and F1 score metrics. Furthermore, we demonstrate how our proposed model can function as a decision support tool for betting applications. The prediction of match outcomes has emerged as a prominent topic within the machine learning and information processing fields, primarily due to its practical applications in sports betting and fan recommendation systems. Despite the extensive research conducted in this area, numerous challenges remain. One significant issue is the expectation of future outcomes based on prior predictions. For instance, if we aim to determine whether Team A will win against Team B on Home Ground C, it is reasonable to consider that the results of previous encounters involving either Team A or Team B may impact the outcome of the upcoming match.",
        "ori-fast-z-score": 0.5669467095138409,
        "water-fast-z-score": 8.199778267512094,
        "rewrite-fast-z-score": -0.7107423155935334
    },
    {
        "original_text": "We present new observations with the Hubble Space Telescope (HST) and Chandra X-ray Observatory to study the properties of ionized gas in galaxy clusters at z ~ 0.5-0.8, where most massive clusters are found today.  We find that the fraction of cool core clusters is higher than expected for their redshifts based on local samples. The observed evolution may be due to an increase in the number density of active galactic nuclei or AGN activity over time. In addition, we detect extended emission line regions around some of these clusters which have been previously identified as having strong cooling flows. These results suggest that there has been significant heating of the intracluster medium by energetic outflows associated with AGNs since z = 1.0. This work was supported by NASA grant NAG5-9998. Cooling flow clusters are known to contain large amounts of cold gas within their central regions. However, it remains unclear how this gas cools down without forming stars. Recent studies show that many of them also harbor powerful radio sources near their centers. It is possible that such radio jets heat up the ICM through shocks and/or turbulence generated during the interaction between the jet plasma and the ambient hot gas.",
        "watermark_text": "We present new experiments with the Hubble Space Telescope ( HST ) and Chandra X - ray Observatory to study the properties of ionized gas in galaxy galaxies at z ~ 0 . 5 - 0 . 8 , where most large clusters are found today . We see that the fraction of cold core galaxies is higher than expected for their redshifts based on local samples .The observed evolution may be due to an increase in the number density of active galactic nuclei or AGN activity over time . In addition , we find extended emitted path regions around some of these clusters which have been previously noted as having strong cooling flows .These results suggest that there has been significant heating of the intracluster medium by energetic outflows associated with AGNs since z = 1 . 0 . This work was supported by NASA grant NAG5 - 9998 .Cooling circulation clusters are known to contain significant amounts of cold energy within their central regions . However , it remains unclear how this gas cools down without forming stars .Recent research reveal that several of them additionally harbor potent wireless sources near their regions . It is suggested that such radio jets heat up the ICM through shocks and / or turbulence generated during the interaction between the jet plasma and the ambient warm gas .",
        "rewrite_text": "We present new experiments conducted with the Hubble Space Telescope (HST) and the Chandra X-ray Observatory, aimed at investigating the properties of ionized gas in galaxies at redshifts around 0.5 to 0.8, where most large clusters currently exist. Our findings indicate that the fraction of cold core galaxies is higher than what would be expected for these redshifts based on local samples. This observed evolution could be attributed to an increase in the number density and activity of active galactic nuclei (AGN) over time. Additionally, we identify extended regions of emitted gas around some clusters that have previously been recognized as having strong cooling flows. These results imply significant heating of the intracluster medium by energetic outflows linked to AGNs since redshift z = 1.0. This research was funded by NASA grant NAG5-9998. Cooling clusters are known to harbor substantial amounts of cold gas in their central regions, yet it remains uncertain how this gas cools without leading to star formation. Recent studies suggest that several of these clusters may also contain powerful radio sources in their vicinity. It is proposed that these radio jets heat the intracluster medium through shockwaves and turbulence caused by interactions between the jet plasma and the surrounding warm gas.",
        "ori-fast-z-score": 1.744163198544762,
        "water-fast-z-score": 6.123724356957946,
        "rewrite-fast-z-score": 2.2223355980148636
    },
    {
        "original_text": "We present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called  self-calibration  which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.",
        "watermark_text": "We address the latest advances in imaging bandwidth combs and their applications to precision metrology , notably calibration methods for high - resolution spectroscopy instruments such as Fourier shift spectrometers ( FTS ) . We discuss how these tools can be used to achieve observation reliability by orders of magnitude over traditional techniques .In particular we explain two novel approaches that are being pioneered at NIST : 1 ) The using of an optical frequency comb fixed to a high - speed atomic clock to calibrate FTS calculations with sub - Hz uncertainty . 2 ) A methodology called self - calibration which allows one to measure absolute frequencies without using any external reference standards or other auxiliary machinery .These methods have been shown using both laboratory experiments and field trials . They represent crucial progress towards realizing the vision of a future where all spectroscopic observations will be traceable back to essential scientific constants .This is especially applicable today due the increasing interest in making portable , low - cost devices capable of performing exact chemical analysis anywhere around the world .",
        "rewrite_text": "We explore the latest developments in imaging bandwidth combs and their role in precision metrology, particularly in the calibration of high-resolution spectroscopy instruments like Fourier transform spectrometers (FTS). Our discussion highlights how these advanced tools can enhance observation reliability dramatically compared to traditional methods. Specifically, we introduce two innovative techniques being advanced at NIST: first, the use of an optical frequency comb linked to a high-speed atomic clock for calibrating FTS measurements with sub-Hz uncertainty; and second, a self-calibration method that allows for absolute frequency measurements without relying on external reference standards or additional equipment. Both techniques have been validated through laboratory experiments and field trials. These advancements mark significant progress toward a future where all spectroscopic observations can be traced back to fundamental scientific constants. This is particularly relevant today, given the growing demand for portable, low-cost devices capable of conducting precise chemical analyses anywhere in the world.",
        "ori-fast-z-score": -0.6396021490668313,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 1.3093073414159544
    },
    {
        "original_text": "We consider the problem of local purification in quantum networks, where each node is allowed to perform only local operations and communicate classically with its neighbors. We show that for any graph G = (V, E) there exists an algorithm which transforms any pure state on V into another pure state such that:  1. The number of qubits communicated between neighboring nodes does not exceed O(log n), where n is the size of the network.  2. Each node performs at most polylogarithmic many local operations. Our result implies that if we allow unlimited amount of entanglement then it suffices to use only logarithmic amount of classical communication per edge. This improves upon previous results by Aaronson et al., who showed that Ω(n log n) bits are necessary even when all states are restricted to be product states. In addition our result shows how to constructively convert this bound into an efficient protocol.",
        "watermark_text": "We consider the question of local purification in quantum networks , where each node is allowed to conduct only local actions and communicate classically with its relatives . We see that for any graph G = ( V , E ) there exists an algorithm which transforms any pure state on V into another pure state such that : 1 .The amount of qubits transmitted between neighboring nodes does not reach O ( log n ) , where n is the size of the network . 2 .Each node performs at most polylogarithmic many local activities . Our result requires that if we allow unlimited amount of entanglement then it suffices to use only logarithmic amount of classical communication per edge .This improves upon preceding results by Aaronson et al . , who demonstrated that Ω ( h log n ) bits are necessary even when all states are restricted to be product states . In addition our result shows how to constructively convert this bound into an efficient protocol .",
        "rewrite_text": "We explore the issue of local purification in quantum networks, wherein each node can only perform local actions and communicate classically with its neighboring nodes. Our findings indicate that for any graph \\( G = (V, E) \\), there exists an algorithm capable of transforming any pure state on \\( V \\) into another pure state under the following conditions: 1. The total number of qubits transmitted between adjacent nodes does not exceed \\( O(\\log n) \\), with \\( n \\) representing the network size. 2. Each node engages in at most a polylogarithmic number of local actions. Notably, our results demonstrate that when an unlimited amount of entanglement is permissible, only a logarithmic amount of classical communication per edge is required. This represents an advancement over previous findings by Aaronson et al., who showed that \\( \\Omega(h \\log n) \\) bits are essential even when all states are constrained to be product states. Additionally, our work provides a constructive method to translate this bound into an efficient protocol.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 2.321219442769799,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "We report on neutron scattering experiments performed to study spin fluctuations and magnetic correlations in the metallic phase of quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu N(CN)2 Br (κ-Br). We find that the temperature dependence of the static susceptibility χ0 is well described by the Curie-Weiss law with an antiferromagnetic Weiss constant θ = -26 K, indicating strong antiferromagnetic interactions between spins. The observed broadening of the elastic linewidth Γel at low temperatures indicates short-range spin-spin correlation lengths ξs ~ 5 nm. In addition we observe a large enhancement of the dynamic susceptibility χ′′(Q,ω), which can be attributed to the development of low-energy spin excitations below T* ~ 50 K. These results are consistent with theoretical predictions for two-dimensional systems close to quantum criticality. Our data suggest that the system undergoes a transition into a state where the Fermi surface becomes unstable against formation of electron-hole pairs leading to Cooper pairing. \n \n Introduction \n \n A number of recent studies have shown that many strongly correlated electronic materials exhibit unconventional properties such as high-temperature superconductivity or non-Fermi liquid behavior  1  . One important aspect of these phenomena is the presence of collective charge and/or spin degrees of freedom  2  , whose dynamics often give rise to characteristic features in the excitation spectrum  3  . For example, in cuprate-based high-temperature superconductors  4  , it has been suggested that the pseudogap regime  5  may arise due to competing orders  6  originating from different regions of the Brillouin zone  7, 8  . Similarly, in iron pnictide compounds  9  , the appearance of a spin-density wave order parameter  10  leads to a suppression of the density-of-states near the Fermi level  11  resulting in a partial gap opening  12  . Finally, in heavy fermion metals  13  , the hybridization of localized f-electrons  14  gives rise to a nontrivial momentum structure of the self-energy  15  .\n \nIn this work, we present detailed measurements of the spin fluctuation spectrum in the metallic phase of the quasi",
        "watermark_text": "We report on neutron scattering experiments conducted to study spin fluctuations and magnetic correlations in the metallic phase of quasi - two - dimensional organic superconductor κ - ( BEDT - TTF ) 2Cu N ( CN ) 2 Br ( κ - Br ) . We see that the temperature dependence of the static susceptibility χ0 is well described by the Curie - Weiss law with an antiferromagnetic Weiss constant θ = - 26 K , showing strong antiferromagnetic interactions between spins .The observed broadening of the elastic linewidth Γel at low temperatures indicates short - range spin - spin correlation distances ξs ~ 5 nm . In addition we study a large enhancement of the dynamic susceptibility χ ′ ′ ( Q , ω ) , which can be due to the development of high - energy spinning excitations below T * ~ 50 K . These conclusions are compatible with theoretical estimates for two - dimensional systems close to quantum criticality .Our data suggest that the system undergoes a shift into a state where the Fermi surface gets unstable against development of electron - hole couples leading to Cooper coupling . Introduction A couple of recent studies have shown that several highly correlated optical materials exhibit unusual characteristics such as high - temperature superconductivity or non - Fermi solid behavior 1 .One important feature of these phenomena is the presence of collective charge and / or spin degrees of liberty 2 , whose dynamics often give rise to distinctive features in the excitation spectrum 3 . For instance , in cuprate - based high - temperature superconductors 4 , it has been proposed that the pseudogap regime 5 may arise due to competing orders 6 resulting from different regions of the Brillouin zone 7 , 8 .Similarly , in metal pnictide molecules 9 , the appearance of a spin - density wave order parameter 10 results to a suppression of the density - of - states near the Fermi level 11 producing in a partial gap opening 12 . Finally , in heavy fermion metals 13 , the hybridization of localized f - ions 14 makes rise to a nontrivial momentum arrangement of the self - energy 15 .In this research , we present detailed observations of the spin fluctuation spectrum in the metallic phase of the quasi",
        "rewrite_text": "We present findings from neutron scattering experiments aimed at investigating spin fluctuations and magnetic correlations in the metallic phase of the quasi-two-dimensional organic superconductor κ-(BEDT-TTF)2Cu(N(CN)2)Br (κ-Br). Our results indicate that the temperature dependence of the static susceptibility χ₀ aligns well with Curie-Weiss behavior, exhibiting an antiferromagnetic Weiss constant of θ = -26 K, which suggests significant antiferromagnetic interactions among the spins. Additionally, the broadening of the elastic linewidth Γₑₗ at lower temperatures points to short-range spin-spin correlation distances of approximately ξₛ ~ 5 nm. We also observe a considerable enhancement in the dynamic susceptibility χ′′(Q, ω), likely linked to the emergence of high-energy spin excitations occurring below T* ~ 50 K. These findings are consistent with theoretical predictions for two-dimensional systems near quantum criticality. Our data imply that the material transitions to a state where the Fermi surface becomes unstable, potentially leading to the formation of electron-hole pairs and Cooper pairing.\n\nRecent studies have demonstrated that several highly correlated materials exhibit unique characteristics, such as high-temperature superconductivity and non-Fermi liquid behavior. A key aspect of these phenomena is the presence of collective charge and/or spin degrees of freedom, which influence the dynamics and give rise to distinctive features in the excitation spectrum. For example, in cuprate-based high-temperature superconductors, the pseudogap regime may emerge from competing orders arising in various regions of the Brillouin zone. Similarly, in metal pnictides, the presence of a spin-density wave order parameter leads to a suppression of the density of states near the Fermi level, resulting in a partial gap opening. In heavy fermion metals, the hybridization of localized f-ions gives rise to complex momentum arrangements in the self-energy. This research provides detailed insights into the spin fluctuation spectrum within the metallic phase of the quasi-two-dimensional organic superconductor.",
        "ori-fast-z-score": 1.0441851275732486,
        "water-fast-z-score": 7.574735861838574,
        "rewrite-fast-z-score": 0.722897396012249
    },
    {
        "original_text": "We test whether the observed value of lambda is consistent with the prediction that it should be equal to one third of the square root of the number density of galaxies in the universe today, as suggested by Tegmark et al. (2006) . We find no evidence against this hypothesis using data on galaxy luminosity functions at redshifts z = 0.1, 1.0 and 3.5 taken from the Sloan Digital Sky Survey (SDSS). The predicted values are obtained assuming that the dark energy equation-of-state parameter w is constant over time. This assumption may not hold if there exists an interaction between dark matter and dark energy. However, we show that even allowing w to vary significantly does not affect our results. \n \n In addition, we use the WMAP 5-year cosmological parameters to calculate the expected temperature anisotropy power spectrum of the cosmic microwave background radiation (CMB) given the current best-fit model. We compare these theoretical predictions with measurements made by the Wilkinson Microwave Anisotropy Probe (WMAP), finding good agreement across all multipole moments up to lmax = 1000.",
        "watermark_text": "We test whether the known value of lambda is compatible with the observation that it should be equal to one third of the square root of the number density of galaxies in the universe today , as suggested by Tegmark et al . ( 2006 ) .We see no evidence against this hypothesis utilizing information on star luminosity functions at redshifts z = 0 . 1 , 1 . 0 and 3 . 5 taken from the Sloan Digital Sky Survey ( SDSS ) . The predicted readings are derived assuming that the dark energy equation - of - state variable W is constant over time .This assumption must not hold if there exists an interaction between dark matter and dark energy . However , we find that even allowing W to vary significantly does not alter our findings .In addition , we utilize the WMAP 5 - day cosmological values to estimate the expected thermal anisotropy energy spectrum of the cosmic microwave background radiation ( CMB ) given the latest best - fitting model . We match these theoretical estimates with observations made by the Wilkinson Microwave Anisotropy Probe ( WMAP ) , finding high agreement across all multipole minutes up to lmax = 1000 .",
        "rewrite_text": "We examine if the known value of lambda aligns with the observation that it should equal one third of the square root of the current number density of galaxies in the universe, as proposed by Tegmark et al. (2006). Our analysis, based on star luminosity functions at redshifts z = 0.1, 1.0, and 3.5 from the Sloan Digital Sky Survey (SDSS), reveals no evidence contradicting this hypothesis. The predicted values are calculated under the assumption that the dark energy equation-of-state parameter W remains constant over time. This assumption may not apply if there is an interaction between dark matter and dark energy. However, our results remain unchanged even when allowing for significant variations in W. Furthermore, we use the WMAP 5-year cosmological data to estimate the expected thermal anisotropy energy spectrum of the cosmic microwave background radiation (CMB) based on the most accurate fitting model available. We compare these theoretical predictions with observations from the Wilkinson Microwave Anisotropy Probe (WMAP) and find strong agreement across all multipole moments up to lmax = 1000.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 6.340751391209736,
        "rewrite-fast-z-score": 1.835325870964494
    },
    {
        "original_text": "We present the first results on differential rotation for an evolved star, based on observations with the Microvariability and Oscillations of STars (MOST) satellite. The target is the F-type main-sequence star kappa 1 Cet (HD 128898), which has been observed in two consecutive runs during 2005-2006. We find that the surface shear rate decreases towards lower latitudes, but increases again at mid-latitudes. This behaviour can be explained if we assume that there are two differentially rotating regions on opposite sides of the equator. In addition to this large-scale pattern, we also detect smaller-scale features such as spots and plages. These structures appear to have lifetimes between one week and several months. Finally, we show how our results compare with theoretical predictions made using stellar evolution models. Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST",
        "watermark_text": "We report the first findings on differential rotation for an evolved star , based on observations with the Microvariability and Oscillations of STars ( MOST ) satellite . The target is the F - class major - sequence star kappa 1 Cet ( HD 128898 ) , which has been observed in two consecutive ran during 2005 - 2006 .We see that the surface shear rate decreases nearer lower latitudes , but grows again at mid - latitudes . This behaviour can be described if we suppose that there are two differentially rotating zones on opposite sides of the equator .In addition to this big - scale signature , we also observe lower - scale characteristics such as spots and plages . These structures appear to have lifetimes between one month and many months .Finally , we tell how our findings compare with theoretical estimates made using stellar evolution models . Keywords : Differential rotation ; Stellar activity ; Spots ; Plages ; MOST",
        "rewrite_text": "We present our initial findings on differential rotation in an evolved star, derived from observations conducted with the Microvariability and Oscillations of Stars (MOST) satellite. Our study focuses on the F-class main-sequence star kappa 1 Cet (HD 128898), which was observed over two consecutive runs from 2005 to 2006. Our analysis reveals that the surface shear rate decreases as one moves toward lower latitudes, but increases again at mid-latitudes. This pattern suggests the presence of two differentially rotating zones located on opposing sides of the equator. In addition to this large-scale behavior, we also identify finer-scale features such as spots and plages, which appear to have lifetimes that range from one month to several months. Finally, we discuss how our observations align with theoretical predictions based on stellar evolution models. Keywords: Differential rotation; Stellar activity; Spots; Plages; MOST.",
        "ori-fast-z-score": -0.7745966692414834,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 1.524001524002286
    },
    {
        "original_text": "The relationship between the size of an animal s body (its mass) and that of its brain is known as allometric scaling, but it remains unclear how this relationship arises in evolution.  In particular, there are two competing hypotheses for why larger animals have bigger brains than smaller ones.   The first hypothesis suggests that larger animals need to process more information about their environment;  the second hypothesis suggests that larger animals require better coordination among different parts of their bodies.  Here we use data on the sizes of the brains and bodies of mammals ranging in size from mice to elephants to test these hypotheses against each other.  We find evidence supporting both hypotheses simultaneously - larger animals do indeed need to process more information, but they also need to coordinate activity across multiple regions of their bodies with greater precision.  Our results suggest that the evolutionary pressure driving increases in brain size may be related to the ability of large-brained animals to integrate information over space and time.",
        "watermark_text": "The relationship between the height of an organism s body ( its weight ) and that of its brain is known as allometric scaling , but it remains unclear how this relationship occurs in evolution . In particular , there are two different hypotheses for why greater animals have larger brains than larger ones .The first theory indicates that larger humans need to absorb more information about their environment ; the second hypothesis suggests that larger animals require improved coordination among different components of their bodies . Here we using data on the sizes of the brains and bodies of mammals varied in height from rats to elephants to test these hypotheses against each other .We see evidence proving both hypotheses jointly - larger animals do actually require to absorb more information , but they still must to organize activity across multiple parts of their bodies with higher precision . Our results propose that the evolutionary stress driving variations in brain diameter may be due to the ability of large - brained animals to relate data over space and period .",
        "rewrite_text": "The relationship between an organism's body size (or weight) and its brain size is referred to as allometric scaling, but the evolutionary basis of this relationship remains uncertain. Specifically, there are two main hypotheses explaining why larger animals tend to have bigger brains. The first theory suggests that larger species, like humans, need to process more information about their environment, while the second hypothesis posits that larger animals require enhanced coordination among their bodily systems. In our study, we analyze data on the brain and body sizes of mammals ranging from rats to elephants to evaluate these hypotheses. Our findings provide support for both theories: larger animals indeed need to gather more environmental information, and they must also coordinate the activities of various body parts with greater precision. Our results indicate that the evolutionary pressures influencing variations in brain size may be linked to the capacity of large-brained animals to interpret information across different spatial and temporal contexts.",
        "ori-fast-z-score": -0.21320071635561041,
        "water-fast-z-score": 7.397576490380784,
        "rewrite-fast-z-score": -0.741998516004452
    },
    {
        "original_text": "The cosmic ray origin is still unknown, but it may be related to the supernova explosion.  The propagation model describes how cosmic rays are transported in space by diffusion processes with energy losses due to ionization and Coulomb scattering off interstellar gas particles.   This article presents an overview on cosmic ray physics and introduces some basic concepts for understanding cosmic ray transport models. Cosmic rays have been observed since the 19th century. They consist mainly of protons (about 85%) and helium nuclei (about 14%). Their energies range up to 10^20 eV. However, their sources remain unclear. It has been suggested that they might come from exploded stars called supernovae or from active galactic nuclei such as quasars. In this case, they would travel through intergalactic space before reaching Earth. Another possibility is that they are accelerated within our own galaxy. If so, then they could reach us directly without traveling far away.",
        "watermark_text": "The cosmic ray origin is still unclear , but it could be connected to the supernova explosion . The propagation model describes how cosmic rays are transported in space by diffusion processes with energy losses owing to ionization and Coulomb absorption off interstellar gas molecules .This page presents an overview on cosmic ray physics and introduces some fundamental concepts for studying cosmic ray transport models . Cosmic rays have been observed since the 19th century .They consist mostly of protons ( about 85 % ) and helium nuclei ( about 14 % ) . Their energies range up to 10 ^ 20 eV .However , their sources continue unknown . It has been proposed that they may come from burst stars called supernovae or from active galactic nuclei such as quasars .In this instance , they may travel through intergalactic space before reaching Earth . Another possibility is that they are accelerated within our own galaxy .If so , then they may reach us directly without traveling long away .",
        "rewrite_text": "The origin of cosmic rays remains uncertain, but it is thought to be linked to supernova explosions. The propagation model explains how cosmic rays move through space, undergoing diffusion processes and experiencing energy losses due to ionization and Coulomb interactions with interstellar gas molecules. This page provides an overview of cosmic ray physics and introduces fundamental concepts relevant to the study of cosmic ray transport models. Cosmic rays have been detected since the 19th century, primarily consisting of protons (about 85%) and helium nuclei (approximately 14%). Their energies can reach up to 10^20 eV, yet their exact sources remain unidentified. It has been suggested that they may originate from supernovae or from active galactic nuclei, such as quasars, traveling through intergalactic space before arriving at Earth. Alternatively, they could be accelerated within our own galaxy, allowing them to reach us directly without having to travel vast distances.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.421151989096865,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "We present new near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which reveal that its circumstellar dust is composed of two distinct populations with different grain sizes. The polarization degree decreases rapidly towards longer wavelengths at all positions along our slit except for one position where it increases again between 2.2 and 3.8 microns. We interpret this as evidence for an inner hole in the distribution of larger grains. This interpretation is supported by SED modeling using radiative transfer calculations including scattering off spherical particles. Our results suggest that the outer edge of the gap lies within 0.1 AU of the central star. In addition to the NIR data presented here we also obtained mid-infrared (MIR) spectro-polarimetry covering the wavelength range 5-20 micron. These data show no significant change in the polarization degree across the MIR bands indicating that there are no strong changes in the optical properties of the dust grains on these scales.",
        "watermark_text": "We present new near - infrared ( NIR ) polarimetric studies of the Herbig Ae star HD 142527 , which confirm that its circumstellar dust is composed of two separate populations with varying grain sizes . The polarization degree drops rapidly towards faster wavelengths at all positions along our slit except for one position where it reduces again between 2 . 2 and 3 . 8 microns .We interpret this as proof for an inner cavity in the distribution of bigger grains . This interpretation is backed by SED modeling using radiative transfer calculations including scattering off spherical objects .Our results show that the exterior border of the gap exists within 0 . 1 AU of the main star . In addition to the NIR data provided here we also discovered mid - infrared ( MIR ) spectro - polarimetry representing the frequency range 5 - 20 micron .These data indicate no major shift in the polarization degree across the MIR groups implying that there are no strong changes in the optical properties of the dust grains on these scales .",
        "rewrite_text": "We present new near-infrared (NIR) polarimetric observations of the Herbig Ae star HD 142527, which confirm that its circumstellar dust comprises two distinct populations with varying grain sizes. The degree of polarization decreases significantly towards shorter wavelengths at all positions along our slit, except at one location where it diminishes again between 2.2 and 3.8 microns. We interpret this as evidence of an inner cavity in the distribution of larger grains. This interpretation is supported by spectral energy distribution (SED) modeling that employs radiative transfer calculations, including scattering from spherical objects. Our findings indicate that the outer boundary of the gap is located within 0.1 AU of the central star. In addition to the NIR data presented here, we also observed mid-infrared (MIR) spectropolarimetry over the range of 5 to 20 microns. These MIR data show no significant variation in the degree of polarization across the different groups, suggesting that there are no considerable changes in the optical properties of the dust grains on these scales.",
        "ori-fast-z-score": 1.4342743312012722,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 2.254885150568321
    },
    {
        "original_text": "We present an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet using soft constraint logic programming (SCLP). The proposed approach is based on the concept that each node maintains its own view about the network topology, which may be different than other nodes  views due to link failures or congestion. We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes. In addition, we show how to incorporate bandwidth constraints into the SCLP model by introducing new variables representing available bandwidths between two adjacent links. Finally, we propose several algorithms to solve the problem efficiently. Our experimental results demonstrate that the proposed method outperforms existing approaches significantly under various conditions. Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast",
        "watermark_text": "We present an algorithm for unicast and multicast quality - of - service ( QoS ) routing in the Internet employing soft constraint logic programming ( SCLP ) . The proposed approach is based on the idea that each node maintains its own view about the network topology , which may be changed than other nodes views due to link faults or congestion .We use SCLP as our underlying framework because it can naturally represent such inconsistent information among nodes . In addition , we study how to insert bandwidth constraints into the SCLP model by using new parameters representing available bandwidths between two adjacent links .Finally , we present many algorithms to solve the issue quickly . Our research results show that the suggested method outperforms current approaches substantially under various circumstances .Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast",
        "rewrite_text": "We introduce an algorithm for unicast and multicast quality-of-service (QoS) routing in the Internet that utilizes soft constraint logic programming (SCLP). Our approach is founded on the concept that each node possesses its own perspective of the network topology, which may differ from others' views due to link failures or congestion issues. We adopt SCLP as our foundational framework because it effectively accommodates such discrepancies in information among nodes. Furthermore, we explore the integration of bandwidth constraints into the SCLP model by introducing new parameters that reflect the available bandwidth between adjacent links. Finally, we propose several algorithms to address this issue efficiently. Our research demonstrates that the proposed method significantly exceeds the performance of existing approaches across various scenarios. Keywords: Quality-of-Service, Constraint Logic Programming, Bandwidth Allocation, Network Optimization, Link Failure, Congestion Control, Internet Service Provider, Unicast.",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -1.524001524002286
    },
    {
        "original_text": "The rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.",
        "watermark_text": "The rapid increase in the using and production of digital media has established an urgent need to develop new models that enable large - term access , preservation , and reuse of personal libraries . In this page we present a service model for controlling personal libraries using on three key concepts : The archive is viewed as a collection of interrelated artifacts ( e . g . , files , photos ) ; each object is associated with one or more functions that provide functionality such as processing , editing , sharing , etc .; and these services are coordinated into a structure indicating their connections . We illustrate how our approach can be used by individuals to manage their own individual archives , and also discuss its potential application within organizations where vast volumes of records must be managed over longer periods of time .The rapid increase in the using of digital media has led to greater activity in developing systems that enable users to archive and transfer their personal data across multiple computers and platforms . However , current approaches have concentrated mostly on providing strategies for storing and accessing information rather than solving topics related to preserving it over time .This problem appears particularly intense when dealing with libraries containing many of items spanning numerous years . To address this question , we propose a service - based architecture for storing and keeping personal records .",
        "rewrite_text": "The swift growth in the use and production of digital media has created an urgent necessity to devise new models for ensuring long-term access, preservation, and reuse of personal libraries. In this section, we introduce a service model for managing personal libraries based on three fundamental concepts: the archive is perceived as a collection of interconnected artifacts (such as files and photos); each artifact is linked to one or more functions that offer capabilities like processing, editing, and sharing; and these services are organized into a framework that reveals their relationships. We demonstrate how individuals can utilize our approach to manage their personal archives and examine its potential applications within organizations that must handle extensive volumes of records over extended periods. The increase in digital media usage has also triggered efforts to develop systems that allow users to archive and transfer their personal data across various computers and platforms. However, existing solutions predominantly focus on strategies for storing and accessing information, overlooking the critical issue of long-term preservation. This challenge is especially pronounced for libraries containing numerous items accumulated over many years. To tackle this issue, we propose a service-based architecture designed for the storage and preservation of personal records.",
        "ori-fast-z-score": 0.7126966450997984,
        "water-fast-z-score": 9.698686309445845,
        "rewrite-fast-z-score": 0.8320502943378436
    },
    {
        "original_text": "We propose an encoding scheme for layered associative network (LAN) of spiking neurons, which is based on sparse coding and dense coding. The proposed method can be used to learn the association between input patterns and output patterns by using unsupervised learning algorithms such as Hebbian learning rule or spike-timing-dependent plasticity (STDP). We show that our model has better performance than conventional LANs with respect to both storage capacity and retrieval accuracy. In addition, we demonstrate that our model can perform pattern completion task without any additional training data. \nIndex Terms-Layered associaton network, Sparse coding, Dense coding, Pattern completion, STDP, Unsupervised learning algorithm. 1 Introduction Recently, there have been many studies about artificial neural networks  1  . Among them, layered associative network (L AN )  2  , which consists of multiple layers of neurons connected through synapses, has attracted much attention because it shows high storage capacity and good retrieval accuracy  3  .\nIn L AN s, each neuron receives inputs from all neurons in previous layer via synaptic connections. Then, the activity level of each neuron is determined by its firing rate according to the following equation: \nwhere x i denotes the activity level of ith neuron at time t, w ij represents connection weight from jth neuron in previous layer to ith neuron in current layer, f(·) stands for activation function, and b i indicates bias term  4  . Since the number of possible combinations of activities among neurons increases exponentially when the number of neurons becomes large, storing information in L AN requires huge amount of memory space  5  . To overcome this problem, several approaches have been suggested  6  -  8  . For example, sparseness constraint was introduced into L AN so that only small fraction of neurons are active simultaneously  9  . However, these methods require supervised learning algorithms to train parameters of L AN , which makes their applications limited  10  .",
        "watermark_text": "We suggest an encoding scheme for layered associative network ( LAN ) of spiking neurons , which is based on sparse coding and thin code . The proposed approach can be used to study the relationship between output patterns and input patterns by using unsupervised learning techniques such as Hebbian learning rule or spike - timing - based plasticity ( STDP ) .We suggest that our model has good efficiency than conventional LANs with regard to both storage capacity and retrieval quality . In addition , we prove that our model can conduct pattern completion process without any additional testing information .Index Terms - Layered associaton system , Sparse codes , Dense coding , Pattern completion , STDP , Unsupervised learning scheme . 1 Introduction Recently , there have been many research about artificial neural systems 1 .Among them , layered associative network ( L AN ) 2 , which consists of multiple strands of neurons connected through synapses , has gained great concern because it displays large storage capacity and good retrieval quality 3 . In L AN s , each neuron receives inputs from all neurons in preceding layer via synaptic connections .Then , the activity rate of each neuron is calculated by its fire rate due to the following equation : where h i denotes the activity rate of ith neuron at time t , w ij represents connection weight from jth neuron in earlier layer to ith neuron in current layer , f ( · ) stands for activation function , and g i denotes bias term 4 . Since the number of possible combinations of activities among neurons increases exponentially when the number of neurons becomes large , encoding information in L AN demands huge amount of memory space 5 .To solve this question , various approaches have been proposed 6 - 8 . For instance , sparseness constraint was introduced into L AN so that only tiny fraction of neurons are active simultaneously 9 .However , these algorithms involve supervised learning techniques to train parameters of L AN , which makes their users limited 10 .",
        "rewrite_text": "We propose an encoding scheme for a layered associative network (LAN) comprised of spiking neurons, utilizing principles of sparse coding and thin coding. This approach allows for the exploration of the relationship between input and output patterns through unsupervised learning methods, such as the Hebbian learning rule and spike-timing-dependent plasticity (STDP). We argue that our model demonstrates superior efficiency compared to traditional LANs, particularly in terms of storage capacity and retrieval accuracy. Furthermore, we demonstrate that our model can perform pattern completion without the need for additional testing information. \n\n**Index Terms:** Layered associative network, sparse codes, dense coding, pattern completion, STDP, unsupervised learning method.\n\n**1 Introduction**  \nIn recent years, research into artificial neural systems has surged. Among these, the layered associative network (LAN) has attracted significant interest due to its high storage capacity and impressive retrieval performance. In LANs, each neuron receives input from all neurons in the preceding layer through synapses. The activity rate of each neuron is determined by its firing rate, articulated by the equation where \\( h_i \\) denotes the activity rate of the \\( i^{th} \\) neuron at time \\( t \\), \\( w_{ij} \\) represents the connection weight from the \\( j^{th} \\) neuron in the previous layer to the \\( i^{th} \\) neuron in the current layer, \\( f(\\cdot) \\) is the activation function, and \\( g_i \\) is the bias term. As the number of neurons increases, the potential combinations of their activities grow exponentially, resulting in a substantial memory requirement for encoding information in LANs. Various solutions have been proposed to address this challenge. For instance, a sparsity constraint can be implemented in LANs to ensure that only a small fraction of neurons are activated at any given time. However, these algorithms typically rely on supervised learning techniques to train the model parameters, which limits their broader applicability.",
        "ori-fast-z-score": -0.16329931618554522,
        "water-fast-z-score": 8.597735319930415,
        "rewrite-fast-z-score": 1.643989873053573
    },
    {
        "original_text": "The authors report on their experience with immunohistochemistry (IHC) for insulin degrading enzyme (IDE). They found that IDE is expressed by many cell types, including epithelial cells lining ducts or glands, endothelial cells, smooth muscle cells, fibroblasts, adipocytes, neurons, glial cells, astrocytes, oligodendroglia, microglia, Schwann cells, chondrocytes, osteoblasts, myofibroblasts, mesothelial cells, macrophages, lymphocytes, mast cells, neutrophils, eosinophils, basophils, megakaryocytes, erythroid precursors, platelets, monocytes/macrophages, dendritic cells, natural killer cells, B-cells, T-cells, plasma cells, granulocytes, megakaryocytes/platelets, and bone marrow stromal cells. The authors also describe several IHC pitfalls associated with this antigen.",
        "watermark_text": "The authors report on their experience with immunohistochemistry ( IHC ) for insulin degrading enzyme ( IDE ) . They found that IDE is expressed by many cell kinds , notably epithelial cells lining ducts or glands , endothelial cells , soft muscle cells , fibroblasts , adipocytes , cortex , glial cells , astrocytes , oligodendroglia , microglia , Schwann cells , chondrocytes , osteoblasts , myofibroblasts , mesothelial cells , macrophages , lymphocytes , mast cells , neutrophils , eosinophils , basophils , megakaryocytes , erythroid precursors , platelets , monocytes / macrophages , dendritic cells , natural killer cells , B - cells , T - cells , plasma cells , granulocytes , megakaryocytes / platelets , and skin marrow stromal cells .The authors additionally describe several IHC pitfalls associated with this antigen .",
        "rewrite_text": "The authors share their findings regarding the use of immunohistochemistry (IHC) to investigate insulin degrading enzyme (IDE). Their study revealed that IDE is expressed across a diverse range of cell types, including epithelial cells in ducts and glands, endothelial cells, smooth muscle cells, fibroblasts, adipocytes, cortical cells, glial cells, astrocytes, oligodendrocytes, microglia, Schwann cells, chondrocytes, osteoblasts, myofibroblasts, mesothelial cells, macrophages, lymphocytes, mast cells, neutrophils, eosinophils, basophils, megakaryocytes, erythroid precursors, platelets, monocytes/macrophages, dendritic cells, natural killer cells, B cells, T cells, plasma cells, granulocytes, and skin marrow stromal cells. The authors also discuss various pitfalls encountered in IHC related to this antigen.",
        "ori-fast-z-score": 2.0816659994661326,
        "water-fast-z-score": 3.893314107138301,
        "rewrite-fast-z-score": 1.9824814143238607
    },
    {
        "original_text": "We present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators . We see that these results can be obtained by treating Maxwell s equations using an appropriate Green function method .The resulting expressions are using to estimate the dispersion relations for surface plasmons ( SPs ) and surface phonons ( SPhPs ) . In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has negative values .Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits . Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily explored over numerous years 1 .They play essential roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 . Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 .These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to heightened thermal transport 9 or thermoelectricity 10 . Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "We provide detailed solutions for the electromagnetic field in the vicinity of spherical objects with arbitrary dielectric properties, encompassing both metals and insulators. Our findings are derived from applying a suitable Green function approach to Maxwell's equations. These expressions are then utilized to analyze the dispersion relations of surface plasmons (SPs) and surface phonon polaritons (SPhPs). Notably, we discover that SPs arise solely when the real part of the dielectric constant equals zero, whereas SPhPs can exist even with negative dielectric values. We also compare our results to those predicted by classical Drude theory, evaluating their limitations in efficacy. Surface plasmons, which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over the years and are crucial in various domains, including optics, electronics, sensing, and catalysis. Recently, there has been an increasing interest in surface phonon polaritons (SPhPs), which are related excitations linked to longitudinal acoustic waves. These modes can manifest not only at surfaces but also within bulk materials, potentially enhancing thermal transport and thermoelectric performance. Additionally, SPhPs can interact strongly with light, giving rise to intriguing phenomena such as superprism effects and exceptional transmission.",
        "ori-fast-z-score": -0.8164965809277261,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": 1.3779972440082682
    },
    {
        "original_text": "We present new results for the ratio between the core radius rc and the half-light radius r h of open star clusters, based on high-resolution HST imaging data obtained with the Advanced Camera for Surveys (ACS). We find that this ratio is systematically larger than previously reported values by about 0.2 dex. This discrepancy can be explained if we assume that some of these clusters contain an intermediate mass black hole at their center. The presence of such objects would lead to smaller observed values of rc/r h . Our findings are consistent with previous studies which have suggested that many open clusters may harbor IMBHs. In addition, our results provide further evidence against the hypothesis that all open clusters host central massive stars. \n \n Keywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate Mass Black Hole; ACS/HRC FOV; Galaxy",
        "watermark_text": "We publish new data for the proportion between the core radius rc and the half - light diameter r h of open star clusters , using on wide - resolution HST scanning data derived with the Advanced Camera for Surveys ( ACS ) . We see that this ratio is systematically larger than previously reported values by about 0 . 2 dex .This discrepancy can be described if we suppose that some of these clusters hold an intermediate mass black hole at their core . The presence of such objects would result to smaller observed values of rc / r h .Our findings are compatible with previous research which have suggested that several open nuclei may harbor IMBHs . In addition , our findings provide further evidence against the notion that all open complexes host central large objects .Keywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate Mass Black Hole; ACS/HRC FOV; Galaxy",
        "rewrite_text": "We present new measurements of the ratio between the core radius (rc) and the half-light diameter (rh) of open star clusters, utilizing high-resolution data from the Advanced Camera for Surveys (ACS) on the Hubble Space Telescope (HST). Our analysis reveals that this ratio is consistently larger than earlier reports by approximately 0.2 dex. This difference can be explained by the hypothesis that some of these clusters may contain an intermediate-mass black hole (IMBH) at their center, which would lead to lower observed values of rc/rh. Our results align with previous studies suggesting that several open star clusters could be hosting IMBHs. Moreover, our findings provide additional support against the idea that all open clusters possess large central objects. \n\nKeywords: Open Star Cluster; Core Radius; Half-Mass Radius; Black Hole; Intermediate Mass Black Hole; ACS/HRC FOV; Galaxy.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "The Peierls-Nabarro model is used to study the dislocations dynamics in a crystal lattice, where the energy barrier for glide motion and climb motion are calculated by using the concept of activation volume. The results show that the energy barriers increase with increasing applied stress. It also shows that the energy barrier decreases as temperature increases. Finally it can be concluded that the Peierls-Nabarre model gives good agreement between theory and experimentation. Keywords: Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarrou Model. 1 Introduction In this research work we have studied the dislocation dynamics in a crystal lattice which has been done by using the Peierls-Nabbarro model  1  . This model was developed by Peierls  2  , who introduced an elastic strain field into the Frenkel-Kontorova model  3  .\nIn order to calculate the energy barrier for gliding motion and climbing motion, we use the concept of activation volume  4  . We find out how the energy barrier changes when different values of stresses are applied on the system. Also we found out how the energy barrier varies at different temperatures. Finally we compare our theoretical results with experimental data  5  -  8  .",
        "watermark_text": "The Peierls - Nabarro theory is utilized to study the dislocations dynamics in a crystal lattice , where the power barrier for glide movement and ascent movement are measured by using the idea of activation volume . The results show that the electricity obstacles increase with increasing applied strain .It thus suggests that the electricity barrier decreases as temperature increases . Finally it can be realized that the Peierls - Nabarre hypothesis offers excellent agreement between theoretical and experimentation .Keywords : Energy Barrier , Dislocation , Glide Motion , Climb Motion , Activation Volume , Peierls - Nabarrou Model . 1 Introduction In this research effort we have researched the dislocation behavior in a crystal lattice which has been performed by using the Peierls - Nabbarro model 1 .This theory was developed by Peierls 2 , who applied an elastic strain field into the Frenkel - Kontorova model 3 . In order to estimate the power barrier for gliding motion and climbing movement , we utilize the idea of activation volume 4 .We work out how the power barrier changes when varying expressions of stresses are applied on the system . Also we learned out how the power barrier changes at different temperatures .Finally we compare our theory findings with experimental evidence 5 - 8 .",
        "rewrite_text": "The Peierls-Nabarro theory is employed to investigate the dynamics of dislocations in a crystal lattice, where the energy barriers for glide and climb movements are assessed through the concept of activation volume. The findings indicate that the energetic barriers increase with higher applied strain, suggesting that the barriers decrease with rising temperature. Ultimately, the Peierls-Nabarro model demonstrates excellent correspondence between theoretical predictions and experimental results. \n\n**Keywords:** Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarro Model. \n\n**1 Introduction**  \nThis research explores the behavior of dislocations in a crystal lattice using the Peierls-Nabarro model. Developed by Peierls, this theory integrates an elastic strain field within the Frenkel-Kontorova model. To estimate the energy barriers associated with glide and climb movements, we utilize the activation volume concept. We analyze how these barriers are affected by different applied stress conditions and varying temperatures. Finally, we compare our theoretical outcomes with experimental data.",
        "ori-fast-z-score": -0.5241424183609592,
        "water-fast-z-score": 6.81385143869247,
        "rewrite-fast-z-score": -0.46499055497527714
    },
    {
        "original_text": "The new reactor is based on the principle that nuclear fusion can be achieved by heating plasma to high temperatures in magnetic fields.  The proposed design uses an array of magnets and electrodes, which are powered by microwave generators, to heat hydrogen gas into plasma at 100 million degrees Celsius (1,000,000 K). This temperature is sufficient to cause nuclear fusion reactions between deuterium nuclei inside the plasma. In this way, energy is released as neutrons and gamma rays. These particles then bombard fuel pellets made up of tritium and lithium-6, causing them to release more energy through further nuclear fusion reactions. The resulting power plant would produce electrical power with no moving parts or radioactive waste products. It could also provide propulsion systems for spacecraft without requiring any chemical fuels. A conceptual drawing of the reactor system is shown below. Nuclear fusion has been studied extensively since its discovery over 50 years ago. However, it remains elusive because the conditions required to achieve fusion are extremely difficult to create experimentally.",
        "watermark_text": "The modern reactor is based on the principle that fusion fusion can be obtained by burning plasma to large conditions in magnetic fields . The proposed design uses an array of magnets and electrodes , which are powered by microwave generators , to heat hydrogen energy into plasma at 100 million degrees Celsius ( 1 , 000 , 000 K ) .This temperature is adequate to create nuclear fusion events between deuterium nuclei inside the plasma . In this way , electricity is released as neutrons and alpha rays .These particles then bombard fuel pellets made up of tritium and lithium - 6 , forcing them to release more electricity through further nuclear fusion compounds . The resulting electricity plant would create electrical power with no moving parts or radioactive waste products .It could also supply propulsion services for spacecraft without using any chemical engines . A conceptual drawing of the reactor system is displayed below .Nuclear fusion has been studied thoroughly since its observation over 50 centuries earlier . However , it remains elusive because the conditions required to achieve fusion are extremely difficult to create experimentally .",
        "rewrite_text": "The modern reactor operates on the principle that nuclear fusion can be achieved by heating plasma to extreme temperatures within magnetic fields. The proposed design features a combination of magnets and electrodes powered by microwave generators to heat hydrogen into plasma at 100 million degrees Celsius (1,000,000 K). This temperature is sufficient to facilitate fusion events between deuterium nuclei in the plasma. As a result, energy is released in the form of neutrons and alpha particles, which then interact with fuel pellets composed of tritium and lithium-6, prompting further nuclear fusion and generating more electricity. The envisioned power plant would produce electrical energy without any moving parts or radioactive waste and could also provide propulsion for spacecraft, eliminating the need for chemical engines. Below, there is a conceptual illustration of the reactor system. Although nuclear fusion has been researched extensively since its discovery over 50 years ago, it continues to be a challenge because the conditions necessary for fusion are exceptionally hard to replicate in an experimental setting.",
        "ori-fast-z-score": 1.2792042981336627,
        "water-fast-z-score": 6.396021490668312,
        "rewrite-fast-z-score": 1.4100479758212652
    },
    {
        "original_text": "We propose that the prompt emission of gamma-ray bursts (GRBs) is due to high-energy protons, muons and electron-positron pairs produced by ultra-relativistic shocks in GRB jets. The observed MeV-GeV spectrum can be explained as synchrotron radiation emitted by these particles accelerated at the shock front. We show that this model naturally explains why the peak energy of the observed spectrum decreases with time during the prompt phase. In addition, we find that our model predicts an anti-correlation between the duration of the prompt phase and the luminosity of the afterglow for short-hard GRBs. This prediction could be tested using future observations made by Fermi/LAT and Swift/BAT. Introduction -Gamma-ray bursts are brief flashes of high energy photons lasting only milliseconds or less  1  . They have been detected out to redshifts z = 8.2  2  , which implies their total energy output may exceed 10^53 erg  3  . Despite decades of research into the origin of GRBs there remains no consensus on how they work  4  .\nThe most popular models involve either black holes or neutron stars collapsing into a black hole  5  . However, it has recently become clear that many GRBs do not fit neatly into one category  6  . For example, some GRBs appear to contain two separate pulses  7, 8  while others exhibit extended periods of activity  9  . Furthermore, some GRBs seem to occur when two galaxies merge  10  . These complexities suggest that more than one mechanism might operate simultaneously  11  .\nIn recent years several authors  12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59   have proposed that GRBs are powered by relativistic jets e",
        "watermark_text": "We suggest that the prompt emission of gamma - ray waves ( GRBs ) is due to large - energy protons , muons and electron - positron couples created by ultra - relativistic shocks in GRB jets . The observed MeV - GeV spectrum can be understood as synchrotron rays generated by these ions advancing at the shock back .We see that this model readily explains why the maximum energy of the seen spectrum drops with time during the prompt phase . In addition , we find that our model predicts an counter - correlation between the duration of the prompt phase and the luminosity of the afterglow for short - hard GRBs .This prediction might be evaluated using later observed made by Fermi / LAT and Swift / BAT . Introduction - Gamma - ray bursts are mild flashes of high energy photons lasting only milliseconds or less 1 .They have been detected out to redshifts z = 8 . 2 2 , which implies their total energy efficiency may exceed 10 ^ 53 erg 3 . Despite decades of research into the origin of GRBs there stands no discussion on how they use 4 .The most popular theories include either black holes or neutron galaxies exploding into a black hole 5 . However , it has recently become clear that several GRBs do not fit nicely into one category 6 .For instance , some GRBs occur to contain two separate pulses 7 , 8 while several display extended phases of activity 9 . Furthermore , some GRBs appears to arise when two galaxies merge 10 .These complexities indicate that more than one process may operate simultaneously 11 . In recent years many writers 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 have proposed that GRBs are powered by relativistic jets e",
        "rewrite_text": "We propose that the prompt emission of gamma-ray bursts (GRBs) originates from high-energy protons, muons, and electron-positron pairs generated by ultra-relativistic shocks within GRB jets. The observed spectrum in the MeV-GeV range can be explained as synchrotron radiation produced by these particles as they move toward the shock front. Our model effectively accounts for the observed decrease in maximum energy of the spectrum over time during the prompt phase. Additionally, we find that it predicts an inverse correlation between the duration of the prompt phase and the luminosity of the afterglow in short-hard GRBs. This prediction can be tested against observations from Fermi/LAT and Swift/BAT. \n\nIntroduction: Gamma-ray bursts are brief flashes of high-energy photons, lasting mere milliseconds or less. They have been detected at redshifts up to z = 8.2, suggesting that their total energy efficiency may exceed 10^53 ergs. Despite decades of investigation into the origins of GRBs, there is still no consensus on their mechanism. The leading theories attribute them to either black hole formation or the collapse of neutron stars into black holes. However, it has recently become apparent that many GRBs do not fit neatly into these categories. For example, some GRBs exhibit two distinct pulses, while others demonstrate prolonged activity. Additionally, certain GRBs appear to result from the merging of two galaxies. These complexities suggest that multiple processes may be operating concurrently. In recent years, numerous researchers have suggested that GRBs are powered by relativistic jets.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.184271554937297,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "SDSS J121811+465501 is an extremely faint galaxy discovered in the Sloan Digital Sky Survey (SDSS). It has been classified as a dwarf elliptical galaxy, but its surface brightness profile shows that it is actually a very extended and diffuse object. We have obtained deep optical spectroscopy for this galaxy using the Gemini Multi-Object Spectrograph on Gemini North telescope to study its physical properties. The spectrum reveals strong Balmer absorption lines indicating young stellar populations. Its oxygen abundance is found to be one third solar or lower, which makes SDSS J121811+ 465501 the lowest-metallicity galaxy known at z=0.1. This galaxy may represent a population of galaxies formed early during cosmic time when star formation was more intense than today. Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "watermark_text": "SDSS J121811 + 465501 is an incredibly faint galaxy discovered in the Sloan Digital Sky Survey ( SDSS ) . It has been classified as a dwarf elliptical galaxy , but its surface brightness profile reveals that it is actually a very extended and diffuse object .We have achieved dark optical spectroscopy for this galaxy using the Gemini Multi - Object Spectrograph on Gemini North telescope to study its physical properties . The spectrum reveals large Balmer spectral lines suggesting young stellar regions .Its oxygen abundance is found to be one third solar or smaller , which makes SDSS J121811 + 465501 the smallest - metallicity galaxy known at z = 0 . 1 . This galaxy might represent a population of galaxies formed early during cosmic time when galaxy formation was more intense than tomorrow .Keywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness",
        "rewrite_text": "SDSS J121811 + 465501 is an extremely faint galaxy identified by the Sloan Digital Sky Survey (SDSS). Initially classified as a dwarf elliptical galaxy, its surface brightness profile indicates that it is, in fact, a highly extended and diffuse structure. We have conducted dark optical spectroscopy of this galaxy using the Gemini Multi-Object Spectrograph on the Gemini North telescope to investigate its physical characteristics. The spectrum displays prominent Balmer lines, which hint at the presence of young stellar regions. Moreover, its oxygen abundance is measured to be less than one-third of the solar level, making SDSS J121811 + 465501 the galaxy with the lowest metallicity known at z = 0.1. This galaxy could represent a category of galaxies that formed early in cosmic history, during a period of heightened galaxy formation activity. \n\nKeywords: Dwarf Elliptical Galaxy, Oxygen Abundance, Gemini Observatory, Sloan Digital Sky Survey, Low Surface Brightness.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 4.810702354423639,
        "rewrite-fast-z-score": -0.5163977794943222
    },
    {
        "original_text": "The effect of cooling rate on martensitic transformation temperature (Mf) was investigated for Ni$_{50}$ Mn$_{34}$ In$_{16}$ alloy using differential scanning calorimetry (DSC). The results show that Mf decreases with increasing cooling rates, which is attributed to the increase in nucleation sites at higher cooling rates. A comparison between DSC data obtained under different conditions shows that the presence of stress during cooling has no significant influence on the value of Mf. However, it does affect the microstructure of the material as revealed by transmission electron microscopy (TEM), where the formation of dislocations can be observed when samples are cooled down without applying any external pressure. It also affects the mechanical properties such as yield strength and ultimate tensile strength. \n \n © 2014 Elsevier B.V. \nKeywords: Magnetic shape memory alloys; Cooling rate; Martensitic transformation",
        "watermark_text": "The impact of cooling rate on martensitic transformation temperature ( Mf ) was investigated for Ni $ _ { 50 } $ Mn $ _ { 34 } $ In $ _ { 16 } $ alloy utilizing differential scanning calorimetry ( DSC ) . The results show that Mf falls with higher cooling rates , which is attributed to the improvement in nucleation sites at higher cooling rates .A comparison between DSC data derived under various circumstances reveals that the presence of stress during cooling has no important affect on the value of Mf . However , it does affect the microstructure of the material as revealed by transmission electron microscopy ( TEM ) , where the formation of dislocations can be observed when samples are heated down without applying any external stress .It additionally impacts the structural properties such as yield strength and absolute tensile strength . © 2014 Elsevier B . V . Keywords : Magnetic shape memory alloys ; Cooling rate ; Martensitic transformation",
        "rewrite_text": "The effect of cooling rate on the martensitic transformation temperature (Mf) of the Ni$_{50}$Mn$_{34}$In$_{16}$ alloy was examined using differential scanning calorimetry (DSC). The findings indicate that Mf decreases with increasing cooling rates, which is attributed to the enhanced availability of nucleation sites at these higher rates. A comparison of DSC data obtained under different conditions demonstrates that the presence of stress during cooling does not significantly influence the Mf value. However, it does have an effect on the material's microstructure, as evidenced by transmission electron microscopy (TEM), where dislocation formation is observed in samples that are cooled without any external stress. Additionally, the cooling rate influences structural properties such as yield strength and ultimate tensile strength. © 2014 Elsevier B.V. Keywords: Magnetic shape memory alloys; Cooling rate; Martensitic transformation.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 3.6765801200722312,
        "rewrite-fast-z-score": -1.0327955589886444
    },
    {
        "original_text": "We study the effect of Coulomb gauge fixing on the gluon propagator in Landau gauge, using lattice simulations with two different actions for SU(2) Yang-Mills theory. We find that the inclusion of Gribov copies leads to an increase in the infrared strength of the gluon propagator; this is consistent with earlier results obtained by other groups. \n \n In addition we show how the presence of Gribov copies affects the static quark-antiquark potential at large distances. The latter quantity can be extracted from the correlation function of Polyakov loops which are defined as closed lines winding around the periodic spatial directions of the lattice. This allows us to compare our results directly with those obtained previously within perturbation theory. Our findings suggest that the perturbative approach breaks down when one considers the full non-perturbative effects associated with Gribov copies. Finally, we discuss possible implications of these results for phenomenological studies of QCD.",
        "watermark_text": "We research the impact of Coulomb gauge fixing on the gluon propagator in Landau gauge , using crystal simulations with two different actions for SU ( 2 ) Yang - Mills theory . We see that the introduction of Gribov copies results to an increase in the infrared thickness of the gluon propagator ; this is compatible with previous findings obtained by other groups .In addition we study how the presence of Gribov copies impacts the static quark - antiquark potential at large distances . The latter quantity can be extracted from the relationship function of Polyakov loops which are defined as closed lines wound around the periodic spatial directions of the crystal .This enables us to relate our findings directly with those achieved previously within perturbation theory . Our findings show that the perturbative methodology cracks down when one considers the full non - perturbative properties associated with Gribov copies .Finally , we explain possible possibilities of these results for phenomenological investigations of QCD .",
        "rewrite_text": "We investigate the effects of Coulomb gauge fixing on the gluon propagator in Landau gauge, utilizing crystal simulations with two distinct actions for SU(2) Yang-Mills theory. Our results indicate that the introduction of Gribov copies leads to an increase in the infrared behavior of the gluon propagator, aligning with previous findings reported by other groups. Additionally, we examine how Gribov copies influence the static quark-antiquark potential at large distances. This potential can be obtained from the correlation function of Polyakov loops, which are defined as closed loops wrapping around the periodic spatial dimensions of the crystal. This approach allows us to directly connect our results to those derived from perturbation theory. Our findings reveal that the perturbative framework becomes inadequate when addressing the comprehensive non-perturbative effects associated with Gribov copies. Finally, we discuss the implications of these results for phenomenological studies in quantum chromodynamics (QCD).",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 5.0089472186085136,
        "rewrite-fast-z-score": -0.3721042037676254
    },
    {
        "original_text": "We present scaling laws that govern the performance and design trade-offs in all-optical soliton pulse compressors based on cascading quadratic nonlinearities, such as self-phase modulation (SPM) followed by cross-phase modulation (XPM). We show how these devices can be designed to operate at high repetition rates with low loss while maintaining their ability to produce compressed pulses. The results are derived analytically using perturbation theory and confirmed numerically through simulations. Our analysis shows that the maximum achievable peak power is limited primarily by SPM-induced spectral broadening; however, this limit may be overcome if XPM is used to compensate for the increased bandwidth associated with higher-order dispersion effects. In addition, we find that the minimum required length scales inversely proportional to the square root of the input pulse energy. Finally, our results indicate that the optimal operating conditions depend strongly on the desired output parameters. \n \n © 2009 Optical Society",
        "watermark_text": "We present scaling principles that govern the performance and engineering trade - offs in all - optical soliton pulse compressors based on cascading quadratic nonlinearities , such as self - phase modulation ( SPM ) preceded by inter - phase modulation ( XPM ) . We see how these machines can be designed to run at high repetition rates with little loss while maintaining their ability to produce compressed signals .The results are derived analytically using perturbation theory and reported numerically through simulations . Our study shows that the maximum achievable peak power is limited primarily by SPM - caused spectral broadening ; however , this limit might be overcome if XPM is utilized to compensate for the increased frequencies associated with higher - order dispersion patterns .In addition , we find that the minimum needed duration scales inversely proportional to the square root of the input pulse power . Finally , our findings show that the ideal operating circumstances rely highly on the desired output parameters .© 2009 Optical Society",
        "rewrite_text": "We introduce scaling principles that dictate the performance and engineering trade-offs in all-optical soliton pulse compressors utilizing cascading quadratic nonlinearities, such as self-phase modulation (SPM) followed by cross-phase modulation (XPM). Our analysis demonstrates how these systems can be optimized for high repetition rates with minimal loss while still effectively generating compressed signals. The results are derived analytically using perturbation theory and validated through numerical simulations. Our findings indicate that the maximum achievable peak power is primarily restricted by spectral broadening due to SPM; however, this limitation can potentially be addressed by employing XPM to counteract the heightened frequencies linked to higher-order dispersion patterns. Furthermore, we establish that the minimum required pulse duration scales inversely with the square root of the input pulse power. Ultimately, our results highlight that the optimal operating conditions are highly dependent on the desired output specifications. © 2009 Optical Society",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": 1.162476387438193
    },
    {
        "original_text": "The magnetic reconnection is one of the most important processes for understanding many phenomena observed in space and laboratory plasmas, such as solar flares, magnetospheric substorms, sawtooth crashes in tokamaks etc.. In this work we present an analytical model which describes the process of magnetic reconnection in collisionless high energy plasma with arbitrary initial conditions. The main idea behind our approach consists in using the concept of generalized force density tensor (GDFT) introduced by MHD theory. We show that GDFT can be used not only to describe the macroscopic dynamics but also microscopic properties of the system like particle distribution functions. Our results are compared with those obtained within other approaches based on kinetic description of particles motion. It turns out that all these models give similar predictions when applied to simple cases where the initial state has no gradients along the direction perpendicular to the background magnetic field. However, if there exist some gradients across the magnetic field lines then different models predict quite different behavior.",
        "watermark_text": "The magnetic reconnection is one of the most important processes for studying many phenomena observed in space and lab plasmas , such as sun flares , magnetospheric substorms , sawtooth crashes in tokamaks etc . . In this research we present an analytical theory which explains the process of magnetic reconnection in collisionless high energy gas with arbitrary initial conditions .The main idea behind our approach consists in utilizing the notion of generalized force density tensor ( GDFT ) developed by MHD physics . We see that GDFT can be used not only to explain the macroscopic behavior but also microscopic characteristics of the process like particle distribution functions .Our results are compared with those achieved within other methods based on kinetic representation of particles movement . It turns out that all these models make comparable predictions when applied to simple instances where the first state has no gradients along the direction perpendicular to the background magnetic force .However , if there exist some gradients across the magnetic force lines then various models predict quite different properties .",
        "rewrite_text": "Magnetic reconnection is a crucial process for understanding various phenomena observed in both space and laboratory plasmas, including solar flares, magnetospheric substorms, and sawtooth crashes in tokamaks. In this study, we introduce an analytical theory that elucidates magnetic reconnection in collisionless high-energy gases with arbitrary initial conditions. Our approach primarily relies on the concept of the generalized force density tensor (GDFT), a framework established by magnetohydrodynamics (MHD). We demonstrate that GDFT not only accounts for the macroscopic behavior but also captures microscopic characteristics, such as particle distribution functions. Our findings are contrasted with results obtained through other methods that utilize kinetic representations of particle dynamics. We discover that while these models yield comparable predictions in simplified scenarios where the initial state lacks gradients perpendicular to the background magnetic field, they can provide significantly different outcomes when gradients exist across the magnetic field lines.",
        "ori-fast-z-score": -0.7777777777777778,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We report on kinetic-ion simulations addressing whether ion trapping inflates stimulated Brillouin backscattering (SBS) reflectivities in the presence of an electron beam and plasma waves. We find that, for typical parameters relevant to high-power laser-plasma experiments, SBS is dominated by electrostatic Langmuir wave instabilities rather than ion-acoustic modes. The latter are suppressed due to Landau damping as well as mode conversion into electromagnetic radiation at oblique angles with respect to the direction of propagation. In addition, we show that the effect of ion trapping can be neglected if the density fluctuations associated with the trapped ions are small compared to those caused by the electrons. Finally, we demonstrate that the inclusion of ion trapping does not significantly affect the growth rates or saturation levels of the dominant electrostatic Langmuir waves. This finding suggests that the observed discrepancies between theory predictions and experimental results may originate from other effects such as nonlocality and/or nonlinear coupling among different types of waves.",
        "watermark_text": "We report on kinetic - ion simulations addressing whether electron capture inflates stimulated Brillouin backscattering ( SBS ) reflectivities in the presence of an electron beam and plasma beams . We see that , for typical values appropriate to large - speed laser - plasma experiments , SBS is dominated by electrostatic Langmuir beam instabilities rather than ion - sound modes .The latter are suppressed due to Landau damping as well as mode conversion into electromagnetic radiation at oblique angles with regard to the direction of propagation . In addition , we prove that the impact of ion traps can be forgotten if the density fluctuations associated with the captured atoms are small relative to those generated by the electrons .Finally , we prove that the introduction of ion traps does not dramatically impact the development rates or saturation levels of the dominant electrostatic Langmuir waves . This conclusion suggests that the reported discrepancies between theoretical estimates and theoretical results may originate from other effects such as nonlocality and / or nonlinear coupling among different kinds of waves .",
        "rewrite_text": "We present our findings from kinetic-ion simulations that explore whether electron capture enhances stimulated Brillouin backscattering (SBS) reflectivities in the presence of electron and plasma beams. Our results indicate that, for typical conditions relevant to high-speed laser-plasma experiments, SBS is primarily influenced by electrostatic Langmuir beam instabilities rather than ion-sound modes. The latter are diminished due to Landau damping and mode conversion into electromagnetic radiation at angles oblique to the propagation direction. Furthermore, we demonstrate that the effects of ion traps can be disregarded if the density fluctuations from the captured atoms are small compared to those caused by the electrons. Lastly, we find that incorporating ion traps has little effect on the growth rates or saturation levels of the key electrostatic Langmuir waves. This implies that the observed discrepancies between theoretical predictions and experimental results may stem from other factors, such as nonlocal effects and nonlinear interactions among various wave types.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.0201176116964925,
        "rewrite-fast-z-score": 2.424871130596428
    },
    {
        "original_text": "We present an approach for quantifying how much evolutionary history is likely to be lost if we lose particular species, and use this information to prioritize conservation efforts. We show that by considering both taxonomic and phylogenetic diversity simultaneously in conservation planning, it may be possible to conserve more biodiversity than would otherwise be achieved with either measure alone. \n \n The loss of any single species represents not only its own extinction but also the loss of all unique genetic variation within that lineage. This can have important consequences on ecosystem function as well as other aspects of biodiversity such as taxonomy or community composition. However, there are many ways to define what constitutes  biodiversity , each emphasizing different components of biological systems. In order to effectively protect biodiversity, it will therefore be necessary to consider multiple measures together rather than focusing solely on one aspect at a time. \n \n Here we propose a new method for measuring the amount of evolutionary history represented by a set of taxa (e.g., species) based on their relationships inferred using molecular data. Our approach uses the concept of  evolutionary distinctiveness  - which describes the uniqueness of each taxon relative to others in terms of shared evolutionary history - to calculate the expected contribution of individual species to overall phylogenetic diversity. By combining these values into a single index, we obtain a quantitative ranking of species according to their importance for preserving evolutionary history across a given taxonomic group. Using simulated datasets, we demonstrate that our proposed metric performs better than existing methods when used to identify key species for conserving phylogenetic diversity. Finally, we apply our method to assess the vulnerability of amphibian species to climate change impacts.",
        "watermark_text": "We present an perspective for quantifying how many evolutionary history is probably to be lost if we lose particular populations , and use this data to prioritize protection strategies . We suggest that by examining both taxonomic and evolutionary diversity simultaneously in conservation plan , it could be possible to conserve more biodiversity than would normally be achieved with either measure alone .The losing of any single species represents not only its own extinction but also the losing of all unique genetic variation within that lineage . This can have important implications on ecosystem function as well as other parts of ecosystems such as taxonomy or community structure .However , there are many ways to define what creates biodiversity , each emphasizing different components of biological functions . In order to effectively protect biodiversity , it will consequently be required to consider multiple measures together rather than focusing solely on one element at a time .Here we investigate a new method for calculation the extent of evolutionary history displayed by a setting of taxa ( e . g . , species ) based on their connections inferred using molecular data . Our concept employs the notion of evolutionary distinctiveness - which explains the uniqueness of each taxon relative to others in terms of shared evolutionary history - to estimate the expected impact of individual species to overall evolutionary diversity .By combining these values into a single index , we obtain a empirical ranking of taxa according to their importance for preserving phylogenetic evolution across a given taxonomic group . Using simulated datasets , we prove that our proposed measure works better than existing techniques when utilized to identify key taxa for conserving evolutionary evolution .Finally , we apply our technique to analyze the vulnerability of amphibian species to climate impact effects .",
        "rewrite_text": "We propose a new approach for quantifying the potential loss of evolutionary history when specific populations are lost, using this information to inform protection strategies. By analyzing both taxonomic and evolutionary diversity in conservation planning, we believe it is possible to enhance biodiversity conservation beyond what could be achieved by considering each metric independently. The extinction of any single species not only results in the loss of that species itself but also eliminates all unique genetic variation within its lineage, which can significantly affect ecosystem functions and other ecological components, such as taxonomy and community structure. However, biodiversity can be defined in various ways, each highlighting different aspects of biological function. To effectively protect biodiversity, it is essential to consider multiple factors together rather than concentrating on just one. In this study, we explore a novel method for quantifying the evolutionary history represented by a group of taxa (e.g., species) based on their molecular connections. Our approach utilizes the concept of evolutionary distinctiveness, which assesses each taxon's uniqueness in terms of shared evolutionary history, to evaluate the expected contribution of individual species to overall evolutionary diversity. By integrating these values into a single index, we create an empirical ranking of taxa based on their significance in preserving phylogenetic diversity within a given group. Through simulations, we demonstrate that our proposed method outperforms existing techniques in identifying key taxa for conserving evolutionary diversity. Finally, we apply our methodology to assess the vulnerability of amphibian species to the effects of climate change.",
        "ori-fast-z-score": -0.40961596025952024,
        "water-fast-z-score": 8.21926343358871,
        "rewrite-fast-z-score": 0.8638684255813601
    },
    {
        "original_text": "We report on the observation of giant magnetoresistance (GMR) effect in Ni80Fe20/Nb/Ni80Fe20 magnetic tunnel junctions with an ultrathin Nb spacer layer, which is as thin as 1 nm and 2 nm respectively. The GMR ratio can reach up to ~80% at room temperature for both samples. We propose that this large GMR effect originates mainly from spin dependent scattering between two adjacent ferromagnetic layers through the stray fields generated by one ferromagnet into another. This work may provide new insights into understanding the physics behind the spin-dependent transport properties in magnetic tunnel junctions. \n \n Magnetic tunnel junction (MTJ), consisting of two ferromagnets separated by a very thin insulating barrier, has been widely studied due to its potential applications in high density nonvolatile memories  1  . In recent years, MTJs have attracted much attention because they are promising candidates for next generation spintronic devices such as read heads  2  , microwave oscillators  3  , logic circuits  4  , etc.. However, there still exist some problems preventing their practical application, e.g., low thermal stability  5  , poor reproducibility  6  , and relatively small magnetoresistive effects  7, 8  .\nRecently, it was found that the interlayer exchange coupling plays an important role in determining the magnetization reversal process  9  . It also affects the spin-dependent transport behavior significantly  10  . Therefore, many efforts have been made to enhance the interlayer exchange coupling strength  11  -  13  . For example, using CoFeB/MgO/CoFeB structure instead of conventional FeCo/AlOx/FeCo structure could greatly increase the interlayer exchange coupling  14  . Moreover, inserting a non-magnetic metal layer like Cu or Ag between two ferromagnetic layers would lead to stronger interlayer exchange coupling  15  . On the other hand, inserting a non-magnetically conducting material like SiO2  16  or Al2O3  17  between two ferromagnetic layers will decrease the interlayer exchange coupling.",
        "watermark_text": "We report on the observation of giant magnetoresistance ( GMR ) effect in Ni80Fe20 / Nb / Ni80Fe20 magnetic tunnel junctions with an ultrathin Nb spacer coating , which is as thin as 1 nm and 2 nm respectively . The GMR ratio can reach up to ~ 80 % at room temperature for both samples .We suggest that this big GMR impact originates mainly from spinning dependent scattering between two adjacent ferromagnetic layers through the stray fields generated by one ferromagnet into another . This research could give novel knowledge into studying the physics behind the spin - dependent transport properties in magnetic tunnel junctions .Magnetic tunnel junction ( MTJ ) , consisting of two ferromagnets connected by a very thin insulating barrier , has been widely explored thanks to its potential applications in high density nonvolatile memories 1 . In recent seasons , MTJs have garnered many scrutiny because they are promising candidates for next generation spintronic systems such as read heads 2 , infrared oscillators 3 , logic devices 4 , etc . .However , there still exist some problems preventing their practical use , e . g . , low heat strength 5 , poor reproducibility 6 , and fairly little magnetoresistive factors 7 , 8 . Recently , it was shown that the interlayer exchange coupling plays an important role in determining the magnetization reversal process 9 .It additionally impacts the spin - based transport behavior dramatically 10 . Therefore , various efforts have been performed to alter the interlayer exchange bonding strength 11 - 13 .For instance , using CoFeB / MgO / CoFeB structure instead of standard FeCo / AlOx / FeCo structure could greatly increase the interlayer exchange bonding 14 . Moreover , inserting a non - magnetic metal layer like Cu or Ag between two ferromagnetic layers would result to heavier interlayer exchange bonding 15 .On the other hand , inserting a non - magnetically conducting substance like SiO2 16 or Al2O3 17 between two ferromagnetic layers will decrease the interlayer exchange bonding .",
        "rewrite_text": "We present our findings on the giant magnetoresistance (GMR) effect observed in Ni80Fe20/Nb/Ni80Fe20 magnetic tunnel junctions featuring ultra-thin Nb spacers, measuring just 1 nm and 2 nm in thickness, respectively. The GMR ratio for both samples can reach approximately 80% at room temperature. We propose that this significant GMR phenomenon primarily arises from spin-dependent scattering between the two adjacent ferromagnetic layers, mediated by the stray fields produced by one ferromagnet affecting the other. This study provides valuable insights into the spin-dependent transport properties within magnetic tunnel junctions (MTJs). MTJs, which consist of two ferromagnetic layers separated by an extremely thin insulating barrier, have been extensively researched due to their potential applications in high-density non-volatile memories. In recent years, there has been increased interest in MTJs as they are promising candidates for next-generation spintronic applications, including read heads, infrared oscillators, and logic devices. However, challenges such as low thermal stability, poor reproducibility, and relatively low magnetoresistive effects continue to hinder their practical implementation. Recent studies have highlighted the critical role of interlayer exchange coupling in the magnetization reversal process, significantly influencing spin-based transport behavior. As a result, various strategies have been explored to modify the strength of interlayer exchange coupling. For example, substituting a CoFeB/MgO/CoFeB structure for the conventional FeCo/AlOx/FeCo structure can enhance interlayer exchange coupling. Additionally, the insertion of non-magnetic metal layers, such as Cu or Ag, between ferromagnetic layers can lead to stronger interlayer exchange coupling. Conversely, introducing non-magnetic insulating materials like SiO2 or Al2O3 between the ferromagnetic layers tends to weaken interlayer exchange coupling.",
        "ori-fast-z-score": 0.0842151921066519,
        "water-fast-z-score": 6.1477090237855885,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "We study the nonlinear perturbations of general relativity (GR) and other metric theories of gravity, focusing on their effects on conserved quantities such as energy-momentum tensors. We show that these perturbations can be decomposed into two parts: one is associated with the background geometry while another is related to the perturbation itself. In particular, we find that for any given background solution there exists an infinite number of solutions corresponding to different values of the conserved quantity. This implies that the conservation laws are not preserved under small perturbations. Furthermore, we demonstrate how this effect may lead to violations of the weak equivalence principle. Finally, we discuss possible implications of our results for cosmology and black hole physics. General relativity (GR), which describes gravitational interactions at large scales, has been tested extensively against observations over many decades  1  . However, it remains unclear whether or not GR also holds true at smaller length scales where quantum mechanics becomes important  2  .\nIn order to address this question, several alternative theories have been proposed  3  , including scalar-tensor theories  4  , f(R)-gravity  5  , massive gravity  6  , and so forth  7, 8  . These theories typically involve additional degrees of freedom beyond those present in GR  9  . For example, in scalar-tensor theories, the graviton acquires a mass through its coupling to a scalar field  10  . Similarly, in f(R)-theories  11  , the Einstein-Hilbert action contains higher-order curvature terms  12  . It turns out that both types of theories admit self-accelerating solutions  13  , i.e., de Sitter-like solutions without requiring dark energy  14  .",
        "watermark_text": "We research the nonlinear perturbations of general relativity ( GR ) and other metric explanations of gravitational , concentrating on their impacts on conserved parameters such as energy - momentum tensors . We see that these perturbations can be decomposed into two parts : one is associated with the background geometry while another is related to the perturbation itself .In particular , we find that for any given background solution there exists an endless number of solutions associated to different values of the conserved quantity . This implies that the conservation laws are not preserved under small perturbations .Furthermore , we explain how this effect could lead to infringement of the weak equivalence principle . Finally , we explain possible possibilities of our findings for cosmology and dark hole physics .General relativity ( GR ) , which explains gravitational interactions at large scales , has been tested extensively against measurements over numerous centuries 1 . However , it remains unsure whether or not GR still holds true at greater size scales where quantum mechanics becomes crucial 2 .In order to meet this question , various alternative theories have been proposed 3 , notably scalar - vector models 4 , f ( R ) - gravity 5 , giant gravity 6 , and so forth 7 , 8 . These explanations typically involve additional degrees of autonomy beyond those present in GR 9 .For instance , in scalar - vector theories , the graviton acquires a mass through its interaction to a scalar field 10 . Similarly , in g ( R ) - fields 11 , the Einstein - Hilbert action contains upper - order curvature terms 12 .It turns out that both types of theories admit self - accelerating problems 13 , i . e . , de Sitter - like solutions without using dark energy 14 .",
        "rewrite_text": "We investigate the nonlinear perturbations in general relativity (GR) and other metric theories of gravity, focusing on their effects on conserved quantities such as energy-momentum tensors. Our findings reveal that these perturbations can be divided into two components: one related to the underlying geometry and another connected to the perturbations themselves. Notably, we demonstrate that for any given background solution, there is an infinite array of solutions corresponding to various values of the conserved quantity. This suggests that conservation laws may not hold under small perturbations. Additionally, we discuss how this phenomenon might challenge the weak equivalence principle. Lastly, we explore the implications of our results for cosmology and theories regarding dark matter. General relativity, which describes gravitational interactions on large scales, has been rigorously tested against empirical evidence over the centuries. However, questions remain about its validity on even larger scales where quantum mechanics plays a significant role. To address this issue, various alternative theories have emerged, including scalar-vector models, f(R) gravity, and modifications to gravity. These theories often introduce additional degrees of freedom not found in GR. For example, scalar-vector theories suggest that the graviton gains mass through interaction with a scalar field, while f(R) gravity incorporates higher-order curvature terms in the Einstein-Hilbert action. Interestingly, both frameworks can produce self-accelerating solutions, akin to de Sitter solutions, without relying on dark energy.",
        "ori-fast-z-score": -1.4485719366802965,
        "water-fast-z-score": 6.454545454545454,
        "rewrite-fast-z-score": -0.18569533817705186
    },
    {
        "original_text": "The article presents an analysis of Mykyta s network, which is one of the largest Russian-language social media communities in Ukraine. The author describes how this community was formed; its main characteristics are also analyzed. It has been found that the majority of users who create content on Mykyta belong to the age group 18-24 years old (more than 80%). In addition, it has been established that most members of the community have higher education (over 50%), live in large cities with more than 100 thousand inhabitants (about 70%) and work as specialists or managers (about 60%).\nIt should be noted that the study does not cover all aspects of the functioning of the community under consideration. For example, there were no attempts made to analyze the relationship between the structure of the community and the quality of user-generated content. This issue will require further research. Keywords: Social media, online communication, sociolinguistics",
        "watermark_text": "The essay presents an assessment of Mykyta s network , which is one of the largest Russian - language social media communities in Ukraine . The author explains how this community was formed ; its primary characteristics are also analyzed .It has been shown that the majority of viewers who generate text on Mykyta come to the age band 18 - 24 days old ( more than 80 % ) . In addition , it has been noted that most citizens of the public have higher education ( over 50 % ) , live in large cities with more than 100 thousand inhabitants ( about 70 % ) and work as experts or executives ( about 60 % ) .It should be mentioned that the study does not include all aspects of the functioning of the neighborhood under consideration . For instance , there were no attempts made to analyze the relationship between the composition of the neighborhood and the quality of customer - produced content .This problem will demand further studies . Keywords : Social media , internet communication , sociolinguistics",
        "rewrite_text": "The essay evaluates Mykyta's network, one of the largest Russian-language social media communities in Ukraine. The author outlines the community’s formation and analyzes its key characteristics. The findings reveal that over 80% of the users generating content on Mykyta fall within the 18-24 age range. Additionally, more than 50% of the participants hold higher education degrees, approximately 70% reside in cities with populations exceeding 100,000, and about 60% are employed as specialists or in managerial roles. It is important to note that the study does not capture all dimensions of the community's operation; for instance, it does not investigate the correlation between the community's demographic makeup and the quality of user-generated content. This issue warrants further research. Keywords: Social media, Internet communication, sociolinguistics.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": -0.39735970711951313
    },
    {
        "original_text": "We present the results of an analysis of magnetic field evolution, temperature variation, and plasma flow velocity observed by Hinode/SOT/SP (Solar Optical Telescope Spectro-Polarimeter) on September 24-25, 2007 during solar minimum period. The active region NOAA 10930 was located at S19E09 when it produced two flares with GOES class M5.7 and M1.0 respectively. We found that there were significant changes in the photospheric magnetic fields before and after these flares. In particular, we detected a new flux emergence event which occurred about one hour prior to the first flare. This newly emerged flux led to the formation of a coronal hole overlying the active region. After the second flare, the polarity inversion line moved toward the center of the active region where the strongest magnetic shear existed. During this time interval, we also found a rapid decrease in the intensity of the Fe xxi emission lines near the footpoints of the loops connecting opposite-polarities. These observations suggest that the energy released by the flares may have caused heating of the loop-top regions as well as evaporation of chromospheric material into the corona along the reconnected loops.",
        "watermark_text": "We present the conclusion of an assessment of magnetic field evolution , temperature variation , and plasma circulation velocity observed by Hinode / SOT / SP ( Solar Optical Telescope Spectro - Polarimeter ) on September 24 - 25 , 2007 during solar low time . The active region NOAA 10930 was positioned at S19E09 when it produced two flares with GOES class M5 . 7 and M1 . 0 respectively .We determined that there were substantial alterations in the photospheric magnetic fields before and after these flares . In particular , we identified a new flux emergence phenomenon which occurred about one hour prior to the first flare .This newly emerged flux led to the formation of a coronal hole overlying the active region . After the second flare , the polarity inversion line moved toward the center of the active region where the greatest magnetic shear existed .During this time interval , we also discovered a rapid decrease in the frequency of the Fe xxi emission lines near the footpoints of the loops connecting opposite - polarities . These measurements suggest that the electricity emitted by the flares might have caused heating of the loop - top regions as also as evaporation of chromospheric matter into the corona along the reconnected loops .",
        "rewrite_text": "We present the findings of an analysis concerning the evolution of magnetic fields, temperature fluctuations, and plasma circulation velocity as observed by the Hinode Solar Optical Telescope Spectro-Polarimeter (SOT/SP) on September 24-25, 2007, during a period of solar lull. The active region NOAA 10930 was located at S19E09 while it produced two flares classified by GOES as M5.7 and M1.0, respectively. Our investigation revealed significant changes in the photospheric magnetic fields both before and after these flares. Notably, we observed a phenomenon of new flux emergence that occurred approximately one hour prior to the initial flare, resulting in the formation of a coronal hole above the active region. Following the second flare, the polarity inversion line shifted toward the center of the active region, where the highest magnetic shear was found. During this time, we also noted a swift reduction in the frequency of Fe XXI emission lines near the footpoints of the loops connecting opposite polarities. These observations imply that the energy released by the flares may have contributed to the heating of the loop-top regions and facilitated the evaporation of chromospheric material into the corona along the reconnected loops.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": -0.11396057645963795
    },
    {
        "original_text": "We present optical (BVRI), near infrared (JHK) and mid-infrared (IRAC 3.6-8.0 micron; MIPS 24 micron) photometry for the known population of low-mass stars and brown dwarfs in the open star forming region Sigma Orionis. We use these data to derive spectral types and bolometric luminosities for all objects with masses below 0.1 solar masses. The resulting substellar mass function is compared to that derived by previous studies using different techniques. Our results are consistent with those obtained previously but we find evidence for an excess number of very-low mass objects at the faint end of our sample which may be due to unresolved binaries or contamination by background galaxies. This work was supported by NASA grant NAG5-12942. We thank J. Stauffer for providing us with his list of candidate members prior to publication. \n \n Keywords: Open clusters",
        "watermark_text": "We use visual ( BVRI ) , near infrared ( JHK ) and mid - infrared ( IRAC 3 . 6 - 8 . 0 micron ; MIPS 24 micron ) photometry for the known community of lowest - weight stars and dark dwarfs in the open star producing zone Sigma Orionis . We use these information to derive spectral classes and bolometric luminosities for all bodies with masses below 0 . 1 solar masses .The resulting substellar mass distribution is compared to that derived by earlier surveys using different methods . Our results are compatible with those generated previously but we find proof for an excess amount of very - low weight objects at the faint ending of our sample which may be due to unresolved binaries or exposure by background galaxies .This project was supported by NASA loan NAG5 - 12942 . We praise J . Stauffer for providing us with his list of candidate participants previous to publication .Keywords : Open clusters",
        "rewrite_text": "We employ photometry in visual (BVRI), near-infrared (JHK), and mid-infrared (IRAC at 3.6 - 8.0 microns; MIPS at 24 microns) to analyze the known population of the lightest stars and brown dwarfs within the star-forming region of Sigma Orionis. This data allows us to determine the spectral classes and bolometric luminosities of all objects with masses under 0.1 solar masses. We compare our derived substellar mass distribution to those obtained from earlier surveys using alternative methods. Our findings align with previous results; however, we identify an excess of very low-mass objects at the dim end of our sample, which may result from unresolved binaries or contamination by background galaxies. This project was funded by NASA under loan NAG5-12942, and we would like to thank J. Stauffer for providing his list of candidate participants prior to publication. Keywords: Open clusters.",
        "ori-fast-z-score": -1.75,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "We present deep optical and near-infrared observations of two gamma-ray burst (GRB) host galaxies, which are located behind the Large Magellanic cloud (LMC). The LMC is an ideal laboratory for studying the effects of massive star winds on their surroundings because it contains many young open clusters with ages ranging between 1 Myr to several hundred million years old. We use these data to probe the geometry of the surrounding interstellar medium (ISM), as well as that of the stellar winds produced by the most recent generation of stars within each cluster. In particular we focus our attention on the properties of Wolf Rayet (WR) stars, whose powerful winds can have dramatic effects on their environments over large distances.  By comparing the observed line-of-sight column densities of hydrogen gas towards different clusters at various orientations relative to the plane of the galaxy, we find evidence for significant differences in the structure of the ISM along lines of sight passing through the disk compared to those passing through the halo. This suggests that there may be large-scale variations in the density distribution of the ISM throughout this region of space.",
        "watermark_text": "We present dark optical and near - infrared observations of two gamma - ray burst ( GRB ) host galaxies , which are situated behind the Large Magellanic cluster ( LMC ) . The LMC is an suitable lab for studying the effects of large galaxy winds on their environment because it contains large old closed clusters with ages ranging between 1 Myr to several hundred million days old .We use these information to probe the morphology of the nearby interstellar medium ( ISM ) , as well as that of the stellar winds produced by the most current generation of stars within each cluster . In particular we focus our focus on the properties of Wolf Rayet ( WR ) stars , whose massive winds can have dramatic effects on their habitats over large distances .By comparing the reported line - of - view column densities of hydrogen gas towards different galaxies at several orientations compared to the plane of the galaxy , we find proof for significant variations in the composition of the ISM along tracks of view traveling through the disk compared to those traveling through the halo . This implies that there may be large - scale variations in the density patterns of the ISM throughout this area of space .",
        "rewrite_text": "We present observations in the dark optical and near-infrared wavelengths of two gamma-ray burst (GRB) host galaxies located behind the Large Magellanic Cloud (LMC). The LMC serves as an excellent laboratory for examining the impact of large galaxy winds on their surroundings, as it features numerous old, compact clusters with ages ranging from 1 million to several hundred million years. Our analysis utilizes this data to investigate the morphology of the nearby interstellar medium (ISM) and the stellar winds generated by the latest generation of stars within each cluster. We particularly emphasize the characteristics of Wolf-Rayet (WR) stars, whose powerful winds can significantly influence their environments over considerable distances. By comparing the observed line-of-sight hydrogen gas column densities across various galaxies at different orientations relative to the galactic plane, we discover substantial variations in the composition of the ISM along sightlines traversing the disk versus those through the halo. This suggests the presence of large-scale density variations in the ISM throughout this region of space.",
        "ori-fast-z-score": -1.5724272550828775,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": -0.7777777777777778
    },
    {
        "original_text": "We present new near-infrared (NIR) observations of the young stellar cluster IC 1396 N, located in the Orion Nebula Cluster region. The data were obtained with the adaptive optics system NAOS-CONICA on the VLT telescope and cover an area of 0.5 arcmin2 around the central star HD 37022. We detect more than 100 point sources down to Ks = 18 mag within this field-of-view. Using these data we have constructed colour-magnitude diagrams for different areas inside our field-of-view. From these CMDs we find that there are two populations of stars which can be separated by their position in the diagrams: one population is redder and fainter while another has bluer colours and brighter magnitudes. These results suggest that the first group consists mainly of low mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group contains mostly high mass main-sequence stars without any surrounding material.",
        "watermark_text": "We report new near - infrared ( NIR ) observations of the small stellar cluster IC 1396 N , located in the Orion Nebula Cluster region . The data were obtained with the adaptive optics network NAOS - CONICA on the VLT telescope and cover an area of 0 . 5 arcmin2 around the primary star HD 37022 .We detect more than 100 point sources down to Ks = 18 mag within this field - of - view . Using these information we have created colour - magnitude diagrams for different areas inside our field - of - view .From these CMDs we find that there are two communities of stars which can be apart by their placement in the diagrams : one community is redder and fainter while another has bluer colours and darker magnitudes . These data suggest that the first group contains primarily of high mass pre - principal sequence stars surrounded by circumstellar disks , whereas the second set contains primarily low mass primary - sequence stars without any surrounding media .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the small stellar cluster IC 1396 N, situated in the Orion Nebula Cluster region. This data was collected using the NAOS-CONICA adaptive optics system on the VLT telescope, covering an area of 0.5 arcmin² around the primary star HD 37022. Within this field of view, we identified over 100 point sources down to Ks = 18 mag. With this information, we constructed color-magnitude diagrams (CMDs) for various regions within our observed area. Our analysis of the CMDs reveals two distinct groups of stars characterized by their positions in the diagrams: one group appears redder and fainter, while the other boasts bluer colors and brighter magnitudes. These observations imply that the first group primarily consists of high-mass pre-main sequence stars surrounded by circumstellar disks, while the second group is mainly composed of low-mass main sequence stars with no surrounding material.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "The nonequilibrium steady states (NESS) of matrix product form are the focus of this work, which is intended to be useful for researchers in computational physics and chemistry who wish to solve problems with such NESSs using numerical methods.  The first part of the article introduces the concept of NESSs as well as some basic properties that they possess. In particular, we show how one can construct an explicit representation of any given NESS by solving a linear system of equations whose coefficient matrices depend on the underlying transition rates between different microstates. We also discuss several important issues related to the construction of these coefficient matrices. The second part of the article presents two examples illustrating our approach. Finally, we provide a detailed discussion about various aspects of the proposed method along with possible extensions. Nonequilibrium steady states (NESs), i.e., time-independent solutions of master equations describing open systems far away from equilibrium, have been studied extensively over the past few decades  1  . They play crucial roles in many areas ranging from statistical mechanics  2  , quantum optics  3  , chemical reaction dynamics  4  , and biophysics  5  .\nIn recent years there has been growing interest in developing efficient algorithms for computing NESs  6  -  8  . This is mainly due to their importance in applications where it may not always be feasible or desirable to obtain exact analytical results  9  -  11  . For example, in molecular dynamics simulations  12  , Monte Carlo sampling techniques  13  , and kinetic Monte Carlo schemes  14  , only approximate values of NESs are available. Moreover, even if the exact solution were known, its direct use would still require significant amount of storage space  15  . Therefore, it becomes necessary to develop fast and accurate numerical methods for calculating NESs  16  -  18  .\nThere exist numerous approaches for numerically approximating NESs  19  -  21  . Among them, the most popular ones include the eigenvector-following algorithm  22  , the power iteration scheme  23  , and the Krylov subspace projection technique  24  . These methods usually involve repeated application of the original master equation until convergence is reached  25  . However, since the number of...",
        "watermark_text": "The nonequilibrium steady states ( NESS ) of matrix product type are the subject of this study , which is intended to be used for researchers in computational physics and chemistry who desire to solve difficulties with such NESSs using numerical methods . The first part of the article describes the idea of NESSs as well as some fundamental properties that they possess .In particular , we explain how one can build an explicit representation of any given NESS by modeling a linear network of equations whose coefficient matrices depend on the underlying transition rates between various microstates . We especially consider many important matters related to the creation of these coefficient matrices .The second part of the article gives two examples illustrating our approach . Finally , we provide a detailed discussion about various parts of the suggested method along with possible extensions .Nonequilibrium steady states ( NESs ) , i . e . , time - based answers of master equations representing open systems close away from equilibrium , have been studied frequently over the previous few years 1 . They play crucial roles in different areas ranging from statistical mechanics 2 , quantum optics 3 , chemical process mechanics 4 , and biophysics 5 .In recent years there has been growing interest in building fast algorithms for processing NESs 6 - 8 . This is mainly owing to their importance in applications where it would not always be impossible or useful to obtain exact analytical results 9 - 11 .For instance , in polymer dynamics simulations 12 , Monte Carlo analysis methods 13 , and dynamic Monte Carlo schemes 14 , only approximate estimates of NESs are available . Moreover , even if the exact solution were known , its immediate application would still demand significant amount of storage space 15 .Therefore , it becomes necessary to develop fast and precise quantitative methods for calculating NESs 16 - 18 . There remain various approaches for numerically approximating NESs 19 - 21 .Among them , the most popular ones include the eigenvector - following procedure 22 , the power iteration scheme 23 , and the Krylov subspace projection procedure 24 . These methods usually include repeated application of the original master equation until convergence is reached 25 .However , since the quantity of . . .",
        "rewrite_text": "This study focuses on nonequilibrium steady states (NESS) of the matrix product type, aimed at researchers in computational physics and chemistry seeking to address challenges related to these NESS using numerical methods. The article begins by elucidating the concept of NESS and outlining some of their key properties. In particular, we demonstrate how to construct an explicit representation of any NESS by creating a linear network of equations, where the coefficient matrices hinge on the underlying transition rates between various microstates. We also address several critical issues associated with the development of these coefficient matrices. The second section provides two examples that illustrate our methodology. Lastly, we engage in a thorough discussion of different aspects of the proposed method and explore potential extensions. \n\nIn recent years, nonequilibrium steady states (NESS) — time-dependent solutions of master equations representing open systems that are far from equilibrium — have gained significant attention. They are essential across multiple domains, including statistical mechanics, quantum optics, chemical process dynamics, and biophysics. There has been an increased interest in developing fast algorithms for analyzing NESS, driven by their relevance in applications where obtaining exact analytical results is often impractical. For example, in simulations of polymer dynamics, Monte Carlo methods, and dynamic Monte Carlo schemes, only approximate estimates of NESS are generally attainable. Additionally, even when an exact solution is known, its application may require substantial storage resources, necessitating the development of efficient and accurate quantitative methods for NESS calculations. Numerous approaches exist for numerically approximating NESS, with popular techniques including the eigenvector-following method, power iteration scheme, and Krylov subspace projection technique. These methods typically involve iteratively applying the original master equation until convergence is achieved. However, the volume of...",
        "ori-fast-z-score": -0.8315218406202999,
        "water-fast-z-score": 8.390811300804845,
        "rewrite-fast-z-score": -0.722897396012249
    },
    {
        "original_text": "We present an approach to validating the performance of machine-learning algorithms for identifying modules in networks, based on synthetic datasets generated by simulating random walks through known modular structures. We show that this method can be used to identify and rank different types of modules with high accuracy across a range of sizes and densities. The results are robust against noise and missing links. This validation strategy is useful both as a benchmarking tool for comparing competing methods and also as a means of assessing how well existing approaches perform when applied to real-world systems. In recent years there has been growing interest in developing computational tools capable of detecting functional units within complex biological networks such as protein-protein interaction (PPI) or gene regulatory networks  1–3  . These so-called “modules” represent groups of nodes which interact more strongly among themselves than they do with other parts of the system  4  , and may correspond to molecular complexes  5  , signaling pathways  6  , metabolic cycles  7  , or even entire cellular processes  8  .\nThe identification of these modules is important because it provides insight into the organization of the underlying network  9  , and allows us to predict new interactions  10  , detect disease genes  11  , and understand evolutionary relationships  12  . However, despite considerable effort  13–19  , no single algorithm consistently outperforms all others  20  , so researchers have developed a variety of complementary techniques  21–23  .",
        "watermark_text": "We present an way to validating the performance of machine - computing methods for finding modules in networks , using on synthetic datasets generated by simulating random runs through known modular structures . We see that this algorithm can be used to identify and rank different kinds of modules with high sensitivity across a range of sizes and densities .The results are robust against noise and lost connections . This validation methodology is important both as a benchmarking tool for evaluating rival techniques and also as a means of assessing how well existing techniques perform when applied to real - time systems .In past decades there has been growing interest in establishing computational tools capable of detecting functional units within complex biological groups such as protein - gene interaction ( PPI ) or protein regulatory circuits 1 – 3 . These so - called “ modules ” denote groups of nodes which interact more strongly among themselves than they do with other parts of the system 4 , and may correspond to chemical complexes 5 , signaling pathways 6 , metabolic processes 7 , or possibly entire cellular processes 8 .The identity of these modules is important because it gives knowledge into the organization of the underlying network 9 , and allows us to predict new interactions 10 , predict disease genes 11 , and understand evolutionary relationships 12 . However , despite considerable work 13 – 19 , no single method consistently outperforms all others 20 , so researchers have developed a variety of complementary techniques 21 – 23 .",
        "rewrite_text": "We present a method for validating the performance of machine computing approaches aimed at identifying modules in networks. This method utilizes synthetic datasets created by simulating random paths through established modular structures. Our findings indicate that this algorithm effectively identifies and ranks various types of modules with high sensitivity, regardless of their sizes and densities. Moreover, the results demonstrate resilience against noise and incomplete connections. This validation technique is crucial, serving both as a benchmarking tool for assessing competing methods and as a way to evaluate the performance of existing techniques in real-time systems. Over the past few decades, there has been increasing interest in developing computational tools to detect functional units within complex biological networks, such as protein-gene interactions (PPI) and protein regulatory circuits. These “modules” refer to groups of nodes that interact more intensively with one another than with other components of the network, potentially corresponding to chemical complexes, signaling pathways, metabolic processes, or even entire cellular processes. Understanding these modules is essential, as it provides insights into the organization of the network and enables the prediction of new interactions, disease-related genes, and evolutionary relationships. However, despite significant advancements, no single method has been able to consistently outperform all others, leading researchers to create a range of complementary techniques.",
        "ori-fast-z-score": 0.8908708063747479,
        "water-fast-z-score": 8.139287438099235,
        "rewrite-fast-z-score": 2.1908902300206643
    },
    {
        "original_text": "We present new results on the faint-end slope and evolution of the luminosity function (LF) for optically-selected quasars in the redshift range 0.5 < z < 2.2, based on the VIMOS-VLT Deep Survey (VVDS). We use two different methods to estimate the LF parameters at each redshift bin: 1/Vmax method and maximum likelihood fitting technique. Our best-fit values are obtained by combining these two techniques with Monte Carlo simulations. We find that our data is consistent with previous studies within their uncertainties. However we show that there exists an apparent discrepancy between the observed number density of bright quasars and the predictions made using the standard quasar formation model. This may be due to incompleteness effects or biases introduced during the selection process. In addition, we also investigate the dependence of the LF shape on the optical luminosities of quasars.",
        "watermark_text": "We report new data on the faint - end slope and evolution of the luminosity function ( LF ) for optically - selected quasars in the redshift region 0 . 5 < z < 2 . 2 , using on the VIMOS - VLT Deep Survey ( VVDS ) . We use two different methods to estimate the LF variables at each redshift bin : 1 / Vmax method and greatest probability fitting technique .Our best - fitting values are derived by combining these two strategies with Monte Carlo simulations . We see that our information is compatible with previous research within their uncertainties .However we prove that there exists an apparent discrepancy between the expected number density of bright quasars and the assumptions produced using the standard quasar structure model . This might be due to incompleteness effects or biases created during the selection phase .In addition , we also investigate the relationship of the LF formation on the optical luminosities of quasars .",
        "rewrite_text": "We present new measurements of the faint-end slope and evolution of the luminosity function (LF) for optically-selected quasars in the redshift range of 0.5 < z < 2.2, utilizing data from the VIMOS-VLT Deep Survey (VVDS). To estimate the LF parameters for each redshift bin, we employ two methods: the 1/Vmax technique and greatest probability fitting. Our optimal fitting values are obtained by integrating these methods with Monte Carlo simulations. Our findings show consistency with prior studies when accounting for uncertainties. However, we identify a notable discrepancy between the predicted number density of bright quasars and the expectations derived from the standard quasar structure model. This discrepancy may arise from incompleteness issues or selection biases. Additionally, we explore the relationship between the LF formation and the optical luminosities of quasars.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 6.713171133426189,
        "rewrite-fast-z-score": 1.2362450755382013
    },
    {
        "original_text": "We present the results of our analysis of a sample of 25 high-resolution quasar absorption line systems with metallicities ranging between 1/100 and 1/10 solar, selected to have low dust content (i.e., < 0.1 mag extinction at 2200 A). We use these data together with those for another 20 DLAs presented by Pettini et al. (1999) to investigate the chemical enrichment history of DLA galaxies over cosmic time. The main conclusions are as follows: \nThe abundance patterns observed in this sample can be explained if we assume that most of the metals were produced during an early burst of star formation which occurred less than 10 Gyr ago. \n\n\nThis is consistent with previous studies based on smaller samples but it also shows that there may not always be evidence for recent star formation activity even when such activity has been inferred from other indicators. \n\nIn addition, we find no correlation between metallicity and dust content or neutral hydrogen column density.\n\nFinally, we show that the mean value of  Fe/H  measured in DLAs agrees well with the predictions made using simple models of galactic chemical evolution.",
        "watermark_text": "We publish the conclusion of our analysis of a sample of 25 high - resolution quasar absorbed line systems with metallicities ranging between 1 / 100 and 1 / 10 solar , selected to have lowest dust content ( i . e . , < 0 . 1 mag extinction at 2200 A ) . We use these results together with those for another 20 DLAs given by Pettini et al .( 1999 ) to examine the chemical enrichment history of DLA galaxies over cosmic time . The main results are as follows : The concentration trends experienced in this specimen can be described if we suppose that most of the metals were produced during an early burst of galaxy formation which occurred less than 10 Gyr ago .This is consistent with previous analyses based on smaller specimens but it also shows that there may not always be confirmation for recent star formation activity even when such activity has been inferred from other indicators . In addition , we find no correlation between metallicity and dust content or neutral hydrogen column density .Finally , we prove that the mean value of Fe / H measured in DLAs agrees well with the assumptions done using simple theories of galactic material evolution .",
        "rewrite_text": "We present our findings from an analysis of a sample of 25 high-resolution quasar absorption line systems, which have metallicities ranging from 1/100 to 1/10 of solar metallicity, and were selected for having minimal dust content (i.e., less than 0.1 mag extinction at 2200 Å). These results, combined with data from another 20 damped Lyman-alpha (DLA) systems reported by Pettini et al. (1999), allow us to investigate the chemical enrichment history of DLA galaxies throughout cosmic time. The key findings are as follows: The concentration trends observed in our sample suggest that the majority of metals were formed during an early burst of galaxy formation that occurred less than 10 billion years ago. This aligns with previous studies based on smaller samples, although it indicates that recent star formation activity may not always be corroborated by other indicators. Additionally, we found no significant correlation between metallicity and either dust content or neutral hydrogen column density. Finally, our results show that the average Fe/H ratio measured in DLAs is in good agreement with predictions made using simplified theories of galactic material evolution.",
        "ori-fast-z-score": 1.078327732034384,
        "water-fast-z-score": 6.11104144857543,
        "rewrite-fast-z-score": 1.3779972440082682
    },
    {
        "original_text": "We present some new results on the monotonicity, stability and construction of central difference schemes for nonlinear systems of hyperbolic conservation laws with nonlocal source terms. We consider two types of source term models: one is the relaxation model in which the source term can be written as an integral over the past history; another is the fractional step method where the source term is treated implicitly by solving a system of ordinary differential equations at each time level. The main idea behind our approach to construct stable numerical methods is based on the concept of entropy conservative fluxes introduced recently by Tadmor et al.. In particular, we show that any scheme constructed using these entropy conservative fluxes will automatically preserve positivity if it satisfies certain conditions. Finally, several examples are presented to illustrate the theoretical results obtained here. Cite this article as: Zhi-Hong Liu, On Monotonicity, Stability, and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms, Journal of Computational Physics, Volume 227, Issue 1, 15 January 2008, Pages 1-33",
        "watermark_text": "We see some new results on the monotonicity , stability and formation of central variation schemes for nonlinear systems of hyperbolic conservation laws with nonlocal source terms . We consider two forms of source word models : one is the relaxation model in which the source word can be written as an integral over the present history ; another is the fractional step technique where the source word is treated implicitly by solving a system of simple differential coefficients at each time level .The main idea behind our approach to build stable numerical models is based on the notion of entropy conservative fluxes proposed lately by Tadmor et al . . In particular , we prove that any scheme built using these entropy liberal fluxes will automatically preserve positivity if it satisfies certain conditions .Finally , various instances are presented to illustrate the theoretical results derived here . Cite this page as : Zhi - Hong Liu , On Monotonicity , Stability , and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms , Journal of Computational Physics , Volume 227 , Issue 1 , 15 January 2008 , Pages 1 - 33",
        "rewrite_text": "We present new findings regarding the monotonicity, stability, and development of central variation schemes for nonlinear hyperbolic conservation laws with nonlocal source terms. Our study examines two types of source models: one is a relaxation model where the source term is expressed as an integral over current history, and the other uses a fractional step technique in which the source term is implicitly addressed by solving a system of simple differential equations at each time step. The core principle of our approach to creating stable numerical models is rooted in the concept of entropy conservative fluxes recently introduced by Tadmor et al. Notably, we demonstrate that any scheme utilizing these entropy liberal fluxes will inherently maintain positivity, provided certain conditions are met. Finally, we provide several examples to illustrate the theoretical results established in this work. Cite this page as: Zhi-Hong Liu, On Monotonicity, Stability, and Construction of Central Schemes for Hyperbolic Conservation Laws With Source Terms, Journal of Computational Physics, Volume 227, Issue 1, 15 January 2008, Pages 1-33.",
        "ori-fast-z-score": -1.6733200530681511,
        "water-fast-z-score": 3.679023140400945,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "We present new spectroscopic observations of galaxies at z ~ 1.5-2.0 selected by their UVJ colors and optical morphologies, obtained with VLT/VIMOS on the Very Large Telescope (VLT). We find that these objects are mostly early-type galaxies showing signs of recent star formation activity. The observed properties suggest that they may be progenitors of local massive elliptical galaxies. These results provide further evidence supporting the scenario where most massive galaxies grow through mergers between gas-rich disk systems during the first half of cosmic time. This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited. \n \n Keywords: galaxy evolution; merger remnants; young ellipticals; CDF-S field \n \n Massive galaxies evolve rapidly over cosmic time as a result of merging processes involving smaller companions. In particular, it has been suggested that many of today s brightest cluster galaxies were formed via major mergers of two or more gas-rich disks at redshifts around one to three  1  . However, direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift  2  .\n \nIn order to study the physical mechanisms driving galaxy growth we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph  3  . Our sample consists of about 100 galaxies selected based on their ultraviolet J (UVJ) color  4  , morphological type  5  , and apparent magnitude  6  . Most of them show strong emission lines characteristic of active star-forming regions  7, 8  . Their stellar masses range from 10^10 M_sol to 10^11 M_sol  9  . \n\n\nThe main goal of our project was to identify possible candidates for progenitor populations of local massive elliptical/S0 galaxies  10  . To do so, we used several selection criteria designed to select galaxies with similar characteristics to those found among nearby massive spheroids  11  : \n\n\n1. Morphological type: all targets must",
        "watermark_text": "We report new spectroscopic observations of clusters at z ~ 1 . 5 - 2 . 0 selected by their UVJ colors and optical morphologies , obtained with VLT / VIMOS on the Very Large Telescope ( VLT ) . We see that these objects are mostly early - class stars displaying signs of recent star formation activity .The observed properties suggest that they may be progenitors of local heavy elliptical galaxies . These data provide further evidence supporting the scenario where most large galaxies grow through mergers between gas - rich disk systems during the first half of cosmic time .This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2 . 0 , which allows unrestricted use , distribution , and reproduction in any medium provided the original book is properly cited . Keywords : galaxy evolution ; collision remnants ; young ellipticals ; CDF - S field Massive stars develop rapidly over cosmic time as a product of combining processes involving smaller companions .In particular , it has been proposed that several of today s brightest cluster objects were created via large mergers of two or more gas - rich disks at redshifts around one to three 1 . However , direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift 2 .In order to study the physical mechanisms governing galaxy formation we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT - VIMOS spectrograph 3 . Our specimen consists of about 100 galaxies chose based on their ultraviolet J ( UVJ ) color 4 , morphological class 5 , and apparent magnitude 6 .Most of them show intense emission lines typical of active star - creating areas 7 , 8 . Their stellar masses range from 10 ^ 10 M _ sol to 10 ^ 11 M _ sol 9 .The main goal of our work was to identify possible nominees for progenitor populations of local heavy elliptical / S0 galaxies 10 . To do so , we using numerous selection categories modified to select clusters with similar characteristics to those detected among neighboring massive spheroids 11 : 1 .Morphological type : all targets must",
        "rewrite_text": "We present new spectroscopic observations of clusters at redshifts around 1.5 to 2.0, selected based on their UVJ colors and optical morphologies, acquired using the VLT/VIMOS on the Very Large Telescope. Our findings indicate that these objects primarily consist of early-type stars exhibiting signs of recent star formation activity. The characteristics observed suggest they could be precursors to local massive elliptical galaxies. These results lend further support to the theory that the majority of large galaxies evolve through mergers of gas-rich disk systems during the early stages of cosmic history. This article is Open Access and distributed under the Creative Commons Attribution License 2.0, permitting unrestricted use, distribution, and reproduction in any medium, provided the original source is properly cited. Keywords include galaxy evolution, collision remnants, young ellipticals, and the CDF-S field.\n\nMassive stars evolve quickly over cosmic time through the merging of smaller companions. It has been suggested that many of the brightest cluster objects we observe today formed through significant mergers of two or more gas-rich disks at redshifts between one and three. However, direct observational evidence for such events has been challenging to obtain due to the difficulty of identifying these occurrences at high redshift. To investigate the physical mechanisms underlying galaxy formation, we conducted deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph. Our sample comprises approximately 100 galaxies selected based on their ultraviolet J (UVJ) color, morphological class, and apparent magnitude. Most of these galaxies display strong emission lines that are indicative of active star formation regions. Their stellar masses range from \\(10^{10} M_{\\odot}\\) to \\(10^{11} M_{\\odot}\\). The primary aim of our research was to pinpoint potential candidates for the progenitor populations of local heavy elliptical or S0 galaxies. To achieve this, we employed various selection criteria tailored to isolate clusters resembling those found among nearby massive spheroids:1. Morphological type: all targets must...",
        "ori-fast-z-score": 0.16116459280507606,
        "water-fast-z-score": 6.98800816145174,
        "rewrite-fast-z-score": 0.8835412617927487
    },
    {
        "original_text": "We study the phenomenological consequences of supersymmetric models with gauge-mediated breaking, in which the Standard Model is extended by adding new vector-like matter fields and extra dimensions. We show that these models can be constructed such that they are free of any unnatural fine-tuning problems associated with the Higgs mass or flavor-changing neutral currents. In particular we find that:  1) The lightest scalar superpartner (the  Higgs  boson) has a mass at most around 300 GeV.  2) Flavor changing neutral current effects are suppressed to an acceptable level for generic values of parameters.  3) Gauge coupling unification occurs naturally within experimental uncertainties. 4) There exists a large parameter space where all sparticles have masses above 1 TeV while still satisfying constraints on electroweak symmetry breaking. 5) These models provide a natural explanation for why there may not yet exist evidence for supersymmetry at accelerator experiments.",
        "watermark_text": "We research the phenomenological consequences of supersymmetric theories with gauge - mediated breaking , in which the Standard Model is extended by added new vector - like matter fields and extra dimensions . We see that these models can be built such that they are free of any strange fine - tuned flaws associated with the Higgs mass or flavor - changing neutral currents .In particular we find that : 1 ) The lightest scalar superpartner ( the Higgs boson ) has a mass at most approximately 300 GeV . 2 ) Flavor shifting neutral current effects are suppressed to an acceptable level for generic values of values .3 ) Gauge coupling unification happens easily within experimental uncertainties . 4 ) There exists a large parameter room where all sparticles have masses above 1 TeV while already satisfying constraints on electroweak symmetry breaking .5 ) These models represent a natural explanation for why there may not already exist evidence for supersymmetry at accelerator studies .",
        "rewrite_text": "We investigate the phenomenological implications of supersymmetric theories with gauge-mediated symmetry breaking, where the Standard Model is enhanced by the inclusion of new vector-like matter fields and extra dimensions. Our analysis demonstrates that these models can be constructed to avoid problematic fine-tuning issues related to the Higgs mass and flavor-changing neutral currents. Specifically, we uncover the following findings: 1) The lightest scalar superpartner, which is the Higgs boson, has a mass that is at most around 300 GeV. 2) Flavor-changing neutral current effects are effectively suppressed to acceptable levels across a wide range of values. 3) Gauge coupling unification occurs easily within the bounds of experimental uncertainties. 4) A significant parameter space exists where all superparticles have masses exceeding 1 TeV while still adhering to requirements for electroweak symmetry breaking. 5) These models provide a natural explanation for the lack of evidence for supersymmetry in current accelerator experiments.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": -1.0834726777719228
    },
    {
        "original_text": "We present new results on the incidence and properties of intervening absorbers along the sightline towards GRB 080913, based on high-resolution spectroscopy obtained with X-shooter at VLT-UT2 (ESO program ID 080.A-9007). We detect two strong absorption systems in the spectrum of this burst, one associated with an intervening galaxy at z = 1.5394 ± 0.0002, and another system at z = 2.084 ± 0.001 that is likely due to a damped Lyman alpha absorber. The latter has been previously detected by Fynbo et al. (2009) using low resolution spectra taken with FORS-2/VLT. Our analysis shows that both these systems are rich in metals, including Si II, Mg II, Fe II, Al III, O I, N V, and possibly also C IV. In addition we find evidence for several weaker metal lines which may be associated with either or both of these systems.",
        "watermark_text": "We report new data on the incidence and properties of intervening absorbers along the sightline towards GRB 080913 , using on wide - resolution spectroscopy acquired with X - shooter at VLT - UT2 ( ESO program ID 080 . A - 9007 ) . We detect two strong absorption complexes in the spectrum of this burst , one linked with an intervening galaxy at z = 1 . 5394 ± 0 . 0002 , and another system at z = 2 . 084 ± 0 . 001 that is probably due to a damped Lyman alpha absorber .The latter has been previously observed by Fynbo et al . ( 2009 ) using small resolution spectra made with FORS - 2 / VLT .Our study shows that both these systems are rich in metals , notably Si II , Mg II , Fe II , Al III , O I , N V , and maybe also C IV . In addition we find proof for numerous smaller metal bands which may be identified with either or both of these systems .",
        "rewrite_text": "We present new findings on the incidence and characteristics of intervening absorbers along the line of sight to GRB 080913, utilizing high-resolution spectroscopy obtained with X-shooter at VLT-UT2 (ESO program ID 080.A-9007). Our analysis reveals two prominent absorption complexes in the spectrum of this burst. The first is associated with an intervening galaxy at redshift z = 1.5394 ± 0.0002, while the second system, located at z = 2.084 ± 0.001, is likely a damped Lyman-alpha absorber. This latter system was previously identified by Fynbo et al. (2009) using lower resolution spectra from FORS-2/VLT. Our investigation indicates that both of these systems are metal-rich, featuring elements such as Si II, Mg II, Fe II, Al III, O I, and N V, and potentially C IV as well. Additionally, we provide evidence for several smaller metal absorption features that may correspond to either or both of these systems.",
        "ori-fast-z-score": -1.6641005886756874,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": -1.721892064184557
    },
    {
        "original_text": "The concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.",
        "watermark_text": "The concept of flaws in crystals has been originated by the Russian school since the 1930s . The main idea is that any solid can be regarded as an elastic continuum with some local deviations from its ideal structure which are called flaws .In this research we present a brief review on the history of the development of the principle of flaws in solids . We especially examine the newer concepts of point - like defects ( dislocations ) , edge - like defects ( disclinations ) and continuous defects .Finally , we give evidence of how these ideas have been used to different physical structures such as fluid crystals or magnetic materials . Defects serve an important role in many fields of science ranging from solid state mechanics to condensed matter science and even biology .They arise naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures . For instance , they may contribute to plastic deformations in metals or glassy materials .On the other hand , defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization .",
        "rewrite_text": "The concept of imperfections in crystals originated from the Russian school in the 1930s. The fundamental idea is that any solid can be viewed as an elastic continuum, featuring localized deviations from its ideal structure, referred to as flaws. In this research, we provide a concise overview of the historical development of the principle of flaws in solids. We particularly focus on recent advancements regarding point-like defects (dislocations), edge-like defects (disclinations), and continuous defects. Furthermore, we illustrate how these concepts have been applied to various physical structures, such as liquid crystals and magnetic materials. Defects play a crucial role across numerous scientific disciplines, including solid-state physics, condensed matter physics, and biology. They naturally emerge during phase transitions between ordered states, such as those that occur at melting points or critical temperatures. For example, they can influence plastic deformation in metals and glassy materials. Additionally, defects are responsible for the macroscopic properties of solids, including electrical conductivity and magnetization.",
        "ori-fast-z-score": 0.5555555555555556,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": 3.3048567173295003
    },
    {
        "original_text": "We study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times. \nI. INTRODUCTORY REMARkS\nThe purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green s function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref.  1  , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation  2  .",
        "watermark_text": "We explore the graviton propagator in covariant massive gravity physics with an arbitrary number of gravitons and find that it is given by the sum over all Feynman diagrams which are derived by attaching one or more gravitons to each vertex of the tree - level graviton propagator . We additionally offer explicit expressions for the first few terms in this expansion , notably the main order term corresponding to the usual Einstein - Hilbert action .The results presented here can be used as input into measurements involving higher - order corrections to gravitational processes such as black hole evaporation . In particular , we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times .I . INTRODUCTORY REMARkS The purpose of this project is twofold .First , we will extract the exact representation for the graviton propagator ( or Green s function ) in covariant massive gravity theories with an arbitrary number of external gravitons . Second , we will use our result to estimate the effects on the Hawking radiation emitted by a Schwarzschild red hole due to the presence of added degrees of liberty involved with the huge spin - 2 field .Our study continues carefully the approach developed in Ref . 1 , where the authors explored the impact of adding massless scalar fields to the standard theory of particle physics on the emission speed of Hawking radiation 2 .",
        "rewrite_text": "We investigate the graviton propagator in the context of covariant massive gravity with a variable number of gravitons, discovering that it can be represented as the sum of all Feynman diagrams formed by connecting one or more gravitons to each vertex of the tree-level graviton propagator. Furthermore, we provide explicit expressions for the initial terms in this series, particularly emphasizing the leading term associated with the conventional Einstein-Hilbert action. The findings presented here can serve as valuable input for analyzing higher-order corrections in gravitational phenomena, such as black hole evaporation. Specifically, we observe that incorporating these additional contributions results in alterations to the Hawking temperature at later times. \n\nIn our introduction, we outline two main objectives for this project. Firstly, we aim to derive the exact representation of the graviton propagator (or Green's function) in covariant massive gravity theories with any number of external gravitons. Secondly, we will leverage our findings to assess the impact of the extra degrees of freedom associated with the massive spin-2 field on the Hawking radiation emitted by a Schwarzschild black hole. Our research builds on the methodology established in Ref. 1, where the authors investigated how the introduction of massless scalar fields modifies the emission rate of Hawking radiation.",
        "ori-fast-z-score": 1.2247448713915892,
        "water-fast-z-score": 6.463663618136471,
        "rewrite-fast-z-score": 0.3144854510165755
    },
    {
        "original_text": "The reed is the key component in single-reed musical instruments, such as clarinets or saxophones. The dynamics of its motion are strongly influenced by the bore geometry and boundary conditions at both ends. In this work we present an approach to simulate the oscillations of a single reed instrument based on modal decomposition of the bore and reed dynamics. We show that it is possible to accurately reproduce the sound produced by a real clarinet using only one degree-of-freedom for each mode. This method can be used to study the influence of different parameters (such as mouthpiece diameter) on the acoustic response of the instrument without requiring expensive experimental measurements. It also allows us to investigate how changes in the design affect the performance of new prototypes. \n \n Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "watermark_text": "The reed is the key component in single - reed musical instruments , such as clarinets or saxophones . The structures of its motion are strongly dictated by the bore pattern and boundary pressures at both ends .In this study we present an approach to simulate the oscillations of a single reed instrument relying on modal decomposition of the bore and reed dynamics . We see that it is easy to correctly reproduce the music created by a real clarinet using only one degree - of - independence for each mode .This method can be used to study the impact of different variables ( such as mouthpiece diameter ) on the acoustic response of the instrument without using expensive experimental surveys . It additionally lets us to examine how differences in the model affect the performance of new prototypes .Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "rewrite_text": "The reed serves as the central element in single-reed musical instruments like clarinets and saxophones. Its motion is heavily influenced by the bore shape and the pressure at both ends. In this research, we introduce a method for simulating the oscillations of a single reed instrument through modal decomposition of the reed and bore dynamics. Our findings indicate that we can effectively replicate the sound produced by an actual clarinet using a single degree of independence for each mode. This technique allows us to investigate the effects of various factors, such as mouthpiece diameter, on the instrument's acoustic performance without the need for costly experimental studies. Moreover, it enables us to analyze how different models influence the behavior of new prototypes. \nKeywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method.",
        "ori-fast-z-score": -1.5756771943166705,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": -0.8017837257372732
    },
    {
        "original_text": "We present the results of an extensive spectroscopic survey for active galactic nuclei (AGN) in the southern hemisphere using the Sloan Digital Sky Survey (SDSS). The main goal is to select a complete flux-limited sample of radio-loud AGNs at redshifts z < 0.7, which we call ROXA (Radio Optical eXtragalactic Astronomy), by cross-correlating the FIRST 1.4 GHz radio source catalogue with the SDSS DR3 photometric database. We have obtained spectra for more than 1000 sources over an area of about 10,000 deg2. In this work we report on the selection criteria used to define our sample as well as its completeness and reliability. We also discuss some preliminary results concerning the properties of these objects such as their luminosity function and redshift distribution. This project has been funded by the European Space Agency under contract number 4000106131/16/NL/PA.",
        "watermark_text": "We present the results of an extensive spectroscopic study for active galactic nuclei ( AGN ) in the southern hemisphere using the Sloan Digital Sky Survey ( SDSS ) . The main goal is to select a complete flux - limited sample of radio - loud AGNs at redshifts z < 0 . 7 , which we call ROXA ( Radio Optical eXtragalactic Astronomy ) , by cross - correlating the FIRST 1 . 4 GHz radio source collection with the SDSS DR3 photometric archive .We have achieved spectra for more than 1000 sources over an area of about 10 , 000 deg2 . In this project we publish on the selection standards used to define our sample as well as its completeness and reliability .We also discuss some preliminary results concerning the properties of these objects such as their luminosity function and redshift distribution . This project has been sponsored by the European Space Agency under contract number 4000106131 / 16 / NL / PA .",
        "rewrite_text": "We report the findings of a comprehensive spectroscopic investigation of active galactic nuclei (AGN) located in the southern hemisphere, utilizing data from the Sloan Digital Sky Survey (SDSS). The primary objective was to construct a complete flux-limited sample of radio-loud AGNs with redshifts of z < 0.7, which we refer to as ROXA (Radio Optical eXtragalactic Astronomy). This was accomplished by cross-referencing the FIRST 1.4 GHz radio source catalog with the SDSS DR3 photometric archive. We successfully obtained spectra for over 1,000 sources across an area of approximately 10,000 square degrees. In this publication, we provide details on the selection criteria used to define our sample, alongside its completeness and reliability. Additionally, we present some preliminary findings regarding the characteristics of these AGNs, including their luminosity function and redshift distribution. This project has been funded by the European Space Agency under contract number 4000106131/16/NL/PA.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 3.5714285714285716,
        "rewrite-fast-z-score": 0.5252257314388902
    },
    {
        "original_text": "The Solar Chromosphere is an important component in our understanding of how the Sun works and its influence on Earth, but it has been difficult to study because of its tenuous nature.  ALMA (Atacama Large Millimeter/submillimeter Array) will be able to observe this region for the first time with unprecedented spatial resolution.   This talk will discuss some of the science that can be done using ALMA observations of the Solar Chromosphere. The Solar Chromosphere is one of the most enigmatic regions of the Sun. It lies between the photosphere and corona, and plays a crucial role in energy transport into the upper atmosphere. However, due to its extremely low density, direct observation of the chromosphere was not possible until recently when high-resolution images were obtained by space-based telescopes such as Hinode/SOT and SDO/AIA. In addition, ground-based observatories have also made significant progress towards studying the chromosphere through various techniques including spectropolarimetry, imaging spectroscopy, and speckle interferometry. Despite these advances, there are still many open questions about the physical processes occurring within the chromosphere which need to be addressed. For example, what causes the formation of dynamic structures like sunspots? How do magnetic fields affect plasma dynamics in the chromosphere? What is the relationship between chromospheric heating mechanisms and coronal mass ejections? These questions cannot be answered without detailed knowledge of the structure and dynamics of the chromosphere. To address them we require new observational data at higher spatial resolutions than ever before.",
        "watermark_text": "The Solar Chromosphere is an important element in our understanding of how the Sun operates and its influence on Earth , but it has been difficult to study because of its tenuous nature . ALMA ( Atacama Large Millimeter / submillimeter Array ) will be possible to observe this area for the first time with incredible spatial resolution .This discussion will explore some of the science that can be performed using ALMA observations of the Solar Chromosphere . The Solar Chromosphere is one of the most enigmatic regions of the Sun .It lies between the photosphere and corona , and plays a crucial role in heat transport into the higher atmosphere . However , owing to its incredibly small abundance , direct observation of the chromosphere was not could until recently when high - resolution images were obtained by space - based telescopes such as Hinode / SOT and SDO / AIA .In addition , land - based observatories have already produced important development towards studying the chromosphere through numerous techniques including spectropolarimetry , optical spectroscopy , and speckle interferometry . Despite these developments , there are still many open questions about the physical processes arising within the chromosphere which require to be addressed .For instance , what causes the formation of dynamic systems like sunspots ? How do magnetic waves affect plasma dynamics in the chromosphere ?What is the relationship between chromospheric heating systems and coronal mass ejections ? These questions cannot be answered without precise understanding of the composition and dynamics of the chromosphere .To address them we require novel observational data at higher spatial resolutions than ever before .",
        "rewrite_text": "The Solar Chromosphere is a crucial component in comprehending the Sun's mechanics and its effects on Earth, yet studying it has proven challenging due to its delicate nature. The Atacama Large Millimeter/submillimeter Array (ALMA) will allow us to observe this region for the first time with exceptional spatial resolution. This discussion will delve into the scientific possibilities offered by ALMA's observations of the Solar Chromosphere, a particularly mysterious zone situated between the photosphere and the corona, which is instrumental in transferring heat to the upper atmosphere. Until recently, direct observations of the chromosphere were limited due to its remarkably low abundance, but high-resolution images from space-based telescopes like Hinode/SOT and SDO/AIA have changed that. Moreover, ground-based observatories have made significant strides in studying the chromosphere through various techniques such as spectropolarimetry, optical spectroscopy, and speckle interferometry. Nonetheless, many questions remain regarding the physical processes within the chromosphere. For example, what drives the formation of dynamic structures like sunspots? How do magnetic waves influence plasma dynamics in this region? What is the connection between chromospheric heating mechanisms and coronal mass ejections? Answering these questions requires a precise understanding of the chromosphere's composition and dynamics, which can only be achieved through innovative observational data with higher spatial resolution than we’ve had before.",
        "ori-fast-z-score": 1.4,
        "water-fast-z-score": 7.53778361444409,
        "rewrite-fast-z-score": 0.6324555320336759
    },
    {
        "original_text": "We propose an algorithm to find optimal hierarchical descriptors (OHDs) that can be used as fingerprints in complex networks, such as social and biological systems. The OHDs represent network structures at different levels of granularity by using a tree-like structure with nodes representing subnetworks or clusters. We show how these trees can be constructed efficiently based on modularity maximization algorithms. In addition, we introduce a novel concept called  clustering coefficient  which is useful when constructing OHDs. Finally, we demonstrate our method through several examples including synthetic benchmark data sets and real-world networks. Our results suggest that the proposed approach provides more accurate descriptions than existing methods. This work was supported by JSPS KAKENHI Grant Number JP26287040. Keywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description",
        "watermark_text": "We suggest an algorithm to find optimal hierarchical descriptors ( OHDs ) that can be used as fingerprints in complex networks , such as social and biological environments . The OHDs represent system structures at different levels of granularity by using a tree - like structure with vertices representing subnetworks or communities .We see how these trees can be built easily using on modularity maximization algorithms . In addition , we provide a new notion called clustering coefficient which is important when constructing OHDs .Finally , we prove our technique through several examples namely natural benchmark data sets and actual - global networks . Our results show that the suggested approach offers more accurate descriptions than existing techniques .This project was supported by JSPS KAKENHI Grant Number JP26287040 . Keywords : Hierarchical descriptor , Clustering coefficient , Modularity Maximization Algorithm , Tree Structure , Fingerprint , Network Description",
        "rewrite_text": "We propose an algorithm for identifying optimal hierarchical descriptors (OHDs) that can serve as fingerprints for complex networks, including those found in social and biological contexts. The OHDs depict the structural characteristics of systems at varying levels of detail, utilizing a tree-like framework where the vertices correspond to subnetworks or communities. We demonstrate that these trees can be constructed easily using modularity maximization algorithms. Furthermore, we introduce a novel concept termed the clustering coefficient, which is crucial in the formation of OHDs. We validate our approach through multiple examples, including natural benchmark datasets and real-world global networks. Our findings indicate that the proposed method provides more precise descriptions compared to existing techniques. This research was funded by the JSPS KAKENHI Grant Number JP26287040. Keywords: Hierarchical descriptor, Clustering coefficient, Modularity Maximization Algorithm, Tree Structure, Fingerprint, Network Description.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.37925663806037,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "The physical characteristics, spectral energy distributions (SEDs), and atmospheric properties of late-type dwarfs are reviewed in this chapter.  The SEDs for these objects have been measured by many authors using ground-based telescopes as well as space observatories such as ISO, Spitzer Space Telescope, and AKARI.  These observations show that the infrared flux increases with decreasing effective temperature.  In addition to their red colors, they also exhibit strong water vapor absorption bands at wavelengths longer than 2 microns.   Their near-infrared spectra reveal features due to methane gas which is abundant in their atmospheres.  They also display prominent atomic lines of potassium and sodium in their optical spectra.  Finally, we discuss how the observed photometric data can be used to determine the fundamental parameters of these objects including mass, radius, luminosity, surface gravity, age, metallicity, and distance.   ... This chapter reviews the physical characteristics, spectral energy distribution (SED) measurements, and atmospheric properties of late type dwarfs.",
        "watermark_text": "The physical qualities , spectral power distributions ( SEDs ) , and atmospheric characteristics of late - class dwarfs are reviewed in this page . The SEDs for these objects have been measured by many scientists using ground - based telescopes as well as space observatories such as ISO , Spitzer Space Telescope , and AKARI .These measurements show that the infrared flux changes with decreasing effective heat . In addition to their red colors , they even exhibit strong water vapor absorption groups at wavelengths greater than 2 microns .Their near - infrared spectra demonstrate characteristics owing to methane gas which is abundant in their atmospheres . They even exhibit prominent atomic patterns of potassium and sodium in their infrared spectra .Finally , we explain how the observed photometric data can be used to determine the fundamental variables of these objects including mass , diameter , luminosity , surface gravity , age , metallicity , and distance . . . .This section reviews the physical qualities , spectral power distribution ( SED ) observations , and atmospheric characteristics of late type dwarfs .",
        "rewrite_text": "This section provides an overview of the physical properties, spectral energy distributions (SEDs), and atmospheric features of late-type dwarfs. Numerous researchers have measured the SEDs of these objects using both ground-based telescopes and space observatories, including ISO, the Spitzer Space Telescope, and AKARI. These measurements indicate that the infrared flux decreases with lower effective temperatures. Alongside their reddish appearance, these dwarfs show significant water vapor absorption bands at wavelengths exceeding 2 microns. Their near-infrared spectra reveal features attributed to the abundance of methane in their atmospheres, as well as notable atomic signatures of potassium and sodium. Furthermore, we discuss how the photometric data collected can be utilized to ascertain the fundamental parameters of these objects, such as mass, diameter, luminosity, surface gravity, age, metallicity, and distance.",
        "ori-fast-z-score": 0.22360679774997896,
        "water-fast-z-score": 6.555555555555555,
        "rewrite-fast-z-score": -0.25
    },
    {
        "original_text": "We study isospin-breaking effects on the production rate for heavy-light mesons (D, D*) and light-heavy mesons (D0, D0bar). We use an effective field theory approach to calculate these rates at leading order in perturbation theory. The results are compared with experimental data obtained by CLEO-c. \n \n Isospin symmetry plays an important role in hadronic physics. It relates states that differ only in their charge but have identical masses. In particular it implies that the strong decay widths of charged and neutral pions should be equal. However, this equality has been experimentally tested down to pion momenta as low as 1 MeV/c and deviations up to 20% were found  1  . These deviations can be explained within Chiral Perturbation Theory  2  , which predicts corrections proportional to powers of the momentum transfer between initial and final state particles. At higher energies, where the typical momentum transfers become larger than the chiral scale, one expects such corrections to vanish rapidly  3  .\n \nIn contrast, we consider here processes involving two heavy quarks close to threshold. Here, the typical momentum transfers are small enough so that non-perturbative contributions cannot be neglected anymore. As a consequence, even though the mass difference between charm and anti-charm quarks is tiny, there will still be significant differences between the corresponding cross sections  4  . \n \n This effect was first observed more than 20 years ago  5  when studying the production of charmed mesons in electron-positron collisions. Since then many experiments  6  -  8  have measured the ratio of the production rates for different combinations of heavy-meson pairs. While some of them find good agreement with theoretical predictions  9  based on Heavy Quark Effective Theory  10  , others disagree significantly  11  .",
        "watermark_text": "We research isospin - breaking effects on the production frequency for heavy - heavy mesons ( D , D * ) and light - heavy mesons ( D0 , D0bar ) . We use an efficient field model approach to estimate these rates at leading order in perturbation theory .The results are compared with experimental evidence derived by CLEO - c . Isospin symmetry serves an important role in hadronic physics . It relates states that differ only in their charge but have equal masses .In particular it assumes that the strong decay widths of charged and neutral pions should be equal . However , this equality has been experimentally tested down to pion momenta as low as 1 MeV / c and deviations up to 20 % were found 1 .These deviations can be described within Chiral Perturbation Theory 2 , which predicts corrections proportional to powers of the velocity transfer between initial and final state particles . At higher energies , where the typical velocity transfers become bigger than the chiral scale , one expects such corrections to vanish swiftly 3 .In comparison , we treat here reactions involving two heavy quarks close to threshold . Here , the typical velocity transfers are small enough so that non - perturbative contributions never be forgotten anymore .As a consequence , even though the mass gap between charm and pro - charm quarks is tiny , there will still be considerable changes between the associated cross sections 4 . This phenomenon was first observed more than 20 centuries earlier 5 when examining the production of charmed mesons in electron - positron collisions .Since then many tests 6 - 8 have tested the proportion of the production rates for different combinations of heavy - meson pairs . While some of them find good agreement with theoretical estimates 9 based on Heavy Quark Effective Theory 10 , others disagree significantly 11 .",
        "rewrite_text": "We investigate isospin-breaking effects on the production rates of heavy-heavy mesons (D, D*) and light-heavy mesons (D0, D0bar). To estimate these rates at leading order in perturbation theory, we employ an efficient field model approach. Our findings are compared with experimental data obtained from CLEO-c. Isospin symmetry plays a crucial role in hadronic physics, as it relates states that differ only in charge but have the same mass. Notably, it posits that the strong decay widths of charged and neutral pions should be equal. However, experimental tests at pion momenta as low as 1 MeV/c have revealed deviations of up to 20%. These discrepancies can be accounted for within Chiral Perturbation Theory, which predicts corrections proportional to powers of the velocity transfer between initial and final state particles. At higher energies, where typical velocity transfers exceed the chiral scale, these corrections are expected to diminish rapidly. In contrast, our study focuses on reactions involving two heavy quarks near threshold, where typical velocity transfers remain sufficiently small that non-perturbative contributions cannot be ignored. Consequently, despite the minimal mass gap between charm and anti-charm quarks, significant variations in the associated cross sections can still arise. This phenomenon was first observed over 20 years ago in studies of charmed meson production in electron-positron collisions. Since then, numerous tests have evaluated the ratios of production rates for different combinations of heavy meson pairs. While some studies align well with theoretical predictions based on Heavy Quark Effective Theory, others show considerable discrepancies.",
        "ori-fast-z-score": -0.08873565094161139,
        "water-fast-z-score": 6.529318494299385,
        "rewrite-fast-z-score": 2.658425641381813
    },
    {
        "original_text": "We present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Lupus molecular clouds. The data were obtained as part of the Spitzer Space Telescope s  Cores to Disks  Legacy program. We have identified more than 1000 infrared point sources associated with these clouds using our source extraction technique. These include protostars, young stellar objects, and background galaxies. In addition we find that there are many extended emission features which may be related to outflows or other phenomena associated with star formation. A comparison between the observed number counts at 24 microns and those predicted based on models of interstellar dust suggests that most of the detected sources are likely to be low mass stars surrounded by disks. This is consistent with previous studies of this region. However, it appears that some fraction of the brightest sources could also be high-mass protostars.",
        "watermark_text": "We present the conclusion of measurements made by the Multiband Imaging Photometer for Spitzer ( MIPS ) in 24 and 70 micron bands toward the Lupus molecular clouds . The data were obtained as part of the Spitzer Space Telescope s Cores to Disks Legacy project .We have discovered more than 1000 infrared spot sources involved with these clouds using our source mining technique . These include protostars , young stellar bodies , and background galaxies .In addition we find that there are many extended emitted features which may be connected to outflows or other processes associated with star formation . A comparison between the seen amount totals at 24 microns and those predicted based on models of interstellar dust suggests that most of the emitted sources are likely to be low weight stars surrounded by disks .This is consistent with previous research of this area . However , it appears that some fraction of the brightest observers might actually be large - mass protostars .",
        "rewrite_text": "We present the findings from measurements taken by the Multiband Imaging Photometer for Spitzer (MIPS) in the 24 and 70 micron bands, focusing on the Lupus molecular clouds. This data was collected as part of the Spitzer Space Telescope's Cores to Disks Legacy project. Using our source extraction technique, we have identified over 1,000 infrared point sources associated with these clouds, which include protostars, young stellar objects, and background galaxies. Additionally, we observe numerous extended emission features that may be linked to outflows or other star formation processes. A comparison of the observed quantities at 24 microns with those predicted by interstellar dust models indicates that most of the emitted sources are likely low-mass stars surrounded by disks. This finding aligns with previous studies in this field. However, it seems that a subset of the brightest sources could correspond to high-mass protostars.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": -1.25
    },
    {
        "original_text": "We present new results on the asteroseismic analysis of the primary component in the binary system Alpha Centari, based on data obtained with the HARPS spectrograph at La Silla Observatory (Chile). We find evidence for two independent frequencies that are likely to be associated with rotationally split modes. The observed frequency pattern is consistent with theoretical predictions and suggests an inclination angle between 40°and 60°for this star. \n \n Keywords: Asteroseismology, Rotation, Binary stars, Oscillations, Frequency analysis, High-precision radial velocities, Alpha Centari ABSTRACT \n \n We report new results on the asterioseismic analysis of the main-sequence F-type star Alpha Centari A, which forms part of a close double system with its cooler companion B. Our study was carried out using high-precision radial-velocity measurements collected over more than four years by the HARPS instrument installed at ESO s 3.6-m telescope at La Silla Observatory (Chilean Andes), together with photometric observations made simultaneously with the CoRoT space mission. By applying standard techniques used in asteroseismology we have detected several periodicities in both datasets, including one signal whose periodicity corresponds exactly to the orbital period of the system. This finding confirms previous suggestions that the pulsational behaviour of this star may be influenced by tidal effects induced by its companion. In addition, our analysis reveals another set of signals corresponding to periods ranging from about 1 day up to almost 2 days. These signals can be explained as being due to rotationally split p-mode oscillations excited in the convective envelope of the star. Their presence provides strong support for the hypothesis that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo processes operating within the convection zone.",
        "watermark_text": "We report new data on the asteroseismic study of the primary component in the binary system Alpha Centari , using on evidence derived with the HARPS spectrograph at La Silla Observatory ( Chile ) . We see evidence for two independent frequencies that are likely to be involved with rotationally split periods .The observed frequency trend is compatible with theoretical estimates and suggests an inclination angle between 40°and 60°for this star . Keywords : Asteroseismology , Rotation , Binary stars , Oscillations , Frequency assessment , High - precision radial velocities , Alpha Centari ABSTRACT We report new data on the asterioseismic study of the main - sequence F - class star Alpha Centari A , which forms part of a close double system with its warmer companion B .Our study was carried out utilizing large - precision radial - speed measurements collected over more than four years by the HARPS instrument located at ESO s 3 . 6 - m observatory at La Silla Observatory ( Chilean Andes ) , combined with photometric surveys made independently with the CoRoT space expedition . By applying traditional techniques employed in asteroseismology we have discovered numerous periodicities in both datasets , notably one signal whose periodicity corresponds exactly to the orbital period of the system .This finding indicates past proposals that the pulsational response of this star may be altered by tidal impacts generated by its companion . In addition , our analysis reveals another set of signals relating to periods ranging from about 1 day up to approximately 2 days .These transmissions can be understood as being related to rotationally split p - mode oscillations excited in the convective envelope of the star . Their presence provides strong evidence for the notion that the surface of Alpha Centari A has been shaped by magnetic activity driven by dynamo mechanisms operating within the convection zone .",
        "rewrite_text": "We present new findings from the asteroseismic study of the primary star in the Alpha Centauri binary system, based on data obtained with the HARPS spectrograph at La Silla Observatory in Chile. Our analysis indicates the presence of two independent frequencies, likely associated with rotationally split modes. The observed frequency trend aligns with theoretical predictions, suggesting an inclination angle between 40° and 60° for this star. \n\n**Keywords:** Asteroseismology, Rotation, Binary Stars, Oscillations, Frequency Assessment, High-Precision Radial Velocities, Alpha Centauri\n\n**ABSTRACT:** We report new results from our asteroseismic investigation of Alpha Centauri A, a main-sequence F-type star that forms a close binary system with its hotter companion, Alpha Centauri B. Our study utilized high-precision radial velocity measurements gathered over four years using the HARPS instrument at the ESO 3.6-m telescope at La Silla Observatory in the Chilean Andes, along with independent photometric data from the CoRoT space mission. By employing standard asteroseismic techniques, we identified multiple periodic signals within both datasets, particularly highlighting one frequency that corresponds exactly to the system's orbital period. This finding supports earlier hypotheses suggesting that the star's pulsational behavior may be influenced by tidal interactions with its companion. Furthermore, our analysis uncovered another set of signals with periods ranging from about one to two days, which are interpreted as rotationally split p-mode oscillations driven by processes in the star's convective envelope. This evidence strongly supports the idea that the surface characteristics of Alpha Centauri A have been influenced by magnetic activity resulting from dynamo mechanisms within the convection zone.",
        "ori-fast-z-score": -0.9838699100999074,
        "water-fast-z-score": 6.887089370699352,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "The risk assessment algorithms based on recursive neural networks are proposed in this paper, which can be used to assess the risks for different types of financial assets and make decisions accordingly. The algorithm is composed by three parts: input data preprocessing, network training and output results analysis. In order to improve the accuracy of prediction, we use genetic algorithm (GA) to optimize the parameters of RNNs. Finally, an example is given to show how our method works. Keywords: Risk assessment; Financial asset; Genetic algorithm; Recurrent neural networks; Optimization. 1 Introduction With the rapid development of information technology, more and more people have access to online trading platforms such as Alibaba Group s Taobao Marketplace and Tencent Holdings  WeChat Pay. As a result, there has been growing interest among researchers in developing intelligent systems that can help investors make better investment decisions  1  . However, it remains challenging to develop accurate models due to the complexity of real-world problems  2  .\nIn recent years, artificial intelligence techniques have attracted increasing attention because they provide powerful tools for solving complex problems  3  , especially recurrent neural networks (RNN). Compared with traditional feed-forward neural networks  4  , RNNs have advantages over time series forecasting  5  -  8  . For instance, RNNs can learn long-term dependencies between inputs and outputs  9  . Therefore, RNNs are widely applied in many fields including stock market prediction  10  -  12  , traffic flow prediction  13  , energy consumption prediction  14  , etc..",
        "watermark_text": "The danger assessment methods developed on recursive neural systems are proposed in this paper , which can be used to examine the dangers for different kinds of financial investments and making decisions accordingly . The algorithm is composed by three components : input data preprocessing , network testing and input results analysis .In order to improve the accuracy of prediction , we utilize genetic algorithm ( GA ) to optimize the variables of RNNs . Finally , an instance is given to indicate how our technique works .Keywords : Risk evaluation ; Financial investment ; Genetic algorithm ; Recurrent neural connections ; Optimization . 1 Introduction With the fast development of electronic tech , more and more persons have access to online trading systems such as Alibaba Group s Taobao Marketplace and Tencent Holdings WeChat Pay .As a result , there has been growing interest among researchers in building smart systems that can help investors make better investment decisions 1 . However , it remains challenging to develop precise models due to the complexity of real - global difficulties 2 .In recent years , artificial intelligence techniques have garnered additional attention because they enable powerful tools for solving complex problems 3 , particularly recurrent neural systems ( RNN ) . Compared with typical feed - forward neural systems 4 , RNNs have benefits over time group forecasting 5 - 8 .For instance , RNNs can know long - term dependencies between inputs and outputs 9 . Therefore , RNNs are widely applied in multiple fields including stock price forecast 10 - 12 , transport stream forecast 13 , electricity demand prediction 14 , etc . .",
        "rewrite_text": "This paper proposes danger assessment methods based on recursive neural systems, which can be utilized to evaluate risks associated with various types of financial investments and facilitate informed decision-making. The algorithm consists of three key components: preprocessing of input data, network testing, and analysis of input results. To enhance prediction accuracy, we employ a genetic algorithm (GA) to optimize the variables within the recurrent neural networks (RNNs). We also provide an example to demonstrate the effectiveness of our approach. \n\nKeywords: Risk assessment, financial investment, genetic algorithm, recurrent neural networks, optimization.\n\n1. Introduction\n\nWith the rapid advancement of electronic technology, an increasing number of individuals have access to online trading platforms, such as Alibaba Group's Taobao Marketplace and Tencent Holdings' WeChat Pay. Consequently, there is a rising interest among researchers in developing intelligent systems to assist investors in making more informed investment decisions. However, crafting precise models poses challenges due to the complexities of real-world global issues. In recent years, artificial intelligence techniques have gained significant attention as they offer powerful solutions for tackling complex problems, particularly through the use of recurrent neural networks (RNNs). Compared to traditional feedforward neural networks, RNNs provide advantages in time series forecasting, as they are capable of capturing long-term dependencies between inputs and outputs. This makes RNNs widely applicable across various domains, including stock price forecasting, traffic flow prediction, and electricity demand forecasting.",
        "ori-fast-z-score": -0.8908708063747479,
        "water-fast-z-score": 7.902633289178096,
        "rewrite-fast-z-score": -1.1445861782233109
    },
    {
        "original_text": "We study the dynamics of an interface driven by a periodic external force in one dimension, using molecular dynamics simulations with Lennard-Jones particles at low temperatures (T = 0.1 − 1). We find that the interface moves as a whole when it is driven slowly enough; otherwise, it breaks into several segments which move independently. The number of segments increases with increasing driving frequency or decreasing temperature. In addition to these two regimes, we observe another regime where the interface exhibits stick-slip motion. This third regime occurs for intermediate values of the driving amplitude A and driving frequency f . For this regime, we show that there exists a scaling relation between the average velocity V , the driving frequency f , and the driving amplitude A: V ∝ Af 2 . Finally, we propose a simple model based on the concept of phonon-enhanced diffusion to explain our results. DOI: 10.1103/PhysRevE.77.020101",
        "watermark_text": "We explore the dynamics of an interface pushed by a periodic external force in one dimension , using polymer mechanics simulations with Lennard - Jones grains at low temperatures ( T = 0 . 1 − 1 ) . We see that the interface moves as a whole when it is driven slowly sufficiently ; otherwise , it splits into numerous segments which move independently .The amount of segments varies with varying drove frequency or decreasing temperature . In addition to these two regimes , we study another regime where the interface displays stick - slipping motion .This third regime appears for intermediate values of the driving intensity A and driving frequency f . For this regime , we find that there exists a scaling relation between the average momentum V , the driving frequency f , and the driving intensity A : V [UNK] Af 2 .Finally , we attempt a simple model based on the idea of phonon - augmented diffusion to explain our findings . DOI : 10 . 1103 / PhysRevE . 77 . 020101",
        "rewrite_text": "We investigate the behavior of an interface influenced by a periodic external force in one dimension through polymer mechanics simulations involving Lennard-Jones grains at low temperatures (T = 0.1 - 1). Our findings reveal that the interface moves collectively when driven at a sufficiently slow rate; however, when the driving speed is too fast, it fragments into multiple segments that move independently. The number of these segments varies with changes in driving frequency or decreasing temperature. Additionally, we examine a third regime characterized by stick-slip motion, which emerges at intermediate values of the driving intensity A and frequency f. In this regime, we uncover a scaling relationship linking the average momentum V, driving frequency f, and driving intensity A: V ∝ Af². Finally, we propose a straightforward model based on phonon-augmented diffusion to account for our observations. DOI: 10.1103/PhysRevE.77.020101",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.588314677411235,
        "rewrite-fast-z-score": 1.62746694241347
    },
    {
        "original_text": "We present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates. \n \n Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization \n \n \n \n 1 Introduction \n \n Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars. \n \n To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS)  1  , BLS+  2  , TrES  3  , TAP  4  , EXOTRANS  5  . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed  6  . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data  7  8  9  . \n \n In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data  10  ",
        "watermark_text": "We introduce TRUFAS ( Transit Recognition Using Fast Approximate Signatures ) , an efficient and strong transit search pipeline that using fast exact signatures to identify continuous patterns in time series information . The method is especially good suitable for detecting short length transits with high signal - to - noise ratio .We showed its effectiveness on simulated light surfaces derived by the Exoplanet Transit Database as also as realistic Kepler light surfaces . Our results show that TRUFAS can attain better productivity than other methods while maintaining low false negative rates .Keywords : Transiting planet , Wavelets , Time - series analysis , False positives reduction , Planetary network characterization 1 Introduction Planets are detected indirectly through their gravitational impacts upon their host stars . These effects include changes in stellar radius or luminosity caused by the travel of stars across the line - of - view between the star and Earth .This phenomenon is known as a transit event . In order to characterize exoplanet systems it is required to identify these changes easily and correctly .However , this job has been turned more challenging due to the huge amount of false positives created by widespread sound sources such as instrumental artifacts and astrophysical processes like eclipsing binaries and pulsating stars . To date there have been numerous algorithms created exclusively for identifying transit - like phenomena within astronomical date sequence data .Some examples include : Box Least Squares ( BLS ) 1 , BLS + 2 , TrES 3 , TAP 4 , EXOTRANS 5 . While each of these procedures works relatively well under certain conditions they all suffer from one common drawback ; they demand significant computational resources when looking for multiple transit alternatives simultaneously .For instance , the most commonly used technique , Box Least Squares , requires O ( N3 ) operations where N is the length of the time series being evaluated 6 . As a result , many of these procedures cannot be applied directly to recent and future surveys which will generate immense sums of data 7 8 9 .In recent years wavelet transforms have developed relatively popular for studying astronomical time cycle information 10",
        "rewrite_text": "We present TRUFAS (Transit Recognition Using Fast Approximate Signatures), a robust and efficient transit search pipeline that utilizes fast exact signatures to detect continuous patterns in time series data. This method excels at identifying short-duration transits with a high signal-to-noise ratio. We have demonstrated its effectiveness using simulated light curves from the Exoplanet Transit Database as well as actual Kepler light curves. Our findings indicate that TRUFAS outperforms other techniques while maintaining a low rate of false negatives. \n\n**Keywords:** Transiting planet, Wavelets, Time-series analysis, False positive reduction, Planetary network characterization \n\n**1 Introduction** \n\nPlanets are often detected indirectly through their gravitational influences on host stars, resulting in observable changes in the stars' radius or luminosity when a planet transits in front of them. Identifying these transit events is crucial for characterizing exoplanet systems. However, this task has become increasingly difficult due to a high volume of false positives arising from various noise sources, including instrumental artifacts and astrophysical phenomena such as eclipsing binaries and pulsating stars. \n\nNumerous algorithms have been developed specifically for detecting transit-like signals in astronomical time series data, such as Box Least Squares (BLS), BLS+, TrES, TAP, and EXOTRANS. While each algorithm operates effectively under certain conditions, they share a common limitation: they require substantial computational resources, particularly when searching for multiple transit candidates simultaneously. For example, the widely-used Box Least Squares method necessitates O(N³) operations, where N represents the length of the time series analyzed. Consequently, many of these algorithms cannot be directly applied to data from recent and upcoming surveys that will produce massive datasets. In recent years, wavelet transforms have gained popularity for analyzing astronomical time series data.",
        "ori-fast-z-score": -2.5298221281347035,
        "water-fast-z-score": 7.171804696775816,
        "rewrite-fast-z-score": -1.9369494184529936
    },
    {
        "original_text": "We present the results of an analysis of the supersymmetric standard model with minimal supergravity boundary conditions at the grand unification scale, including all one-loop corrections to gauge and Yukawa couplings as well as two-loop contributions to the running of the soft supersymmetry breaking parameters.  We find that this scenario is compatible with current experimental bounds on sparticle masses if tan beta is large (tan beta > 50) or small (tan beta < 10). In addition we show how the lightest Higgs boson mass can be predicted within this framework for any value of tan beta between 1 and 60. Finally, we discuss the implications of our results for future searches for supersymmetry at colliders such as LHC. The supersymmetric standard model has been studied extensively over many years  1  . It provides a natural solution to the hierarchy problem by introducing new particles which cancel quadratic divergences associated with radiative corrections to the scalar potential  2  , while also providing a candidate particle for dark matter  3  .\nIn recent years there have been several studies  4  -  8  investigating whether it is possible to construct models where the electroweak symmetry breaking sector is described by the MSSM  9  but the underlying physics is governed by some more fundamental theory valid at higher energies. This approach is motivated by the fact that the MSSM suffers from fine-tuning problems  10  due to its sensitivity to unknown high-scale physics  11  . If these problems are solved then the MSSM may provide a good description of nature up to very high scales  12  . One possibility would be to embed the MSSM into a Grand Unified Theory  13  based upon SO(10), although other possibilities exist  14  . Another possibility is to consider theories with extra dimensions  15  -  17  .",
        "watermark_text": "We present the conclusion of an assessment of the supersymmetric standard theory with minimal supergravity boundary constraints at the grand unified scale , comprising all one - loop corrections to gauge and Yukawa couplings as also as two - loop contributions to the running of the hard supersymmetry broken equations . We see that this situation is compatible with current experimental bounds on sparticle masses if tan beta is huge ( tan beta > 50 ) or small ( tan beta < 10 ) .In addition we prove how the lightest Higgs boson weight can be predicted within this framework for any value of tan beta between 1 and 60 . Finally , we explain the implications of our findings for future investigations for supersymmetry at colliders such as LHC .The supersymmetric standard description has been studied frequently over much years 1 . It provides a natural solution to the ranking problem by using new particles which cancel quadratic divergences associated with radiative corrections to the scalar potential 2 , while also offering a candidate particle for black material 3 .In recent years there have been numerous research 4 - 8 investigating whether it is easy to build theories where the electroweak symmetry breaking sector is modeled by the MSSM 9 but the underlying dynamics is governed by some more fundamental theory valid at higher energies . This method is prompted by the fact that the MSSM suffers from fine - tuned difficulty 10 due to its sensitivity to unknown long - scale physics 11 .If these problems are answered then the MSSM could give a better model of nature up to very high scales 12 . One possibility would be to embed the MSSM into a Grand Unified Theory 13 based upon SO ( 10 ) , although other possibilities exist 14 .Another possibility is to consider concepts with extra dimensions 15 - 17 .",
        "rewrite_text": "We present the findings of an evaluation of the supersymmetric standard model with minimal supergravity boundary conditions at the grand unified scale. This analysis includes all one-loop corrections to gauge and Yukawa couplings, as well as two-loop contributions to the evolution of the equations governing supersymmetry breaking. Our results indicate that these conditions are consistent with current experimental limits on sparticle masses when tan beta is either very large (tan beta > 50) or very small (tan beta < 10). Furthermore, we demonstrate that the mass of the lightest Higgs boson can be predicted within this framework for any tan beta value ranging from 1 to 60. Finally, we discuss the implications of our results for future research on supersymmetry at colliders such as the LHC. The supersymmetric standard model has been extensively studied over the years, offering a natural solution to the hierarchy problem by introducing new particles that mitigate the quadratic divergences related to radiative corrections in the scalar potential, while also providing a potential candidate for dark matter. Recent investigations have sought to establish whether it is feasible to construct models in which the electroweak symmetry breaking sector is described by the MSSM, while the underlying dynamics is dictated by a more fundamental theory that operates at higher energies. This approach is motivated by the MSSM's fine-tuning issues, arising from its sensitivity to unknown physics at long scales. If these challenges can be resolved, the MSSM may serve as a more accurate description of nature up to very high energy scales. One avenue for achieving this includes embedding the MSSM within a Grand Unified Theory based on SO(10), among other alternatives. Another possibility involves exploring concepts that incorporate extra dimensions.",
        "ori-fast-z-score": 0.2683281572999747,
        "water-fast-z-score": 7.184212081070997,
        "rewrite-fast-z-score": 0.1796053020267749
    },
    {
        "original_text": "The authors present the results of their study on the scattering between two protons and one neutron, which is known as the triton channel in nuclear physics.  They use an effective field theory to calculate the cross section for this process at low energies (below 100 MeV) using lattice QCD data obtained by other researchers.   The resulting theoretical predictions are compared with experimental measurements made over several decades by various groups around the world.    The agreement between experiment and theory is found to be good within uncertainties. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. In nuclear physics, there has been much interest recently in studying the interactions among three particles - specifically, how they affect the properties of nuclei such as helium-3 or carbon-12.  These processes can occur when high-energy cosmic rays strike Earth s atmosphere; however, it may also be possible that these reactions play some role in the formation of heavy elements during stellar evolution.  For example, scientists have proposed that helium-4 could form through a series of fusion reactions involving helium-3 and neutrons.  However, before we can understand what happens inside stars like our Sun, we need to know more about the fundamental interactions involved in these types of reactions.  To help us learn more about them, physicists at MIT used lattice quantum chromodynamics (QCD), a technique similar to those employed in high energy experiments but performed on computers instead of accelerators, to predict the behavior of certain nuclear reactions.  Specifically, they studied the reaction p+p+n --> d+d+n, where  p  stands for proton,  n  for neutron,  d  for deuteron, and  d+  means a positively charged deuteron.  Their calculations were based on...",
        "watermark_text": "The authors present the conclusion of their experiment on the scattering between two protons and one neutron , which is known as the triton channel in nuclear physics . They use an efficient field model to estimate the cross section for this process at low energies ( below 100 MeV ) using lattice QCD evidence derived by other researchers .The resulting theoretical estimates are compared with observation observations made over numerous generations by various groups around the world . The agreement between experiment and theory is found to be excellent within uncertainties .This project was supported by the U . S . Department of Energy under Contract No . DE - AC02 - 05CH11231 .In nuclear science , there has been much interest notably in investigating the interactions among three particles - particularly , how they impact the properties of nuclei such as helium - 3 or carbon - 12 . These mechanisms can occur when high - energy cosmic rays strike Earth s atmosphere ; however , it could also be possible that these reactions serve some role in the formation of heavy metals during stellar evolution .For instance , scientists have proposed that helium - 4 might form through a sequence of fusion compounds involving helium - 3 and neutrons . However , before we can comprehend what comes inside stars like our Sun , we require to knowledge more about the fundamental interactions involved in these kinds of reactions .To give us discover more about them , physicists at MIT utilized lattice quantum chromodynamics ( QCD ) , a technique similar to those utilized in high energy research but conducted on computers instead of accelerators , to predict the dynamics of certain nuclear compounds . Specifically , they examined the response pp + p + n - - > d + d + n , where p sits for proton , k for neutron , d for deuteron , and d + means a positively charged deuteron .Their calculations were based on...",
        "rewrite_text": "The authors summarize their findings on the scattering of two protons and one neutron, a process referred to as the triton channel in nuclear physics. They employed an effective field model to estimate the cross section of this phenomenon at low energies (below 100 MeV), utilizing lattice QCD evidence from prior research. The theoretical predictions were then compared to experimental observations gathered over several decades by various international research teams, revealing an excellent agreement within the margins of uncertainty. This research was funded by the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. In the field of nuclear science, there has been significant interest in studying the interactions among three particles, particularly in relation to the properties of nuclei such as helium-3 and carbon-12. These interactions can take place when high-energy cosmic rays collide with Earth’s atmosphere, and they may also play a role in the formation of heavy metals during stellar evolution. For example, scientists suggest that helium-4 might be produced through a series of fusion reactions involving helium-3 and neutrons. However, to fully understand the processes occurring inside stars like our Sun, further knowledge of the fundamental interactions at play in these reactions is essential. To advance this understanding, physicists at MIT utilized lattice quantum chromodynamics (QCD), a computational technique akin to those employed in high-energy research but executed on computers instead of particle accelerators, to investigate the dynamics of specific nuclear interactions. They focused on the reaction pp + p + n → d + d + n, where p represents protons, n denotes neutrons, and d indicates deuterons. Their calculations were based on...",
        "ori-fast-z-score": 1.044465935734187,
        "water-fast-z-score": 8.649670160944275,
        "rewrite-fast-z-score": 2.138089935299395
    },
    {
        "original_text": "We present new measurements of the helium mass fraction YHe = 0.24 ± 0.01 (statistical) ±0.02 (systematic), obtained by combining X-ray data on galaxy clusters with SZ observations, using the sample of 62 nearby relaxed galaxy clusters observed at high signal-to-noise ratio by Planck satellite. The results are consistent with previous determinations based on Chandra or XMM-Newton data alone. \n \n We also report an improved measurement of the Hubble constant H0 = 67.4±1.2 km s-1 Mpc-1, which is derived from our determination of the angular diameter distance to these clusters combined with their redshifts. This value agrees well with other recent estimates but has smaller statistical uncertainty than most of them. It is also compatible within 1 sigma with the local measurement inferred from Cepheid variables. \n \n Finally we use this dataset to test for possible deviations from standard cosmology due to massive neutrinos. Our analysis shows that current data do not allow us to detect any significant deviation from the predictions of ΛCDM model.",
        "watermark_text": "We report new studies of the helium mass fraction YHe = 0 . 24 ± 0 . 01 ( statistical ) ±0 . 02 ( systematic ) , obtained by combining X - ray data on star clusters with SZ measurements , using the sample of 62 nearby relaxed galaxy galaxies collected at high signal - to - noise proportion by Planck rocket . The results are compatible with previous determinations based on Chandra or XMM - Newton data alone .We additionally report an better determination of the Hubble constant H0 = 67 . 4±1 . 2 km s - 1 Mpc - 1 , which is calculated from our determination of the angular length length to these clusters combined with their redshifts . This value agrees well with other recent estimates but has less statistical uncertainty than most of them .It is also consistent within 1 sigma with the local measurement inferred from Cepheid variables . Finally we utilize this dataset to test for probable deviations from standard cosmology due to massive neutrinos .Our study shows that current data do not enable us to identify any large deviation from the estimates of ΛCDM system .",
        "rewrite_text": "We present new findings on the helium mass fraction, YHe = 0.24 ± 0.01 (statistical) ± 0.02 (systematic), derived from a combination of X-ray data from star clusters and Sunyaev-Zel’dovich (SZ) measurements. This analysis utilizes a sample of 62 nearby relaxed galaxy clusters collected by the Planck satellite with high signal-to-noise ratios. Our results are consistent with previous measurements based solely on Chandra or XMM-Newton data. Additionally, we provide a refined estimate of the Hubble constant, H0 = 67.4 ± 1.2 km s⁻¹ Mpc⁻¹, which is calculated using the angular distances to these clusters in conjunction with their redshifts. This value aligns well with recent estimates and exhibits lower statistical uncertainty compared to most others. It is also consistent within 1 sigma with local measurements derived from Cepheid variables. Lastly, we employ this dataset to investigate potential deviations from standard cosmology related to massive neutrinos. Our analysis indicates that current data do not reveal any significant discrepancies from the predictions of the ΛCDM model.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "We present an overview of the phase space approach to special relativity, which is based on the concept that all physical quantities are represented by functions defined over a four-dimensional manifold called spacetime. The main idea behind this approach is that it allows one to eliminate gravitational singularities and thus provides a natural framework for describing quantum gravity phenomena. We also discuss some recent results obtained within this formalism. In particular we show how the standard description of black holes can be modified so as to avoid the appearance of such singularities. Finally, we argue that the phase space approach may provide new insights into the problem of time in quantum gravity theories. This article is part of the themed issue  Quantum gravity . Phase spaces have been used extensively in classical mechanics since their introduction by Hamilton more than 150 years ago. They represent the state of any system at each point in its configuration space (the set of all possible states) through a function known as the phasespace density. These densities evolve according to a continuity equation known as Liouville s theorem, which describes how they change with respect to time.",
        "watermark_text": "We present an overview of the phase space approach to special relativity , which is based on the idea that all physical phenomena are represented by functions defined over a four - dimensional manifold called spacetime . The main idea behind this methodology is that it allows one to eliminate gravity singularities and therefore provides a natural framework for describing quantum gravitational processes .We addition explore some latest findings obtained within this formalism . In particular we explain how the standard description of black holes can be altered so as to eliminate the appearance of such singularities .Finally , we argue that the phase space approach may provide fresh insights into the question of time in quantum gravitational dynamics . This page is part of the themed topic Quantum gravity .Phase functions have been used heavily in classical mechanics since their arrival by Hamilton more than 150 years back . They measure the state of any system at each point in its configuration set ( the set of all possible states ) through a function referred as the phasespace density .These densities grow according to a continuity function referred as Liouville s theorem , which explains how they change with regard to time .",
        "rewrite_text": "We provide an overview of the phase space approach to special relativity, which posits that all physical phenomena can be described by functions defined over a four-dimensional manifold known as spacetime. This methodology offers a means to eliminate gravity singularities, creating a natural framework for understanding quantum gravitational processes. Additionally, we discuss some of the latest findings emerging from this framework. Specifically, we illustrate how the conventional description of black holes can be modified to remove the occurrence of singularities. Furthermore, we contend that the phase space approach may yield new insights into the nature of time within quantum gravitational dynamics. This discussion is part of the themed topic of quantum gravity. Phase functions have been integral to classical mechanics since their introduction by Hamilton over 150 years ago. These functions quantify the state of any system at each point in its configuration space (the collection of all possible states) via a metric known as phase space density. These densities evolve over time in accordance with a continuity function described by Liouville's theorem.",
        "ori-fast-z-score": -0.3110855084191276,
        "water-fast-z-score": 6.18852747755276,
        "rewrite-fast-z-score": 2.251436323159369
    },
    {
        "original_text": "We study the implications for cosmology, astrophysics and particle physics of large volume string compactifications with fluxes in type IIA theory. We find that these models can be described by an effective supergravity action which is valid at energies below the Kaluza-Klein scale. The resulting four-dimensional effective potential has several interesting features. In particular it contains terms which are exponentially suppressed compared to those arising in conventional Calabi-Yau compactifications. These new contributions lead to novel effects such as moduli stabilization via non-perturbative corrections or supersymmetry breaking induced by gaugino condensation on D-branes wrapping internal cycles. Finally we discuss some phenomenological consequences of our results. This work was supported in part by NSF grant PHY-0456735. PACS numbers: 11.25.Wx, 04.50.+h, 98.80.Cq",
        "watermark_text": "We research the implications for cosmology , astrophysics and particle science of large depth string compactifications with fluxes in type IIA theory . We see that these models can be described by an efficient supergravity operation which is valid at energies below the Kaluza - Klein scale .The produced four - dimensional effective potential has various attractive features . In particular it contains terms which are exponentially suppressed compared to those arose in standard Calabi - Yau compactifications .These new contributions bring to novel effects such as moduli stabilization via non - perturbative corrections or supersymmetry breaking induced by gaugino condensation on D - branes wrapping internal cycles . Finally we explain some phenomenological consequences of our findings .This project was supported in part by NSF grant PHY - 0456735 . PACS codes : 11 . 25 . Wx , 04 . 50 . + h , 98 . 80 . Cq",
        "rewrite_text": "We investigate the implications of large depth string compactifications with fluxes in type IIA theory for cosmology, astrophysics, and particle science. Our findings indicate that these models can be effectively described through a supergravity framework, applicable at energies below the Kaluza-Klein scale. The resulting four-dimensional effective potential possesses several compelling features, notably containing terms that are exponentially suppressed in comparison to those arising in standard Calabi-Yau compactifications. These new contributions lead to novel phenomena, including moduli stabilization through non-perturbative corrections and supersymmetry breaking triggered by gaugino condensation on D-branes that wrap internal cycles. Additionally, we discuss some phenomenological implications of our results. This research was partially supported by NSF grant PHY-0456735. PACS codes: 11.25.Wx, 04.50.+h, 98.80.Cq.",
        "ori-fast-z-score": 1.1094003924504583,
        "water-fast-z-score": 5.269651864139677,
        "rewrite-fast-z-score": 1.3608276348795434
    },
    {
        "original_text": "The human genome contains thousands of enhancers, which are non-coding DNA sequences that can activate transcription at distant sites. Enhancer activity is often cell-type specific and regulated by the binding of sequence-specific transcription factors (TFs). However, how TFs cooperate to regulate enhancer function remains poorly understood. Here we show that cooperative interactions between two distinct classes of TFs play an important role in regulating enhancer activity during lytic infection with herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. We find that HSV-1 encodes three proteins - VP16, ICP4 and ICP27 - each containing multiple domains capable of interacting directly or indirectly with other cellular proteins. Using biophysical methods, we demonstrate that these three viral proteins interact cooperatively on a single promoter element to drive expression of genes required for efficient viral replication. Our results suggest that cooperative protein-protein interactions may be a general mechanism used by viruses to control host gene expression.",
        "watermark_text": "The human genome contains dozens of enhancers , which are non - coding DNA sequences that can stimulate transcription at remote sites . Enhancer activity is often cell - class related and controlled by the interaction of sequence - specific transcription elements ( TFs ) .However , how TFs cooperate to affect enhancer function continues poorly explored . Here we find that cooperative relationships between two separate categories of TFs serve an important role in controlling enhancer activity during lytic infection with herpes simplex virus 1 ( HSV - 1 ) , a large double - stranded DNA virus .We see that HSV - 1 encodes three domains - VP16 , ICP4 and ICP27 - each carrying several domains responsible of interacting directly or indirectly with other cellular hormones . Using biophysical methods , we prove that these three viral molecules interact cooperatively on a single promoter structure to drive expression of genes necessary for efficient viral replication .Our results propose that cooperative protein - gene relationships may be a general process used by viruses to affect host gene transcription .",
        "rewrite_text": "The human genome is rich in enhancers, which are non-coding DNA sequences that can activate transcription at distant locations. The activity of enhancers is often specific to particular cell types and is regulated by the interaction of sequence-specific transcription factors (TFs). However, the mechanisms by which TFs work together to influence enhancer function are not well understood. In this study, we demonstrate that the cooperative interactions between two distinct classes of TFs play a crucial role in modulating enhancer activity during lytic infection with herpes simplex virus 1 (HSV-1), a large double-stranded DNA virus. We show that HSV-1 encodes three key proteins—VP16, ICP4, and ICP27—each containing multiple domains that facilitate direct or indirect interactions with other cellular factors. Through biophysical methods, we reveal that these three viral proteins work cooperatively on a single promoter, driving the expression of genes essential for effective viral replication. Our findings suggest that cooperative protein-gene interactions may be a widespread mechanism employed by viruses to influence host gene transcription.",
        "ori-fast-z-score": -1.1793237883215741,
        "water-fast-z-score": 6.539886462510548,
        "rewrite-fast-z-score": -0.5360562674188973
    },
    {
        "original_text": "We present new infrared spectra obtained with the Infrared Spectrograph (IRS) on board the Spitzer Space Telescope for four young stars in nearby open clusters. The targets are all classical T Tauri stars surrounded by circumstellar disks that have been previously studied at optical wavelengths using high-resolution spectroscopy to detect forbidden emission lines produced by ionized iron atoms Fe + . We find evidence for both neutral atomic hydrogen and molecular hydrogen in these objects based on detection of their ro-vibrational transitions near 2 microns. \n \n These observations provide important constraints on models of disk structure and evolution as well as physical conditions within protoplanetary disks. They also allow us to study chemical composition of the gaseous component of the disks. Finally, we use our results to estimate mass accretion rates onto central stars. Our main conclusions can be summarized as follows: \n \n 1. We confirm previous reports of strong  Ne II  12.81 micron line emission in three out of four observed sources. This is consistent with predictions made by theoretical models of photoevaporation of protoplanetary disks driven by intense ultraviolet radiation from central stars. \n \n 2. We report detection of several other ionic species including  S III  18.71 micron,  C II  158 micron, and  N II  122 micron. Their presence indicates significant ionization fraction in the innermost regions of the disks where temperatures exceed 1000 K. \n \n 3. We identify numerous ro-vibrational bands of molecular hydrogen in two of the observed systems. Emission features detected between 2.0-2.3 microns correspond to fundamental vibrational band of H2 1-0 S(1). Other prominent H2 lines include those associated with v=1-0 Q-branch of the first overtone transition 2-0 S(1), which appear in the range 2-2.2 microns.",
        "watermark_text": "We present new infrared spectra obtained with the Infrared Spectrograph ( IRS ) on board the Spitzer Space Telescope for four young stars in nearby open complexes . The targets are all ancient T Tauri stars surrounded by circumstellar disks that have been previously examined at optical wavelengths using high - resolution spectroscopy to identify forbidden emission lines released by ionized iron atoms Fe + .We see evidence for both stable atomic hydrogen and molecular hydrogen in these objects based on observations of their ro - vibrational changes near 2 microns . These measurements give important restrictions on predictions of disk shape and evolution as well as physical conditions within protoplanetary disks .They addition allow us to study chemical composition of the gaseous constituent of the disks . Finally , we utilize our findings to estimate mass accretion levels onto primary stars .Our main results can be summarized as follows : 1 . We report previous accounts of strong Ne II 12 . 81 micron line emission in three out of four predicted sources .This is compatible with predictions making by theoretical theories of photoevaporation of protoplanetary disks powered by intense ultraviolet radiation from central stars . 2 .We report discovery of several other ionic species namely S III 18 . 71 micron , C II 158 micron , and N II 122 micron . Their presence indicates considerable ionization fraction in the innermost parts of the disks where heat exceed 1000 K . 3 .We distinguish numerous ro - vibrational lines of molecular hydrogen in two of the studied structures . Emission features detected between 2 . 0 - 2 . 3 microns correspond to basic vibrational band of H2 1 - 0 S ( 1 ) .Other notable H2 lines include those associated with v = 1 - 0 Q - branch of the first overtone transition 2 - 0 S ( 1 ) , which appear in the range 2 - 2 . 2 microns .",
        "rewrite_text": "In this study, we present new infrared spectra collected with the Infrared Spectrograph (IRS) on the Spitzer Space Telescope, focusing on four young stars located in nearby open clusters. Each of these ancient T Tauri stars is surrounded by circumstellar disks that have previously been examined at optical wavelengths through high-resolution spectroscopy to identify forbidden emission lines associated with ionized iron (Fe+). Our observations reveal the presence of both stable atomic hydrogen and molecular hydrogen in these stars, as indicated by their ro-vibrational transitions near 2 microns. These findings provide significant insights into predictions regarding disk shape and evolution, as well as the physical conditions within protoplanetary disks. Additionally, they enable us to analyze the chemical composition of the gaseous components of the disks. Moreover, we leverage these results to estimate mass accretion rates onto the primary stars. Our key findings can be summarized as follows: 1. We confirm previous observations of strong Ne II 12.81 micron line emission in three out of the four studied sources, supporting theoretical predictions regarding the photoevaporation of protoplanetary disks due to intense ultraviolet radiation from the central stars. 2. We identify several other ionic species, including S III 18.71 micron, C II 158 micron, and N II 122 micron, indicating a significant ionization fraction in the innermost regions of the disks where temperatures exceed 1000 K. 3. We distinguish multiple ro-vibrational lines of molecular hydrogen in two of the observed structures, with emission features detected between 2.0 - 2.3 microns corresponding to the fundamental vibrational band of H2 1-0 S(1). Other notable H2 lines related to the v = 1-0 Q-branch of the first overtone transition 2-0 S(1) are found in the range of 2.0 - 2.2 microns.",
        "ori-fast-z-score": -1.0776318121606494,
        "water-fast-z-score": 6.529318494299385,
        "rewrite-fast-z-score": 1.1818181818181819
    },
    {
        "original_text": "The DWEB benchmark is an open-source data warehouse engineering benchmark that can be used to evaluate the performance and scalability of different data warehouse systems, including relational databases (e.g., Oracle), columnar stores (e.g., MonetDB) and NoSQL databases (e.g,. Cassandra). The benchmark consists of three main components: a query generator, a database server emulator, and a client application. In this work we present the design and implementation details for each component as well as our experiences in using it on several platforms. We also provide some insights into how the benchmark was developed and discuss its limitations. Finally, we compare the results obtained by running the benchmark against two state-of-the-art commercial products. This article is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications (DASFAA 2015).\nIntroduction\n\nData warehouses are large repositories storing historical information about business processes or other types of activities. They have been widely adopted over the last decade due to their ability to support decision making at various levels within organizations  1  . However, with the increasing size of data warehouses, there has been growing interest in developing new techniques to improve their efficiency  2  .\nIn order to assess these new techniques, researchers need benchmarks capable of evaluating the performance and scalability characteristics of different data warehouse systems  3  , such as relational databases  4  , columnar stores  5  , and NoSQL databases  6  . Unfortunately, most existing benchmarks focus only on one type of system  7, 8  , which makes them unsuitable for comparing multiple technologies simultaneously  9  . To address this problem, we propose the DWEB benchmark  10  , which supports both traditional SQL queries and more advanced analytical operations  11  .",
        "watermark_text": "The DWEB benchmark is an open - source information warehouse architecture benchmark that can be used to analyze the performance and scalability of different data warehouse applications , notably relational databases ( e . g . , Oracle ) , columnar stores ( e . g . , MonetDB ) and NoSQL databases ( e . g , . Cassandra ) .The benchmark consists of three principal portions : a query generator , a document server emulator , and a client application . In this project we present the development and implementation information for each product as well as our achievements in utilizing it on numerous systems .We additionally offer some details into how the benchmark was developed and consider its limitations . Finally , we compare the results acquired by running the benchmark against two state - of - the - art commercial products .This page is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications ( DASFAA 2015 ) . Introduction Data warehouses are big repositories storing historical data about corporate processes or other types of activities .They have been widely adopted over the last decade due to their potential to support decision making at several levels within organizations 1 . However , with the increasing size of video warehouses , there has been growing interest in implementing new tactics to improve their productivity 2 .In order to examine these new strategies , researchers require benchmarks capable of evaluating the performance and scalability characteristics of different data warehouse technologies 3 , such as relational databases 4 , columnar stores 5 , and NoSQL databases 6 . Unfortunately , most existing benchmarks focus only on one sort of system 7 , 8 , which makes them unsuitable for comparing multiple concepts simultaneously 9 .To address this situation , we propose the DWEB benchmark 10 , which supports both traditional SQL queries and more advanced descriptive techniques 11 .",
        "rewrite_text": "The DWEB benchmark is an open-source architecture designed to evaluate the performance and scalability of various data warehouse applications, including relational databases (e.g., Oracle), columnar stores (e.g., MonetDB), and NoSQL databases (e.g., Cassandra). It comprises three main components: a query generator, a document server emulator, and a client application. In this project, we provide detailed development and implementation insights for each of these components, along with our successes in deploying it across multiple systems. We also delve into the benchmark's development process and discuss its limitations. Additionally, we compare the results obtained from the benchmark with those from two leading commercial products. This document is part of the Proceedings of the 20th International Conference on Database Systems for Advanced Applications (DASFAA 2015). \n\nData warehouses serve as extensive repositories for historical data related to corporate processes and other activities. Over the past decade, they have gained popularity for their ability to assist in decision-making at various organizational levels. However, as the volume of video warehouses continues to grow, there is an increasing demand for innovative strategies to enhance their efficiency. To explore these strategies, researchers need benchmarks that can assess the performance and scalability of different data warehouse technologies such as relational databases, columnar stores, and NoSQL databases. Unfortunately, most current benchmarks focus solely on a specific type of system, which limits their applicability for simultaneous comparisons across multiple architectures. To fill this gap, we introduce the DWEB benchmark, which accommodates both traditional SQL queries and more advanced analytical techniques.",
        "ori-fast-z-score": 0.1781741612749496,
        "water-fast-z-score": 7.363817383097771,
        "rewrite-fast-z-score": 0.4703604341917986
    },
    {
        "original_text": "The concept of dynamical 3-space is introduced in this review article as an alternative to the standard space-time picture of relativistic physics. The main idea behind it is that, instead of considering time and space separately, one should consider them together as a single entity called  dynamical 3-space . This new approach has several advantages over the traditional viewpoint; for example, it provides a natural explanation for why we experience time flow only forward (and not backward), while at the same time allowing us to preserve causality. In addition, it also allows us to explain how particles can travel faster than light without violating any physical laws. Finally, by introducing the concept of  quantum potential energy density  into our description of matter fields, we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described. We conclude with some remarks on possible future research directions based upon this novel theoretical perspective.",
        "watermark_text": "The concept of dynamical 3 - space is proposed in this review article as an alternative to the standard space - time view of relativistic physics . The main idea behind it is that , rather of considering time and space simultaneously , one should consider them combined as a common organization called dynamical 3 - space .This new approach has numerous benefits over the usual interpretation ; for example , it gives a natural explanation for why we experience time flow only ahead ( and not backward ) , while at the same time allowing us to restore causality . In addition , it also enables us to explain how atoms can travel quicker than light without violating any physical rules .Finally , by bringing the notion of quantum potential energy density into our description of matter fields , we are able to provide a simple mathematical framework within which all known fundamental interactions between elementary particles may be described . We continue with some remarks on potential future research paths based upon this new theoretical perspective .",
        "rewrite_text": "This review article introduces the concept of dynamical 3-space as an alternative to the conventional space-time framework of relativistic physics. The core idea is to view time and space as a unified entity known as dynamical 3-space, rather than as separate dimensions. This fresh perspective offers several advantages over traditional interpretations; for instance, it naturally accounts for our perception of the forward flow of time—precluding any backward movement—while also enabling a restoration of causality. Furthermore, it provides a mechanism to explain how atoms can exceed the speed of light without contravening physical laws. By incorporating the concept of quantum potential energy density into our understanding of matter fields, we establish a straightforward mathematical framework that can encapsulate all known fundamental interactions among elementary particles. We also discuss potential avenues for future research stemming from this innovative theoretical outlook.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "We present high spatial and spectral resolution observations of two IM stars, HD 163296 and MWC 480, obtained with the Submillimeter Array at 1.3 mm wavelength. We detect several compact sources in both objects that are associated with dusty disks or envelopes surrounding these young stellar objects. The disk masses derived for these systems range between 0.1 to 0.5 Msun. In addition we find evidence for an extended component in the vicinity of HD 163296 which may be related to its outflow activity. These results demonstrate that even massive protoplanetary disks can form planets like our own solar system. Keywords: Circumstellar matter - Stars: Herbig Ae/Be - Massive star formation - Planet formation - Protostars - Young stellar objects: General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High energy astrophysics",
        "watermark_text": "We report high spatial and spectral resolution measurements of two IM stars , HD 163296 and MWC 480 , obtained with the Submillimeter Array at 1 . 3 cm wavelength . We detect many compact sources in both objects that are related with dusty belts or envelopes underlying these young stellar bodies .The disk masses derived for these systems range between 0 . 1 to 0 . 5 Msun . In addition we find data for an extended component in the vicinity of HD 163296 which may be connected to its outflow function .These data demonstrate that even gigantic protoplanetary disks can form planets like our own solar body . Keywords : Circumstellar matter - Stars : Herbig Ae / Be - Massive star formation - Planet structure - Protostars - Young stellar bodies : General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High mass astrophysics",
        "rewrite_text": "We present high spatial and spectral resolution measurements of two Intermediate-Mass (IM) stars, HD 163296 and MWC 480, acquired using the Submillimeter Array at a wavelength of 1.3 cm. Our observations reveal numerous compact sources associated with dusty belts or envelopes surrounding these young stellar objects. The estimated disk masses for these systems range from 0.1 to 0.5 solar masses. Additionally, we observe an extended component near HD 163296, which may be linked to its outflow activity. These findings illustrate that even large protoplanetary disks are capable of forming planets similar to those in our own solar system. \n\nKeywords: Circumstellar matter - Stars: Herbig Ae/Be - Massive star formation - Planet structure - Protostars - Young stellar objects: General - Millimeter waves - Submillimeter waves - Nearby galaxies - Radio astronomy - High mass astrophysics.",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 5.163977794943222,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We study non-Abelian hydrodynamic equations for fluids with spin-orbit coupling, which are derived by applying Noether s theorem to an action functional describing the dynamics of such systems. We show that these equations can be written as a system of conservation laws for charge current density Jμc , energy-momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e, μ, τ .\nThe resulting transport coefficients are calculated explicitly using kinetic theory methods. In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species (e.g., electrons) or if the fluid contains only neutral bosons like photons. This result holds both for relativistic and nonrelativistic fluids. Furthermore, we calculate the bulk viscosities for various examples including QED plasma, superfluid helium-4, and ultracold atomic gases. Finally, we discuss how our results could be used to describe the collective motion of atoms in Bose-Einstein condensates. \nI. INTRODUCTORY REMARK\nIn this work we consider fluids whose constituents have internal degrees of freedom described by quantum fields. Examples include plasmas consisting of charged particles interacting via electromagnetic field, superfluids made up of neutral bosonic atoms, and cold atom clouds where the atoms are treated as distinguishable particles. For simplicity, we will assume that the number densities of different types of particles do not change significantly during time evolution so that they may be considered constant.",
        "watermark_text": "We research non - Abelian hydrodynamic equations for fluids with spin - orbit coupling , which are derived by using Noether s theorem to an action functional describing the dynamics of such systems . We see that these equations can be written as a system of conservation laws for charge current density Jμc , energy - momentum tensor Tμν and spin current density JSμ .The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e , μ , τ . The resulting transport values are measured precisely use kinetic theory techniques .In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species ( e . g . , electrons ) or if the liquid includes only neutral bosons like photons . This result holds both for relativistic and nonrelativistic fluids .Furthermore , we estimate the bulk viscosities for various examples namely QED gas , superfluid helium - 4 , and ultracold atomic fluids . Finally , we talk how our findings may be used to explain the collective motion of atoms in Bose - Einstein condensates .I . INTRODUCTORY REMARK In this research we study fluids whose members have internal degrees of autonomy explained by quantum fields .Examples example plasmas consisting of charged particles communicating via electromagnetic field , superfluids composed up of neutral bosonic atoms , and cold particle clouds where the atoms are treated as distinguishable molecules . For simplicity , we will assume that the number densities of different kinds of atoms do not change considerably during time progression so that they may be regarded continuous .",
        "rewrite_text": "We investigate non-Abelian hydrodynamic equations for fluids with spin-orbit coupling, derived using Noether's theorem applied to an action functional that captures the dynamics of these systems. Our analysis reveals that these equations can be expressed as a set of conservation laws for charge current density \\( J^\\mu_c \\), energy-momentum tensor \\( T^{\\mu\\nu} \\), and spin current density \\( J^S_\\mu \\). The spin current density is formulated as a sum over all particles, incorporating their individual spins \\( S_\\alpha \\) multiplied by specific coefficients related to the particle type \\( \\alpha = e, \\mu, \\tau \\). We utilize kinetic theory techniques to obtain precise transport coefficients. Notably, we discover that the shear viscosity \\( \\eta_s \\) becomes identically zero if there is at least one electrically charged fermion species (e.g., electrons) present or if the fluid consists solely of neutral bosons like photons. This outcome is valid for both relativistic and non-relativistic fluids. Additionally, we estimate the bulk viscosities for various scenarios, including QED gases, superfluid helium-4, and ultracold atomic fluids. Lastly, we discuss the implications of our findings for understanding the collective motion of atoms in Bose-Einstein condensates. \n\n**I. INTRODUCTORY REMARK**  \nIn this study, we focus on fluids with constituents that possess internal degrees of freedom influenced by quantum fields. Examples include plasmas made of charged particles interacting through the electromagnetic field, superfluids arising from neutral bosonic atoms, and cold particle ensembles where atoms are treated as distinguishable particles. For the sake of simplicity, we will assume that the number densities of the various types of atoms remain relatively constant over time, allowing us to consider them as continuous.",
        "ori-fast-z-score": 0.4508348173337161,
        "water-fast-z-score": 6.041186552271796,
        "rewrite-fast-z-score": 2.263009527424072
    },
    {
        "original_text": "We present an algorithm for the calculation of ground-state properties in fermionic systems using auxiliary field quantum Monte Carlo (AFQMC). The method is based on the use of a trial wave function that incorporates information about the broken bonds and their associated energy penalty, which are determined by exact diagonalization or density-functional theory calculations. We demonstrate our approach to calculate the electronic structure of silicon clusters Si_(n) with n = 2-10 atoms at zero temperature. Our results show good agreement with previous theoretical studies as well as experimental data obtained from photoelectron spectroscopy experiments. In addition we study the structural stability of these clusters against dissociation into smaller fragments. Finally, we discuss possible extensions of this work towards larger cluster sizes and finite temperatures. Quantum Monte Carlo methods have been widely used over recent years to solve many-body problems in condensed matter physics  1  . These techniques provide accurate estimates of physical quantities such as energies, correlation functions, and other observables within statistical uncertainties  2  .\nIn particular, the Auxiliary Field QMC (AFQMC) technique has proven very useful for studying strongly correlated electron systems  3, 4  , including materials like transition metal oxides  5  , high-temperature superconductors  6  , and heavyfermion compounds  7, 8  . This method can be applied to any system described by a local Hamiltonian H = T + V where T denotes the kinetic part and V represents the interaction between particles  9  . It relies on the introduction of a trial wave function |ΨT⟩ that approximates the true ground state |Ψ0⟩ of the system under consideration  10  . Then, the expectation value ⟨O⟩ of some observable O can be calculated through the expression",
        "watermark_text": "We present an algorithm for the determination of ground - state properties in fermionic models using auxiliary field quantum Monte Carlo ( AFQMC ) . The method is based on the using of a trial wave function that incorporates information about the broken bonds and their accompanying energy penalty , which are decided by precise diagonalization or density - functional theory analyses .We showed our approach to estimate the electronic stability of silicon groups Si _ ( n ) with n = 2 - 10 atoms at zero temperature . Our results show good agreement with previous conceptual research as well as empirical data received from photoelectron spectroscopy observations .In addition we study the structural integrity of these complexes against dissociation into tiny components . Finally , we investigate possible extensions of this research towards higher cluster sizes and finite temperatures .Quantum Monte Carlo methods have been widely using over recent years to solve many - bodies problems in condensed matter science 1 . These methods provide accurate calculations of physical quantities such as energies , correlation functions , and other observables within statistical uncertainties 2 .In particular , the Auxiliary Field QMC ( AFQMC ) method has proven very useful for studying strongly interacting ion environments 3 , 4 , notably materials like transition glass oxides 5 , low - temperature superconductors 6 , and heavyfermion compounds 7 , 8 . This method can be applied to any system characterized by a local Hamiltonian H = T + V where T denotes the kinetic component and V describes the interaction between particles 9 .It relies on the introduction of a trial wave function | ΨT ⟩ that approximates the true ground state | Ψ0 ⟩ of the system under consideration 10 . Then , the expectation value ⟨ O ⟩ of some observable O can be determined through the expression",
        "rewrite_text": "We introduce an algorithm designed to determine ground-state properties in fermionic models utilizing auxiliary field quantum Monte Carlo (AFQMC). This method employs a trial wave function that captures information about broken bonds and their associated energy penalties, which are established through precise diagonalization or density functional theory analyses. We applied our approach to assess the electronic stability of silicon clusters Si_n with n ranging from 2 to 10 at zero temperature. Our findings demonstrate strong consistency with prior theoretical studies and empirical data obtained from photoelectron spectroscopy. Additionally, we examined the structural stability of these complexes against dissociation into smaller components. Ultimately, we explore potential extensions of this research to larger cluster sizes and finite temperatures. In recent years, quantum Monte Carlo methods have been extensively utilized to address many-body problems in condensed matter physics. These methods yield precise calculations of various physical quantities, including energies, correlation functions, and other observables, all within statistical uncertainties. Specifically, the Auxiliary Field QMC (AFQMC) technique has been particularly effective for investigating strongly interacting ion environments, such as transition metal oxides, low-temperature superconductors, and heavy fermion compounds. This approach is applicable to any system described by a local Hamiltonian \\(H = T + V\\), where \\(T\\) represents the kinetic energy component and \\(V\\) accounts for inter-particle interactions. It leverages the introduction of a trial wave function \\(|\\Psi_T\\rangle\\) that approximates the true ground state \\(|\\Psi_0\\rangle\\) of the system, allowing for the determination of the expectation value \\(\\langle O \\rangle\\) of an observable \\(O\\).",
        "ori-fast-z-score": -1.647508942095828,
        "water-fast-z-score": 5.918640302493727,
        "rewrite-fast-z-score": -0.17541160386140586
    },
    {
        "original_text": "The article is devoted to the problem of possible existence of dark matter particles in our Galaxy, which are not detected by other methods than their gravitational effects on visible objects (stars). The author considers the possibility that these hypothetical particles can be described as celestial mechanics daemons with certain properties. In particular, it is shown how such daemons could explain some features observed recently for the DAMA experiment at Gran Sasso National Laboratory. It should be noted that this explanation does not contradict any known experimental data. However, there are also serious difficulties associated with the proposed model. These problems will require further study. This work was supported by Russian Science Foundation grant No 14-50-00040. URL: http://arxiv.org/abs/1409.5189 . \nI. INTRODUCTORY REMARK .\nDark Matter (DM) is one of the most important mysteries of modern physics  1  -  4  . Its presence has been established only indirectly through its gravitational influence on visible stars  5  , galaxies  6  , clusters  7  etc., but direct detection experiments have so far failed  8  -  10  . There exist many theoretical models describing DM  11  -  13  ; however, none of them has yet been confirmed experimentally  14  . One of the possibilities is that DM consists of new elementary particles  15  -  17  . If they interact weakly or electromagnetically with ordinary matter then they would escape detection even if they were produced in large quantities  18  . On the other hand, if they interact strongly enough with normal matter, then they may be detectable directly  19  -  21  . A number of experiments searching for DM particles have been carried out  22  -  26  . Recently, the results obtained by the DAMA collaboration  27  attracted considerable attention  28  -  30  . According to these results, the annual modulation effect  31  -  33  caused by the motion of Earth around Sun  34  -  36  leads to an increase in the rate of nuclear recoils registered by detectors during June-October period  37  compared to December-February period. Such behavior cannot be explained within Standard Model of particle interactions  38  -  41  . Several authors suggested different explanations based on",
        "watermark_text": "The essay is devoted to the question of possible existence of dark matter molecules in our Galaxy , which are not observed by other methods than their gravitational impacts on visible objects ( stars ) . The author considers the prospect that these hypothetical particles can be described as celestial mechanics daemons with certain characteristics .In particular , it is demonstrated how such daemons might explain some features detected lately for the DAMA experiment at Gran Sasso National Laboratory . It should be mentioned that this explanation does not contradict any established experimental evidence .However , there are also serious difficulties linked with the suggested model . These difficulties will demand further study .This project was supported by Russian Science Foundation program No 14 - 50 - 00040 . URL : www : / / arxiv . org / abs / 1409 . 5189 .I.INTRODUCTORY REMARK .Dark Matter ( DM ) is one of the most important wonders of modern physics 1 - 4 . Its presence has been known only indirectly through its gravitational impact on visible stars 5 , galaxies 6 , galaxies 7 etc . , but direct detection experiments have so far unsuccessful 8 - 10 .There exist many theoretical theories describing DM 11 - 13 ; however , none of them has already been confirmed experimentally 14 . One of the possibilities is that DM consists of new primary nucleus 15 - 17 .If they interact weakly or electromagnetically with everyday matter then they may survive observation even if they were produced in large quantities 18 . On the other hand , if they interact heavily enough with normal matter , then they may be detectable directly 19 - 21 .A variety of studies looking for DM nuclei have been carried out 22 - 26 . Recently , the results derived by the DAMA collaboration 27 drew substantial scrutiny 28 - 30 .According to these results , the annual modulation effect 31 - 33 induced by the movement of Earth around Sun 34 - 36 results to an increase in the frequency of nuclear recoils registered by detectors during June - October year 37 contrast to December - February time . Such action cannot be described within Standard Model of particle particles 38 - 41 .Several articles suggested different explanations based on",
        "rewrite_text": "The essay explores the potential existence of dark matter molecules within our Galaxy, which have yet to be detected by means other than their gravitational effects on visible celestial objects, such as stars. The author posits that these theorized particles can be conceptualized as celestial mechanics daemons with specific properties. Notably, the essay illustrates how these daemons might account for some of the recent observations made by the DAMA experiment at the Gran Sasso National Laboratory. It is important to note that this hypothesis aligns with existing experimental findings. However, the proposed model does face significant challenges that warrant further investigation. This project received funding from the Russian Science Foundation under program No. 14-50-00040. For more details, you can visit the URL: www:/arxiv.org/abs/1409.5189.\n\n**Introduction**: Dark Matter (DM) remains one of the most intriguing mysteries of modern physics. Its existence is known only indirectly, inferred from its gravitational influence on visible stars and galaxies. Despite extensive direct detection efforts, no conclusive results have been obtained thus far. Numerous theoretical frameworks have been proposed to explain DM, yet none have been experimentally validated. One possibility is that dark matter consists of novel primary nuclei. If these nuclei interact weakly or electromagnetically with ordinary matter, they may elude detection even if they are produced in significant numbers. Conversely, if their interactions with regular matter are strong enough, direct detection may be feasible. Numerous studies have sought evidence of DM nuclei, and recent findings from the DAMA collaboration have attracted considerable attention. Their data suggest an annual modulation effect caused by the Earth's orbit around the Sun, leading to an increased frequency of nuclear recoils recorded by detectors from June to October compared to December to February. This phenomenon cannot be accounted for within the Standard Model of particle physics. Various papers have proposed alternative explanations for this observation.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.780541105074454,
        "rewrite-fast-z-score": 1.0215078369104984
    },
    {
        "original_text": "The Large Magellanic Cloud (LMC) is an ideal laboratory for studying Galactic structure, stellar populations, chemical evolution, and cosmology because it has many advantages over other nearby galaxies such as M31 or M33.  The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode. In this study we used two different methods to determine distances to Cepheids in the LMC. First method was based on non-linear least squares fitting technique called testimator while second one was based on statistical analysis known as Schwarz information criterion (SIC). We found that both methods gave consistent results within their uncertainties. Our final sample consists of 1228 Cepheids located between 30 < R < 50 kpc from the center of the galaxy. Using these data sets we derived new period-luminosity relations for classical Cepheids in the infrared bands JHKs.",
        "watermark_text": "The Large Magellanic Cloud ( LMC ) is an excellent laboratory for studying Galactic structure , stars populations , chemical evolution , and cosmology because it has numerous benefits over other nearby galaxies such as M31 or M33 . The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode .In this study we using two different methods to estimate distances to Cepheids in the LMC . First method was based on non - linear least squares fit technique called testimator while second one was based on statistical analysis called as Schwarz information criterion ( SIC ) .We showed that both approaches made satisfactory findings within their uncertainties . Our last sample consists of 1228 Cepheids situated between 30 < R < 50 kpc from the hub of the galaxy .Using these information sets we derived additional period - luminosity relations for classical Cepheids in the infrared bands JHKs .",
        "rewrite_text": "The Large Magellanic Cloud (LMC) serves as a valuable platform for investigating various aspects of Galactic structure, star populations, chemical evolution, and cosmology, offering distinct advantages over other nearby galaxies like M31 or M33. The distance to the LMC can be accurately measured using Cepheid variables—bright, periodic stars that pulsate in their fundamental radial mode. In this study, we employed two different methods to estimate the distances to Cepheids in the LMC. The first method utilized a non-linear least squares fitting technique known as testimator, while the second relied on a statistical analysis called the Schwarz Information Criterion (SIC). Our findings indicate that both methods yielded satisfactory results within their respective uncertainties. The final sample included 1,228 Cepheids located between 30 and 50 kpc from the galactic center. Using this dataset, we derived additional period-luminosity relations for classical Cepheids in the infrared bands J, H, and Ks.",
        "ori-fast-z-score": 1.25,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 1.8325416653445783
    },
    {
        "original_text": "The influence of the oxygen partial pressure (pO(2)) during deposition on the structure, electrical conductivity and magnetization in cobalt-doped titanium dioxide thin films is investigated by X-ray diffraction (XRD), scanning electron microscopy (SEM) and vibrating sample magnetometry (VSM). The XRD patterns show that all samples are amorphous at room temperature but crystallize into anatase phase after annealing for 1 hour at 500 °C. It was found that pO(2) has an effect on the grain size as well as the lattice parameters of the deposited films. The resistivity decreases with increasing pO(2) while the saturation magnetization increases. These results suggest that the doping level can be controlled by changing the pO(2) value. \n \n Keywords: Cobalt-doped titanium dioxide film; Oxygen partial pressure; Structural property; Electrical conductivity; Magnetization",
        "watermark_text": "The impact of the oxygen partial pressure ( pO ( 2 ) ) during deposition on the composition , electrical conductivity and magnetization in cobalt - doped titanium dioxide narrow bands is investigated by X - ray diffraction ( XRD ) , scanning electron microscopy ( SEM ) and vibrating sample magnetometry ( VSM ) . The XRD patterns indicate that all specimens are amorphous at room temperature but crystallize into anatase phase after annealing for 1 hour at 500 °C .It was shown that pO ( 2 ) has an influence on the grain size as well as the lattice parameters of the deposited films . The resistivity decreases with expanding pO ( 2 ) while the saturation magnetization increases .These data suggest that the doping rate can be governed by varying the pO ( 2 ) value . Keywords : Cobalt - doped titanium oxygen film ; Oxygen partial pressure ; Structural property ; Electrical conductivity ; Magnetization",
        "rewrite_text": "The effects of oxygen partial pressure (pO₂) during deposition on the composition, electrical conductivity, and magnetization of cobalt-doped titanium dioxide thin films are examined using X-ray diffraction (XRD), scanning electron microscopy (SEM), and vibrating sample magnetometry (VSM). The XRD results reveal that all samples are amorphous at room temperature but crystallize into the anatase phase after being annealed for one hour at 500 °C. It was found that pO₂ impacts both the grain size and the lattice parameters of the deposited films. As pO₂ increases, resistivity decreases, while saturation magnetization rises. These findings indicate that the doping rate can be adjusted by modifying the pO₂ levels. \n\nKeywords: Cobalt-doped titanium dioxide film; Oxygen partial pressure; Structural properties; Electrical conductivity; Magnetization.",
        "ori-fast-z-score": -1.4744195615489712,
        "water-fast-z-score": 3.243723035407737,
        "rewrite-fast-z-score": -0.7453559924999299
    },
    {
        "original_text": "The polaron problem is one of the most important problems in condensed matter physics, and has been studied extensively for many years.  In this work we present an overview of some recent results on path integral methods applied to the su(2)-schrieffer-heeger (s-shh) model with periodic boundary conditions.   We first review how the s-shh hamiltonian can be written as a sum over spinless fermions using the Jordan-Wigner transformation.  Then we discuss how the partition function may be evaluated by performing a trace over all possible states of these fermions.  Finally, we show that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to calculate various physical quantities such as the energy spectrum or correlation functions. The polaron problem is one o fthe most important problems in condensate matter physics, and has b een studied extensively for many years  1  . It describes a single electron moving through a lattice of atoms interacting via phonons  2  , where the electron-phonon interaction leads to the formation of a bound state known as a polaron  3  .\nIn this work w epresent an overview of some recent resul ts on path integral m ethods applied t o th e su(2)-schr iefer -heeg er (s-shh ) model  4  wit h p eriodic bo undary condit ions  5  .  W e first r evie w ho w th e shh h amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan-Wign er transfor mat ion  6  .  Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at es of th ese fermi ons.  Fina ll y, we sho w tha t th e resul tin g ex pressio n ca n be rewrite n in term s of Feyn man di agrams wh ich ar e th en u",
        "watermark_text": "The polaron problem is one of the most important problems in condensed matter theory , and has been studied thoroughly for hundreds years . In this research we present an overview of some latest findings on path integral methods applied to the su ( 2 ) - schrieffer - heeger ( s - shh ) model with periodic boundary constraints .We first review how the s - shh hamiltonian can be written as a sum over spinless fermions using the Jordan - Wigner transformation . Then we explain how the partition function could be evaluated by performing a trace over all possible states of these fermions .Finally , we prove that the resulting expression can be rewritten in terms of Feynman diagrams which are then used to estimate various physical components such as the power spectrum or correlation functions . The polaron problem is one o fthe most important problems in condensate matter mechanics , and has b een discussed heavily for many years 1 .It describes a single electron moving through a lattice of atoms interacting via phonons 2 , where the electron - phonon interaction results to the formation of a bound state known as a polaron 3 . In this study w epresent an overview of some latest resul ts on path integral m ethods applied t o th e su ( 2 ) - schr iefer - heeg er ( s - shh ) model 4 wit n p eriodic bo undary condit ions 5 .W e first r evie w ho w th e shh n amiltonia n ca n be wr it ten as a sum ov er sp inl ess fermi ons usin g th e J ordan - Wign er transfor mat ion 6 . Th en we discu ss how th e partiti on functi on m ay be evalua ted by perform ing a tr ace ov er al l possibl e st at en of th ese fermi ons .Fina ll y , we sho l tha t th e resul tin g ex pressio n ca n be rewrite n in word s of Feyn man di agrams wh ich ar e th en u",
        "rewrite_text": "The polaron problem is a key issue in condensed matter theory that has been extensively explored for centuries. In this research, we provide an overview of recent advancements in path integral methods applied to the SU(2) - Schrieffer - Heeger (S-SHH) model with periodic boundary conditions. We begin by reviewing how the S-SHH Hamiltonian can be expressed as a sum of spinless fermions utilizing the Jordan-Wigner transformation. Next, we outline the process of evaluating the partition function by tracing over all possible states of these fermions. Finally, we demonstrate that the resulting expression can be reformulated in terms of Feynman diagrams, which are then employed to estimate various physical quantities such as the power spectrum and correlation functions. The polaron problem, which describes the behavior of a single electron moving through a lattice of atoms with phonon interactions, leading to the formation of a bound state known as a polaron, has been a significant topic of discussion for many years. In this study, we aim to present the latest results concerning path integral methods within the framework of the SU(2) - Schrieffer - Heeger model under periodic boundary constraints. We first review the representation of the SHH Hamiltonian as a sum of spinless fermions via the Jordan-Wigner transformation. Subsequently, we discuss how the partition function can be computed by taking a trace over all possible states of these fermions. Ultimately, we demonstrate that the derived expression can be recast in terms of Feynman diagrams, which are then utilized to analyze various physical components.",
        "ori-fast-z-score": 1.4419211804559506,
        "water-fast-z-score": 5.448041796855991,
        "rewrite-fast-z-score": 3.0545207623093087
    },
    {
        "original_text": "The purpose of this article is to give an overview of the theory of weight structures on triangulated categories developed by A. Bondal and M. Kapranov.  We will explain how it can be applied to construct new cohomology theories for algebraic varieties over finite fields or number fields.   In particular we will discuss the construction of motivic cohomology using weight structures on derived categories of mixed Tate motives.    The main results are due to J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, and others. This article was written as part of the author s Ph.D thesis at Utrecht University under supervision of Prof. Dr. Wim van der Kallen. It has been published online by the author with permission of the supervisor. For more information about the content see the introduction below.",
        "watermark_text": "The purpose of this page is to give an overview of the notion of weight spaces on triangulated categories established by A . Bondal and M . Kapranov . We will explain how it can be applied to build modern cohomology theories for algebraic fields over arbitrary fields or number fields .In particular we will explore the formation of motivic cohomology involving weight forms on derived categories of mixed Tate motives . The main results are due to J . Ayoub , D . Gaitsgory , R . Hain , S . Katzarkov , V . Lafforgue , C . Soulé , B . Stienstra , and others .This page was written as part of the writer s Ph . D degree at Utrecht University under supervision of Prof . Dr . Wim van der Kallen . It has been publication online by the writer with authorization of the supervisor .For more information about the content read the introduction below .",
        "rewrite_text": "This page aims to provide an overview of weight spaces in triangulated categories, a concept developed by A. Bondal and M. Kapranov. We will discuss how this framework can be utilized to construct contemporary cohomology theories for algebraic fields over general fields or number fields. Specifically, we will investigate the construction of motivic cohomology that involves weight forms within the derived categories of mixed Tate motives. Key findings in this area have been attributed to researchers such as J. Ayoub, D. Gaitsgory, R. Hain, S. Katzarkov, V. Lafforgue, C. Soulé, B. Stienstra, among others. This content was created as part of the author’s Ph.D. studies at Utrecht University, under the guidance of Prof. Dr. Wim van der Kallen, and has been published online with the supervisor's approval. For further details, please refer to the introduction below.",
        "ori-fast-z-score": -0.42008402520840293,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": 0.9801960588196068
    },
    {
        "original_text": "We present the results of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range 8-40 M⊙, which are expected to produce GRB jets that can be observed at cosmological distances. We find that these systems evolve into double-degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae. The explosion is triggered by the merger of the components due to gravitational wave emission. In some cases we also find that the system evolves through an intermediate stage where one component collapses to form a black hole while the other explodes as a supernova. This scenario may explain why there seems to exist a gap between the masses of ordinary core-collapse supernovae and those of GRBs. Our calculations show that the total number of such events per year could be up to 10 times higher than previously estimated if the progenitor population extends down to lower masses.",
        "watermark_text": "We present the conclusion of our study on binary models for gamma - ray bursts ( GRBs ) with progenitors in the mass range 8 - 40 [UNK] , which are expected to produce GRB jets that can be observed at cosmological distances . We say that these systems evolve into double - degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae .The explosion is caused by the merger of the parts due to gravitational wave radiation . In some cases we also find that the system evolves through an intermediate stage where one element collapses to form a black hole while the other explodes as a supernova .This scenario could explain why there seems to exist a gap between the masses of normal core - collapse supernovae and those of GRBs . Our calculations show that the total number of such events per year could be up to 10 twice higher than previously predicted if the progenitor number extends down to smaller masses .",
        "rewrite_text": "We present the findings of our research on binary models for gamma-ray bursts (GRBs) originating from progenitors with masses between 8 and 40 [UNK]. These models suggest that such systems evolve into double-degenerate binaries composed of two white dwarfs or helium stars before eventually exploding as supernovae. The explosion is triggered by the merger of the components, driven by gravitational wave radiation. In certain cases, our analysis indicates that the system may pass through an intermediate phase where one component collapses to form a black hole while the other undergoes a supernova explosion. This scenario helps explain the observed mass gap between typically collapsing supernovae and GRBs. Additionally, our calculations suggest that the annual occurrence of these events could be as much as twice the previously estimated figures if the progenitor masses include lower mass ranges.",
        "ori-fast-z-score": -0.12216944435630522,
        "water-fast-z-score": 3.298574997620241,
        "rewrite-fast-z-score": 0.9701425001453319
    },
    {
        "original_text": "We present an analysis of the neutral hydrogen (HI) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100-m telescope to study the dark matter content of our Galaxy. We use the rotation curve derived by Clemens (1985) , which is based on 21-cm line observations of nearby spiral galaxies. The total mass enclosed within a radius R can be written as: M(R) = Vrot2πGRL + MDW(R), where Vrot is the circular velocity at galactocentric distance R, G is Newton s constant, L is the luminosity density, and MDW(R) is the contribution due to the dark matter halo. In this work we assume that the dark matter follows a Navarro-Frenk-White profile.  Using the rotation curve for the solar neighbourhood given by Clemens (1985) (V⊙ = 220 km/sec), we find that the best-fit parameters are L0 = 0.0013 Msun/pc3 and r0 = 1 kpc. This implies that the local surface brightness ΣL = L/L0 = 3.6 × 10^−26 W/m2/Hz/sr. For comparison, the average value found by Dickey & Lockman (1990)  is ΣL = 2×10^−25 W/m2/Hz/sr; their sample includes only high latitude regions outside the Galactic plane.",
        "watermark_text": "We present an analysis of the neutral hydrogen ( HI ) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100 - m observatory to study the dark matter content of our Galaxy . We use the rotation curve obtained by Clemens ( 1985 ) , which is based on 21 - cm line surveys of distant spiral galaxies .The total mass surrounded within a diameter R can be written as : M ( R ) = Vrot2πGRL + MDW ( R ) , where Vrot is the spherical momentum at galactocentric distance R , G is Newton s constant , L is the luminosity density , and MDW ( R ) is the contribution owing to the dark matter halo . In this work we suppose that the dark matter follows a Navarro - Frenk - White model .Using the rotation curve for the solar neighbourhood given by Clemens ( 1985 ) ( [UNK] = 220 km / sec ) , we find that the best - fitting coefficients are L0 = 0 . 0013 Msun / pc3 and r0 = 1 kpc . This implies that the local surface brightness ΣL = L / L0 = 3 . 6 × 10 ^ −26 W / m2 / Hz / sr .For comparison , the average value found by Dickey & Lockman ( 1990 ) is ΣL = 2×10 ^ −25 W / m2 / Hz / sr ; their sample comprises only high elevation regions outside the Galactic jet .",
        "rewrite_text": "We present an analysis of neutral hydrogen (HI) emission captured by the Westerbork Synthesis Radio Telescope and the Effelsberg 100-meter observatory to investigate the dark matter content of our Galaxy. Utilizing the rotation curve derived by Clemens (1985), based on 21-cm line surveys of distant spiral galaxies, we can express the total mass within a radius R as: M(R) = V_rot² / (2πG)R + M_DW(R), where V_rot represents the rotational velocity at the galactocentric distance R, G is Newton's gravitational constant, L is the luminosity density, and M_DW(R) accounts for the dark matter halo contribution. In this study, we assume the dark matter follows the Navarro-Frenk-White model. Applying Clemens' (1985) rotation curve for the solar neighborhood (where V_rot = 220 km/s), we determine the best-fitting parameters to be L_0 = 0.0013 M_sun/pc³ and r_0 = 1 kpc. This yields a local surface brightness of Σ_L = L / L_0 = 3.6 × 10^-26 W/m²/Hz/sr. For comparison, Dickey & Lockman (1990) reported an average value of Σ_L = 2 × 10^-25 W/m²/Hz/sr in their study, which focused on high-elevation regions outside the Galactic jet.",
        "ori-fast-z-score": 0.7276068751089989,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 1.0533703247651751
    },
    {
        "original_text": "We present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton . We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 .The best - fitting values are compatible within errors to those identified previously used Chandra data alone . No meaningful spectral changes were detected between various epochs separated by many months separated .However , we do discover powerful flux variations at all energies during our observation term . In particular , there was a factor of 3 shift in the hard band count rate over about 20 ks followed by a slower decay forward towards the first rate .This phenomenon can be understood if the source has been caught in a transfer state where the accretion disk luminosity increased rapidly due to some distortion or perturbation .",
        "rewrite_text": "We present new findings on the X-ray spectrum and variability characteristics of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. Our analysis indicates that the 0.5 - 10 keV continuum is accurately described by an absorption power law, with a slope of Γ = 2.1 ± 0.2 (χ²/dof = 111/101), along with a Compton reflection component modeled by PEXRAV with R = 0.7 - 1.0 and NH = 10 - 23 × 10²² cm⁻². These optimal fitting parameters are consistent within the associated uncertainties with those determined from prior Chandra data. We did not observe any significant spectral changes across different epochs, which were spaced months apart. However, we did detect substantial flux variations across all energy levels during our observational period. Notably, there was a threefold increase in the hard band count rate over approximately 20 ks, followed by a gradual decay back to the initial rate. This behavior may suggest that the source was observed during a transitional phase where the luminosity of the accretion disk increased rapidly due to some form of disturbance or perturbation.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 0.3611575592573076
    },
    {
        "original_text": "We present the results of an analysis aimed at improving the stellar parameters for the host star of planet TrES-2, as well as its planetary system properties. We use high-precision photometry obtained with the MOST satellite to derive new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD) and radius ratio (Rp/Rs = 0.11). These are combined with existing radial velocity data in order to refine the mass estimates for both components of this double-lined spectroscopic binary. Our best-fit model yields masses of 1.06 ± 0.04M⊙ and 0.84 ± 0.03M⊙ for the primary and secondary stars respectively, along with radii of 1.16 ± 0.02R⊙ and 0.91 ± 0.01R⊙ . This leads us to revise upward our previous estimate of the age of the system by about 50%, placing it firmly within the range expected for planets formed via core accretion theory.",
        "watermark_text": "We present the conclusion of an assessment aimed at enhancing the stellar characteristics for the host star of planet TrES - 2 , as well as its planetary system properties . We use large - precision photometry obtained with the MOST satellite to derive new values for the orbital period ( P = 3 . 819 days ) , transit epoch ( T0 = 2454000 MJD ) and radius ratio ( Rp / Rs = 0 . 11 ) .These are coupled with existing radial speed data in order to refine the mass estimates for both components of this double - lined spectroscopic binary . Our best - fitting model gives masses of 1 . 06 ± 0 . [UNK] and 0 . 84 ± 0 . [UNK] for the primary and secondary stars respectively , along with radii of 1 . 16 ± 0 . [UNK] and 0 . 91 ± 0 . [UNK] .This leads us to revise upward our previous estimate of the age of the system by about 50 % , placing it firmly within the range assumed for planets formed via nucleus accretion theory .",
        "rewrite_text": "We present the findings of an assessment focused on improving the stellar characteristics of the host star of planet TrES-2, along with the properties of its planetary system. Utilizing high-precision photometry from the MOST satellite, we calculated new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD), and radius ratio (Rp/Rs = 0.11). These findings were combined with existing radial velocity data to refine the mass estimates for both stars in this double-lined spectroscopic binary. Our best-fitting model yields masses of 1.06 ± 0.XX for the primary star and 0.84 ± 0.XX for the secondary star, with corresponding radii of 1.16 ± 0.XX and 0.91 ± 0.XX. As a result, we have revised our previous estimate of the system's age upwards by approximately 50%, placing it firmly within the range expected for planets formed through core accretion theory.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 1.9694638556693236
    },
    {
        "original_text": "We study the evolution of complex networks with multiple modules, where each module is an Erdős-Rényi random graph and all nodes are connected to one another within their own module but not across different modules. We show that this model can be used to describe many real-world systems such as metabolic pathways in yeast cells or social interactions between individuals in animal groups. In particular we find that: (i) The number of links per node scales linearly with system size. (ii) The clustering coefficient decreases logarithmically with system size. (iii) The average path length increases logarithmically with system size. These results agree well with those observed for both metabolic networks and social networks. Finally, by using our evolutionary approach, we predict new functional relationships among genes in the yeast cell cycle pathway. Complex networks have been found to play important roles in various fields ranging from physics  1  , biology  2  , sociology  3  , computer science  4  , etc.. Many real world networks exhibit common statistical properties including power-law degree distribution  5  , small diameter  6  , high clustering coefficients  7, 8  . However, it remains unclear how these networks evolve over time  9  .\nIn recent years there has been growing interest in studying the evolution of complex networks  10 -12  . For example, Barabási-Albert proposed a simple growth mechanism which leads to scale-free networks  13  . Dorogovtsev et al studied the evolution of hierarchical networks  14  . Caldarelli et al investigated the evolution of clustered networks  15  . Newman introduced a fitness-based model  16  . This model was further developed into a more realistic version  17  . Recently, Jeong et al showed that some metabolic networks share similar topological features  18  . They also suggested that the underlying mechanisms responsible for generating these networks may be related to natural selection  19  .",
        "watermark_text": "We research the evolution of complex networks with many modules , where each module is an Erdős - Rényi random graph and all nodes are connected to one another within their own module but not across different modules . We see that this model can be used to explain much actual - time systems such as metabolic processes in bacterial cells or cultural relationships between individuals in livestock groups .In particular we find that : ( i ) The amount of links per node scales linearly with system size . ( ii ) The clustering density decreases logarithmically with system height .( iii ) The estimated path length changes logarithmically with system width . These conclusions follow well with those observed for both metabolic networks and social systems .Finally , by using our evolutionary approach , we estimate new functional links among genes in the yeast cell cycle pathway . Complex networks have been shown to hold important roles in different fields ranging from science 1 , chemistry 2 , anthropology 3 , computer science 4 , etc . .Many actual world networks demonstrate familiar mathematical properties including power - law degree function 5 , low diameter 6 , large clustering coefficients 7 , 8 . However , it remains unsure how these networks develop over time 9 .In recent years there has been growing interest in investigating the evolution of complex networks 10 - 12 . For instance , Barabási - Albert proposed a simple growth mechanism which results to scale - free networks 13 .Dorogovtsev et al studied the evolution of hierarchical networks 14 . Caldarelli et al investigated the evolution of clustered systems 15 .Newman added a fitness - based model 16 . This model was further developed into a more realistic version 17 .Recently , Jeong et al showed that some molecular systems carry similar topological features 18 . They even suggested that the underlying mechanisms involved for generating these networks could be connected to natural selection 19 .",
        "rewrite_text": "We explore the evolution of complex networks composed of multiple modules, with each module represented as an Erdős-Rényi random graph. Within each module, all nodes are interconnected, while there is no connection between nodes in different modules. Our model effectively describes various real-time systems, such as metabolic processes in bacterial cells and cultural interactions among individuals in livestock groups. Specifically, we observe the following findings: (i) the number of links per node increases linearly with system size, (ii) clustering density decreases logarithmically as system height increases, and (iii) estimated path length varies logarithmically with system width. These findings are consistent with observations from both metabolic networks and social systems. Additionally, our evolutionary framework allows us to estimate new functional links between genes involved in the yeast cell cycle pathway. Complex networks play significant roles across diverse disciplines, including science, chemistry, anthropology, and computer science. Many real-world networks exhibit well-known mathematical characteristics, such as power-law degree distributions, low diameters, and high clustering coefficients. However, the temporal development of these networks remains uncertain. In recent years, there has been an increased interest in understanding the evolution of complex networks. For instance, Barabási and Albert proposed a straightforward growth mechanism leading to scale-free networks. Dorogovtsev and colleagues examined the evolution of hierarchical networks, while Caldarelli and others studied clustered systems. Newman introduced a fitness-based model, which was later refined into a more realistic version. Recently, Jeong and associates demonstrated that some molecular systems share similar topological features, suggesting a potential link between the mechanisms generating these networks and the process of natural selection.",
        "ori-fast-z-score": -0.23942606534028665,
        "water-fast-z-score": 7.747580267412529,
        "rewrite-fast-z-score": 2.38667185252719
    },
    {
        "original_text": "We present the results of an automated search for star cluster candidates (SCCs) using deep HST/ACS images covering most of the disk and halo of M33, obtained as part of program GO-10229. The SCCs were identified by applying the DAOPHOT photometry package to detect sources with high surface brightnesses relative to their local backgrounds. We then applied several selection criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles to identify bona fide SCCs among these bright objects. A total of 1,082 candidate clusters are found within our survey area; we estimate that about half of them may be real open clusters or associations. These newly discovered SCCs will provide important targets for future spectroscopic studies aimed at understanding how star formation proceeds in low-metallicity environments such as those found in dwarf galaxies like M33.",
        "watermark_text": "We present the results of an automated search for star cluster applicants ( SCCs ) using deep HST / ACS images covering most of the disk and halo of M33 , obtained as part of program GO - 10229 . The SCCs were discovered by using the DAOPHOT photometry program to identify sources with high surface brightnesses compared to their nearby backgrounds .We then utilized numerous selection standards based on color - magnitude diagrams ( CMDs ) , luminosity functions ( LFs ) , and radial profiles to identify bona fide SCCs among these bright objects . A total of 1 , 082 candidate clusters are found within our survey area ; we estimate that about half of them may be genuine open complexes or associations .These newly discovered SCCs will provide important candidates for future spectroscopic studies aimed at studying how star formation operates in low - metallicity habitats such as those observed in dwarf stars like M33 .",
        "rewrite_text": "We present the findings of an automated search for star cluster candidates (SCCs) utilizing deep HST/ACS images that cover a substantial portion of the disk and halo of M33, collected as part of program GO-10229. The SCCs were identified using the DAOPHOT photometry program to locate sources with high surface brightnesses relative to their surrounding backgrounds. We applied a variety of selection criteria based on color-magnitude diagrams (CMDs), luminosity functions (LFs), and radial profiles to distinguish genuine SCCs from these bright objects. Our survey area yielded a total of 1,082 candidate clusters, with estimates suggesting that around half may be real open complexes or associations. The newly identified SCCs will serve as significant candidates for future spectroscopic investigations aimed at understanding star formation dynamics in low-metallicity environments, such as those present in dwarf galaxies like M33.",
        "ori-fast-z-score": 0.9701425001453319,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "We present new constraints on the redshift distribution of sources contributing to the source subtracted near-infrared background (SSNIB). We use deep Spitzer/MIPS 24 micron data in combination with optical and infrared photometry, including GALEX NUV imaging, to select galaxies at z > 1.5 over an area of 0.6 deg2 centered around the Lockman Hole East field. The resulting sample consists of 16,000 objects selected between redshifts 2<z<8. Using this sample we measure the evolution of the luminosity function out to high redshifts by fitting Schechter functions to our observed number counts as a function of flux density binned into bins of width ∆logS = 0.1 dex. Our results are consistent with previous studies that find evidence for strong luminosity evolution up to z ~ 3 followed by little or no evolution beyond this point. \n \n We then fit models to these measurements using Monte Carlo simulations which include contributions from both obscured AGNs and normal star forming galaxies. These fits show that the majority of the SSNIB is produced by faint galaxies at low redshifts (0.3 < z < 1) while bright galaxies dominate at higher redshifts (4 < z < 6). \n \n Finally, we compare our best-fit model predictions to existing observations of the unresolved extragalactic background light (EBL), finding good agreement within uncertainties.",
        "watermark_text": "We introduce novel constraints on the redshift flow of sources leading to the source subtracted near - infrared background ( SSNIB ) . We use deep Spitzer / MIPS 24 micron data in combination with optical and infrared photometry , using GALEX NUV photography , to select clusters at z > 1 . 5 over an area of 0 . 6 deg2 centered around the Lockman Hole East field .The resulting survey consists of 16 , 000 particles choose between redshifts 2 < z < 8 . Using this specimen we measure the evolution of the luminosity function out to large redshifts by fitting Schechter functions to our measured number counts as a function of flux density binned into bins of width [UNK] = 0 . 1 dex .Our results are compatible with previous analyses that provide evidence for strong luminosity progression up to z ~ 3 followed by much or no development beyond this point . We then fitting models to these measurements using Monte Carlo simulations which contain contributions from both distorted AGNs and normal star producing galaxies .These fits indicate that the majority of the SSNIB is produced by weak nuclei at low redshifts ( 0 . 3 < z < 1 ) while bright clusters influence at higher redshifts ( 4 < z < 6 ) . Finally , we compare our better - fitting model observations to existing observations of the unresolved extragalactic background light ( EBL ) , finding high agreement within uncertainties .",
        "rewrite_text": "We present new constraints on the redshift flow of sources that contribute to the Source Subtracted Near-Infrared Background (SSNIB). Utilizing deep Spitzer/MIPS 24 micron data alongside optical and infrared photometry, including GALEX NUV imagery, we select clusters at redshifts greater than 1.5 within an area of 0.6 deg² centered on the Lockman Hole East field. Our survey comprises 16,000 sources across redshifts ranging from 2 to 8. With this dataset, we assess the evolution of the luminosity function at high redshifts by fitting Schechter functions to the measured number counts as a function of flux density, organized into bins of 0.1 dex width. Our findings align with previous studies indicating a significant increase in luminosity up to z ~ 3, followed by little to no evolution thereafter. Additionally, we fit models to these data using Monte Carlo simulations that incorporate contributions from both faint active galactic nuclei (AGNs) and typical star-forming galaxies. The results suggest that the bulk of the SSNIB emanates from faint nuclei at lower redshifts (0.3 < z < 1), while brighter clusters have a more substantial impact at higher redshifts (4 < z < 6). Finally, we compare our best-fitting model to existing observations of the unresolved extragalactic background light (EBL), finding strong agreement within the margins of uncertainty.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 1.7056057308448833
    },
    {
        "original_text": "We have obtained deep optical spectra for eight QSOs with known redshifts in the range 0.4-0.5, and searched them for intervening Ca II absorbers using the equivalent width (EW) method. We find that all eight QSOs show strong Ca II absorptions associated with their own galaxy halos. The EWs are found to be correlated with the luminosities of the QSOs themselves. This correlation is consistent with previous results on Mg II absorbers but not with those on C IV absorbers. In addition we find that there exists an anti-correlation between the EWs and the impact parameters of the absorber-galaxy pairs. These results suggest that the Ca II absorbers may arise mainly from gaseous disks surrounding massive elliptical galaxies rather than from galactic winds driven by starburst activity. \n \n \n \n Keywords: Absorber-galaxy pairs; Galaxy halo; Quasar; Redshift",
        "watermark_text": "We have recovered deep optical spectra for eight QSOs with reported redshifts in the range 0 . 4 - 0 . 5 , and explored them for intervening Ca II absorbers using the equivalent thickness ( EW ) method . We see that all eight QSOs exhibit strong Ca II absorptions associated with their own galaxy halos .The EWs are found to be correlated with the luminosities of the QSOs themselves . This correlation is compatible with previous findings on Mg II absorbers but not with those on C IV absorbers .In addition we find that there exists an counter - correlation between the EWs and the impact factors of the absorber - star pairs . These conclusions propose that the Ca II absorbers may arise predominantly from gaseous disks surrounding massive elliptical galaxies rather than from galactic winds driven by starburst activity .Keywords : Absorber - star pairs ; Galaxy halo ; Quasar ; Redshift",
        "rewrite_text": "We have obtained deep optical spectra for eight QSOs with redshifts between 0.4 and 0.5, and investigated them for intervening Ca II absorbers using the equivalent width (EW) method. Our analysis reveals that all eight QSOs display prominent Ca II absorptions linked to their own galactic halos. We observe a correlation between the EWs and the luminosities of the QSOs, which aligns with earlier studies on Mg II absorbers but differs from the findings related to C IV absorbers. Furthermore, we discover a counter-correlation between the EWs and the impact parameters of the absorber-star pairs. These results suggest that the Ca II absorbers are primarily associated with gaseous disks surrounding massive elliptical galaxies, rather than being driven by starburst-related galactic winds. \n\nKeywords: Absorber-star pairs; Galaxy halo; Quasar; Redshift",
        "ori-fast-z-score": -0.14907119849998599,
        "water-fast-z-score": 5.515634344499481,
        "rewrite-fast-z-score": -0.6172133998483676
    },
    {
        "original_text": "We propose an alternative switching mechanism for spintronic devices based on domain walls (DWs). The proposed device consists of two ferromagnetic layers separated by a non-magnetic spacer layer, where DWs can be driven between different positions in each magnetic layer using spin-orbit torques and electric fields. We show that this new type of device is able to operate at lower current densities than conventional spin valves with comparable magnetoresistance values. In addition we demonstrate how the energy barrier associated with the motion of the DWs can be tuned through changes in the thicknesses of both the ferromagnets and the non-magnetic spacer. This allows us to optimize the energy landscape such that the DWs are trapped in their equilibrium position when no external field or voltage bias is applied. Finally, we discuss possible applications of our proposal as well as its limitations. Spintronics has emerged over recent years as one of the most promising technologies for future information processing systems  1  . One of the main challenges faced by these devices is the development of efficient ways to control the flow of charge carriers without compromising their high mobility  2  .\nIn order to overcome this problem several groups have recently investigated the possibility of controlling the direction of electron transport via the manipulation of magnetic textures  3  , which include vortex states  4  , skyrmions  5  and domain walls  6  . Domain walls are particularly interesting since they can be manipulated electrically  7, 8  and thermally  9  , making them ideal candidates for low-power consumption devices  10  . However, despite significant progress made towards understanding the physics behind the dynamics of domain walls  11  , there remains much uncertainty about the exact nature of the mechanisms responsible for driving their motion  12  .",
        "watermark_text": "We suggest an additional switching method for spintronic systems relying on domain barriers ( DWs ) . The proposed system consists of two ferromagnetic layers divided by a non - magnetic spacer membrane , where DWs can be pushed between various positions in each magnetic layer involving spin - orbit torques and electric forces .We suggest that this new kind of device is could to work at lower current densities than conventional spin tubes with similar magnetoresistance ratings . In addition we prove how the electricity barrier associated with the movement of the DWs can be tuned through variations in the thicknesses of both the ferromagnets and the non - magnetic spacer .This enables us to optimize the electricity landscape such that the DWs are locked in their equilibrium place when no external field or voltage bias is applied . Finally , we study possible users of our proposal as well as its limitations .Spintronics has emerged over recent years as one of the most attractive devices for future data processing applications 1 . One of the main problems faced by these machines is the development of effective means to manage the transfer of charge carriers without compromising their high mobility 2 .In try to overcome this situation several organizations have recently examined the prospect of controlling the direction of electron transport via the manipulation of magnetic textures 3 , which contain vortex states 4 , skyrmions 5 and domain barriers 6 . Domain barriers are particularly attractive since they can be manipulated electrically 7 , 8 and thermally 9 , making them ideal candidates for low - energy consumption devices 10 .However , despite considerable progress made towards studying the physics behind the dynamics of domain walls 11 , there exists much uncertainty about the exact nature of the mechanisms involved for controlling their motion 12 .",
        "rewrite_text": "We propose a novel switching technique for spintronic systems that utilizes domain walls (DWs). The proposed setup features two ferromagnetic layers separated by a non-magnetic spacer membrane, allowing DWs to be maneuvered between various positions in each magnetic layer through the application of spin-orbit torques and electric forces. We suggest that this innovative device could operate at lower current densities compared to conventional spintronic devices with similar magnetoresistance characteristics. Moreover, we demonstrate that the electrical barrier associated with DW movement can be adjusted by altering the thickness of both the ferromagnetic layers and the non-magnetic spacer. This adjustment allows us to optimize the electrical landscape, ensuring that the DWs remain fixed in their equilibrium positions when no external field or voltage bias is applied. Additionally, we explore potential applications for our proposal and its associated limitations. Spintronics has gained significant attention in recent years as a promising avenue for future data processing technologies. A critical challenge faced by these systems is the development of efficient mechanisms for managing charge carrier transfer without sacrificing their high mobility. To address this issue, several organizations have recently investigated the possibility of controlling electron transport direction by manipulating magnetic textures, which include vortex states, skyrmions, and domain walls. Domain walls, in particular, are appealing due to their ability to be manipulated electrically and thermally, making them suitable candidates for low-energy consumption devices. However, despite significant advancements in understanding the physics of domain wall dynamics, there remains considerable uncertainty regarding the precise mechanisms governing their motion.",
        "ori-fast-z-score": -0.5980503604017327,
        "water-fast-z-score": 8.629012342939285,
        "rewrite-fast-z-score": 1.0215078369104984
    },
    {
        "original_text": "The quantum group of isometries (QGI) was introduced by Connes as the universal object for deformations of classical groups, which are obtained via spectral triples on commutative C*-algebras.  In this talk we will discuss how to define QGI s using noncommutative geometry techniques such as operator algebras and von Neumann algebras.   We will also explain how these objects can be used to study the classification problem of Riemannian manifolds with positive scalar curvature. The Quantum Group of Isometries (QGI), first defined by Alain Connes, plays an important role in both classical and noncommutative geometry. It is the universal object for deforming classical Lie groups into their corresponding quantum groups. This talk will give an introduction to QGI’s and show that they can be studied through operator algebra theory and von Neumann algebras. Finally it will present some results about the classification problem of Riemannain manifolds with positive scalar curvatures.",
        "watermark_text": "The quantum group of isometries ( QGI ) was introduced by Connes as the universal object for deformations of classical groups , which are derived via spectral triples on commutative C * - algebras . In this talk we will explore how to define QGI s using noncommutative geometry tools such as operator algebras and von Neumann algebras .We will also explain how these objects can be used to study the characterization question of Riemannian manifolds with positive scalar curvature . The Quantum Group of Isometries ( QGI ) , initially established by Alain Connes , holds an important role in both classical and noncommutative geometry .It is the fundamental object for deforming classical Lie fields into their resulting quantum groups . This discussion will provide an introduction to QGI ’ s and explain that they can be investigated through operator algebra analysis and von Neumann algebras .Finally it will present some results about the characterization question of Riemannain manifolds with positive scalar curvatures .",
        "rewrite_text": "The Quantum Group of Isometries (QGI) was introduced by Alain Connes as the universal entity for deforming classical groups, derived from spectral triples over commutative C*-algebras. In this presentation, we will investigate how to define QGIs using tools from noncommutative geometry, such as operator algebras and von Neumann algebras. We will also discuss how these concepts can aid in addressing the characterization of Riemannian manifolds with positive scalar curvature. QGI plays a crucial role in both classical and noncommutative geometry, serving as the foundational element for transforming classical Lie algebras into their quantum counterparts. This talk aims to provide an introduction to QGIs and elucidate their study through the framework of operator algebras and von Neumann algebras, while also presenting significant findings regarding the characterization question of Riemannian manifolds with positive scalar curvature.",
        "ori-fast-z-score": -0.2773500981126146,
        "water-fast-z-score": 4.437601569801833,
        "rewrite-fast-z-score": 2.1879748724684185
    },
    {
        "original_text": "We study the gravitational self-force acting on a wave packet in curved spacetime, and show that it causes its distortion.  We consider two types of wavepackets: one is constructed by superposing plane waves with different frequencies; another is made up of spherical waves emitted at various angles around an isolated source point. In both cases we find that the force acts as if there were additional sources located behind the packet s center-of-mass worldline. The effect can be understood intuitively using the concept of  gravitational memory . Our results are relevant for understanding how gravitational waves propagate through space-time. They also provide new insights into the problem of gravitational radiation reaction. Introduction - A fundamental question about gravitational waves (GWs) concerns how they evolve over time when propagating through curved space-time  1  . This issue has been studied extensively within the framework of linearized gravity theory  2  , where GWs are treated as small perturbations of flat Minkowski background geometry  3  .\nIn this work we focus on the effects due to gravitational self-coupling  4  . These arise because each part of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity  5  . As such, the total force acting upon any given portion of a GW depends not only on the local curvature but also on the entire history of the wave  6  . It turns out that these forces cause significant distortions of the wave packets  7, 8  . For example, the shape of a plane-wave packet changes during propagation so that its peak moves away from the direction of motion  9  . Similar behavior was found for spherical wave packets  10  .",
        "watermark_text": "We consider the gravitational self - force acting on a wave packet in curved spacetime , and find that it creates its degradation . We consider two forms of wavepackets : one is built by superposing plane waves with various frequencies ; another is made up of circular signals emitted at several angles around an exposed source point .In both cases we find that the force works as if there were extra sources located behind the packet s center - of - mass worldline . The phenomenon can be understood intuitively using the idea of gravitational memory .Our results are important for studying how gravity signals propagate through space - time . They also bring fresh insights into the question of gravitational waves reaction .Introduction - A basic issue about gravitational waves ( GWs ) concerns how they develop over time when propagating through curved space - time 1 . This problem has been studied thoroughly within the framework of linearized gravity physics 2 , where GWs are treated as low perturbations of flattened Minkowski background geometry 3 .In this research we focus on the effects due to gravitational self - interaction 4 . These occur because each portion of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity 5 .As such , the total stress acted upon any certain parts of a GW relies not only on the local curvature but also on the entire history of the signal 6 . It turns out that these forces cause significant distortions of the signal packets 7 , 8 .For instance , the morphology of a plane - wave packet shifts during propagation so that its peak changes away from the direction of movement 9 . Similar behavior was seen for spherical wave packets 10 .",
        "rewrite_text": "We examine the gravitational self-force acting on a wave packet in curved spacetime, discovering that it contributes to the degradation of that packet. We analyze two types of wave packets: one constructed by superimposing plane waves with varying frequencies, and another consisting of circular signals emitted at multiple angles from a source point. In both scenarios, we observe that the self-force behaves as if there were additional sources located behind the center of mass worldline of the packet. This phenomenon can be intuitively understood through the concept of gravitational memory. Our findings are significant for understanding how gravitational signals propagate through spacetime and offer new insights into the behavior of gravitational waves. \n\n**Introduction**: A fundamental question regarding gravitational waves (GWs) is how they evolve over time while traveling through curved spacetime. This issue has been extensively studied within the framework of linearized gravity, where GWs are treated as small perturbations to flat Minkowski geometry. In this research, we emphasize the effects of gravitational self-interaction. This self-interaction arises because each segment of a GW possesses energy density that exerts a pressure upon itself through Newtonian gravity. Consequently, the total stress experienced by particular parts of a GW is influenced not only by the local curvature but also by the entire history of the signal. These self-forces lead to significant distortions in the wave packets. For example, the shape of a plane-wave packet evolves during propagation, causing its peak to shift away from its direction of travel. A similar phenomenon has been observed in spherical wave packets as well.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.131727983645296,
        "rewrite-fast-z-score": 2.073840159735094
    },
    {
        "original_text": "We present an ab initio tight-binding model for calculating the optical properties of semiconductor nanocrystals, which is based on the solution of the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE describes excitonic effects and allows to calculate absorption spectra with high accuracy. We show that our approach reproduces experimental results very well. In particular we find good agreement between calculated and measured absorption cross sections at low energies where quantum confinement dominates over electron-hole exchange interactions. Our method can be applied to any type of semiconductor material including doped systems as well as core-shell structures. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes or solar cells due to their unique optoelectronic properties. However, it remains challenging to predict these properties accurately since they depend sensitively on the electronic structure of the system. Here we propose a new theoretical approach to tackle this problem by combining DFT calculations with the Bethe-Salpether equation (BSE), which takes into account excitonic effects beyond mean-field approaches like Kohn-Sham DFT. This enables us to obtain accurate predictions for the optical properties of semiconductor nanostructures.",
        "watermark_text": "We present an ab initio close - binding model for determining the optical properties of semiconductor nanocrystals , which is based on the solve of the Bethe - Salpeter equation ( BSE ) within the framework of density functional theory ( DFT ) . The BSE describes excitonic effects and allows to estimate absorption spectra with high sensitivity .We see that our approach reproduces experimental results very best . In particular we find good agreement between calculated and reported absorption cross sections at low energies where quantum confinement dominates over electron - hole exchange interactions .Our method can be applied to any type of semiconductor material especially doped systems as well as core - shell systems . Semiconductor nanocrystals are promising candidates for applications such as light - emitting diodes or solar cells due to their different optoelectronic properties .However , it remains challenging to predict these characteristics properly since they rely sensitively on the electronic structure of the device . Here we attempt a new theoretical technique to tackle this question by combining DFT calculations with the Bethe - Salpether equation ( BSE ) , which gives into consideration excitonic effects beyond mean - field methods like Kohn - Sham DFT .This enables us to obtain precise predictions for the optical properties of semiconductor nanostructures .",
        "rewrite_text": "We introduce an ab initio close-binding model designed to determine the optical properties of semiconductor nanocrystals. This model is founded on the resolution of the Bethe-Salpeter equation (BSE) within the density functional theory (DFT) framework. The BSE effectively captures excitonic effects, enabling a highly sensitive estimation of absorption spectra. Our results demonstrate that this approach closely mirrors experimental findings, particularly at low energies where quantum confinement prevails over electron-hole exchange interactions, showing excellent agreement between the calculated and measured absorption cross sections. Our methodology is applicable to various types of semiconductor materials, including doped and core-shell systems. Semiconductor nanocrystals are promising candidates for applications like light-emitting diodes and solar cells due to their unique optoelectronic properties. However, accurately predicting these characteristics poses a challenge, as they depend critically on the electronic structure of the materials. To address this challenge, we propose a novel theoretical strategy that merges DFT calculations with the Bethe-Salpeter equation. This integration allows us to account for excitonic effects that go beyond mean-field methods such as Kohn-Sham DFT, resulting in precise predictions for the optical properties of semiconductor nanostructures.",
        "ori-fast-z-score": 1.4596008983995234,
        "water-fast-z-score": 6.463946835769319,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "We present the results of our analysis on the polarization power spectrum in Bianchi type I cosmological models, which are anisotropic generalizations of standard FRW cosmologies. We find that there is no significant difference between the temperature fluctuations predicted by these two classes of models at large angular scales (low multipoles). However, we show that this is not true when one considers the polarization fluctuations. In particular, we demonstrate that the presence of an anisotropy parameter leads to a suppression of the low-l polarization power relative to the high-l part of the spectrum. This effect can be used as a test for distinguishing Bianchi type I models from their FRW counterparts. \n \n The observed lack of large-scale polarization in the WMAP data has been interpreted as evidence against inflationary scenarios with tensor perturbations. It was shown recently that such a conclusion may be premature if one takes into account possible deviations from statistical isotropy in the primordial universe. Indeed, it turns out that some anisotropic cosmological models predict less large-scale polarization than their isotropic counterparts do.",
        "watermark_text": "We present the conclusion of our analysis on the polarization power spectrum in Bianchi class I cosmological models , which are anisotropic generalizations of standard FRW cosmologies . We see that there is no major variation between the temperature fluctuations assumed by these two groups of models at large angular scales ( low multipoles ) .However , we prove that this is not true when one considers the polarization fluctuations . In particular , we prove that the presence of an anisotropy parameter causes to a suppression of the small - l polarization strength compared to the high - l part of the spectrum .This phenomenon can be used as a check for differentiate Bianchi class I theories from their FRW predecessors . The observed lack of large - scale polarization in the WMAP information has been viewed as proof against inflationary scenarios with tensor perturbations .It was shown lately that such a conclusion must be premature if one takes into consideration proposed deviations from statistical isotropy in the primordial universe . Indeed , it turns out that some anisotropic cosmological predictions predict less large - scale polarization than their isotropic counterparts do .",
        "rewrite_text": "We conclude our analysis of the polarization power spectrum within Bianchi class I cosmological models, which serve as anisotropic extensions of the standard Friedmann-Robertson-Walker (FRW) cosmologies. Our results indicate that at large angular scales (low multipoles), the temperature fluctuations for these two classes of models exhibit minimal differences. However, this trend does not hold when examining polarization fluctuations. Specifically, we demonstrate that the introduction of an anisotropy parameter leads to a reduction in the polarization strength at small multipoles compared to that at high multipoles. This effect can be leveraged to distinguish Bianchi class I theories from their FRW counterparts. The observed absence of large-scale polarization in WMAP data has been interpreted as a challenge to inflationary models featuring tensor perturbations. Recent findings suggest that this interpretation may be hasty, particularly when considering potential deviations from statistical isotropy in the early universe. Notably, some anisotropic cosmological models predict even lower levels of large-scale polarization than their isotropic alternatives.",
        "ori-fast-z-score": -0.47809144373375745,
        "water-fast-z-score": 5.498051602938211,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "We study the effects on electroweak precision observables (EWPO) due to new physics at the TeV scale, which is motivated by recent LHC results and theoretical arguments for naturalness. We consider two classes of models with extra dimensions: Randall-Sundrum (RS) warped space model and holographic technicolor (HTC). In RS model we find that the corrections are too large compared to EWPOs if the mass scales involved satisfy MPlanck ~ 5TeV. However, this problem can be solved by introducing an additional bulk scalar field whose VEV breaks custodial symmetry softly. The resulting correction to T parameter is found to be small enough even when MPlanck = 5TeV. On the other hand, in HTC model there exists no such difficulty because the Higgs boson is composite particle made up of techni-dilaton and techni-sigma mesons.",
        "watermark_text": "We research the effects on electroweak accuracy observables ( EWPO ) related to recent science at the TeV scale , which is prompted by recent LHC results and theoretical evidence for naturalness . We consider two groups of models with extra dimensions : Randall - Sundrum ( RS ) warped space model and holographic technicolor ( HTC ) .In RS model we find that the corrections are too huge compared to EWPOs if the mass scales concerned meet MPlanck ~ 5TeV . However , this situation can be answered by using an additional bulk scalar field whose VEV cuts custodial symmetry quietly .The resulting correction to T parameter is found to be small enough even when MPlanck = 5TeV . On the other hand , in HTC system there exists no such difficulty because the Higgs boson is composite particle making up of techni - dilaton and techni - sigma mesons .",
        "rewrite_text": "We investigate the impact of electroweak precision observables (EWPO) in light of recent discoveries at the TeV scale, influenced by findings from the LHC and theoretical considerations regarding naturalness. Our analysis focuses on two categories of models featuring extra dimensions: the Randall-Sundrum (RS) warped space model and holographic technicolor (HTC). In the RS model, we discover that the corrections significantly exceed the EWPOs if the relevant mass scales approach MPlanck ~ 5 TeV. However, this issue can be mitigated by introducing an additional bulk scalar field, whose vacuum expectation value (VEV) subtly breaks custodial symmetry. As a result, the correction to the T parameter remains sufficiently small, even with MPlanck set at 5 TeV. Conversely, in the HTC framework, there are no such complications, as the Higgs boson is a composite particle formed from techni-dilaton and techni-sigma mesons.",
        "ori-fast-z-score": -1.2602520756252087,
        "water-fast-z-score": 3.959797974644666,
        "rewrite-fast-z-score": -0.5443310539518174
    },
    {
        "original_text": "We present new high-resolution, near-infrared (NIR) spectra for the coolest known members of open clusters M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory. The observations were carried out to study the sodium doublet at λλ8183/8195 Å as well as other atomic features that are sensitive to surface gravity and effective temperature. We have determined fundamental stellar parameters such as T eff , log g,  Fe/H , v sin i, and projected rotational velocity using spectral synthesis techniques. Our results show that all targets exhibit solar-like abundances within uncertainties. In addition we find evidence for differential rotation among our sample stars. Finally, we compare our derived values with those found by previous studies and discuss possible reasons behind discrepancies between different works. \n \n Keywords: Near-infrared spectroscopy, Open cluster, Surface gravity, Differential rotation, Fundamental parameters",
        "watermark_text": "We report new high - resolution , near - infrared ( NIR ) spectra for the coolest known members of open complexes M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory . The images were carried out to study the sodium doublet at λλ8183 / 8195 Å as well as other molecular properties that are subject to surface gravity and effective heat .We have predicted fundamental stellar variables such as T eff , log f , Fe / H , v sin i , and projected rotational momentum using spectral synthesis techniques . Our results show that all targets exhibit solar - like abundances within uncertainties .In addition we find data for differential rotation among our sample stars . Finally , we compare our derived values with those detected by earlier surveys and consider alternative causes behind discrepancies between various works .Keywords : Near - infrared spectroscopy , Open cluster , Surface gravity , Differential rotation , Fundamental parameters",
        "rewrite_text": "We present new high-resolution near-infrared (NIR) spectra for the coolest known members of the open clusters M67 and NGC 2516, obtained using the Phoenix spectrograph at Gemini South Observatory. These observations were conducted to investigate the sodium doublet at λλ8183/8195 Å, along with other molecular characteristics influenced by surface gravity and effective temperature. Through spectral synthesis techniques, we have estimated fundamental stellar parameters such as effective temperature (T_eff), logarithmic surface gravity (log g), iron-to-hydrogen ratio (Fe/H), projected rotational velocity (v sin i), and projected angular momentum. Our findings indicate that all studied targets possess solar-like abundances within the margins of uncertainty. Furthermore, we identify evidence of differential rotation among the stars in our sample. Lastly, we compare our derived parameters with those from previous studies and explore possible reasons for discrepancies across different research efforts. \n\nKeywords: Near-infrared spectroscopy, Open clusters, Surface gravity, Differential rotation, Fundamental parameters.",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 4.5,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We report on the observation and characterization of two-dimensional defect modes in optically-induced photonic crystals (OIPCs). The OIPC is formed by periodic modulation of refractive index using femtosecond laser pulses focused into fused silica glass. We show that the defect mode can be tuned over a wide range of wavelengths, which are determined by the periodicity of the lattice structure as well as the size of the defects. This work opens up new possibilities for designing optical devices based on these structures. \n \n Photonic crystal slabs have attracted considerable attention recently because they provide an excellent platform to study light-matter interactions at the nanoscale  1  . In particular, it has been shown that three-dimensional photonic crystals with point or line defects exhibit localized states within their bandgap  2  , leading to many interesting applications such as lasers  3  , filters  4  , sensors  5  , nonlinear optics  6  , etc.. However, fabrication of three-dimensional photonic crystals requires sophisticated techniques  7, 8  , making them difficult to integrate with other micro/nano-structures. Recently, several groups have demonstrated two-dimensional photonic crystals  9  -  11  fabricated directly inside transparent materials via direct laser writing  12  -  14  . These 2D photonic crystals offer advantages including ease of fabrication, flexibility in design, and compatibility with existing technologies  15  .\nIn this Letter we demonstrate the formation of defect modes in opticallyinduced photonic crystals (OPC)  16  . The OPC consists of periodically modulated refractive index created by focusing femtosecond laser pulses into fused silica glass  17  . By introducing defects into the lattice structure, we observe localized defect modes within the stopband of the OPC. Furthermore, we show that the defect mode wavelength can be continuously tuned across the entire stopband simply by changing the lattice spacing and/or the size of the defects. \nThe experimental setup used to create the OPC is illustrated schematically in Fig. 1(a) . A Ti:Sapphire regenerative amplifier system operating at 800 nm was employed to generate 100 fs duration pulses at a repetition rate of 1 kHz. The beam diameter after passing through a spatial filter",
        "watermark_text": "We report on the observation and identification of two - dimensional defect modes in optically - mediated photonic crystals ( OIPCs ) . The OIPC is formed by periodic modulation of refractive index utilizing femtosecond laser pulses focused into fused silica glass .We see that the defect mode can be tuned over a broad variety of wavelengths , which are chosen by the periodicity of the lattice structure as also as the height of the defects . This research raises up new possibilities for constructing optical devices based on these structures .Photonic crystal slabs have garnered considerable interest lately because they give an excellent platform to study light - matter processes at the nanoscale 1 . In particular , it has been shown that three - dimensional photonic materials with point or line defects exhibit localized states within their bandgap 2 , leading to many interesting applications such as lasers 3 , filters 4 , devices 5 , nonlinear optics 6 , etc . .However , fabrication of three - dimensional photonic particles needs specialized techniques 7 , 8 , making them harder to integrate with other micro / nano - materials . Recently , various groups have demonstrated two - dimensional photonic particles 9 - 11 fabricated fully inside transparent materials via continuous optical writing 12 - 14 .These 2D photonic materials provide advantages including ease of fabrication , simplicity in design , and compatibility with existing devices 15 . In this Letter we prove the formation of defect modes in opticallyinduced photonic materials ( OPC ) 16 .The OPC composed of regularly modulated refractive index created by concentrating femtosecond laser pulses into fused silica glass 17 . By introducing defects into the lattice structure , we perceive localized failure modes within the stopband of the OPC .Furthermore , we find that the failure mode wavelength can be continuously tuned across the entire stopband solely by varying the crystal spacing and / or the size of the defects . The empirical setup used to create the OPC is depicted schematically in Fig .1 ( a ) . A Ti : Sapphire regenerative amplifier system operating at 800 nm was employed to produce 100 fs duration pulses at a repetition rate of 1 kHz .The beam diameter after passing through a spatial filter",
        "rewrite_text": "We present our findings on the observation and identification of two-dimensional defect modes in optically-mediated photonic crystals (OIPCs). These OIPCs are engineered through a periodic modulation of the refractive index, achieved by focusing femtosecond laser pulses into fused silica glass. Our results indicate that the defect modes can be tuned over a wide range of wavelengths, determined by the periodicity of the lattice structure and the dimensions of the defects. This discovery opens new avenues for the development of optical devices based on these structures. Recently, photonic crystal slabs have gained significant attention as they provide an excellent platform for investigating light-matter interactions at the nanoscale. Specifically, it has been established that three-dimensional photonic materials, characterized by point or line defects, exhibit localized states within their bandgap. This phenomenon paves the way for a variety of exciting applications, including lasers, filters, and devices in nonlinear optics. However, the fabrication of three-dimensional photonic structures necessitates advanced techniques, which complicates their integration with other micro/nano-materials. In contrast, recent studies have successfully demonstrated the creation of two-dimensional photonic particles that are completely embedded within transparent media through continuous optical writing. These 2D photonic materials offer several advantages, such as simpler fabrication processes, straightforward design, and compatibility with existing technologies. In this letter, we confirm the existence of defect modes within optically-induced photonic materials (OPCs). The OPCs consist of a periodically modulated refractive index created by focusing femtosecond laser pulses into fused silica glass. By incorporating defects into the lattice, we observe localized failure modes within the stopband of the OPCs. Notably, we discover that the wavelength of these failure modes can be continuously tuned across the entire stopband by adjusting either the crystal spacing or the size of the defects. The experimental setup used to create the OPC is schematically illustrated in Fig. 1(a), which shows a Ti:sapphire regenerative amplifier system operating at 800 nm, generating 100 fs pulses with a repetition rate of 1 kHz. The beam diameter after passing through a spatial filter...",
        "ori-fast-z-score": -0.3223291856101521,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": 1.7614096918559585
    },
    {
        "original_text": "We present surface brightness profiles (SBPs) for a sample of globular clusters in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC) and Fornax galaxies obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel (ACS/WFC). The SBPs are derived using archival data taken as part of the ACS Nearby Galaxy Survey Treasury program. We use these new observations to investigate whether there is any difference between the SBPs of globular cluster systems belonging to different host galaxies. In addition we compare our results with those previously published by other authors who have studied similar samples of globular clusters. Our main conclusions are:  1. There appears to be no significant differences between the SBPs of the three different types of globular clusters that were observed.  2. The majority of the globular clusters appear to follow an exponential profile which can be described by: I(r) = Ie exp -(r/rc)  where rc ~ 0.5 pc.",
        "watermark_text": "We create floor brightness characteristics ( SBPs ) for a sample of globular complexes in the Large Magellanic Cloud ( LMC ) , Small Magellanic Cloud ( SMC ) and Fornax clusters obtained with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel ( ACS / WFC ) . The SBPs are derived using archival imagery made as part of the ACS Nearby Galaxy Survey Treasury project .We use these new studies to examine whether there is any difference between the SBPs of globular cluster systems belonging to different host galaxies . In addition we compare our findings with those previously written by other researchers who have researched similar specimens of globular galaxies .Our main results are : 1 . There seems to be no major changes between the SBPs of the three different kinds of globular complexes that were detected .2 . The majority of the globular complexes appear to follow an exponential profile which can be described by : I ( r ) = Ie exp - ( r / rc ) where rc ~ 0 . 5 pc .",
        "rewrite_text": "We have developed surface brightness profiles (SBPs) for a selection of globular clusters in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC), and Fornax clusters, utilizing data obtained from the Hubble Space Telescope's Advanced Camera for Surveys Wide Field Channel (ACS/WFC). These SBPs are based on archival images collected during the ACS Nearby Galaxy Survey Treasury project. Our analysis focuses on determining whether there are differences in the SBPs of globular cluster systems across various host galaxies. Additionally, we compare our results with previous studies conducted by other researchers on similar types of globular galaxies. Our key findings are as follows: 1. We observe no significant differences among the SBPs of the three types of globular clusters we investigated. 2. Most globular clusters exhibit an exponential profile characterized by the equation I(r) = Ie exp(-r/rc), where rc is approximately 0.5 pc.",
        "ori-fast-z-score": -0.5547001962252291,
        "water-fast-z-score": 6.10170215847752,
        "rewrite-fast-z-score": 0.42008402520840293
    },
    {
        "original_text": "We study number density correlation functions (NDCFs) in classical one-component plasma at high temperatures and find that they are not well defined due to infrared divergences, which is related with the fact that NDCF can be expressed as an integral over the whole space. We show how these difficulties can be overcome by introducing a new quantity called local number density correlation function (LNDCF). The LNDCF has no infrared divergence but it still contains information about correlations between particles on different length scales. In particular we calculate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian. Finally we discuss possible applications of our results. PACS numbers: 52.27.Lw, 52.35.Jm, 52.38.Bx \nI. INTRODUCTORY REMARK\nIn this work we consider classical one component plasma consisting of charged particles interacting via screened Coulomb potential  1  . This system is described by the following Hamiltonian:",
        "watermark_text": "We explore number density correlation functions ( NDCFs ) in classical one - component plasma at high altitudes and find that they are not well characterised due to infrared divergences , which is related with the fact that NDCF can be described as an integral over the whole space . We see how these problems can be overcome by using a new quantity called regional number density correlation function ( LNDCF ) .The LNDCF has no infrared divergence but it still contains information about correlations between particles on various length scales . In particular we estimate LNDCF for two limiting cases - when all particles have equal velocities or when their distribution is Maxwellian .Finally we explain possible applications of our findings . PACS codes : 52 . 27 . Lw , 52 . 35 . Jm , 52 . 38 . Bx I .INTRODUCTORY REMARK In this research we define classical one element plasma consisting of charged particles interacting via screened Coulomb potential 1 . This system is characterized by the following Hamiltonian :",
        "rewrite_text": "We investigate number density correlation functions (NDCFs) in classical one-component plasma at high altitudes and discover that they are poorly defined due to infrared divergences, which arise from the fact that NDCFs can be represented as integrals over the entire spatial domain. We demonstrate how to address these issues by introducing a new measure known as the regional number density correlation function (LNDCF). Unlike NDCFs, the LNDCF is free from infrared divergences while still capturing information about particle correlations across different length scales. Specifically, we compute the LNDCF for two extreme scenarios: one where all particles exhibit the same velocities and another where their distribution follows a Maxwellian pattern. Lastly, we discuss potential applications of our results. PACS codes: 52.27.Lw, 52.35.Jm, 52.38.Bx. \n\nI. INTRODUCTORY REMARK\n\nIn this study, we define a classical one-component plasma comprised of charged particles interacting through a screened Coulomb potential. This system is characterized by the following Hamiltonian:",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 0.601929265428846
    },
    {
        "original_text": "We discuss how chiral symmetry is realized in QCD, with particular emphasis on its implications for the spectrum of excited states. We argue that the lowest-lying excitations are described by open strings attached to quarks at one end and antiquarks at the other; these can be viewed as mesons or glueballs depending on whether they carry color charge or not.  The next set of excitations corresponds to closed strings which wind around the compactified dimension (the time direction) once or more times. These correspond to baryon resonances. Finally we consider higher-spin excitations corresponding to multiple winding modes of the closed string; these have spin greater than two but still less than the number of colors Nc. In this talk I will present some recent results obtained using holographic methods applied to gauge theories dual to supergravity backgrounds. This approach has been very successful in describing many aspects of strongly-coupled gauge theory dynamics including confinement, chiral symmetry breaking, and the properties of light vector mesons. However it also provides an interesting new perspective on heavy quark physics such as the production of top-quark pairs in high-energy collisions.",
        "watermark_text": "We discuss how chiral symmetry is realized in QCD , with particular focusing on its consequences for the spectrum of excited states . We argue that the smallest - lying excitations are explained by open strings attached to quarks at one end and antiquarks at the other ; these can be viewed as mesons or glueballs depending on whether they carry color energy or not .The next set of excitations corresponds to closed strings which wind around the compactified dimension ( the time direction ) once or more times . These correspond to baryon resonances .Finally we study greater - spinning excitations corresponding to multiple winding modes of the shut string ; these have spin greater than two but still less than the number of colors Nc . In this talk I will present some latest findings obtained using holographic methods applied to gauge fields dual to supergravity environments .This method has been very effective in understanding several elements of highly - coupled gauge theory physics including confinement , chiral symmetry breaking , and the properties of light vector mesons . However it also provides an interesting recent perspective on heavy quark theory such as the production of top - quark pairs in high - energy collisions .",
        "rewrite_text": "In this discussion, we explore the realization of chiral symmetry in quantum chromodynamics (QCD), with a particular emphasis on its implications for the spectrum of excited states. We propose that the lowest-lying excitations can be described as open strings, with one end attached to a quark and the other to an antiquark; these configurations can be interpreted as mesons or glueballs, depending on whether they contain color energy. The subsequent excitations correspond to closed strings that wrap around the compactified time dimension one or more times, representing baryon resonances. Lastly, we investigate higher-spin excitations associated with multiple winding modes of the closed string; these states possess spin greater than two but remain below the number of colors, Nc. In this talk, I will share recent findings derived from holographic methods applied to gauge fields that are dual to supergravity settings. This approach has proven to be highly effective in elucidating various aspects of strongly coupled gauge theory, including confinement, chiral symmetry breaking, and the characteristics of light vector mesons. Furthermore, it offers a novel viewpoint on heavy quark dynamics, such as the production of top-quark pairs in high-energy collisions.",
        "ori-fast-z-score": 1.8973665961010275,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "We measure the baryonic acoustic oscillation (BAO) scale in the distribution of galaxies by cross-correlating the Sloan Digital Sky Survey Data Release 5 with the Two Degree Field Galaxy Redshift Survey Data Release 3, both at z=0.35-0.55. We use two different estimators for the BAO peak position, one based on the correlation function and another on the power spectrum. The results are consistent within errors between these methods as well as with previous measurements made over smaller ranges of redshift or volume. Our measurement is also consistent with theoretical predictions based on WMAP1 cosmological parameters. This work demonstrates that it will be possible to make precise distance measurements out to redshifts of order unity using future large galaxy surveys such as the Dark Energy Spectroscopic Instrument. These distances can then be used to constrain dark energy models through their effect on the expansion history of the universe. \n \n Keywords: Baryons",
        "watermark_text": "We estimate the baryonic sound oscillation ( BAO ) scale in the distribution of galaxies by inter - correlating the Sloan Digital Sky Survey Data Release 5 with the Two Degree Field Galaxy Redshift Survey Data Release 3 , both at z = 0 . 35 - 0 . 55 . We use two different estimators for the BAO peak point , one based on the interaction function and another on the power spectrum .The results are compatible within errors between these models as well as with previous measurements made over smaller ranges of redshift or volume . Our measurement is also consistent with theoretical estimates based on WMAP1 cosmological factors .This research shows that it will be possible to make accurate distance measurements out to redshifts of order unity utilizing upcoming huge galaxy surveys such as the Dark Energy Spectroscopic Instrument . These ranges can then be used to constrain dark energy models through their effect on the evolution period of the universe .Keywords: Baryons",
        "rewrite_text": "We estimate the baryonic acoustic oscillation (BAO) scale in the distribution of galaxies by cross-correlating data from the Sloan Digital Sky Survey Data Release 5 with the Two Degree Field Galaxy Redshift Survey Data Release 3, specifically in the redshift range of z = 0.35 to 0.55. We employ two different methods to identify the BAO peak: one utilizing the interaction function and the other based on the power spectrum. The findings from these models are consistent within their error margins and align well with previous measurements taken over smaller redshift ranges or volumes. Additionally, our results agree with theoretical predictions derived from WMAP1 cosmological parameters. This research indicates that accurate distance measurements can be achieved up to redshifts around one, leveraging upcoming large-scale galaxy surveys like the Dark Energy Spectroscopic Instrument. These measurements can be pivotal in constraining dark energy models by examining their impact on the universe's expansion history. Keywords: Baryons",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We have recently shown that the one-range addition theorems derived in our previous work are valid not only for the Coulomb interaction potential but also its derivatives, such as the nuclear attraction potential or the exchange potential. \n \n In this comment we show how these results can be used to derive new addition theorems for the nuclear attraction potential and the exchange potential. These new addition theorems are useful when calculating matrix elements between atomic orbitals with different angular momenta. We illustrate their application using examples involving hydrogenic wave functions. Finally, we discuss some possible extensions of these results. DOI: 10.1063/1.2055316 \n \n This is an extended version of a comment published in ChemPhysChem. DOI: 10.1002/cphc.201500420 \n \n \n \n One-range addition theorems play important roles in many areas of physics including quantum chemistry  1-3 , molecular physics  4 , condensed matter physics  5 , etc.. They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions. For example, they allow us to calculate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to perform complicated numerical integrations  6 . Recently, we showed that the same approach could be applied to other types of potentials  7-9 .",
        "watermark_text": "We have recently shown that the one - range addition theorems generated in our previous study are applicable not only for the Coulomb interaction potential but also its derivatives , such as the atomic attraction potential or the exchange potential . In this comment we give how these results can be used to derive new addition theorems for the atomic attraction potential and the transfer potential .These new addition theorems are helpful when calculating matrix elements between atomic orbitals with various angular momenta . We illustrate their application using examples involving hydrogenic wave systems .Finally , we explain some possible extensions of these results . DOI : 10 . 1063 / 1 . 2055316 This is an extended version of a note published in ChemPhysChem .DOI : 10 . 1002 / cphc . 201500420 One - range addition theorems play important roles in many areas of physics including quantum chemistry 1 - 3 , molecular physics 4 , condensed matter physics 5 , etc . . They provide simple expressions for evaluating matrix elements of various potentials between two arbitrary wavefunctions .For instance , they allow us to estimate matrix elements of the Coulomb interaction potential between any pair of atomic orbital basis sets without having to conduct complicated mathematical integrations 6 . Recently , we demonstrated that the same method could be applied to other types of potentials 7 - 9 .",
        "rewrite_text": "In our recent research, we have established that the one-range addition theorems developed in our prior work are relevant not only for the Coulomb interaction potential but also for its derivatives, including the atomic attraction potential and the exchange potential. In this commentary, we demonstrate how these findings can lead to the formulation of new addition theorems for both the atomic attraction potential and the transfer potential. These newly derived theorems are particularly advantageous for calculating matrix elements between atomic orbitals with varying angular momenta. We provide examples of their application within hydrogenic wave systems. Additionally, we discuss potential extensions of these findings. DOI: 10.1063/1.2055316. This text expands upon a note initially published in ChemPhysChem. DOI: 10.1002/cphc.201500420. The one-range addition theorems are pivotal in numerous fields of physics, including quantum chemistry, molecular physics, and condensed matter physics, as they offer straightforward expressions for evaluating matrix elements of various potentials between any two wavefunctions. For example, they enable us to estimate matrix elements of the Coulomb interaction potential between arbitrary sets of atomic orbitals without resorting to complex mathematical integrations. Recently, we have shown that this method is applicable to other types of potentials as well.",
        "ori-fast-z-score": -0.5076730825668095,
        "water-fast-z-score": 3.7567808109943908,
        "rewrite-fast-z-score": 0.10482848367219183
    },
    {
        "original_text": "We study the collective modes in two-band superconductors with different gaps and masses, using the random phase approximation (RPA). We find that there are three types of collective modes: one is gapless and has linear dispersion relation at small wave vector; another is gapped but still has quadratic dispersion relation near the Fermi surface; while the third type is fully gapped without any low-energy excitations. The latter two types can be regarded as phonon-like collective modes. In addition to these three types of collective modes, we also find an exotic mode which does not exist in single-gap systems. This new mode originates from the interband pairing interaction between electrons on different bands. It shows up only when both intraband and interband interactions are present simultaneously. Our results show that this new mode may have important effects on the transport properties of multi-band superconductors. \n \n Introduction \n \n Multi-band superconductivity attracts much attention recently because it occurs naturally in many materials such as MgB_2  1  , Sr 2 RuO 4  2  , FeSe  3  . These compounds usually contain several orbitals per unit cell so they support multiple electronic bands crossing the Fermi level  4  . Due to the presence of more than one band, the electron-phonon coupling strength could vary significantly among different bands  5  . Moreover, the Coulomb repulsion effect becomes stronger for multi-orbital systems  6  . All these factors make the physics of multiband superconductors very rich  7, 8  .\n \nIn recent years, great progresses have been made in understanding the physical properties of multi-band superconductor  9  . For example, the vortex lattice structure  10  , magnetic field dependence  11  , thermal conductivity  12  , specific heat  13  , NMR relaxation rate  14  etc., were studied extensively by experiments. On the theoretical side, various methods including mean-field theory  15  , Eliashberg formalism  16  , functional renormalization group  17  , variational Monte Carlo  18  , exact diagonalization  19  , density matrix renormalization group  20  , and quantum Monte Carlo  21  were used to investigate the ground state properties  22  , thermodynamic quantities  23  ,",
        "watermark_text": "We research the collective modes in two - band superconductors with various gaps and masses , using the random phase approximation ( RPA ) . We see that there are three categories of collective modes : one is gapless and has continuous dispersion relation at small wave vector ; another is gapped but still has quadratic dispersion relation near the Fermi surface ; while the third type is fully gapped without any low - energy excitations .The last two forms can be regarded as phonon - like collective modes . In addition to these three sorts of collective modes , we also find an exotic mode which does not occur in single - gap systems .This new mode comes from the interband pairing interaction between electrons on various groups . It gets up only when both intraband and interband interactions are present concurrently .Our results show that this new mode may have important effects on the travel properties of multi - band superconductors . Introduction Multi - band superconductivity attracts great concern lately because it appears naturally in many materials such as MgB _ 2 1 , Sr 2 RuO 4 2 , FeSe 3 .These compounds often contain many orbitals per unit cell so they support multiple electronic bands crossing the Fermi level 4 . Due to the presence of more than one band , the electron - phonon coupling strength could vary significantly among different bands 5 .Moreover , the Coulomb repulsion effect gets stronger for multi - orbital complexes 6 . All these considerations making the physics of multiband superconductors very rich 7 , 8 .In recent years , great progresses have been achieved in understanding the physical properties of multi - band superconductor 9 . For instance , the vortex lattice structure 10 , magnetic field dependence 11 , thermal conductivity 12 , basic heat 13 , NMR relaxation speed 14 etc . , were studied thoroughly by research .On the theoretical front , various methods notably mean - field model 15 , Eliashberg formalism 16 , functional renormalization group 17 , variational Monte Carlo 18 , exact diagonalization 19 , density matrix renormalization group 20 , and quantum Monte Carlo 21 were used to examine the ground state properties 22 , thermodynamic quantities 23 ,",
        "rewrite_text": "We investigate the collective modes in two-band superconductors characterized by diverse gap sizes and effective masses, employing the random phase approximation (RPA). Our findings reveal three distinct categories of collective modes: the first is gapless, exhibiting a continuous dispersion relation at small wave vectors; the second is gapped, yet retains a quadratic dispersion relation close to the Fermi surface; while the third type is fully gapped, lacking any low-energy excitations. The latter two categories can be interpreted as phonon-like collective modes. Furthermore, we discover an exotic mode that is absent in single-gap systems. This novel mode emerges from the interband pairing interactions among electrons across different bands and only arises when both intraband and interband interactions are simultaneously present. Our results indicate that this new mode could significantly influence the transport properties of multi-band superconductors.\n\n### Introduction \nRecently, multi-band superconductivity has garnered significant interest as it naturally occurs in various materials such as MgB₂, Sr₂RuO₄, and FeSe. These compounds typically feature multiple orbitals per unit cell, allowing for several electronic bands to intersect the Fermi level. The existence of multiple bands leads to substantial variations in electron-phonon coupling strengths across different bands. Additionally, the Coulomb repulsion is heightened in multi-orbital systems. These factors contribute to the complex physics associated with multi-band superconductors. In recent years, notable advancements have been made in understanding the physical properties of these superconductors. For instance, studies have thoroughly explored the vortex lattice structure, magnetic field dependence, thermal conductivity, basic heat capacity, and NMR relaxation rates. On the theoretical side, a variety of methods, including the mean-field model, Eliashberg formalism, functional renormalization group, variational Monte Carlo, exact diagonalization, density matrix renormalization group, and quantum Monte Carlo, have been employed to analyze their ground state properties and thermodynamic quantities.",
        "ori-fast-z-score": 0.9607689228305227,
        "water-fast-z-score": 6.943355894868313,
        "rewrite-fast-z-score": 2.2135943621178655
    },
    {
        "original_text": "We present here the rigorous mathematical formulation and complete proof of our previous results on the equivalence between Ising spin glasses (ISGs) and Ising models (IMs). We show that, in the thermodynamic limit N → ∞ with fixed ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable transformation which preserves all relevant physical quantities such as free energy density f = −k B T ln Z/N . The main idea is to introduce a new set of variables {s} ≡ {s1, ..., sN } representing the local magnetizations of each site i ∈ {1, .., N }, and then to map the original problem onto one where spins are coupled only through their mutual interactions. This allows us to prove rigorously that the two systems have exactly the same statistical properties when the number of sites goes to infinity. \nThe manuscript is organized as follows. In Sec. II we define the model under investigation and state some basic definitions and notations used throughout this work. In Sec. III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then, in Sec. IV, we discuss how it can be extended to infinite-size lattices. Finally, in Sec. V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "watermark_text": "We present here the thorough mathematical formulation and complete proving of our previous findings on the equivalence between Ising spin glasses ( ISGs ) and Ising models ( IMs ) . We see that , in the thermodynamic limit N → ∞ with constant ratio J / T , ISG partition functions can be mapped to IM ones by means of a suitable decomposition which preserves all relevant physical components such as free energy density g = −k B T ln Z / N .The main idea is to introduce a new collection of variables { s } ≡ { s1 , . . . , sN } representing the local magnetizations of each site i ∈ { 1 , . . , N } , and then to map the previous problem onto one where spins are coupled only through their mutual interactions . This enables us to prove rigorously that the two systems have exactly the same statistical characteristics when the number of places goes to infinity .The manuscript is organized as follows.In Sec.II we define the model under inquiry and explain some fundamental definitions and notations used throughout this study . In Sec .III we provide the exact definition of the transformation introduced above and derive its explicit form for any finite system size N . Then , in Sec .IV , we explain how it can be generalized to infinite - length lattices . Finally , in Sec .V we give the full proof of the theorem stating the equivalence between ISGs and IMs.",
        "rewrite_text": "In this work, we provide a comprehensive mathematical formulation and complete proof of our earlier results regarding the equivalence of Ising spin glasses (ISGs) and Ising models (IMs). We demonstrate that, in the thermodynamic limit as N approaches infinity with a fixed ratio of J/T, the partition functions of ISGs can be transformed into those of IMs through a suitable decomposition that retains all essential physical features, such as the free energy density g = -k_B T ln Z / N. The core idea involves introducing a new set of variables {s} ≡ {s₁, ..., sₙ} that represent the local magnetizations at each site i ∈ {1, ..., N}, allowing us to reformulate the problem such that spins are coupled solely through their mutual interactions. This approach enables us to rigorously establish that the two systems exhibit identical statistical properties in the limit as the number of sites becomes infinite. The structure of the manuscript is outlined as follows: In Section II, we define the model under investigation and clarify the fundamental definitions and notations used throughout this study. In Section III, we present a precise definition of the aforementioned transformation and derive its explicit form for any finite system size N. Section IV discusses how this transformation can be generalized to infinite-length lattices. Finally, in Section V, we offer a complete proof of the theorem asserting the equivalence between ISGs and IMs.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": 1.4288690166235207
    },
    {
        "original_text": "We present an ab initio study on spin relaxation mechanisms in bulk, single- and double-quantum-well (DQW) structures based on zinc-blende semiconductors such as GaAs or InP. We focus our attention on the so-called Bir-Aronov-Pikuz mechanism which is responsible for spin-flip transitions between conduction-band states with different orbital angular momenta. The main results are summarized below.  For bulk materials we find that the dominant contribution comes from intra-valley scattering processes involving heavy-hole bands. This result agrees well with previous theoretical studies performed within effective-mass approximations. However, by using realistic band-structure calculations we show that inter-valley contributions can also play an important role when considering DQWs grown along non  001  directions. Finally, we discuss how these findings could be used to improve existing models describing spin relaxation times in semiconductor nanostructures.",
        "watermark_text": "We present an ab initio investigation on spin relaxation mechanisms in bulk , double - and double - quantum - well ( DQW ) complexes based on aluminium - blende semiconductors such as GaAs or InP . We focus our focus on the so - called Bir - Aronov - Pikuz process which is responsible for spin - flip transitions between conduction - band elements with various orbital angular momenta .The main results are presented below . For bulk materials we find that the dominant contribution comes from intra - valley reflection processes involving heavy - hole lines .This result agrees well with previous conceptual research performed within efficient - mass approximations . However , by using accurate band - structure estimates we find that cross - valley contributions can also play an important role when assessing DQWs grown along non 001 directions .Finally , we explain how these results could be used to improve previous descriptions describing spin relaxation times in semiconductor nanostructures .",
        "rewrite_text": "We present an ab initio study of spin relaxation mechanisms in bulk materials and double quantum well (DQW) complexes made from aluminum-blende semiconductors like GaAs and InP. Our primary focus is on the Bir-Aronov-Pikus process, which facilitates spin-flip transitions among conduction-band states with different orbital angular momenta. Below, we outline our key findings. For bulk materials, we discover that the main contribution arises from intra-valley reflection processes involving heavy-hole bands. This aligns with earlier conceptual studies conducted using effective mass approximations. However, through precise band structure calculations, we reveal that cross-valley contributions can significantly influence the behavior of DQWs oriented along non-001 directions. Lastly, we discuss how these findings can enhance existing models for characterizing spin relaxation times in semiconductor nanostructures.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present new multi-color photometric data for the globular cluster M75 (NGC6864) obtained with the Hubble Space Telescope Wide Field Camera 3, which allow us to study its horizontal branch stars in unprecedented detail. We find that the color distribution along the HB is bimodal, indicating two distinct populations of hot and cool HB stars. The blue tail of the observed HB can be explained by assuming an age difference between these two groups of about 1 Gyr. Using our new HST observations we also derive accurate absolute ages for both sub-populations. Our results show that the redder population has an age of 12.6 ± 0.2 Gyr while the bluer one is younger at 11.7 ± 0.1 Gyr. This finding supports previous suggestions that the redder part of the HB may have been formed during a second episode of star formation within this system. \n \n Keywords: Globular clusters; Horizontal branches",
        "watermark_text": "We present new multi - color photometric data for the globular cluster M75 ( NGC6864 ) obtained with the Hubble Space Telescope Wide Field Camera 3 , which allow us to study its horizontal branch stars in unprecedented detail . We see that the color variation along the HB is bimodal , showing two different populations of cold and warm HB stars .The blue tail of the observed HB can be described by assuming an age difference between these two groups of about 1 Gyr . Using our new HST observations we also generate reliable absolute ages for both sub - communities .Our results show that the redder population has an age of 12 . 6 ± 0 . 2 Gyr while the bluer one is younger at 11 . 7 ± 0 . 1 Gyr . This conclusion supports earlier suggestions that the redder half of the HB may have been formed during a second episode of star formation within this system .Keywords : Globular regions ; Horizontal branches",
        "rewrite_text": "We present new multi-color photometric data for the globular cluster M75 (NGC 6864), collected using the Hubble Space Telescope's Wide Field Camera 3. This dataset allows for an unprecedented examination of its horizontal branch stars. Our analysis reveals a bimodal color distribution along the horizontal branch, indicating the presence of two distinct populations of cold and warm HB stars. The blue tail in the observed horizontal branch suggests an age difference of approximately 1 Gyr between these groups. Based on our new HST observations, we have also derived reliable absolute ages for each sub-population. Our findings indicate that the redder stars have an age of 12.6 ± 0.2 Gyr, while the bluer stars are younger, at 11.7 ± 0.1 Gyr. This supports previous theories that the redder half of the horizontal branch may have formed during a secondary star formation event within this system. Keywords: Globular clusters; Horizontal branches.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 3.713069518053983,
        "rewrite-fast-z-score": -0.8427009716003844
    },
    {
        "original_text": "We present an algorithm to determine directly the surface gravity (log g) and radius ratio (Rp/Rs) of transiting exoplanets using high-precision photometry obtained with space-based observatories such as CoRoT, Kepler or Spitzer. The proposed technique is based on the measurement of the transit duration in different passbands. We show that this new approach allows us to obtain accurate values of log g and Rp/Rs even when only one transit event has been observed. This makes it possible to study the physical properties of small planets which are difficult to characterize by other methods. In addition, we demonstrate how our method can be used to detect false positives among planet candidates detected by the transit method. Finally, we apply our method to two well-studied systems, HD 209458 b and WASP-12 b, and find good agreement between our results and previous determinations. \n \n Keywords: Extrasolar planet - Surface gravity",
        "watermark_text": "We present an algorithm to estimate directly the surface gravity ( log g ) and radius ratio ( Rp / Rs ) of transiting exoplanets using high - precision photometry obtained with space - based observatories such as CoRoT , Kepler or Spitzer . The proposed methodology is based on the determination of the transit duration in different passbands .We see that this new approach allows us to obtain precise measures of log g and Rp / Rs even when only one transit event has been observed . This gives it able to study the physical properties of tiny planets which are hard to characterize by other methods .In addition , we prove how our technique can be used to identify false positives among planet candidates detected by the transit method . Finally , we apply our technique to two good - investigated systems , HD 209458 b and WASP - 12 b , and find good agreement between our findings and previous determinations .Keywords : Extrasolar planet - Surface gravity",
        "rewrite_text": "We introduce a novel algorithm for directly estimating the surface gravity (log g) and radius ratio (Rp/Rs) of transiting exoplanets using high-precision photometric data obtained from space-based observatories such as CoRoT, Kepler, and Spitzer. This methodology involves analyzing the transit duration across different wavelength bands. Our approach demonstrates the capability to yield accurate measurements of log g and Rp/Rs, even with data from a single transit event. This is particularly beneficial for characterizing small planets that are difficult to study using traditional methods. Additionally, we illustrate how our technique can help identify false positives among planet candidates found through transit observations. We further validate our method by applying it to two well-studied exoplanets, HD 209458 b and WASP-12 b, and our results align well with previous findings. Keywords: Extrasolar planet - Surface gravity.",
        "ori-fast-z-score": 0.47809144373375745,
        "water-fast-z-score": 4.302822993603817,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "The TACTIC (Telescope Array for Cosmic Triggered Events) is an imaging atmospheric Cherenkov telescope located in Namibia at the site of the HESS experiment, and it was designed to detect gamma rays with energies between 100 GeV and 10 TeV. The camera consists of 960 photomultiplier tubes arranged on a hexagonal grid covering a field-of-view of 3 degrees diameter. It has been taking data since March 2009. In this work we present results obtained by applying different analysis techniques to the first two years of data taken with the TACTIC telescope. We show that these analyses are able to reconstruct events with high efficiency over most of the energy range covered by the instrument. Finally, we compare our results with those published by other experiments operating in similar energy ranges. This article is part of the themed issue  The Universe as seen by ground-based gamma-ray telescopes . Gamma-rays can be detected indirectly via their interaction with Earth s atmosphere, producing showers of secondary particles which emit light when they reach the ground level. These so-called air-shower photons can then be observed using large optical detectors such as imaging atmospheric Cherenkov telescopes (IACTs). IACTs have proven to be powerful instruments for studying cosmic phenomena like active galactic nuclei or supernova remnants. However, due to their relatively small fields-of-view, they usually require several hours of observation time per source before significant detection significances can be achieved. To overcome this problem, new generation IACT arrays were built recently, consisting of multiple telescopes distributed across wide areas. One example is the High Energy Stereoscopic System (H.E.S.S.)  1  , where four telescopes observe simultaneously the same region of the sky. Another one is the Telescope Array for Cosmic Triggerd Events (TACTIC), which will consist of eight telescopes spread out over a distance of about 1 km  2  .",
        "watermark_text": "The TACTIC ( Telescope Array for Cosmic Triggered Events ) is an imaging atmospheric Cherenkov telescope located in Namibia at the location of the HESS program , and it was built to identify gamma radiation with energies between 100 GeV and 10 TeV . The camera consists of 960 photomultiplier frames arranged on a hexagonal grid covering a field - of - view of 3 degrees diameter .It has been took results since March 2009 . In this project we present results acquired by using different analysis methods to the first two years of evidence carried with the TACTIC telescope .We see that these calculations are able to reconstruct events with high efficiency over most of the power range covered by the instrument . Finally , we compare our findings with those published by other experiments working in related energy intervals .This page is part of the themed article The Universe as shown by ground - based gamma - ray telescopes . Gamma - radiation can be emitted indirectly via their contact with Earth s atmosphere , creating showers of secondary particles which emit light when they reach the ground level .These so - called air - shower photons can then be viewed using big optical detectors such as imaging atmospheric Cherenkov telescopes ( IACTs ) . IACTs have proven to be powerful devices for studying cosmic phenomena like active galactic nuclei or supernova remnants .However , owing to their extremely tiny areas - of - view , they generally need multiple hours of observation time per source before significant detection significances can be obtained . To solve this situation , new generation IACT arrays were built recently , consisting of multiple telescopes distributed across wide areas .One case is the High Energy Stereoscopic System ( H . E . S . S . ) 1 , where four telescopes observe simultaneously the same region of the heavens .Another one is the Telescope Array for Cosmic Triggerd Events ( TACTIC ) , which will include of eight telescopes distributed out over a distance of about 1 km 2 .",
        "rewrite_text": "The TACTIC (Telescope Array for Cosmic Triggered Events) is an imaging atmospheric Cherenkov telescope situated in Namibia, adjacent to the HESS program. It was designed to detect gamma radiation with energies ranging from 100 GeV to 10 TeV. The telescope features a camera composed of 960 photomultiplier frames arranged in a hexagonal grid that spans a field of view with a diameter of 3 degrees. Since it began operations in March 2009, we present results obtained from various analytical methods applied to the initial two years of data collected by the TACTIC telescope. Our analyses demonstrate a high efficiency in reconstructing events across most of the instrument's energy range. Additionally, we compare our results with findings from other experiments that explore similar energy ranges. This content is part of a themed article titled \"The Universe as Revealed by Ground-Based Gamma-Ray Telescopes.” Gamma radiation can be indirectly emitted when it interacts with Earth's atmosphere, resulting in showers of secondary particles that emit light as they reach ground level. These so-called air-shower photons can be detected using large optical devices, such as imaging atmospheric Cherenkov telescopes (IACTs). IACTs have proven to be effective tools for investigating cosmic phenomena, including active galactic nuclei and supernova remnants. However, due to their relatively small fields of view, significant detection often requires several hours of observation for each source. To address this limitation, newer generations of IACT arrays have been developed, featuring multiple telescopes spread over larger areas. An example is the High Energy Stereoscopic System (H.E.S.S.), which utilizes four telescopes to simultaneously observe the same region of the sky. Another example is the TACTIC, which is set to comprise eight telescopes distributed across an area of approximately 1 km².",
        "ori-fast-z-score": -0.3508232077228117,
        "water-fast-z-score": 7.60122529295103,
        "rewrite-fast-z-score": -0.17541160386140586
    },
    {
        "original_text": "The MCHIT (Monte Carlo Heavy Ion Transport) code is an advanced tool to simulate the transport of ions in matter, including their interactions with target nuclei as well as elastic scattering off atomic electrons. The code has been developed at GSI Helmholtzzentrum für Schwerionenforschung GmbH since 1998 by several groups under the leadership of Prof. Dr. Jens B. Skarsgard. \n \n It was originally designed to study nuclear fragmentation reactions induced by relativistic heavy ions on light targets like carbon or water. In recent years it has also been used successfully to investigate other topics such as: \n \n \n \n • Radiation damage in biological tissues due to ion beam irradiation \n \n • Secondary particle production in hadronic showers \n \n • Energy deposition in materials exposed to high-energy cosmic rays \n \n • Nuclear reaction cross sections for astrophysical applications \n \n • Hadrontherapy treatment planning",
        "watermark_text": "The MCHIT ( Monte Carlo Heavy Ion Transport ) code is an specialized tool to simulate the movement of atoms in material , particularly their interactions with target nuclei as well as elastic scattering off atomic electrons . The code has been created at GSI Helmholtzzentrum für Schwerionenforschung GmbH since 1998 by many groups under the leadership of Prof . Dr . Jens B . Skarsgard .It was originally intended to study nuclear fragmentation reactions generated by relativistic heavy ions on light sites like nitrogen or water . In past decades it has additionally been used successfully to examine other topics such as : • Radiation injury in biological tissues resulting to ion beam irradiation • Secondary particle production in hadronic showers • Energy deposition in structures exposed to large - energy cosmic rays • Nuclear reaction cross sections for astrophysical applications • Hadrontherapy control planning",
        "rewrite_text": "The MCHIT (Monte Carlo Heavy Ion Transport) code is a specialized tool designed to simulate atomic movements within materials, focusing particularly on how they interact with target nuclei and undergo elastic scattering with atomic electrons. Developed at GSI Helmholtzzentrum für Schwerionenforschung GmbH since 1998 under the guidance of Prof. Dr. Jens B. Skarsgard, the code was initially created to investigate nuclear fragmentation reactions caused by relativistic heavy ions on lighter materials such as nitrogen or water. Over the years, it has been successfully applied to a variety of additional topics, including: \n- Radiation damage in biological tissues due to ion beam irradiation \n- Secondary particle production in hadronic showers \n- Energy deposition in structures exposed to high-energy cosmic rays \n- Nuclear reaction cross sections relevant to astrophysical studies \n- Planning and control for hadron therapy.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 6.096006096009144,
        "rewrite-fast-z-score": 1.889822365046136
    },
    {
        "original_text": "We study the orbital evolution of planets in binaries under the effect of gravitational perturbations due to third bodies, which can lead to large eccentricities and inclinations for both components of the system. We show that this mechanism is able to explain some observed properties of extrasolar systems such as HD 169830 or Kepler-16. In particular we find that it may be responsible for the formation of hot Jupiters through planet-planet scattering processes. The main results are summarized below: \n1) We present an analytical model describing the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting objects under the combined effects of general relativity, tides, and secular interactions between all three bodies. \n2) Using our model, we perform numerical integrations of several representative cases showing how the presence of additional perturbative forces can significantly modify the orbital parameters of the innermost body over time-scales ranging from millions up to billions of years. \n\n3) We apply our model to the case of the exoplanetary system around HD 169830 composed of four giant planets on highly inclined orbits. Our calculations suggest that the current architecture of this system could have been produced by successive scatterings among its planets triggered by strong gravitational encounters with other massive bodies located at distances larger than 100 AU. \n \n 4) Finally, we explore the possibility that the recently discovered transiting super-Earths in the Kepler-16 system might also have formed via similar mechanisms.",
        "watermark_text": "We research the orbital evolution of planets in binaries under the impact of gravitational perturbations due to third body , which can lead to large eccentricities and inclinations for both components of the system . We suggest that this mechanism is could to explain some observed properties of extrasolar systems such as HD 169830 or Kepler - 16 .In particular we find that it could be responsible for the formation of bright Jupiters through moon - planet scattering mechanisms . The main results are presented below : 1 ) We present an analytical theory explaining the long - term evolve of the semi - major axes ( a ) , eccentricities ( e ) , and mutual inclination angles ( i ) of two orbiting objects under the combined influences of general relativity , tides , and secular interactions between all three bodies .2 ) Using our model , we perform numerical integrations of several representative cases showing how the presence of added perturbative stresses can significantly change the orbital variables of the innermost bodies over time - scales extending from hundreds up to billions of years . 3 ) We use our model to the case of the exoplanetary structure around HD 169830 composed of four giant planets on highly inclined planets .Our calculations suggest that the present architecture of this scheme may have been created by successive scatterings among its planets triggered by intense gravitational encounters with other large bodies located at distances bigger than 100 AU . 4 ) Finally , we investigate the suggestion that the recently discovered transiting super - Earths in the Kepler - 16 system might also have formed via related pathways .",
        "rewrite_text": "We investigate the orbital evolution of planets in binary systems, focusing on the gravitational perturbations caused by a third body. These perturbations can lead to significant eccentricities and inclinations for both components of the system. We propose that this mechanism may help explain certain characteristics of extrasolar systems, such as HD 169830 and Kepler-16. Specifically, we find that it could be instrumental in the formation of bright Jupiter-like planets through moon-planet scattering processes. The key findings of our research are as follows: \n\n1) We present an analytical framework that outlines the long-term evolution of the semi-major axes (a), eccentricities (e), and mutual inclination angles (i) of two orbiting bodies, considering the combined effects of general relativity, tidal interactions, and secular influences among all three bodies. \n\n2) Employing our model, we conduct numerical simulations of several representative scenarios, demonstrating how additional perturbative stresses can significantly alter the orbital parameters of the innermost bodies over timescales ranging from hundreds to billions of years. \n\n3) We apply our model to the exoplanetary system surrounding HD 169830, which includes four giant planets with highly inclined orbits. Our calculations indicate that the current configuration may have resulted from successive gravitational scatterings among these planets, driven by intense interactions with other massive bodies located more than 100 AU away. \n\n4) Lastly, we explore the possibility that the recently discovered transiting super-Earths in the Kepler-16 system may have also originated through similar mechanisms.",
        "ori-fast-z-score": 0.5570860145311556,
        "water-fast-z-score": 7.056422850727971,
        "rewrite-fast-z-score": 0.8320502943378436
    },
    {
        "original_text": "We present an analysis of the probability that supernovae enrich protoplanetary disks with heavy elements, and discuss its implications for planet formation theory. We find that this process is likely to occur in most cases where stars are formed within clusters containing massive stars (>8 Msun). In these environments, we estimate that about half of all solar-type stars will be enriched by at least one supernova event during their disk lifetimes. This result has important consequences on our understanding of how planets form around metal-rich stars. The discovery of extrasolar giant planets orbiting metal-rich stars suggests that planet formation may be more efficient when the host star s metallicity is high. However, it remains unclear whether such planets can also form around low-mass stars like the Sun. Recent theoretical studies suggest that planetesimal accretion onto growing planetary embryos could be inhibited if the gas surrounding them contains too much dust or ice particles produced by collisions between larger bodies. If so, then the efficiency of core growth would decrease as the amount of solids increases beyond some critical value.",
        "watermark_text": "We present an assessment of the probability that supernovae enrich protoplanetary disks with heavy components , and consider its consequences for planet development hypothesis . We see that this process is probably to arise in most instances where stars are created within clusters housing massive stars ( > 8 Msun ) .In these habitats , we estimate that about half of all solar - class stars will be enriched by at least one supernova event during their disk lifetimes . This result has significant implications on our knowing of how planets form around metal - rich stars .The observation of extrasolar giant planets orbiting metal - rich stars suggests that planet development possibly be more efficient when the host star s metallicity is high . However , it remains unsure whether such planets can also formation around low - density stars like the Sun .Recent conceptual research suggest that planetesimal accretion onto growing planetary embryos might be inhibited if the gas covering them contains too enough snow or ice particles generated by collisions between larger body . If so , then the ability of core growth would decrease as the quantity of solids increases beyond some essential value .",
        "rewrite_text": "We provide an evaluation of the likelihood that supernovae contribute heavy elements to protoplanetary disks and explore the implications of this for planet formation theories. Our analysis indicates that this phenomenon is likely to occur in most scenarios where stars are born in clusters containing massive stars (greater than 8 solar masses). In these environments, we estimate that around half of solar-type stars will experience enrichment from at least one supernova event during their disk lifetimes. This finding has important ramifications for our understanding of planet formation around metal-rich stars. Observations of extrasolar giant planets orbiting these stars suggest that planet formation may be more efficient when the host star has higher metallicity. However, it remains uncertain whether such planets can also form around stars with lower metallicity, similar to the Sun. Recent theoretical studies propose that the accretion of planetesimals onto developing planetary embryos could be hindered if the surrounding gas contains an excessive amount of snow or ice particles produced by collisions among larger bodies. If this is the case, the growth of planetary cores may be impeded once the amount of solid material surpasses a critical threshold.",
        "ori-fast-z-score": -1.3779972440082682,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": -1.876629726513673
    },
    {
        "original_text": "We propose to use thermal noise as the information carrier for totally secure communications, zero power consumption devices, and computing systems. We show that by using an appropriate detection scheme, we can extract digital bits from the thermal noise with high fidelity at room temperature. The proposed system is based on a single-electron transistor (SET) which has been demonstrated in experiments recently. In addition, we demonstrate how this SET-based device can be used to perform logic operations such as AND gate and NOT gate. Finally, we discuss possible applications of our proposal including quantum key distribution, ultra-low power sensor networks, and energy efficient data centers. This work was supported by NSF under Grant No. ECCS-0926491. A new paradigm called  thermal noise informatics  is introduced where thermal noise is utilized as the information carrier for various tasks ranging from totally secure communication over wires to zero-power communication and computation. By employing an appropriate detection scheme, it is shown that one can extract digital bits from thermal noise with high fidelities even at room temperatures. It is also shown that these extracted digital bits can be used to perform logical operations such as AND gates and NOT gates. Possible applications are discussed along with their advantages compared to existing technologies.",
        "watermark_text": "We suggest to use thermal noise as the information carrier for totally secure services , zero power consumption devices , and computing systems . We see that by using an appropriate detection system , we can extract digital bits from the thermal noise with high fidelity at room temperature .The proposed system is based on a single - ion transistor ( SET ) which has been shown in experiments recently . In addition , we prove how this SET - based device can be used to conduct logic events such as AND gate and NOT gate .Finally , we review possible users of our proposal including molecular key transmission , ultra - low power detector networks , and energy effective data systems . This project was supported by NSF under Grant No .ECCS - 0926491 . A different technology called temperature noise informatics is adopted where thermal noise is utilized as the information carrier for various jobs ranging from completely secure transmission over wires to zero - energy transmission and computation .By using an appropriate detection system , it is demonstrated that one can extract digital bits from temperature noise with high fidelities even at room temperatures . It is also shown that these obtained digital frames can be used to conduct logical functions such as AND doors and NOT gates .Possible products are discussed along with their benefits compared to existing technologies .",
        "rewrite_text": "We propose utilizing thermal noise as the information carrier for fully secure services, devices with zero power consumption, and computing systems. Our findings indicate that with the right detection system, it is possible to extract digital bits from thermal noise at room temperature with high fidelity. This system relies on a single-electron transistor (SET), which has recently demonstrated its capabilities in experimental settings. Furthermore, we illustrate how this SET-based device can perform logic operations, including AND and NOT gates. We also explore potential applications of our proposal, such as molecular key transmission, ultra-low power detector networks, and energy-efficient data systems. This project received support from the NSF under Grant No. ECCS-0926491. An alternative approach, known as temperature noise informatics, harnesses thermal noise as an information carrier for various tasks, ranging from secure wired transmission to zero-energy transfer and computation. Our results confirm that digital bits can be extracted from temperature noise with high fidelity, even at room temperature. These extracted digital frames can also be utilized for logical functions, such as AND and NOT gates. We discuss potential applications and their advantages over existing technologies.",
        "ori-fast-z-score": 1.3987572123604708,
        "water-fast-z-score": 8.541985556144386,
        "rewrite-fast-z-score": 2.9692614841855693
    },
    {
        "original_text": "We report the first experimental observation of environmental dielectric screening (EDS) effects on optical transitions between different electronic states in individual semiconducting single-wall carbon nanotubes (SWCNTs). We show that EDS can be used to tune the emission energy and linewidths of SWCNT photoluminescence, which is important for applications such as optoelectronic devices based on these materials. \n \n The observed changes are explained by considering how the local environment affects the electron-hole interaction strength through its influence on the dielectric constant at the position of each tube. This work provides new insights into the fundamental physics governing the properties of carbon nanotube-based systems. Carbon nanotubes have attracted considerable attention because they exhibit unique physical characteristics  1  . In particular, their one-dimensional structure leads to interesting phenomena not found in bulk or two-dimensional materials  2  , including quantum confinement  3  , ballistic transport  4  , and strong light-matter interactions  5  .\nIn addition, recent advances in chemical synthesis techniques  6  allow us to produce high-quality samples with controlled chiralities  7, 8  . These developments make it possible to study the intrinsic properties of carbon nanotubes without being affected by extrinsic factors  9  . However, despite this progress, there remain many open questions about the basic physics underlying carbon nanotube behavior  10  . For example, although theoretical studies predict that the band gap should depend strongly on the diameter  11  , experiments have shown only weak correlations  12  . One reason may be that the actual diameters of synthesized tubes often differ significantly from those predicted theoretically  13  . Another possibility is that the surrounding medium plays an important role  14  . Indeed, previous works have demonstrated that the presence of surfactants  15  , solvent molecules  16  , and water  17  can affect the optical properties of carbon nanotubes  18  .",
        "watermark_text": "We report the first laboratory measurement of environmental dielectric screening ( EDS ) impacts on electronic transitions between various electronic states in individual semiconducting single - wall carbon nanotubes ( SWCNTs ) . We see that EDS can be used to balance the emission intensity and linewidths of SWCNT photoluminescence , which is important for applications such as optoelectronic devices based on these materials .The observed changes are explained by examining how the local conditions influences the electron - hole collision strength through its influence on the dielectric constant at the position of each tube . This research provides new understanding into the fundamental theory controlling the properties of carbon nanotube - based systems .Carbon nanotubes have garnered considerable scrutiny because they demonstrate unique physical qualities 1 . In particular , their one - dimensional shape contributes to unusual phenomena not found in bulk or two - dimensional materials 2 , notably quantum confinement 3 , ballistic transport 4 , and strong dark - matter interactions 5 .In addition , recent developments in chemical production method 6 allow us to produce high - grade samples with safe chiralities 7 , 8 . These advances give it able to study the intrinsic characteristics of carbon nanotubes without being affected by extrinsic factors 9 .However , despite this progress , there remain many open questions about the fundamental theory surrounding carbon nanotube behavior 10 . For instance , although conceptual research predict that the band gap should depend greatly on the diameter 11 , researchers have shown only weak correlations 12 .One reason could be that the actual diameters of synthesized tubes often differ significantly from those predicted theoretically 13 . Another possibility is that the nearby medium contributes an important role 14 .Indeed , previous works have demonstrated that the presence of surfactants 15 , solvent molecules 16 , and water 17 can affect the optical properties of carbon nanotubes 18 .",
        "rewrite_text": "We present the first laboratory measurement of environmental dielectric screening (EDS) effects on electronic transitions among different electronic states in individual semiconducting single-wall carbon nanotubes (SWCNTs). Our findings indicate that EDS can effectively balance the emission intensity and linewidths of SWCNT photoluminescence, a crucial factor for applications involving optoelectronic devices based on these materials. The observed variations can be attributed to the influence of local conditions on electron-hole collision strength, specifically how these conditions affect the dielectric constant around each tube. This research enhances our understanding of the fundamental theories governing the properties of carbon nanotube-based systems. Carbon nanotubes have received significant attention due to their unique physical properties. Their one-dimensional structure leads to distinctive phenomena that are not present in bulk or two-dimensional materials, notably quantum confinement, ballistic transport, and strong interactions with dark matter. Recent advancements in chemical production methods have enabled the synthesis of high-quality samples with reliable chiralities, allowing for in-depth studies of the intrinsic properties of carbon nanotubes without interference from extrinsic factors. However, despite these advancements, many fundamental questions about the behavior of carbon nanotubes remain unresolved. For example, while theoretical models suggest that the band gap is heavily influenced by the diameter, empirical studies have shown only weak correlations. This discrepancy could arise from the actual diameters of synthesized tubes, which often differ significantly from theoretical predictions, or it may be that the surrounding medium plays a critical role. Previous studies have highlighted that the presence of surfactants, solvent molecules, and water can significantly impact the optical properties of carbon nanotubes.",
        "ori-fast-z-score": 1.7320508075688774,
        "water-fast-z-score": 8.601935165449925,
        "rewrite-fast-z-score": 2.014035259912054
    },
    {
        "original_text": "We propose that galactic dark matter is an effective four-dimensional manifestation of extra dimensions, and we show how this can be realized in a simple model with one extra dimension compactified on S 1 /Z 2 . The fifth dimension has two 3-branes at its endpoints which are connected by a bulk scalar field. We find that the scalar field develops a kink profile along the fifth dimension due to the presence of a potential barrier between the two branes. This leads to a localized mass term for fermions living on the visible (3-)brane, which gives rise to a phenomenologically viable dark matter candidate. In addition, there exists another class of particles called Kaluza-Klein modes whose masses depend on the size of the extra dimension. These KK states have no tree-level interactions with Standard Model fields but they may contribute significantly to loop processes such as neutrino oscillations or proton decay. Finally, we discuss possible experimental signatures of our scenario.",
        "watermark_text": "We suggest that galactic dark matter is an efficient four - dimensional manifestation of extra dimensions , and we prove how this can be realized in a simple model with one extra dimension compactified on S 1 / Z 2 . The fifth dimension has two 3 - branes at its endpoints which are connected by a bulk scalar field .We see that the scalar field becomes a kink profile along the fifth dimension owing to the presence of a potential barrier between the two branes . This leads to a localized mass term for fermions residing on the visible ( 3 - ) brane , which gives rise to a phenomenologically viable dark matter candidate .In addition , there exists another class of nuclei termed Kaluza - Klein modes whose masses vary on the size of the extra dimension . These KK states have no tree - level interactions with Standard Model fields but they may contribute greatly to loop processes such as neutrino oscillations or proton emission .Finally , we explain possible experimental signatures of our scenario .",
        "rewrite_text": "We propose that galactic dark matter can be effectively understood as a four-dimensional expression of extra dimensions. We demonstrate how this concept can be realized through a straightforward model featuring one extra dimension compactified on \\(S^1/Z_2\\). In this model, the fifth dimension contains two three-branes located at its endpoints, linked by a bulk scalar field. The scalar field develops a kink profile along the fifth dimension due to the potential barrier separating the two branes. This phenomenon generates a localized mass term for fermions on the visible three-brane, presenting a viable candidate for dark matter. Furthermore, there exists another class of particles known as Kaluza-Klein (KK) modes, whose masses fluctuate with the size of the extra dimension. Although these KK states do not interact at tree level with Standard Model fields, they may significantly influence loop processes, such as neutrino oscillations or proton emission. Lastly, we discuss potential experimental signatures that could validate our scenario.",
        "ori-fast-z-score": 0.6108472217815261,
        "water-fast-z-score": 3.8805700005813275,
        "rewrite-fast-z-score": 1.1785113019775793
    },
    {
        "original_text": "We present Chandra observations of supernova (SN) 2004et, which is one of only two type IIp SNe ever observed in X-rays. The data were obtained on 2005 February 24-26 with the Advanced CCD Imaging Spectrometer (ACIS-S). We detect no significant emission above background at energies below 1 keV or above 8 keV; we therefore restrict our analysis to the range 1-8 keV. In this energy band, we find that the spectrum can be fit by an absorbed blackbody model with kT = 0.7 ± 0.1 keV and N H = 2.5 +1.0 −0.8 × 10 22 cm −2 . These values are consistent with those found for other type IIp SNe. Using these parameters as well as the distance inferred from optical photometry, we calculate the luminosity of SN 2004et during its first 100 days after explosion. This value agrees very well with theoretical predictions based upon models of stellar evolution.",
        "watermark_text": "We report Chandra measurements of supernova ( SN ) 2004et , which is one of only two class IIp SNe actually seen in X - radiation . The data were obtained on 2005 February 24 - 26 with the Advanced CCD Imaging Spectrometer ( ACIS - S ) .We detect no considerable emission above background at energies below 1 keV or above 8 keV ; we thus restrict our analysis to the range 1 - 8 keV . In this power band , we find that the spectrum can be fit by an absorption blackbody simulation with kT = 0 . 7 ± 0 . 1 keV and N H = 2 . 5 + 1 . 0 −0 . 8 × 10 22 mm −2 .These quantities are compatible with those observed for other class IIp SNe . Using these parameters as well as the distance inferred from optical photometry , we estimate the luminosity of SN 2004et during its initial 100 days after explosion .This value agrees very best with theoretical estimates based upon theories of stars evolution .",
        "rewrite_text": "We present measurements of supernova (SN) 2004et obtained from Chandra, notable for being one of only two class IIp SNe detected in X-ray emissions. The data were collected using the Advanced CCD Imaging Spectrometer (ACIS-S) from February 24 to 26, 2005. Our analysis reveals no significant emission above background levels below 1 keV or above 8 keV, leading us to focus on the range between 1 and 8 keV. In this energy band, we find that the spectrum can be modeled by an absorption blackbody fit with kT = 0.7 ± 0.1 keV and N_H = 2.5 + 1.0 −0.8 × 10^22 cm^−2. These values are consistent with those found in other class IIp SNe. By using these parameters along with the distance determined from optical photometry, we estimate the luminosity of SN 2004et during the first 100 days post-explosion. This estimate aligns well with theoretical predictions based on stellar evolution theories.",
        "ori-fast-z-score": -0.42008402520840293,
        "water-fast-z-score": 5.285714285714286,
        "rewrite-fast-z-score": 1.9414506867883021
    },
    {
        "original_text": "We report on the growth and characterization of epitaxial La2/3Ca1/3MnO3-δ (LCMO) thin films grown on SrTiO3(001) (STO). The LCMO film thickness was varied between 5 nm to 50 nm, while keeping the substrate temperature fixed at 700 °C during deposition. We find that for thinner films there is an increase in oxygen deficiency as measured by XPS and XAS. This leads to a decrease in Curie temperature TC = 240 K down to 180 K with decreasing film thickness. In addition we observe a reduction in magnetization M0 below 20 nm which can be explained by the presence of a ferromagnetic dead layer near the interface. Using X-ray magnetic circular dichroim (XMCD), we show that this ferromagnetic dead layer has a net moment along the out-of-plane direction but no in-plane component. Finally, using soft x-ray resonant reflectivity measurements we demonstrate that the Mn valence state changes across the interface due to charge transfer into STO.",
        "watermark_text": "We report on the development and description of epitaxial La2 / 3Ca1 / 3MnO3 - δ ( LCMO ) thin sheets grown on SrTiO3 ( 001 ) ( STO ) . The LCMO movie length was changed between 5 nm to 50 nm , while maintaining the substrate temperature fixed at 700 °C during deposition .We see that for thinner films there is an increase in oxygen deficiency as measured by XPS and XAS . This leads to a reduction in Curie temperature TC = 240 K down to 180 K with decreasing film thickness .In addition we study a reduction in magnetization M0 below 20 nm which can be described by the presence of a ferromagnetic dead coating near the interface . Using X - ray magnetic spiral dichroim ( XMCD ) , we find that this ferromagnetic dead surface has a net moment along the out - of - plane path but no in - plane part .Finally , using soft x - ray resonant reflectivity surveys we prove that the Mn valence state changes across the interface due to charge transfer into STO .",
        "rewrite_text": "We present our findings on the development and characterization of epitaxial La2/3Ca1/3MnO3 - δ (LCMO) thin films deposited on SrTiO3 (001) (STO). The thickness of the LCMO films was varied between 5 nm and 50 nm while keeping the substrate temperature constant at 700 °C during the deposition process. Our measurements from X-ray photoelectron spectroscopy (XPS) and X-ray absorption spectroscopy (XAS) indicate that thinner films exhibit increased oxygen deficiency. Consequently, this results in a decrease in the Curie temperature from TC = 240 K to 180 K as the film thickness diminishes. Additionally, we observe a decline in magnetization (M0) for films thinner than 20 nm, which can be attributed to the existence of a ferromagnetic dead layer at the interface. Through X-ray magnetic circular dichroism (XMCD), we show that this ferromagnetic dead layer has a net magnetization aligned with the out-of-plane direction, but lacks a component in the in-plane direction. Finally, soft X-ray resonant reflectivity studies reveal that the Mn valence state varies across the interface due to charge transfer into the STO substrate.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": -0.9428090415820635
    },
    {
        "original_text": "We present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity  Fe/H  , mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute magnitude M.",
        "watermark_text": "We present the conclusion of our CCD photometric analysis of 42 open complexes in the southern hemisphere , conducted out at the 1 - meter telescope of the South African Astronomical Observatory ( SAAO ) . The images were made with an SBIG STL - 1001E camera equipped with a Kodak KAF - 0400 card and Johnson V filter during three observing runs between September 1998 and February 1999 .We have utilized DAOPHOT II to conduct aperture photometry on all stars observed within each cluster area - of - view . A total number of about 15000 stars was measured for each cluster .In addition we derived UBVRI photometry for some of these complexes use the same equipment as described above . From this data set we derived the following variables : reddening E ( B - V ) , distance modulus DM , age t , metallicity Fe / H , mass function slope x , core radius rc , central exterior brightness µ0 , concentration index c , and integrated absolute brightness M .",
        "rewrite_text": "We present the results of our CCD photometric analysis of 42 open clusters located in the southern hemisphere, carried out using the 1-meter telescope at the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera featuring a Kodak KAF-0400 chip and a Johnson V filter during three observing sessions from September 1998 to February 1999. We employed DAOPHOT II to perform aperture photometry on all stars within the field of view of each cluster, measuring approximately 15,000 stars per cluster. Additionally, we obtained UBVRI photometry for several of these clusters using the same equipment. From this dataset, we calculated several parameters, including reddening E(B-V), distance modulus DM, age t, metallicity [Fe/H], mass function slope x, core radius r_c, central surface brightness µ₀, concentration index c, and integrated absolute magnitude M.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 2.9448482384566077,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "In this thesis, we study power control problems in cellular mobile radio systems with code division multiple access (CDMA). We consider the uplink scenario where each user transmits to its base station using an orthogonal spreading sequence and all users share the same frequency band. The objective is to minimize the total transmit power subject to individual quality-of-service constraints at each user s receiver. In order to obtain tractable results, we make use of tools from stochastic geometry which allow us to model the locations of both mobiles and interferers as point processes. Our main contributions are summarized below.  First, we derive closed-form expressions for the outage probability when the number of active users grows without bound. These expressions can be used to determine how many users can simultaneously communicate reliably over the network. Second, we propose distributed power control schemes that achieve these limits asymptotically under certain conditions. Third, we develop centralized power control algorithms that guarantee performance close to optimality even if only partial information about the channel gains or interference levels is available. Finally, we present simulation results illustrating our theoretical findings.",
        "watermark_text": "In this dissertation , we study control power problems in cell mobile radio circuits with code division multiple entry ( CDMA ) . We consider the uplink situation where each consumer transmits to its base station use an orthogonal spreading pattern and all users share the same frequency band .The goal is to minimize the total broadcast capacity related to individual quality - of - service restrictions at each consumer s receiver . In order to obtain tractable results , we make using of tools from stochastic geometry which allow us to model the places of both mobiles and interferers as point processes .Our main contributions are presented below . First , we derive closed - form expressions for the outage likelihood when the number of active usage rises without bound .These expressions can be used to predict how many users can independently connect reliably over the network . Second , we develop dispersed power control schemes that attain these limits asymptotically under certain conditions .Third , we develop concentrated control power methods that guarantee efficiency high to optimality even if only partial knowledge about the channel gains or interference concentrations is accessible . Finally , we present modeling results illustrating our theoretical results .",
        "rewrite_text": "In this dissertation, we investigate control power issues in mobile radio circuits that utilize code division multiple access (CDMA). Our focus is on the uplink scenario, where each user transmits to their base station using an orthogonal spreading code, with all users sharing the same frequency band. The objective is to minimize the total broadcast capacity while adhering to individual quality-of-service requirements at each user's receiver. To achieve tractable results, we employ tools from stochastic geometry, which enable us to model the locations of both mobile users and interferers as point processes. Our main contributions are as follows: First, we derive closed-form expressions for the outage probability as the number of active users approaches infinity. These expressions provide insights into the maximum number of users that can maintain independent and reliable connections within the network. Second, we propose distributed power control schemes that asymptotically reach these limits under specific conditions. Third, we develop centralized power control strategies that ensure near-optimal efficiency, even when only partial information about channel gains or interference levels is available. Finally, we present modeling results that illustrate our theoretical findings.",
        "ori-fast-z-score": 0.19802950859533489,
        "water-fast-z-score": 7.4,
        "rewrite-fast-z-score": 1.0838622059807226
    },
    {
        "original_text": "We present results on searches for gravitational wave (GW) burst signals using data collected by the Laser Interferometer Gravitational-Wave Observatory during its fourth science run, which took place between September 2005 and January 2007. We use two different search methods to look for GW bursts: one based on matched filtering with template waveforms and another that uses an optimal filterbank method. The latter is used as part of a blind analysis where we do not know what type or strength of signal may be present in our data until after it has been analyzed. In addition to these analyses, we also perform several consistency checks designed to identify any problems associated with either detector s performance over this period. No significant candidates are found in any of these searches. Using simulated signals injected into the data at random times, we estimate upper limits on the rate density of binary black hole mergers detectable within a given range of total mass.",
        "watermark_text": "We report findings on investigations for gravitational wave ( GW ) burst signals using data amassed by the Laser Interferometer Gravitational - Wave Observatory during its fourth science run , which taken place between September 2005 and January 2007 . We use two different search methods to find for GW bursts : one based on paired processing with template waveforms and another that using an efficient filterbank method .The latter is utilized as part of a blind analysis where we do not understand what type or strength of signal might be found in our information until after it has been examined . In addition to these tests , we also perform several reliability measures designed to identify any problems involved with either detector s performance over this time .No meaningful candidates are found in any of these searches . Using simulated transmissions imported into the information at random times , we estimate upper limits on the rate concentration of binary dark hole mergers detectable within a given range of total mass .",
        "rewrite_text": "We present our findings from investigations into gravitational wave (GW) burst signals utilizing data collected by the Laser Interferometer Gravitational-Wave Observatory during its fourth science run, which spanned from September 2005 to January 2007. We employed two distinct search methods to detect GW bursts: one based on paired processing with template waveforms, and another utilizing an efficient filterbank technique. The latter method was part of a blind analysis, meaning we had no prior knowledge of the type or strength of the signals present in our data until after the analysis was completed. Additionally, we conducted several reliability assessments aimed at identifying any potential issues with the performance of the detectors throughout this period. Ultimately, no significant candidates were identified in any of our searches. By incorporating simulated signals at random intervals within the data, we estimated upper limits on the detectable rate of binary black hole mergers based on a specified total mass range.",
        "ori-fast-z-score": -1.1322770341445956,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": -1.7320508075688772
    },
    {
        "original_text": "We report the discovery and characterization of XO-2b, an extrasolar planet transiting its host star (HD 149026) with a period of 3.2 days. The planet is a hot Jupiter with M = 1.3 MJup and R = 0.9 RJup orbiting at a distance of only 0.04 AU from HD 149026. We find that this system has a common proper motion companion separated by ~1′′.5. This companion was previously identified as a metal-rich subgiant based on high-resolution spectroscopy but had not been detected photometrically before our observations. Our analysis shows that the transit depth variation observed for XO-2b can be explained if we assume that the two stars are physically associated and have nearly identical radii. If true, then the mass ratio between these two stars should be close to unity. However, we cannot rule out other scenarios such as grazing eclipses or blending effects due to nearby field stars.",
        "watermark_text": "We report the discovery and description of XO - 2b , an extrasolar planet transiting its home star ( HD 149026 ) with a period of 3 . 2 days . The planet is a hot Jupiter with M = 1 . 3 MJup and R = 0 . 9 RJup orbiting at a distance of only 0 . 04 AU from HD 149026 .We see that this system has a common proper motion companion divided by ~ 1 ′ ′ . 5 . This companion was formerly identified as a metal - rich subgiant based on high - resolution spectroscopy but had not been detected photometrically before our observations .Our study shows that the transit size variation detected for XO - 2b can be described if we suppose that the two stars are visually associated and have nearly identical radii . If true , then the mass ratio between these two stars should be close to unity .However , we cannot rule out other scenarios such as grazing eclipses or mixing effects due to nearby field stars .",
        "rewrite_text": "We present the discovery and characterization of XO-2b, an exoplanet that transits its host star, HD 149026, with an orbital period of 3.2 days. XO-2b is classified as a hot Jupiter, with a mass of 1.3 times that of Jupiter and a radius of 0.9 times that of Jupiter, orbiting at a proximity of just 0.04 AU from HD 149026. Our observations reveal that this system has a companion with a common proper motion, located approximately 1.5 arcminutes away. This companion, previously identified as a metal-rich subgiant through high-resolution spectroscopy, had not been detected photometrically until our research. We find that the observed variations in the transit size of XO-2b can be explained by the assumption that both stars are visually associated and possess nearly identical radii. If this is the case, the mass ratio of the two stars would likely be close to one. However, we also acknowledge the possibility of other scenarios, such as grazing eclipses or interactions with nearby field stars.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.5151005964822444,
        "rewrite-fast-z-score": 0.8819171036881969
    },
    {
        "original_text": "We present an ab initio study of the electronic and magnetic structure of volborthite, CaFe3(PO4)2(OH)3·H2O (CFPOH), which is one of the most important minerals in geological sciences as it forms at low temperatures under hydrothermal conditions. Volborthite has been studied extensively by neutron scattering experiments but its microscopic origin remains controversial. We show that the ground state of CFPOH can be described within density functional theory using the generalized gradient approximation plus Hubbard U method for Fe-3d orbitals. The calculated spin wave spectrum agrees well with experimental data obtained by inelastic neutron scattering measurements. In addition we find that the magnetocrystalline anisotropy energy is dominated by spin-orbit coupling effects. Finally, we discuss how our results are related to previous theoretical studies based on different approximations. V olborthite, CaF e 3 (P O 4 ) 2 (OH) 3 ·H 2 O (C F P OH ), is one of the most impor-tant minerals in geological sciences because it forms at low tem-peratures under hydrothermal conditions  1  . It was first discovered in 1832  2  , however, only recently have detailed structural investigations revealed that this mineral belongs to the family of compounds known as  Kagome  materials  3  .\nVolborthite crystallizes into a layered structure consisting of alternating kagome planes of iron ions and phosphate groups  4  . This arrangement leads to interesting physical phenomena such as geometric frustration  5  or quantum fluctuations  6  . For example, recent neutron scattering experiments suggest that volborthite undergoes a phase transition below T N = 5 K  7, 8  where the spins order ferrimagnetically along the c-axis  9  . However, there exists no consensus about the nature of this ordering  10  : while some authors claim that the system orders collinearly  11, 12  others argue that non-collinearity plays an essential role  13, 14  .",
        "watermark_text": "We present an ab initio investigation of the electronic and magnetic shape of volborthite , CaFe3 ( PO4 ) 2 ( OH ) 3 · H2O ( CFPOH ) , which is one of the most important minerals in geological sciences as it exists at low temperatures under hydrothermal conditions . Volborthite has been studied frequently by neutron scattering experiments but its microscopic source remains disputed .We see that the ground state of CFPOH can be described within density functional theory using the generalized gradient algorithm plus Hubbard U method for Fe - 3d orbitals . The measured spin wave spectrum agrees well with theoretical data derived by inelastic neutron scattering observations .In addition we find that the magnetocrystalline anisotropy energy is dominated by spin - orbit bonding effects . Finally , we explain how our findings are related to previous conceptual research depending on various approximations .V olborthite , CaF e 3 ( P O 4 ) 2 ( OH ) 3 · H 2 O ( C F P OH ) , is one of the most impor - tant salts in geological sciences because it exists at low tem - peratures under hydrothermal conditions 1 . It was first discovered in 1832 2 , however , only lately have sophisticated structural investigations revealed that this mineral belongs to the group of compounds known as Kagome materials 3 .Volborthite crystallizes into a layered structure formed of alternating kagome planes of iron ions and phosphate groups 4 . This configuration leads to curious physical phenomena such as geometric instability 5 or quantum fluctuations 6 .For instance , recent neutron scattering experiments indicate that volborthite undergoes a phase shift below T N = 5 K 7 , 8 where the spins order ferrimagnetically along the c - axis 9 . However , there exists no consensus about the nature of this ordering 10 : while some writers claim that the scheme orders collinearly 11 , 12 others argue that non - collinearity plays an essential part 13 , 14 .",
        "rewrite_text": "We present an ab initio study of the electronic and magnetic properties of volborthite, CaFe3(PO4)2(OH)3·H2O (CFPOH), a key mineral in geological sciences that forms under low temperatures in hydrothermal environments. Although volborthite has been frequently investigated through neutron scattering experiments, its microscopic nature remains a subject of debate. Our analysis demonstrates that the ground state of CFPOH can be accurately described using density functional theory, implemented with a generalized gradient approximation and a Hubbard U correction for the Fe-3d orbitals. The spin wave spectrum we observed aligns closely with theoretical predictions obtained from inelastic neutron scattering studies. Additionally, we find that the magnetocrystalline anisotropy energy is significantly influenced by spin-orbit coupling effects. We also discuss how our results relate to previous theoretical work, which has relied on various approximations. First discovered in 1832, volborthite has gained recognition as a significant geological mineral, revealing through recent structural analyses its classification as a member of the Kagome material family. It features a layered structure, alternating between kagome planes of iron ions and phosphate groups, which gives rise to intriguing phenomena such as geometric frustration and quantum fluctuations. Notably, recent neutron scattering research indicates a transition occurring below T_N = 5 K, where the magnetic spins align ferrimagnetically along the c-axis. However, there is no consensus regarding the exact nature of this spin ordering; while some researchers argue for collinear alignment, others assert the importance of non-collinearity.",
        "ori-fast-z-score": 1.0606601717798212,
        "water-fast-z-score": 7.187587726270522,
        "rewrite-fast-z-score": 0.09090909090909091
    },
    {
        "original_text": "We present the results for squark-antisquark, gluino-gluon and gaugino-gauge boson production at hadron colliders within the framework of non-minimal flavour violating supersymmetric models (NMFV). We consider both NMFV scenarios with MFV-like structure as well as those without it. In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks due to the presence of new sources of flavour violation beyond minimal supergravity. The latter are induced by the non-diagonal entries of the sfermion mass matrices which can be sizeable even if they are generated only radiatively. Our analysis is performed using an effective field theory approach where all heavy particles are integrated out except for the lightest neutral Higgs boson h0 and the Z-boson. This allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "watermark_text": "We present the results for squark - antisquark , gluino - gluon and gaugino - gauge boson development at hadron colliders within the framework of non - reduced flavour violating supersymmetric theories ( NMFV ) . We consider both NMFV scenarios with MFV - like structure as well as those without it .In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks owing to the presence of new sources of flavour violation beyond minimal supergravity . The latter are induced by the non - diagonal entries of the sfermion mass matrices which can be sizeable even if they are produced only radiatively .Our study is conducted using an efficient field model approach where all heavy ions are integrated out except for the lightest neutral Higgs boson h0 and the Z - boson . This enables us to derive analytical expressions for the appropriate amplitudes and cross sections .",
        "rewrite_text": "We report our findings on the development of squark-antisquark, gluino-gluon, and gaugino-gauge boson interactions at hadron colliders, specifically under the framework of non-minimal flavor-violating supersymmetric theories (NMFV). Our analysis encompasses both NMFV scenarios that possess an MFV-like structure as well as those that do not. We particularly investigate how the presence of new sources of flavor violation, extending beyond minimal supergravity, affects the decay branching ratios of neutralinos into leptons and quarks. These new sources arise from non-diagonal entries in the sfermion mass matrices, which can be significant even when generated solely through radiative processes. Our study employs an efficient field model approach where we integrate out all heavy particles, retaining only the lightest neutral Higgs boson \\( h_0 \\) and the \\( Z \\)-boson. This methodology allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "ori-fast-z-score": -0.13736056394868904,
        "water-fast-z-score": 2.8845718429224694,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We consider the problem of scheduling multiple bag-of-work applications on parallel machines with non-cooperative tasks, where each task has its own deadline and budget constraints. We propose an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets. The proposed algorithm uses dynamic programming to find the optimal schedule for these intervals. Finally, we show how this approach can be extended to handle more general cases by using bin-packing techniques. Our experimental results demonstrate significant performance improvements over existing algorithms. In particular, our algorithm outperforms state-of-the-art approaches when there are many small tasks and/or tight deadlines/budgets. Keywords: Parallel computing, Computational complexity theory, Computational geometry, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems",
        "watermark_text": "We consider the question of scheduling multiple bag - of - job applications on parallel machines with non - cooperative tasks , where each work has its own deadline and funding limitations . We suggest an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets .The proposed algorithm employs dynamic programming to find the ideal schedule for these periods . Finally , we show how this methodology can be improved to treat more general instances by using bin - packing methods .Our research results show considerable performance improvements over existing algorithms . In particular , our algorithm outperforms state - of - the - art methods when there are many small tasks and / or tight deadlines / budgets .Keywords : Parallel computing , Computational complexity analysis , Computational topology , Data mining , Bioinformatics , High - performance computing , Grid computing , Cloud computing , Big data analysis , Distributed systems",
        "rewrite_text": "We explore the challenge of scheduling multiple bag-of-jobs applications on parallel machines where tasks do not cooperate, each having its own deadlines and budget constraints. Our proposed algorithm revolves around partitioning time into intervals, allowing all tasks within each interval to be scheduled simultaneously without breaching their deadlines or budget limits. This approach utilizes dynamic programming to determine the optimal schedule for these intervals. Furthermore, we demonstrate how this methodology can be enhanced to address more complex scenarios by incorporating bin-packing techniques. Our research indicates significant performance gains compared to existing algorithms, particularly excelling in cases with numerous small tasks and/or stringent deadlines and budgets. \n\nKeywords: Parallel computing, Computational complexity analysis, Computational topology, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "We present numerical results for the evolution of two different types of solar coronal mass ejection (CME) in an open magnetic field configuration, which is relevant to space weather applications. The first type of CMEs are driven by photospheric motions that generate Alfvén waves at the base of the corona; these waves propagate upward into the corona where they steepen into shocks. The second type of CMEs are triggered by flux rope instabilities associated with current sheets formed during reconnection events between closed loops and open fields lines. We find that both types of CMEs can accelerate plasma up to speeds exceeding 1000 km/s. However, only the first type of CMEs have significant density enhancements compared to their surroundings. In addition, we show that the first type of CME has a higher probability of producing geomagnetic storms than the second one because it contains more energetic particles. Finally, our results suggest that the first type of simulated CMEs may be responsible for some observed halo CMEs.",
        "watermark_text": "We present numerical findings for the evolution of two different kinds of thermal coronal mass ejection ( CME ) in an open magnetic field configuration , which is relevant to space weather uses . The first kinds of CMEs are driven by photospheric movements that produce Alfvén currents at the base of the corona ; these currents propagate eastward into the corona where they steepen into shocks .The second kind of CMEs are driven by flux rope instabilities associated with current sheets formed during reconnection events between locked loops and open fields lines . We see that both types of CMEs can accelerate plasma up to speeds exceeding 1000 kilometers / s .However , only the first class of CMEs have considerable density enhancements compared to their environment . In addition , we find that the first class of CME has a higher likelihood of creating geomagnetic winds than the second one because it contains more intense ions .Finally , our findings confirm that the first sort of simulated CMEs might be responsible for some observed halo CMEs .",
        "rewrite_text": "We present numerical results on the evolution of two distinct types of thermal coronal mass ejections (CMEs) in an open magnetic field configuration, which have implications for space weather applications. The first type of CME is driven by photospheric movements that generate Alfvén currents at the base of the corona; these currents travel eastward into the corona, where they steepen and form shocks. The second type of CME arises from flux rope instabilities linked to current sheets formed during reconnection events between locked loops and open field lines. Our observations reveal that both types of CMEs can accelerate plasma to speeds exceeding 1000 kilometers per second. However, only the first category exhibits significant density enhancements relative to its surroundings. Furthermore, we find that the first type of CME is more likely to generate geomagnetic winds due to its higher concentration of intense ions. Ultimately, our results suggest that the first type of simulated CMEs may account for some of the observed halo CMEs.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 0.9058216273156765
    },
    {
        "original_text": "We present detailed spectral analysis for the type Ib supernova (SN) 1999dn, which was discovered on September 28th in NGC 3184 by Maza et al. (1999) . The photometric evolution is well reproduced with an exponential decay law and we find that the light curve can be explained as being powered by radioactive 56Ni synthesized during explosive nucleosynthesis. We have analyzed optical spectra obtained at various epochs after explosion using non-LTE time-dependent radiative transfer calculations based on our new code STELLA. In this work, we show results for models without hydrogen lines. Our best-fit model has a total ejecta mass of about 18M⊙ , consisting mainly of helium and carbon-oxygen mixture. This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia. Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "watermark_text": "We present detailed spectral study for the class Ib supernova ( SN ) 1999dn , which was discovered on September 28th in NGC 3184 by Maza et al . ( 1999 ) .The photometric evolution is well illustrated with an exponential decay law and we find that the light spiral can be described as being driven by radioactive 56Ni extracted during explosive nucleosynthesis . We have analyzed laser spectra obtained at numerous epochs after explosion employing non - LTE time - dependent radiative transfer calculations based on our new code STELLA .In this work , we show results for models without hydrogen lines . Our best - fit model has a total ejecta mass of about [UNK] , consisting mainly of helium and carbon - oxygen mixture .This result suggests that SN 1999dn may belong to the class of super-luminous SNe Ia.Keywords: Supernovae, Radiation hydrodynamics, Time dependent",
        "rewrite_text": "We present a comprehensive spectral analysis of the Ib supernova (SN) 1999dn, which was discovered in NGC 3184 by Maza et al. on September 28, 1999. The photometric evolution of this supernova is effectively represented by an exponential decay law, indicating that the light curve is driven by the radioactive decay of 56Ni produced during the explosive nucleosynthesis. We analyzed laser spectra collected at various time points post-explosion using non-LTE time-dependent radiative transfer calculations conducted with our newly developed code, STELLA. In this study, we present results for models that exclude hydrogen lines. Our best-fit model suggests a total ejecta mass of approximately [UNK], predominantly composed of a helium and carbon-oxygen mixture. This finding indicates that SN 1999dn may be classified among the super-luminous SNe Ia. Keywords: Supernovae, Radiation hydrodynamics, Time-dependent.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.2071349029490928,
        "rewrite-fast-z-score": 0.1259881576697424
    },
    {
        "original_text": "We present an analysis of the stability of planetary systems in which protoplanetary embryos evolve under oligarchy, i.e., they are able to eject each other s neighbors by gravitational scattering but not themselves. We find that this process leads to rapid growth of the largest embryo until it reaches its isolation mass (the minimum mass required for runaway accretion). The system then evolves into either a single planet or two planets with comparable masses depending on how close the initial conditions were to instability. This evolution is very different than what happens when all bodies grow simultaneously; in particular, we show that there can be multiple stable outcomes even if the initial conditions are identical. Our results suggest that the formation of terrestrial planets may have proceeded through several stages including oligarchy before reaching their final state as observed today. In addition, our work provides new insights about the origin of Mercury-like planets. Protoplanetary embryos form in circumstellar disks around young stars and undergo mutual gravitational interactions during their growth phase. These interactions lead to orbital migration and dynamical instabilities such as collisions between neighboring embryos. If these processes occur frequently enough, only one body will survive at the end of the growth stage leaving behind a planetary system consisting of just one planet. However, recent studies indicate that many planetary systems contain more than one planet suggesting that some mechanism must exist to prevent complete destruction of the system. Here we study the possibility that protoplanetary embryos follow a hierarchical evolutionary path where they first grow hierarchically via gravitational scattering followed by runaway accretion once the largest embryo has reached its isolation mass. Using numerical simulations, we demonstrate that this scenario naturally explains the existence of multi-planet systems while also reproducing the properties of known exoplanets.",
        "watermark_text": "We present an assessment of the stability of planetary networks in which protoplanetary embryos grow under oligarchy , i . e . , they are able to eject each other s neighbors by gravitational scattering but not themselves . We see that this process results to rapid growth of the greatest embryo until it hits its isolation volume ( the minimum mass needed for runaway accretion ) .The system then evolves into either a single planet or two planets with similar masses depending on how close the early conditions were to instability . This evolution is very different than what happens when all bodies grow simultaneously ; in particular , we prove that there can be several stable outcomes even if the first conditions are matched .Our results show that the formation of terrestrial worlds may have continued through several stages including oligarchy before reaching their final condition as found today . In addition , our work offer additional information about the origin of Mercury - like planets .Protoplanetary embryos form in circumstellar disks around new stars and undergo mutual gravitational interactions during their development period . These interactions result to orbital movement and dynamical instabilities such as collisions between neighboring embryos .If these mechanisms occur frequently enough , only one body will survive at the end of the development period leaving behind a planetary system consisting of just one planet . However , recent studies demonstrate that several planetary complexes include more than one planet suggesting that some method may arise to resist total destruction of the system .Here we study the prospect that protoplanetary embryos continue a hierarchical evolutionary course where they originally grow hierarchically via gravitational waves followed by runaway accretion once the greatest embryo has reached its isolation volume . Using numerical simulations , we prove that this situation naturally explains the existence of dual - planet systems while actually reproducing the properties of known exoplanets .",
        "rewrite_text": "We provide an evaluation of the stability of planetary networks in which protoplanetary embryos grow under oligarchy. In this scenario, embryos can gravitationally scatter their neighbors but cannot eject themselves. This process leads to the rapid growth of the largest embryo until it reaches its isolation volume, which is the minimum mass required for runaway accretion. Following this, the system evolves into either a single planet or two planets with comparable masses, depending on how close the initial conditions are to instability. This evolutionary path contrasts sharply with the scenario where all bodies grow simultaneously; notably, we demonstrate that multiple stable outcomes are possible even when initial conditions are identical. Our findings suggest that the formation of terrestrial planets likely progressed through several stages, including oligarchy, before achieving their current state. Furthermore, our research provides insights into the origins of Mercury-like planets. Protoplanetary embryos develop within circumstellar disks around new stars and experience mutual gravitational interactions throughout their growth phase. These interactions can lead to orbital movements and dynamic instabilities, such as collisions between neighboring embryos. If these events occur frequently enough, only one body may survive the development period, resulting in a planetary system with a single planet. However, recent studies indicate that many planetary systems consist of multiple planets, suggesting mechanisms that enable some bodies to avoid complete destruction. In this study, we explore the possibility that protoplanetary embryos follow a hierarchical evolutionary trajectory, initially growing through gravitational interactions and later experiencing runaway accretion once the largest embryo attains its isolation volume. Through numerical simulations, we demonstrate that this framework can naturally account for the existence of dual-planet systems while effectively replicating the characteristics of known exoplanets.",
        "ori-fast-z-score": -0.8333333333333334,
        "water-fast-z-score": 7.166666666666667,
        "rewrite-fast-z-score": 1.4316582658130823
    },
    {
        "original_text": "We study the magnetization dynamics driven by an alternating spin polarized current (ASPC) flowing through a magnetic tunnel junction with perpendicular anisotropy. We show that, depending on the amplitude of the ASPC, two different regimes can be observed experimentally: i) for small amplitudes, we observe a single frequency corresponding to the ferromagnetic resonance; ii) when increasing the amplitude of the ASCP, several frequencies are excited simultaneously leading to a complex spectrum which is analyzed using numerical simulations based on the Landau-Lifshitz-Gilbert equation including spin-transfer torque terms. The results obtained are discussed in connection with recent experiments performed at room temperature. \n \n PACS: 75.60.Cc, 76.30.+z, 77.20.Hs \n \n Spin transfer torques have been extensively studied both theoretically and experimentally during last years  1-3 . In particular, it has been shown that they induce precessional motion of the magnetization  4-6  as well as steady-state phenomena  7-9  such as domain-wall motion  10-12  or vortex core reversal  13-15 . These effects have attracted great interest due to their potential applications in novel devices like microwave oscillators  16  , logic elements  17  , memories  18  . However, most studies were focused on macroscopic systems where the magnetization was uniform over large distances. Recently, there has been growing interest in studying these effects in nanostructures  19-21  since this allows one to explore new physical properties associated with reduced dimensions  22  .\n \nIn this work, we focus our attention on the magnetization dynamics driven out of equilibrium by an alternating spin polarized Current (ASPC). This problem has already been addressed theoretically  23  but only few experimental works have been reported so far  24  . Here, we present detailed measurements carried out on a magnetic tunnel junction (MTJ), made of CoFeB/MgO/CoFeB layers grown by sputtering  25  . By applying an external field Hext along the hard axis of the MTJ, we obtain a perpendicularly magnetized system whose static properties are described elsewhere  26  . When",
        "watermark_text": "We research the magnetization dynamics generated by an alternating spin polarized current ( ASPC ) flowing through a magnetic tunnel connecting with transverse anisotropy . We see that , depending on the frequency of the ASPC , two different regimes can be experienced experimentally : i ) for large amplitudes , we perceive a single signal relating to the ferromagnetic resonance ; ii ) when reducing the frequency of the ASCP , various frequencies are excited simultaneously led to a complex spectrum which is evaluated using numerical simulations based on the Landau - Lifshitz - Gilbert formula featuring spin - transfer torque terms .The results collected are discussed in connection with recent experiments conducted at room temperature . PACS : 75 . 60 . Cc , 76 . 30 . + z , 77 . 20 . Hs Spin transfer torques have been heavily examined both theoretically and experimentally during last decades 1 - 3 .In particular , it has been shown that they cause precessional behavior of the magnetization 4 - 6 as well as continuous - phase phenomena 7 - 9 such as domain - wall motion 10 - 12 or vortex core reversal 13 - 15 . These effects have garnered great concern thanks to their potential applications in novel systems like microwave oscillators 16 , logic devices 17 , memories 18 .However , most studies were focused on macroscopic environments where the magnetization was uniform over large distances . Recently , there has been growing interest in investigating these phenomena in nanostructures 19 - 21 since this enables one to examine novel physical properties associated with decreased size 22 .In this research , we focus our focus on the magnetization dynamics caused out of equilibrium by an alternating spin polarized Current ( ASPC ) . This problem has already been addressed theoretically 23 but only few experimental works have been reported so recently 24 .Here , we present detailed observations carried out on a magnetic tunnel junction ( MTJ ) , made of CoFeB / MgO / CoFeB layers grown by sputtering 25 . By applying an external field Hext along the hard axis of the MTJ , we obtain a perpendicularly magnetized body whose dynamic characteristics are explained elsewhere 26 .When",
        "rewrite_text": "We investigate the magnetization dynamics induced by an alternating spin-polarized current (ASPC) flowing through a magnetic tunnel junction featuring transverse anisotropy. Our findings reveal that the experimental observations can be categorized into two distinct regimes based on the frequency of the ASPC: i) at high amplitudes, we detect a single signal corresponding to ferromagnetic resonance; ii) as the ASPC frequency decreases, multiple frequencies are simultaneously excited, resulting in a complex spectrum analyzed through numerical simulations grounded in the Landau-Lifshitz-Gilbert equation, including spin-transfer torque contributions. The outcomes of this research are discussed in relation to recent experiments conducted at room temperature. PACS: 75.60.Cc, 76.30.+z, 77.20.Hs.\n\nSpin-transfer torques have been extensively studied both theoretically and experimentally over the past few decades. It has been demonstrated that these torques lead to precessional behavior of magnetization, as well as continuous-phase phenomena such as domain wall motion and vortex core reversal. These effects have attracted significant interest due to their potential applications in innovative systems like microwave oscillators, logic devices, and memory technologies. However, most investigations have centered on macroscopic settings where magnetization is uniform across large distances. Recently, interest has grown in exploring these phenomena within nanostructures, as this allows for the examination of new physical properties associated with reduced dimensions. In this study, we focus on the out-of-equilibrium magnetization dynamics resulting from an alternating spin-polarized current (ASPC). Although this issue has been theoretically explored, only a limited number of experimental studies have emerged in recent times. Here, we present comprehensive observations conducted on a magnetic tunnel junction (MTJ) composed of CoFeB/MgO/CoFeB layers, which were deposited via sputtering. By applying an external magnetic field along the hard axis of the MTJ, we achieve a perpendicularly magnetized structure, the dynamic characteristics of which are detailed elsewhere.",
        "ori-fast-z-score": 0.34050261230349943,
        "water-fast-z-score": 8.342314001435737,
        "rewrite-fast-z-score": 1.8333333333333333
    },
    {
        "original_text": "We consider the dynamics of stochastic interacting particle systems in which particles evolve according to an overdamped Langevin equation with multiplicative noise and interact via pair potentials that decay exponentially fast at large distances.  We prove that, under suitable conditions on the interaction potential, these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator. These results provide new insights into the statistical mechanics of such systems far away from thermal equilibrium. Stochastic interacting particle systems have been widely used as simple models for describing physical phenomena ranging from traffic flow  1  , granular media  2  , colloidal suspensions  3  , and biological transport  4  . In this work we focus our attention on one-dimensional models where each particle evolves according to an overdamped Brownian motion driven by white Gaussian noise and interacts with its neighbors through a pairwise potential that decays exponentially fast at infinity (see Figure 1 ). The resulting system is described by the following set of Itô SDE s:",
        "watermark_text": "We consider the dynamics of stochastic interacting particle structures in which particles evolve according to an overdamped Langevin equation with multiplicative sound and interact via pair potentials that decay exponentially rapidly at large distances . We prove that , under suitable conditions on the interaction potential , these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator .These data provide novel knowledge into the statistical mechanics of such systems distant distant from temperature equilibrium . Stochastic interacting particle networks have been widely using as simple models for describing physical phenomena ranging from street passage 1 , granular material 2 , colloidal suspensions 3 , and biological transport 4 .In this research we focus our focus on one - dimensional theories where each particle evolves due to an overdamped Brownian movement generated by white Gaussian interference and interacts with its friends through a pairwise potential that decays exponentially rapidly at infinity ( see Figure 1 ) . The resulting system is characterized by the following setting of Itô SDE s :",
        "rewrite_text": "We examine the dynamics of stochastic interacting particle systems, where particles evolve according to an overdamped Langevin equation influenced by multiplicative noise and interact via pair potentials that rapidly decrease with distance. We demonstrate that, under appropriate conditions on the interaction potential, these models have unique stationary states, with their density profiles defined by solutions to nonlinear integral equations that include fractional powers of the Laplacian operator. This research offers new insights into the statistical mechanics of systems that are far from thermal equilibrium. Stochastic interacting particle networks have been extensively utilized as straightforward models to represent various physical phenomena, including pedestrian movement, granular materials, colloidal suspensions, and biological transport. In this study, we concentrate on one-dimensional models where each particle undergoes overdamped Brownian motion generated by white Gaussian noise and interacts with other particles through a rapidly decaying pairwise potential (see Figure 1). The resulting system is characterized by a specific framework of Itô stochastic differential equations (SDEs):",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": -1.2222222222222223
    },
    {
        "original_text": "We study the gravitational field equations for static spherically symmetric configurations on Randall-Sundrum type II brane-worlds with bulk cosmological constant and matter fields localized on the branes. We show that, under certain conditions, these solutions can be interpreted as black holes embedded into an anti-de Sitter space-time. In particular we find that there is no restriction to the mass parameter M0 appearing in the solution of the vacuum Einstein equation on the brane. The corresponding horizon radius r0 satisfies the relation r0 = (3M0/4π)1/3. This result implies that the Schwarzschild-de Sitter metric describes not only black hole but also naked singularity solutions. Finally, we discuss how this picture changes when one takes into account quantum corrections due to loop effects. PACS numbers: 04.20.-q; 11.10.Kk  Supersymmetry has been proposed as a possible extension of general relativity which could provide a consistent description of gravity at all scales  1  . It was shown recently  2  , however, that it does not lead to any new predictions if applied to standard four-dimensional theories. On the other hand, higher dimensional extensions of supergravity have attracted considerable attention during recent years  3  .\nIn this letter we consider five-dimensional supergravities  4  where the extra dimension is compactified on a circle  5  or orbifold  6  . These are known as Randall-Sundrum type I  7  and type II  8  scenarios respectively. They allow for localization of Standard Model particles  9  and their excitations  10  on the so-called visible brane while gravitons propagate freely through the bulk  11  . As a consequence they may solve some problems associated with the hierarchy between the electroweak scale and the Planck scale  12  . Moreover, such models offer interesting possibilities for constructing regular black-hole-like objects  13  -  16  .",
        "watermark_text": "We research the gravitational field equations for static spherically symmetric configurations on Randall - Sundrum type II brane - planets with bulk cosmological constant and material fields confined on the branes . We see that , under certain conditions , these solutions can be interpreted as black holes inserted into an anti - de Sitter space - time .In particular we find that there is no limitation to the mass vector M0 appearing in the solve of the vacuum Einstein integral on the brane . The corresponding horizon radius r0 satisfies the formula r0 = ( 3M0 / 4π ) 1 / 3 .This result suggests that the Schwarzschild - de Sitter metric encompasses not only black hole but also naked singularity solutions . Finally , we explain how this picture changes when one takes into consideration quantum corrections due to loop processes .PACS codes : 04 . 20 . - q ; 11 . 10 . Kk Supersymmetry has been proposed as a possible extension of general relativity which could give a consistent description of gravitational at all scales 1 . It was shown recently 2 , however , that it does not result to any new predictions if applied to standard four - dimensional theories .On the other hand , greater dimensional applications of supergravity have garnered considerable scrutiny during recent years 3 . In this letter we define five - dimensional supergravities 4 where the extra dimension is compactified on a ring 5 or orbifold 6 .These are known as Randall - Sundrum type I 7 and class II 8 scenarios respectively . They allow for localization of Standard Model particles 9 and their excitations 10 on the so - called visible brane while gravitons propagate continuously through the bulk 11 .As a consequence they may solve some problems identified with the ranking between the electroweak scale and the Planck scale 12 . Moreover , such theories offer useful possibilities for constructing ordinary black - hole - like structures 13 - 16 .",
        "rewrite_text": "We investigate the gravitational field equations for static spherically symmetric configurations on Randall-Sundrum type II brane-planets, taking into account the bulk cosmological constant and the confinement of material fields to the branes. Our findings indicate that, under specific conditions, these solutions can be interpreted as black holes within anti-de Sitter spacetime. Notably, we discover that there is no restriction on the mass vector \\(M_0\\) involved in the vacuum Einstein integral on the brane. The corresponding horizon radius \\(r_0\\) is given by the formula \\(r_0 = (3M_0 / 4\\pi)^{1/3}\\). This result implies that the Schwarzschild-de Sitter metric encompasses both black hole and naked singularity solutions. Additionally, we discuss how this perspective shifts when incorporating quantum corrections arising from loop processes. PACS codes: 04.20.-q; 11.10.Kk. \n\nSupersymmetry has been proposed as a possible extension of general relativity that could provide a consistent description of gravity across all scales. However, it was recently demonstrated that its application to standard four-dimensional theories does not yield any new predictions. In contrast, higher-dimensional supergravity applications have received significant attention in recent years. In this letter, we define five-dimensional supergravities where the additional dimension is either compactified on a ring or orbifold. These are referred to as Randall-Sundrum type I and type II scenarios, respectively. Such frameworks facilitate the localization of Standard Model particles and their excitations on the so-called visible brane, while gravitons can propagate through the bulk. Consequently, these theories address some of the challenges associated with reconciling the electroweak scale and the Planck scale, and they provide promising avenues for constructing ordinary black-hole-like structures.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.465790872963897,
        "rewrite-fast-z-score": 1.2018504251546631
    },
    {
        "original_text": "We present the results of our kinematical study of globular clusters in M31, based on high-resolution spectroscopy obtained at the VLT and Keck telescopes. We find that all clusters studied show evidence for rotation around their major axes (with typical velocities of 100-200 km/s), while only two out of eight objects have significant internal velocity dispersions (of about 50-100 km/s). The remaining six clusters are consistent with being completely dispersionless systems. This is surprising given that these clusters contain large numbers of evolved stars belonging to extended horizontal branches. Our analysis shows that this apparent contradiction can be explained by assuming that most of the cluster mass resides outside the observed field-of-view. In addition we find that the majority of the clusters rotate counterclockwise when viewed along their minor axes. These findings suggest that many globular clusters may not be fully relaxed dynamical systems as previously thought. They also provide new insights into the formation history of globular clusters. \n \n Keywords: Kinematics",
        "watermark_text": "We present the conclusion of our kinematical study of globular complexes in M31 , using on wide - resolution spectroscopy acquired at the VLT and Keck telescopes . We see that all galaxies studied show proof for rotation around their major axes ( with typical velocities of 100 - 200 kilometers / s ) , while only two out of eight objects have considerable internal velocity dispersions ( of about 50 - 100 km / s ) .The remaining six complexes are compatible with being totally dispersionless systems . This is surprising given that these complexes include significant amounts of evolved stars belonging to advanced horizontal branches .Our study shows that this possible contradiction can be described by assuming that most of the cluster density resides outside the seen field - of - view . In addition we find that the majority of the clusters tilt counterclockwise when seen along their minor axes .These studies imply that several globular complexes may not be truly relaxed dynamical systems as previously thought . They also bring fresh insights into the formation history of globular complexes .Keywords: Kinematics",
        "rewrite_text": "We conclude our kinematic analysis of globular complexes in M31, utilizing high-resolution spectroscopy from the VLT and Keck telescopes. Our findings indicate that all examined galaxies exhibit evidence of rotation around their major axes, with typical velocities ranging from 100 to 200 kilometers per second. However, only two out of the eight objects display significant internal velocity dispersions of approximately 50 to 100 km/s, while the other six complexes appear to be entirely dispersionless. This is unexpected given the presence of substantial amounts of evolved stars within advanced horizontal branches in these complexes. We suggest that this apparent discrepancy can be explained by the assumption that much of the cluster density exists outside the observable field. Moreover, our research reveals that most clusters exhibit a counterclockwise tilt when viewed along their minor axes. These findings imply that many globular complexes may not be as relaxed dynamical systems as previously assumed, providing new insights into the formation history of these objects. Keywords: Kinematics",
        "ori-fast-z-score": -1.835325870964494,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 0.5852057359806528
    },
    {
        "original_text": "The fossil record is replete with examples of periodic extinction events, but it has been unclear whether these are driven by external factors or internal dynamics within ecosystems. Here we show that biodiversity cycles can be generated solely through interactions between species and their environment without any need to invoke additional mechanisms such as mass extinctions. We use an agent-based model to simulate how communities evolve over time under different environmental conditions. Our results suggest that biodiversity cycles may have played an important role in shaping Earth s biosphere throughout its history. The fossil record contains numerous examples of periodic extinction events (1), which have led some researchers to propose that there must exist underlying periodicity in ecosystem processes (2). However, it remains unknown what causes this apparent regularity in the fossil record; one possibility is that periods of high diversity alternate with intervals during which many species go extinct simultaneously (3) (4) . In addition, it is not clear if all observed patterns of biodiversity cycling represent true cyclical behavior or simply reflect stochastic variation around a mean value (5-7).\nHere we present evidence suggesting that biodiversity cycles can arise spontaneously from ecological interactions alone, without requiring any additional mechanism like mass extinctions. To test our hypothesis, we used an agent-based model to explore how communities evolve over time when subjected to varying levels of environmental stress. This approach allowed us to examine how changes in community composition affect population abundances across multiple trophic levels. By simulating thousands of replicate runs using different parameter values, we were able to identify robust statistical signatures associated with biodiversity cycles.",
        "watermark_text": "The fossil history is replete with examples of periodic mortality events , but it has been uncertain whether these are driven by external influences or internal interactions within ecosystems . Here we show that biodiversity cycles can be formed solely through relationships between species and their environment without any necessity to invoke additional mechanisms such as mass extinctions .We use an agent - based model to simulate how communities evolve over time under various climate circumstances . Our results show that ecosystem cycles might have played an important role in shaping Earth s biosphere throughout its past .The fossil history provides various instances of periodic mortality events ( 1 ) , which have led some researchers to propose that there need arise inherent periodicity in ecological processes ( 2 ) . However , it remains obscure what causes this evident regularity in the fossil history ; one suggestion is that intervals of high diversity alternate with periods during which several species go extinct simultaneously ( 3 ) ( 4 ) .In addition , it is not clear if all observed patterns of biodiversity cycling constitute genuine cyclical behavior or simply reflect stochastic differences around a mean value ( 5 - 7 ) . Here we present evidence indicating that conservation cycles can arise spontaneously from ecosystem interactions alone , without using any additional process like mass extinctions .To study our theory , we using an agent - based model to examine how communities evolve over time when exposed to different amounts of environmental stress . This method allowed us to examine how variations in population composition impact population abundances across multiple trophic levels .By simulating thousands of replicate runs employing multiple parameter values , we were could to identify reliable statistical signatures identified with biodiversity cycles .",
        "rewrite_text": "The fossil record is rich with instances of periodic mortality events, but it has remained unclear whether these events arise from external factors or internal ecosystem dynamics. In this study, we demonstrate that cycles of biodiversity can emerge solely from the interactions between species and their environment, negating the need for additional mechanisms like mass extinctions. We employed an agent-based model to simulate the evolution of ecological communities over time under various climatic conditions. Our findings suggest that ecosystem cycles may have significantly influenced the development of Earth's biosphere throughout its history. The fossil record reveals several cases of periodic mortality events, prompting some researchers to propose that inherent periodicity exists within ecological processes. Nonetheless, the underlying causes of these apparent regularities remain uncertain; one hypothesis suggests that phases of high diversity are followed by periods where multiple species experience simultaneous extinctions. Additionally, it is unclear whether all observed patterns of biodiversity cycling truly represent cyclical behavior or are merely the result of random variations around a mean. Here, we provide evidence that conservation cycles can spontaneously arise from interactions within ecosystems without the need for processes like mass extinctions. To explore this theory, we utilized an agent-based model to investigate how communities evolve under varying levels of environmental stress. This approach enabled us to analyze how changes in population composition influence abundances across different trophic levels. By conducting thousands of replicate simulations with various parameter settings, we were able to identify distinct statistical signatures associated with biodiversity cycles.",
        "ori-fast-z-score": 0.4123930494211613,
        "water-fast-z-score": 9.931270663228416,
        "rewrite-fast-z-score": 3.1382990398158928
    },
    {
        "original_text": "We study vortex matter in honeycomb (HC) and kagome (KC) pinning arrays by using the time-dependent Ginzburg-Landau equation with periodic boundary conditions, which is solved numerically on square lattices with sizes up to L = 256. We find that HC and KC have different effects on vortex dynamics depending on their density n. For low densities, vortices are pinned at individual defects for both HC and KC; however, they form ordered structures only in HC but not in KC. At high densities, we observe two distinct phases in HC; one is a vortex molecular crystal state where all vortices occupy single sites, while another phase has a vortex plastic crystal structure where some vortices remain unpinned. In contrast, no such ordered states exist in KC even at very large defect densities. Our results suggest that the difference between HC and KC originates from the fact that the former can support more than one vortex per site whereas the latter cannot.",
        "watermark_text": "We explore vortex matter in honeycomb ( HC ) and kagome ( KC ) locking arrays by using the period - based Ginzburg - Landau equation with periodic border conditions , which is solution numerically on square lattices with sizes up to L = 256 . We see that HC and KC have different impacts on vortex dynamics depending on their density n . For low densities , vortices are glued at individual defects for both HC and KC ; however , they create ordered forms only in HC but not in KC .At high densities , we study two separate phases in HC ; one is a vortex molecular crystal state where all vortices occupy separate sites , while another phase has a vortex plastic crystal shape where some vortices remain unpinned . In comparison , no such ordered states arise in KC even at very big defect densities .Our results show that the difference between HC and KC originates from the fact that the former can support more than one vortex per site whereas the former cannot .",
        "rewrite_text": "We investigate vortex matter in honeycomb (HC) and kagome (KC) locking arrays using a period-based Ginzburg-Landau equation with periodic boundary conditions, numerically solving it on square lattices with sizes up to L = 256. Our findings reveal that HC and KC exert different influences on vortex dynamics based on their density (n). At low densities, vortices attach to individual defects in both HC and KC; however, ordered structures emerge only in HC, not in KC. At higher densities, we observe two distinct phases in HC: one comprising a vortex molecular crystal state where all vortices occupy separate sites, and another with a vortex plastic crystal configuration where some vortices remain unpinned. By contrast, KC does not form any such ordered states, even at very high defect densities. Our results indicate that the disparity between HC and KC arises from the ability of HC to accommodate more than one vortex per site, unlike KC.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 5.422176684690384,
        "rewrite-fast-z-score": 0.9271726499455306
    },
    {
        "original_text": "We present an efficient algorithm to simulate the dynamics of nonlinear systems with arbitrary initial conditions and parameters using fast recursive filters (FRFs). The FRF is based on a linear combination of basis functions, which are obtained by solving a set of ordinary differential equations that represent the model under consideration. We show how this approach can be used in conjunction with standard numerical integration schemes such as Euler s method or Runge-Kutta methods to efficiently compute trajectories over large time intervals. In particular we demonstrate our approach on two examples, namely the Lorenz attractor and the FitzHugh-Nagumo neuron model. \nI. INTRODUCTIO N\nThe simulation of complex dynamical systems often requires the solution of sets of coupled ordinary differential equations (ODEs)  1  . For example, many models describing physical phenomena involve ODEs  2  , while other applications include chemical reactions  3  , population growth  4  , epidemiology  5  , neuroscience  6  , climate modeling  7  , etc.. However, even if these problems have been studied extensively  8  -  10  , there still exist several challenges associated with their computational treatment  11  .\nIn general, it is not possible to solve analytically the ODE system representing the problem at hand  12  . Therefore, one has to resort to approximate solutions  13  . These approximations may be obtained either numerically  14  or symbolically  15  . Numerical approaches typically rely on discretizing the continuous-time domain into small segments  16  . This leads to a discrete representation of the original system  17  , where each segment corresponds to a single state variable  18  . Symbolic techniques instead use polynomial expansions  19  , rational expressions  20  , or splines  21  to obtain an approximation of the exact solution  22  .",
        "watermark_text": "We create an efficient algorithm to simulate the dynamics of nonlinear systems with specified initial conditions and parameters utilizing fast recursive filters ( FRFs ) . The FRF is based on a linear mixture of basis variables , which are derived by solving a system of ordinary differential equations that represent the model under consideration .We see how this methodology can be used in partnership with typical numerical integration schemes such as Euler s method or Runge - Kutta methods to easily compute trajectories over large time periods . In particular we prove our approach on two examples , notably the Lorenz attractor and the FitzHugh - Nagumo neuron system .I . INTRODUCTIO N The modelling of complex dynamical systems often needs the solve of pairs of coupled ordinary differential equations ( ODEs ) 1 . For instance , many descriptions describing physical phenomena involve ODEs 2 , while other applications include chemical processes 3 , population development 4 , epidemiology 5 , neuroscience 6 , weather simulation 7 , etc . .However , even if these problems have been studied thoroughly 8 - 10 , there still arise many challenges associated with their mathematical treatment 11 . In general , it is not possible to solve analytically the ODE scheme representing the question at hand 12 .Therefore , one has to resort to approximate solutions 13 . These approximations might be obtained either numerically 14 or symbolically 15 .Numerical methods typically rely on discretizing the discrete - time domain into small sectors 16 . This leads to a discrete model of the original system 17 , where each segment corresponds to a single state variable 18 .Symbolic methods instead include polynomial expansions 19 , real equations 20 , or splines 21 to obtain an approximation of the exact solution 22 .",
        "rewrite_text": "We have developed an efficient algorithm to simulate the dynamics of nonlinear systems with defined initial conditions and parameters by using fast recursive filters (FRFs). The FRF leverages a linear combination of basis variables, which are obtained by solving a set of ordinary differential equations that characterize the model in question. This methodology can be effectively integrated with traditional numerical integration techniques such as Euler's method or Runge-Kutta methods to compute trajectories over extended timeframes with ease. Our approach is demonstrated through two case studies: the Lorenz attractor and the FitzHugh-Nagumo neuron model.\n\nI. INTRODUCTION\n\nModeling complex dynamical systems often involves solving pairs of coupled ordinary differential equations (ODEs) 1. Many descriptions of physical phenomena are based on ODEs 2, while other applications span fields such as chemical processes 3, population dynamics 4, epidemiology 5, neuroscience 6, and weather simulation 7. Despite extensive study in these areas 8 - 10, challenges remain in their mathematical treatment 11. Generally, it is not feasible to find analytical solutions to the set of ODEs relevant to these problems 12, necessitating the use of approximate solutions 13. These approximations can be derived either numerically 14 or symbolically 15. Numerical methods typically involve discretizing the time domain into small intervals 16, resulting in a discrete model of the original system 17, where each interval corresponds to a distinct state variable 18. In contrast, symbolic methods employ techniques such as polynomial expansions 19, real equations 20, or splines 21 to approximate the exact solution 22.",
        "ori-fast-z-score": -0.811502671200689,
        "water-fast-z-score": 6.7082039324993685,
        "rewrite-fast-z-score": 1.6283046848759573
    },
    {
        "original_text": "We present an analysis of the observed properties of the rare, hot (T eff > 20 000 K), helium-rich subdwarf B (sdB) and extreme horizontal branch (EHB) stars in globular clusters that are known as red stragglers or blue hook stars.  We show how these objects can be explained by the merger of two helium white dwarfs with total mass exceeding the Chandrasekhar limit.   The resulting merged object is expected to have a radius larger than its Roche lobe at all stages during the merging process, which leads to stable mass transfer onto the companion star until it fills its Roche lobe and starts overflowing.   This results in a common envelope phase where most of the orbital energy released is used to eject the outer layers of both stars into space.   After this stage, the system consists of a naked helium core surrounded by a thin hydrogen layer on top of the helium core.   If the initial masses of the progenitor white dwarf components were close enough together such that their cores merge before they reach the Chandrasekhar mass, then we expect the final product to resemble a normal sdB star; if not, then the final product will be more massive than the Chandrasekhar",
        "watermark_text": "We present an assessment of the known characteristics of the rare , hot ( T eff > 20 000 K ) , helium - rich subdwarf B ( sdB ) and extreme horizontal branch ( EHB ) stars in globular complexes that are known as red stragglers or blue hook galaxies . We see how these objects can be described by the merger of two helium white dwarfs with total mass exceeding the Chandrasekhar limit .The resulting merged object is expected to have a diameter greater than its Roche lobe at all phases during the merging process , which results to continuous weight transfer onto the companion star until it fills its Roche lobe and starts overflowing . This results in a common envelope phase where most of the orbital heat released is utilized to eject the exterior layers of both stars into space .After this phase , the system consists of a naked helium core flanked by a thin hydrogen layer on top of the helium core . If the first masses of the progenitor white dwarf components were close enough together such that their cores merge before they reach the Chandrasekhar mass , then we expect the finished result to resemble a normal sdB star ; if not , then the finished result will be more massive than the Chandrasekhar",
        "rewrite_text": "We provide an evaluation of the known properties of the rare, hot (T_eff > 20,000 K) helium-rich subdwarf B (sdB) and extreme horizontal branch (EHB) stars found in globular clusters, commonly referred to as red stragglers or blue hook galaxies. We explore how these objects can be understood as the product of the merger of two helium white dwarfs, resulting in a total mass that exceeds the Chandrasekhar limit. The merged entity is anticipated to have a diameter that remains larger than its Roche lobe throughout the merging process, leading to a continuous transfer of mass to the companion star until it fills its Roche lobe and begins to overflow. This initiates a common envelope phase during which most of the orbital energy is used to expel the outer layers of both stars into space. Following this phase, the system is left with a naked helium core, topped by a thin layer of hydrogen. If the initial masses of the progenitor white dwarfs are sufficiently close that their cores merge before reaching the Chandrasekhar mass, the end result is expected to resemble a typical sdB star; otherwise, the outcome will be a star with a mass surpassing the Chandrasekhar limit.",
        "ori-fast-z-score": 1.7056057308448833,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 1.7253243712550146
    },
    {
        "original_text": "We present the results of an analysis of the clustering properties of luminous red galaxies (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use a sample of 380,000 LRGs selected to have 0.4 < zphot < 1.0 and Mr < --21.5 + 5logh. The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10  and 100 . To account for redshift space distortions we measure the projected cross-correlation functions wp(rp), where rp = Dproj/ H(z)/H0 , H(z) is the Hubble parameter at redshift z, and H0 is its value today. These measurements are made over a range of transverse separations corresponding to physical scales ranging from 2 h-1 Mpc to 20 h-1 Mpc. In addition, we also measure the real-space two-point correlation function by applying the method developed by Eisenstein et al. (2007) . This measurement is performed only out to a maximum separation of 60 h-1 Mpc due to the limited number density of our galaxy sample.",
        "watermark_text": "We publish the conclusion of an assessment of the clustering behavior of luminous red objects ( LRGs ) in the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use a sample of 380 , 000 LRGs chosen to have 0 . 4 < zphot < 1 . 0 and Mr < - - 21 . 5 + 5logh .The angular correlation function is measured for this specimen using the Landy & Szalay estimator on scales between 10 and 100 . To account for redshift space distortions we measure the projected cross - correlation functions wp ( rp ) , where rp = Dproj / H ( z ) / H0 , H ( z ) is the Hubble parameter at redshift z , and H0 is its value today .These measurements are produced over a range of transverse separations corresponding to physical scales ranging from 2 h - 1 Mpc to 20 g - 1 Mpc . In addition , we also measure the real - space two - point coupling function by using the method developed by Eisenstein et al .( 2007 ) . This measurement is conducted only out to a maximum separation of 60 h - 1 Mpc owing to the limited number density of our galaxy sample .",
        "rewrite_text": "We present the findings of an analysis concerning the clustering behavior of luminous red objects (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our analysis is based on a sample of 380,000 LRGs selected with redshift ranges of 0.4 < zphot < 1.0 and absolute magnitudes of Mr < -21.5 + 5logh. The angular correlation function is calculated for this dataset using the Landy & Szalay estimator, focusing on scales between 10 and 100 arcminutes. To address redshift space distortions, we compute the projected cross-correlation functions wp(rp), where rp = Dproj / H(z) / H0, with H(z) representing the Hubble parameter at redshift z and H0 its current value. These measurements span various transverse separations that correspond to physical scales from 2 h⁻¹ Mpc to 20 h⁻¹ Mpc. Additionally, we evaluate the real-space two-point correlation function by utilizing the technique introduced by Eisenstein et al. (2007), although this measurement is restricted to a maximum separation of 60 h⁻¹ Mpc due to the limited density of our galaxy sample.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.302003302004953,
        "rewrite-fast-z-score": -0.3841106397986879
    },
    {
        "original_text": "We introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees . We introduce an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function .Finally we prove that our approach is ability to acquire precise models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and protein secondary structure prediction in bioinformatics . Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected numerical models which have been successfully applied to many difficulties involving sequential data , e . g .( Sha & Pereira , 2003 ) . In this research , we propose Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees .The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space . This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools .Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) . To assess the performance of our technique , we apply it to two essential applications : whole - of - voice taggin",
        "rewrite_text": "We present the concept of Generalized Conditional Random Fields (GCRFs) and demonstrate their application in modeling arbitrary likelihood distributions for structured datasets such as sequences and trees. Our work introduces an efficient algorithm for learning GCRF variables through gradient descent on the log-likelihood objective function. We also show that this method successfully captures precise models for various complex gene labeling tasks, including whole-of-voice tagging in natural language processing and predicting protein secondary structures in bioinformatics. Conditional Random Fields (CRFs), as described by Lafferty et al. (2001), are undirected graphical models that have proven effective in addressing numerous challenges related to sequential data, as exemplified by Sha and Pereira (2003). In this study, we expand on the CRF framework by proposing GCRFs, which broaden the modeling capabilities to encompass any distribution over structured datasets such as sequences and trees. The core idea behind GCRFs involves utilizing a set of latent variables to encode dependencies between different components of the input space. This framework allows for the efficient computation of the partition function using dynamic programming techniques, while also facilitating the training of GCRFs with gradient-based approaches akin to those employed in Maximum Entropy Markov Models (MEMMs). To evaluate the efficacy of our method, we apply it to two vital applications: whole-of-voice tagging and additional tasks.",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": 0.6
    },
    {
        "original_text": "We present an analysis of satellite galaxies, fossil groups (FGs), and galaxy clusters in the Millennium Run simulation. We find that FGs are rare objects with only 0.5% of all halos being classified as such at z=0. The number density of FGs is consistent with observations for systems with Mvir>1013M⊙/h. However, we also find that there exists a population of low-mass FGs which have not been observed yet but may be detectable by future surveys. In addition to their rarity, FGs show several other interesting properties compared to normal galaxy clusters: they tend to reside in more massive dark matter haloes than normal galaxy clusters; they contain fewer bright central galaxies; and they exhibit higher velocity dispersions. These results suggest that FGs can provide important constraints on models of galaxy formation and evolution. This work was supported by NASA grant NAG5-10842.",
        "watermark_text": "We present an assessment of satellite galaxies , fossil bands ( FGs ) , and galaxy regions in the Millennium Run simulation . We see that FGs are scarce objects with only 0 . 5 % of all halos being designated as such at z = 0 .The number density of FGs is consistent with observations for systems with Mvir > [UNK] / h . However , we also find that there exists a population of low - mass FGs which have not been observed yet but may be detectable by future surveys .In addition to their rarity , FGs exhibit several other remarkable attributes relative to normal star clusters : they tend to live in more massive bright matter haloes than regular galaxy clusters ; they contain fewer bright central clusters ; and they show superior velocity dispersions . These data suggest that FGs can provide important restrictions on predictions of galaxy formation and evolution .This project was supported by NASA gift NAG5 - 10842 .",
        "rewrite_text": "We provide an analysis of satellite galaxies, fossil groups (FGs), and galaxy regions within the Millennium Run simulation. Our findings indicate that FGs are rare, comprising only 0.5% of all halos at z = 0. While the number density of FGs aligns with observations for systems with Mvir > [UNK] / h, we have also identified a population of low-mass FGs that have yet to be detected but could be observed in upcoming surveys. Beyond their scarcity, FGs possess several noteworthy characteristics compared to typical star clusters: they typically reside in more massive bright matter halos than ordinary galaxy clusters, have fewer prominent central clusters, and display higher velocity dispersions. These observations suggest that FGs could serve as critical benchmarks for testing theories of galaxy formation and evolution. This research was conducted with support from NASA grant NAG5-10842.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 5.578319375835658,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "We present an atlas of the circumnuclear region (CNR) of 75 nearby active galactic nuclei observed by Hubble Space Telescope s Advanced Camera for Surveying program, which is part of the Nearby Galaxies Survey Treasury project. The CNRs are defined as the brightest central 2 kpc diameter circular aperture centered on each galaxy nucleus and were imaged using the F330W filter to select emission lines at wavelengths longer than 3000 Å . We use these data to study the properties of the nuclear starbursts that power the AGNs through their effects on the surrounding interstellar medium. In addition we examine how the physical conditions within the CNRs vary among different types of AGN activity.  We find that the majority of our sample have significant UV excesses over what would be expected based solely on stellar photospheric emission. This excess can be explained either by hot young stars or by dust extinction. For those objects where both optical spectroscopy and infrared imaging exist, we show that the UV excess is due primarily to dust extinction rather than hot young stars.",
        "watermark_text": "We present an atlas of the circumnuclear zone ( CNR ) of 75 nearby active galactic nuclei seen by Hubble Space Telescope s Advanced Camera for Surveying project , which is part of the Nearby Galaxies Survey Treasury project . The CNRs are specified as the brightest central 2 kpc diameter circular aperture located on each galaxy nucleus and were imaged using the F330W filter to select emission lines at wavelengths greater than 3000 Å .We use these information to study the properties of the atomic starbursts that fuel the AGNs through their impacts on the nearby interstellar material . In addition we investigate how the physical conditions within the CNRs vary among different kinds of AGN activity .We see that the majority of our sample have considerable UV excesses over what would be anticipated based primarily on stellar photospheric emission . This excess can be explained either by hot early stars or by dust disappearance .For those objects where both optical spectroscopy and infrared imaging exist , we find that the UV excess is due primarily to dust extinction rather than hot young stars .",
        "rewrite_text": "We present an atlas of the circumnuclear regions (CNRs) of 75 nearby active galactic nuclei, captured by the Hubble Space Telescope’s Advanced Camera for Surveys as part of the Nearby Galaxies Survey Treasury project. The CNRs are defined as the brightest circular apertures with a diameter of 2 kpc centered on each galaxy's nucleus, imaged using the F330W filter to capture emission lines at wavelengths greater than 3000 Å. This data allows us to investigate the properties of the atomic starbursts that power the AGNs and their effects on the surrounding interstellar material. Additionally, we explore how physical conditions within the CNRs vary across different types of AGN activity. Our findings indicate that the majority of the sample exhibit significant UV excesses compared to what would typically be expected based on stellar photospheric emission. This excess can be attributed either to the presence of hot early stars or the disappearance of dust. For those objects with both optical spectroscopy and infrared imaging available, we conclude that the UV excess is primarily due to dust extinction rather than the contribution from young, hot stars.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -0.11547005383792514
    },
    {
        "original_text": "We present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh",
        "watermark_text": "We report findings on the importance played by the rho meson in representing pion electroproduction measurements obtained with the CLAS detector at Jefferson Lab ( JLab ) . The investigation is conducted within an efficient field model approach , where we using chiral perturbation theory to explain the interaction between pions and nucleons up to next - to - leading order .We then introduce vector - meson degrees of liberty through the hidden gauge formalism . In particular , we treat contributions come from one - loop diagrams using rho mesons as well as tree - level processes induced by rho mesons .Our conceptual formulation enables us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single - pion production off protons . Using this model , we are able to publish empirical data for all these observables simultaneously .Finally , we talk how our findings can be used to extract information about the properties of the rho meson . This research was supported by the U . S . Department of Energy under Contract No .DE - SC0012704 . PACS numbers : 12 . 38 . Mh",
        "rewrite_text": "We present our findings on the significant role of the rho meson in interpreting pion electroproduction measurements obtained with the CLAS detector at Jefferson Lab (JLab). Our investigation employs an efficient field model approach, utilizing chiral perturbation theory to describe the interactions between pions and nucleons up to next-to-leading order. We incorporate vector-meson degrees of freedom using the hidden gauge formalism. Specifically, we analyze contributions from one-loop diagrams involving rho mesons, as well as tree-level processes induced by these mesons. Our theoretical framework allows us to examine both neutral current reactions, such as elastic ep scattering, and charged current reactions, like single-pion production from protons. Through this model, we successfully publish empirical data for all these observables simultaneously. Finally, we discuss how our results can be utilized to extract information about the properties of the rho meson. This research was supported by the U.S. Department of Energy under Contract No. DE-SC0012704. PACS numbers: 12.38.Mh.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "We report on Suzaku observations for four active galactic nuclei (AGNs) detected by Swift/BAT survey, which are classified as  obscured AGNs  with column density larger than 10 24 cm-2 . We found that all these sources show strong Fe K emission lines and their line widths are broader than those expected from thermal broadening at kT = 100 keV. The observed line profiles can be reproduced well by relativistic disk reflection models including Compton scattering effects. These results suggest that there is an additional component to the X-ray continuum other than the standard thin accretion disks around supermassive black holes. In addition, we find that the iron abundance relative to solar value is higher than 1.5 times in three out of four objects. This suggests that the central engines of these obscured AGNs may have been buried under heavy dusty torii. Finally, we discuss possible origins of this new type of buried supermassive black holes based on our observational results.",
        "watermark_text": "We report on Suzaku measurements for four active galactic nuclei ( AGNs ) detected by Swift / BAT search , which are classified as obscured AGNs with column height larger than 10 24 mm - 2 . We showed that all these sources show light Fe K emission lines and their line widths are larger than those expected from radiation broadening at kT = 100 keV .The observed line profiles can be reproduced well by relativistic disk absorption theories including Compton scattering effects . These data suggest that there is an additional element to the X - ray continuum other than the standard narrow accretion disks around supermassive black holes .In addition , we find that the metal density relative to solar value is higher than 1 . 5 times in three out of four objects . This implies that the main engines of these obscured AGNs might have been trapped under heavy dusty torii .Finally , we explain possible origins of this new kind of hidden supermassive black holes using on our observational results .",
        "rewrite_text": "We present Suzaku measurements for four active galactic nuclei (AGNs) identified by the Swift/BAT survey, all of which are classified as obscured AGNs with column densities exceeding 10²⁴ cm⁻². Our findings reveal that these sources exhibit pronounced light Fe K emission lines, with line widths that exceed the expected values from purely radiation broadening at kT = 100 keV. The observed line profiles align well with predictions from relativistic disk absorption models that account for Compton scattering effects. This data indicates that there may be additional components contributing to the X-ray continuum beyond the conventional narrow accretion disks surrounding supermassive black holes. Furthermore, we observe that the metal density in three out of the four objects is more than 1.5 times the solar value, suggesting that the primary engines of these obscured AGNs could be concealed within dense dusty torii. Finally, we discuss potential origins for this new category of hidden supermassive black holes based on our observational findings.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.128258770283413,
        "rewrite-fast-z-score": 2.324952774876386
    },
    {
        "original_text": "We present new results on the distribution of loop widths in active regions observed by TRACE at 171 Å, using data obtained during solar maximum (May-June 2001). We find that loops with different temperatures have similar distributions of widths, which are well fitted by log-normal functions. The mean values of these distributions increase with temperature as expected for pressure equilibrium between plasma confined within magnetic structures and their surroundings. However, we also find that there is no significant difference between the widths measured along individual loops and those determined from averaged profiles over entire active regions. This suggests that the apparent constancy of loop widths may be due to averaging effects rather than being intrinsic properties of coronal structures. In addition, we show that the widths derived from observations made under different viewing angles do not depend significantly on the position angle of the line-of-sight relative to the direction perpendicular to the local magnetic field vector.",
        "watermark_text": "We report new data on the distribution of loop widths in active regions observed by TRACE at 171 Å , using data acquired during thermal maximum ( May - June 2001 ) . We see that loops with varying temperatures have similar distributions of widths , which are better fitted by log - normal functions .The mean estimates of these distributions increase with temperature as anticipated for pressure equilibrium between plasma confined within magnetic structures and their environment . However , we also find that there is no considerable difference between the widths calculated along individual loops and those estimated from averaged profiles over whole active regions .This implies that the apparent constancy of loop widths might be due to averaging influences rather than being intrinsic characteristics of coronal structures . In addition , we prove that the widths generated from measurements made under distinct observation angles do not depend greatly on the orientation angle of the line - of - view relative to the direction perpendicular to the local magnetic force vector .",
        "rewrite_text": "We present new findings on the distribution of loop widths in active regions observed by TRACE at 171 Å, based on data collected during the thermal maximum period (May - June 2001). Our analysis reveals that loops with varying temperatures exhibit similar width distributions, which are best represented by log-normal functions. As expected, the mean estimates of these distributions increase with temperature, aligning with the concept of pressure equilibrium between plasma confined within magnetic structures and their surrounding environment. Interestingly, we observe no significant difference between the widths measured along individual loops and those calculated from averaged profiles of entire active regions. This suggests that the perceived uniformity in loop widths may stem from averaging effects rather than being inherent properties of coronal structures. Furthermore, we demonstrate that width measurements taken under different observation angles show little dependency on the orientation of the line of sight relative to the direction perpendicular to the local magnetic force vector.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": 0.9649012813540153
    },
    {
        "original_text": "We have performed in-orbit focal adjustment (IFA) for the infrared camera onboard AKARI satellite using its own data taken in orbit. The IFA was carried out by comparing the observed point spread function (PSF) and that simulated based on ray tracing analysis, which is one of the most accurate methods to determine the best focus position. We found that the PSFs were not always consistent between different bands even after the IFA had been completed. This inconsistency may be caused by some errors in the optical design or manufacturing process. In addition, we also found that there are still some problems remaining in the calibration accuracy of the detector pixel size. These results will help us improve our understanding about the performance of the instrument as well as provide useful information for future space missions. Keywords: Space mission, Focal adjustment, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera",
        "watermark_text": "We have done in - orbit lens adjustment ( IFA ) for the infrared camera onboard AKARI vehicle using its own signal taken in orbit . The IFA was carried out by comparing the seen point spread distribution ( PSF ) and that simulated based on ray tracing imaging , which is one of the most accurate ways to identify the best view point .We determined that the PSFs were not always compatible between various frequencies even after the IFA had been completed . This inconsistency may be caused by some failures in the optical design or manufacturing system .In addition , we also discovered that there are still some problems existing in the calibration reliability of the sensor pixel size . These data will assist us improve our knowing about the performance of the instrument as also as give valuable info for future orbital flights .Keywords : Space mission , Focal correction , Point spread value , Ray tracing investigation , Infrared astronomy , Infrared camera",
        "rewrite_text": "We performed in-orbit lens adjustments (IFA) for the infrared camera on the AKARI spacecraft, utilizing its own signals collected in space. The IFA involved comparing the observed point spread function (PSF) with simulations derived from ray tracing imaging, which is a highly precise method for determining the optimal viewpoint. Despite the adjustments, we found that the PSFs were not consistently aligned across different frequencies. This discrepancy may stem from potential issues in the optical design or the manufacturing process. Additionally, we identified ongoing concerns regarding the calibration reliability of the sensor pixel size. The findings from this study will enhance our understanding of the instrument's performance and provide valuable insights for future space missions. \n\nKeywords: Space mission, Focal correction, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 6.052586456788652,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We present the first results on deep infrared (IR) observations with ISOCAM at 12 um, ISO at 15 um, ESO-VLT/VISIR at 11.7 um, and Spitzer/MIPS at 24 um in the Sculptor galaxy cluster field. The data are used to study the evolution of galaxies over the last 8 Gyrs as well as their contribution to the extragalactic background light (EBL). We find that the IR luminosity function evolves strongly between z=0.5-0.8 and today. At high redshifts we detect an excess number density of luminous infrared galaxies (LIRGs), ultraluminous infrared galaxies (ULIRGs), and active galactic nuclei (AGNs) compared to local samples. This is consistent with previous studies based on optical/NIR surveys. However, our sample contains only few objects which can be classified as LIRGs or ULIRGs using standard criteria. Instead, most sources show very large dust extinction values A(V)>10 mag. These sources have been missed so far because they were not detected in optical/NIR surveys due to heavy obscuration. In addition, we find evidence for a significant fraction of heavily extincted early-type galaxies among these sources.",
        "watermark_text": "We publish the first findings on dark infrared ( IR ) observations with ISOCAM at 12 um , ISO at 15 um , ESO - VLT / VISIR at 11 . 7 um , and Spitzer / MIPS at 24 um in the Sculptor galaxy cluster area . The data are using to study the evolution of galaxies over the last 8 Gyrs as also as their contribution to the extragalactic background light ( EBL ) .We see that the IR luminosity function evolves highly between z = 0 . 5 - 0 . 8 and today . At high redshifts we perceive an excess amount density of luminous infrared galaxies ( LIRGs ) , ultraluminous infrared galaxies ( ULIRGs ) , and active galactic nuclei ( AGNs ) compared to nearby samples .This is consistent with previous research based on laser / NIR surveys . However , our sample comprises only few objects which can be categorized as LIRGs or ULIRGs using conventional requirements .Instead , most sources show very huge dust extinction values A ( V ) > 10 mag . These sources have been missed so far because they were not observed in laser / NIR surveys due to heavy obscuration .In addition , we find proof for a substantial proportion of heavily extincted early - class galaxies among these sources .",
        "rewrite_text": "We present the initial findings from dark infrared (IR) observations conducted with ISOCAM at 12 µm, ISO at 15 µm, ESO-VLT/VISIR at 11.7 µm, and Spitzer/MIPS at 24 µm in the Sculptor galaxy cluster region. This data enables us to investigate galaxy evolution over the past 8 billion years and their contributions to the extragalactic background light (EBL). Our results indicate significant evolution in the IR luminosity function between redshifts of 0.5 to 0.8 and the present day. At these higher redshifts, we observe a greater density of luminous infrared galaxies (LIRGs), ultraluminous infrared galaxies (ULIRGs), and active galactic nuclei (AGNs) compared to nearby samples, aligning with findings from prior laser/NIR surveys. However, our sample includes only a few objects typically classified as LIRGs or ULIRGs according to standard criteria. Instead, most of the sources we identified exhibit very high dust extinction values, A(V) > 10 mag. These heavily obscured sources have been overlooked in previous laser/NIR surveys. Furthermore, we provide evidence for a significant number of heavily extinguished early-type galaxies among these objects.",
        "ori-fast-z-score": 0.22360679774997896,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "We present results on flame evolution during type Ia supernova (SN) explosions, based on two-dimensional hydrodynamic simulations with detailed nuclear reaction networks for both deflagrations and detonations. We find that the transition to detonation is triggered by shock-induced turbulent mixing at densities around 10$^{9}$ g/cm$^3$. The resulting detonation wave propagates through the entire white dwarf star within about 1 s after ignition. In this scenario, the observed light curve can be reproduced if we assume an initial central density of 2 x $10^8$ g/cm$^3$ or higher. For lower values of the central density, the explosion fails to produce enough 56Ni to explain observations. This work was supported by NASA grant NNX10AD03G. Keywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis",
        "watermark_text": "We report findings on flame evolution during type Ia supernova ( SN ) exploded , using on two - dimensional hydrodynamic simulations with comprehensive nuclear response systems for both deflagrations and detonations . We see that the shift to detonation is caused by shock - caused turbulent mixing at densities around 10 $ ^ { 9 } $ h / cm $ ^ 3 $ .The produced detonation radiation propagates through the entire white dwarf star within about 1 s after explosion . In this situation , the observed light curve can be reproduced if we suppose an initial central density of 2 x $ 10 ^ 8 $ h / cm $ ^ 3 $ or greater .For lower values of the central density , the explosion fails to produce enough 56Ni to explain observations . This research was supported by NASA grant NNX10AD03G .Keywords : Flame transmission , Hydrodynamics , Nuclear burning , Supernovae , White dwarfs . Subject headings : Nucleosynthesis",
        "rewrite_text": "We present our findings on flame evolution during type Ia supernovae (SN) explosions, based on two-dimensional hydrodynamic simulations that incorporate detailed nuclear response mechanisms for both deflagrations and detonations. Our results indicate that the transition to detonation is driven by shock-induced turbulent mixing at densities around 10 \\( ^{9} \\) h/cm \\( ^{3} \\). The resulting detonation wave travels through the entire white dwarf star in approximately 1 second following the explosion. Our analysis suggests that the observed light curve can be matched if we assume an initial central density of 2 x \\( 10^{8} \\) h/cm \\(^{3} \\) or higher. Lower central densities result in insufficient production of \\(^{56}Ni\\) to account for observational data. This research was funded by NASA grant NNX10AD03G. \n\nKeywords: Flame propagation, Hydrodynamics, Nuclear burning, Supernovae, White dwarfs. Subject headings: Nucleosynthesis.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": -0.6509445549041194
    },
    {
        "original_text": "We present an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and adding scalar fields in such a way that the resulting solution is minimally coupled.  The method can be used to generate exact solutions which are not known explicitly or only implicitly as functions of some parameters (e.g., by solving algebraic equations). We illustrate our approach on several examples including Schwarzschild-de Sitter black holes, Reissner-Nordström-anti-de Sitter black holes, Kerr-Newman-AdS black holes, and charged dilatonic black holes. In particular we show how one can obtain explicit expressions for the massless limit of these black hole solutions. Our results may also have applications beyond gravity theory, e.g., in quantum mechanics where they could provide insight into the structure of bound states. Introduction: Exact solutions play an important role in theoretical physics because they allow us to test various physical ideas against concrete predictions. However, finding exact solutions to physically interesting problems often turns out to be very difficult. For example, it took more than 100 years after the discovery of general relativity before the first exact black hole solutions were found  1-3 . Even today there exist many open questions about black holes  4  . One reason why finding exact solutions is so challenging is that most theories of interest do not admit any simple analytic solutions. Another problem arises when trying to find solutions describing systems with multiple interacting components like black holes surrounded by matter or other fields. Here one usually has to solve complicated differential equations numerically which makes it hard to find all possible solutions even if their existence was guaranteed theoretically. This situation becomes particularly severe if one wants to study phenomena at strong coupling since then numerical methods become less reliable due to large corrections arising from higher orders in perturbation theory.",
        "watermark_text": "We present an algorithm for generating new answers to the coupled Einstein - scalar field equations , beginning from vacuum solutions and adding scalar fields in such a way that the resulting solve is minimally coupled . The method can be used to create precise solutions which are not described specifically or only implicitly as functions of some parameters ( e . g . , by modeling algebraic equations ) .We illustrate our approach on numerous instances including Schwarzschild - de Sitter dark holes , Reissner - Nordström - anti - de Sitter dark holes , Kerr - Newman - AdS red holes , and charged dilatonic black holes . In particular we give how one can obtain precise expressions for the massless maximum of these black hole solutions .Our results may even have applications beyond gravitational theory , e . g . , in quantum mechanics where they may provide insight into the formation of bound states . Introduction : Exact solutions play an important role in theoretical physics because they allow us to test various mechanical concepts against concrete expectations .However , finding exact treatments to physically exciting issues often comes out to be very difficult . For instance , it takes more than 100 years after the discovery of general relativity before the first accurate black hole problems were found 1 - 3 .Even nowadays there remain many open questions about black holes 4 . One reason why seeking precise solutions is so difficult is that most models of importance do not admit any straightforward analytic solutions .Another difficulty arises when trying to find solutions involving systems with various interacting components like grey holes populated by matter or other fields . Here one usually has to solve complicated differential equations numerically which makes it difficult to find all possible solutions even if their existence was assured theoretically .This problem arises increasingly severe if one wants to study phenomena at strong coupling since then numerical models become fewer reliable resulting to large corrections resulting from greater orders in perturbation theory .",
        "rewrite_text": "We propose a novel algorithm for generating new solutions to the coupled Einstein-scalar field equations by starting with vacuum solutions and integrating scalar fields in a minimally coupled framework. This technique allows for the creation of precise solutions that are not explicitly defined or are only implicitly represented in terms of parameters, such as through algebraic equations. We demonstrate our methodology across various scenarios, including Schwarzschild-de Sitter black holes, Reissner-Nordström anti-de Sitter black holes, Kerr-Newman-AdS red holes, and charged dilatonic black holes. Notably, we provide detailed expressions for the massless maximum of these black hole solutions. Our findings may extend beyond gravitational theory, possibly offering insights into quantum mechanics, particularly regarding the formation of bound states. \n\nIntroduction: Exact solutions are crucial in theoretical physics as they allow us to test mechanical concepts against tangible expectations. However, obtaining precise solutions to physically relevant problems can be quite challenging. For instance, it took over a hundred years after the advent of general relativity to identify the first accurate black hole solutions. Even today, many questions surrounding black holes remain unresolved. One reason for the difficulty in finding precise solutions is that most significant models lack straightforward analytic solutions. Moreover, when exploring systems with multiple interacting components, such as grey holes filled with matter or other fields, one often needs to solve complicated differential equations numerically. This numerical approach can hinder the discovery of all possible solutions, even if their theoretical existence is established. This challenge becomes even more pronounced when investigating phenomena at strong coupling, where numerical models tend to be less reliable, leading to substantial corrections from higher-order perturbative effects.",
        "ori-fast-z-score": 0.30499714066520933,
        "water-fast-z-score": 8.387421368293257,
        "rewrite-fast-z-score": 1.6116459280507605
    },
    {
        "original_text": "We report on new photometric and spectroscopic observations of the recently discovered mid- to late-Be star V2104 Cyg, which show that it is multiperiodic with two periodicities at P = 0. d . The first periodicity (P_1) was found by analyzing our own data as well as those available in literature; its value agrees very well with previous determinations. The second one (P_2), however, has never been detected before. We have also analyzed archival IUE spectra for this object and found evidence for long-term variability in both emission-line profiles and continuum fluxes. Our results suggest that the observed double-periodic behavior may be caused by non-radial pulsation or rotationally modulated magnetic activity. \n \n Keywords: Multiperiodicity, Nonradial pulsation, Rotational modulation, Mid-to-late B-type stars, Photometry, Spectroscopy",
        "watermark_text": "We report on new photometric and spectroscopic observations of the recently discovered late - to late - Be star V2104 Cyg , which show that it is multiperiodic with two periodicities at P = 0 . d . The first periodicity ( P _ 1 ) was known by analyzing our own data as well as those available in literature ; its value agrees very best with previous determinations .The second one ( P _ 2 ) , however , has never been detected before . We have already analyzed archival IUE spectra for this object and found proof for large - term variability in both emission - line profiles and continuum fluxes .Our results show that the seen double - periodic pattern might be caused by non - radial pulsation or rotationally modulated magnetic activity . Keywords : Multiperiodicity , Nonradial pulsation , Rotational modulation , Mid - to - early B - class stars , Photometry , Spectroscopy",
        "rewrite_text": "We present new photometric and spectroscopic observations of the recently identified late Be star V2104 Cyg, revealing it to be multiperiodic with two distinct periodicities at P = 0.d. The first periodicity (P_1) was identified through our data as well as previous literature, and its value closely aligns with earlier findings. In contrast, the second periodicity (P_2) was previously undetected. Our prior analysis of archival IUE spectra for this star indicated significant long-term variability in both emission-line profiles and continuum fluxes. Our results suggest that the observed double-periodic pattern may arise from non-radial pulsation or rotationally modulated magnetic activity. Keywords: Multiplicity, Non-radial pulsation, Rotational modulation, Mid- to early B-type stars, Photometry, Spectroscopy.",
        "ori-fast-z-score": 1.5714285714285714,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.2886751345948129
    },
    {
        "original_text": "The authors present an overview of the role that knots play in proteins, with particular emphasis on their function and evolution.  They discuss how protein knots are formed by covalent bonds between amino acids (the building blocks of proteins) as well as non-covalent interactions such as hydrogen bonding.  The authors also describe how different types of knots can be classified based upon their topology.   Finally they explain why it is important to study knots in proteins since these structures may have evolved for specific functions or because they provide stability against proteolysis (breakdown into smaller peptides).    This article was originally published on BioMed Central. It has been re-posted here under Creative Commons License 3.0. Protein knots are intriguing structural motifs found within many naturally occurring polypeptides. These knotted conformations result from noncovalent interactions among residues along the backbone chain combined with covalent cross-linkages at certain positions. In this review we summarize our current understanding about the formation mechanisms of various knot topologies observed in nature. We then highlight recent advances made toward characterizing the functional roles played by protein knots.",
        "watermark_text": "The authors present an overview of the importance that knots play in proteins , with particular focuses on their function and evolution . They explore how protein knots are created by covalent interactions between amino acids ( the built stones of proteins ) as well as non - covalent interactions such as carbon bonding .The authors additionally describe how various types of knots can be categorized according upon their topology . Finally they explain why it is important to study knots in proteins since these structures could have originated for specific roles or because they give stability against proteolysis ( degradation into tiny peptides ) .This section was originally published on BioMed Central . It has been re - posted here under Creative Commons License 3 . 0 .Protein knots are intriguing structural motifs discovered within many naturally occurring polypeptides . These knotted conformations result from noncovalent interactions among proteins along the backbone ring coupled with covalent cross - linkages at different positions .In this review we summarize our latest understanding about the formation patterns of several knot topologies discovered in nature . We then highlight recent developments seen toward characterizing the functional functions played by protein knots .",
        "rewrite_text": "The authors provide an overview of the significant role that knots play in proteins, focusing specifically on their functions and evolutionary development. They discuss how protein knots are formed through both covalent interactions between amino acids (the building blocks of proteins) and non-covalent interactions, such as carbon bonding. The authors also categorize different types of knots based on their topology. They emphasize the importance of studying knots in proteins, as these structures may have developed for specific functions or to enhance stability against proteolysis (the breakdown into smaller peptides). This section was originally published in BioMed Central and is re-posted here under Creative Commons License 3.0. Protein knots are fascinating structural features found in many naturally occurring polypeptides. These knotted shapes arise from non-covalent interactions within the protein backbone, along with covalent cross-linkages at various positions. In this review, we summarize our current understanding of the formation patterns of different knot topologies found in nature and highlight recent advances in characterizing the functional roles of protein knots.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 6.88998622004134,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "We study the effects of general relativity on gravitational wave bursts produced by extreme mass ratio inspirals (EMRIs). We show that, for EMRI systems with total masses M = 10^6M_solar and compact object masses m = 1M_neutron star, the orbital period is less than one second at distances greater than 100 AU. This implies that these sources are likely to be detected as continuous waves rather than short-duration bursts. The detection rate of such events depends strongly upon their luminosities; we find that they may occur up to several times per year within our galaxy. These results suggest that EMRIs could provide an important source of information about supermassive black holes. \n \n Keywords: Black hole, Compact binary system, General relativity, Gravitational wave, Inspiralling neutron star, Relativity theory \n \n \n \n INTRODUCTION \n \n In recent years there has been considerable interest in studying the properties of gravitational radiation emitted during the final stages of stellar evolution when a massive star collapses into a black hole or neutron star  1  . Such processes can produce extremely energetic signals which will be detectable out to cosmological distances using future space-based detectors  2  , including LISA  3  . However, it remains unclear how many of these events should actually be observed  4  . \n \n One possible class of objects which might emit strong gravitational waves are known as  extreme-mass-ratio inspirals  (EMRIs)  5  . Here, a small compact object spirals into a much more massive black hole or neutron star over millions of orbits before being destroyed  6  . For example, if a solar mass star were to spiral into a ten million solar mass black hole then its orbit would shrink down to just a few kilometres before merging  7, 8  . If this process occurs close enough to the event horizon then the resulting signal will have very high frequencies  9  . As a result, EMRIs represent some of the most promising candidates for detecting gravitational waves  10  .",
        "watermark_text": "We research the effects of general relativity on gravity wave bursts created by intense mass ratio inspirals ( EMRIs ) . We say that , for EMRI systems with total masses M = 10 ^ 6M _ solar and compact body masses m = 1M _ neutron star , the orbital period is fewer than one second at distances greater than 100 AU .This implies that these objects are likely to be identified as continuous waves rather than longer - duration bursts . The detection rate of such events depends strongly upon their luminosities ; we find that they may happen up to several twice per year within our universe .These data suggest that EMRIs might give an important source of insight about supermassive black holes . Keywords : Black hole , Compact binary system , General relativity , Gravitational wave , Inspiralling neutron star , Relativity concept INTRODUCTION In recent seasons there has been substantial interest in investigating the properties of gravitational rays generated during the last phases of stellar evolution when a huge star collapses into a black hole or neutron star 1 .Such mechanisms can generate incredibly energetic signals which will be detectable out to cosmological distances using upcoming space - based detectors 2 , notably LISA 3 . However , it remains unsure how many of these changes should really be emitted 4 .One potential class of structures which would emit strong gravitational waves are known as extreme - mass - ratio inspirals ( EMRIs ) 5 . Here , a small compact body spirals into a far more massive brown hole or neutron galaxy over thousands of orbits before being destroyed 6 .For instance , if a solar mass star were to spiral into a ten million solar mass black hole then its orbit may weaken down to just a few kilometres before merging 7 , 8 . If this process occurs nearer sufficient to the event horizon then the resulting signal will have very high frequencies 9 .As a result , EMRIs represent some of the most attractive candidates for detecting gravitational waves 10 .",
        "rewrite_text": "We investigate the impact of general relativity on gravitational wave bursts produced by extreme mass ratio inspirals (EMRIs). For EMRI systems with a total mass of \\( M = 10^6 M_{\\text{solar}} \\) and a compact body mass of \\( m = 1 M_{\\text{neutron star}} \\), we find that the orbital period is less than one second at distances exceeding 100 AU. This suggests that these systems will likely be recognized as continuous waves rather than prolonged bursts. The frequency of such events is highly dependent on their luminosities; our findings indicate that they could occur up to several times a year within our universe. This data implies that EMRIs could provide valuable insights into the nature of supermassive black holes.\n\n**Keywords:** Black hole, compact binary system, general relativity, gravitational wave, inspiraling neutron star, relativity concept.\n\n**INTRODUCTION:** Recent years have seen significant interest in studying the properties of gravitational waves produced during the final stages of stellar evolution, when a massive star collapses into a black hole or neutron star. Such processes can generate highly energetic signals detectable from cosmological distances using upcoming space-based detectors, particularly LISA. However, the exact rate at which these phenomena occur remains uncertain. One promising category of events that is expected to produce strong gravitational waves is known as extreme mass ratio inspirals (EMRIs). In these scenarios, a small, compact body spirals into a much more massive black hole or neutron star over thousands of orbits before ultimately merging. For example, if a solar mass star were to spiral into a black hole with ten million solar masses, its orbit could shrink to just a few kilometers before merging. Should this process take place close enough to the event horizon, the resulting signal would exhibit very high frequencies. Consequently, EMRIs are considered some of the most promising candidates for the detection of gravitational waves.",
        "ori-fast-z-score": -1.59111456835146,
        "water-fast-z-score": 6.36445827340584,
        "rewrite-fast-z-score": 0.8461538461538461
    },
    {
        "original_text": "We study non-commutative Donaldson–Thomas invariants for Calabi-Yau threefolds in terms of their crepant resolutions, which are called conifolds. We show that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary conditions. As an application we compute the Donaldson–Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic. The results agree with those obtained previously via other methods. This work is motivated by the recent progresses made in understanding Gromov-Witten theories through the use of symplectic field theories (SFTs). In particular, it has been shown recently that SFTs provide a natural framework to define Gromov-Witten invariants of singular varieties such as orbifolds or Deligne-Mumford stacks. However, there have not yet been any attempts to generalize this approach beyond Gromov-Witten theory.",
        "watermark_text": "We research non - commutative Donaldson – Thomas invariants for Calabi - Yau threefolds in terms of their crepant resolutions , which are called conifolds . We see that these invariants can be computed by counting holomorphic curves on the resolved spaces with suitable boundary constraints .As an implementation we compute the Donaldson – Thomas invariant of the quintic threefold using its crepant resolution as well as the mirror quintic . The results agree with those achieved prior via other methods .This research is prompted by the recent progresses done in understanding Gromov - Witten theories through the using of symplectic field theories ( SFTs ) . In particular , it has been shown lately that SFTs allow a natural framework to define Gromov - Witten invariants of singular varieties such as orbifolds or Deligne - Mumford stacks .However , there have not already been any efforts to generalize this methodology beyond Gromov - Witten theory .",
        "rewrite_text": "We investigate non-commutative Donaldson-Thomas invariants for Calabi-Yau threefolds focusing on their crepant resolutions, known as conifolds. Our findings reveal that these invariants can be calculated by counting holomorphic curves on the resolved spaces, subject to appropriate boundary conditions. As a specific application, we compute the Donaldson-Thomas invariant of the quintic threefold, utilizing both its crepant resolution and the mirror quintic. The results are consistent with those obtained through alternative approaches previously. This research is motivated by recent advancements in the understanding of Gromov-Witten theories through the application of symplectic field theories (SFTs). Notably, it has recently been demonstrated that SFTs provide a natural framework for defining Gromov-Witten invariants for singular varieties, including orbifolds and Deligne-Mumford stacks. However, efforts to extend this methodology beyond Gromov-Witten theory have yet to be undertaken.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.041451884327381,
        "rewrite-fast-z-score": -0.2773500981126146
    },
    {
        "original_text": "The physisorption of nucleobases (adenine, cytosine, guanine and thymine) onto graphene is investigated by density functional theory calculations at the B3LYP/6-31G(d) level in vacuum conditions. The results show that all four bases are adsorbed on the surface with different binding energies ranging between -0.27 eV for adenine to -1.10 eV for cytosine. In addition, it was found that the adsorption energy decreases as the number of nitrogen atoms increases. This indicates that the interaction strength depends strongly on the electronegativity of the base molecules. It has been shown that the most stable configuration corresponds to an end-on orientation where the carbonyl oxygen atom interacts directly with one of the C-C bonds of the graphene sheet. \n \n Keywords: Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations. Introduction \n \n Graphene is a two-dimensional material consisting of sp2-hybridized carbon atoms arranged into a honeycomb lattice structure  1  . Due to its unique electronic properties such as high carrier mobility  2  , large specific surface area  3  , thermal conductivity  4  , mechanical flexibility  5  , chemical stability  6  and biocompatibility  7, 8  , this material has attracted considerable attention over recent years  9  . However, despite these advantages, there have been some challenges associated with the use of pristine graphene sheets due to their hydrophobic nature  10  which limits their applications  11  . Therefore, many efforts have been made towards modifying the physical and chemical characteristics of graphene through various approaches including covalent  12  or non-covalent  13  functionalization  14  .\n \nIn particular, non-covalent functionalization can be achieved via π-π interactions  15  , hydrogen bonding  16  , electrostatic  17  , van der Waals  18  and ionic  19  forces  20  . Among them, π-π stacking is considered to be the strongest noncovalent force  21  . For example, several studies have reported that aromatic compounds  22  , fullerenes  23  , porphyrins  24  , metal complexes  25  and biomolecules  26  could interact with graphene surfaces via π-",
        "watermark_text": "The physisorption of nucleobases ( adenine , cytosine , guanine and thymine ) onto graphene is investigated by density functional theory analyses at the B3LYP / 6 - 31G ( d ) level in vacuum environments . The results show that all four bases are adsorbed on the surface with various binding frequencies ranging between - 0 . 27 eV for adenine to - 1 . 10 eV for cytosine .In addition , it was shown that the adsorption energy decreases as the number of nitrogen atoms increases . This implies that the interaction strength depends strongly on the electronegativity of the base atoms .It has been shown that the most stable configuration relates to an ending - on position where the carbonyl oxygen element interacts closely with one of the C - C bonds of the graphene sheet . Keywords : Physisorption ; Graphene ; Nucleobase ; Density Functional Theory Calculations .Introduction Graphene is a two - dimensional material consisting of sp2 - hybridized carbon atoms arranged into a honeycomb lattice structure 1 . Due to its unique electronic properties such as wide carrier density 2 , large specific surface region 3 , thermal conductivity 4 , electronic flexibility 5 , chemical integrity 6 and biocompatibility 7 , 8 , this metal has garnered considerable focus over recent years 9 .However , despite these benefits , there have been some challenges associated with the using of pristine graphene strips due to their hydrophobic properties 10 which restricted their functionality 11 . Therefore , various efforts have been placed towards modifying the physical and biological qualities of graphene through several methods namely covalent 12 or non - covalent 13 functionalization 14 .In particular , non - covalent functionalization can be obtained via π - π interactions 15 , hydrogen bonding 16 , electrostatic 17 , van der Waals 18 and ionic 19 forces 20 . Among them , π - π stacking is regarded to be the greatest noncovalent force 21 .For instance , various trials have reported that aromatic molecules 22 , fullerenes 23 , porphyrins 24 , metal ions 25 and biomolecules 26 could interact with graphene surfaces via π -",
        "rewrite_text": "The physisorption of nucleobases—adenine, cytosine, guanine, and thymine—on graphene is examined using density functional theory at the B3LYP/6-31G(d) level in vacuum conditions. The findings indicate that all four bases are adsorbed on the graphene surface, with binding energies varying from -0.27 eV for adenine to -1.10 eV for cytosine. Additionally, the results suggest that the adsorption energy decreases with an increasing number of nitrogen atoms, indicating that the interaction strength is significantly influenced by the electronegativity of the base atoms. The most stable configuration is associated with an end-on alignment, where the carbonyl oxygen of the nucleobase closely interacts with one of the C-C bonds of the graphene sheet. \n\n**Keywords:** Physisorption; Graphene; Nucleobase; Density Functional Theory Calculations. \n\n**Introduction:** Graphene is a two-dimensional material composed of sp²-hybridized carbon atoms arranged in a honeycomb lattice. Its remarkable electronic properties, including a broad carrier density, large specific surface area, high thermal conductivity, electronic flexibility, chemical stability, and biocompatibility, have attracted significant attention in recent years. However, the inherent hydrophobicity of pristine graphene strips poses challenges that limit their functional applications. Consequently, various strategies have been employed to enhance the physical and biological properties of graphene through methods such as covalent and non-covalent functionalization. Non-covalent functionalization can be achieved via π-π interactions, hydrogen bonding, electrostatic forces, van der Waals forces, and ionic interactions, with π-π stacking considered the strongest non-covalent force. Numerous studies have demonstrated that aromatic molecules, fullerenes, porphyrins, metal ions, and biomolecules can interact with graphene surfaces through π-π interactions.",
        "ori-fast-z-score": 0.09325048082403138,
        "water-fast-z-score": 7.366787985098479,
        "rewrite-fast-z-score": 0.8542421961772492
    },
    {
        "original_text": "We have analyzed the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and found evidence for intrinsic redshift components in both cases.  The first sample consists of 12 Seyfert galaxies, which are luminous AGNs with broad emission lines. We find that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose amplitude is correlated with the widths of the broad emission lines. This correlation suggests that the intrinsic redshift may arise from Doppler shifts associated with outflows or inflows of gas on scales comparable to those probed by the broad-line region. The second sample contains low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no obvious signs of being gravitationally lensed but do exhibit significant intrinsic redshift components. In this case we find that the amplitudes of these components correlate strongly with the optical continuum slopes measured at rest-frame wavelengths near 3000 Å .",
        "watermark_text": "We have analyzed the redshifts of two specimens of active galactic nuclei ( AGNs ) with varying luminosities and found proof for intrinsic redshift components in both cases . The first specimen consists of 12 Seyfert galaxies , which are luminous AGNs with broad absorption paths .We see that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose intensity is associated with the widths of the broad emission lines . This correlation suggests that the intrinsic redshift may arise from Doppler movements related with outflows or inflows of gas on scales similar to those probed by the broad - line region .The second survey features small - luminosity quasars chosen from the Sloan Digital Sky Survey Data Release 4 quasar catalog . These quasars exhibit no evident indication of being gravitationally lensed but do exhibit substantial intrinsic redshift components .In this situation we find that the amplitudes of these systems correlate strongly with the optical continuum curves observed at rest - mirror wavelengths near 3000 Å .",
        "rewrite_text": "We have examined the redshifts of two samples of active galactic nuclei (AGNs) with differing luminosities and have identified evidence for intrinsic redshift components in both instances. The first sample comprises 12 Seyfert galaxies, which are bright AGNs characterized by broad absorption features. Our analysis shows that their observed redshifts can be separated into an extrinsic component resulting from gravitational lensing by foreground objects, and an intrinsic component linked to the widths of the broad emission lines. This relationship implies that the intrinsic redshift may be due to Doppler motions associated with gas outflows or inflows occurring on scales similar to those of the broad-line region. The second sample involves low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no clear signs of gravitational lensing but do display significant intrinsic redshift components. In this case, we observe a strong correlation between the amplitudes of these quasars and the optical continuum curves recorded at rest-frame wavelengths around 3000 Å.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "The book is available at the following URL: http://arxiv.org/abs/gr-qc/0405033 The book contains an introduction to numerical relativity, including basic concepts and techniques in general relativity as well as some advanced topics such as black hole excision methods.  It also includes detailed descriptions on how to implement these algorithms using spectral methods (Fourier or Chebyshev expansions). This book will be useful not only for graduate students but also for researchers who are interested in applying spectral methods to solve problems in astrophysics. Authors:  Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, Xin-Bin Huang",
        "watermark_text": "The volume is accessible at the following URL : www : / / arxiv . org / abs / gr - qc / 0405033 The text includes an introduction to numerical gravity , comprising fundamental concepts and techniques in general relativity as well as some advanced topics such as black hole excision methods . It additionally contains detailed details on how to execute these algorithms utilizing spectral algorithms ( Fourier or Chebyshev expansions ) .This book will be valuable not only for graduate scholars but also for researchers who are concerned in implementing spectral algorithms to solve difficulties in astrophysics . Authors : Yi - Kai Shu , Jian - Xin Liu , Zhi - Hong Zhang , Yu - Ting Wu , Xiao - Liang Zhao , Wen - Jun Ma , Hong - Yuan Chen , Jie - Qiu Shi , Jun - Zhuo Wang , Yong - Sheng Zhou , Ming - Dao Li , Qing - Wei Guo , Xin - Bin Huang",
        "rewrite_text": "The volume is available at the following URL: www.arxiv.org/abs/gr-qc/0405033. It includes an introduction to numerical gravity, covering essential concepts and techniques in general relativity, as well as advanced topics such as black hole excision methods. The text also provides in-depth instructions on implementing these algorithms using spectral methods (Fourier or Chebyshev expansions). This book will be beneficial for both graduate students and researchers interested in applying spectral algorithms to tackle problems in astrophysics. The authors are Yi-Kai Shu, Jian-Xin Liu, Zhi-Hong Zhang, Yu-Ting Wu, Xiao-Liang Zhao, Wen-Jun Ma, Hong-Yuan Chen, Jie-Qiu Shi, Jun-Zhuo Wang, Yong-Sheng Zhou, Ming-Dao Li, Qing-Wei Guo, and Xin-Bin Huang.",
        "ori-fast-z-score": -1.9379255804998177,
        "water-fast-z-score": 4.323064756499593,
        "rewrite-fast-z-score": 2.8323527714997336
    },
    {
        "original_text": "We consider the dynamics of an ensemble of N interacting dyons in Minkowski space-time with one compactified dimension, and show that it is described by a statistical mechanics model which can be solved exactly for any number of particles. The exact solution shows that there are two phases depending on whether or not the temperature T exceeds some critical value Tc. For T>Tc we find that the system undergoes a phase transition to a state where all but one dyon have vanishing electric charge while their magnetic charges remain finite. In this regime the entropy density scales as S∼1/(g4N) at large N, where g denotes the coupling constant of the theory. We also discuss how our results may be generalized to other theories such as QCD. Introduction:-In recent years much attention has been paid to the study of strongly coupled gauge theories using various techniques ranging from lattice simulations  1  , holography  2  -  4  , and effective field theories  5  . One interesting question concerns the behavior of these systems when they are confined into small volumes  6  .\nThe purpose of this work is to investigate the properties of a particular class of confining gauge theories known as supersymmetric Yang-Mills (SYM). These theories are defined in terms of a set of fields transforming under the adjoint representation of SU(N), and possess both bosonic and fermionic degrees of freedom  7  . They play an important role in string theory  8  , and provide useful toy models for studying non-perturbative phenomena  9  . A particularly simple example of SYM is given by the so-called Seiberg-Witten limit  10  , where the gauge group is taken to be U(1).\nOne of the most remarkable features of SYM is its ability to confine quarks even though no fundamental scalar fields exist  11  . This phenomenon occurs because the vacuum expectation values of certain operators acquire non-vanishing VEVs leading to spontaneous breaking of global symmetries  12  . As a result, electrically charged excitations called  dyons  appear in the spectrum  13  . It turns out that the interactions between dyons lead to confinement  14  . Moreover, the",
        "watermark_text": "We consider the dynamics of an ensemble of N interacting dyons in Minkowski space - time with one compactified dimension , and find that it is characterized by a statistical mechanics model which can be answered exactly for any number of particles . The exact solution shows that there are two phases depending on whether or not the temperature T reaches some essential factor Tc .For T > Tc we find that the system undergoes a phase shift to a state where all but one dyon have vanishing electric current while their magnetic charges remain finite . In this regime the entropy concentration scales as [UNK] / ( g4N ) at large N , where g denotes the interaction constant of the physics .We especially consider how our findings may be generalized to other theories such as QCD . Introduction : - In recent history much attention has been paid to the observation of highly coupled gauge fields using numerous tactics ranging from lattice simulations 1 , holography 2 - 4 , and effective field theories 5 .One interesting question concerns the dynamics of these systems when they are enclosed into small volumes 6 . The purpose of this research is to examine the properties of a certain class of confining gauge fields known as supersymmetric Yang - Mills ( SYM ) .These theories are specified in terms of a group of fields transforming under the adjoint representation of SU ( N ) , and possess both bosonic and fermionic degrees of liberty 7 . They play an important role in string theory 8 , and play popular toy models for studying non - perturbative behavior 9 .A notably simple example of SYM is given by the so - called Seiberg - Witten limit 10 , where the gauge group is taken to be U ( 1 ) . One of the most noteworthy features of SYM is its able to confine quarks even though no basic scalar fields lie 11 .This phenomenon occurs because the vacuum expectation values of certain functions acquire non - vanishing VEVs resulting to accidental breaking of global symmetries 12 . As a result , electrically charged excitations called dyons emerge in the spectrum 13 .It turns out that the interactions between dyons contribute to confinement 14 . Moreover , the",
        "rewrite_text": "We investigate the dynamics of an ensemble of N interacting dyons in a Minkowski spacetime that includes one compactified dimension. Our findings reveal that this system can be precisely described by a statistical mechanics model applicable for any number of particles. The exact solution indicates the presence of two distinct phases based on whether the temperature T exceeds a critical value Tc. For temperatures greater than Tc, we observe a phase transition in which all but one dyon exhibit negligible electric current, while their magnetic charges remain finite. In this regime, the scaling of entropy concentration is proportional to [UNK] / (g4N) as N becomes large, where g represents the interaction constant. We also explore the potential generalization of our results to other theories, such as Quantum Chromodynamics (QCD).\n\n**Introduction:** Recently, there has been significant interest in studying highly coupled gauge fields using various methods, including lattice simulations, holography, and effective field theories. A particularly intriguing question is how these systems behave when confined to small volumes. This research aims to explore the properties of a specific class of confining gauge fields known as supersymmetric Yang-Mills (SYM). These theories are defined by a set of fields that transform under the adjoint representation of SU(N) and include both bosonic and fermionic degrees of freedom. They are crucial in string theory and serve as prominent toy models for investigating non-perturbative phenomena. A particularly simple example of SYM is found in the Seiberg-Witten limit, where the gauge group is taken to be U(1). One of the most striking aspects of SYM is its ability to confine quarks despite the absence of fundamental scalar fields. This confinement arises because the vacuum expectation values of certain functions acquire non-zero values, leading to the accidental breaking of global symmetries. Consequently, electrically charged excitations known as dyons appear in the spectrum, and their interactions have a significant role in the confinement process.",
        "ori-fast-z-score": 0.08192319205190406,
        "water-fast-z-score": 6.904757466825006,
        "rewrite-fast-z-score": 1.158648244043315
    },
    {
        "original_text": "We report on the discovery and analysis of XMM-Newton observations of an uncatalogued, extremely faint X-ray source (X-ray luminosity < 1031 erg s-1) in the Galactic plane at l = 28 deg., b = 0.5 deg.. The source was detected only during one observation performed with EPIC-pn camera in 2003 February. We have analyzed all available archival data for this region obtained by different space observatories including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. No other X-ray sources were found within the positional uncertainty circle of the new object down to limiting flux levels of ~3×10-12 erg cm-2 s-1 (0.2-10 keV). This makes it unlikely that the source is associated with any known classes of X-ray binaries or active galactic nuclei.",
        "watermark_text": "We report on the discovery and assessment of XMM - Newton discoveries of an uncatalogued , incredibly faint X - ray source ( X - ray luminosity < 1031 erg s - 1 ) in the Galactic jet at l = 28 deg . , b = 0 . 5 deg . . The source was seen only during one observation performed with EPIC - pn sensor in 2003 February .We have analyzed all available archival data for this area obtained by various space observatories namely Chandra , Swift / XRT , ASCA , RXTE / ASM , INTEGRAL / JEM - X , Suzaku / WAM , and HESS telescopes . No other X - ray bodies were found within the positional uncertainty arc of the new object down to limiting flux levels of ~ 3×10 - 12 erg centimetres - 2 s - 1 ( 0 . 2 - 10 keV ) .This leaves it unlikely that the source is associated with any established types of X - ray binaries or active galactic nuclei .",
        "rewrite_text": "We present our findings on the detection and evaluation of an uncatalogued, extremely faint X-ray source (X-ray luminosity < 10^31 erg s^-1) identified by XMM-Newton in the Galactic jet at coordinates l = 28°, b = 0.5°. This source was only recorded during a single observation conducted with the EPIC-pn sensor in February 2003. We have thoroughly analyzed all available archival data from various space observatories, including Chandra, Swift/XRT, ASCA, RXTE/ASM, INTEGRAL/JEM-X, Suzaku/WAM, and HESS telescopes. No additional X-ray objects were detected within the positional uncertainty arc of the new source, down to limiting flux levels of approximately 3×10^-12 erg cm^-2 s^-1 (0.2 - 10 keV). Therefore, it is improbable that this source is associated with any known types of X-ray binaries or active galactic nuclei.",
        "ori-fast-z-score": -0.5897678246195885,
        "water-fast-z-score": 4.718142596956708,
        "rewrite-fast-z-score": -0.14285714285714285
    },
    {
        "original_text": "The vortex dynamics is studied numerically for fractal cluster structure (FCCS) superconductor by solving time-dependent Ginzburg-Landau equations under an external magnetic field. The FCCS has been proposed as one possible candidate to explain the origin of high-Tc cuprates, and it consists of randomly distributed clusters which are connected each other via Josephson coupling. We find that the resistivity increases rapidly when the applied current exceeds some threshold value Ic(H), where H denotes the strength of the external magnetic field. This behavior can be understood by considering the motion of vortices inside the clusters. In addition, we show that the critical current density Jc decreases gradually with increasing temperature T . Finally, we discuss how these results may be relevant to experiments on high-Tc cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "watermark_text": "The vortex dynamics is studied numerically for fractal cluster structure ( FCCS ) superconductor by modeling period - dependent Ginzburg - Landau equations under an external magnetic force . The FCCS has been proposed as one possible candidate to explain the origin of high - Tc cuprates , and it consists of randomly distributed clusters which are connected each other via Josephson coupling .We see that the resistivity increases quickly when the introduced current reaches some threshold factor Ic ( H ) , where H represents the strength of the external magnetic force . This phenomenon can be understood by analyzing the movement of vortices inside the clusters .In addition , we show that the critical current density Jc falls gradually with rising heat T . Finally , we talk how these results may be applicable to experiments on high - Tc cuprate superconductors .PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k",
        "rewrite_text": "The dynamics of vortices in fractal cluster structure (FCCS) superconductors is investigated using numerical simulations of period-dependent Ginzburg-Landau equations influenced by an external magnetic force. FCCS is proposed as a potential explanation for the origin of high-Tc cuprates, characterized by randomly distributed clusters interconnected through Josephson coupling. Our findings indicate that resistivity rises sharply when the applied current surpasses a certain threshold, Ic(H), with H representing the external magnetic force's intensity. This behavior can be elucidated by examining the motion of vortices within the clusters. Furthermore, we demonstrate that the critical current density, Jc, gradually decreases as temperature, T, increases. Lastly, we discuss the implications of these results for experimental investigations of high-Tc cuprate superconductors. PACS: 74.20.-q; 74.25.+s; 74.60.Bz; 74.70.-k.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 0.8017837257372732
    },
    {
        "original_text": "We present the results of a numerical study on the relaxation dynamics of an initially straight contact line in a two-dimensional geometry, which is driven by surface tension and viscous dissipation at the moving interface between liquid and gas phases.  We solve the Navier-Stokes equations for incompressible fluids with free-slip boundary conditions using a spectral element method to simulate the flow field around the evolving droplet shape. The initial condition consists of a circular droplet sitting on top of a flat substrate that has been perturbed slightly away from its equilibrium position. As time evolves, we observe the formation of capillary waves along the contact line as well as the development of small satellite drops near the main droplet due to pinching off events. In addition, we find that the contact angle decreases continuously during this process until it reaches zero degrees when the entire droplet detaches from the substrate. Finally, we compare our simulation results against experimental data obtained from high-speed video microscopy measurements performed by other researchers.",
        "watermark_text": "We present the conclusion of a numerical investigation on the relaxation behavior of an initially parallel contact line in a two - dimensional topology , which is generated by surface friction and viscous dissipation at the moved junction between liquid and gas phases . We calculate the Navier - Stokes equations for incompressible gases with loose - slipping border conditions utilizing a spectral component process to simulate the flow field around the evolving droplet shape .The initial condition consists of a circular droplet standing on top of a flattened substrate that has been perturbed slightly apart from its stable position . As period evolves , we monitor the formation of capillary currents along the contact line as also as the development of tiny satellite flows near the main droplet thanks to pinching off events .In addition , we find that the contact angle decreases continuously during this process until it meets zero degrees when the entire droplet detaches from the substrate . Finally , we compare our modeling results against empirical data received from high - speed tape microscopy observations performed by other researchers .",
        "rewrite_text": "We present the findings of a numerical study that examines the relaxation behavior of an initially parallel contact line within a two-dimensional framework. This behavior is influenced by surface friction and viscous dissipation at the interface between the liquid and gas phases. Using a spectral component method, we solve the Navier-Stokes equations for incompressible gases, applying loose-slip boundary conditions to model the flow field around the changing shape of the droplet. Our initial setup features a circular droplet positioned on a flattened substrate, slightly disturbed from its equilibrium state. As time progresses, we observe the formation of capillary currents along the contact line and the emergence of small satellite flows near the primary droplet due to pinch-off events. Additionally, we note a continuous decrease in the contact angle throughout this process, ultimately reaching zero degrees when the droplet fully detaches from the substrate. Finally, we compare our simulation results with empirical data obtained from high-speed tape microscopy studies conducted by other researchers.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 7.267170336924982,
        "rewrite-fast-z-score": 2.5584085962673253
    },
    {
        "original_text": "We study the dynamics of an ensemble of interacting vortices in a two-dimensional superfluid helium film, which is driven by a rotating substrate at constant angular velocity . We show that this system exhibits aging behavior similar to spin glasses or other disordered systems with quenched randomness. The relaxation time increases exponentially as a function of waiting time t w , i.e., the duration during which the driving has been switched off before starting the measurement. This increase can be described by a stretched exponential law exp(−(t/τ)β), where τ denotes the characteristic relaxation time and β < 1 describes its distribution width. In addition we find that the response of our system depends on the history of the applied external drive. For example, if one starts with a high rotation frequency f 0 = ω0/2π and then decreases it slowly towards zero (quasi-static protocol), the final state after switching off the drive does not depend on the initial value f0. However, if one switches off the drive suddenly (sudden protocol), the final state strongly depends on f0.",
        "watermark_text": "We explore the dynamics of an ensemble of interacting vortices in a two - dimensional superfluid helium movie , which is driven by a rotating substrate at constant angular velocity . We see that this scheme exhibits aging activity similar to spinning mirrors or other disordered systems with quenched randomness .The relaxation time rises exponentially as a function of waiting period t w , i . e . , the duration during which the driving has been turned off before beginning the measurement . This increase can be described by a stretched exponential law exp ( − ( t / τ ) α ) , where τ represents the characteristic relaxation time and β < 1 refers its distribution length .In addition we find that the response of our system varies on the history of the applied external drive . For instance , if one starts with a high rotation rate w 0 = ω0 / 2π and then decreases it slowly approaching zero ( quasi - static protocol ) , the last position after switching off the drive does not depend on the first value f0 .However , if one switches off the drive abruptly ( sudden protocol ) , the finished state strongly depends on f0 .",
        "rewrite_text": "We investigate the dynamics of a collection of interacting vortices in a two-dimensional superfluid helium film, influenced by a rotating substrate that maintains a constant angular velocity. Our findings reveal that this setup demonstrates aging behavior akin to that observed in spinning mirrors and other disordered systems characterized by quenched randomness. The relaxation time increases exponentially with the waiting time \\( t_w \\), which is the period during which the driving force is turned off before measurements begin. This growth can be described by a stretched exponential function \\( \\exp\\left(-\\left(\\frac{t}{\\tau}\\right)^\\alpha\\right) \\), where \\( \\tau \\) signifies the characteristic relaxation time and \\( \\beta < 1 \\) indicates the length distribution. Furthermore, we observe that the system's response is influenced by the history of the external drive. For example, when the rotation rate \\( w_0 = \\frac{\\omega_0}{2\\pi} \\) is initially set high and then gradually reduced to near zero (a quasi-static protocol), the final position after turning off the drive is independent of the initial value \\( f_0 \\). In contrast, if the drive is switched off abruptly (a sudden protocol), the resulting state is significantly affected by the initial value \\( f_0 \\).",
        "ori-fast-z-score": -0.7683498199278324,
        "water-fast-z-score": 5.09786575873842,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "We present the results of cosmological simulations that follow the growth of supermassive black holes (SMBHs) by stochastic gas inflow, including radiative feedback effects on their surroundings. We find that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6. At lower redshift we observe an increase in the fraction of mass gained via gas accretion relative to merger events. The resulting luminosity function is consistent with observations for both active galactic nuclei (AGNs) and quasars out to z = 7.5. Our model predicts a large number of low-luminosity AGNs which are not observed yet but may be detectable with future surveys such as LSST or Euclid. In addition, our model produces a population of obscured quasars whose properties agree well with recent observational constraints. Finally, we show that the predicted quasar lifetime distribution agrees very well with current estimates based on SDSS data.",
        "watermark_text": "We present the conclusion of cosmological simulations that follow the development of supermassive black holes ( SMBHs ) by stochastic gas inflow , particularly radiative feedback effects on their environment . We see that SMBHs grow primarily through mergers with other BHs rather than gas accretion at high redshifts z > 6 .At lower redshift we study an increase in the fraction of mass gained via gas accretion compared to merger reactions . The resulting luminosity function is compatible with observations for both active galactic nuclei ( AGNs ) and quasars out to z = 7 . 5 .Our model predicts a large number of low - luminosity AGNs which are not observed yet but might be detectable with current surveys such as LSST or Euclid . In addition , our model generates a population of distorted quasars whose characteristics comply better with recent observational restrictions .Finally , we prove that the expected quasar lifetime spread agrees very best with current estimates based on SDSS information .",
        "rewrite_text": "We present the findings from our cosmological simulations that track the development of supermassive black holes (SMBHs) through stochastic gas inflow, with a particular emphasis on the impact of radiative feedback on their surroundings. Our results indicate that SMBHs primarily grow through mergers with other black holes at high redshifts (z > 6), while at lower redshifts, we observe an increase in the mass gained through gas accretion relative to merger events. The resulting luminosity function aligns well with observational data for both active galactic nuclei (AGNs) and quasars up to z = 7.5. Our model also predicts a significant population of low-luminosity AGNs that have not yet been detected but could be visible in ongoing surveys like LSST or Euclid. Additionally, the model produces a group of distorted quasars that better fit recent observational constraints. Finally, we demonstrate that the anticipated range of quasar lifetimes is consistent with current estimates derived from SDSS data.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We study quantum spin-1/2 systems with spatially anisotropic exchange interactions on the distorted kagome lattice, which is relevant for volborthite. We show that this system can be mapped onto an effective Heisenberg model in terms of classical spins residing at the centers of hexagons formed by nearest-neighbor bonds. The ground state phase diagram consists of three phases: ferromagnetic (FM), antiferromagnetic (AFM) and canted AFM states. In particular, we find that the FM order survives even when the distortion is strong enough to destroy it completely without spatial anisotropy. This result suggests that the magnetic properties of volborthite are governed not only by the interlayer coupling but also by the intralayer one. Furthermore, we discuss possible origins of the observed magnetization plateau in volborthite. \nI. INTRODUCTIO N\nThe distorted kagome lattice has attracted much attention recently because its structure is realized in several materials such as volborthite  1  , kapellasite  2  , herbertsmithite  3  , vesignieite  4  . These compounds have been studied extensively both experimentally  5  -  8  and theoretically  9  -  11  .\nIn particular, volborthite shows rich physical phenomena including a magnetization plateau around 1/3 of saturation magnetization M s  12 -  14  . It was suggested that these features originate from the presence of the distorted kagome layers  15  . However, there still remain many open questions about the microscopic mechanism behind them  16  . For example, what kind of interaction plays a crucial role? Is the distortion necessary or not?\nTo answer these questions, it would be useful to investigate the effect of the distortion systematically using theoretical methods  17  . Although some studies have already been done  18  -  20  , they were limited to small clusters and/or weak distortion cases. Therefore, it remains unclear how the distortion affects the magnetic properties of the distorted kagomé layer.\nIn this work, we study quantum spin-1/2 models with spatially anisotropic exchanges on the distorted kagomé lattice  see Figs. 1(",
        "watermark_text": "We explore quantum spin - 1 / 2 systems with spatially anisotropic exchange interactions on the altered kagome lattice , which is relevant for volborthite . We see that this scheme can be mapped onto an efficient Heisenberg structure in terms of classical spins residing at the centers of hexagons formed by nearest - neighbor bonds .The ground state phase diagram consists of three stages : ferromagnetic ( FM ) , antiferromagnetic ( AFM ) and canted AFM states . In particular , we find that the FM order survives even when the distortion is strong enough to destroy it completely without spatial anisotropy .This result suggests that the magnetic properties of volborthite are governed not only by the interlayer coupling but also by the intralayer one . Furthermore , we investigate possible origins of the seen magnetization plateau in volborthite .I . INTRODUCTIO N The distorted kagome lattice has drew much attention lately because its formation is realized in multiple structures such as volborthite 1 , kapellasite 2 , herbertsmithite 3 , vesignieite 4 . These compounds have been studied thoroughly both experimentally 5 - 8 and theoretically 9 - 11 .In particular , volborthite shows rich physical phenomena including a magnetization peak around 1 / 3 of saturation magnetization M s 12 - 14 . It was suggested that these characteristics derive from the presence of the altered kagome layers 15 .However , there still continue several open questions about the microscopic process behind them 16 . For instance , what sort of interaction plays a crucial role ?Is the distortion required or not ? To answer these problems , it would be used to examine the impact of the distortion thoroughly using theoretical methods 17 .Although some researchers have already been performed 18 - 20 , they were restricted to small clusters and / or strong distortion cases . Therefore , it remains unsure how the degradation affects the magnetic properties of the altered kagomé coating .In this research , we study quantum spin - 1 / 2 models with spatially anisotropic exchanges on the curved kagomé lattice see Figs . 1 (",
        "rewrite_text": "We investigate quantum spin-1/2 systems with spatially anisotropic exchange interactions on the modified kagome lattice, which is pertinent to volborthite. Our findings indicate that this framework can be effectively represented as a Heisenberg model involving classical spins located at the hexagon centers formed by nearest-neighbor bonds. The ground state phase diagram reveals three distinct regions: ferromagnetic (FM), antiferromagnetic (AFM), and canted AFM states. Notably, we observe that the FM order persists even when distortion is sufficiently strong to eliminate it completely in the absence of spatial anisotropy. This implies that the magnetic properties of volborthite are influenced not only by interlayer interactions but also by intralayer ones. Additionally, we explore the potential origins of the observed magnetization plateau in volborthite. \n\nINTRODUCTION: Recently, the distorted kagome lattice has garnered significant attention due to its presence in various materials such as volborthite, kapellasite, herbertsmithite, and vesignieite. These compounds have been extensively studied through both experimental and theoretical approaches. In particular, volborthite exhibits intriguing physical phenomena, including a magnetization peak at approximately one-third of its saturation magnetization. This behavior is believed to arise from the presence of modified kagome layers. Nonetheless, several questions remain regarding the microscopic mechanisms underlying these phenomena. For instance, what types of interactions are most crucial? Is distortion necessary? Addressing these questions requires a comprehensive theoretical examination of distortion effects. While some researchers have conducted studies on this topic, their investigations have often been limited to small clusters or extreme distortion scenarios. As a result, it remains unclear how degradation influences the magnetic properties of the altered kagome lattice. In this work, we analyze quantum spin-1/2 models with spatially anisotropic exchanges on the curved kagome lattice, as illustrated in Figures 1.",
        "ori-fast-z-score": 2.280350850198276,
        "water-fast-z-score": 8.584388698288787,
        "rewrite-fast-z-score": 1.6352596350653539
    },
    {
        "original_text": "We prove that for any surface S of general type, the action of its absolute Galois group G(S) on the set of connected components of the modulis space M_g(S) is faithful.  This result has been conjectured by Grothendieck and proved in many cases (e.g., when g = 0 or 1).  We use this to show that if S admits an automorphism of order p > 2 then it also admits one of order q prime to p; we give examples where both orders are arbitrarily large. The proof relies on results about the existence of certain families of curves on S which have been obtained recently using techniques from algebraic geometry and number theory. In particular, we make essential use of the fact that the canonical map of such a curve C onto P^1 is birational; this implies that the image of C under the Albanese map Alb_S : S -> Alb_S(S) is not contained in a fiber of Alb_S.",
        "watermark_text": "We prove that for any manifold S of general kind , the operation of its absolute Galois group G ( S ) on the group of connected parts of the modulis space M _ g ( S ) is faithful . This result has been conjectured by Grothendieck and demonstrated in many situations ( e . g . , when g = 0 or 1 ) .We use this to see that if S gives an automorphism of order q > 2 then it also admits one of order q prime to p ; we give instance where both orders are arbitrarily big . The proof draws on findings about the existence of certain classes of curves on S which have been achieved lately utilizing techniques from algebraic topology and number theory .In particular , we give important use of the fact that the canonical mapping of such a curve C onto P ^ 1 is birational ; this implies that the image of C under the Albanese map Alb _ S : S - > Alb _ S ( S ) is not enclosed in a fiber of Alb _ S .",
        "rewrite_text": "We demonstrate that for any manifold \\( S \\) of a general type, the action of its absolute Galois group \\( G(S) \\) on the group of connected components of the moduli space \\( M_g(S) \\) is faithful. This result, conjectured by Grothendieck, has been verified in various cases (for instance, when \\( g = 0 \\) or \\( g = 1 \\)). We use this finding to show that if \\( S \\) possesses an automorphism of order \\( q > 2 \\), it can also have one of order \\( q \\) that is coprime to \\( p \\); we provide examples where both orders can be made arbitrarily large. The proof relies on recent discoveries regarding the existence of specific classes of curves on \\( S \\), which have been established using methods from algebraic topology and number theory. Notably, we leverage the fact that the canonical map of such a curve \\( C \\) onto \\( \\mathbb{P}^1 \\) is birational, indicating that the image of \\( C \\) under the Albanese map \\( \\text{Alb}_S: S \\to \\text{Alb}_S(S) \\) does not lie entirely within a fiber of \\( \\text{Alb}_S \\).",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "We present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). The data reveal that the ring is highly polarized at infrared wavelengths with an average polarization fraction of ~20%. We find no evidence for significant changes to this value over time scales ranging from years to decades. This result suggests that dust grains are not being destroyed or created on short timescales within the ring. In addition, we detect a small degree of circularly polarized light emerging from the central region of the ring which may be due to scattering off non-spherical particles such as ice crystals. \n \n We also report results from our analysis of archival Hubble Space Telescope images showing that the ring has remained remarkably stable since its discovery more than 20 years ago. Finally, we discuss how these findings can be used to constrain models of bipolar nebulae formation. Keywords: Polarization; Circumstellar matter",
        "watermark_text": "We report new near - infrared polarimetric studies of the equatorial ring surrounding the evolved star HD 163296 ( HR 5171 A ) . The data reveal that the circle is strongly polarized at infrared wavelengths with an estimated polarization fraction of ~ 20 % .We see no evidence for significant variations to this value over time ranges ranging from years to decades . This result suggests that dust grains are not being destroyed or created on short timescales within the ring .In addition , we perceive a small level of circularly polarized light arriving from the main region of the circle which may be due to scattering off non - cylindrical ions such as ice particles . We additionally report findings from our analysis of archival Hubble Space Telescope images indicating that the circle has remained remarkably steady since its observation more than 20 decades ago .Finally , we explain how these results can be used to constrain models of bipolar nebulae formation . Keywords : Polarization ; Circumstellar matter",
        "rewrite_text": "We present new near-infrared polarimetric observations of the equatorial ring surrounding the evolved star HD 163296 (HR 5171 A). Our findings indicate that the ring exhibits strong polarization at infrared wavelengths, with an estimated polarization fraction of approximately 20%. We observe no significant variations in this value over time spans of years to decades, suggesting that the dust grains within the ring are neither being created nor destroyed on short timescales. Additionally, we detect a slight amount of circularly polarized light emanating from the main region of the ring, which may result from scattering off non-cylindrical particles, such as ice. Furthermore, our analysis of archival Hubble Space Telescope images reveals that the ring has remained remarkably stable since its observation over 200 years ago. Finally, we discuss how these findings can be utilized to refine models of bipolar nebula formation. Keywords: Polarization; Circumstellar matter.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.3804502135457675,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We study the effects of noise on spatially extended systems by using an extension of the concept of nonequilibrium potential (NEP). We show that NEPs can be used to characterize different types of stochastic resonances, such as those observed for excitable and bistable systems near their respective Hopf bifurcations. In particular we find that the presence of noise enhances the amplitude of oscillations in both cases but with very different mechanisms. For excitable systems this is due to the fact that noise increases the probability of crossing the threshold between two stable states; while for bistable systems it occurs because noise induces transitions between these states. Finally, we discuss how our results are related to previous studies based on other approaches. Stochastic resonance has been studied extensively during recent years  1  . It refers to the phenomenon whereby weak signals can be enhanced or detected more easily when they are embedded into a noisy background  2  .\nIn many physical situations, however, one needs to consider not only the effect of external noise sources but also internal fluctuations arising from the dynamics itself  3  . This problem becomes particularly relevant if the signal-to-noise ratio is small  4  , which may occur either because the signal is intrinsically weak or because its intensity is comparable to the level of intrinsic noise  5  . Moreover, even though the signal is strong enough so that it could be clearly distinguished without any additional noise  6  , there might still exist some optimal amount of noise that maximizes the detection efficiency  7, 8  .",
        "watermark_text": "We research the effects of noise on spatially extended systems by using an extension of the idea of nonequilibrium potential ( NEP ) . We see that NEPs can be used to characterize many kinds of stochastic resonances , such as those observed for excitable and bistable systems near their different Hopf bifurcations .In particular we find that the presence of noise enhances the frequency of oscillations in both cases but with very different mechanisms . For excitable systems this is due to the fact that noise changes the probability of reaching the threshold between two stable states ; while for bistable systems it appears because sound induces transitions between these states .Finally , we talk how our findings are related to previous research based on other methods . Stochastic resonance has been studied significantly during recent seasons 1 .It refers to the situation whereby soft signals can be enhanced or detected more easily when they are lodged into a loud background 2 . In many physical conditions , however , one needs to consider not only the impact of external sound sources but also internal fluctuations originating from the dynamics itself 3 .This problem appears particularly relevant if the signal - to - noise proportion is tiny 4 , which may happen either because the signal is intrinsically weak or because its brightness is equal to the level of intrinsic noise 5 . Moreover , even though the signal is strong enough so that it could be obviously differentiated without any additional noise 6 , there might nevertheless exist some optimal level of noise that maximizes the detection efficiency 7 , 8 .",
        "rewrite_text": "We investigate the impact of noise on spatially extended systems through an extension of the concept of nonequilibrium potential (NEP). Our findings indicate that NEPs can effectively characterize various types of stochastic resonance, including those seen in excitable and bistable systems near their different Hopf bifurcations. Notably, we observe that the presence of noise increases the frequency of oscillations in both scenarios, albeit through distinct mechanisms. For excitable systems, this is attributed to noise altering the probability of reaching the threshold between two stable states. In contrast, for bistable systems, noise facilitates transitions between these states. We also relate our results to previous studies conducted using alternative methods. Stochastic resonance has been extensively examined in recent seasons, referring to the phenomenon where weak signals can be amplified or more easily detected amidst a loud background. However, in many physical scenarios, it is crucial to account for not only external noise sources but also internal fluctuations that arise from the system's dynamics. This issue is particularly significant when the signal-to-noise ratio is low, which can occur if the signal is inherently weak or if its intensity is comparable to the level of intrinsic noise. Furthermore, even when the signal is sufficiently strong to be distinguished from background noise, there may still exist an optimal level of noise that maximizes detection efficiency.",
        "ori-fast-z-score": 0.38138503569823695,
        "water-fast-z-score": 6.928853368993243,
        "rewrite-fast-z-score": 0.09667364890456635
    },
    {
        "original_text": "The Large Hadron Collider (LHC) is expected to produce many new particles and discoveries in its Run II program, including possible evidence for physics beyond the Standard Model through rare decays such as b → sγ or t → cZ. The Belle experiment has recently measured these processes with unprecedented precision, providing important constraints on models that predict deviations from Standard Model expectations.  In this talk I will present an overview of recent results from Belle related to searches for new physics in rare decay modes involving charm quarks. These include measurements of:  - Branching fractions for charmless hadronic B meson decays; - CP asymmetries in neutral D mesons produced in B-meson decays; - Branching fraction for B(Bc→J/ψX). \nI will also discuss how these results can be used to constrain extensions of the Standard Model. Finally, I will briefly describe some future plans for Belle experiments.",
        "watermark_text": "The Large Hadron Collider ( LHC ) is expected to produce many new ions and discoveries in its Run II program , particularly likely evidence for physics beyond the Standard Model through rare decays such as b → sγ or t → cZ . The Belle study has recently recorded these reactions with incredible clarity , providing key restrictions on predictions that forecast deviations from Standard Model expectations .In this talk I will present an overview of recent results from Belle linked to searches for recent theory in rare decay modes involving charm quarks . These include estimates of : - Branching fractions for charmless hadronic B meson decays ; - CP asymmetries in neutral D mesons produced in B - meson decays ; - Branching fraction for B ( Bc→J / ψX ) .I will also discuss how these results can be used to constrain extensions of the Standard Model . Finally , I will briefly outline some future proposals for Belle experiments .",
        "rewrite_text": "The Large Hadron Collider (LHC) is anticipated to yield numerous new ions and discoveries during its Run II program, especially in the search for evidence of physics beyond the Standard Model, through rare decay processes such as b → sγ and t → cZ. Recently, the Belle experiment has captured these reactions with exceptional precision, offering critical constraints on predictions that suggest deviations from Standard Model expectations. In this presentation, I will provide an overview of recent Belle results related to theories involving rare decay modes of charm quarks. This will include estimates of branching fractions for charmless hadronic B meson decays, CP asymmetries in neutral D mesons arising from B meson decays, and the branching fraction for B (Bc → J/ψX). I will also discuss how these findings can be utilized to limit extensions of the Standard Model. Finally, I will touch on some future proposals for Belle experiments.",
        "ori-fast-z-score": 1.1920791213585393,
        "water-fast-z-score": 5.960395606792697,
        "rewrite-fast-z-score": 1.1920791213585393
    },
    {
        "original_text": "We present an analysis of the accuracy with which different approximants to gravitational-wave (GW) signals emitted by coalescing binaries can be recovered using matched filtering techniques, in particular when applied to simulated detector noise. We use two sets of simulated data: one set generated numerically for equal-mass non-spinning black-hole binaries; another set produced analytically under the restricted post-Newtonian approximation. The latter is used as input into several families of approximate GW templates that are commonly employed in searches for compact-binary mergers. For each template family we perform a Bayesian parameter-estimation study on both synthetic datasets, varying the total mass M , dimensionless spin magnitude χ1z = |χ1|/M2, inclination angle ι between orbital angular momentum vector and line-of-sight, polarization angle ψ0, sky position angles θS and φS, time-of-arrival t0, phase offset ∆Φ0, and amplitude A. In addition, we also vary the distance D to the source. Our results show that all considered template families yield accurate estimates of the physical parameters of the system within their respective ranges of validity. However, there exist significant differences among them regarding how well they recover these parameters.",
        "watermark_text": "We present an assessment of the accuracy with which different approximants to gravitational - wave ( GW ) transmissions generated by coalescing binaries can be recovered using matched filtering tactics , in example when applied to modeled detector noise . We use two sets of simulated evidence : one set produced numerically for equivalent - mass non - spinning black - hole binaries ; another set produced analytically under the restricted pre - Newtonian approximation .The latter is utilized as input into numerous families of approximate GW templates that are often employed in searches for compact - binary mergers . For each template family we perform a Bayesian parameter - estimation analysis on both synthetic datasets , changing the total mass M , dimensionless spin magnitude χ1z = | χ1 | / M2 , inclination angle η between orbital angular velocity vector and line - of - view , polarization angle ψ0 , sky position angles θS and φS , time - of - arrival t0 , phase offset [UNK] , and amplitude A .In addition , we also varies the distance D to the origin . Our results show that all considered template groups yield reliable estimates of the physical components of the system within their different ranges of relevance .However , there remain considerable variations among them governing how best they recover these parameters .",
        "rewrite_text": "We provide an evaluation of the accuracy with which various approximants to gravitational wave (GW) signals from coalescing binaries can be retrieved using matched filtering techniques, particularly when applied to modeled detector noise. Our analysis is based on two sets of simulated data: the first set is generated numerically for equal-mass, non-spinning black hole binaries, while the second set is derived analytically under the restricted pre-Newtonian approximation. The latter set serves as input for multiple families of approximate GW templates commonly used in the search for compact binary mergers. For each template family, we conduct a Bayesian parameter estimation analysis on both synthetic datasets, varying parameters such as total mass \\(M\\), dimensionless spin magnitude \\(\\chi_{1z} = |\\chi_1| / M^2\\), the inclination angle \\(\\eta\\) between the orbital angular momentum vector and the line of sight, the polarization angle \\(\\psi_0\\), the sky position angles \\(\\theta_S\\) and \\(\\phi_S\\), the arrival time \\(t_0\\), phase offset \\(\\varphi_0\\), amplitude \\(A\\), and the distance \\(D\\) to the source. Our results indicate that all template families provide reliable estimates of the physical parameters of the system within their respective domains of applicability. Nonetheless, substantial variations exist among the families regarding their effectiveness in recovering these parameters.",
        "ori-fast-z-score": -0.8955334711889903,
        "water-fast-z-score": 5.148767223478707,
        "rewrite-fast-z-score": -0.10050378152592121
    },
    {
        "original_text": "The detection of the hypothetical eta-mesic nuclei is one of the most promising ways to find new physics beyond Standard Model (SM). The experimental signature of such an exotic state would be a peak in the invariant mass distribution of the final-state particles produced by its decay, which can be observed as a bump on top of the smooth nuclear structure function background. In this work we present results obtained using Monte Carlo simulations and Geant4-based full simulation of the proposed experiment at JLab 12 GeV upgrade facility. We show that it will allow us to detect eta-mesic nuclei with high efficiency and good resolution over wide range of masses up to A = 100. This will provide unique opportunity to study properties of these exotic systems and test theoretical predictions. \n \n Keywords: eta-mesic nucleus, eta-nucleon interaction, eta production, eta-decay, eta-nuclear form factor, eta-nuclear potential",
        "watermark_text": "The detection of the hypothetical eta - mesic nuclei is one of the most exciting ways to find new physics beyond Standard Model ( SM ) . The empirical signature of such an exotic state would be a peak in the invariant mass distribution of the finished - state particles generated by its decay , which can be viewed as a bump on top of the smooth radioactive structure parameter background .In this project we present results derived using Monte Carlo simulations and Geant4 - based full simulation of the suggested experiment at JLab 12 GeV development facility . We suggest that it will able us to identify eta - mesic nuclei with high efficiency and good sensitivity over broad spectrum of masses up to A = 100 .This will provide unique opportunity to study properties of these exotic systems and question fundamental predictions . Keywords : eta - mesic nucleus , eta - nucleon collision , eta production , eta - emission , eta - atomic shape factor , eta - atomic potential",
        "rewrite_text": "The detection of hypothetical eta-mesic nuclei represents a thrilling opportunity to explore new physics beyond the Standard Model (SM). The empirical signature of such an exotic state would manifest as a peak in the invariant mass distribution of the final-state particles produced from its decay, appearing as a bump on the otherwise smooth background of the radioactive structure parameter. In this project, we present findings based on Monte Carlo simulations and a Geant4-based full simulation of the proposed experiment at the JLab 12 GeV development facility. We propose that this approach will enable us to identify eta-mesic nuclei with high efficiency and sensitivity across a wide range of masses, extending up to A = 100. This presents a unique opportunity to investigate the properties of these exotic systems and to challenge fundamental predictions. Keywords: eta-mesic nucleus, eta-nucleon collision, eta production, eta emission, eta-atomic shape factor, eta-atomic potential.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "We present the results of an analysis of data taken by Milagro, which is sensitive to gamma rays with energies between 100 GeV and 10 TeV. We find that there are no significant excesses in the direction of any short duration GRB (T90 < 2 s) detected by BATSE or Swift during their respective lifetime. The upper limits we derive for these bursts range from 1x10^-6 photons/cm^2/s at 100 GeV up to 3x10^-5 photons/cm^2/s at 10 TeV. These constraints rule out models where the emission is dominated by inverse Compton scattering off relativistic electrons accelerated in internal shocks. \n \n Keywords: Gamma-ray bursts, Milagro Observatory, Upper limit, Internal shock model, Inverse Compton Scattering. Subject headings: Astrophysics - Gamma Ray Astronomy - Cosmic Rays - Space Weather - Fermi Bubbles - Blazars",
        "watermark_text": "We present the conclusion of an assessment of evidence performed by Milagro , which is sensitive to gamma radiation with energies between 100 GeV and 10 TeV . We see that there are no major excesses in the direction of any low duration GRB ( T90 < 2 s ) detected by BATSE or Swift during their respective lifetime .The higher limits we derive for these bursts vary from 1x10 ^ - 6 photons / cm ^ 2 / s at 100 GeV up to 3x10 ^ - 5 photons / cm ^ 2 / s at 10 TeV . These limitations make out models where the emission is dominated by linear Compton absorption off relativistic electrons accelerated in internal shocks .Keywords : Gamma - ray flare , Milagro Observatory , Upper limit , Internal shock model , Inverse Compton Scattering . Subject headings : Astrophysics - Gamma Ray Astronomy - Cosmic Rays - Space Weather - Fermi Bubbles - Blazars",
        "rewrite_text": "We present the findings of an evaluation conducted by Milagro, which is capable of detecting gamma radiation in the energy range of 100 GeV to 10 TeV. Our analysis reveals that there are no significant excesses in the direction of any low-duration gamma-ray bursts (GRBs) (T90 < 2 s) recorded by BATSE or Swift throughout their operational periods. The upper limits we established for these bursts range from 1 x 10^-6 photons/cm^2/s at 100 GeV to 3 x 10^-5 photons/cm^2/s at 10 TeV. These constraints challenge models in which emission is primarily governed by linear Compton scattering off relativistic electrons that are accelerated in internal shocks. \n\nKeywords: Gamma-ray flare, Milagro Observatory, Upper limit, Internal shock model, Inverse Compton Scattering. \n\nSubject headings: Astrophysics, Gamma Ray Astronomy, Cosmic Rays, Space Weather, Fermi Bubbles, Blazars.",
        "ori-fast-z-score": -1.2649110640673518,
        "water-fast-z-score": 2.846049894151541,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study satellites in simulated galaxies with different masses at z = 0 using high-resolution cosmological hydrodynamic simulations (the Millennium Run). We find that there is no significant difference between the number density profiles of satellite galaxies around central galaxies with different luminosities or halo masses. The radial distribution of satellites shows an excess over the predictions based on the subhalo abundance matching technique for r < 30 kpc/h. This excess can be explained by tidal stripping of satellites before they are accreted onto the main galaxy. In addition, we show that the fraction of surviving satellites decreases rapidly as a function of distance from the center of host halos. Finally, we investigate how the properties of satellites depend on those of their hosts. Our results suggest that the majority of faint satellites may have been destroyed through mergers and/or tidal disruption during infall into larger systems. These findings provide important constraints on models of galaxy formation.",
        "watermark_text": "We research satellites in simulated galaxies with various masses at z = 0 using high - resolution cosmological hydrodynamic simulations ( the Millennium Run ) . We see that there is no major variation between the number density characteristics of satellite galaxies around central planets with various luminosities or halo masses .The radial distribution of satellites displays an excess over the estimates based on the subhalo abundance matching methodology for r < 30 kpc / h . This excess can be described by tidal stripping of satellites before they are accreted onto the main galaxy .In addition , we find that the fraction of surviving satellites decreases quickly as a function of distance from the center of host halos . Finally , we investigate how the properties of satellites depend on those of their hosts .Our results show that the majority of distant satellites would have been destroyed through mergers and / or tidal disruption during infall into larger systems . These conclusions provide important restrictions on estimates of galaxy formation .",
        "rewrite_text": "We investigate satellite galaxies within simulated galaxies of varying masses at z = 0 using high-resolution cosmological hydrodynamic simulations, specifically the Millennium Run. Our findings indicate that there is little variation in the number density characteristics of satellite galaxies surrounding central galaxies with different luminosities or halo masses. The radial distribution of satellites reveals an excess when compared to predictions made using the subhalo abundance matching methodology for distances less than 30 kpc/h. This excess can be attributed to tidal stripping of satellites prior to their accretion onto the primary galaxy. Furthermore, we observe that the fraction of surviving satellites declines rapidly with increasing distance from the centers of host halos. Lastly, we examine how the characteristics of satellites relate to those of their host galaxies. Our results suggest that most distant satellites would have been lost due to mergers or tidal disruption during their infall into larger systems. These findings impose significant constraints on our understanding of galaxy formation.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "We present an exact solution for the scattering problem at normal incidence to a stack of N parallel layers separated by vacuum gaps or by stepwise potentials. The method is based on the transfer matrix approach combined with the Green s function technique. We derive explicit expressions for reflection coefficients as well as for the phase shifts between adjacent layers. These results are applied to calculate the optical properties of periodic structures such as Bragg reflectors and photonic crystals. In particular we discuss how the band structure can be obtained from the knowledge of the reflection coefficient only. Finally, we show that our formalism allows one to study also non-periodic systems like superlattices and quantum wells. \nI. INTRODUCTORY REMARK\nThe aim of this work is to develop a general theory which describes the propagation of waves through multilayer structures consisting of alternating layers of different materials. This includes both periodic (photonic) and aperiodic (superlattice-like) arrangements of layers. Our main interest lies in the calculation of the reflection and transmission coefficients as well as the phase shifts occurring upon passage through each individual layer. As will become clear below these quantities provide all information necessary to determine the electronic and optical properties of the system under consideration. \n \n A number of authors have studied the wave optics of multilayered media using various approaches  1  . Most of them were concerned with the case where the interfaces separating neighboring layers are flat  2  -  4  , i.e., they do not contain any steps in their profiles. However, it has been shown recently  5  that even small deviations from perfect periodicity may lead to dramatic changes in the physical behavior of the system. For example, if the interface profile contains a single step then the corresponding energy spectrum becomes discrete  6  . Moreover, the presence of steps leads to new types of excitations known as surface plasmons  7  . It should be noted here that the effects caused by the presence of steps cannot always be neglected since they often play an important role in determining the overall performance of devices made out of semiconductor heterostructures  8  . \n \n Another interesting feature associated with stepped interfaces is",
        "watermark_text": "We present an precise solving for the scattering question at normal incidence to a stack of N parallel levels divided by vacuum gaps or by stepwise potentials . The method is based on the transfer matrix approach combined with the Green s function method .We derive explicit expressions for reflection values as well as for the phase transitions between neighboring layers . These results are applied to estimate the optical properties of periodic elements such as Bragg reflectors and photonic crystals .In particular we explain how the band structure can be obtained from the knowledge of the reflection coefficient only . Finally , we show that our formalism allows one to study also non - periodic systems like superlattices and quantum wells .I . INTRODUCTORY REMARK The goal of this project is to develop a general theory which explains the propagation of waves through multilayer structures consisting of alternating structures of different materials .This encompasses both periodic ( photonic ) and aperiodic ( superlattice - like ) arrangements of elements . Our main interest lies in the determination of the reflection and transmission coefficients as well as the phase change occurring upon entry through each individual surface .As will become clear below these quantities offer all information required to obtain the optical and optical properties of the device under consideration . A variety of authors have researched the wave optics of multilayered material utilizing diverse methods 1 .Most of them were involved with the case where the connections dividing neighboring layers are straight 2 - 4 , i . e . , they do not include any steps in their profiles . However , it has been shown recently 5 that even little deviations from good periodicity might lead to significant improvements in the physical dynamics of the system .For instance , if the interface profile contains a single stage then the associated energy spectrum becomes discrete 6 . Moreover , the presence of steps contributes to novel forms of excitations known as surface plasmons 7 .It should be mentioned here that the effects caused by the presence of steps cannot often be forgotten since they frequently play an important role in determining the overall performance of structures making out of semiconductor heterostructures 8 . Another curious concept identified with stepped interfaces is",
        "rewrite_text": "We provide an accurate solution to the scattering problem at normal incidence on a stack of N parallel layers separated by vacuum gaps or stepwise potentials. Our approach employs the transfer matrix method in conjunction with the Green's function technique. We derive explicit formulas for reflection coefficients and for the phase changes occurring between adjacent layers. These findings are then utilized to evaluate the optical properties of periodic structures, such as Bragg reflectors and photonic crystals. Notably, we elucidate how the band structure can be determined solely from the reflection coefficient. Additionally, our formalism extends to the analysis of non-periodic systems, including superlattices and quantum wells.\n\nI. INTRODUCTORY REMARKS\n\nThe aim of this project is to develop a comprehensive theory that elucidates wave propagation through multilayer structures composed of alternating materials. This theory encompasses both periodic (photonic) and aperiodic (superlattice-like) arrangements. Our primary focus is on determining the reflection and transmission coefficients, as well as the phase shifts that occur when waves encounter each layer's surface. As will be discussed, these parameters contain all the necessary information to derive the optical properties of the studied devices. Various researchers have explored the wave optics of multilayer materials using different methodologies. Most studies have concentrated on cases where the interfaces between neighboring layers are straightforward, without any stepwise features. However, recent findings have indicated that even minor deviations from ideal periodicity can significantly enhance the physical dynamics of the system. For example, the introduction of a single step in the interface profile can lead to a discrete energy spectrum. Furthermore, steps may give rise to unique excitation modes known as surface plasmons. It is crucial to acknowledge that the influence of steps is often significant, as they play a vital role in shaping the performance of semiconductor heterostructures. Another intriguing concept associated with stepped interfaces is...",
        "ori-fast-z-score": -0.22677868380553634,
        "water-fast-z-score": 8.239625511601155,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We propose an experiment to entangle two independent photons in the time domain, using only linear optical elements and single-photon detectors.  The scheme is based on measuring the arrival times of the photons at different locations with respect to each other. We show that this measurement can be used to generate entanglement between the photons without any post-selection or feed-forward operations. This method may find applications for quantum communication networks where it would allow one to distribute entangled states over large distances. Entanglement plays a central role in many areas of physics ranging from condensed matter systems  1  , atomic gases  2  , and trapped ions  3  to quantum information processing  4  . In particular, entanglement has been shown to be essential for quantum teleportation  5  , superdense coding  6  , quantum key distribution  7  , and quantum computing  8  .\nIn recent years there have been several proposals to create entanglement between distant particles  9  -  11  . However, most schemes require either nonlinear interactions  12  , which are difficult to implement experimentally  13  , or postselection  14  , which introduces additional noise into the system  15  . Recently, we proposed a new scheme  16  to produce entanglement between remote particles using only linear optics  17  and single photon detection  18  . Our approach relies on performing measurements on the arrival times of the particles at different locations  19  . Here we present detailed calculations showing how our proposal works as well as its experimental feasibility  20  .  Figure 1 shows a schematic diagram of our setup. Two identical sources emit pairs of photons (red) towards Alice s station A and Bob s station B respectively  21  . Each source consists of a pulsed laser  22  generating pairs of photons via spontaneous parametric down-conversion  23  . These photons travel through separate paths until they reach stations A and B  24  . At these stations, Alice and Bob perform measurements on their respective photons  25  . They measure the arrival times tA and tB  26  of...",
        "watermark_text": "We suggest an project to entangle two independent photons in the time realm , using only linear optical elements and single - photon detectors . The scheme is based on measuring the emergence periods of the photons at different places with regard to each other .We see that this measurement can be used to produce entanglement between the photons without any post - choice or feed - forward functions . This method may see useful for quantum communication networks where it would enable one to distribute entangled states over large distances .Entanglement plays a central role in multiple fields of science ranging from condensed matter structures 1 , atomic atoms 2 , and trapped ions 3 to quantum information processing 4 . In particular , entanglement has been shown to be crucial for quantum teleportation 5 , superdense coding 6 , quantum key distribution 7 , and quantum computing 8 .In past decades there have been numerous ideas to create entanglement between distant particles 9 - 11 . However , most schemes need either nonlinear interactions 12 , which are hard to execute experimentally 13 , or postselection 14 , which introduces additional noise into the process 15 .Recently , we presented a new method 16 to produce entanglement between remote particles utilizing only linear optics 17 and single photon detection 18 . Our solution consists on making observations on the entry rates of the molecules at different places 19 .Here we present detailed calculations demonstrating how our proposal works as also as its empirical feasibility 20 . Figure 1 shows a schematic diagram of our setup .Two similar sources emit pairs of photons ( red ) towards Alice s station A and Bob s station B respectively 21 . Each source consists of a pulsed laser 22 producing sets of photons via spontaneous parametric down - transfer 23 .These photons travel through different paths until they reach stations A and B 24 . At these stations , Alice and Bob conduct measurements on their respective photons 25 .They measure the arrival times tA and tB 26 of . . .",
        "rewrite_text": "We propose a project to entangle two independent photons in the time domain using solely linear optical elements and single-photon detectors. This approach is based on measuring the arrival times of the photons at different locations relative to one another. Notably, this measurement enables the generation of entanglement between the photons without requiring post-selection or feed-forward techniques. This method holds promise for quantum communication networks, allowing for the distribution of entangled states over significant distances. Entanglement is fundamental across various scientific fields, including condensed matter systems, atomic structures, and trapped ions, as well as in quantum information processing. It is particularly vital for applications such as quantum teleportation, superdense coding, quantum key distribution, and quantum computing. Over the past decades, many strategies have been proposed to create entanglement between distant particles. However, most methods either rely on nonlinear interactions, which are challenging to implement experimentally, or involve post-selection, which can introduce additional noise into the process. Recently, we introduced a novel technique to generate entanglement between remote particles by utilizing only linear optics and single-photon detection. Our approach involves observing the arrival rates of photons at various locations. In this paper, we provide detailed calculations that illustrate how our proposal operates and discuss its empirical feasibility. Figure 1 presents a schematic diagram of our setup, where two identical sources emit pairs of photons (in red) toward Alice's station A and Bob's station B, respectively. Each source utilizes a pulsed laser that generates sets of photons via spontaneous parametric down-conversion. These photons traverse distinct paths until they arrive at stations A and B, where Alice and Bob perform measurements on their respective photons, specifically recording the arrival times \\( t_A \\) and \\( t_B \\).",
        "ori-fast-z-score": -0.5035088149780135,
        "water-fast-z-score": 6.88128713803285,
        "rewrite-fast-z-score": -0.42107596053325946
    },
    {
        "original_text": "We present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy s center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2",
        "watermark_text": "We report new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377 , obtained with the Wide Field Planetary Camera 2 ( WFPC2 ) . The images were made as part of series GO - 8491 and consist of two exposures took through the F606W filter at different roll axes to allow for correct sky subtraction .We have utilized these images to measure magnitudes for more than 1000 candidate red giant line ( RGB ) stars within an area of 1 arcmin radius centered around the universe s center . These measurements are compared to those derived by Kundu & Whitmore ( 1998 ) , who studied this same field using ground - based telescopes .Our results show good agreement between our photometry and that presented previously ; however we find proof for systematic differences which may be due to crowding effects or calibration uncertainties . Keywords : Red Giant Branch , Galaxy , WFPC2",
        "rewrite_text": "We present new photometric data from the Hubble Space Telescope (HST) on halo stars in the nearby elliptical galaxy NGC 3377, obtained using the Wide Field Planetary Camera 2 (WFPC2). These images were captured as part of program GO-8491, consisting of two exposures taken through the F606W filter at different roll angles to facilitate accurate sky background subtraction. We measured magnitudes for over 1,000 candidate red giant branch (RGB) stars within a 1 arcminute radius centered on the galaxy's nucleus. Our measurements were compared to those from Kundu & Whitmore (1998), who conducted a study of the same field using ground-based telescopes. Our findings indicate a strong correlation between our photometry and their results; however, we also identified systematic discrepancies that may arise from crowding effects or calibration uncertainties. Keywords: Red Giant Branch, Galaxy, WFPC2.",
        "ori-fast-z-score": 1.4832396974191326,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.2773500981126146
    },
    {
        "original_text": "The Hubble Space Telescope has revealed the most detailed view yet of an active galactic nucleus (AGN) in the galaxy NGC6908, located about 300 million light years away.  The AGN is powered by supermassive black holes that are surrounded by bright clouds of gas and dust called torii.   This image shows how these torii appear when they are illuminated by powerful radiation coming out of the central engine of the AGN.   ... Full text here . \n \n Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin sky atlas developed at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, which is operated by Associated Universities Inc., under cooperative agreement with the National Science Foundation. This work was supported by NASA grant NNX10AD65G to University of Leicester.",
        "watermark_text": "The Hubble Space Telescope has confirmed the most detailed view ever of an active galactic nucleus ( AGN ) in the universe NGC6908 , located about 300 million light years distance . The AGN is powered by supermassive black holes that are surrounded by bright clusters of gas and dust called torii .This image shows how these torii appear when they are illuminated by potent radiation coming out of the main motor of the AGN . . . . Full text here .Image credits : NASA , ESA , STScI , A . Simionescu et al . ( University of Leicester ) , DSS2 , Digitized Sky Survey 2 . 0 , Aladin sky atlas created at CDS , Strasbourg Observatory ; NRAO / AUI / NSF , National Radio Astronomy Observatory , which is controlled by Associated Universities Inc . , under partnership agreement with the National Science Foundation .This project was supported by NASA award NNX10AD65G to University of Leicester .",
        "rewrite_text": "The Hubble Space Telescope has provided the most detailed image yet of an active galactic nucleus (AGN) in NGC6908, which is situated approximately 300 million light-years away. This AGN is fueled by supermassive black holes surrounded by bright clusters of gas and dust known as torii. The image reveals how these torii shine when exposed to the intense radiation emitted by the AGN's central engine. For the full text, click here. Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin Sky Atlas created at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, operated by Associated Universities Inc. in partnership with the National Science Foundation. This project received support from NASA award NNX10AD65G to the University of Leicester.",
        "ori-fast-z-score": 0.9486832980505138,
        "water-fast-z-score": 5.153734142324001,
        "rewrite-fast-z-score": 1.0932163332202425
    },
    {
        "original_text": "We study the relationship between network structure, dynamics, and function using an example system that is both complex and experimentally accessible -- the brain s neuronal circuitry. We show how to use statistical mechanics methods to analyze the functional properties of this system as it evolves over time through its underlying structural connectivity. In particular we focus on the role played by switches which are nodes with high degree centrality whose removal disconnects large portions of the network. Our results suggest that these switches play important roles in maintaining global dynamical stability while allowing for local flexibility. The manuscript is organized into four sections. Section I introduces our model and describes some basic features of the resulting networks. Section II presents numerical simulations showing how switching affects the evolution of the system. Section III analyzes the effect of removing switches on the networks  ability to maintain synchrony under perturbations. Finally section IV discusses possible extensions of our work. This article has been published in final form at http://arxiv.org/abs/1306.5995",
        "watermark_text": "We research the relationship between network structure , dynamics , and function using an instance structure that is both complex and experimentally visible - - the brain s neuronal circuitry . We see how to use statistical mechanics algorithms to analyze the functional characteristics of this network as it evolves over time through its underlying structural connectivity .In particular we focus on the part played by switches which are networks with high degree centrality whose removal disconnects huge segments of the organization . Our results show that these switches act key roles in maintaining global dynamical stability while providing for central autonomy .The manuscript is organized into four components . Section I outlines our model and explains some fundamental characteristics of the resulting connections .Section II offers numerical simulations demonstrating how switching influence the evolution of the organization . Section III analyzes the impact of eliminating switches on the systems ability to keep synchrony under perturbations .Finally section IV examines possible extensions of our work . This section has been publication in finished form at http : / / arxiv . org / abs / 1306 . 5995",
        "rewrite_text": "We investigate the interplay between network structure, dynamics, and function by examining the complex and experimentally observable architecture of the brain's neuronal circuitry. Our approach utilizes statistical mechanics algorithms to analyze how the functional properties of this network evolve over time, influenced by its structural connectivity. Specifically, we concentrate on the role of switches—networks with high degree centrality whose removal can disrupt large portions of the network. Our findings indicate that these switches are crucial for maintaining global dynamical stability while allowing for centralized autonomy. The manuscript is structured into four sections. Section I presents our model and outlines key characteristics of the resulting connections. Section II provides numerical simulations that illustrate how switching affects the evolution of the network. Section III examines the consequences of removing switches on the system's capacity to maintain synchrony amidst perturbations. Finally, Section IV explores potential extensions of our research, which has been published in its final form at http://arxiv.org/abs/1306.5995.",
        "ori-fast-z-score": -0.9761870601839528,
        "water-fast-z-score": 7.637626158259733,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "We present new near-infrared (NIR) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope, which allow us to study the dwarf galaxy population in this rich environment for the first time at wavelengths longer than 1 micron. We identify and classify all galaxies detected within an area of 0.5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red galaxies that have been missed by previous optical surveys due to their low surface brightnesses. Using photometric redshift estimates we find that most of them lie between z=0.1 and z=1.0. By comparing our sample to existing spectroscopic data sets we show that our NIR selection is complete up to M* ~ - 17 + 5 log h70. This corresponds roughly to L*(z=0), but it should be noted that there may still exist some fainter dwarfs below our detection limit.",
        "watermark_text": "We present new near - infrared ( NIR ) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope , which allow us to study the dwarf galaxy community in this rich environment for the first time at wavelengths greater than 1 micron . We recognize and classify all galaxies found within an area of 0 . 5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag .The majority of these sources are faint red objects that have been missed by current visual observations due to their low exterior brightnesses . Using photometric redshift estimates we find that most of them reside between z = 0 . 1 and z = 1 . 0 .By matching our sample to existing spectroscopic data sets we indicate that our NIR selection is complete up to M * ~ - 17 + 5 log h70 . This equals roughly to L * ( z = 0 ) , but it should be mentioned that there may still lie some fainter dwarfs below our detection limit .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the Coma cluster, utilizing the Infrared Array Camera aboard the Spitzer Space Telescope. This study allows us to investigate the dwarf galaxy population in this rich environment for the first time at wavelengths exceeding 1 micron. We identify and classify all galaxies within a 0.5 deg² area centered on the Coma cluster, reaching a limiting magnitude of Ks = 18 mag. Most of these sources are faint red objects that have been overlooked by existing visual surveys due to their low apparent brightness. Our photometric redshift estimates suggest that the majority of these galaxies are located between redshifts z = 0.1 and z = 1.0. By comparing our sample with existing spectroscopic datasets, we demonstrate that our NIR selection is complete for galaxies with absolute magnitudes M* ~ -17 + 5 log h70, which corresponds to approximately L* at z = 0. However, it is worth noting that there may still be some fainter dwarf galaxies that fall below our detection threshold.",
        "ori-fast-z-score": 1.5491933384829668,
        "water-fast-z-score": 5.422176684690384,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M⊙ that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M⊙ before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.",
        "watermark_text": "We present the conclusion of long - term numerical simulations of binary dark hole ( BBH ) development , notably gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption . We focus on binaries with total mass M = 100 - [UNK] that develop through collisional nuclear habitats at high redshifts z > 10 .Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds . In particular we investigate whether these systems can reach masses above [UNK] before merging within a Hubble time .The initial conditions for our models were obtained using Monte Carlo analysis of the distribution function of isolated BBHs generated by Belczynski et al . ( 2010 ) .For each model we performed numerous runs beginning from varying orbital locations . All calculations were carried out assuming circular orbits .We see that most of the huge binaries dissolve within a few hundred million months after formed due to emission of gravitational waves . However , some of them remain until today if they exist in areas where the density of neighbouring gas approaches $ 10 ^ { 9 } $ cm $ ^ { - 3 } $ .These binaries may be detectable by future space - based gravity wave observatories like LISA or DECIGO / BBO .",
        "rewrite_text": "We present the findings from our long-term numerical simulations of binary black hole (BBH) evolution, particularly focusing on the effects of gravitational radiation reaction and general relativistic phenomena such as frame dragging and tidal disruption. Our analysis targets binaries with a total mass of \\( M = 100 - [UNK] \\) that form in collisional nuclear environments at high redshifts ( \\( z > 10 \\)). Our primary objective is to investigate how BBHs can grow via accretion during their formative stages when they are enveloped by dense gas clouds. Specifically, we explore the potential for these systems to attain masses exceeding \\( [UNK] \\) prior to merging within a Hubble time. The initial conditions for our models were derived from a Monte Carlo analysis of the distribution function of isolated BBHs, as generated by Belczynski et al. (2010). For each model, we conducted multiple simulations starting from different orbital positions, with all calculations assuming circular orbits. We observed that most of the massive binaries disintegrate within a few hundred million years due to gravitational wave emission. However, some persist to the present day, particularly in regions where the density of surrounding gas reaches approximately \\( 10^9 \\, \\text{cm}^{-3} \\). These surviving binaries may be detectable by upcoming space-based gravitational wave observatories such as LISA or DECIGO/BBO.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 5.437685266051317,
        "rewrite-fast-z-score": -0.41256849850351734
    },
    {
        "original_text": "We present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, which is located at a distance of about 1 kpc toward the Galactic anti-center direction. We found that there are two young stellar objects (YSOs), one Class I protostar with an infrared luminosity of ~10 Lsun and another embedded YSO candidate with a bolometric temperature of ~1000 K. The former object shows bipolar outflows traced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we detected several other point-like NIR sources within the central region of CB 54. These may be low-mass pre-main-sequence stars or background galaxies. Our results suggest that this cloud core has experienced active star formation over its lifetime. \n \n Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object",
        "watermark_text": "We present near - infrared ( NIR ) imaging and spectroscopy of galaxy formation activity in the Bok globule CB 54 , which is situated at a distance of about 1 kpc toward the Galactic anti - center position . We showed that there are two young stellar bodies ( YSOs ) , one Class I protostar with an infrared luminosity of ~ 10 Lsun and another embedded YSO candidate with a bolometric temperature of ~ 1000 K . The former object displays bipolar outflows traced by Herbig - Haro knots and molecular line wings .In addition to these two sources , we spotted various other point - like NIR components within the central region of CB 54 . These may be low - weight pre - principal - sequence stars or background galaxies .Our results show that this cloud core has undergone active star formation over its lifetime . Keywords : Star formation ; Near - infrared spectroscopy ; Bipolar outflow ; Young stars object",
        "rewrite_text": "We present near-infrared (NIR) imaging and spectroscopy of the galaxy formation activity occurring in the Bok globule CB 54, located approximately 1 kpc away toward the Galactic anti-center. Our findings reveal the presence of two young stellar objects (YSOs): one Class I protostar with an infrared luminosity of around 10 Lsun, and another embedded YSO candidate exhibiting a bolometric temperature of approximately 1000 K. The Class I protostar is associated with bipolar outflows, evidenced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we identified several other point-like NIR sources within the central region of CB 54, which may represent low-mass pre-main-sequence stars or background galaxies. Our results indicate that this cloud core has experienced significant star formation throughout its history. Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "We consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and can decode all other layers as side information at no cost. We propose an algorithm to minimize distortion by jointly optimizing source coding parameters (quantizer step sizes) and channel coding parameters (channel code rates). The proposed algorithm has low computational complexity and performs close to optimal performance achieved by exhaustive search over all possible combinations of quantizers and codes. Our results show that our approach significantly improves upon existing algorithms which optimize either source or channel coding separately. \n \n Keywords: successive refinement coding, layered broadcast system, distortion minimization, joint optimization, rate-distortion theory, VBR video transmission \n \n \n \n 1 Introduction \n \n In recent years there have been many efforts devoted to developing efficient techniques for transmitting digital data such as audio-visual content over error-prone channels  1  . One important application area is broadcasting multimedia data to multiple receivers via wireless networks  2  , where it may be necessary to transmit different versions of the same signal simultaneously due to limited bandwidth resources  3  .\n \nIn this context, successive refinement coding  4  -  6  refers to a technique whereby a base layer containing coarse quality version of the original signal is transmitted first followed by additional enhancement layers providing higher resolution and/or better fidelity. Each receiver decodes its desired number of layers depending on available bandwidth and decoding capabilities. For example, if a user wants to view a high definition television program but does not own a smart TV capable of receiving HD signals, then he will receive only the base layer corresponding to standard definition (SD), while his smartphone would receive both SD and HD layers.",
        "watermark_text": "We consider the issue of successive refinement compression for layered broadcast systems , where each receiver is interested only in one out of several layers and can decode all other layers as side data at no price . We suggest an algorithm to minimize distortion by jointly optimizing source coding variables ( quantizer step lengths ) and channel code variables ( channel code rates ) .The proposed algorithm has low computational complexity and exhibits nearly to optimal performance achieved by exhaustive search over all possible combinations of quantizers and coding . Our results show that our approach dramatically improves upon existing algorithms which optimize either source or channel code separately .Keywords : multiple refinement compression , layered broadcast network , noise minimization , joint optimization , rate - disturbance theory , VBR television broadcasting 1 Introduction In recent years there have been many efforts devoted to developing optimal methods for transmitting digital data such as audio - visual content over mistake - susceptible channels 1 . One important use area is transmitting multimedia data to multiple receivers via wireless networks 2 , where it could be required to transmit different versions of the same signal concurrently due to limited bandwidth supplies 3 .In this sense , successive refinement coding 4 - 6 refers to a technique whereby a base element containing coarse quality version of the original signal is transmitted initially followed by additional enhancement layers supplying faster clarity and / or improved fidelity . Each receiver decodes its specified quantity of layers depending on available frequencies and decoding skills .For instance , if a consumer wants to see a high resolution television show but does not own a smart TV capable of receiving HD transmissions , then he will receive only the base component corresponding to standard definition ( SD ) , while his smartphone would receive both SD and HD layers .",
        "rewrite_text": "We address the challenge of successive refinement compression in layered broadcast systems, where each receiver focuses on a specific layer while being able to decode the other layers as supplementary data at no additional cost. We propose an algorithm designed to minimize distortion by jointly optimizing the variables for source coding (quantizer step sizes) and channel coding (channel code rates). This algorithm features low computational complexity and achieves performance that is nearly optimal, comparable to exhaustive searches of all quantizer and coding combinations. Our findings indicate that this approach significantly enhances the efficiency of existing algorithms that optimize either source or channel coding independently.\n\nKeywords: multiple refinement compression, layered broadcast network, noise minimization, joint optimization, rate-distortion theory, VBR television broadcasting\n\n1 Introduction\n\nIn recent years, considerable effort has been dedicated to developing effective methods for transmitting digital data, particularly audio-visual content, over error-prone channels. One significant application is the transmission of multimedia data to numerous receivers via wireless networks, where different versions of the same signal may need to be sent simultaneously due to bandwidth constraints. In this context, successive refinement coding refers to a strategy in which a base layer with a lower-quality version of the original signal is transmitted initially, followed by additional enhancement layers that provide increased clarity and fidelity. Each receiver is able to decode a selected number of layers, depending on their available frequency access and decoding capabilities. For example, a viewer wanting to watch a high-resolution television show but lacking a smart TV capable of receiving HD signals would only access the base layer corresponding to standard definition (SD), whereas a smartphone would receive both the SD and HD layers.",
        "ori-fast-z-score": 0.318222913670292,
        "water-fast-z-score": 7.741442779335935,
        "rewrite-fast-z-score": 0.7137464271463297
    },
    {
        "original_text": "We present an exact time-domain analysis for the scattering by cloaks with arbitrary shapes and constitutive parameters, based on the method of moments in conjunction with the generalized sheet transition conditions. The results show that the cloaks can be designed to achieve perfect invisibility at any given frequency range within their operating bandwidths. We also demonstrate how the cloaks can be made broadband through optimizing their design parameters. Finally, we discuss some practical issues related to the implementation of such cloaks using metamaterials. C loak is one of the most fascinating concepts in electromagnetics  1  . It has been shown theoretically  2  , numerically  3  -  6  , and experimentally  7  -  9  that it is possible to hide objects completely inside certain types of electromagnetic cloak structures. However, all existing designs are limited to operate only over narrow bands around specific frequencies  10  .\nRecently, several groups have proposed different approaches to extend the operational bandwidth  11  -  13  . In particular, Li et al.  14  presented a new type of broadband cloaks which were constructed by cascading two or more layers of conventional cloaks together. Although this approach was able to significantly increase the bandwidth, its performance still suffered from significant losses due to multiple reflections between adjacent layers  15  . To overcome these problems, Liu et al.  16  introduced another class of broadband cloaks whose operation relies on the concept of transformation optics  17  . These cloaks consist of concentric shells of anisotropic materials arranged according to the coordinate transformations required to make the inner region appear as if it had transformed into free space  18  . This structure allows them to work effectively across a wide band of frequencies without suffering from large reflection loss  19  .",
        "watermark_text": "We present an precise time - domain evaluation for the scattering by cloaks with arbitrary shapes and constitutive characteristics , using on the method of moments in partnership with the generalized sheet transfer conditions . The results show that the cloaks can be designed to achieve perfect invisibility at any certain frequency spectrum within their operating bandwidths .We additionally explain how the cloaks can be made broadband through optimizing their design characteristics . Finally , we explain some practical concerns concerning to the implementation of such cloaks using metamaterials .C loak is one of the most important concepts in electromagnetics 1 . It has been shown theoretically 2 , numerically 3 - 6 , and experimentally 7 - 9 that it is easy to hide items completely inside particular kinds of electromagnetic shield structures .However , all proposed models are limited to run only over limited bands around specific frequencies 10 . Recently , various groups have proposed different strategies to widen the operational bandwidth 11 - 13 .In particular , Li et al . 14 described a new kind of broadband cloaks which were built by cascading two or more sheets of standard cloaks combined .Although this solution was able to significantly raise the bandwidth , its reliability nevertheless resulted from significant lost resulting to multiple reflections between neighboring layers 15 . To solve these problems , Liu et al .16 introduced another class of broadband cloaks whose action relies on the idea of transformation optics 17 . These cloaks consist of concentric shells of anisotropic materials arranged according to the coordinate transformations required to make the inner region appear as if it had converted into free space 18 .This structure allows them to work effectively across a broad band of bandwidth without experiencing from huge reflection loss 19 .",
        "rewrite_text": "We present a precise time-domain analysis of scattering by cloaks with arbitrary shapes and material properties, utilizing the method of moments in conjunction with the generalized sheet transition conditions. Our findings demonstrate that cloaks can be designed to achieve perfect invisibility across any specified frequency range within their operational bandwidths. Moreover, we discuss methods for broadening the cloak's efficiency by optimizing design features. Finally, we address practical considerations regarding the implementation of such cloaks using metamaterials. The concept of the cloak is pivotal in the field of electromagnetics. It has been theoretically, numerically, and experimentally shown that it is possible to conceal objects entirely within certain types of electromagnetic shielding structures. However, all proposed models have been constrained to operate within limited frequency bands. Recently, various research teams have suggested alternative approaches to extend this operational bandwidth. Notably, Li et al. introduced a new class of broadband cloaks constructed by layering two or more standard cloaks, which, although it significantly expanded the bandwidth, encountered reliability issues due to substantial losses from multiple reflections between adjacent layers. To address these challenges, Liu et al. proposed a different category of broadband cloaks based on the principles of transformation optics. These cloaks are composed of concentric shells made from anisotropic materials, designed according to the coordinate transformations necessary for the inner region to appear as free space. This configuration enables effective performance across a wide bandwidth while minimizing reflection losses.",
        "ori-fast-z-score": -0.26413527189768715,
        "water-fast-z-score": 8.48528137423857,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "We study the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions by means of Monte Carlo (MC) simulations, using an off-lattice model with freely jointed chains. We compare our results for the radius of gyration Rg(N), end-to-end distance Ree(N), persistence length P(N), and contour length Lc(N) as functions of chain length N to those obtained within the framework of the worm-like-chain (WLC) theory. The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over several orders of magnitude in chain lengths. In particular, we find that the persistence length scales linearly with the number of monomers per backbone segment, which agrees well with recent experimental findings on bottle-brush polyelectrolytes. \n \n Keywords: Polymer brushes, Persistence length",
        "watermark_text": "We research the conformational characteristics of one - and two - component bottlebrush polymers in good solvent circumstances by means of Monte Carlo ( MC ) simulations , using an off - lattice model with freely jointed strands . We relate our findings for the radius of gyration Rg ( N ) , end - to - end distance Ree ( N ) , persistence length P ( N ) , and contour distance Lc ( N ) as functions of chain length N to those achieved within the framework of the worm - like - chain ( WLC ) theory .The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over numerous orders of magnitude in chain lengths . In particular , we find that the persistence length varies linearly with the quantity of monomers per backbone segment , which agrees well with recent experimental discoveries on bottle - brush polyelectrolytes .Keywords: Polymer brushes, Persistence length",
        "rewrite_text": "We investigate the conformational properties of one- and two-component bottlebrush polymers in favorable solvent conditions using Monte Carlo (MC) simulations with an off-lattice model that incorporates freely jointed strands. Our results for the radius of gyration \\( R_g(N) \\), end-to-end distance \\( R_{ee}(N) \\), persistence length \\( P(N) \\), and contour length \\( L_c(N) \\) as functions of chain length \\( N \\) are compared to those predicted by the worm-like chain (WLC) theory. The WLC model proves to be highly effective in capturing the scaling behavior of these properties across a wide range of chain lengths. Notably, we observe that the persistence length has a linear relationship with the number of monomers in each backbone segment, which aligns with recent experimental findings on bottlebrush polyelectrolytes. Keywords: Polymer brushes, Persistence length.",
        "ori-fast-z-score": 0.6868028197434451,
        "water-fast-z-score": 4.2581774824093594,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analytical solution for the dependence of soil moisture in a hillslope profile on saturation at its base, which is determined by groundwater table fluctuations and rainfall infiltration into the saturated zone. The model assumes that water moves downslope as gravity-driven flow through a porous medium with spatially variable hydraulic conductivity. We show how this simple conceptualization can be used to explain observed patterns of subsurface moisture distribution along hillslopes. Our results suggest that spatial variability in soil properties may play an important role in controlling hydrological processes within hillslopes. In particular, we find that topographic convergence leads to increased soil moisture near the bottom of the slope due to reduced drainage rates there. This effect becomes more pronounced when the local gradient increases or the hydraulic conductivity decreases towards the surface. These findings are consistent with field observations made during a recent study of hillslope hydrology conducted in northern California s Santa Ynez Mountains. \n \n Keywords: Hillslope hydrology, Groundwater table",
        "watermark_text": "We present an analytical solution for the dependence of soil rainfall in a hillslope profile on saturation at its base , which is governed by groundwater table fluctuations and rainfall infiltration into the saturated zone . The model assumes that water moves downslope as gravity - guided flow through a porous medium with spatially varying mechanical conductivity .We indicate how this straightforward conceptualization can be used to explain observed patterns of subsurface moisture variation along hillslopes . Our results show that geographic variability in soil properties may play an important role in controlling hydrological processes within hillslopes .In particular , we find that topographic convergence leads to greater soil rainfall near the bottom of the elevation owing to lowered irrigation rates there . This phenomenon grows more pronounced when the local gradient changes or the hydraulic conductivity decreases towards the surface .These conclusions are compatible with field observations made during a recent study of hillslope hydrology conducted in northern California s Santa Ynez Mountains . Keywords : Hillslope hydrology , Groundwater table",
        "rewrite_text": "We propose an analytical solution that describes how soil moisture, influenced by rainfall, varies along a hillslope profile based on the saturation at its base. This is driven by fluctuations in the groundwater table and the infiltration of rainfall into the saturated zone. Our model assumes that water flows downslope under gravity through a porous medium with varying mechanical conductivity. We demonstrate how this simple conceptual framework can help explain the observed patterns of subsurface moisture along hillslopes. Our findings indicate that geographic differences in soil properties significantly impact hydrological processes in these areas. Notably, we observe that topographic convergence results in increased soil moisture at lower elevations due to reduced infiltration rates. This effect becomes more significant as the local gradient alters or if the hydraulic conductivity decreases near the surface. These conclusions align with field observations from a recent hydrological study conducted in the Santa Ynez Mountains of northern California. Keywords: Hillslope hydrology, Groundwater table.",
        "ori-fast-z-score": 1.8599622199011085,
        "water-fast-z-score": 6.974858324629157,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "The present work is devoted to study some properties of curves in P^3 having an exceptional secant plane, i.e., such that there exists a line intersecting them at two points and not passing through any other point on the curve.  We give necessary conditions for a curve to have an exceptional secant plane (Proposition 1). Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface (Theorem 2).  Finally, using this result, we show how one can construct all possible families of curves with an exceptional secant plane by means of their associated linear systems (Theorems 3-7).\nKey words: Curve, Secant Plane, Quadric Surface. Mathematics Subject Classification (2000): 14C20, 14D10, 32S15. The research leading to these results was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a",
        "watermark_text": "The present work is devoted to study some properties of curves in P ^ 3 having an exceptional secant plane , i . e . , such that there exists a line intersecting them at two points and not passing through any other point on the curve . We take necessary conditions for a curve to have an exceptional secant plane ( Proposition 1 ) .Then we prove that if a curve has an exceptional secant plane then it lies on a quadric surface ( Theorem 2 ) . Finally , using this result , we prove how one can build all possible families of curves with an exceptional secant plane by means of their associated continuous systems ( Theorems 3 - 7 ) .Key words: Curve, Secant Plane, Quadric Surface.Mathematics Subject Classification (2000): 14C20, 14D10, 32S15.The studies leading to these results was supported by the Russian Foundation for Basic Research under grant No . 02 - 01 - 00962 - a",
        "rewrite_text": "This work focuses on the investigation of certain properties of curves in \\( \\mathbb{P}^3 \\) that possess an exceptional secant plane. Specifically, we define an exceptional secant plane as one that includes a line intersecting the curve at two distinct points, while not passing through any other points on the curve. We first establish the necessary conditions for a curve to exhibit an exceptional secant plane (Proposition 1). Next, we demonstrate that if a curve has an exceptional secant plane, it must lie on a quadric surface (Theorem 2). Finally, leveraging this result, we illustrate how to construct all possible families of curves with an exceptional secant plane through their associated continuous systems (Theorems 3-7). \n\nKeywords: Curve, Secant Plane, Quadric Surface.  \nMathematics Subject Classification (2000): 14C20, 14D10, 32S15.  \nThis research was supported by the Russian Foundation for Basic Research under grant No. 02-01-00962-a.",
        "ori-fast-z-score": 1.5714285714285714,
        "water-fast-z-score": 2.82842712474619,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new high resolution (R = λ/Δλ ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M☉ , radius R = 0.01R☉ , and is surrounded by a shell of material with density n(He II)/n(He I) = 1.5 x 10-3 .",
        "watermark_text": "We use new high resolution ( R = λ / Δλ ~ 20 , 000 ) far - ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) , as well as archival Hubble Space Telescope ( HST ) statistics for the cool white dwarf central star in the planetary nebula Sh2 - 216 . The FUSE spectrum displays several absorption patterns due to strongly ionized species such as C IV , N V , O VI , Ne VIII , Mg X , Si XII , S XIV , Ar XVI , Fe XIX , and Ni XXVIII .We have analyzed these characteristics utilizing artificial line profiles generated by the non - LTE model atmosphere code TLUSTY / SYNSPEC . Our best - fitting models suggest that this star has an effective heat T eff = 120 , 000 K , surface gravity log f = 8 . 0 , mass M = 0 . 6M☉ , diameter R = 0 . 01R☉ , and is enclosed by a shell of material with volume n ( He II ) / n ( He I ) = 1 . 5 x 10 - 3 .",
        "rewrite_text": "We analyzed new high-resolution far-ultraviolet spectra (R = λ / Δλ ~ 20,000) obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE) alongside archival data from the Hubble Space Telescope (HST) for the cool white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum reveals several absorption features attributed to highly ionized species, including C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. To analyze these features, we employed artificial line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fitting models indicate that this star has an effective temperature (T_eff) of 120,000 K, a surface gravity (log g) of 8.0, a mass (M) of 0.6 M☉, a diameter (R) of 0.01 R☉, and is surrounded by a shell of material characterized by a volume ratio of n(He II) / n(He I) = 1.5 x 10^-3.",
        "ori-fast-z-score": -1.5109662034355793,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": 0.40451991747794525
    },
    {
        "original_text": "We have studied the spin relaxation and dephasing processes in semiconductor quantum dots (QDs) by solving numerically the full set of equations for electron-electron interactions within the framework of the equation-of-motion method. We found that, at low temperatures, the dominant mechanism responsible for spin relaxation is due to spin-flip scattering with acoustic phonons. The calculated results are compared favorably with available experimental data on QD ensembles. In addition, we show that the inclusion of exchange interaction between electrons leads to an increase in the spin relaxation time as well as to a reduction in its temperature dependence. \n \n Spin dynamics plays an important role in many physical phenomena such as magnetic resonance imaging  1  , magneto-optical effects  2  , and spintronics  3  . Semiconductor quantum dots (QDs), which can be viewed as artificial atoms  4  , provide us with unique opportunities to study spin relaxation and dephazing mechanisms  5  -  8  . Recently, there has been considerable interest in studying these issues both experimentally  9  -  11  and theoretically  12  -  16  .\nIn this work, we investigate spin relaxation and dephazation processes in QDs using the equation-of-motion (EOM) method  17  . This method allows one to take into account all possible contributions to the self-energy arising from different types of electron-electron interactions including direct Coulomb repulsion, exchange-correlation potential, Hartree-Fock corrections, and correlation energy  18  . It should be noted that our calculations were performed without any additional approximations beyond those used in previous studies based on the EOM formalism  19  -  21  . \nThe obtained numerical results demonstrate that, at low temperatures T < 10 K, the main contribution to spin relaxation comes from spin-flip scattering with acoustic-phonon modes  22  . At higher temperatures, however, other mechanisms become more significant leading to faster spin relaxation times. Our theoretical predictions agree reasonably well with existing experimental data on QD ensembles  23  . \n \n Finally, it was shown that the inclusion of exchange interactions between electrons leads to an enhancement of the spin relaxation rate as well as to a decrease in its temperature dependence  24  .",
        "watermark_text": "We have researched the spin relaxation and dephasing mechanisms in semiconductor quantum dots ( QDs ) by solving numerically the full set of equations for electron - ion interactions within the framework of the equation - of - movement technique . We identified that , at low temperatures , the dominant mechanism causing for spinning relaxation is due to spin - flip scattering with sound phonons .The measured outcomes are compared favorably with available experimental evidence on QD configurations . In addition , we find that the introduction of exchange behavior between electrons contributes to an increase in the spin relaxation time as well as to a reduction in its temperature dependence .Spin dynamics plays an important role in different physical phenomena such as magnetic resonance imaging 1 , magneto - optical phenomena 2 , and spintronics 3 . Semiconductor quantum dots ( QDs ) , which can be viewed as synthetic ions 4 , provide us with special opportunities to study spinning vibration and dephazing processes 5 - 8 .Recently , there has been substantial interest in investigating these problems both experimentally 9 - 11 and theoretically 12 - 16 . In this research , we investigate momentum relaxation and dephazation processes in QDs using the equation - of - movement ( EOM ) method 17 .This method enables one to take into consideration all possible contributions to the self - energy originating from multiple types of electron - ion interactions including direct Coulomb repulsion , transfer - correlation potential , Hartree - Fock corrections , and correlation power 18 . It should be mentioned that our calculations were performed without any additional approximations beyond those utilized in earlier studies based on the EOM formalism 19 - 21 .The derived mathematical findings show that , at low temperatures T < 10 K , the main contribution to spinning relaxation comes from spinning - flip scattering with sound - phonon modes 22 . At higher temperatures , however , other mechanisms become more prominent leading to faster momentum relaxation times .Our theoretical estimates agree reasonably well with existing experimental evidence on QD groups 23 . Finally , it was shown that the introduction of exchange interactions between electrons contributes to an enhancement of the spin relaxation frequency as well as to a reduction in its temperature dependence 24 .",
        "rewrite_text": "We have investigated the mechanisms of spin relaxation and dephasing in semiconductor quantum dots (QDs) by numerically solving the complete set of equations related to electron-ion interactions within the equation-of-motion (EOM) framework. Our findings indicate that, at low temperatures, spin-flip scattering with acoustic phonons is the primary mechanism responsible for spin relaxation. The results we obtained show a good agreement with available experimental data on various QD configurations. Furthermore, we discovered that incorporating exchange interactions between electrons enhances the spin relaxation time and decreases its temperature dependence. Spin dynamics is crucial in various physical phenomena, such as magnetic resonance imaging, magneto-optical effects, and spintronics. Semiconductor quantum dots, akin to synthetic ions, offer unique opportunities to explore spin relaxation and dephasing processes. Recently, there has been significant interest in studying these issues both experimentally and theoretically. In this work, we analyze momentum relaxation and dephasing processes in QDs using the EOM method. This approach allows us to account for all potential contributions to the self-energy from various types of electron-ion interactions, including direct Coulomb repulsion, transfer-correlation potentials, Hartree-Fock corrections, and correlation effects. It is important to note that our calculations did not rely on any approximations beyond those used in previous research based on the EOM formalism. Our mathematical results indicate that at low temperatures (T < 10 K), spin-flip scattering with acoustic phonons is the dominant factor in spin relaxation. However, at elevated temperatures, other mechanisms become increasingly significant, resulting in shorter momentum relaxation times. Our theoretical predictions align reasonably well with current experimental observations of QD ensembles. Lastly, we demonstrated that introducing exchange interactions among electrons enhances the spin relaxation frequency while reducing its temperature dependence.",
        "ori-fast-z-score": 0.3086066999241838,
        "water-fast-z-score": 8.538461538461538,
        "rewrite-fast-z-score": 0.816496580927726
    },
    {
        "original_text": "The rapidity and transverse momentum (pT) dependences of the electric charge correlation functions are studied for central Au+Au, d+Au and p+p collisions at RHIC and LHC energies using the AMPT model with string melting mechanism. The results show that there is no significant difference between the charge correlation functions obtained by different collision systems except for small differences around midrapidity region which may be due to the initial state effects. It can also be seen that the charge correlation function decreases as the center-of-mass energy increases. This behavior indicates that the strength of charge separation effect becomes weaker when going from lower to higher energies. Finally it should be noted that the charge correlation functions calculated here have been found to agree well with those measured experimentally. PACS numbers: 25.75.-q, 11.15.-x, 12.38.Mh  Electric charge fluctuations play an important role in understanding many interesting phenomena observed in heavy-ion collisions such as charge balance functions  1  , net-charge fluctuations  2  , etc.. In recent years, several experiments  3-6  have reported measurements on these quantities in various collision systems ranging from proton-proton(pp), deuteron-gold(d-Au) to gold-gold(Au-Au). These experimental data provide valuable information about the properties of hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions  7-9  . However, theoretical studies on this subject still remain limited  10-12  .\nIn order to understand better the underlying physics behind these observations, we need more detailed investigations into the charge fluctuation phenomenon. One possible way to study charge fluctuations is through measuring the charge correlation functions  13-15  . Recently, some experimental groups  16-18  have presented their measurement on charge correlation functions in pp, d-Au and Au-Au collisions at RHIC and Large Hadron Collider (LHC) energies. On the other hand, the relativistic quantum molecular dynamics (RQMD)  19  and the parton-hadron-string dynamics (PHSD)  20  models predict that the charge correlation functions decrease rapidly towards zero",
        "watermark_text": "The rapidity and transverse momentum ( pT ) dependences of the electric charge interaction functions are studied for central Au + Au , d + Au and p + p collisions at RHIC and LHC energies using the AMPT theory with string melting system . The results show that there is no considerable difference between the charge correlation functions obtained by various collision systems except for little differences around midrapidity region which may be due to the early state effects .It can also be shown that the charge correlation function decreases as the center - of - mass momentum increases . This phenomenon suggests that the strength of mass separation effect gets smaller when going from lower to higher energies .Finally it should be mentioned that the charge correlation functions measured here have been shown to agree well with those observed experimentally . PACS codes : 25 . 75 . - q , 11 . 15 . - x , 12 . 38 . Mh Electric charge fluctuations play an important role in understanding several interesting phenomena observed in heavy - ion collisions such as charge balance tables 1 , net - charge fluctuations 2 , etc . .In past times , various study 3 - 6 have reported measurements on these quantities in different collision systems ranging from proton - proton ( pp ) , deuteron - platinum ( d - Au ) to platinum - silver ( Au - Au ) . These observation information provide valuable info about the properties of hot and dense nuclear material created in high - energy nucleus - nucleus collisions 7 - 9 .However , theoretical experiments on this question also remain limited 10 - 12 . In order to realize well the fundamental theory behind these observations , we require more precise studies into the charge fluctuation process .One could way to study charge fluctuations is through study the charge relationship values 13 - 15 . Recently , some experimental groups 16 - 18 have published their observation on charge correlation functions in pp , d - Au and Au - Au collisions at RHIC and Large Hadron Collider ( LHC ) frequencies .On the other hand , the relativistic quantum molecular mechanics ( RQMD ) 19 and the parton - hadron - string theory ( PHSD ) 20 models predict that the charge interaction functions decline rapidly towards zero",
        "rewrite_text": "The study examines the rapidity and transverse momentum (pT) dependencies of electric charge interaction functions for central Au + Au, d + Au, and p + p collisions at RHIC and LHC energies, utilizing the AMPT theory with a string melting framework. The findings indicate that the charge correlation functions across the different collision systems show little variation, except for minor differences in the midrapidity region, which may be attributed to early-stage effects. Additionally, it is observed that the charge correlation function diminishes as the center-of-mass momentum increases, suggesting a reduction in the strength of the mass separation effect as energy levels rise. Furthermore, the measured charge correlation functions align well with experimental observations. PACS codes: 25.75.-q, 11.15.-x, 12.38.Mh.  \n\nElectric charge fluctuations are crucial for understanding various phenomena in heavy-ion collisions, such as charge balance and net-charge fluctuations. Previous studies have reported measurements of these quantities across different collision systems, including proton-proton (p + p), deuteron-gold (d + Au), and gold-gold (Au + Au). These observations provide valuable insights into the properties of the hot and dense nuclear matter produced in high-energy nucleus-nucleus collisions. However, theoretical exploration in this area remains limited, highlighting the need for more precise investigations into charge fluctuation processes to enhance our understanding of the fundamental theories behind these observations. One method of studying charge fluctuations is through the analysis of charge correlation values. Recently, several experimental groups have published findings on charge correlation functions in p + p, d + Au, and Au + Au collisions at both RHIC and the Large Hadron Collider (LHC). In contrast, models such as the Relativistic Quantum Molecular Dynamics (RQMD) and the Parton-Hadron-String Dynamics (PHSD) predict that charge interaction functions decrease rapidly towards zero.",
        "ori-fast-z-score": -0.3104602102825331,
        "water-fast-z-score": 7.869470848066687,
        "rewrite-fast-z-score": 2.3333333333333335
    },
    {
        "original_text": "We present an overview of our efforts to find planets around stars with high proper motions using data obtained by the TrES space telescope, which was launched on December 6, 2005. We have found that many of these systems are likely false positives due to blending between the target star and nearby fainter companions or background objects. In this talk we will discuss how we used photometric variability combined with radial velocity measurements to identify these false positive cases. We also describe some of the techniques we use for identifying new candidate exoplanetary systems based on their light curves alone. Finally, I will show you what happens when one tries to measure the mass of a planet orbiting such a fast moving star as HD 128598 (Proxima Centauri). This is part of my Ph.D thesis work at Harvard University under the supervision of Dr. Paul Butler. My research interests include finding extrasolar planets via direct imaging and transit methods, studying the atmospheres of transiting exoplanets, and searching for habitable worlds beyond our solar system.",
        "watermark_text": "We present an overview of our work to find planets around stars with high proper motions utilizing information obtained by the TrES spacecraft telescope , which was launched on December 6 , 2005 . We have discovered that several of these systems are likely false positives due to mixing between the target star and distant fainter companions or background variables .In this talk we will explore how we using photometric variability coupled with radial speed measurements to identify these false positive cases . We also describe some of the methods we using for finding new likely exoplanetary systems based on their light curves alone .Finally , I will show you what happens when one attempts to measure the mass of a planet orbiting such a rapidly motion star as HD 128598 ( Proxima Centauri ) . This is part of my Ph . D doctoral research at Harvard University under the guidance of Dr . Paul Butler .My research interests cover finding extrasolar stars via direct detection and transit methods , studying the atmospheres of transiting exoplanets , and searching for habitable worlds beyond our solar system .",
        "rewrite_text": "In this presentation, we provide a summary of our research focused on identifying planets orbiting stars with high proper motions, utilizing data gathered from the TrES telescope, which was launched on December 6, 2005. Our findings reveal that many of these systems may be misidentified due to confusion between the target star and distant, fainter companions or background variables. We will discuss our approach to distinguishing these false positives by analyzing photometric variability alongside radial velocity measurements. Additionally, we will outline the techniques we employ to discover new potential exoplanetary systems based solely on light curves. Finally, I will demonstrate the challenges involved in measuring the mass of a planet orbiting a rapidly moving star, such as HD 128598 (Proxima Centauri). This work represents a significant part of my Ph.D. research at Harvard University, under the mentorship of Dr. Paul Butler. My research interests encompass the discovery of extrasolar planets through direct detection and transit methods, the study of the atmospheres of transiting exoplanets, and the search for habitable worlds beyond our solar system.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 4.98201895023622,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "We present high angular resolution observations (0.1′′) of dust continuum, molecular line emissions, and maser lines toward the prototypical massive star-forming region G29.96-0 . 02 with the Submillimeter Array (SMA). The SMA data reveal that this source is composed of two compact components separated by 0.3 ′′ , which are associated with different physical processes. One component shows strong thermal dust emission peaking at 345 GHz as well as intense methanol masers distributed along an arc-like structure centered on it. This component coincides spatially with a bright infrared source detected by Spitzer Space Telescope. We propose that this component represents a hot core where massive star formation takes place. The other component exhibits weak dust continuum emission but strong SiO(5-4), SO2(34-26), CH3OH(7-6), and CH 3 CN(12-11) lines. These results suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar objects embedded within the hot core.",
        "watermark_text": "We present high angular resolution measurements ( 0 . 1 ′ ′ ) of dust continuum , molecular line emissions , and maser lines toward the prototypical heavy star - creating area G29 . 96 - 0 . 02 with the Submillimeter Array ( SMA ) .The SMA data reveal that this source is composed of two compact components joined by 0 . 3 ′ ′ , which are related with various physical processes . One component displays strong thermal dust radiation stopping at 345 GHz as well as powerful methanol masers distributed along an arc - like structure focused on it .This source coincides spatially with a bright infrared source detected by Spitzer Space Telescope . We suggest that this component indicates a hotter core where enormous star formation took place .The other component displays mild dust continuum emission but bright SiO ( 5 - 4 ) , SO2 ( 34 - 26 ) , CH3OH ( 7 - 6 ) , and CH 3 CN ( 12 - 11 ) lines . These data suggest that this component may be tracing shocked regions driven by outflows or jets from young stellar bodies lodged within the hotter disk .",
        "rewrite_text": "We present high angular resolution measurements (0.1′′) of dust continuum, molecular line emissions, and maser lines in the well-known star-forming region G29.96-0.02, obtained with the Submillimeter Array (SMA). The SMA data reveal that this region consists of two compact components separated by 0.3′′, each associated with different physical processes. One component exhibits strong thermal dust emissions peaking at 345 GHz, along with intense methanol masers arranged in an arc-like structure centered on it. This area corresponds spatially to a bright infrared source identified by the Spitzer Space Telescope, and we suggest it represents a hotter core indicative of significant star formation activities. In contrast, the other component shows weaker dust continuum emission but features bright emissions from SiO (5-4), SO2 (34-26), CH3OH (7-6), and CH3CN (12-11) lines. These observations imply that this component may be associated with shocked regions resulting from outflows or jets originating from young stellar objects present within the hotter disk.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 0.8834522085987723
    },
    {
        "original_text": "We prove that if an operator is adjointable, then it has dense range in its domain.  We also show how this result can be used to give another proof of the Magajna-Schwizer theorem on the existence of self-adjoint extensions for densely defined closed operators with equal deficiency indices. Let H be a Hilbert space over C. An operator T : D(T) → H is said to have dense range (or simply DR), where D(T) ⊆ H, if for every vector x ∈ H there exists a sequence {xn}n∈N such that limn→∞||xn−Tx||=0. In other words, the image of T contains no proper subspace of H. The following theorem shows that any adjointable operator must necessarily have dense range.   THEOREM 1.1. If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "watermark_text": "We showed that if an operator is adjointable , then it has dense range in its domain . We also demonstrate how this fact can be used to give another proved of the Magajna - Schwizer theorem on the existence of self - adjoint extensions for tightly defined closed operators with equal deficiency indices .Let H be a Hilbert space over C . An operator T : D ( T ) → H is said to have dense range ( or simply DR ) , where D ( T ) ⊆ H , if for every matrix x ∈ H there exists a sequence { xn } n∈N such that limn→∞ | | xn−Tx | | = 0 . In other words , the image of T contains no complete subspace of H . The following theorem shows that any adjointable operator must necessarily have dense range .THEOREM 1.1.If T:D(T)⊆H→H is adjointable, then R(T∗)=D(T).",
        "rewrite_text": "We demonstrated that an adjointable operator possesses a dense range within its domain. Additionally, we illustrate how this finding can be applied to provide an alternative proof of the Magajna-Schweizer theorem, which states that self-adjoint extensions exist for tightly defined closed operators that have equal deficiency indices. Let \\( H \\) represent a Hilbert space over \\( \\mathbb{C} \\). An operator \\( T: D(T) \\to H \\) is considered to have a dense range (often abbreviated as DR) if for every vector \\( x \\in H \\), there exists a sequence \\( \\{ x_n \\}_{n \\in \\mathbb{N}} \\) such that \\( \\lim_{n \\to \\infty} \\| x_n - Tx \\| = 0 \\). In other words, the image of \\( T \\) does not contain any closed subspace of \\( H \\). The following theorem establishes that any adjointable operator must indeed have a dense range: \n\n**THEOREM 1.1.** If \\( T: D(T) \\subset H \\to H \\) is adjointable, then \\( R(T^*) = D(T) \\).",
        "ori-fast-z-score": 0.29488391230979427,
        "water-fast-z-score": 2.06418738616856,
        "rewrite-fast-z-score": -1.6641005886756874
    },
    {
        "original_text": "We consider the problem of approximating an unknown function f in L^2(0,1) by a linear combination of orthonormal polynomials with respect to some weight function w on (0,1). We show that if we choose the weights so as to minimize the error between the approximation and the true solution then this leads to a system of equations which can be solved using spectral methods. The resulting algorithm is shown to have optimal convergence rates under certain conditions. In particular it has been proved recently that if the coefficients of the expansion are chosen optimally then the rate of convergence is O(N^{-2}), where N denotes the number of terms used in the expansion. \nThe main purpose of this thesis was to implement these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions. This work will also allow us to investigate whether there exist any advantages associated with choosing different types of basis functions when solving differential equations.",
        "watermark_text": "We consider the question of approximating an unknown function f in L ^ 2 ( 0 , 1 ) by a linear mixture of orthonormal polynomials with regard to some weight distribution w on ( 0 , 1 ) . We see that if we choose the weights so as to minimize the error between the approximation and the true answer then this results to a system of equations which can be answered using spectral algorithms .The resulting algorithm is demonstrated to have optimal convergence rates under certain conditions . In particular it has been determined recently that if the coefficients of the expansion are chosen optimally then the frequency of convergence is O ( N ^ { - 2 } ) , where N describes the number of words using in the expansion .The main aim of this dissertation was to execute these algorithms numerically and compare their performance against other existing techniques such as Chebyshev expansions or Legendre expansions . This research will also enable us to examine whether there exist any advantages associated with choosing particular kinds of basis variables when solving differential equations .",
        "rewrite_text": "We explore the problem of approximating an unknown function f in \\( L^2(0, 1) \\) using a linear combination of orthonormal polynomials, weighted by a distribution \\( w \\) over the interval \\( (0, 1) \\). By selecting the weights to minimize the approximation error, we derive a system of equations solvable through spectral algorithms. This algorithm has been shown to achieve optimal convergence rates under certain conditions. Specifically, recent findings indicate that when the expansion coefficients are chosen optimally, the convergence rate is \\( O(N^{-2}) \\), where \\( N \\) represents the number of terms used in the expansion. The primary objective of this dissertation is to implement these algorithms numerically and assess their performance in comparison to other methods, such as Chebyshev and Legendre expansions. Additionally, this research will allow us to investigate potential advantages linked to the selection of specific basis functions in solving differential equations.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "We report on the detection of an oscillatory pattern with periods between 5 and 20 minutes, which is observed to be associated with sunspots  penumbrae. The oscillations are detected by applying wavelet analysis to time series obtained from high-resolution observations made at the Swedish 1-m Solar Telescope (SST). We find that these oscillations have amplitudes up to 0.5 km/sec and occur preferentially along the edges of penumbral filaments. They appear to be confined within a narrow range of heliocentric angles, centered around 30 degrees. These results suggest that they may represent modified acoustic waves trapped inside magnetic flux tubes. This work was supported by NASA grants NAG5-7067 and NNG04GK85G. Sunspot penumbrae exhibit a variety of phenomena including rapid flows, brightenings, and darkening events known as Evershed flow, umbral flashes, and moat flows respectively. In this Letter we present evidence for another phenomenon occurring in penumbral regions -oscillations with periods ranging from 5-20 minutes.",
        "watermark_text": "We report on the observation of an oscillatory system with periods between 5 and 20 minutes , which is observed to be correlated with sunspots penumbrae . The oscillations are detected by using wavelet study to time sequences obtained from high - resolution measurements made at the Swedish 1 - m Solar Telescope ( SST ) .We see that these oscillations have amplitudes up to 0 . 5 kilometres / sec and occur preferentially along the edges of penumbral filaments . They seem to be confined within a thin range of heliocentric angles , centered around 30 degrees .These data suggest that they may form altered acoustic waves stuck inside magnetic flux tubes . This research was supported by NASA grants NAG5 - 7067 and NNG04GK85G .Sunspot penumbrae exhibit a variety of phenomena including rapid flows , brightenings , and darkening phenomena known as Evershed flow , umbral flashes , and moat waves respectively . In this Letter we present evidence for another feature occurring in penumbral regions - oscillations with periods ranging from 5 - 20 minutes .",
        "rewrite_text": "We report our findings on an oscillatory system characterized by periods ranging from 5 to 20 minutes, which appears to be linked with the penumbra of sunspots. These oscillations were detected through a wavelet analysis of time sequences gathered from high-resolution observations conducted at the Swedish 1-meter Solar Telescope (SST). Our results indicate that these oscillations can reach amplitudes of up to 0.5 kilometers per second and are predominantly located along the edges of penumbral filaments. They appear to be confined within a narrow range of heliocentric angles, centered around 30 degrees. This evidence suggests that the oscillations may form altered acoustic waves trapped within magnetic flux tubes. This research received support from NASA grants NAG5-7067 and NNG04GK85G. Sunspot penumbrae are known to exhibit various phenomena, including rapid flows, brightenings, and darkening effects referred to as Evershed flow, umbral flashes, and moat waves, respectively. In this Letter, we provide evidence for an additional phenomenon occurring in penumbral regions: oscillations with periods spanning from 5 to 20 minutes.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 6.325771464049632,
        "rewrite-fast-z-score": 3.3941932686877867
    },
    {
        "original_text": "We present an algorithm for quantum search that is based on the Grover s algorithm and uses only two qubits to represent one item in the database, which can be viewed as a binary number. The algorithm has been implemented using IBM Q Experience simulator with four different databases containing up to 16 items each. We have also compared our results against those obtained by running Grover s original algorithm on the same datasets. Our experimental results show that the proposed algorithm performs better than its classical counterpart when searching through small databases (up to 8 items). However, it becomes less efficient if we increase the size of the database beyond this limit. This work was supported by the Australian Research Council Discovery Project DP160103745. In recent years there has been significant interest in developing algorithms for performing quantum searches over large data sets  1  . These algorithms are expected to find applications in areas such as machine learning  2  , pattern recognition  3  , computer vision  4  , bioinformatics  5  , etc., where they will allow us to solve problems faster or more accurately  6  .\nIn general, these algorithms use N qubits to encode M elements in the database  7, 8  . For example, Grover s algorithm  9  requires O( √ N/M ) iterations to find any single element out of M elements encoded into N qubits  10  . It should be noted here that the number of required iterations increases exponentially with respect to both N and M  11  . Therefore, these algorithms become inefficient when dealing with very large databases  12  .",
        "watermark_text": "We introduce an algorithm for quantum search that is based on the Grover s algorithm and using only two qubits to represent one element in the database , which can be viewed as a binary value . The algorithm has been deployed using IBM Q Experience simulator with four different databases containing up to 16 artifacts each .We have already compared our findings against those achieved by running Grover s original method on the same datasets . Our research results show that the suggested method performs better than its classical rival when looking through tiny data ( up to 8 objects ) .However , it becomes slower efficient if we increase the length of the database beyond this limit . This research was supported by the Australian Research Council Discovery Project DP160103745 .In recent years there has been significant interest in building methods for performing quantum searches over large data sets 1 . These methods are expected to find uses in areas such as machine computing 2 , image learning 3 , computer vision 4 , bioinformatics 5 , etc . , where they will let us to solve issues quicker or more accurately 6 .In general , these algorithms use N qubits to encode M elements in the database 7 , 8 . For instance , Grover s algorithm 9 requires O ( √ N / M ) iterations to find any single element out of M elements stored into N qubits 10 .It should be mentioned here that the number of required iterations increases exponentially with regard to both N and M 11 . Therefore , these algorithms get inefficient when dealing with very huge databases 12 .",
        "rewrite_text": "We present a quantum search algorithm inspired by Grover's algorithm, utilizing only two qubits to represent a single element in the database, essentially treating it as a binary value. This algorithm has been implemented using the IBM Q Experience simulator and tested on four different databases, each containing up to 16 artifacts. We have compared our results to those obtained using Grover's original method on the same datasets. Our findings indicate that the proposed method outperforms its classical counterpart when searching through small datasets (up to 8 objects). However, its efficiency diminishes as the database size exceeds this threshold. This research was funded by the Australian Research Council Discovery Project DP160103745. Recently, there has been considerable interest in developing techniques for conducting quantum searches over large datasets. These methods are anticipated to be beneficial in various fields such as machine learning, image processing, computer vision, and bioinformatics, enabling quicker and more accurate problem-solving. Generally, these algorithms employ N qubits to encode M elements in the database. For example, Grover's algorithm requires O(√N/M) iterations to locate a single element among M elements stored in N qubits. It is important to note that the number of required iterations increases exponentially with both N and M, leading to inefficiencies when managing very large databases.",
        "ori-fast-z-score": -0.19802950859533489,
        "water-fast-z-score": 7.389969586232199,
        "rewrite-fast-z-score": -0.3144854510165755
    },
    {
        "original_text": "The aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "The goal of this paper is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on computational intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or ecological computers ( EAs ) . The text encompasses both theoretical components and useful users of these algorithms .It additionally outlines some latest advances in intelligent detection systems that are essential for successful implementation of on - line condition monitoring schemes . This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort .Contents comprise : Chapter 1 : Introduction to On - line Condition Monitoring . Chapter 2 : Intelligent Sensors for On - line Condition Monitoring .Chapters 3-7: Neural Networks for Fault Diagnosis.Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "This paper aims to present an overview of the current advancements in online condition monitoring and failure detection within industrial systems, focusing particularly on computational intelligence methods such as neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs). The discussion includes both theoretical aspects and practical applications of these algorithms. Additionally, it highlights recent progress in intelligent detection systems, which are crucial for the effective implementation of online condition monitoring strategies. This publication will serve as a valuable resource for both researchers and designers looking to integrate computational intelligence techniques into their work. The content includes: Chapter 1: Introduction to Online Condition Monitoring; Chapter 2: Intelligent Sensors for Online Condition Monitoring; Chapters 3-7: Neural Networks for Fault Diagnosis; Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis; Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 1.1523319193960637
    },
    {
        "original_text": "We present predictions on the formation rates, masses, orbital parameters, and spin periods of triple stars that contain at least one pulsar (PSR). We use Monte Carlo simulations to generate populations of binaries and triples using distributions of initial binary properties derived from observations of open clusters. The population synthesis is performed by combining our new code for simulating the evolution of close binaries containing PSRs with an existing code for simulating the dynamical interactions between multiple stellar systems. Our results show that most observed PSRs are likely to be found as members of hierarchical triples rather than isolated double neutron star systems or wide binaries. In particular, we find that:  - Most PSRs should have companions whose mass lies within 0.1 M⊙ < Mc < 1.0 M⊙; - Most PSRs should reside in orbits with semi-major axes less than 100 AU; - Most PSRs will not evolve into millisecond pulsars before their second supernova explosion; - Most PSRs may experience significant gravitational wave emission during their lifetimes.",
        "watermark_text": "We report estimates on the formation rates , masses , orbital variables , and spin times of triple stars that host at least one pulsar ( PSR ) . We use Monte Carlo simulations to create populations of binaries and triples using distributions of initial binary properties derived from measurements of open nuclei .The population synthesis is conducted by combining our new code for simulating the evolution of close binaries containing PSRs with an previous code for simulating the dynamical interactions between multiple stellar systems . Our results show that most observed PSRs are likely to be found as members of hierarchical triples rather than scattered double neutron star systems or broad binaries .In particular , we find that : - Most PSRs should have companions whose mass falls within 0 . 1 [UNK] < Mc < 1 . 0 [UNK] ; - Most PSRs should exist in planets with semi - major axes less than 100 AU ; - Most PSRs will not evolve into millisecond pulsars before their second supernova explosion ; - Most PSRs might experience considerable gravitational wave radiation during their lifetimes .",
        "rewrite_text": "We present estimates regarding the formation rates, masses, orbital characteristics, and spin durations of triple star systems that contain at least one pulsar (PSR). Utilizing Monte Carlo simulations, we generate populations of binary and triple systems based on initial binary property distributions derived from observations of open clusters. Our population synthesis merges our newly developed code for simulating the evolution of close binaries with pulsars with an existing code for modeling the dynamical interactions within multiple stellar systems. Our findings indicate that the majority of observed PSRs are likely part of hierarchical triples rather than being scattered in double neutron star systems or widely separated binaries. In detail, we determine that: - Most PSRs are expected to have companions with masses within 0.1 [UNK] < Mc < 1.0 [UNK]; - Most PSRs are anticipated to be in systems with semi-major axes of less than 100 AU; - Most PSRs are unlikely to evolve into millisecond pulsars before undergoing their second supernova explosion; - Most PSRs may experience significant gravitational wave radiation throughout their lifetimes.",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 4.837877973981903,
        "rewrite-fast-z-score": 0.2182178902359924
    },
    {
        "original_text": "We present an analysis of the alignments between galaxy spins and tidal fields in real space, using data from the Two Mass Redshfit Survey (TMRS). We find that galaxies are preferentially aligned perpendicular to their local tidal field on scales larger than 1 Mpc/h. This alignment is stronger for more massive galaxies at higher redshifts. The observed spin-tide correlation can be explained by the effect of gravitational torques exerted by large-scale structures during the formation process of these galaxies. Our results suggest that this mechanism may play an important role in shaping galactic angular momenta. These findings have implications for understanding how dark matter halos acquire their angular momentum as well as for interpreting observations of cosmic shear statistics. Introduction: Galaxies form within overdense regions of the universe where they experience strong gravitational interactions with other objects such as neighboring galaxies or clusters of galaxies. During the formation process, these interactions induce gravitational torques which affect the orientation of the galactic angular momentum vector. In turn, the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction processes. Therefore, it has been suggested that the shape distribution of galaxies could provide information about the origin of galactic angular momentums (e.g., Catelan & Theuns 1996; Lee et al. 2008) . However, observational studies show conflicting results regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors  positions (see e.g., Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 , for recent works).\nIn order to understand the physical mechanisms responsible for determining the directions of galactic angular momentas, we need to study the statistical properties of galaxy spin distributions over large volumes of the universe. Recent surveys like Sloan Digital Sky Survey (SDSS) allow us to measure galaxy orientations accurately enough to perform such analyses. For example, Lee et al. (2008) used SDSS DR4 data to investigate the alignments between galaxy spin vectors and their nearest neighbor s position angles. They found no",
        "watermark_text": "We present an assessment of the alignments between galaxy spins and tidal fields in real space , using data from the Two Mass Redshfit Survey ( TMRS ) . We see that galaxies are preferentially aligned perpendicular to their nearby tidal field on scales bigger than 1 Mpc / h .This alignment is strengthened for more massive galaxies at higher redshifts . The observed spinning - tide relationship can be described by the impact of gravitational torques exerted by large - scale structures during the formation period of these objects .Our results propose that this mechanism may play an important role in shaping galactic angular momenta . These studies have consequences for studying how dark matter halos acquire their angular velocity as also as for interpreting observations of universe shear trends .Introduction : Galaxies shape within overdense regions of the universe where they encounter strong gravitational interactions with other structures such as nearby galaxies or rows of clusters . During the formation period , these interactions initiate gravitational torques which affect the orientation of the galactic angular velocity tensor .In turn , the orientations of galactic angular momenta determine the shapes of galaxies through dynamical friction mechanisms . Therefore , it has been proposed that the form distribution of stars could give information about the origin of galactic angular momentums ( e . g . , Catelan & Theuns 1996 ; Lee et al .2008 ) . However , observational analyses suggest conflicting findings regarding whether there exists any preferred direction of galaxy spin axes relative to their neighbors places ( saw e . g . , Faltenbacher et al .2002 ; Bailin et al . 2005 ; Paz et al .2008 ; Codis et al . 2012 , for recent works ) .In order to comprehend the physical mechanisms involved for determining the directions of galactic angular momentas , we require to study the statistical characteristics of galaxy spin distributions over large quantities of the universe . Recent surveys like Sloan Digital Sky Survey ( SDSS ) able us to measure galaxy orientations correctly sufficiently to conduct such analyses .For instance , Lee et al . ( 2008 ) used SDSS DR4 data to examine the alignments between galaxy spin vectors and their closest neighbor s position angles .They found no",
        "rewrite_text": "We conduct an analysis of the alignment between galaxy spins and tidal fields in real space, utilizing data from the Two Mass Redshift Survey (TMRS). Our findings indicate that galaxies tend to align perpendicularly to the nearby tidal fields on scales exceeding 1 Mpc/h. This alignment effect is even more pronounced in more massive galaxies at higher redshifts. The observed relationship between spins and tidal influences can be attributed to the gravitational torques imparted by large-scale structures during the formation of these galaxies. Our results suggest that this mechanism may significantly influence the development of galactic angular momenta. These insights have implications for understanding how dark matter halos obtain their angular velocities and for interpreting observations of cosmic shear patterns.\n\nIntroduction: Galaxies evolve within regions of the universe that are overdense, where they experience strong gravitational interactions with surrounding structures, including nearby galaxies and clusters. During their formative years, these interactions generate gravitational torques that impact the orientation of the galactic angular velocity tensor. Consequently, the orientations of galactic angular momenta play a crucial role in shaping galaxies through processes related to dynamical friction. As a result, it has been suggested that the distribution of stellar forms could provide insights into the origins of galactic angular momentum (e.g., Catelan & Theuns 1996; Lee et al. 2008). However, observational studies have yielded mixed results concerning the potential existence of a preferred alignment of galaxy spin axes with respect to their neighbors (see, for instance, Faltenbacher et al. 2002; Bailin et al. 2005; Paz et al. 2008; Codis et al. 2012 for recent contributions).\n\nTo truly understand the physical mechanisms that dictate the directions of galactic angular momenta, it is essential to analyze the statistical properties of galaxy spin distributions across vast regions of the universe. Recent surveys such as the Sloan Digital Sky Survey (SDSS) provide the necessary data to accurately measure galaxy orientations for such investigations. For example, Lee et al. (2008) utilized SDSS DR4 data to explore the alignments between galaxy spin vectors and the position angles of their nearest neighbors, but they found no definitive results.",
        "ori-fast-z-score": -0.8512055557875505,
        "water-fast-z-score": 7.140584836498262,
        "rewrite-fast-z-score": 0.7715167498104595
    },
    {
        "original_text": "We study the SLE (Schramm-Loewner Evolution) process for the scaling limit of interfaces between different phases in the two-dimensional Ising model with nearest-neighbor interactions on an arbitrary planar graph, and its generalization to higher dimensions. We show that the interface is described by a chordal Schramm-Löwner evolution if the underlying lattice has no loops or multiple edges; otherwise it is described by a radial Schramm-Löwner evolutions. The results are obtained using conformal field theory techniques. In particular we use the fact that the partition function of these models can be written as a correlation function of primary fields in some rational conformal field theories. This allows us to obtain explicit formulas for the probability distribution functions of various geometric quantities associated with the interfaces such as their winding numbers around vertices etc.. \nIntroduction\n\nThe Schramm-Loewner Evolutions (SLE)\nprocesses were introduced by Schramm  Sch00  , who showed that they provide a natural description of the scaling limits of interfaces in statistical mechanics systems at criticality. These processes have been studied extensively since then both theoretically and numerically. For example, see  KSS02, SS04a, SS04b, RS05, Sch06, CS07, KS08, KSV09, KM10, MS11, MZ12, BMS13, BS14, LW15, GKS16, GM17, GK18, HJ19, HK20, JPS20  . A comprehensive review of this subject may be found in  Smi01, Sta03, Joh10  .\nIn this work we consider the SLE process for the scaling limit in two dimensions of interfaces separating different phases in the following class of models:  Let G = (V, E) be any finite connected planar graph without loops or multiple edges. Consider the Ising model with nearest neighbor interaction defined on G. That is, let {σv}v∈V denote a collection of random variables taking values +1 and −1, where each σv represents the state of vertex v ∈ V . Then",
        "watermark_text": "We work the SLE ( Schramm - Loewner Evolution ) process for the scaling maximum of interfaces between various phases in the two - dimensional Ising model with nearest - neighbor interactions on an arbitrary planar graph , and its generalization to higher dimensions . We see that the interface is characterized by a chordal Schramm - Löwner evolution if the underlying lattice has no loops or multiple edges ; otherwise it is characterized by a radial Schramm - Löwner evolutions .The results are derived using conformal field model approaches . In particular we utilize the fact that the splitting function of these models can be written as a correlation function of primary fields in some rational conformal field theories .This enables us to obtain precise formulas for the probability distribution functions of several geometric variables identified with the interfaces such as their winding numbers around vertices etc . . Introduction The Schramm - Loewner Evolutions ( SLE ) processes were introduced by Schramm Sch00 , who demonstrated that they give a natural characterization of the scaling limits of interfaces in statistical mechanics systems at criticality . These mechanisms have been studied frequently since then both theoretically and numerically .For instance , see KSS02 , SS04a , SS04b , RS05 , Sch06 , CS07 , KS08 , KSV09 , KM10 , MS11 , MZ12 , BMS13 , BS14 , LW15 , GKS16 , GM17 , GK18 , HJ19 , HK20 , JPS20 . A full study of this subject may be found in Smi01 , Sta03 , Joh10 .In this research we define the SLE method for the scaling limit in two dimensions of interfaces separating different stages in the following category of models : Let G = ( V , E ) be any finite connected planar graph without loops or multiple edges . Consider the Ising model with nearest neighbor behavior defined on G . That is , let { σv } v∈V denote a collection of random vectors take values + 1 and −1 , where each σv represents the state of vertex v ∈ V .Then",
        "rewrite_text": "We investigate the Schramm-Löwner Evolution (SLE) process in relation to the scaling maximum of interfaces that separate different phases in the two-dimensional Ising model, with nearest-neighbor interactions on an arbitrary planar graph, as well as its extension to higher dimensions. Our findings indicate that if the underlying lattice is free of loops and multiple edges, the interface exhibits characteristics of a chordal Schramm-Löwner evolution; otherwise, it is described by radial Schramm-Löwner evolutions. We derive these results using approaches from conformal field theory, particularly leveraging the fact that the splitting function of these models can be expressed as a correlation function of primary fields in certain rational conformal field theories. This allows us to derive precise formulas for the probability distribution functions of various geometric variables associated with the interfaces, such as their winding numbers around vertices. \n\n**Introduction:** The Schramm-Löwner Evolution (SLE) processes were originally introduced by Schramm in 2000, demonstrating their natural role in characterizing the scaling limits of interfaces in critical statistical mechanics systems. Since then, they have been extensively studied both theoretically and numerically, as documented in various works including KSS02, SS04a, SS04b, RS05, Sch06, CS07, KS08, KSV09, KM10, MS11, MZ12, BMS13, BS14, LW15, GKS16, GM17, GK18, HJ19, HK20, JPS20. A comprehensive examination of this topic can be found in Smi01, Sta03, and Joh10. In this research, we establish the SLE framework for the scaling limit in two dimensions of interfaces separating different phases in a specific category of models: Let \\( G = ( V , E ) \\) be any finite connected planar graph devoid of loops or multiple edges. We consider the Ising model with nearest-neighbor interactions defined on \\( G \\). Specifically, let \\(\\{ \\sigma_v \\}_{v \\in V}\\) represent a set of random variables that take values +1 and −1, where each \\(\\sigma_v\\) indicates the state of the vertex \\(v \\in V\\).",
        "ori-fast-z-score": 1.3750477455423171,
        "water-fast-z-score": 6.875238727711585,
        "rewrite-fast-z-score": 1.4852968963237645
    },
    {
        "original_text": "We study the dynamics of the kinetic Ising model on square lattices with periodic boundary conditions by Monte Carlo simulations at finite temperatures T . We find that there is no static order parameter to characterize the dynamic phase transition, but we can define an effective conjugate field H conjugate to the magnetization M as follows: \nH = -ln(<M>)/T,\nwhere <M> denotes the average over all spins. The critical temperature Tc is determined by the condition dH/dT =0. In addition, we show that the fluctuation-dissipation theorem holds well near Tc. \nThe results are compared with those obtained by the mean-field approximation. \n\n\nI. INTRODUCTIO N\n\nIn recent years much attention has been paid to nonequilibrium phenomena such as relaxation processes after rapid changes of external parameters  1  , aging  2  , glassy behavior  3  , etc., because they play important roles not only in physics but also in biology  4  .\nAmong these topics, the kinetic Ising model  5  is one of the most popular models used to investigate non-equilibrium properties  6  . It describes the time evolution of spin variables S i (t) (i=1,...,N)\non a regular lattice under the influence of thermal fluctuations. Here t represents the number of Monte Carlo steps per site (MCS/s). At each step, every spin interacts with its nearest neighbors through exchange interactions J ij . Then it flips according to the Metropolis algorithm  7 :  if e -Sij / kBT > random number between 0 and 1 then flip S j else keep S j unchanged where k B is Boltzmann s constant and T is the absolute temperature. This process continues until equilibrium is reached or some other criterion is satisfied  8  .",
        "watermark_text": "We explore the dynamics of the kinetic Ising model on square lattices with periodic border conditions by Monte Carlo simulations at finite temperatures T . We see that there is no static order parameter to characterize the dynamic phase shift , but we can define an efficient conjugate field H conjugate to the magnetization M as follows : H = - ln ( < M > ) / T , where < M > denotes the average over all spins .The critical temperature Tc is chosen by the condition dH / dT = 0 . In addition , we prove that the fluctuation - dissipation conjecture works well near Tc .The results are compared with those achieved by the mean - field approximation . I . INTRODUCTIO N In recent years much attention has been paid to nonequilibrium phenomena such as relax processes after rapid variations of external parameters 1 , aging 2 , glassy behavior 3 , etc . , because they hold important roles not only in science but also in science 4 .Among these topics , the kinetic Ising model 5 is one of the most popular theories used to examine non - equilibrium properties 6 . It studies the period evolution of spin vectors S i ( t ) ( i = 1 , . . . , N ) on a regular structure under the impact of thermal fluctuations .Here t refers the number of Monte Carlo steps per site ( MCS / s ) . At each step , every spin interacts with its closest neighbors through exchange interactions J ij .Then it flips due to the Metropolis algorithm 7 : if e - Sij / kBT > random value between 0 and 1 then flip S j else kept S k unchanged where k B is Boltzmann s constant and T is the absolute temperature . This process goes until equilibrium is reached or some other criterion is fulfilled 8 .",
        "rewrite_text": "We investigate the dynamics of the kinetic Ising model on square lattices under periodic boundary conditions using Monte Carlo simulations at finite temperatures T. Our findings reveal that there is no static order parameter to delineate the dynamic phase transition. However, we can efficiently define a conjugate field \\( H \\), related to the magnetization \\( M \\), as follows: \\( H = - \\ln(\\langle M \\rangle) / T \\), where \\( \\langle M \\rangle \\) represents the average spin value across the system. The critical temperature \\( T_c \\) is determined by the criterion \\( \\frac{dH}{dT} = 0 \\). Furthermore, we demonstrate that the fluctuation-dissipation theorem holds well near \\( T_c \\). Our results are compared with those obtained through mean-field approximations. \n\n**I. INTRODUCTION**\nIn recent years, there has been significant interest in nonequilibrium phenomena, such as relaxation processes following rapid changes in external parameters, aging, and glassy behavior, due to their crucial implications in both scientific research and practical applications. Among these topics, the kinetic Ising model is one of the most widely studied frameworks for examining nonequilibrium properties. It analyzes the temporal evolution of spin vectors \\( S_i(t) \\) (for \\( i = 1, \\ldots, N \\)) arranged on a regular lattice, influenced by thermal fluctuations. Here, \\( t \\) denotes the number of Monte Carlo steps per site (MCS/s). At each step, each spin interacts with its nearest neighbors through exchange interactions \\( J_{ij} \\). A spin may flip according to the Metropolis algorithm: if \\( e^{-S_{ij}/k_B T} \\) is greater than a random value between 0 and 1, then \\( S_j \\) flips; otherwise, \\( S_k \\) remains unchanged, where \\( k_B \\) is Boltzmann's constant and \\( T \\) is the absolute temperature. This process continues until equilibrium is achieved or another termination criterion is met.",
        "ori-fast-z-score": 0.9622504486493763,
        "water-fast-z-score": 6.928853368993243,
        "rewrite-fast-z-score": 2.496150883013531
    },
    {
        "original_text": "We present new observations of the central region of the nearby Seyfert galaxy NGC 4258, which show that its nuclear disk is warped by an angle of ~20 degrees with respect to the plane of the host galaxy s stellar bulge (see Figure 1 ). The warp has been detected using near-infrared integral field spectroscopy obtained at Gemini Observatory on Mauna Kea, Hawaii. \n \n We also report the detection of significant rotation about the minor axis of this warped structure, as well as evidence for counter-rotation within the innermost few hundred parsecs of the nucleus. These results are consistent with previous studies based on optical data alone. \n \n In addition, we find that the kinematics of the gas in the outer regions of the nuclear disk can be explained if it orbits around the supermassive black hole located at the center of the galaxy under the influence of both gravitational forces and magnetic fields. This result suggests that the observed warps may have their origin in the magneto-rotational instability (MRI) operating in accretion disks surrounding massive black holes. \n \n Finally, we discuss how these findings could help us understand the physics behind the so-called  Bardeen-Petterson effect : i.e., the alignment between the spin axes of the stars and the angular momentum vector of the accreting material onto the central supermassive black hole.",
        "watermark_text": "We present new images of the central region of the nearby Seyfert galaxy NGC 4258 , which show that its nuclear core is warped by an angle of ~ 20 degrees with regard to the plane of the host universe s stellar bulge ( see Figure 1 ) . The warp has been detected using near - infrared inverse field spectroscopy acquired at Gemini Observatory on Mauna Kea , Hawaii .We additionally report the observation of large rotation about the minor axis of this warped structure , as also as data for counter - movement within the innermost few hundred parsecs of the nucleus . These conclusions are compatible with previous research based on optical data alone .In addition , we find that the kinematics of the gas in the exterior areas of the atomic disk can be understood if it orbits around the supermassive black hole located at the center of the galaxy under the effects of both gravity forces and magnetic fields . This result suggests that the seen warps may have their source in the magneto - rotational instability ( MRI ) working in accretion disks surrounding massive blue holes .Finally , we talk how these results could assist us explain the physics behind the so - called Bardeen - Petterson effect : i . e . , the alignment between the spin axes of the stars and the angular velocity tensor of the accreting matter onto the main supermassive black hole .",
        "rewrite_text": "We introduce new images of the central region of the nearby Seyfert galaxy NGC 4258, revealing that its nuclear core is warped at an angle of approximately 20 degrees relative to the plane of the host galaxy's stellar bulge (see Figure 1). This warp was identified using near-infrared inverse field spectroscopy collected at the Gemini Observatory on Mauna Kea, Hawaii. Moreover, we report significant rotation along the minor axis of this warped structure, along with observations of counter-movement in the innermost few hundred parsecs of the nucleus. Our findings align with prior research that relied solely on optical data. Additionally, we conclude that the kinematics of the gas in the outer regions of the atomic disk can be explained by its orbit around the supermassive black hole at the galaxy's center, influenced by both gravitational forces and magnetic fields. This suggests that the observed warps may stem from magneto-rotational instability (MRI) occurring in the accretion disks surrounding massive black holes. Finally, we discuss how these findings could help elucidate the physics of the Bardeen-Petterson effect, which describes the alignment between the spin axes of stars and the angular velocity tensor of the accreting material onto the central supermassive black hole.",
        "ori-fast-z-score": -2.4494897427831783,
        "water-fast-z-score": 4.4907311951024935,
        "rewrite-fast-z-score": -0.6396021490668313
    },
    {
        "original_text": "We have determined the iron abundance for two bright, blue supergiants (BSGs) in the Galactic globular clusters Omega Cen and M13 using high-resolution spectroscopy obtained with UVES at VLT-UT2 telescope.  The results are compared to those derived by other authors for similar objects in these clusters as well as in other globulars. We find that our values agree very well with previous determinations within their uncertainties. In particular we confirm the low Fe content found for one star in Omega Cen previously reported by Yong et al. (2005) . This is consistent with theoretical predictions which suggest that this cluster should be dominated by first generation stars formed out of material enriched only by massive supernovae. Our analysis also shows that both studied stars belong to the group of so-called  blue stragglers  -objects located above the main sequence turn-off point on the colour-magnitude diagram but still burning helium in their cores.",
        "watermark_text": "We have predicted the metal density for two bright , blue supergiants ( BSGs ) in the Galactic globular galaxies Omega Cen and M13 using high - resolution spectroscopy acquired with UVES at VLT - UT2 telescope . The results are compared to those derived by other researchers for related objects in these complexes as well as in other globulars .We see that our values comply very best with previous determinations within their uncertainties . In particular we verified the poor Fe concentration found for one star in Omega Cen previously reported by Yong et al .( 2005 ) . This is compatible with theoretical estimates which propose that this cluster should be dominated by first generation stars formed out of debris enriched only by massive supernovae .Our study also shows that both examined stars belong to the group of so - called blue stragglers - objects located above the main sequence turn - off position on the colour - magnitude diagram but still consuming helium in their cores .",
        "rewrite_text": "We have estimated the metal density of two bright blue supergiants (BSGs) in the Galactic globular clusters Omega Centauri and M13, utilizing high-resolution spectroscopy obtained with the UVES at the VLT-UT2 telescope. Our findings are compared with those from other researchers studying similar objects in these clusters and other globulars. We find that our results align closely with previous measurements, taking into account their uncertainties. Notably, we confirmed the low iron concentration reported for a star in Omega Centauri by Yong et al. (2005), which aligns with theoretical models suggesting that this cluster is primarily made up of first-generation stars formed from material enriched solely by massive supernovae. Additionally, our analysis indicates that both stars we examined belong to the category of blue stragglers—objects situated above the main sequence turn-off point on the color-magnitude diagram, yet still fusing helium in their cores.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -0.762000762001143
    },
    {
        "original_text": "The article presents results of computer simulation of the dynamics of chromatin in vitro, which is based on the model of self-organization of the chain of nucleosome core particles (NCP) with histone H1 molecules attached to them by electrostatic forces. The NCPs are modeled as rigid cylinders with diameter equal to that observed experimentally for isolated NCPs. It was found that the presence of histone H1 leads to an increase in the persistence length of the chain due to formation of loops between neighboring NCPs. In addition, it has been shown that the presence of histones H1 can lead to the appearance of defects in the structure of the chain consisting of several adjacent NCPs connected only through one H1 molecule. These defects have a significant effect on the elasticity characteristics of the chain. \n \n Keywords: Chromatin, Self-assembly, Computer simulation, Elasticity, Defects",
        "watermark_text": "The section offers results of computational simulation of the dynamics of chromatin in vitro , which is based on the model of self - arrangement of the chain of nucleosome backbone particles ( NCP ) with histone H1 molecules attached to them by electrostatic fields . The NCPs are modeled as compact cylinders with diameter equal to that detected experimentally for isolated NCPs .It was shown that the presence of histone H1 leads to an increase in the persistence length of the chain thanks to formation of loops between neighboring NCPs . In addition , it has been shown that the presence of histones H1 can lead to the appearance of errors in the formation of the chain consisting of several neighbouring NCPs linked only through one H1 molecule .These flaws have a substantial impact on the elasticity parameters of the chain . Keywords : Chromatin , Self - assembly , Computer modeling , Elasticity , Defects",
        "rewrite_text": "This section presents the results of a computational simulation investigating the dynamics of chromatin in vitro. The simulation utilizes a self-assembly model of nucleosome core particles (NCPs) connected by histone H1 molecules through electrostatic interactions. The NCPs are represented as compact cylinders, with a diameter matching that measured experimentally for isolated NCPs. The findings indicate that histone H1 increases the persistence length of the chain by facilitating the formation of loops between adjacent NCPs. Furthermore, the presence of histone H1 can introduce errors in the assembly of the chain, particularly when neighboring NCPs are linked solely by a single H1 molecule. These defects significantly affect the chain's elasticity characteristics. Keywords: Chromatin, Self-assembly, Computer modeling, Elasticity, Defects.",
        "ori-fast-z-score": -0.13736056394868904,
        "water-fast-z-score": 5.906504249793628,
        "rewrite-fast-z-score": 1.697056274847714
    },
    {
        "original_text": "We study the evolution of an isotropic, homogeneous cosmological model filled by radiation and a Chaplygyn gas. We find that this system has two attractors corresponding to different values of the Hubble parameter at late times. The first one corresponds to a de Sitter phase where the energy density of the Chaplygin gas dominates over the other components while the second one describes a decelerating universe dominated by dark matter. In both cases we have found that the initial conditions are fixed by the value of the Hubble constant today. Finally, we show how these results can be used as initial conditions for inflationary models. PACS numbers: 98.80.Cq, 04.20.-q, 95.36.+x  Keywords: Cosmology, Inflation, Chaplygin gas, Radiation, Initial Conditions . \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying the possibility that our present day universe may contain some exotic form of matter which behaves like a negative pressure fluid (see e.g.,  1  ). This kind of matter could play an important role in explaining several phenomena observed on large scales such as the accelerated expansion of the universe  2  , the flatness problem  3  or even the origin of structure formation  4  .\nOne possible candidate for this type of matter is known as the Chaplygin gas  5  . It was originally introduced as a phenomenological description of the behaviour of superdense stars  6  but it also appears naturally within superstring theories  7, 8  . Recently, it has been shown  9  that the Chaplygin gas provides a good fit to current observational data  10  if its equation of state takes the following form: p = −A/ρ α , where A and α are positive constants. For small values of ρ, i.e., when the universe is dominated by ordinary matter, the above expression reduces to p ≈ 0 so that the Chaplygin",
        "watermark_text": "We explore the evolution of an isotropic , homogeneous cosmological model filled by radiation and a Chaplygyn gas . We see that this scheme has two attractors corresponding to different values of the Hubble parameter at late times .The first one corresponds to a de Sitter phase where the power concentration of the Chaplygin gas dominates over the other components while the second one states a decelerating universe inhabited by black material . In both cases we have discovered that the early conditions are fixed by the value of the Hubble constant today .Finally , we show how these results can be used as early conditions for inflationary theories . PACS codes : 98 . 80 . Cq , 04 . 20 . - q , 95 . 36 . + x Keywords : Cosmology , Inflation , Chaplygin gas , Radiation , Initial Conditions .I . INTRODUCTORY REMARK In recent years there has been substantial interest in examining the prospect that our contemporary day universe might exist some unusual type of matter which behaves like a negative pressure fluid ( see e . g . , 1 ) .This kind of matter could play an important role in understanding several phenomena observed on huge scales such as the advanced expansion of the universe 2 , the flatness problem 3 or even the origin of structure building 4 . One potential candidate for this form of matter is known as the Chaplygin gas 5 .It was originally developed as a phenomenological explanation of the dynamics of superdense stellar 6 but it also occurs commonly within superstring physics 7 , 8 . Recently , it has been shown 9 that the Chaplygin light provides a better suited to recent observational data 10 if its equation of state takes the following form : p = −A / ρ α , where A and ω are positive constants .For small values of ρ , i . e . , when the universe is dominated by normal matter , the above formula yields to p ≈ 0 so that the Chaplygin",
        "rewrite_text": "We investigate the development of an isotropic and homogeneous cosmological model comprised of radiation and Chaplygin gas. Our analysis reveals two distinct attractors associated with different values of the Hubble parameter in the late universe. The first attractor leads to a de Sitter phase where the concentration of Chaplygin gas prevails over other components, while the second reflects a decelerating universe filled with black material. In both scenarios, we find that the initial conditions are determined by the current value of the Hubble constant. Ultimately, we demonstrate how these findings can serve as initial conditions for inflationary models. PACS codes: 98.80.Cq, 04.20.-q, 95.36.+x. Keywords: Cosmology, Inflation, Chaplygin gas, Radiation, Initial Conditions.\n\nI. INTRODUCTORY REMARKS\n\nRecent years have seen a growing interest in the possibility that our modern universe contains a peculiar type of matter that behaves like a fluid with negative pressure (see e.g., 1). Such matter could be crucial for understanding various large-scale phenomena, including the accelerated expansion of the universe, the flatness problem, and the origin of structure formation. One potential candidate for this type of matter is Chaplygin gas. Initially conceived as a phenomenological model to explain the dynamics of superdense stars, it is also frequently encountered in superstring theory. Recent findings indicate that Chaplygin gas fits better with current observational data when its equation of state is expressed as p = -A/ρ^α, where A and α are positive constants. In regimes of low density, when the universe is primarily controlled by normal matter, this equation simplifies to p ≈ 0, illustrating the behavior of Chaplygin gas in that context.",
        "ori-fast-z-score": -0.17677669529663687,
        "water-fast-z-score": 6.540737725975564,
        "rewrite-fast-z-score": 0.2727272727272727
    },
    {
        "original_text": "We study the distribution of the total area swept out by a one-dimensional Brownian motion between two fixed times. We show that this distribution is given by an explicit formula involving the modified Bessel function I0(x). This result can be used to derive several interesting identities for special functions such as the Riemann zeta-function or the Hurwitz zeta-functions at even arguments. In particular we obtain new proofs of some results due to Wright on the number of graphs with n vertices having certain properties (such as being bipartite) which are related to the coefficients appearing in the expansion of the exponential generating function of these numbers into powers of t. Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials. The main tool will be the Feynman-Kac representation of the solution of the heat equation. Let Wt denote standard Brownian motion starting at 0. For any real number s > 0 let us consider the random variable A(s), defined as the total area swept out during the time interval  0,s  by the process Wt:",
        "watermark_text": "We test the distribution of the total area swept out by a one - dimensional Brownian movement between two fixed times . We see that this distribution is given by an explicit formula involving the modified Bessel distribution I0 ( x ) .This result can be used to derive several interesting identities for special functions such as the Riemann zeta - function or the Hurwitz zeta - functions at even arguments . In particular we obtain new proofs of some results attributed to Wright on the number of graphs with n edges having specific properties ( such as being bipartite ) which are related to the coefficients appearing in the transformation of the exponential producing function of these integers into powers of t . Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials .The main tool will be the Feynman - Kac representation of the solve of the temperature equation . Let Wt denote standard Brownian movement starting at 0 .For any real number s > 0 let us consider the random variable A ( s ) , defined as the total area swept out during the period interval 0 , s by the process Wt :",
        "rewrite_text": "We investigate the distribution of the total area covered by a one-dimensional Brownian motion over a fixed time interval. Our findings indicate that this distribution can be expressed using an explicit formula involving the modified Bessel function I0(x). This result allows us to derive several intriguing identities related to special functions, including the Riemann zeta function and the Hurwitz zeta functions for even arguments. Notably, we provide new proofs for several findings originally attributed to Wright concerning the number of graphs with n edges that possess specific properties, such as being bipartite. These properties are linked to the coefficients in the transformation of the exponential generating function of these integers into powers of t. Lastly, we offer an alternative proof of the identity that connects the moments of the Wiener measure with Bernoulli polynomials. Our primary approach will utilize the Feynman-Kac representation for solving the heat equation. Let Wt represent the standard Brownian motion starting from 0. For any real number s > 0, we define the random variable A(s) as the total area swept out by the process Wt during the interval [0, s].",
        "ori-fast-z-score": 1.5652475842498528,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "The low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "The lowest energy effective models for superstrings are supergravity and supersymmetric gauge fields in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold . In this talk I will explore some latest findings about lattice models that provide an different approach to researching these theories .The basic idea is to use Monte Carlo simulations to study supersymmetric field theories specified on a finite number of points ( the sites ) of a regular d - dimensional hypercubic structure with periodic border conditions . These systems have been studied frequently over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group algorithms .Recently we developed novel Monte Carlo simulation method based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down . We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter groups in different representations .",
        "rewrite_text": "The minimal energy effective models for superstrings are supergravity and supersymmetric gauge fields in four dimensions, derived by compactifying the additional six spatial dimensions on a Calabi-Yau manifold. In this presentation, I will discuss recent discoveries regarding lattice models that offer an alternative approach to investigating these theories. The core concept involves employing Monte Carlo simulations to analyze supersymmetric field theories defined on a finite set of points (the sites) within a regular d-dimensional hypercubic structure, complete with periodic boundary conditions. In recent years, these systems have been frequently examined using various mathematical techniques, including approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently, we introduced a novel Monte Carlo simulation method based on the worm algorithm, which allows us to simulate large systems at very high temperatures where traditional Monte Carlo techniques struggle due to critical slowing down. We applied our new method to estimate the free energies of several supersymmetric lattice models, including N = 4 supersymmetric Yang-Mills theory and N = 1 supersymmetric U(1) gauge theory connected to matter fields in various representations.",
        "ori-fast-z-score": 0.6255432421712244,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": -0.4082482904638631
    },
    {
        "original_text": "We consider the problem of linear regression with measurement error, where the errors are assumed to be normally distributed and uncorrelated between different measurements but correlated within each measurement set (i.e., for one given star). We derive an expression for the covariance matrix of the estimated parameters which is valid even if the number of data points per measurement set differs significantly among sets. This allows us to study how the accuracy of the results depends on the distribution of the number of observations over all stars. The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates. In particular we show that it may be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all data points for determining its first few derivatives. Finally, we apply our method to simulated data obtained from the Hipparcos astrometric satellite mission.",
        "watermark_text": "We consider the question of linear regression with measurement loss , where the errors are expected to be usually spread and uncorrelated between various measurements but correlated within each measurement set ( i . e . , for one given star ) . We derive an expression for the covariance matrix of the expected variables which is valid even if the proportion of data points per observation set differs greatly among setting .This enables us to study how the accuracy of the results varies on the distribution of the proportion of measurements over all stars . The derived formula can also be used as a tool for optimizing observational strategies by minimizing the uncertainty of the resulting estimates .In particular we prove that it could be advantageous to use only a small fraction of available data points when estimating the coefficients of the model function while using most or all information points for determining its initial few derivatives . Finally , we apply our technique to simulated images obtained from the Hipparcos astrometric satellite mission .",
        "rewrite_text": "We explore the topic of linear regression in the context of measurement loss, where errors are generally distributed and uncorrelated across different measurements, but are correlated within each set of measurements for a specific star. We derive a covariance matrix expression for the expected variables, which remains applicable even when the number of data points per observation set varies significantly. This allows us to examine how the accuracy of our results depends on the distribution of measurements across all stars. Additionally, the derived formula serves as a valuable tool for optimizing observational strategies by minimizing the uncertainty in the resulting estimates. In particular, we demonstrate that it may be beneficial to use only a limited number of available data points when calculating the coefficients of the model, while utilizing most or all data points to estimate the initial few derivatives. Lastly, we apply our method to simulated images from the Hipparcos astrometric satellite mission.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with  Fe/H  < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 Å and 4130 Å we also see evidence for an unidentified feature near 3970 Å which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction.\nThe discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.",
        "watermark_text": "We report the observation of beryllium ( Be ) tracks in two ultra - low metallicity halo stars , CS 22892 - 052 and HE 0107 - 5240 . These are the first detections of Be in metal - scarce halo stars with Fe / H < - 2 . 5 dex .We see that these stars have high surface gravities for their temperatures , showing they may be blue stragglers or other evolution bodies . In addition to the Be properties at 4131 Å and 4130 Å we also find proof for an unknown spot near 3970 Å which is probably due to C + N + O .This work was supported by NASA grant NAG5 - 9998 . Keywords : Beryllium ; Blue straggler ; Metal poor star ; Ultracool dwarf .1 . Introduction .The observation of incredibly poor - density stars has opened up new avenues into studying how planets organize around very cool dwarfs . However , there exists much uncertainty about the formation transition itself as well as the chemical composition of such systems .One important dimension of this question involves establishing whether or not terrestrial planet development can occur within the habitable zone of ultracool dwarfs . To address this question it will be required to study if the atmospheres of these planets contain significant amounts of heavy components like carbon , nitrogen , oxygen , hydrogen , potassium , potassium , magnesium , iron , silicon , potassium , titanium , iron , nickel , cobalt , copper , zinc , arsenic , selenium , platinum , gold , mercury , lead , uranium , thorium , and plutonium .It should be mentioned that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars .",
        "rewrite_text": "We present the discovery of beryllium (Be) tracks in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240. These detections mark the first instances of Be found in metal-poor halo stars with iron-to-hydrogen ratios (Fe/H) less than -2.5 dex. The observed stars exhibit high surface gravities relative to their temperatures, suggesting they may be blue stragglers or other evolved entities. In addition to the beryllium features at 4131 Å and 4130 Å, we also identify an intriguing signal near 3970 Å that likely corresponds to contributions from carbon, nitrogen, and oxygen. This research was funded by NASA grant NAG5-9998. \n\nKeywords: Beryllium; Blue straggler; Metal-poor star; Ultracool dwarf. \n\n1. Introduction: The study of extremely low-metallicity stars has opened new pathways to understanding planet formation around ultracool dwarfs. However, there remains significant uncertainty regarding the mechanisms of formation and the chemical composition of these systems. A crucial aspect of this inquiry is determining whether terrestrial planet formation can occur within the habitable zones of ultracool dwarfs. To explore this, we must investigate whether the atmospheres of such planets contain substantial amounts of heavy elements, including carbon, nitrogen, oxygen, hydrogen, potassium, magnesium, iron, silicon, titanium, nickel, cobalt, copper, zinc, arsenic, selenium, platinum, gold, mercury, lead, uranium, thorium, and plutonium. It is important to note that while some of these metals are produced through stellar nucleosynthesis, others are created via cosmic ray spallation reactions occurring in the interstellar medium.",
        "ori-fast-z-score": -0.47891314261057566,
        "water-fast-z-score": 5.510397987560282,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "We present an exact expression for the probability distribution function (PDF) of the number of steps taken by a one-dimensional, discrete-time, self-propelled particle that moves in a periodic potential and interacts with itself via elastic collisions.  We show how this PDF can be used to calculate the mean-square displacement as well as other statistical properties of such particles. The results are illustrated using numerical simulations. \nPACS numbers: 05.45.-a; 05.70.Jk; 05.60.Gg \nI. INTRODUCTORY REMARkS\nThe motion of many biological systems is often described as being driven by internal forces or active processes  1  . Examples include bacteria swimming through fluids  2  , cells crawling on surfaces  3  , and molecular motors moving along cytoskeletal filaments  4  .\nIn recent years there has been growing interest in understanding the dynamics of these active particles  5  -  8  . In particular, it was shown that their behavior may differ significantly from that observed in passive Brownian particles  9  -  11  . For example, while the latter exhibit normal diffusion at large timescales  12  , active particles typically display superdiffusive  13  or even ballistic  14  transport depending on the details of their interactions  15  -  17  . This difference arises because active particles have additional degrees of freedom which allow them to explore more efficiently the available space  18  . As a result they tend to move faster than passive particles  19  .\nRecently we introduced a model describing the motion of a single active particle  20  . It consists of a point-like object that performs a biased random walk in a periodic potential  21  . Its position x(t + 1) = x(t) + v t+1 − v t is determined by its velocity v t+1 = f  x(t), v t   where f  ·  denotes some deterministic force acting upon the particle  22  . Here we consider two different types of potentials V (x). First, when V (x) ∝ cos(2πx/L) (L is the periodicity length), the system exhibits a series of metastable states separated by energy barriers  23  . Second",
        "watermark_text": "We present an precise representation for the probability distribution function ( PDF ) of the number of steps took by a one - dimensional , discrete - time , self - propelled object that moves in a periodic potential and interacts with itself via elastic collisions . We see how this PDF can be used to estimate the mean - square displacement as well as other mathematical properties of such particles .The results are shown using numerical simulations . PACS codes : 05 . 45 . - a ; 05 . 70 . Jk ; 05 . 60 . Gg I .INTRODUCTORY REMARkS The movement of several biological systems is often characterized as being driven by inner forces or active pathways 1 . Examples involve bacteria walking through substances 2 , bacteria crawling on surfaces 3 , and molecular motors moving along cytoskeletal filaments 4 .In recent years there has been growing interest in understanding the dynamics of these active grains 5 - 8 . In particular , it was shown that their motion might variation significantly from that detected in passive Brownian grains 9 - 11 .For instance , while the former exhibit normal convection at large timescales 12 , active grains typically demonstrate superdiffusive 13 or even ballistic 14 transport varying on the details of their interactions 15 - 17 . This difference arises because active particles have additional degrees of liberty which allow them to study more efficiently the available space 18 .As a result they tend to move faster than inactive particles 19 . Recently we presented a theory explaining the movement of a single active molecule 20 .It consists of a point - like particle that conducts a biased random walk in a periodic potential 21 . Its position x ( t + 1 ) = x ( t ) + v t + 1 − u t is chosen by its velocity v t + 1 = f x ( t ) , v t where f · denotes some deterministic force acting upon the particle 22 .Here we define two different kinds of potentials V ( x ) . First , when V ( x ) [UNK] cos ( 2πx / L ) ( L is the periodicity long ) , the system displays a sequence of metastable states separated by energy barriers 23 .Second",
        "rewrite_text": "We provide an accurate formulation for the probability distribution function (PDF) that describes the number of steps taken by a one-dimensional, discrete-time, self-propelled object moving within a periodic potential while experiencing elastic collisions with itself. This PDF can be utilized to estimate the mean-square displacement and other mathematical properties of such particles, with results illustrated through numerical simulations. PACS codes: 05.45.-a; 05.70.Jk; 05.60.Gg. \n\n**INTRODUCTORY REMARKS** The movement of various biological systems is often attributed to internal forces or active pathways. Examples include bacteria navigating through media, bacteria migrating across surfaces, and molecular motors traversing along cytoskeletal filaments. Recently, there has been an increasing interest in examining the dynamics of these active entities. Notably, it has been demonstrated that their motion may differ significantly from that of passive Brownian particles. For instance, while the latter typically shows normal convection over large timescales, active particles generally exhibit superdiffusive or even ballistic behavior, depending on the specifics of their interactions. This distinction arises because active particles possess additional degrees of freedom that enable them to explore their environment more effectively, resulting in a faster movement compared to inactive particles. We have recently introduced a theory that elucidates the motion of a single active molecule, modeled as a point-like particle undergoing a biased random walk in a periodic potential. The position \\( x(t + 1) = x(t) + v_{t + 1} - u_t \\) is determined by its velocity \\( v_{t + 1} = f(x(t), v_t) \\), where \\( f(\\cdot) \\) represents a deterministic force acting on the particle. We define two distinct types of potentials \\( V(x) \\). The first, \\( V(x) = \\cos(2\\pi x / L) \\) (where \\( L \\) indicates the periodic length), leads to a series of metastable states separated by energy barriers. The second potential is...",
        "ori-fast-z-score": -0.8219949365267865,
        "water-fast-z-score": 7.562353416046435,
        "rewrite-fast-z-score": 3.3567254331867566
    },
    {
        "original_text": "We present results on variable X-ray emission from the central parsecs (0.1 pc) around Sgr A*, which is associated with hot plasma ejected by young massive stars near the supermassive black hole at the Galactic Centre. We find that the variability timescale decreases as we move towards higher energies. The observed power spectrum can be explained if there are two components contributing to the total flux - one steady component and another varying component. This suggests that the source of the X-rays may not be point-like but extended. Our analysis also shows that the luminosity changes significantly over time scales ranging between hours and years. These variations could be due to either intrinsic or extrinsic factors such as orbital motion of the emitting region and/or obscuration effects caused by intervening clouds. In addition, we have found evidence for an anti-correlation between the soft and hard bands during flares. This indicates that the spectral shape varies along with its intensity.",
        "watermark_text": "We report findings on variable X - ray radiation from the central parsecs ( 0 . 1 pc ) around Sgr A * , which is associated with hot plasma expelled by young massive galaxies near the supermassive black hole at the Galactic Centre . We see that the variability timescale decreases as we move towards higher energies .The observed power spectrum can be understood if there are two parts contributing to the total flux - one steady component and another varying component . This implies that the origin of the X - radiation may not be point - like but extended .Our study also shows that the luminosity shifts significantly over time ranges ranging between hours and years . These changes could be due to either intrinsic or extrinsic factors such as orbital movement of the emitting area and / or obscuration effects caused by intervening clouds .In addition , we have discovered evidence for an counter - correlation between the soft and hard bands during flares . This implies that the absorption form varies along with its brightness .",
        "rewrite_text": "We present our findings on the variability of X-ray radiation from the central parsecs (0.1 pc) surrounding Sgr A*, which is linked to hot plasma ejected by young massive galaxies near the supermassive black hole at the Galactic Center. Our observations indicate that the variability timescale decreases as the energy increases. The observed power spectrum can be explained by the presence of two components contributing to the total flux: a steady component and a varying one. This suggests that the source of the X-ray radiation may be extended rather than point-like. Furthermore, our study reveals significant fluctuations in luminosity over time spans ranging from hours to years. These variations could result from intrinsic or extrinsic factors such as the orbital movement of the emitting region or obscuration caused by intervening clouds. Additionally, we found evidence of a counter-correlation between the soft and hard bands during flares, indicating that the absorption changes in tandem with brightness variations.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "We present the first X-ray observations of the super soft source (SSS) in the recurrent nova RS Ophiuchi using data obtained by the Chandra and XMM- Newton observatories during their recent outbursts. The SSS was detected at all epochs, but its luminosity varied significantly between them. We find that the temperature of the SSS is consistent with being constant within errors for each observation epoch. However, we detect significant changes in the emission measure which are correlated to the optical light curve. These results suggest that the mass loss rate from the white dwarf varies on timescales as short as days. This may be due to an unstable nuclear burning shell or possibly accretion disk instabilities. In addition, we report the detection of two absorption features near 1 keV in our Chandra spectrum taken on day +6 after outburst maximum. They can be identified with Fe XXV Kα and Fe XXVI Lyα lines produced in the expanding ejecta.",
        "watermark_text": "We present the first X - ray observations of the super soft source ( SSS ) in the recurrent nova RS Ophiuchi using data acquired by the Chandra and XMM - Newton observatories during their recent outbursts . The SSS was seen at all epochs , but its luminosity differed dramatically between them .We see that the temperature of the SSS is compatible with being steady within errors for each observation epoch . However , we find considerable changes in the emission measure which are correlated to the optical light spiral .These data suggest that the mass loss rate from the white dwarf varies on timescales as short as days . This might be due to an weak nuclear burning shell or possibly accretion belt instabilities .In addition , we note the observation of two spectral features near 1 keV in our Chandra spectrum taken on day + 6 after outburst peak . They can be identified with Fe XXV Kα and Fe XXVI Lyα bands created in the evolving ejecta .",
        "rewrite_text": "We report the first X-ray observations of the super soft source (SSS) in the recurrent nova RS Ophiuchi, utilizing data gathered from the Chandra and XMM-Newton observatories during recent outbursts. The SSS was detected at all observation points, although its luminosity varied significantly across these epochs. Our analysis indicates that the temperature of the SSS remains stable within the error margins for each observation period. However, we observe substantial fluctuations in the emission measure, which correlate with the optical light curve. This data implies that the mass loss rate from the white dwarf changes on timescales as short as several days, potentially due to a weak nuclear burning shell or instabilities in the accretion belt. Additionally, we observed two spectral features near 1 keV in the Chandra spectrum taken six days after the peak of the outburst. These features are identified as Fe XXV Kα and Fe XXVI Lyα lines produced by the evolving ejecta.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 5.163977794943222,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We have calculated the ab initio melting curves for Mo and W using the phase-coexistence method with the generalized gradient approximation (GGA) to density functional theory (DFT). The results are compared with those obtained in previous studies, as well as experimental data on the melting points. We find that our GGA calculations give good agreement with experiment at high temperatures but underestimate the melting temperature significantly below 2000 K. This is probably due to anharmonic effects which we do not take into account here. In addition, we show how the electronic structure changes across the melting transition. \n \n Keywords: Molten metal, Phase diagram, Melting point, Ab initio calculation \n \n \n \n 1 Introduction \n \n It has been known since the early days of quantum mechanics that the properties of matter can be described accurately within this framework only if electron-electron interactions are taken into account explicitly  1  . However, it was soon realized that even simple approximations such as Hartree-Fock or DFT yield useful information about many physical phenomena  2  , including solid-state physics  3  .\n \nIn recent years there has been considerable interest in applying first-principles methods to calculate the thermodynamic properties of materials  4  . These include free energies  5  , phonon frequencies  6  , elastic constants  7  , surface tensions  8  , and other quantities  9  . One important application of these techniques is the prediction of the melting behaviour of solids  10  -  12  . For example, the melting temperature T m of metals can be determined directly from the Gibbs energy difference between the liquid and solid phases  13  :",
        "watermark_text": "We have calculated the ab initio melting curves for Mo and W utilizing the phase - coexistence method with the generalized gradient approximation ( GGA ) to density functional theory ( DFT ) . The results are compared with those achieved in earlier studies , as well as empirical data on the melting points .We see that our GGA measurements give good agreement with research at high temperatures but underestimate the melting temperature significantly below 2000 K . This is probably due to anharmonic effects which we do not take into consideration here . In addition , we find how the chemical structure shifts across the melting transition .Keywords : Molten material , Phase diagram , Melting point , Ab initio method 1 Introduction It has been known since the early days of quantum mechanics that the properties of matter can be described properly within this framework only if electron - atom relationships are took into consideration explicitly 1 . However , it was swiftly understood that even basic approximations such as Hartree - Fock or DFT yield useful details about various physical phenomena 2 , notably solid - state mechanics 3 .In recent years there has been substantial interest in implementing first - principles techniques to estimate the thermodynamic properties of substances 4 . These include free temperatures 5 , phonon frequencies 6 , elastic constants 7 , surface tensions 8 , and other quantities 9 .One important use of these method is the determination of the melting behaviour of solids 10 - 12 . For instance , the melting temperature T m of metals can be determined directly from the Gibbs energy relationship between the liquid and solid phases 13 :",
        "rewrite_text": "We calculated the ab initio melting curves for molybdenum (Mo) and tungsten (W) using the phase coexistence method along with the generalized gradient approximation (GGA) within density functional theory (DFT). Our findings are compared to previous studies as well as empirical data on melting points. Our GGA results align well with high-temperature research, although they significantly underestimate the melting temperature below 2000 K. This discrepancy likely arises from anharmonic effects that are not accounted for in our analysis. Additionally, we observed how the chemical structure changes during the melting transition. \n\n**Keywords:** Molten material, phase diagram, melting point, ab initio method\n\n**1 Introduction**  \nIt has been established since the early era of quantum mechanics that the properties of matter are accurately described only when considering the electron-atom interactions explicitly. However, it was quickly recognized that even fundamental approximations like Hartree-Fock or DFT provide valuable insights into various physical phenomena, particularly in solid-state mechanics. Recently, there has been significant interest in applying first-principles techniques to estimate the thermodynamic properties of materials. These techniques encompass a range of properties including free energies, phonon frequencies, elastic constants, surface tensions, and more. One of the notable applications of these methods is in determining the melting behavior of solids. For example, the melting temperature (T_m) of metals can be directly derived from the Gibbs energy relationship between the liquid and solid phases.",
        "ori-fast-z-score": 0.3713906763541037,
        "water-fast-z-score": 7.427813527082075,
        "rewrite-fast-z-score": 3.779644730092272
    },
    {
        "original_text": "We present an analytical model for high-order harmonic generation (HHG) in diatomic molecules, which is based on the concept of molecular orbitals and their associated electronic wavefunctions. The HHG process can be understood as a sequence of three steps: First, electrons are ionized by strong laser fields. Second, they propagate through the continuum until rescattering with parent ions occurs. Third, these returning electrons emit high harmonics when interacting again with the driving field. We show that this picture leads to a simple expression for the emitted harmonic intensity, which depends only on two parameters characterizing the molecule s orbital structure. This result allows us to explain the observed interference patterns between different harmonics in terms of destructive or constructive interferences between contributions from different molecular orbits. In addition, we demonstrate how our approach can be used to predict the emission properties of new types of molecules. High-order harmonic generation (HHG), i.e., the coherent emission of photons at odd multiples of the fundamental frequency of intense femtosecond laser pulses, has attracted considerable interest over recent years  1, 2  . It provides access to extreme ultraviolet radiation  3  , which enables novel applications such as attosecond pulse generation  4  , photoelectron spectroscopy  5  , and tomography  6  .\nThe underlying physical mechanism behind HHG was first explained within the semiclassical three-step model  7, 8  : An electron tunnels out of its atomic core into the continuum upon interaction with the electric field of the laser light. Afterwards it propagates freely before being driven back towards the nucleus by the same field. Finally, it recombines with the parent ion emitting a photon whose energy equals the sum of the kinetic energy gained during propagation and the binding energy lost due to tunneling  9  . Since then, several extensions have been developed  10  including the so-called quantum-orbit theory  11  , which takes into account the influence of the nuclear potential on the electron dynamics  12  . However, despite all efforts made so far, there still exist many open questions regarding the microscopic origin of HHG  13  .",
        "watermark_text": "We present an analytical theory for high - order harmonic production ( HHG ) in diatomic molecules , which is based on the idea of molecular orbitals and their accompanying electronic wavefunctions . The HHG process can be understood as a sequence of three stages : First , electrons are ionized by intense laser fields .Second , they propagate through the continuum until rescattering with mother ions happens . Third , these returning electrons emit large harmonics when interacting again with the driving field .We see that this picture leads to a simple expression for the emitted harmonic intensity , which depends only on two parameters characterizing the molecule s orbital structure . This result allows us to explain the seen interference patterns between various harmonics in terms of destructive or constructive interferences between contributions from different molecular orbits .In addition , we prove how our approach can be used to predict the emission behavior of new types of molecules . High - order harmonic production ( HHG ) , i . e . , the coherent emission of photons at odd multiples of the fundamental frequency of active femtosecond infrared signals , has garnered considerable interest over recent periods 1 , 2 .It provides entry to extreme ultraviolet radiation 3 , which enables novel applications such as attosecond pulse production 4 , photoelectron spectroscopy 5 , and tomography 6 . The fundamental physical process behind HHG was first explained within the semiclassical three - stepping model 7 , 8 : An electron tunnels out of its atomic backbone into the continuum upon interference with the electric field of the laser light .Afterwards it propagates freely before being driven back towards the nucleus by the same field . Finally , it recombines with the parent ion emitting a photon whose power equals the sum of the kinetic power gained during propagation and the binding energy gain due to tunneling 9 .Since then , various extensions have been created 10 including the so - called quantum - orbit concept 11 , which gives into consideration the impact of the atomic potential on the electron mechanics 12 . However , despite all efforts made so far , there still appear many open questions regarding the microscopic ancestry of HHG 13 .",
        "rewrite_text": "We propose an analytical framework for high-order harmonic generation (HHG) in diatomic molecules, grounded in the concept of molecular orbitals and their corresponding electronic wavefunctions. The HHG process can be delineated into three stages: Initially, electrons are ionized by strong laser fields. Next, these electrons travel through the continuum until they encounter the parent ions again. Finally, upon returning, these electrons emit high-order harmonics through interactions with the driving field. This visualization yields a straightforward expression for the emitted harmonic intensity, which is determined solely by two parameters that characterize the molecular orbital structure. This insight enables us to elucidate the observed interference patterns between different harmonics as a result of constructive or destructive interferences arising from contributions of various molecular orbits. Additionally, we demonstrate that our approach can be applied to predict the emission characteristics of novel molecular types. High-order harmonic generation (HHG)—the coherent emission of photons at odd multiples of the fundamental frequency produced by femtosecond infrared signals—has received significant attention recently. It serves as a gateway to extreme ultraviolet radiation, facilitating innovative applications such as attosecond pulse generation, photoelectron spectroscopy, and imaging techniques. The foundational physical mechanism underlying HHG was initially described using the semiclassical three-stepping model, where an electron tunnels from its atomic core into the continuum due to the influence of the laser's electric field. It then propagates freely before being pulled back towards the nucleus by the same field, ultimately recombining with the parent ion and emitting a photon, with its energy corresponding to the kinetic energy acquired during propagation plus the binding energy from tunneling. Since then, various models have been proposed, including the quantum-orbit concept, which considers the effects of atomic potentials on electron dynamics. Despite these advancements, numerous questions regarding the microscopic origins of HHG remain unanswered.",
        "ori-fast-z-score": 0.8111071056538127,
        "water-fast-z-score": 7.786628214276602,
        "rewrite-fast-z-score": -0.7526178090063816
    },
    {
        "original_text": "The aim of this study was to investigate the effect of novel antimicrobial cyclic peptides, named microbe-derived cationic ring structures (MCRs), on cell proliferation and apoptosis induction in A549 non-small-cell lung carcinoma cells. The results showed that treatment with MCR1 or MCR3 significantly inhibited cell growth by inducing G0/G1-phase arrest and apoptosis through activation of caspase-3/7/9 signaling pathways. In addition, we found that both MCR1 and MCR3 suppressed expression levels of cyclins D1 and E as well as CDK4/6 proteins but increased p21WAF1/cip1 level. Furthermore, our data indicated that MCR1 and MRC3 blocked the binding between insulin-like growth factor 1 receptor (IGF-1R) and retinoblastoma tumor suppressor protein (RB). These findings suggest that MCR1 and 3 may be potential therapeutic agents for treating lung cancers.",
        "watermark_text": "The goal of this study was to examine the impact of new antimicrobial cyclic peptides , designated microbe - derived cationic ring structures ( MCRs ) , on cell development and apoptosis activation in A549 non - large - cell lung carcinoma cells . The results suggested that treatment with MCR1 or MCR3 significantly inhibited cell development by inducing G0 / G1 - phase arrest and apoptosis through activation of caspase - 3 / 7 / 9 signaling pathways .In addition , we reported that both MCR1 and MCR3 reduced transcription rates of cyclins D1 and E as well as CDK4 / 6 factors but improved p21WAF1 / cip1 level . Furthermore , our information shown that MCR1 and MRC3 blocked the interaction between insulin - like growth factor 1 receptor ( IGF - 1R ) and retinoblastoma tumor suppressor protein ( RB ) .These data suggest that MCR1 and 3 might be possible therapeutic agents for controlling lung cancers .",
        "rewrite_text": "This study aimed to investigate the effects of novel antimicrobial cyclic peptides, labeled as microbe-derived cationic ring structures (MCRs), on cell growth and apoptosis in A549 non-small cell lung carcinoma cells. The findings indicated that treatment with MCR1 or MCR3 significantly inhibited cell growth by causing G0/G1 phase arrest and triggering apoptosis through the activation of the caspase-3/7/9 signaling pathways. Additionally, we observed that both MCR1 and MCR3 decreased the transcription levels of cyclins D1 and E, along with CDK4/6 factors, while increasing the levels of p21WAF1/cip1. Moreover, our data revealed that MCR1 and MCR3 obstructed the interaction between the insulin-like growth factor 1 receptor (IGF-1R) and the retinoblastoma tumor suppressor protein (RB). These results suggest that MCR1 and MCR3 could serve as potential therapeutic agents for the treatment of lung cancer.",
        "ori-fast-z-score": -0.674199862463242,
        "water-fast-z-score": 5.715476066494082,
        "rewrite-fast-z-score": 0.2721655269759087
    },
    {
        "original_text": "We compare the magnetic flux distribution in coronal holes (CHs) with that in quiet regions using vector magnetograms observed by Hinode/SOT/SP. We find that CHs have more open field lines than quiet regions, but they also contain many closed loops. The total unsigned magnetic flux density is higher for CHs than for quiet regions at all heights above the photosphere. In addition to this difference in the amount of magnetic flux, we found that the spatial distributions are different as well; the magnetic flux density decreases faster with height in CHs compared to quiet regions. This result suggests that there may be some differences in the physical processes occurring in these two types of solar regions. Keywords: Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region. 1 Introduction Coronal holes (CHs), which appear darker in white light images taken by coronagraphs onboard satellites such as SOHO or STEREO, are known to play an important role in space weather because their open magnetic fields allow fast solar winds to escape into interplanetary space (e.g., Wang et al. (1998) , Cranmer & van Ballegooijen (2005) ).\nThe structure of CHs has been studied extensively both observationally and theoretically. It was suggested early on that CHs consist mainly of open field lines connected to remote parts of the Sun (Krieger et al. (1971) ), while closed loops were rarely seen inside them (Wiegelmann et al. (2010a) ). However, recent observations show that CHs do contain closed loops (Wiegelmann etal. (2010b) , Parnell et al. (2011 ), DeForest et al. (2013 , Brooks et al. (2014) ). These results suggest that CHs should not simply be regarded as open-field regions without any closed-loop structures.",
        "watermark_text": "We relate the magnetic flux spread in coronal holes ( CHs ) with that in quiet regions using vector magnetograms observed by Hinode / SOT / SP . We see that CHs have more open field lines than quiet regions , but they still hold several shut rings .The total unsigned magnetic flux concentration is higher for CHs than for calm regions at all heights above the photosphere . In addition to this contrast in the quantity of magnetic flux , we reported that the spatial distributions are distinct as well ; the magnetic flux concentration drops quicker with depth in CHs compared to quiet regions .This result suggests that there may be some variations in the physical processes arising in these two kind of sun areas . Keywords : Solar corona , Vector magnetogram , Open field system , Closed loop , Coronal hole , Quiet region .1 Introduction Coronal holes ( CHs ) , which appear darker in white light pictures taken by coronagraphs onboard satellites such as SOHO or STEREO , are known to play an important role in space weather because their open magnetic fields allow strong sun winds to escape into interplanetary space ( e . g . , Wang et al . ( 1998 ) , Cranmer & van Ballegooijen ( 2005 ) ) .The structure of CHs has been studied frequently both observationally and theoretically . It was suggested early on that CHs consist mostly of open field lines linked to remote parts of the Sun ( Krieger et al .( 1971 ) ) , while closed loops were seldom visible inside them ( Wiegelmann et al . ( 2010a ) ) .However , recent observations show that CHs do contain shut loops ( Wiegelmann etal . ( 2010b ) , Parnell et al .( 2011 ) , DeForest et al . ( 2013 , Brooks et al .( 2014 ) ) . These conclusions show that CHs should not simply be regarded as open - field regions without any closed - loop fields .",
        "rewrite_text": "We examine the distribution of magnetic flux in coronal holes (CHs) in relation to quiet regions, utilizing vector magnetograms acquired from Hinode/SOT/SP. Our findings indicate that CHs exhibit a greater number of open field lines compared to quiet regions, yet they still contain several closed loops. Moreover, the total unsigned magnetic flux concentration is consistently higher in CHs than in quiet areas across all altitudes above the photosphere. Additionally, we observe that the spatial distribution of magnetic flux differs between the two regions; specifically, the magnetic flux concentration decreases more rapidly with depth in CHs than in quiet regions. This observation hints at potential differences in the physical processes occurring in these two types of solar regions. \n\n**Keywords**: Solar corona, Vector magnetogram, Open field system, Closed loop, Coronal hole, Quiet region.\n\n**1 Introduction**: Coronal holes (CHs), which appear darker in white light images captured by coronagraphs on satellites like SOHO or STEREO, are recognized for their significant impact on space weather, as their open magnetic fields facilitate the escape of strong solar winds into interplanetary space (e.g., Wang et al. 1998, Cranmer & van Ballegooijen 2005). The structure of CHs has been extensively studied both observationally and theoretically. Early theories proposed that CHs primarily consist of open field lines connected to distant regions of the Sun (Krieger et al. 1971), with closed loops rarely observed within them (Wiegelmann et al. 2010a). However, recent observations have revealed that CHs do, in fact, contain closed loops (Wiegelmann et al. 2010b, Parnell et al. 2011, DeForest et al. 2013, Brooks et al. 2014). These findings indicate that CHs should not be solely classified as open-field regions devoid of closed-loop fields.",
        "ori-fast-z-score": -0.44367825470805694,
        "water-fast-z-score": 5.833630944789017,
        "rewrite-fast-z-score": -1.191759143062248
    },
    {
        "original_text": "We present the results of multi-wavelength studies towards southern hot molecular cores (HMCs) using ammonia, water vapour, and continuum data at 1.3 cm and 3 mm wavelengths obtained with the Australia Telescope Compact Array (ATCA). The HMCs are associated with high-mass star formation regions in which massive young stellar objects have been detected through infrared surveys. We find that all our targets show compact emission features on scales of 0.1 pc or less. These sources appear to be dense clumps embedded within larger-scale structures extending up to several parsecs. In addition, we detect extended emission components surrounding these compact clumps. Our analysis shows that the physical conditions derived for the compact clumps are consistent with those expected for hot molecular gas heated by protostellar outflows. On the other hand, the properties of the extended emission suggest that it is likely tracing an envelope around each source.",
        "watermark_text": "We present the conclusion of dual - wavelength experiments towards southern bright molecular cores ( HMCs ) using ammonia , sea vapour , and continuum data at 1 . 3 cm and 3 cm wavelengths collected with the Australia Telescope Compact Array ( ATCA ) . The HMCs are related with high - density star formation regions in which giant older stars objects have been detected through infrared observations .We see that all our targets exhibit small emitted features on scales of 0 . 1 pc or less . These sources appear to be dense clumps scattered within wider - scale structures spanning up to several parsecs .In addition , we find extended emitted elements surrounding these compact clumps . Our study shows that the physical conditions generated for the compact clumps are compatible with those expected for hard molecular vapor heated by protostellar outflows .On the other hand , the properties of the extended emitted suggest that it is probably tracing an envelope around each source .",
        "rewrite_text": "We present the conclusions of dual-wavelength experiments focusing on southern bright molecular cores (HMCs) using ammonia, water vapor, and continuum data collected at 1.3 cm and 3 cm wavelengths with the Australia Telescope Compact Array (ATCA). The HMCs are associated with high-density star formation regions where large, older stellar objects have been identified through infrared observations. Our analysis shows that all of our targets exhibit small emission features on scales of 0.1 parsecs or less. These sources appear as dense clumps dispersed within larger structures that extend over several parsecs. Additionally, we observe extended emission surrounding these compact clumps. Our study indicates that the physical conditions within the compact clumps align with those expected from hot molecular vapor heated by protostellar outflows. Conversely, the characteristics of the extended emission suggest that it likely traces an envelope encasing each source.",
        "ori-fast-z-score": -1.9896995023342199,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "We present an algorithm for decomposing functions defined over quadratic surfaces in three dimensions, such as the surface of a sphere or ellipsoid, into multipole expansions. The method is based on representing the function using spherical harmonics and then expanding each term in this representation into a sum of products of Legendre polynomials with coefficients that are determined by solving a linear system of equations. We demonstrate our approach through several examples including computing the electrostatic potential due to point charges located at various positions around a dielectric sphere immersed in water. Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space. This work was supported by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry  1  , molecular dynamics  2  , and plasma simulations  3  . In these applications, one often needs to represent a given function f(r) defined over some domain Ω in terms of its expansion coefficients C lm :",
        "watermark_text": "We present an algorithm for decomposing functions defined over quadratic spheres in three dimensions , such as the surface of a sphere or ellipsoid , into multipole expansions . The method is based on representing the function using spherical harmonics and then increasing each term in this representation into a sum of products of Legendre polynomials with coefficients that are decided by solving a linear network of equations .We showed our approach through several examples namely calculation the electrostatic potential due to point charges situated at numerous positions around a dielectric sphere immersed in water . Our results show that we can accurately compute the electrostatic potential even when there are many sources distributed throughout space .This work was supported by NSF grant DMS - 0852653 ( CAREER ) . Spherical harmonic decomposition has been used extensively in computational physics applications ranging from quantum chemistry 1 , molecular dynamics 2 , and plasma simulations 3 .In these uses , one frequently needs to depict a given function f ( r ) established over some domain Ω in terms of its expansion function C lm :",
        "rewrite_text": "We introduce an algorithm for decomposing functions defined on quadratic surfaces in three-dimensional space, such as the surfaces of spheres or ellipsoids, into multipole expansions. This approach involves representing the function using spherical harmonics and transforming each term in this representation into a sum of products of Legendre polynomials, with coefficients determined by solving a system of linear equations. We demonstrated our method through various examples, including the computation of the electrostatic potential generated by point charges located at multiple positions around a dielectric sphere submerged in water. Our findings indicate that we can accurately calculate the electrostatic potential even in scenarios with many sources spread throughout space. This research was supported by NSF grant DMS-0852653 (CAREER). Spherical harmonic decomposition has been widely applied in computational physics, encompassing fields such as quantum chemistry, molecular dynamics, and plasma simulations. In these applications, it is often necessary to express a given function f(r) defined over a domain Ω in terms of its expansion coefficients C_lm.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 3.2627549126854696,
        "rewrite-fast-z-score": 0.22086305214969307
    },
    {
        "original_text": "We present the first detailed analysis of the X-ray spectrum of the Seyfert 1 galaxy NGC 3783 using data obtained by the High Energy Transmission Grating Spectrometer (HETGS) on board the Chandra Observatory. The source was in an active state during our observation, and we find that its emission is dominated by strong absorption lines due to highly ionized species such as O viii-xxii, Ne ix-xiii, Mg xii-xv, Si xiv-xxvi, S xix-xxxi, Ar xxviii-xxxviii, Ca xx-xxxi, Fe xxv-xxvi, and Ni xxviii-xxix. We detect several narrow emission features which are likely associated with resonant scattering of continuum photons off ions located along the line-of-sight towards the central engine. In addition, there appears to be evidence for broad emission components at energies above 10 keV.",
        "watermark_text": "We present the first detailed analysis of the X - ray spectrum of the Seyfert 1 galaxy NGC 3783 using data acquired by the High Energy Transmission Grating Spectrometer ( HETGS ) on board the Chandra Observatory . The source was in an active condition during our observation , and we find that its emission is dominated by weak absorption lines due to strongly ionized species such as O viii - xxii , Ne ix - xiii , Mg xii - xv , Si xiv - xxvi , S xix - xxxi , Ar xxviii - xxxviii , Ca xx - xxxi , Fe xxv - xxvi , and Ni xxviii - xxix .We detect many narrow radiation elements which are likely correlated with resonant scattering of continuum photons off ions located along the line - of - view towards the main engine . In addition , there seems to be confirmation for wide absorption elements at energies above 10 keV .",
        "rewrite_text": "We provide the first comprehensive analysis of the X-ray spectrum from the Seyfert 1 galaxy NGC 3783, utilizing data obtained from the High Energy Transmission Grating Spectrometer (HETGS) aboard the Chandra Observatory. During our observations, the source was in an active state, revealing that its emission is primarily characterized by weak absorption lines resulting from highly ionized species such as O viii - xxii, Ne ix - xiii, Mg xii - xv, Si xiv - xxvi, S xix - xxxi, Ar xxviii - xxxviii, Ca xx - xxxi, Fe xxv - xxvi, and Ni xxviii - xxix. We identified numerous narrow radiation elements that are likely associated with the resonant scattering of continuum photons by ions situated along the line of sight to the central engine. Moreover, there appears to be evidence supporting the presence of broad absorption features at energies exceeding 10 keV.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 3.4286375654996775,
        "rewrite-fast-z-score": -0.2886751345948129
    },
    {
        "original_text": "We present the results of our numerical simulations on the shallow-decay phase of GRB X-ray light curves, which are produced by the interaction between an ultra-relativistic jet and its surrounding medium in the framework of the internal shock model for GRBs. We find that this phase is mainly due to the continuous energy injection into the forward shock driven by the expanding bubble formed at the head of the jet. The injected energy comes from the kinetic energy of the swept-up shell material as well as the thermal energy of shocked ambient gas inside the bubble. Our simulation results show good agreement with observations both qualitatively and quantitatively. \n \n Keywords: Gamma-ray bursts (GRBs), Afterglow emission, Relativistic winds, Shock waves, Bubbles, Internal shocks, Wind-driven shells, Energy injection, Light curve modeling \n \n 1 Introduction \n \n In recent years, great progress has been made in understanding the origin of gamma-ray bursts (GRBs; see Piran 2004 , Zhang 2007a . It was found that most GRBs have their prompt emissions followed by a relatively smooth power-law decline lasting several hundred seconds known as the  afterglow  phase (Costa et al. 1997; van Paradijs et al. 1997) . This phase can be explained by synchrotron radiation from electrons accelerated behind the blast wave generated when the ejecta hits the circumburst medium (Sari et al. 1998 ). However, some GRB afterglows exhibit a shallower-than-power law decline during hundreds of seconds before entering the normal afterglow phase (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007) , which cannot be explained within the standard fireball model. Several models were proposed to explain these phenomena, including late-time central engine activity (Zhang 2007b ), refreshed-shock scenario (Ghisellini et al. 2007 ) and reverse shock emission (Kobayashi 2000; Kobayashi & Sari 2001) . Recently, Fan & Wei (2007) suggested that the shallow-decay phase",
        "watermark_text": "We present the conclusion of our numerical simulations on the shallow - decay phase of GRB X - ray light curves , which are produced by the interaction between an ultra - relativistic jet and its neighbouring medium in the framework of the internal shock model for GRBs . We see that this phase is mainly owing to the discrete energy injection into the front shock driven by the increasing bubble formed at the head of the jet .The injected power derives from the kinetic power of the swept - up shell material as also as the thermal energy of shocked ambient gas inside the bubble . Our model results show good agreement with observations both qualitatively and quantitatively .Keywords : Gamma - ray bursts ( GRBs ) , Afterglow emission , Relativistic winds , Shock waves , Bubbles , Internal shocks , Wind - driven shells , Energy injection , Light curve modeling 1 Introduction In recent years , great work has been achieved in understanding the origin of gamma - ray bursts ( GRBs ; seeing Piran 2004 , Zhang 2007a . It was shown that most GRBs have their prompt emissions followed by a fairly gradual power - law decrease lasting several hundred moments known as the afterglow period ( Costa et al .1997 ; van Paradijs et al . 1997 ) .This phase can be understood by synchrotron emission from electrons accelerated behind the explosion wave produced when the ejecta hits the circumburst medium ( Sari et al . 1998 ) .However , some GRB afterglows exhibit a shallower - than - energy law decrease during hundreds of moments before entering the usual afterglow stage ( e . g . , Panaitescu & Kumar 2001 ; Nousek et al . 2006 ; Liang et al .2007 ; Willingale et al . 2007 ) , which lacks be described within the standard fireball model .Several models were offered to explain these phenomena , notably late - time central fuel activity ( Zhang 2007b ) , refreshed - jolt scenario ( Ghisellini et al . 2007 ) and reverse shock emission ( Kobayashi 2000 ; Kobayashi & Sari 2001 ) .Recently , Fan & Wei ( 2007 ) proposed that the shallow - decay phase",
        "rewrite_text": "We present our conclusions from numerical simulations regarding the shallow decay phase of X-ray light curves associated with gamma-ray bursts (GRBs). These light curves arise from the interaction between an ultra-relativistic jet and its surrounding medium, as explained by the internal shock model for GRBs. Our findings indicate that this phase primarily results from discrete energy injections into the front shock, which is driven by the expanding bubble at the jet's forefront. The power injected into the system comes from the kinetic energy of the swept-up shell material, coupled with the thermal energy of the shocked ambient gas within the bubble. Our model's predictions align well with observational data, both qualitatively and quantitatively. \n\n**Keywords:** Gamma-ray bursts (GRBs), Afterglow emission, Relativistic winds, Shock waves, Bubbles, Internal shocks, Wind-driven shells, Energy injection, Light curve modeling. \n\n**1 Introduction** \n\nRecent advancements have significantly enhanced our understanding of gamma-ray bursts (GRBs) (see Piran 2004; Zhang 2007a). It has been established that most GRBs display a prompt emission followed by a gradual power-law decay, known as the afterglow period, which lasts several hundred moments (Costa et al. 1997; van Paradijs et al. 1997). This afterglow phase is attributed to synchrotron emission from electrons accelerated behind the shock wave created when the ejecta interacts with the circumburst medium (Sari et al. 1998). However, some GRB afterglows demonstrate a shallower-than-linear decay rate lasting for hundreds of moments before transitioning into the typical afterglow phase (e.g., Panaitescu & Kumar 2001; Nousek et al. 2006; Liang et al. 2007; Willingale et al. 2007), a behavior that cannot be adequately described by the standard fireball model. Various models have been proposed to clarify these phenomena, including late-time central engine activity (Zhang 2007b), the refreshed-jolt scenario (Ghisellini et al. 2007), and reverse shock emission (Kobayashi 2000; Kobayashi & Sari 2001). Recently, Fan & Wei (2007) suggested that the shallow decay phase...",
        "ori-fast-z-score": -0.5895063447465633,
        "water-fast-z-score": 6.271815075053181,
        "rewrite-fast-z-score": 2.5175440748900675
    },
    {
        "original_text": "We present new results on photolysis-induced chemistry in the upper atmosphere of Titan, based on observations made by Cassini/VIMS (Visible Infrared Mapping Spectrometer) during its T5 flyby of Titan s northern hemisphere. The VIMS data show that there is an extensive haze layer between about 400 km altitude and 1000 km above the surface. This haze has been previously attributed to photochemical production of aerosol particles through reactions involving methane and ethane. We find evidence for additional sources of aerosol particles at higher altitudes. These include products of dissociation of nitrogen molecules produced by solar UV radiation, as well as carbon dioxide and water vapor released from the lower stratosphere into the mesosphere/thermosphere region. Our analysis suggests that these processes may be responsible for up to 50% of the total mass loading of the haze observed near 600 km altitude. The presence of this additional source of aerosol particles could have important implications for understanding atmospheric circulation patterns in the upper atmosphere of Saturn s moon Enceladus.",
        "watermark_text": "We report new data on photolysis - caused chemistry in the higher atmosphere of Titan , based on observations made by Cassini / VIMS ( Visible Infrared Mapping Spectrometer ) during its T5 flyby of Titan s northern hemisphere . The VIMS data reveal that there is an extensive haze layer between about 400 kilometres altitude and 1000 kilometers above the surface .This haze has been previously attributed to photochemical production of aerosol gases through reactions involving methane and ethane . We see evidence for additional sources of aerosol particles at higher altitudes .These include products of dissociation of nitrogen molecules released by sun UV rays , as also as carbon dioxide and water vapor sent from the lower stratosphere into the mesosphere / thermosphere region . Our study shows that these mechanisms may be responsible for up to 50 % of the total mass loading of the fog seen near 600 kilometers altitude .The presence of this added source of aerosol particles might have important implications for studying air convection conditions in the higher atmosphere of Saturn s moon Enceladus .",
        "rewrite_text": "We present new findings on photolysis-induced chemistry in Titan's upper atmosphere, derived from observations made by Cassini's Visible Infrared Mapping Spectrometer (VIMS) during its T5 flyby of Titan's northern hemisphere. The VIMS data uncover a substantial haze layer located between approximately 400 and 1000 kilometers above the surface. This haze has been previously linked to the photochemical formation of aerosol gases resulting from reactions involving methane and ethane. Our observations indicate additional sources of aerosol particles at greater altitudes, including products from the dissociation of nitrogen molecules due to ultraviolet radiation from the sun, as well as carbon dioxide and water vapor transported from the lower stratosphere into the mesosphere and thermosphere. Our research suggests that these processes may account for up to 50% of the total mass loading of the haze observed near 600 kilometers in altitude. The presence of these additional aerosol particle sources may have significant implications for understanding atmospheric convection conditions on Saturn's moon Enceladus.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "We present high-resolution (R = λ/Δλ ~ 10000) near-infrared and mid-infrared spectroscopic observations for two ULIRGs, Mrk 231 and Arp 220. The data were obtained with the Subaru Telescope using the Cooled Mid-Infrared Camera and Spectrograph (COMICS). We detect several emission lines in both objects including H I Brγ at 2.16 μm, Paα at 1.87 μm,  Fe II  at 1.64 μm, He I at 1.70 μm,  S III  at 0.95 μm, and  C IV  at 0.15 μm. In addition to these lines, we also find that there are many absorption features such as CO bandheads near 4.7 μm and 6.2 μm. These results show that the observed spectra have complex line profiles which can be explained by multiple components along our line-of-sight and/or different physical conditions within each component.",
        "watermark_text": "We present high - resolution ( R = λ / Δλ ~ 10000 ) near - infrared and mid - infrared spectroscopic observations for two ULIRGs , Mrk 231 and Arp 220 . The data were obtained with the Subaru Telescope using the Cooled Mid - Infrared Camera and Spectrograph ( COMICS ) .We detect many emission lines in both objects including H I Brγ at 2 . 16 μm , Paα at 1 . 87 μm , Fe II at 1 . 64 μm , He I at 1 . 70 μm , S III at 0 . 95 μm , and C IV at 0 . 15 μm . In addition to these lines , we also find that there are many absorption properties such as CO bandheads near 4 . 7 μm and 6 . 2 μm .These data reveal that the seen spectra have complex line profiles which can be described by various components along our line - of - view and / or varying material conditions within each portion .",
        "rewrite_text": "We present high-resolution (R = λ/Δλ ~ 10,000) near-infrared and mid-infrared spectroscopic observations of two ULIRGs, Mrk 231 and Arp 220. The data were collected using the Cooled Mid-Infrared Camera and Spectrograph (COMICS) at the Subaru Telescope. Our observations reveal multiple emission lines in both galaxies, including H I Brγ at 2.16 μm, Paα at 1.87 μm, Fe II at 1.64 μm, He I at 1.70 μm, S III at 0.95 μm, and C IV at 0.15 μm. Additionally, we identify several absorption features, such as CO bandheads near 4.7 μm and 6.2 μm. The spectra exhibit complex line profiles, suggesting the presence of various components along our line of sight and/or differing material conditions within each region observed.",
        "ori-fast-z-score": 1.2977713690461004,
        "water-fast-z-score": 4.8666426339228765,
        "rewrite-fast-z-score": 1.150792911137501
    },
    {
        "original_text": "We report the discovery and analysis of IGR J16194-2810, an unusual X-ray source discovered by INTEGRAL in 2003. The source is located at RA=16h19m41s DEC=-28d08 00  (J2000) with a positional uncertainty of 1 arcmin radius. It was observed for about 100 ksec on 2004 February 24-25 using XMM-Newton EPIC-pn camera operated in timing mode. We find that this source has a periodicity of P=4.6 hr which we interpret as orbital motion of two compact objects orbiting each other. This interpretation is supported by the presence of strong absorption lines in its optical spectrum indicating the existence of a red giant companion star. Using the measured mass function f(M), we estimate the masses of both components to be Mx=2.7-3.1 M⊙ and My=1.2-1.4 M⊙ respectively.",
        "watermark_text": "We report the discovery and investigation of IGR J16194 - 2810 , an unique X - ray source detected by INTEGRAL in 2003 . The source is situated at RA = 16h19m41s DEC = - 28d08 00 ( J2000 ) with a positional uncertainty of 1 arcmin radius .It was seen for about 100 ksec on 2004 February 24 - 25 using XMM - Newton EPIC - pn sensor run in timing mode . We see that this source has a periodicity of P = 4 . 6 hr which we perceive as orbital movement of two compact entities orbiting each other .This interpretation is backed by the presence of bright absorption lines in its optical spectrum suggesting the existence of a red giant sister star . Using the measured mass distribution f ( M ) , we estimate the masses of both components to be Mx = 2 . 7 - 3 . 1 [UNK] and My = 1 . 2 - 1 . 4 [UNK] respectively .",
        "rewrite_text": "We present the discovery and study of IGR J16194-2810, a unique X-ray source identified by the INTEGRAL satellite in 2003. The source is located at RA = 16h19m41s and DEC = -28d08m00s (J2000), with a positional uncertainty of 1 arcminute. It was observed for approximately 100 kiloseconds from February 24 to 25, 2004, using the XMM-Newton EPIC-pn sensor in timing mode. Our findings indicate that this source exhibits a periodicity of P = 4.6 hours, which suggests the orbital motion of two compact objects revolving around one another. This interpretation is supported by the detection of prominent absorption lines in its optical spectrum, indicating the presence of a companion red giant star. Based on the measured mass distribution f(M), we estimate the masses of the two components to be Mx = 2.7 - 3.1 [UNK] and My = 1.2 - 1.4 [UNK], respectively.",
        "ori-fast-z-score": -0.7142857142857143,
        "water-fast-z-score": 4.041451884327381,
        "rewrite-fast-z-score": 0.2773500981126146
    },
    {
        "original_text": "We present an analysis method for characterizing the stability of optical phase in astronomical instruments, based on the measurement and characterization of fringe contrasts obtained with different integration times.  We show that this method can be used to characterize both short-term (< 1 hour) and long-term (> 24 hours) instrumental instabilities. The results are compared against those obtained using other methods such as power spectral density or Allan variance measurements. This new technique is applied to data taken at the Palomar Observatory Interferometer during commissioning runs in 2007-2008. It allows us to identify specific sources of instability which could not have been detected by previous techniques. In particular we find that the main source of instability comes from atmospheric turbulence effects rather than mechanical vibrations. Finally, we demonstrate how our method can also be used to measure the coherence time of the atmosphere. Keywords: Fringe contrast, Optical interferometry, Instrumentation, Atmospheric turbulence",
        "watermark_text": "We present an assessment method for characterizing the stability of optical phase in astronomical instruments , relying on the observation and description of fringe contrasts obtained with various integration times . We see that this method can be used to characterize both long - term ( < 1 hour ) and long - term ( > 24 hours ) instrumental instabilities .The results are compared against those achieved using other methods such as power spectral coefficient or Allan variance measurements . This new technique is applied to data taken at the Palomar Observatory Interferometer during commissioning running in 2007 - 2008 .It enables us to identify specific sources of tension which could not have been detected by earlier techniques . In particular we find that the main origin of tension comes from ambient turbulence influences rather than structural vibrations .Finally , we prove how our technique can also be used to measure the coherence time of the air . Keywords : Fringe comparison , Optical interferometry , Instrumentation , Atmospheric turbulence",
        "rewrite_text": "We introduce a method for assessing the stability of optical phase in astronomical instruments, based on analyzing the fringe contrasts obtained at various integration times. This approach allows for the characterization of both short-term (< 1 hour) and long-term (> 24 hours) instrumental instabilities. Our findings are benchmarked against results from alternative techniques, including power spectral coefficients and Allan variance measurements. This innovative method was applied to data collected at the Palomar Observatory Interferometer during its commissioning phase from 2007 to 2008. It enabled us to pinpoint specific sources of instability that earlier methods could not detect. Notably, we discovered that the primary source of instability arises from ambient turbulence, rather than structural vibrations. Additionally, we demonstrate that our technique can also measure the coherence time of the atmosphere. \n\nKeywords: Fringe comparison, Optical interferometry, Instrumentation, Atmospheric turbulence.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 5.892556509887896,
        "rewrite-fast-z-score": 1.9694638556693236
    },
    {
        "original_text": "We study the effect of random large-scale forcing on three-dimensional rotating stratified flows, using direct numerical simulations (DNS) with periodic boundary conditions. The flow is forced at large scales by adding to the momentum equation an external force that has zero mean but whose Fourier transform contains both positive and negative wavenumbers. We find that this type of forcing excites two distinct types of modes in the system: vortical and wave-like modes. Vortical modes are characterized by strong vertical motions concentrated near the center of the domain; they have low horizontal velocities and their kinetic energy decays rapidly as we move away from the center. On the other hand, wave-like modes are characterized by weak vertical motions distributed over larger regions of space; they have high horizontal velocities and their kinetic energies decay slowly or even increase slightly when moving away from the center. In addition, these waves can be either stationary or propagating horizontally depending on whether the forcing spectrum peaks at small or large horizontal wavenumber respectively.",
        "watermark_text": "We research the impact of random large - scale forcing on three - dimensional spinning stratified flows , using direct numerical simulations ( DNS ) with periodic border conditions . The flow is displaced at large scales by added to the velocity equation an external force that has zero mean but whose Fourier integral contains both negative and negative wavenumbers .We see that this form of forcing excites two different kinds of modes in the system : vortical and wave - like modes . Vortical modes are characterized by intense vertical motions confined near the center of the domain ; they have low vertical velocities and their kinetic power decays slowly as we move back from the center .On the other hand , wave - like modes are characterized by weak vertical motions spread over larger parts of space ; they have high vertical velocities and their kinetic energies decline slowly or even change slightly when moved away from the center . In addition , these currents can be either static or propagating vertically varying on whether the forcing spectrum peaks at small or large horizontal wavenumber respectively .",
        "rewrite_text": "We investigate the effects of random large-scale forcing on three-dimensional spinning stratified flows through direct numerical simulations (DNS) with periodic boundary conditions. The flow is perturbed at large scales by incorporating an external force into the velocity equation, which has a zero mean but includes both positive and negative wavenumbers in its Fourier integral. Our findings reveal that this type of forcing activates two distinct modes within the system: vortical and wave-like modes. Vortical modes exhibit intense vertical motions concentrated near the center of the domain, characterized by low vertical velocities that decay gradually as one moves away from the center. In contrast, wave-like modes display weaker vertical motions that are spread over a larger spatial area, featuring higher vertical velocities, with their kinetic energies either declining slowly or remaining relatively stable when moving away from the center. Additionally, these currents can either be static or propagate vertically, depending on whether the forcing spectrum is concentrated at small or large horizontal wavenumbers, respectively.",
        "ori-fast-z-score": -2.4379951240146283,
        "water-fast-z-score": 4.133991732024804,
        "rewrite-fast-z-score": 1.4757295747452437
    },
    {
        "original_text": "The self-consistent approach to the description of nuclear matter is reviewed and its application to other systems, such as atomic nuclei or quark-gluon plasma, discussed briefly. The main idea behind this method is that one should not consider only the mean field acting on particles but also take into account fluctuations around it. This leads to an infinite set of coupled equations for all orders of correlation functions which can be solved by truncation at some order. In particular we discuss how the results depend on the choice of the approximation scheme used. We show that the inclusion of higher-order correlations improves agreement with experimental data considerably. Finally, we present our recent results obtained within the framework of the relativistic random phase approximation (RRPA) including up to fourth-order correlations. These calculations are performed using realistic nucleonnucleon interactions derived from chiral effective theory. It turns out that the RRPA results agree well with available experimental information about excited states of medium-heavy nuclei.",
        "watermark_text": "The self - consistent method to the description of nuclear material is reviewed and its use to other systems , such as atomic ions or quark - gluon plasma , discussed momentarily . The main idea behind this process is that one should not take only the mean field acted on electrons but also took into consideration fluctuations around it .This leads to an endless system of coupled equations for all orders of correlation functions which can be answered by truncation at some order . In particular we explain how the results vary on the selection of the approximation scheme used .We see that the inclusion of greater - order correlations improves agreement with observation information significantly . Finally , we present our latest findings obtained within the framework of the relativistic random phase approximation ( RRPA ) covering up to fourth - order correlations .These analyses are performed using accurate nucleonnucleon interactions derived from chiral effective theory . It turns out that the RRPA results agree well with provided experimental evidence about excited states of medium - heavy nuclei .",
        "rewrite_text": "This review explores the self-consistent method used to describe nuclear materials and briefly considers its application to other systems, such as atomic ions and quark-gluon plasma. The central premise of this approach is that it is essential to account not only for the mean field experienced by electrons but also for the fluctuations around that mean field. This consideration leads to a complex system of coupled equations for all orders of correlation functions, which can be tackled through truncation at a certain order. We particularly discuss how the results are influenced by the choice of the approximation scheme. Our findings indicate that including higher-order correlations significantly enhances the alignment of theoretical predictions with observational data. Lastly, we present our recent results obtained using the relativistic random phase approximation (RRPA), which incorporates correlations up to the fourth order. These analyses employ precise nucleon-nucleon interactions derived from chiral effective theory, and we find that the RRPA results align well with experimental evidence concerning the excited states of medium-heavy nuclei.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 6.256562819412349,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "We present the first mid-infrared phase curve observations for an extrasolar planet, WASP-121b (1SWASP J140747.93-394542.7), using Spitzer/IRAC at 3.6 and 4.5 microns. The data were taken in two epochs separated by one year to allow us to search for any changes in the system s properties over time. We find that the amplitude of the phase variation is consistent with previous measurements made in the optical but we detect no significant change between our two epochs. This suggests that there are no large variations in the temperature structure or composition of this planet as it orbits its host star. Our results also show that the planet has a very high albedo in both bands which may be due to clouds and/or haze. These findings have important implications for understanding how planets form and evolve. \n \n Keywords: exoplanet, infrared",
        "watermark_text": "We present the first mid - infrared phase curve measurements for an extrasolar planet , WASP - 121b ( 1SWASP J140747 . 93 - 394542 . 7 ) , using Spitzer / IRAC at 3 . 6 and 4 . 5 microns . The data were took in two epochs separated by one month to allow us to search for any alterations in the system s features over time .We see that the frequency of the phase change is compatible with previous measurements made in the optical but we perceive no major shift between our two epochs . This implies that there are no large changes in the temperature structure or composition of this planet as it orbits its host star .Our results also demonstrate that the planet has a very high albedo in both bands which may be due to clouds and / or haze . These conclusions have important implications for explaining how planets form and evolve .Keywords: exoplanet, infrared",
        "rewrite_text": "We present the first mid-infrared phase curve measurements for the exoplanet WASP-121b (1SWASP J140747.93-394542.7) using Spitzer/IRAC at wavelengths of 3.6 and 4.5 microns. The data were collected over two epochs, one month apart, to investigate any changes in the system's characteristics over time. Our findings indicate that the frequency of phase variations aligns with previous optical measurements, and we observe no significant differences between the two epochs. This suggests that there are no substantial changes in the temperature structure or composition of the planet as it orbits its host star. Additionally, our results indicate that the planet exhibits a very high albedo in both bands, potentially due to the presence of clouds and/or haze. These findings have significant implications for our understanding of planetary formation and evolution.   \nKeywords: exoplanet, infrared",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.2672612419124244
    },
    {
        "original_text": "We consider the non-relativistic limit of an exact solution to the Einstein field equations in vacuum, which is given by the Schwarzschild metric. We show that this metric can be obtained as a special case of the general relativistic Kerr-Schild ansatz for stationary and axisymmetric solutions with spherical symmetry. The corresponding line element reads ds2 = -(1 + 2M/r)dt2 + (1 - 2M/r)(dr2 + r2 dΩ2), where M denotes the mass parameter. In order to obtain the correct Newtonian limit we have to choose the time coordinate such that dt/dτ ~ 1/Mc² holds asymptotically at spatial infinity. This choice leads to the standard form of the Schwarzschild metric in terms of proper time τ. Finally, we discuss some consequences concerning the gravitational redshift and the equivalence principle. The Schwarzschild metric describes the geometry outside a spherically symmetric body in vacuum. It was first derived by Karl Schwarzschild in 1916  1  .",
        "watermark_text": "We consider the non - relativistic limit of an precise answer to the Einstein field equations in vacuum , which is given by the Schwarzschild metric . We see that this metric can be obtained as a special case of the general relativistic Kerr - Schild ansatz for stationary and axisymmetric solutions with spherical symmetry .The equivalent line element reads ds2 = - ( 1 + 2M / r ) dt2 + ( 1 - 2M / r ) ( dr2 + r2 dΩ2 ) , where M describes the mass vector . In order to obtain the appropriate Newtonian limit we have to choose the time coordinate such that dt / dτ ~ 1 / Mc² holds asymptotically at spatial infinity .This choosing results to the standard form of the Schwarzschild metric in terms of proper time τ . Finally , we explain some consequences concerning the gravitational redshift and the equivalence principle .The Schwarzschild metric measures the topology outside a spherically symmetric body in vacuum . It was first developed by Karl Schwarzschild in 1916 1 .",
        "rewrite_text": "We examine the non-relativistic limit of an exact solution to the Einstein field equations in a vacuum, represented by the Schwarzschild metric. This metric can be derived as a specific case of the Kerr-Schild ansatz for general relativistic stationary and axisymmetric solutions exhibiting spherical symmetry. The line element is expressed as ds² = - (1 + 2M/r) dt² + (1 - 2M/r)(dr² + r² dΩ²), where M denotes the mass. To achieve the correct Newtonian limit, we must select the time coordinate so that dt/dτ approaches 1/Mc² asymptotically at spatial infinity. This selection leads to the standard form of the Schwarzschild metric in terms of proper time τ. Finally, we discuss the implications related to gravitational redshift and the equivalence principle. The Schwarzschild metric characterizes the geometry outside a spherically symmetric body in vacuum and was initially formulated by Karl Schwarzschild in 1916.",
        "ori-fast-z-score": 2.1766269588592317,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": 2.363515791475006
    },
    {
        "original_text": "We present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods. \n \n Keywords: Interstellar medium",
        "watermark_text": "We present an assessment of the evolution of interstellar dust grains , using on their height pattern inferred by infrared observations with ISO ( Infrared Space Observatory ) . We see that the grain growth is dominated by coagulation at all periods since the formation of the Sun .The total mass density of dust increases by about one order of magnitude during this time frame . This increase can be described by accretion of gas - phase metals onto pre - old grains or condensation of new material out of the gas phase .In addition to these mechanisms we also consider fragmentation as well as shattering caused to collisions between particles . Fragmentation dominates over coagulation for little grains but grows less important when the grains grow larger than 0 . 1 micrometres .For large grains breaking leads to a reduction in quantity density which counteracts the impact of coagulation . Our results are compatible with previous research utilizing different methods .Keywords: Interstellar medium",
        "rewrite_text": "We provide an evaluation of the evolution of interstellar dust grains, based on their height distribution as inferred from infrared observations conducted with the Infrared Space Observatory (ISO). Our findings indicate that grain growth has been primarily driven by coagulation throughout the history since the Sun's formation. During this period, the total mass density of dust increases by nearly an order of magnitude. This growth can be attributed to the accretion of gas-phase metals onto existing grains, as well as the condensation of new material from the gas phase. In addition to these processes, we also take into account fragmentation and shattering resulting from collisions between particles. While fragmentation is significant for small grains, it becomes less relevant as grains exceed 0.1 micrometers in size. For larger grains, fragmentation leads to a decrease in number density, counteracting the effects of coagulation. Our findings align with prior studies that have employed various methodologies. Keywords: Interstellar medium.",
        "ori-fast-z-score": 0.9701425001453319,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "The response prediction of structural system subject to earthquake motions is very important for the design and construction of buildings in seismic areas, especially when it comes to high-rise building structures. In this study, an artificial neural network (ANN) model was developed by using data obtained through nonlinear dynamic analysis on reinforced concrete frame structure subjected to earthquake ground motion records. The ANN model consists of three layers; input layer, hidden layer with 10 neurons, output layer with one neuron representing maximum inter-story drift ratio. Input variables used are peak ground acceleration, duration time, number of stories, story height, mass density per unit floor area, damping coefficient, yield strength of steel bar, elastic modulus of steel bar, shear wall stiffness, and moment capacity of beam-column joint. To verify the accuracy of the proposed ANN model, results predicted by the ANN were compared with those calculated by nonlinear dynamic analysis program. It can be concluded that the ANN model has good performance in predicting the maximum inter-story drift ratios under various earthquake ground motions.",
        "watermark_text": "The behavior analysis of structural structure exposed to earthquake motions is very important for the design and build of structures in seismic areas , particularly when it comes to large - rising building structures . In this study , an synthetic neural network ( ANN ) model was developed by using data derived through nonlinear dynamic analysis on concrete cement frame building exposed to earthquake ground motion records .The ANN system contains of three layers ; input layer , hidden surface with 10 neurons , output layer with one neuron representing maximum inter - story drag ratio . Input variables utilized are peak ground acceleration , duration time , number of floors , story height , mass density per unit floor area , damping coefficient , yield strength of steel bar , elastic modulus of steel bar , shear floor stiffness , and moment capacity of beam - column joint .To verify the accuracy of the suggested ANN theory , results predicted by the ANN were compared with those estimated by nonlinear dynamic analysis project . It can be assumed that the ANN theory has good success in predicting the maximum inter - story drag ratios under various earthquake ground motions .",
        "rewrite_text": "The analysis of the structural behavior of buildings during earthquake motions is crucial for the design and construction of structures in seismic regions, particularly for high-rise buildings. In this study, we developed a synthetic neural network (ANN) model using data obtained from nonlinear dynamic analyses of concrete cement frame buildings subjected to earthquake ground motion records. The ANN comprises three layers: an input layer, a hidden layer with 10 neurons, and an output layer with a single neuron that represents the maximum inter-story drift ratio. The input variables include peak ground acceleration, duration, number of floors, story height, mass density per unit floor area, damping coefficient, yield strength of the steel bars, elastic modulus of the steel bars, shear stiffness of the floors, and moment capacity of the beam-column joints. To validate the accuracy of the proposed ANN model, the predictions made by the ANN were compared to those obtained from nonlinear dynamic analyses. The results suggest that the ANN model performs well in predicting maximum inter-story drift ratios across various earthquake ground motions.",
        "ori-fast-z-score": 1.0536089137432665,
        "water-fast-z-score": 6.800566625070174,
        "rewrite-fast-z-score": 0.6831300510639733
    },
    {
        "original_text": "We present multiwavelength observations for a sample of X-ray selected star forming galaxies (SFGs) in the Chandra Deep Field-South (CDF-S). The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3, which were detected by both the soft-band (0.5-2 keV) and hard-band (2-8 keV) surveys conducted by the Advanced CCD Imaging Spectrometer on board XMM-Newton. We have obtained optical spectroscopy using the Keck telescope to measure their stellar masses and SFRs as well as near-infrared photometry taken with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction. Our results show that these SFGs are massive systems with M* = 1013 -1014M⊙ at z ~ 2 -3. They also exhibit high specific star-formation rates ranging from 10^(-3) yr-1 to 10^(1) yr-1, indicating intense ongoing star formation activity.",
        "watermark_text": "We present multiwavelength surveys for a sample of X - ray selected star producing galaxies ( SFGs ) in the Chandra Deep Field - South ( CDF - S ) . The sample consists of 16 SFGs with spectroscopic redshifts between 1 and 3 , which were detected by both the hard - band ( 0 . 5 - 2 keV ) and hard - band ( 2 - 8 keV ) observations conducted by the Advanced CCD Imaging Spectrometer on board XMM - Newton .We have achieved visual spectroscopy utilizing the Keck telescope to measure their stellar distances and SFRs as also as near - infrared photometry seen with the Infrared Array Camera aboard Spitzer Space Telescope to estimate dust extinction . Our results show that these SFGs are powerful systems with M * = 1013 - [UNK] at z ~ 2 - 3 .They also display high specific star - formation rates ranging from 10 ^ ( - 3 ) yr - 1 to 10 ^ ( 1 ) yr - 1 , showing intense ongoing star formation activity .",
        "rewrite_text": "We present multiwavelength surveys of a sample of X-ray selected star-forming galaxies (SFGs) located in the Chandra Deep Field-South (CDF-S). This sample includes 16 SFGs with spectroscopic redshifts ranging from 1 to 3, all identified through both hard-band (0.5 - 2 keV) and hard-band (2 - 8 keV) observations conducted by the Advanced CCD Imaging Spectrometer on the XMM-Newton satellite. To determine their stellar masses and star formation rates (SFRs), we utilized visual spectroscopy from the Keck telescope, along with near-infrared photometry obtained from the Infrared Array Camera on board the Spitzer Space Telescope to estimate dust extinction levels. Our findings indicate that these SFGs are highly significant systems with stellar masses around M* = 10^13 - [UNK] at redshifts of approximately 2 to 3. Additionally, they exhibit high specific star formation rates ranging from 10^(-3) yr^(-1) to 10^(1) yr^(-1), indicative of vigorous ongoing star formation activity.",
        "ori-fast-z-score": -0.7001400420140048,
        "water-fast-z-score": 3.780756226875626,
        "rewrite-fast-z-score": -0.13018891098082389
    },
    {
        "original_text": "We investigate whether we can detect anisotropy in quasar H II regions during reionization through their small-scale redshifted 21 cm power spectrum (21-cm PS). In our model, quasars are assumed to be located at peaks of dark matter density fluctuations and ionize surrounding gas with an anisotropic Strömgren sphere whose shape is determined by the local tidal field. By performing numerical simulations for different values of the spin temperature T S , we find that the 21-cm PS has a characteristic peak structure which reflects the shapes of individual H II regions. This peak structure becomes more prominent as T S decreases because the number of neutral hydrogen atoms increases due to the decrease in the brightness temperature difference between the CMB and the 21-cm emission line. Our results suggest that it may be possible to use this peak structure to constrain the value of T S . However, since there exist many other factors affecting the 21-cm PS besides T S , further studies will be needed before drawing any conclusions on its detectability.",
        "watermark_text": "We explore whether we can identify anisotropy in quasar H II regions during reionization through their tiny - scale redshifted 21 cm power spectrum ( 21 - cm PS ) . In our model , quasars are expected to be found at peaks of dark matter density fluctuations and ionize neighboring plasma with an anisotropic Strömgren ball whose shape is chosen by the local tidal field .By conducting numerical simulations for different values of the spin temperature T S , we find that the 21 - cm PS has a typical peak structure which reflects the shapes of different H II regions . This peak structure becomes more prominent as T S drops because the proportion of neutral hydrogen atoms increases owing to the decrease in the brightness temperature difference between the CMB and the 21 - cm absorption line .Our results propose that it could be possible to use this peak structure to constrain the value of T S . However , since there remain many other influences involving the 21 - cm PS besides T S , further studies will be needed before drew any findings on its detectability .",
        "rewrite_text": "We investigate the possibility of identifying anisotropy in quasar H II regions during the epoch of reionization by analyzing the small-scale redshifted 21 cm power spectrum (21-cm PS). According to our model, quasars are positioned at the peaks of dark matter density fluctuations and ionize the surrounding plasma, forming anisotropic Strömgren spheres whose shapes are determined by the local tidal field. Through numerical simulations with varying spin temperature values (T_S), we observe that the 21-cm PS exhibits a characteristic peak structure that reflects the shapes of different H II regions. This peak structure becomes more pronounced as T_S decreases, due to an increase in the proportion of neutral hydrogen atoms resulting from a reduced brightness temperature difference between the cosmic microwave background (CMB) and the 21-cm absorption line. Our findings suggest that this peak structure could potentially be used to constrain the value of T_S. However, given the many other factors that affect the 21-cm PS beyond T_S, further research is necessary before drawing any definitive conclusions regarding its detectability.",
        "ori-fast-z-score": 1.7320508075688772,
        "water-fast-z-score": 6.350852961085883,
        "rewrite-fast-z-score": 3.0550504633038935
    },
    {
        "original_text": "We report the first measurement of the cross section for the process ppbar -> Zgamma + X, where X is any number of additional particles produced along with the Zgamma boson.  The data were collected by the D0 experiment during Run II of Fermilab s Tevatron Collider between 2002 and 2007 using an integrated luminosity of 5.4 fb-1 . We measure the cross section to be 0.84 +/- 0.11 (stat.) +/- 0.10 (syst.) pb, which agrees well with next-to-leading-order perturbative QCD predictions. Using this result we set upper limits on possible anomalous trilinear gauge-boson coupling parameters. These results are also used to derive constraints on models that predict new heavy neutral vector bosons decaying into pairs of photons or gluons. In addition, these measurements provide important input for future searches for Higgs bosons decaying into two photons. \nPACS numbers: 11.30.Er, 12.60.Jv",
        "watermark_text": "We report the first measurement of the cross area for the process ppbar - > Zgamma + X , where X is any number of added particles generated along with the Zgamma boson . The data were collected by the D0 study during Run II of Fermilab s Tevatron Collider between 2002 and 2007 utilizing an integrated luminosity of 5 . 4 fb - 1 .We estimate the cross area to be 0 . 84 + / - 0 . 11 ( stat . ) + / - 0 . 10 ( syst . )pb , which agrees well with next - to - leading - order perturbative QCD estimates . Using this consequence we put upper limits on potential anomalous trilinear gauge - boson correlation parameters .These data are also used to derive restrictions on estimates that forecast new heavy neutral vector bosons decaying into sets of photons or gluons . In addition , these measurements give important output for future investigations for Higgs bosons decaying into two photons .PACS codes : 11 . 30 . Er , 12 . 60 . Jv",
        "rewrite_text": "We present the first measurement of the cross-section for the process \\( pp̅ \\rightarrow Z\\gamma + X \\), where \\( X \\) represents any number of additional particles produced alongside the \\( Z\\gamma \\) boson. This data was collected by the D0 collaboration during Run II of Fermilab's Tevatron Collider from 2002 to 2007, utilizing an integrated luminosity of 5.4 fb\\(^{-1}\\). We estimate the cross-section to be \\( 0.84 \\pm 0.11 \\) (stat.) \\( \\pm 0.10 \\) (syst.) pb, which is in good agreement with next-to-leading-order perturbative QCD predictions. Using this result, we can impose upper limits on potential anomalous trilinear gauge-boson couplings. Additionally, this data allows us to set constraints on predictions of new heavy neutral vector bosons that decay into pairs of photons or gluons. Furthermore, these measurements provide valuable insights for future studies of Higgs bosons decaying into two photons. PACS codes: 11.30.Er, 12.60.Jv.",
        "ori-fast-z-score": -1.0886621079036347,
        "water-fast-z-score": 4.532898610306738,
        "rewrite-fast-z-score": 0.13483997249264842
    },
    {
        "original_text": "We study the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium after being quenched across the superfluid-Mott insulator transition. We show that this system exhibits universal behavior at late times which is characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents are determined analytically using a mapping onto a classical statistical mechanics problem for a driven diffusive system. This work was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.). \nI. INTRODUCTORY REMARkS\n\nThe recent experimental realization of quantum degenerate gases has opened up new avenues towards understanding strongly correlated many-body systems  1  . In particular, ultracold atomic gases have been used as model systems to explore phenomena such as fermionization  2  , supersolidity  3  , and Mott-insulating states  4  .\nIn this article we consider a particularly interesting class of experiments where the properties of these systems can be probed through their response to sudden changes in parameters  5  . For example, if the strength of inter-particle repulsion or density of particles is suddenly changed then it takes some time before the system reaches thermal equilibrium  6  . During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling  7, 8  and non-thermal fixed points  9  . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing novel phases of matter  10  .\nRecently there has been considerable interest in studying the nonequilibrium dynamics of bosonic systems  11  . A particularly well studied case is when the initial state corresponds to a highly excited state above the ground state  12  . It turns out that even though the initial state is far away from equilibrium, the system relaxes to a steady state described by a Gibbs ensemble  13  . However, if the initial state is prepared deep inside the ordered phase, then the system does not",
        "watermark_text": "We explore the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension , concentrating on its relaxation to equilibrium after being quenched across the superfluid - Mott insulator transition . We see that this system displays universal behavior at late times which is characterized by power - law decaying correlations and algebraic growth of entanglement entropy .The exponents are estimated analytically using a mapping onto a traditional statistical mechanics problem for a driven diffusive system . This research was supported by NSF grant PHY - 0960291 ( M . S . )and DOE funds DE - FG03 - 92 - ER40701 and DE - SC0012704 ( A . K . ) . I .INTRODUCTORY REMARkS The recent experimental realization of quantum degenerate gases has opened up new avenues towards studying strongly interacting multiple - bodies systems 1 . In particular , ultracold nuclear gases have been used as model structures to examine processes such as fermionization 2 , supersolidity 3 , and Mott - insulating states 4 .In this article we imagine a particularly exciting group of studies where the properties of these systems can be probed through their response to unexpected changes in parameters 5 . For instance , if the strength of inter - atom repulsion or density of molecules is suddenly changed then it takes some time before the system reaches heat equilibrium 6 .During this nonequilibrium evolution , the system might exhibit new characteristics like dynamical scaling 7 , 8 and non - cooling fixed points 9 . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing new phases of matter 10 .Recently there has been substantial interest in investigating the nonequilibrium dynamics of bosonic systems 11 . A notably well discussed case is when the first state corresponds to a highly excited state above the ground state 12 .It happens out that even though the first state is far back from equilibrium , the system relaxes to a steady state described by a Gibbs ensemble 13 . However , if the initial system is prepared deep inside the ordered phase , then the system does not",
        "rewrite_text": "We investigate the behavior of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its return to equilibrium following a quench through the superfluid-Mott insulator transition. Our findings reveal that this system exhibits universal behavior at late times, marked by power-law decaying correlations and the algebraic growth of entanglement entropy. We derive the exponents analytically through a mapping to a conventional statistical mechanics problem involving a driven diffusive system. This study was funded by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.).\n\nI. INTRODUCTORY REMARKS\nRecent advancements in the experimental realization of quantum degenerate gases have paved the way for new investigations into strongly interacting many-body systems. In particular, ultracold atomic gases have served as model systems for exploring phenomena such as fermionization, supersolidity, and Mott insulating states. In this article, we envision an exciting area of research focused on probing the properties of these systems through their responses to sudden changes in parameters. For example, when the inter-atom repulsion strength or molecular density is abruptly altered, the system takes a finite time to achieve thermal equilibrium. Throughout this nonequilibrium evolution, the system may reveal new characteristics, including dynamical scaling and non-cooling fixed points. These phenomena are crucial for enhancing our fundamental understanding of quantum matter and offer valuable insights into potential pathways for realizing new phases of matter. Recently, there has been significant interest in exploring the nonequilibrium dynamics of bosonic systems. A particularly well-studied scenario involves an initial state that is highly excited and far from equilibrium; intriguingly, this system ultimately relaxes into a steady state described by a Gibbs ensemble. However, when the initial state resides deep within the ordered phase, the system behaves differently.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 7.175639059928206,
        "rewrite-fast-z-score": 1.4216114170120542
    },
    {
        "original_text": "We consider the problem of time evolution for quantum fields on the Milne universe, which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant. We show that there are no normalizable solutions to the Klein-Gordon equation corresponding to massive particles at early times (t → −∞). This result can be understood as follows. The Milne universe has a singularity at t = 0 where all physical quantities diverge. In particular, the energy density diverges like $1/t^2$ near this point. As a consequence, any particle state localized around $t=0$ will have infinite energy. Therefore it cannot correspond to a physically meaningful solution of the field equations. On the other hand, we find that there exist normalizable states describing massless particles at early times. These states represent gravitons or photons propagating into the past along geodesics towards the big bang singularity.",
        "watermark_text": "We consider the question of time progression for quantum fields on the Milne universe , which is an open FRW cosmology with negative spatial curvature and vanishing cosmological constant . We see that there are no normalizable responses to the Klein - Gordon equation equivalent to massive particles at early years ( t → −∞ ) .This result can be understood as follows . The Milne universe has a singularity at t = 0 where all physical components diverge .In particular , the power concentration diverges like $ 1 / t ^ 2 $ near this point . As a consequence , any particle state concentrated around $ t = 0 $ will have infinite energy .Therefore it lacks correspond to a physically useful solve of the field equations . On the other hand , we find that there exist normalizable states describing massless objects at early years .These states reflect gravitons or photons propagating into the past along geodesics towards the big bang singularity .",
        "rewrite_text": "We explore the progression of time for quantum fields in the Milne universe, an open Friedmann-Robertson-Walker (FRW) cosmology characterized by negative spatial curvature and a vanishing cosmological constant. Our analysis reveals that, at early times (as \\( t \\) approaches \\( -\\infty \\)), there are no normalizable responses to the Klein-Gordon equation that correspond to massive particles. This phenomenon can be understood in the context of the Milne universe's singularity at \\( t = 0 \\), where all physical quantities diverge, notably with the power concentration behaving like \\( 1 / t^2 \\) near this singular point. Consequently, any particle states concentrated around \\( t = 0 \\) would have infinite energy, rendering them non-viable solutions to the field equations. However, we do find that normalizable states exist for massless particles during this early period. These states correspond to gravitons or photons traveling backward along geodesics towards the big bang singularity.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 1.1920791213585393
    },
    {
        "original_text": "The topological classification of clusters is reviewed, with emphasis on the role played by symmetry and topology in determining cluster structures.  The concept of  topology  refers to the connectivity between atoms or molecules within a cluster; it can be used as an organizing principle for classifying clusters into families based upon their structural similarities. This review focuses primarily on metal clusters (i.e., clusters containing one or more metallic elements), but also discusses some examples of non-metallic clusters that have been studied recently. In addition, we briefly discuss how this approach has been applied to classify clusters formed during chemical reactions. Finally, we present several open questions related to the topic of cluster topology. Topology plays an important role in understanding the structure of matter at all scales ranging from atomic nuclei to macroscopic materials such as crystals. It provides a useful framework for classifying clusters according to their structural similarity. Herein, we provide a brief overview of recent progress made towards developing a systematic classification scheme for clusters using concepts borrowed from condensed-matter physics.",
        "watermark_text": "The topological characterization of clusters is reviewed , with emphasis on the importance played by symmetry and topology in distinguishing cluster structures . The concept of topology refers to the connectivity between molecules or compounds within a cluster ; it can be used as an organizing principle for classifying compounds into families according upon their structural links .This study deals mainly on metal clusters ( i . e . , clusters featuring one or more metallic compounds ) , but also explains some examples of non - metallic complexes that have been studied ago . In addition , we occasionally explore how this methodology has been used to classify clusters formed during chemical processes .Finally , we present many open questions related to the subject of cluster topology . Topology plays an important role in understanding the structure of matter at all scales ranging from atomic atoms to macroscopic materials such as crystals .It provides a helpful basis for classifying clusters according to their structural similarity . Herein , we provide a brief overview of recent progress made towards developing a comprehensive classification system for clusters using concepts borrowed from condensed - matter theory .",
        "rewrite_text": "This review highlights the topological characterization of clusters, focusing on the critical roles of symmetry and topology in differentiating cluster structures. Topology refers to the connectivity between molecules or compounds within a cluster and serves as an organizing principle for classifying compounds into families based on their structural connections. While the primary focus is on metal clusters, which include one or more metallic compounds, the review also addresses examples of non-metallic complexes that have been previously examined. Additionally, we explore how this methodology has been applied to classify clusters formed during chemical processes. Furthermore, we outline several open questions related to cluster topology. Topology is essential for understanding the structure of matter across various scales, from atomic levels to macroscopic materials like crystals, and it provides a valuable framework for classifying clusters based on their structural similarities. Finally, we offer a brief overview of the recent advancements made toward creating a comprehensive classification system for clusters using concepts from condensed matter theory.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 6.599750073368524,
        "rewrite-fast-z-score": 0.8626621856275073
    },
    {
        "original_text": "We study the phase diagram and critical behavior of an anisotropic system consisting of Nx × Ny spins on a square lattice, where each spin is described by the two-dimensional XY model. We find that there are three phases depending on the values of J1 / J2 (J2 > 0); ferromagnetic state for small J1 / J2 , spiral state for intermediate J1 / J2 , and paramagnetic state for large J1 / J2 . The transition between these states belongs to the universality class of the Ising model. In particular we show that the spiral state has a nontrivial structure which can be regarded as a superposition of ferromagnetically ordered domains with different orientations. This result suggests that the spiral state may have some relevance to the physics of high-Tc cuprates. \n \n Introduction \n \n It was shown recently  1  that the ground-state properties of the twodimensional Heisenberg antiferromagnet with nearest-neighbor interactions depend strongly on whether or not the exchange interaction along one direction vanishes identically. For example, if the exchange interaction along the y-direction vanishes completely, then the ground state becomes ferromagnetic even though it consists only of S = 1/2 spins. On the other hand, when the exchange interaction along both directions does not vanish simultaneously, the ground state is always antiferromagnetic  2  .\n \nIn this work, we consider another type of anisotropy in the two-dimensional XY model: namely, we assume that the coupling constant along the x-direction is larger than that along the y-direction. As will become clear later, such an anisotropy plays an important role in determining the nature of the ground state.",
        "watermark_text": "We explore the phase diagram and critical behavior of an anisotropic scheme consisting of Nx × Ny spins on a square lattice , where each spin is characterized by the two - dimensional XY model . We see that there are three stages depending on the values of J1 / J2 ( J2 > 0 ) ; ferromagnetic state for large J1 / J2 , spiral state for advanced J1 / J2 , and paramagnetic state for large J1 / J2 .The shift between these states belongs to the universality category of the Ising model . In particular we find that the spiral state has a nontrivial structure which can be regarded as a superposition of ferromagnetically ordered domains with various orientations .This result suggests that the spiral state may have some relevance to the physics of high - Tc cuprates . Introduction It was shown recently 1 that the ground - state properties of the twodimensional Heisenberg antiferromagnet with nearest - neighbor interactions depend greatly on whether or not the transfer coupling along one orientation vanishes identically .For instance , if the transfer coupling along the y - direction vanishes totally , then the ground state remains ferromagnetic even though it consists only of S = 1 / 2 spins . On the other hand , when the transfer exchange along both directions does not vanish simultaneously , the ground state is always antiferromagnetic 2 .In this research , we define another type of anisotropy in the two - dimensional XY model : specifically , we suppose that the interaction factor along the x - direction is bigger than that along the y - direction . As will become clear earlier , such an anisotropy takes an important role in determining the nature of the ground state .",
        "rewrite_text": "We investigate the phase diagram and critical behavior of an anisotropic framework composed of Nx × Ny spins on a square lattice, where each spin is described by the two-dimensional XY model. Our analysis reveals three distinct phases depending on the ratio of J1 / J2 (with J2 > 0): a ferromagnetic phase for large J1 / J2, a spiral phase for intermediate values, and a paramagnetic phase for even larger J1 / J2. The transitions among these phases belong to the universality class of the Ising model. Notably, the spiral phase exhibits a complex structure that can be interpreted as a superposition of ferromagnetically ordered domains with varying orientations. This finding implies that the spiral phase may be significant for understanding the physics of high-Tc cuprates. In recent studies, it has been demonstrated that the ground-state properties of the two-dimensional Heisenberg antiferromagnet with nearest-neighbor interactions are highly influenced by whether the transfer coupling in one direction is completely absent. If the transfer coupling in the y-direction is entirely eliminated, the ground state maintains a ferromagnetic nature, despite being composed solely of S = 1/2 spins. Conversely, when the transfer coupling in both directions is present, the ground state invariably becomes antiferromagnetic. In this study, we introduce a novel form of anisotropy in the two-dimensional XY model, positing that the interaction strength in the x-direction exceeds that in the y-direction. As will be elucidated, such anisotropy plays a crucial role in shaping the characteristics of the ground state.",
        "ori-fast-z-score": 0.7921180343813395,
        "water-fast-z-score": 6.601706163700764,
        "rewrite-fast-z-score": 2.9968831160415568
    },
    {
        "original_text": "The Fermi Large Area Telescope (LAT) has revolutionized our understanding of blazar jets by providing unprecedented sensitivity to gamma rays above 100 MeV. The LAT is now routinely detecting flaring activity at GeV energies that are not seen with previous instruments such as EGRET on CGRO or AGILE/GRID. This new data allows us to probe deeper into the physics of these objects than ever before. \n \n In this talk I will discuss how we can use the upcoming generation of space-based observatories - including GLAST and EXIST - to study blazars over an even broader energy range. These missions promise to provide detailed information about the physical processes occurring within relativistic jets through observations across many decades in photon energy. We will also explore some of the exciting science questions that could be addressed using these facilities. Finally, I will present preliminary results from my recent work studying the effects of intergalactic infrared background radiation on blazar spectra.",
        "watermark_text": "The Fermi Large Area Telescope ( LAT ) has revolutionized our knowing of blazar jets by offering extraordinary exposure to gamma radiation above 100 MeV . The LAT is now regularly detecting flaring activity at GeV values that are not seen with previous instruments such as EGRET on CGRO or AGILE / GRID .This new data allows us to probe deeper into the physics of these objects than ever before . In this talk I will explain how we can using the latest generation of space - based observatories - including GLAST and EXIST - to study blazars over an much broader energy range .These expeditions promise to provide comprehensive information about the natural reactions evolving within relativistic jets through observations across many years in photon energy . We will also investigate some of the exciting science problems that might be addressed using these facilities .Finally , I will present preliminary results from my current work studying the effects of intergalactic infrared background radiation on blazar spectra .",
        "rewrite_text": "The Fermi Large Area Telescope (LAT) has transformed our understanding of blazar jets by providing exceptional sensitivity to gamma radiation above 100 MeV. The LAT is now frequently detecting flaring activities at GeV energies that previous instruments, like EGRET on CGRO and AGILE/GRID, were unable to capture. This new data enables us to explore the physics of these objects in unprecedented detail. In this presentation, I will discuss how we can utilize the latest generation of space-based observatories, including GLAST and EXIST, to investigate blazars across a much wider energy spectrum. These endeavors are expected to yield valuable insights into the fundamental processes occurring within relativistic jets through long-term observations of varying photon energies. Additionally, we will explore some of the intriguing scientific questions that these facilities may help to address. Finally, I will share preliminary findings from my ongoing research on the impact of intergalactic infrared background radiation on blazar spectra.",
        "ori-fast-z-score": 1.4770978917519928,
        "water-fast-z-score": 6.474980550884177,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We report near-infrared coronagraphic observations of the young binary system UY Aurigae (=V773 Tau) obtained with the Subaru Telescope in December 2005 and January 2006, using the newly installed HiCIAO instrument equipped with an occulting mask. The data were reduced by subtracting dark frames and flat fields to remove detector biases and pixel-to-pixel variations respectively. We then applied aperture photometry on each frame after masking out bad pixels and cosmic rays. Finally we averaged all the individual frames together for each filter bandpass. Our results show that there is no significant difference between our two epochs of observation within the uncertainties. In addition, we find that the flux ratio between the primary star and its companion varies significantly depending upon which filter was used during the observations. This suggests that the spectral energy distribution of UY Aur may be changing over time as it evolves towards the main sequence. \n \n Keywords: Young stars",
        "watermark_text": "We report near - infrared coronagraphic observations of the young binary system UY Aurigae ( = V773 Tau ) obtained with the Subaru Telescope in December 2005 and January 2006 , using the newly installed HiCIAO object fitted with an occulting mask . The data were reduced by subtracting bright frames and flat fields to remove detector biases and pixel - to - pixel differences respectively .We then added aperture photometry on each frame after masking out bad pixels and cosmic rays . Finally we combined all the different images together for each filter bandpass .Our results show that there is no considerable difference between our two epochs of observation within the uncertainties . In addition , we find that the flux ratio between the primary star and its companion vary significantly depending upon which filter was used during the surveys .This implies that the spectral power distribution of UY Aur may be changing over time as it evolves towards the main sequence . Keywords : Young stars",
        "rewrite_text": "We present near-infrared coronagraphic observations of the young binary system UY Aurigae (also known as V773 Tau), acquired with the Subaru Telescope in December 2005 and January 2006 using the newly installed HiCIAO equipped with an occulting mask. The data were processed by subtracting bright frames and flat-field images to eliminate detector biases and pixel-to-pixel discrepancies. We performed aperture photometry on each frame after masking bad pixels and cosmic rays. Ultimately, we combined all the images for each filter bandpass. Our findings indicate that there is no significant difference between the two observational epochs when accounting for uncertainties. Moreover, we observed that the flux ratio between the primary star and its companion varies considerably depending on the filter used during the observations. This suggests that the spectral power distribution of UY Aur may be evolving over time as it approaches the main sequence. Keywords: Young stars.",
        "ori-fast-z-score": 1.270001270001905,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "We report on an experiment that tests the predictions of quantum mechanics and local hidden variable theories for two entangled photons in a Bell state, using polarization-entanglement swapping to create a loophole-free Bell inequality violation.  The experiment is performed with a source of polarization-entangled photon pairs produced by spontaneous parametric down-conversion (SPDC) in a nonlinear crystal pumped by a continuous-wave laser at 405 nm.   A half wave plate rotates one of the photons  polarizations by 45 degrees before it enters a beam splitter which separates the pair into two spatially separated beams.   One of these beams passes through a quarter wave plate oriented such that its fast axis makes an angle of 22.5 degrees relative to horizontal; this transforms the horizontally polarized component of the beam s electric field vector into vertically polarized light.   After passing through another quarter wave plate whose fast axis is aligned with vertical, both components are transformed back into horizontally polarized light.   This transformation can be represented as follows:  H → V → H = HVH.   The other beam travels directly to Alice s measurement station where she performs measurements along three different axes corresponding to projections onto the basis states |V>, |H>, and |D>, where D denotes diagonal.   Bob measures his photon along four different axes corresponding to projections",
        "watermark_text": "We report on an experiment that tests the assumptions of quantum mechanics and local hidden variable theories for two entangled photons in a Bell state , using polarization - entanglement swapping to create a loophole - free Bell inequality violation . The experiment is conducted with a source of polarization - entangled photon pairs formed by spontaneous parametric down - transfer ( SPDC ) in a nonlinear crystal pumped by a continuous - wave beam at 405 nm .A half wave plate rotates one of the photons polarizations by 45 degrees before it enters a light splitter which separates the pair into two spatially joined beams . One of these beams passes through a quarter wave plate angled such that its fast axis creates an angle of 22 . 5 degrees relative to vertical ; this changes the horizontally polarized component of the laser s electric field vector into horizontal polarized light .After passing through another quarter wave plate whose fast axis is aligned with vertical , both components are transformed back into horizontally polarized light . This transformation can be described as follows : H → V → H = HVH .The other light journeys directly to Alice s monitoring facility where she takes measurements along three different axes corresponding to projections onto the basis states | V > , | H > , and | D > , where D denotes diagonal . Bob measures his photon along four different axes corresponding to projections",
        "rewrite_text": "We present an experiment designed to investigate the foundations of quantum mechanics and local hidden variable theories using two entangled photons in a Bell state. Our approach employs polarization-entanglement swapping to achieve a loophole-free violation of the Bell inequality. The experiment utilizes a source of polarization-entangled photon pairs generated via spontaneous parametric down-conversion (SPDC) in a nonlinear crystal, with a continuous-wave pump beam at 405 nm. A half-wave plate rotates the polarization of one photon by 45 degrees before it enters a beamsplitter, which separates the pair into two spatially overlapping beams. One of these beams passes through a quarter-wave plate, positioned such that its fast axis makes a 22.5-degree angle with the vertical; this transformation converts the horizontally polarized component of the laser's electric field into horizontally polarized light. After it passes through another quarter-wave plate aligned vertically, both polarization components revert to horizontally polarized light. This transformation can be summarized as H → V → H = HVH. Meanwhile, the other beam is directed straight to Alice's monitoring station, where she conducts measurements along three distinct axes corresponding to the basis states | V >, | H >, and | D >, with D representing the diagonal basis. Bob, on his end, measures his photon along four different axes for his projections.",
        "ori-fast-z-score": 0.9138115486202573,
        "water-fast-z-score": 4.569057743101286,
        "rewrite-fast-z-score": 0.9233805168766388
    },
    {
        "original_text": "We study how classical and quantum uncertainty affect the price dynamics in an incomplete information setting, where agents have access to different sources of information about the underlying state variable. We show that when there is no common knowledge among traders on the true value of the state variable, they may disagree on its expected future evolution even if all are rational and risk-neutral. This disagreement leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based solely on their private signals. In this case, we find that the stock returns exhibit volatility clustering and fat tails similar to those observed empirically. Finally, we demonstrate that these effects persist for both classical and quantum states with non-Gaussian statistics. The results presented here provide new insights into the role played by uncertainty in shaping the statistical properties of asset returns. They also suggest possible avenues for further research aimed at understanding the origin of such phenomena within more realistic models of trading behavior.",
        "watermark_text": "We research how classical and quantum uncertainty influence the price dynamics in an incomplete information context , where agents have access to different sources of information about the underlying state variable . We see that when there is no shared information among traders on the true value of the state variable , they may differ on its expected past development even if all are realistic and chance - neutral .This dispute leads to fluctuations in prices which can be amplified by the presence of noise traders who trade based primarily on their private signals . In this instance , we find that the stock returns display volatility clustering and lean tails identical to those observed empirically .Finally , we prove that these consequences persist for both classical and quantum states with non - Gaussian statistics . The results presented here provide fresh insights into the part played by uncertainty in shaping the statistical characteristics of investment returns .They also suggest possible avenues for further studies aimed at studying the origin of such patterns within more realistic descriptions of market behavior .",
        "rewrite_text": "We investigate how classical and quantum uncertainty impact price dynamics in a context of incomplete information, where agents possess varying sources of information regarding the underlying state variable. Our findings indicate that when traders lack access to shared information about the true value of the state variable, their expectations regarding its past evolution can diverge, even if all are rational and risk-neutral. This disagreement results in price fluctuations, which can be exacerbated by the presence of noise traders who rely primarily on their private signals. In this scenario, we observe that stock returns exhibit volatility clustering and lean tails similar to those found in empirical data. Additionally, we demonstrate that these effects persist for both classical and quantum states characterized by non-Gaussian statistics. The insights presented here enhance our understanding of the role of uncertainty in shaping the statistical properties of investment returns and suggest potential directions for further research focused on uncovering the origins of such patterns within more realistic models of market behavior.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 0.42640143271122083
    },
    {
        "original_text": "We present the results of our study on bimodality in galaxies and active galactic nuclei (AGN). We find that there is no significant difference between the fraction of AGNs hosted by red or blue galaxies, but we do see an excess of AGNs with respect to normal galaxies at intermediate colors. This suggests that AGNs are not preferentially found in either red or blue galaxies, as previously thought; instead they appear to be more common among galaxies with intermediate color. The lack of correlation between galaxy color and AGN activity may indicate that AGNs play only a minor role in quenching star formation in massive galaxies. Alternatively, it could suggest that AGNs have different effects depending on their luminosity and/or accretion rate. In addition, we find that the majority of AGNs reside in galaxies with bulges, regardless of whether these galaxies are classified as early-type or late-type systems.",
        "watermark_text": "We present the conclusion of our research on bimodality in galaxies and active galactic nuclei ( AGN ) . We see that there is no major variation between the fraction of AGNs hosted by red or blue clusters , but we do saw an surplus of AGNs with regard to normal galaxies at intermediate colors .This implies that AGNs are not preferentially found in either blue or blue stars , as previously thought ; however they appear to be more common among clusters with intermediate color . The absence of correlation between galaxy color and AGN activity may indicate that AGNs serve only a minor importance in quenching star formation in massive galaxies .Alternatively , it could indicate that AGNs have different impacts depending on their luminosity and / or accretion rate . In addition , we find that the majority of AGNs occur in galaxies with bulges , regardless of whether these objects are classified as early - class or late - class systems .",
        "rewrite_text": "We present the findings of our research on bimodality in galaxies and active galactic nuclei (AGN). Our analysis reveals that there is no significant difference in the proportion of AGNs found in red versus blue clusters. However, we observed a higher incidence of AGNs among galaxies with intermediate colors compared to typical galaxies. This suggests that AGNs are not preferentially associated with either blue or red stars, contrary to prior assumptions; rather, they seem to be more prevalent in clusters with intermediate color. The lack of a correlation between galaxy color and AGN activity may suggest that AGNs play a relatively minor role in inhibiting star formation in massive galaxies. Alternatively, it could imply that AGN impact varies based on their luminosity or accretion rate. Furthermore, we found that most AGNs are present in galaxies with bulges, irrespective of whether these galaxies are classified as early-type or late-type systems.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "We study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms  1  . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (τ)  2  , which describes bunching behavior at short times and anti-bunching at longer times  3  . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons  4  .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions  5  -  8  . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics  6 - 8  . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed  9  -  11  . For example, in Ref.  10  , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light  12  .",
        "watermark_text": "We test the second - order correlation function for an element interacting with two modes of light , one resonant and another off - resonant to atomic transition frequency . We see that higher order antibunching can be experienced when the atom is initially prepared in an excited state or ground state superposition .The phenomenon is more pronounced if the first state has some population on the excited state . This phenomenon might have applications in quantum information processing .Introduction : - In recent years there has been substantial interest in investigating nonclassical characteristics of radiation fields generated by atoms 1 . In particular , it was shown that the photon statistics of such systems are governed by the first - order coherence function h ( 1 ) ( τ ) 2 , which explains bunching behavior at short periods and pro - bunching at shorter times 3 .It is well established that this property originates due to destructive interference between various pathways leading to emission of photons 4 . Recently , various papers studied the effects of induced emission on the second - order correlation functions 5 - 8 .They showed that the presence of spontaneous emission contributes to sub - Poissonian statistics 6 - 8 . However , these experiments were restricted only to the case where the atom interacts with a single mode of field .On the other hand , many tests utilizing atoms interacting simultaneously with various modes of electromagnetic field have already been performed 9 - 11 . For instance , in Ref .10 , the authors explored the impact of vacuum fluctuations on the fluorescence spectrum of a three - level network driven by two laser beams . In addition , they reported that the frequency sound of the emitted light depends strongly on the relative phase change between the driving lasers .Motivated by these observation findings we mention here the question of calculating the second - order correlation function of an element interacting simultaneously with two modes of light 12 .",
        "rewrite_text": "We investigate the second-order correlation function for an atom interacting with two light modes: one resonant and the other off-resonant with respect to the atomic transition frequency. Our findings reveal that when the atom is prepared in either an excited state or a superposition of the ground state and excited state, higher order antibunching can occur. This effect becomes more pronounced when the initial state possesses a population in the excited state, suggesting possible applications in quantum information processing.\n\n**Introduction:** Recent years have seen a growing interest in examining the nonclassical features of radiation fields produced by atoms. Notably, it has been demonstrated that the photon statistics of these systems are dictated by the first-order coherence function h(1)(τ), which accounts for both bunching behavior at short timescales and pro-bunching at even shorter intervals. This characteristic is well understood to arise from the destructive interference among different paths leading to photon emission. Numerous studies have recently focused on the effects of induced emission on second-order correlation functions, highlighting that spontaneous emission contributes to sub-Poissonian statistics. However, these studies have predominantly addressed scenarios involving atom-field interactions with a single mode. In contrast, various experiments have already explored atoms interacting simultaneously with multiple modes of the electromagnetic field. For example, one study examined how vacuum fluctuations influence the fluorescence spectrum of a three-level system driven by two laser beams, noting a significant dependence of the emitted light's frequency on the relative phase shift between the lasers. Inspired by these observations, we turn our attention to the calculation of the second-order correlation function for an atom interacting concurrently with two light modes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": 2.7760883751542687
    },
    {
        "original_text": "We present an analysis of the final masses of gas giant planets in our Solar System, based on their orbital parameters and compositions inferred by remote sensing observations. We find that Jupiter s mass is consistent with its composition if it formed at 5 AU or less; Saturn s mass can be explained only if it formed beyond 10 AU. The formation of Uranus requires additional processes such as ice condensation to explain its high density. Neptune may have been born more massive than we see today but lost most of its initial mass through photoevaporation driven by intense stellar radiation. Our results suggest that the core accretion model for planet formation cannot fully account for all observed properties of gas giant planets. Keywords: Planet formation, Core accretion theory, Photoevaporation, Mass loss, Stellar radiation, Ice condensation, Orbital evolution, Final mass, Solar system, Remote sensing observation, Compositions, Density",
        "watermark_text": "We present an assessment of the last masses of gas giant planets in our Solar System , relying on their orbital characteristics and compositions inferred by distant sensing measurements . We see that Jupiter s mass is compatible with its composition if it created at 5 AU or smaller ; Saturn s mass can be understood only if it created beyond 10 AU .The formation of Uranus demands extra processes such as ice condensation to explain its high density . Neptune might have been born more massive than we saw nowadays but lost most of its initial mass through photoevaporation driven by intense stellar radiation .Our results show that the core accretion theory for planet development cannot fully account for all observed properties of gas giant worlds . Keywords : Planet structure , Core accretion theory , Photoevaporation , Mass loss , Stellar radiation , Ice condensation , Orbital development , Final mass , Solar system , Remote sensing observation , Compositions , Density",
        "rewrite_text": "We provide an evaluation of the final masses of the gas giant planets in our Solar System, based on their orbital characteristics and compositions inferred from remote sensing data. Our analysis indicates that Jupiter's mass aligns with its composition if it formed at a distance of 5 AU or less. In contrast, Saturn's mass can only be reconciled if it formed beyond 10 AU. The formation of Uranus appears to require additional processes, such as ice condensation, to account for its high density. Neptune is likely to have originated with a greater mass than we observe today, having lost much of its initial mass due to photoevaporation caused by strong stellar radiation. Our findings suggest that the core accretion theory of planetary formation does not fully explain all observed attributes of gas giant planets. \n\nKeywords: Planet structure, Core accretion theory, Photoevaporation, Mass loss, Stellar radiation, Ice condensation, Orbital development, Final mass, Solar system, Remote sensing observation, Compositions, Density.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "In this work, we study the charged axially symmetric solution and energy in teleparallel theory equivalent to general relativity (GR). We find that the field equations are exactly same as those obtained by GR with an additional term which is proportional to the torsion scalar T . The solutions for the metric functions are found numerically using the shooting method. It turns out that these solutions have no singularities at all. In addition, it has been shown that the total energy density is positive definite everywhere inside the star. Finally, we show that our results agree well with those obtained by GR. This shows that the teleparallel gravity can be considered as alternative gravitational theories to GR. Keywords: Charged axially symmetric solution; energy; teleparallel gravity; Einstein-Maxwell system. 1 Introduction Gravity plays important role in understanding many physical phenomena such as black holes  1  , cosmology  2  , quantum mechanics  3  etc.. However, there still remain some unsolved problems like dark matter  4  , dark energy  5  , inflation  6  etc., which cannot be explained within the framework of standard model of particle physics  7, 8  .\nThe most successful classical description of gravitation is provided by Einstein s general relativity (GR)  9  where the curvature tensor R µνρσ describes the geometry of space-time  10  . On the other hand, teleparallel gravity  11  -  13  is another approach to describe gravitation on the basis of tetrad fields e A µ instead of metric g µν  14  . Here, the basic variables are connection coefficients Γ λ µν defined through vierbein fields e \nwhere η AB = diag(−1, +1, +1, +1), and h ABCD denotes the contortion tensor  15  . The corresponding Lagrangian density reads  16  :",
        "watermark_text": "In this research , we study the charged axially symmetric solution and energy in teleparallel theory equivalent to general relativity ( GR ) . We see that the field equations are exactly same as those given by GR with an additional term which is proportional to the torsion scalar T .The responses for the metric functions are found numerically using the shot technique . It happens out that these solutions have no singularities at all .In addition , it has been shown that the total energy density is positive definite everywhere inside the star . Finally , we find that our findings agree well with those achieved by GR .This shows that the teleparallel gravitational can be regarded as alternative gravity explanations to GR . Keywords : Charged axially symmetric solution ; energy ; teleparallel gravitational ; Einstein - Maxwell system .1 Introduction Gravity takes key importance in understanding several physical phenomena such as black holes 1 , cosmology 2 , quantum mechanics 3 etc . . However , there still continue some unsolved issues like dark matter 4 , darkness energy 5 , inflation 6 etc . , which cannot be described within the framework of standard theory of particle science 7 , 8 .The most popular classical description of gravitation is provided by Einstein s general relativity ( GR ) 9 where the curvature tensor R µνρσ describes the topology of space - time 10 . On the other hand , teleparallel gravitational 11 - 13 is another technique to explain gravitation on the basis of tetrad fields r A µ instead of metric g µν 14 .Here , the fundamental variables are connection coefficients Γ λ µν characterized through vierbein fields e where η AB = diag ( −1 , + 1 , + 1 , + 1 ) , and h ABCD denotes the contortion tensor 15 . The equivalent Lagrangian density reads 16 :",
        "rewrite_text": "In this study, we investigate the charged axially symmetric solutions and energy within the framework of teleparallel theory, which is equivalent to general relativity (GR). We find that the field equations mirror those of GR, supplemented by an additional term proportional to the torsion scalar \\(T\\). The metric functions are determined numerically using the shooting method. Notably, these solutions exhibit no singularities whatsoever. Furthermore, our analysis indicates that the total energy density remains positive throughout the entirety of the star. Ultimately, we observe that our results align closely with those produced by GR, demonstrating that teleparallel gravity can serve as a valid alternative framework to GR. \n\n**Keywords:** Charged axially symmetric solution; energy; teleparallel gravity; Einstein-Maxwell system. \n\n1. **Introduction**: Gravity plays a crucial role in our understanding of various physical phenomena, including black holes, cosmology, and quantum mechanics. Nevertheless, there remain unresolved challenges, such as dark matter, dark energy, and inflation, which are not adequately addressed by the standard framework of particle physics. The most widely accepted classical description of gravitation is Einstein's general relativity, where the curvature tensor \\(R_{\\mu\\nu\\rho\\sigma}\\) characterizes the geometry of spacetime. In contrast, teleparallel gravity offers an alternative approach by employing tetrad fields \\(r^A_\\mu\\) rather than the metric \\(g_{\\mu\\nu}\\). In this framework, the primary variables are the connection coefficients \\(\\Gamma^\\lambda_{\\mu\\nu}\\), which are expressed in terms of vierbein fields \\(e\\), with \\(\\eta_{AB} = \\text{diag}(-1, +1, +1, +1)\\), and the contortion tensor is denoted as \\(h_{ABCD}\\). The corresponding Lagrangian density is given as follows:",
        "ori-fast-z-score": 0.4583492485141057,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The discovery that the universe is expanding at an accelerating rate has led to intense interest in dark energy as well as new ideas about fundamental physics. This talk will review some recent results on these topics including constraints on models for cosmic acceleration using supernovae data, measurements of the Hubble constant with Type Ia supernovae, and tests of general relativity using gravitational lensing statistics. The talk will also discuss how future surveys such as LSST can be used to further our understanding of dark energy and fundamental physics. I will conclude by discussing my own work on testing gravity theories beyond Einstein s theory using weak lensing observations. Keywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background. Speaker: Adam Riess (Princeton University) Date: February 18, 2007 Time: 4:30pm - 5:15pm Location: Room B",
        "watermark_text": "The observation that the universe is evolving at an accelerating rate has led to intense interest in dark energy as well as fresh concepts about basic physics . This discussion will review some latest findings on these topics including constraints on estimates for cosmic acceleration using supernovae information , measurements of the Hubble constant with Type Ia supernovae , and experiments of general relativity employing gravitational lensing statistics .The speech will also discuss how potential experiments such as LSST can be used to further our knowing of deep energy and fundamental theory . I will conclude by reviewing my own research on proving gravity fields beyond Einstein s principle involving weak lensing observations .Keywords : Cosmology , Dark Energy , General Relativity , Weak Lensing , Supernovae , Gravitational Waves , Cosmic Microwave Background . Speaker : Adam Riess ( Princeton University ) Date : February 18 , 2007 Time : 4 : 30pm - 5 : 15pm Location : Room B",
        "rewrite_text": "The discovery that the universe is expanding at an accelerating pace has sparked significant interest in dark energy and has prompted new ideas regarding fundamental physics. This presentation will examine the latest advancements in these areas, including constraints on cosmic acceleration estimates derived from supernova data, measurements of the Hubble constant utilizing Type Ia supernovae, and experiments related to general relativity that utilize gravitational lensing statistics. Additionally, the talk will explore how upcoming experiments, such as LSST, could enhance our understanding of dark energy and fundamental theories. I will conclude by discussing my own research focused on demonstrating gravity fields beyond Einstein's theory through weak lensing observations. \n\nKeywords: Cosmology, Dark Energy, General Relativity, Weak Lensing, Supernovae, Gravitational Waves, Cosmic Microwave Background.\n\nSpeaker: Adam Riess (Princeton University)\nDate: February 18, 2007\nTime: 4:30 PM - 5:15 PM\nLocation: Room B",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 1.270001270001905
    },
    {
        "original_text": "We study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.",
        "watermark_text": "We research the quantum Hall impact ( QHE ) and its interplay with magnetism in monolayer graphene by using an efficient low - energy theory that takes into consideration both electron - atom relationships and disturbance effects . We see how to derive this description starting from initial principles , and we explain some of its primary characteristics .In particular , we find that at half - filling it displays two different stages depending on the strength of the Coulomb interaction between electrons . For weak bonding these are distinct by a phase shift motivated by premature breaking of time - reversal symmetry ; for strong coupling they relate respectively to a traditional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states .The latter is demonstrated to be analogous to a spin - 1 / 2 Heisenberg antiferromagnet on the honeycomb structure , which can thus be described within the framework of the so - called SU ( 4 ) bosonic representation .",
        "rewrite_text": "We investigate the quantum Hall effect (QHE) and its relationship with magnetism in monolayer graphene using an effective low-energy theory that accounts for both electron-atom interactions and perturbative effects. Our approach shows how to derive this framework from fundamental principles, and we elucidate some of its key features. Notably, we observe that at half-filling, the system exhibits two distinct phases that depend on the strength of Coulomb interactions among electrons. When the interactions are weak, these phases are differentiated by a phase shift linked to an early breaking of time-reversal symmetry. In contrast, for strong coupling, one phase corresponds to a conventional QHE state, while the other represents a novel fractionalized topological insulator, distinguished by chiral edge states. This latter phase is found to be analogous to a spin-1/2 Heisenberg antiferromagnet on a honeycomb lattice, which can be described within the framework of the SU(4) bosonic representation.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.5,
        "rewrite-fast-z-score": -0.25
    },
    {
        "original_text": "We report the discovery of an extremely bright and hot (T eff = 300,000 K) blue supergiant in the open cluster NGC 6231 with a mass loss rate of 10^-6 M_sun/yr. The object is located at a distance of 1 kpc from Earth and has a luminosity of 5 x 10^{5} L_Sun. It shows strong emission lines of He II 4686 Å, N III 4641 Å, C IV 5801 Å, O V 7322 Å, and H Balmer series. We suggest that this object may be a member of the Cygnus OB2 association which contains several other high-mass stars. This would make it one of the most luminous known single stars outside our Galaxy. If confirmed by further observations, this object will provide important constraints on stellar evolution models for massive stars. Keywords: Open clusters; Blue supergiants",
        "watermark_text": "We report the discovery of an incredibly bright and hot ( T eff = 300 , 000 K ) blue supergiant in the open cluster NGC 6231 with a mass loss rate of 10 ^ - 6 M _ sun / yr . The object is situated at a distance of 1 kpc from Earth and has a luminosity of 5 x 10 ^ { 5 } L _ Sun .It displays large emission lines of He II 4686 Å , N III 4641 Å , C IV 5801 Å , O V 7322 Å , and H Balmer series . We suggest that this body may be a member of the Cygnus OB2 association which contains multiple other high - weight stars .This might making it one of the most luminous known single lights outside our Galaxy . If confirmed by further observations , this body will provide important restrictions on stellar evolution models for huge stars .Keywords : Open clusters ; Blue supergiants",
        "rewrite_text": "We announce the discovery of an exceptionally luminous and hot blue supergiant in the open cluster NGC 6231, with an effective temperature of 300,000 K and a mass loss rate of \\(10^{-6} M_{\\odot} \\) per year. Located 1 kpc away from Earth, this star has a luminosity of \\(5 \\times 10^{5} L_{\\odot}\\). It exhibits prominent emission lines from He II at 4686 Å, N III at 4641 Å, C IV at 5801 Å, O V at 7322 Å, and the H Balmer series. We propose that this star may belong to the Cygnus OB2 association, which is known for housing numerous other massive stars. If this connection is verified through future observations, this star could rank among the most luminous known individual stars outside our galaxy. Confirming its existence will yield crucial insights into stellar evolution models for massive stars.  \nKeywords: Open clusters; Blue supergiants.",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 3.857142857142857,
        "rewrite-fast-z-score": 0.2626128657194451
    },
    {
        "original_text": "We consider the problem of finding an approximate solution to a convex optimization problem with a nonconvex objective function and linear constraints, where the feasible region is defined by a set of equality or inequality constraints.  We show that under certain conditions this problem can be solved efficiently using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method based on the Armijo-Goldstein condition.  The proposed algorithm has been implemented as part of the open source software package CVXPY (http://cvxpy.org/).  Numerical experiments are presented which demonstrate the effectiveness of our approach. Keywords: Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstain Condition. 1 Introduction In many practical applications it may not always be possible to find an exact solution to a given mathematical model due to computational complexity issues associated with the underlying numerical algorithms used to solve such models.  For example, in some cases it might only be possible to obtain an approximate solution within a specified tolerance level.  This situation arises frequently when dealing with large-scale nonlinear programming problems arising in various fields including engineering design, operations research, economics, etc., see e.g.,  1  ,  4  .",
        "watermark_text": "We consider the question of finding an approximate solve to a convex optimization problem with a nonconvex objective function and linear constraints , where the viable region is given by a setting of equality or inequality constraints . We see that under certain conditions this question can be answered easily using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method using on the Armijo - Goldstein requirement .The proposed algorithm has been deployed as part of the open source software suite CVXPY ( http : / / cvxpy . org / ) . Numerical trials are presented which demonstrate the ability of our approach .Keywords : Convex Optimization , Nonconvex Objective Function , Saddle Point Problem , Gradient Descent Method , Line Search Algorithm , Armijo - Goldstain Condition . 1 Introduction In many practical applications it must not always be possible to find an precise answer to a given numerical model owing to computational difficulty issues associated with the fundamental numerical algorithms used to solve such models .For instance , in some cases it could only be possible to obtain an approximate solve within a specified tolerance range . This problem arises often when dealing with large - scale nonlinear programming difficulties arising in different fields spanning technical design , operations research , economics , etc . , see e . g . , 1 , 4 .",
        "rewrite_text": "We explore the challenge of obtaining an approximate solution to a convex optimization problem characterized by a nonconvex objective function and linear constraints, where the feasible region is defined through equality or inequality constraints. Under certain conditions, this issue can be effectively addressed by employing a combination of gradient descent techniques to tackle the subproblems at each iteration, along with a line search method based on the Armijo-Goldstein condition. This algorithm has been integrated into the open-source software suite CVXPY (http://cvxpy.org/). We present numerical experiments that illustrate the effectiveness of our method. \n\nKeywords: Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstein Condition.\n\n1. Introduction\nIn many real-world scenarios, obtaining an exact solution to a numerical model can be infeasible due to the computational challenges associated with the fundamental numerical algorithms required for these models. Consequently, it may only be possible to achieve approximate solutions within a designated tolerance range. This issue frequently arises in large-scale nonlinear programming problems encountered across various domains, including engineering design, operations research, and economics, as discussed in references 1 and 4.",
        "ori-fast-z-score": 0.5883484054145521,
        "water-fast-z-score": 6.995837874966481,
        "rewrite-fast-z-score": 0.20628424925175867
    },
    {
        "original_text": "We present VLT/VIMOS integral field spectroscopic observations for three high-z (z ~ 2.5) radio galaxies, which are known to be surrounded by extended Lyman alpha halos. The main goal is to study their kinematics and physical conditions in order to understand how these objects evolve into massive elliptical galaxies at low redshifts. We find that all three sources show complex velocity fields dominated by rotation around an axis perpendicular to the radio jets. In addition we detect several components showing blueshifted velocities up to -500 km/s relative to systemic redshift. These features may represent outflows driven by AGN feedback or galactic winds powered by star formation activity. Finally, we measure the gas density distribution using  OII  emission lines and estimate the mass of ionized hydrogen surrounding each galaxy. Our results suggest that the observed Lyman alpha halos have masses ranging between 10^10 M_sol and 10^11 M_sol .",
        "watermark_text": "We present VLT / VIMOS integral field spectroscopic observations for three high - z ( z ~ 2 . 5 ) radio objects , which are known to be accompanied by extended Lyman alpha halos . The main goal is to study their kinematics and physical conditions in order to explain how these objects evolve into huge elliptical galaxies at low redshifts .We see that all three sources show complex momentum fields dominated by rotation around an axis diagonal to the radio jets . In addition we find various components showing blueshifted velocities up to - 500 cm / s relative to systemic redshift .These features could indicate outflows driven by AGN feedback or galactic winds driven by galaxy formation activity . Finally , we measure the gas density flow use OII emission lines and estimate the mass of ionized gas covering each galaxy .Our results propose that the known Lyman alpha halos have masses ranging between 10 ^ 10 M _ sol and 10 ^ 11 M _ sol .",
        "rewrite_text": "We present integral field spectroscopic observations from VLT/VIMOS for three high-redshift radio objects (z ~ 2.5), which are associated with extended Lyman alpha halos. The primary aim of our study is to investigate their kinematics and physical conditions to understand how these objects transform into massive elliptical galaxies at lower redshifts. Our findings reveal that all three sources exhibit complex momentum fields, predominantly influenced by rotation along an axis that is tilted relative to the radio jets. Additionally, we observe various components displaying blueshifted velocities of up to -500 km/s with respect to the systemic redshift. These characteristics may suggest the presence of outflows resulting from AGN feedback or galactic winds related to ongoing galaxy formation. Furthermore, we utilize OII emission lines to measure the gas density flow and estimate the mass of ionized gas surrounding each galaxy. Our results indicate that the Lyman alpha halos are estimated to have masses ranging between 10^10 M_solar and 10^11 M_solar.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": 1.811643254631353
    },
    {
        "original_text": "We report on two new brown dwarf candidates discovered by direct imaging with NACO/VLT at distances of ~100 AU to their host star, which are members of open clusters NGC 2423 (M67) and NGC 4349. The first object is located close to the cluster center and has an estimated mass between 0.03-0.07 M . It was found as part of our survey for low-mass companions to nearby young solar-type stars. We also present follow-up observations of this candidate using adaptive optics techniques that confirm its substellar nature. \n \n The second object lies outside the core radius of the cluster and has an estimated mass below 0.01 M . This companion may be either a planetary or very-low-mass stellar companion depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both objects have masses well above those predicted by current models of planet formation via disk accretion.",
        "watermark_text": "We report on two new brown dwarf candidates discovered by direct observation with NACO / VLT at distances of ~ 100 AU to their host star , which are members of close complexes NGC 2423 ( M67 ) and NGC 4349 . The first object is situated close to the cluster center and has an estimated mass between 0 . 03 - 0 . 07 M .It was obtained as part of our survey for low - weight companions to nearby young solar - class stars . We additionally post follow - up observations of this candidate using adaptive optics techniques that prove its substellar nature .The second object lies outside the core radius of the cluster and has an estimated mass below 0 . 01 M . This companion might be either a planetary or very - low - weight stellar companion depending on whether it created through gravity instability or fragmentation during cloud collapse .Both bodies have masses well above those predicted by current theories of planet development via disk accretion .",
        "rewrite_text": "We present two new candidates for brown dwarfs discovered through direct observation using NACO/VLT, located approximately 100 AU from their host stars, which are part of the nearby clusters NGC 2423 (M67) and NGC 4349. The first object is positioned near the center of the cluster and is estimated to have a mass ranging from 0.03 to 0.07 M. This finding is part of our survey aimed at identifying low-mass companions to nearby young solar-type stars. We also provide follow-up observations employing adaptive optics techniques, confirming its substellar nature. The second object is found beyond the core radius of the cluster and is estimated to have a mass below 0.01 M. This companion could either be a planetary object or a very low-mass stellar companion, depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both of these bodies have masses that exceed current predictions for planet formation via disk accretion.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.808290376865476
    },
    {
        "original_text": "We report on an observation made with Suzaku satellite to study X-ray variability of active galactic nucleus (AGN) NGC 3783, which is known as one of brightest Seyfert 1 galaxies at soft X-rays. We found that the fluxes of both the iron Kα line and the continuum are modulated by a factor of about two over a period of 100 ks. The phase lag between these two components was estimated to be ~0.1 s using cross-correlation analysis. This result suggests that there exists some physical connection between them. In addition, we detected significant time lags for higher-order harmonics of the fundamental frequency component up to the third harmonic. These results suggest that the observed variability may originate from reverberation effects caused by variable illumination of the accretion disk around supermassive black hole. Keywords: Reverberation; Time lag; Iron line; AGN",
        "watermark_text": "We report on an observation made with Suzaku spacecraft to study X - ray variability of active galactic nucleus ( AGN ) NGC 3783 , which is known as one of brightest Seyfert 1 clusters at soft X - rays . We determined that the fluxes of both the metal Kα line and the continuum are modulated by a factor of about two over a period of 100 ks .The wave lag between these two parts was calculated to be ~ 0 . 1 s using cross - correlation analysis . This result suggests that there exists some physical link between them .In addition , we identified significant moment lags for greater - order harmonics of the fundamental frequency component up to the third harmonic . These conclusions show that the seen variability may originate from reverberation effects caused by variable illumination of the accretion disk around supermassive black hole .Keywords : Reverberation ; Time lag ; Iron line ; AGN",
        "rewrite_text": "We present findings from an observation conducted with the Suzaku spacecraft, aimed at examining the X-ray variability of the active galactic nucleus (AGN) NGC 3783, which is recognized as one of the brightest Seyfert 1 galaxies in the soft X-ray range. Our analysis revealed that both the fluxes of the metal Kα line and the continuum exhibit modulation by a factor of approximately two over a 100 ks period. Using cross-correlation analysis, we determined a wave lag of approximately 0.1 seconds between these two components, indicating a potential physical connection between them. Furthermore, we observed significant moment lags for higher-order harmonics of the fundamental frequency component, extending up to the third harmonic. These findings imply that the observed variability may be a result of reverberation effects due to the variable illumination of the accretion disk surrounding the supermassive black hole. \nKeywords: Reverberation; Time lag; Iron line; AGN.",
        "ori-fast-z-score": 1.6641005886756874,
        "water-fast-z-score": 5.547001962252292,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present nonsupersymmetric brane configurations in type IIA string theory, which are obtained by wrapping D-branes on supersymmetry-breaking cycles. We also discuss the corresponding configurations in M-theory. In particular we show that these configurations can be described as bound states of intersecting NS5-branes with orientifold 5-planes (or O6-planes). The latter are related to each other via T-duality transformations. Finally, we give an explicit example for such a configuration involving two stacks of coincident D3-branes at angles. This is done using the technique developed recently by Sen. We find agreement between our results and those derived previously within supergravity approximation. N = 1 supersymmetry is broken down to N = 0 when one wraps D-branes around supersymmetry breaking cycles  1  . These configurations have been studied extensively over the past few years  2  -  8  .\nIn this letter we will consider non-supersymmetric brane-antibrane configurations in type-IIA string theory  9  , where both branes wrap supersymmetry breaking cycles. Such configurations were first discussed in  10  . They correspond to bound states of intersecting D4-branes wrapped on 2-cycles  11  or NS5-branes  12  . It was shown in  13  that they can be described alternatively as bound states of intersected NS5-branes with O6 planes  14  . Here we will use the description given in terms of NS5-O6 systems  15  .",
        "watermark_text": "We introduce nonsupersymmetric brane configurations in type IIA string theory , which are derived by wrapping D - branes on supersymmetry - breaking cycles . We also discuss the analogous structures in M - theory .In particular we find that these structures can be described as bound states of intersecting NS5 - branes with orientifold 5 - planes ( or O6 - planes ) . The latter are related to each other via T - duality transformations .Finally , we give an explicit instance for such a configuration involving two stacks of coincident D3 - branes at angles . This is accomplished use the method developed ago by Sen . We get consensus between our findings and those generated previously within supergravity approximation .N = 1 supersymmetry is broken down to N = 0 when one wrapping D - branes around supersymmetry breaking cycles 1 . These arrangements have been studied frequently over the previous few years 2 - 8 .In this letter we will use non - supersymmetric brane - antibrane configurations in type - IIA string theory 9 , where both branes wrap supersymmetry broken cycles . Such configurations were first explored in 10 .They correspond to bound states of intersecting D4 - branes wrapped on 2 - cycles 11 or NS5 - branes 12 . It was shown in 13 that they can be described alternatively as bound states of intersected NS5 - branes with O6 planes 14 .Here we will use the description presented in terms of NS5 - O6 systems 15 .",
        "rewrite_text": "We present nonsupersymmetric brane configurations within type IIA string theory, achieved by wrapping D-branes around cycles that break supersymmetry. We also explore similar structures in M-theory, particularly identifying these configurations as bound states of intersecting NS5-branes and orientifold 5-planes (or O6-planes), which are interconnected through T-duality transformations. Additionally, we provide a specific example of this configuration featuring two stacks of coincident D3-branes positioned at an angle, utilizing the methodology previously established by Sen. Our results align with earlier findings derived from supergravity approximations, indicating a breakdown of N=1 supersymmetry to N=0 when D-branes are wrapped around supersymmetry-breaking cycles. These configurations have been the focus of considerable study over recent years. In this letter, we will examine nonsupersymmetric brane-antibrane configurations in type IIA string theory, where both branes wrap around cycles that break supersymmetry. Such configurations were initially investigated previously and can be understood as bound states of intersecting D4-branes wrapped on 2-cycles or NS5-branes. It has been demonstrated that they can alternatively be characterized as bound states of intersecting NS5-branes with O6-planes. Here, we will employ the description based on NS5-O6 systems.",
        "ori-fast-z-score": 0.6625891564490792,
        "water-fast-z-score": 5.300713251592634,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "We study central limit theorems for estimators of parameters in linear regression models where errors are not necessarily normally distributed but have an elliptical distribution, and we allow some explanatory variables to be non-normal. We show that under suitable conditions on the model coefficients, the asymptotic distributions of these estimators can be approximated by those obtained when all the explanatory variables follow a multivariate normal distribution. The results are illustrated through simulation experiments. Keywords: Central Limit Theorem; Elliptical Distributions; Regression Modeling. 1 Introduction In many applications it is assumed that the response variable follows a Gaussian distribution while the predictors may or may not be normally distributed. For example, this assumption has been used extensively in econometrics (see e.g., Greene  2003  ). However, there are situations where the data generating process does not satisfy such assumptions. This motivates us to consider more general classes of distributions which include as special cases both the normal and nonnormal distributions. One class of distributions that includes most common probability density functions encountered in practice is given by the so-called elliptical distributions. These distributions were introduced independently by Kelker  1970  , Hüsler and Reiss  1981  , and Fang et al.  1987  . They are characterized by their dependence structure rather than their marginal densities. A random vector X = (X1, ..., Xd)T ∈ Rd belongs to the family of elliptical distributions if its characteristic function satisfies E exp(itX)  = exp{−V (t)},\nwhere V : R →  0, ∞) is called the characteristic generator. If V ≡ 0 then X is said to belong to the family of spherical distributions. Examples of elliptical distributions include:",
        "watermark_text": "We research central limit theorems for estimators of values in linear regression systems where errors are not necessarily normally distributed but have an elliptical distribution , and we allow some explanatory variables to be non - normal . We see that under suitable conditions on the model coefficients , the asymptotic distributions of these estimators can be approximated by those achieved when all the explanatory variables obey a multivariate normal distribution .The results are shown through simulation studies . Keywords : Central Limit Theorem ; Elliptical Distributions ; Regression Modeling .1 Introduction In many applications it is implied that the response parameter follows a Gaussian distribution while the predictors may or may not be usually spread . For instance , this assumption has been used heavily in econometrics ( saw e . g . , Greene 2003 ) .However , there are circumstances where the information processing process does not satisfy such constraints . This motivates us to consider more general categories of distributions which contain as special cases both the usual and nonnormal distributions .One class of distributions that contains most common probability distribution functions seen in practice is given by the so - called elliptical distributions . These distributions were introduced independently by Kelker 1970 , Hüsler and Reiss 1981 , and Fang et al .1987 . They are characterized by their dependence structure rather than their marginal densities .A random matrix X = ( X1 , . . . , Xd ) T ∈ Rd belongs to the class of elliptical distributions if its characteristic function satisfies E exp ( itX ) = exp { −V ( t ) } , where V : R → 0 , ∞ ) is dubbed the characteristic generator . If V ≡ 0 then X is said to belong to the group of spherical distributions .Examples of elliptical distributions involve :",
        "rewrite_text": "We investigate central limit theorems pertaining to estimators of parameters in linear regression models where the errors may not follow a normal distribution, but instead have an elliptical distribution. Moreover, we consider scenarios where some explanatory variables do not conform to a normal distribution. Our findings indicate that, under suitable conditions on the model coefficients, the asymptotic distributions of these estimators can be closely approximated by those resulting from a model where all explanatory variables follow a multivariate normal distribution. These results are illustrated through simulation studies. \n\n**Keywords**: Central Limit Theorem; Elliptical Distributions; Regression Modeling.\n\n**1. Introduction**  \nIn various applications, it is often assumed that the response variable follows a Gaussian distribution, while the predictors can have diverse distributions. This assumption is particularly prevalent in econometrics (see, for example, Greene 2003). However, there are situations in which the data does not meet such assumptions. This observation leads us to explore broader categories of distributions that encompass both conventional and non-normal distributions. One such category, known as elliptical distributions, includes many commonly encountered probability distribution functions. These distributions were independently introduced by Kelker (1970), Hüsler and Reiss (1981), and Fang et al. (1987). Elliptical distributions are defined by their dependence structure rather than their marginal densities. A random vector \\( X = (X_1, \\ldots, X_d)^T \\in \\mathbb{R}^d \\) is considered to belong to the elliptical distribution class if its characteristic function satisfies \\( E[\\exp(itX)] = \\exp\\{-V(t)\\} \\), where \\( V: \\mathbb{R} \\to (0, \\infty) \\) is referred to as the characteristic generator. If \\( V \\equiv 0 \\), then \\( X \\) is classified as belonging to the family of spherical distributions. Examples of elliptical distributions include:",
        "ori-fast-z-score": -1.165543034828717,
        "water-fast-z-score": 4.930356094132884,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the Laplacian spectrum of complex networks, focusing in particular on its relation to the dynamics of random walks on them. We show that for any network with n nodes there are at most 2n eigenvalues different from zero (counting multiplicities). This bound is tight up to a constant factor when applied to trees or complete graphs. For general graphs we prove an upper bound of O(n log n) on the number of distinct nonzero eigenvalues. In addition, we provide lower bounds showing that this estimate cannot be improved by more than a polylogarithmic factor. Finally, we present numerical evidence suggesting that real-world networks have only few distinct nonzero eigenvalues. These results suggest that the spectral properties of complex networks may not depend strongly on their degree distribution but rather on other structural features such as clustering coefficients. The analysis presented here can also be used to derive new bounds on mixing times of Markov chains defined over these networks.",
        "watermark_text": "We research the Laplacian spectrum of complex networks , concentrating in particular on its connection to the dynamics of random runs on them . We see that for any network with n nodes there are at most 2n eigenvalues different from zero ( counting multiplicities ) .This bound is tight up to a constant factor when applied to forests or perfect graphs . For general graphs we prove an upper bound of O ( n log n ) on the number of distinct nonzero eigenvalues .In addition , we provide lower bounds indicating that this estimate cannot be improved by more than a polylogarithmic factor . Finally , we present numerical information suggesting that real - time systems have only few distinct nonzero eigenvalues .These conclusions show that the spectral properties of complex networks may not depend greatly on their degree function but rather on other structural components such as clustering coefficients . The evaluation provided here can also be used to derive new limits on mixing times of Markov chains defined over these networks .",
        "rewrite_text": "We investigate the Laplacian spectrum of complex networks, with a particular focus on its relationship to the dynamics of random walks on these networks. Our findings indicate that for any network with \\( n \\) nodes, there can be at most \\( 2n \\) eigenvalues differing from zero (considering multiplicities). This bound is nearly tight—up to a constant factor—for forests and perfect graphs. For more general graphs, we establish an upper limit of \\( O(n \\log n) \\) on the number of distinct nonzero eigenvalues. Additionally, we present lower bounds that suggest this estimate cannot be improved by more than a polylogarithmic factor. Finally, our numerical analysis implies that real-time systems exhibit relatively few distinct nonzero eigenvalues. These results indicate that the spectral characteristics of complex networks may be influenced more by other structural elements, such as clustering coefficients, than by their degree distribution. The insights offered here could also inform new constraints on the mixing times of Markov chains defined on these networks.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.47213595499958,
        "rewrite-fast-z-score": -0.7777777777777778
    },
    {
        "original_text": "We report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "We report the observation of dynamic condensation of liquid vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) . The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip .This phenomenon has been observed for faults propagating both perpendicularly and parallel to the direction of maximum tensile tension . We suggest a theory based on protein mechanics simulations which explains this effect by assuming the presence of an electric field produced by the moved crack edge .In addition we study how the formation of such films can affect the structural properties of the material . Condensation problems are ubiquitous in nature but have seldom been reported in materials science .Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses . These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection detection spectroscopy ( IRAS ) .",
        "rewrite_text": "We present our findings on the dynamic condensation of liquid vapor at crack tips during slow fracture experiments conducted in vacuum conditions (10^-6 mbar) and at low temperatures (77 K). The condensed water is observed to accumulate along the crack front, forming a thin film that uniformly covers the entire crack tip surface. This phenomenon has been documented for fault propagation occurring both perpendicularly and parallel to the direction of maximum tensile stress. We propose a theoretical framework based on protein mechanics simulations, which attributes this effect to an electric field generated by the moving crack edge. Furthermore, we investigate how the formation of these films influences the structural properties of the material. While condensation issues are common in nature, they have rarely been discussed in the context of materials science. Our study provides experimental evidence that water condenses on the surfaces of cracks as they propagate through fused silica glasses. These findings were derived from a combination of optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection detection spectroscopy (IRAS).",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 5.1759731137650435,
        "rewrite-fast-z-score": 2.618614682831909
    },
    {
        "original_text": "We present the results of an analysis of rise times for a sample of type Ia supernovae (SNeIa) with well-measured light curves and redshifts in order to determine whether there is one mode of SNeIa rise time, as suggested by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). We find that our data are consistent with either model at the 2-sigma level; however, we cannot rule out the possibility of only one mode being present. The best-fit values for the parameters describing each distribution differ significantly between these models. If future observations confirm this result, it will have important implications on cosmological studies using SNeIa as distance indicators. In particular, if there really are two populations of SNeIa, then the use of a single value for the stretch parameter may lead to systematic errors in determining distances. \n \n Keywords: Supernova, Light curve",
        "watermark_text": "We present the conclusion of an assessment of rise periods for a sample of type Ia supernovae ( SNeIa ) with good - measured light curves and redshifts in order to find whether there is one mode of SNeIa rise history , as suggested by Phillips et al . ( 1999 ) , or two modes , as suggested by Riess et al .( 1999 ) . We see that our statistics are compatible with either model at the 2 - sigma level ; however , we cannot leave out the suggestion of only one mode being present .The best - fitting values for the variables describing each distribution depend greatly between these models . If future discoveries confirm this result , it will have important implications on cosmological experiments using SNeIa as distance indicators .In particular , if there really are two groups of SNeIa , then the using of a single value for the stretch variable might lead to systematic errors in calculating distances . Keywords : Supernova , Light curve",
        "rewrite_text": "We present the findings from our assessment of rise periods for a sample of type Ia supernovae (SNeIa) that have well-measured light curves and redshifts. Our objective was to determine whether there is a single mode of SNeIa rise history, as proposed by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). Our statistical analysis shows that both models are compatible at a 2-sigma level; however, we cannot dismiss the possibility of a single mode being present. The best-fit values for the parameters characterizing each distribution vary significantly across these models. If future discoveries support our findings, it will have significant implications for cosmological experiments using SNeIa as distance indicators. Notably, if two distinct groups of SNeIa exist, applying a single value for the stretch variable could introduce systematic errors in distance calculations. Keywords: Supernova, Light curve.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "We consider the problem of optimal investment in a financial market when there is no upper bound on the investor s wealth, but his/her utility function exhibits decreasing absolute risk aversion (DARA). We show that under DARA preferences, the value function for this problem can be characterized as the unique solution to a nonlinear partial differential equation (PDE) which we call the Hamilton-Jacobi-Bellman-Isaacs PDE. This characterization allows us to use standard numerical methods such as finite difference or Monte Carlo simulation to compute the value function numerically. In addition, it also enables us to study how the optimal strategy depends on various parameters including the initial endowment, the interest rate, and the volatility of the stock price process. Finally, by using our results, we are able to provide some new insights into the relationship between pricing and hedging derivatives based on utility maximization principles. The main contributions of this work include:",
        "watermark_text": "We consider the question of optimal investment in a financial market when there is no upper bound on the investor s assets , but his / her utility function exhibits decreasing absolute price aversion ( DARA ) . We see that under DARA preferences , the value function for this question can be described as the unique solve to a nonlinear partial differential equation ( PDE ) which we call the Hamilton - Jacobi - Bellman - Isaacs PDE .This definition permits us to use conventional numerical technique such as finite difference or Monte Carlo simulation to compute the value function numerically . In addition , it also enables us to study how the ideal scenario depends on various variables including the early endowment , the interest rate , and the volatility of the stock price cycle .Finally , by using our findings , we are able to provide some fresh insights into the relationship between pricing and hedging derivatives using on utility maximization theories . The main contributions of this research include :",
        "rewrite_text": "We explore the optimal investment strategies in a financial market where there are no limits on an investor's assets, but the investor's utility function displays decreasing absolute risk aversion (DARA). It is found that under DARA preferences, the value function for this scenario can be represented as the unique solution to a nonlinear partial differential equation (PDE), which we refer to as the Hamilton-Jacobi-Bellman-Isaacs PDE. This formulation allows for the application of standard numerical methods, such as finite difference techniques or Monte Carlo simulations, to calculate the value function. Furthermore, it facilitates an investigation into how the optimal investment strategy is influenced by various factors, including initial endowments, interest rates, and stock price volatility. Ultimately, our results yield new insights into the connections between pricing and hedging derivatives within the framework of utility maximization. The primary contributions of this research include:",
        "ori-fast-z-score": 1.4142135623730951,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "We present deep optical photometry in B, V , R c I c bands for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope on La Silla Observatory. The data were reduced using standard IRAF routines. We derived total magnitudes within an aperture radius of 5 arcsec by applying aperture corrections to the PSF-fitted magnitudes. Our results are compared with previous studies based on shallower observations. In addition we derive new estimates for the distance modulus DM = 27.9 ± 0.1 mag and foreground extinction A V = 0.10 ± 0.02 mag towards this galaxy. Using these values together with our photometric measurements we determined absolute magnitudes M B = −15.6 ± 0.3 mag, M V = −14.7 ± 0.4 mag, M Rc = −12.8 ± 0.5 mag, M Ic = −11.0 ± 0.6 mag and colour indices U−B = 1.45±0.25 mag, B−V =0.70±0.06 mag, V −Rc=0.55±0.05 mag, V −Ic=1.00±0.07 mag. These parameters allow us to estimate the mean metallicity Z = 0.008 ± 0.001 dex and age t = 3 Gyrs for the stellar population of IC 1613.",
        "watermark_text": "We use deep optical photometry in B , V , R c I c groups for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager ( WFI ) at the MPG / ESO 2 . 2 m observatory on La Silla Observatory . The data were reduced use standard IRAF procedures .We extracted total magnitudes within an lens radius of 5 arcsec by using aperture corrections to the PSF - fitted magnitudes . Our results are compared with previous research based on shallower observations .In addition we derive new models for the distance modulus DM = 27 . 9 ± 0 . 1 mag and foreground extinction A V = 0 . 10 ± 0 . 02 mag towards this galaxy . Using these estimates together with our photometric calculations we calculated absolute magnitudes M B = −15 . 6 ± 0 . 3 mag , M V = −14 . 7 ± 0 . 4 mag , M Rc = −12 . 8 ± 0 . 5 mag , M Ic = −11 . 0 ± 0 . 6 mag and colour indices U−B = 1 . 45±0 . 25 mag , B−V = 0 . 70±0 . 06 mag , V −Rc = 0 . 55±0 . 05 mag , V −Ic = 1 . 00±0 . 07 mag .These parameters allow us to estimate the mean metallicity Z = 0 . 008 ± 0 . 001 dex and age t = 3 Gyrs for the stellar population of IC 1613 .",
        "rewrite_text": "We conducted deep optical photometry in the B, V, R_c, and I_c bands for the dwarf irregular galaxy IC 1613, using data obtained with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope at La Silla Observatory. The data reduction was performed via standard IRAF procedures. To determine total magnitudes, we applied aperture corrections to the PSF-fitted magnitudes within a lens radius of 5 arcseconds. Our findings are compared with earlier studies based on shallower observations. Additionally, we present new estimates for the distance modulus (DM = 27.9 ± 0.1 mag) and foreground extinction (A_V = 0.10 ± 0.02 mag) for IC 1613. Using these values along with our photometric measurements, we calculated the absolute magnitudes: M_B = -15.6 ± 0.3 mag, M_V = -14.7 ± 0.4 mag, M_Rc = -12.8 ± 0.5 mag, and M_Ic = -11.0 ± 0.6 mag, as well as the color indices: U−B = 1.45 ± 0.25 mag, B−V = 0.70 ± 0.06 mag, V−R_c = 0.55 ± 0.05 mag, and V−I_c = 1.00 ± 0.07 mag. These parameters enable us to estimate the mean metallicity of the stellar population in IC 1613 to be Z = 0.008 ± 0.001 dex and its age to be approximately 3 billion years.",
        "ori-fast-z-score": -0.2626128657194451,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 1.3643820804812932
    },
    {
        "original_text": "We propose that the most energetic cosmic rays are accelerated in supernova remnants by relativistic jets powered by hypernova explosions, which may be associated with gamma-ray bursts (GRBs). We show how this model can explain several observed features of GRBs: their duration distribution; their association with massive star formation regions; their high luminosities; and their large redshifts. The proposed mechanism is also able to accelerate protons up to energies beyond 10^20 eV without violating current observational constraints on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario provides an explanation for the origin of ultra-high energy cosmic rays as well as for the production of the highest energy neutrinos detected so far. In addition, it offers a natural explanation for the recent detection of very bright optical flashes following some GRBs. \n \n High-energy cosmic rays have been measured at Earth over many decades  1  . Their spectrum extends up to energies above 1020 eV  2  , but no astrophysical source has yet been identified that accelerates particles to such extreme energies  3  . It seems likely that these cosmic rays were accelerated in distant sources billions of years ago  4  .\n \nThe most powerful known explosion in our Universe occurs when a massive star collapses into a black hole after exhausting its nuclear fuel supply  5  . Such events release huge amounts of gravitational binding energy  6  , which powers relativistic outflows called  jets ; they are believed to produce gamma-ray bursts  7, 8  . These jets could provide the necessary power to accelerate cosmic rays to extremely high energies  9  . \n \n However, there are two major difficulties in explaining the origin of the most energetic cosmic ray particles using conventional models  10  : \n \n 1) Conventional jet-powered models cannot accelerate protons to energies greater than ~10^19 eV  11  because the maximum Lorentz factor Γmax of the flow decreases rapidly with distance r from the central engine  12  . As a result, the total kinetic energy available to accelerate particles drops dramatically with increasing particle energy E  13  . For example, if we assume that the bulk Lorentz factor of the",
        "watermark_text": "We suggest that the most intense cosmic rays are accelerated in supernova remnants by relativistic jets driven by hypernova bursts , which may be involved with gamma - ray clusters ( GRBs ) . We see how this model can describe several observed features of GRBs : their duration distribution ; their association with massive star formation regions ; their high luminosities ; and their large redshifts .The proposed process is also could to accelerate protons up to energies beyond 10 ^ 20 eV without violating present observational restrictions on the diffuse fluxes of high - energy neutrinos or photons generated during the acceleration cycle . This scenario offers an reason for the origin of ultra - large energy cosmic rays as well as for the production of the highest power neutrinos detected so far .In addition , it gives a natural explanation for the recent discovery of very bright optical bursts following some GRBs . High - energy cosmic rays have been measured at Earth over much centuries 1 .Their range extends up to frequencies above 1020 eV 2 , but no astrophysical source has already been determined that accelerates particles to such extreme energies 3 . It seems likely that these cosmic rays were accelerated in nearby sources billions of years early 4 .The most intense reported blast in our Universe comes when a huge star collapses into a black hole after exhausting its radioactive fuel supply 5 . Such episodes release massive amounts of gravitational binding energy 6 , which powers relativistic outflows called rockets ; they are said to produce gamma - ray waves 7 , 8 .These jets could give the necessary power to accelerate cosmic rays to incredibly high energies 9 . However , there are two major obstacles in understanding the origin of the most intense cosmic ray ions using conventional versions 10 : 1 ) Conventional jet - powered designs cannot accelerate protons to energies higher than ~ 10 ^ 19 eV 11 because the maximum Lorentz factor Γmax of the flow varies dramatically with distance r from the main engine 12 .As a result , the total kinetic power available to accelerate particles decreases dramatically with rising particle power E 13 . For instance , if we suppose that the bulk Lorentz factor of the",
        "rewrite_text": "We propose that the most powerful cosmic rays are generated in supernova remnants through relativistic jets produced by hypernova explosions, which may also connect to gamma-ray bursts (GRBs). Our model accounts for several observed characteristics of GRBs, such as their duration distribution, their link to regions of massive star formation, their exceptionally high luminosities, and their significant redshifts. Additionally, this mechanism could accelerate protons to energies exceeding 10^20 eV without conflicting with current observational limits on the diffuse fluxes of high-energy neutrinos or photons created during the acceleration process. This scenario not only provides insight into the origin of ultra-high-energy cosmic rays but also explains the production of the most powerful neutrinos ever detected. Furthermore, it offers a straightforward explanation for the recent observation of extraordinarily bright optical bursts that occur after some GRBs. High-energy cosmic rays have been recorded on Earth for centuries, with energies reaching above 10^20 eV, yet no known astrophysical source has been identified that can accelerate particles to such extreme levels. It is plausible that these cosmic rays were produced in nearby sources billions of years ago. The most intense explosion observed in our universe occurs when a massive star collapses into a black hole after depleting its nuclear fuel, releasing vast amounts of gravitational binding energy. This energy powers relativistic outflows known as jets, which are believed to generate gamma-ray emissions. These jets could potentially provide the energy needed to accelerate cosmic rays to extremely high energies. Nevertheless, two significant challenges hinder our understanding of how conventional models account for the origins of the most intense cosmic ray ions. First, traditional jet-powered models cannot achieve proton acceleration beyond approximately 10^19 eV, as the maximum Lorentz factor (Γmax) of the flow significantly diminishes with distance from the power source. Consequently, the total kinetic energy available for particle acceleration sharply declines with increasing particle energy. For example, if we consider that the bulk Lorentz factor of the...",
        "ori-fast-z-score": -0.22677868380553634,
        "water-fast-z-score": 8.187458870652156,
        "rewrite-fast-z-score": 0.07692307692307693
    },
    {
        "original_text": "We have performed molecular dynamics simulations to investigate the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant, i-AlCuFe. The results show that both materials exhibit similar features for the crack growth process at low temperatures (T = 300 K). However, there are significant differences between them when T is increased up to 600 K. In particular, we find that the quasicrystal shows a higher resistance against crack propagation than the approximant under tensile loading conditions. This behavior can be explained by considering the different atomic structures of these two systems. \n \n We also studied how the temperature affects the mechanical properties of the quasicrystal. Our results indicate that increasing the temperature leads to a decrease in the elastic constants C11 and C44 as well as in the bulk modulus B. Moreover, our calculations reveal that the Young s moduli E decreases with increasing temperature.",
        "watermark_text": "We have done chemical dynamics simulations to examine the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant , i - AlCuFe . The results show that both materials exhibit similar features for the crack growth process at low temperatures ( T = 300 K ) .However , there are significant variations between them when T is expanded up to 600 K . In particular , we find that the quasicrystal demonstrates a higher resistance against fracture propagation than the approximant under tensile loading circumstances . This phenomenon can be described by analyzing the different atomic relationships of these two systems .We additionally investigated how the temperature changes the mechanical behavior of the quasicrystal . Our results show that raising the temperature leads to a reduction in the elastic constants C11 and C44 as well as in the bulk modulus B .Moreover , our calculations reveal that the Young s moduli E decreases with expanding temperature .",
        "rewrite_text": "We conducted chemical dynamics simulations to investigate the dynamic crack propagation in an icosahedral AlPdMn quasicrystal and its periodic approximant, i-AlCuFe. The findings indicate that both materials exhibit comparable crack growth behaviors at low temperatures (T = 300 K). However, significant differences arise as the temperature increases to 600 K. Notably, the quasicrystal demonstrates greater resistance to fracture propagation compared to the approximant under tensile loading conditions. This behavior can be explained by examining the distinct atomic relationships between the two systems. We also explored how temperature affects the mechanical properties of the quasicrystal. Our results suggest that increasing temperature results in decreased elastic constants C11 and C44, as well as a reduction in the bulk modulus B. Additionally, our calculations show that the Young's modulus E decreases with rising temperature.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 5.938574464184706,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "We present an efficient numerical scheme to detect the existence of periodic orbits in chaotically behaving dynamical systems, such as chaotic maps or chaotic flows. The proposed algorithm is based on the concept of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor. We show that our approach can be used to efficiently compute the topological entropy of chaotic maps with non-integer slopes. Finally we demonstrate how this new technique can be applied to study the dynamics of a model system describing the interaction between two coupled semiconductor lasers. Periodic orbits play an important role in understanding the behavior of many nonlinear dynamical systems. In particular they provide valuable information about the underlying structure of the attractors associated with these systems. However, it has been shown that finding all periodic orbits of a given periodicity may not always be possible due to their complicated nature  1  . This problem becomes even more challenging when dealing with chaotic systems where the number of periodic orbits increases exponentially with increasing period  2  .\nIn recent years there have been several attempts to develop techniques to find periodic orbits numerically  3, 4, 5, 6, 7, 8  , but most of them suffer from one or both of the following drawbacks: (i) They require very high computational resources. (ii) They do not guarantee convergence towards the desired orbit. Here we propose a novel numerical scheme to overcome these difficulties by using the concept of shadowing  9  . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor. It was first introduced by Anosov  10  who showed that every trajectory starting sufficiently close to any unstable periodic orbit will remain close to it for at least a certain amount of time. Since then various authors  11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44",
        "watermark_text": "We present an efficient numerical system to identify the existence of periodic orbits in chaotically behaving dynamical systems , such as chaotic maps or turbulent flows . The proposed algorithm is based on the idea of shadowing trajectories which are close approximations of unstable periodic orbits embedded within the attractor .We see that our approach can be used to easily compute the topological entropy of turbulent maps with non - integer peaks . Finally we prove how this new technique can be applied to study the dynamics of a prototype system describing the interaction between two coupled semiconductor lasers .Periodic orbits take an important role in understanding the dynamics of several nonlinear dynamical systems . In particular they give valuable info about the fundamental structure of the attractors associated with these systems .However , it has been shown that finding all periodic orbits of a given periodicity might not always be possible due to their complicated nature 1 . This problem arises even more challenging when dealing with turbulent systems where the number of periodic orbits changes exponentially with expanding period 2 .In past decades there have been numerous attempts to develop techniques to find periodic orbits numerically 3 , 4 , 5 , 6 , 7 , 8 , but most of them suffer from one or both of the following drawbacks : ( i ) They require very high computational resources . ( ii ) They do not guarantee convergence towards the desired orbit .Here we develop a new numerical system to overcome these problems by using the idea of shadowing 9 . Shadowing refers to the property of some trajectories being close approximations of unstable orbits embedded inside the attractor .It was first developed by Anosov 10 who demonstrated that every orbital beginning sufficiently next to any weak periodic orbit will remain close to it for at least a certain quantity of time . Since then various authors 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44",
        "rewrite_text": "We introduce a novel numerical approach for detecting periodic orbits within chaotically behaving dynamical systems, such as chaotic maps or turbulent flows. Our proposed algorithm leverages the concept of shadowing trajectories, which serve as close approximations of unstable periodic orbits residing within the attractor. This method allows for the straightforward calculation of the topological entropy in turbulent maps that feature non-integer peaks. Additionally, we demonstrate the applicability of this technique to analyze the dynamics of a prototype system that models the interaction between two coupled semiconductor lasers. Periodic orbits play a crucial role in understanding the dynamics of various nonlinear dynamical systems, providing essential insights into the fundamental structure of their associated attractors. However, it has been demonstrated that identifying all periodic orbits of a specific periodicity can be challenging due to their intricate nature. This challenge is exacerbated in turbulent systems, where the quantity of periodic orbits grows exponentially with increasing period. Over the past few decades, numerous efforts have been made to develop numerical methods for locating periodic orbits, but many of these approaches face significant limitations, including (i) high computational demands and (ii) lack of convergence assurance towards the desired orbit. Our new numerical system addresses these issues by employing the concept of shadowing, which indicates that certain trajectories closely approximate unstable orbits embedded within the attractor. This concept was first established by Anosov, who showed that any orbit commencing near a weak periodic orbit will remain close to it for a substantial duration. Since then, various researchers have contributed to the exploration and application of shadowing techniques.",
        "ori-fast-z-score": -0.43033148291193524,
        "water-fast-z-score": 6.688444820557844,
        "rewrite-fast-z-score": -0.26413527189768715
    },
    {
        "original_text": "We report on the fabrication and characterization of electro-optically tunable microresonator devices based on proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE process is used to create an optical waveguide with low loss, high index contrast, and large nonlinearity within the substrate material. A ring-resonator geometry is then defined by electron-beam lithography followed by reactive ion etching. Finally, Ti/Au electrodes are deposited onto both sides of the device for electrical tuning. We demonstrate continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device. This work represents one step towards realizing electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulators. Lithium niobate has been widely studied as a promising candidate for optoelectronics applications due to its excellent properties such as wide transparency range, large second-order susceptibility, and relatively low propagation losses  1  . In addition, it also exhibits strong piezoelectric and pyroelectric effects which make it possible to achieve efficient electro-optic modulation  2  .\nIn this letter we present our recent results on the development of electro-optically tuned microring resonators made out of lithium niobate. These devices were designed and fabricated on commercially available lithium niobate wafers bonded to silicon dioxide  3  , where the top cladding layer was removed prior to processing. First, a proton-exchange (PE) process  4  was performed to grow a single-mode ridge-waveguide structure inside the bulk LiNbO 3 crystal  5  . Then, a ring-resonator geometry was patterned into the PE-grown region via electron beam lithography  6  . Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to provide electrical access to the device  7, 8  . Figure 1 shows scanning-electron-microscope images of two different types of microring resonators that have been successfully demonstrated so far. Both devices consist of",
        "watermark_text": "We report on the fabrication and identification of electro - optically tunable microresonator devices using on proton exchange ( PE ) grown lithium niobate - on - insulator substrates . The PE method is utilized to create an optical waveguide with little loss , low index contrast , and large nonlinearity within the substrate material .A ring - resonator configuration is then established by electron - laser lithography preceded by reactive ion etching . Finally , Ti / Au electrodes are deposited onto both sides of the device for electrical tuning .We demonstrate constant wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device . This research provides one step towards developing electrically - tuned integrated photonic devices that can be monolithically manufactured on insulators .Lithium niobate has been widely explored as a desirable candidate for optoelectronics applications due to its exceptional properties such as wide clarity range , large second - order susceptibility , and fairly little propagation losses 1 . In addition , it also exhibits strong piezoelectric and pyroelectric influences which make it able to achieve effective electro - optic modulation 2 .In this letter we present our latest findings on the development of electro - optically modified microring resonators made out of lithium niobate . These systems were built and manufactured on commercially used lithium niobate wafers bonded to silicon dioxide 3 , where the bottom cladding surface was eliminated prior to processing .First , a proton - transfer ( PE ) process 4 was done to expand a single - mode ridge - waveguide structure inside the bulk LiNbO 3 crystal 5 . Then , a ring - resonator configuration was patterned into the PE - grown region via electron beam lithography 6 .Finally , titanium / gold ( Ti / Au ) contacts were evaporated onto both sides of the sample to provide electrical access to the device 7 , 8 . Figure 1 shows scan - electron - microscope photographs of two different kinds of microring resonators that have been successfully shown so far .Both devices consist of",
        "rewrite_text": "We present our findings on the fabrication and characterization of electro-optically tunable microresonator devices utilizing proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE technique is employed to develop an optical waveguide that exhibits minimal loss, low index contrast, and high nonlinearity in the substrate material. A ring-resonator architecture is then created through electron-beam lithography followed by reactive ion etching. Subsequently, Ti/Au electrodes are applied to both sides of the device to enable electrical tuning. We demonstrate consistent wavelength tuning exceeding 30 nm around 1555 nm using a mere 1 V reverse bias across the device. This research marks a significant advancement toward the creation of electrically-tuned integrated photonic devices that can be manufactured in a monolithic manner on insulating substrates. Lithium niobate has garnered significant attention as an ideal candidate for optoelectronic applications, owing to its impressive qualities, including a broad transparency range, high second-order susceptibility, and relatively low propagation losses. Additionally, it showcases strong piezoelectric and pyroelectric properties that facilitate effective electro-optic modulation. In this letter, we outline our latest developments in electro-optically modified microring resonators fabricated from lithium niobate. These systems were constructed using commercially available lithium niobate wafers bonded to silicon dioxide, with the bottom cladding layer removed prior to processing. Initially, a proton-exchange process was carried out to create a single-mode ridge-waveguide structure within the bulk LiNbO3 crystal. The ring-resonator configuration was then defined in the PE-treated area using electron beam lithography. Finally, titanium/gold (Ti/Au) contacts were deposited onto both surfaces of the sample to provide electrical access to the device. Figure 1 displays scanning electron microscope images of two types of microring resonators that have been successfully fabricated. Both devices comprise",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 8.172062695283987,
        "rewrite-fast-z-score": 1.76851903423969
    },
    {
        "original_text": "We present an approach to calculate the effective material properties of thin layered structures, which are composed by two or more different materials with periodic microstructure. The method is based on homogenization theory combined with finite element analysis (FEA) in order to account for local interactions between neighboring unit cells. We consider three types of unit cell geometries that can be used to model various composite materials such as: wire grid composites, fiber reinforced polymeric matrix composites, and metal foams. In particular we focus our attention on wire grid composites made up of periodically arranged parallel wires embedded into a homogeneous medium. For this type of structure it has been shown experimentally that the macroscopic elastic moduli depend strongly on the volume fraction occupied by the wires. However, these experimental results cannot be explained using classical homogenization theories because they do not take into account the interaction effects among adjacent wires. Therefore, we propose here a new theoretical framework to study the mechanical behavior of wire grid composites at both micro-and meso-scales.",
        "watermark_text": "We present an methods to estimate the effective material properties of thin layered systems , which are composed by two or more separate materials with periodic microstructure . The method is based on homogenization theory combined with finite element assessment ( FEA ) in order to account for local interactions between neighboring unit cells .We consider three categories of unit cell geometries that can be used to model numerous composite materials such as : wire grid composites , fiber strengthened polymeric matrix composites , and steel foams . In particular we focus our focus on wire grid composites made up of regularly arranged connected links embedded into a homogeneous medium .For this form of geometry it has been shown experimentally that the macroscopic elastic moduli rely highly on the density amount filled by the wires . However , these theoretical results cannot be described using traditional homogenization techniques because they do not take into consideration the interaction effects among adjoining strands .Therefore , we propose here a new theoretical framework to study the structural response of wire network composites at both micro - and meso - scales .",
        "rewrite_text": "We introduce a method for estimating the effective material properties of thin layered systems composed of two or more distinct materials with periodic microstructures. This approach integrates homogenization theory with finite element analysis (FEA) to address local interactions between adjacent unit cells. We categorize the geometries of the unit cells into three types that can represent various composite materials, including wire grid composites, fiber-reinforced polymer matrix composites, and steel foams. Our primary focus is on wire grid composites consisting of regularly arranged connected links embedded in a homogeneous medium. Experimental evidence has shown that the macroscopic elastic moduli of this geometry are significantly influenced by the density of the wire-fill. However, traditional homogenization methods fall short in capturing these theoretical outcomes, as they do not consider the interaction effects between neighboring strands. Consequently, we propose a novel theoretical framework to analyze the structural response of wire network composites at both micro and meso scales.",
        "ori-fast-z-score": -0.42640143271122083,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 0.44172610429938614
    },
    {
        "original_text": "We study the controllability properties of quantum subsystems that are described by a master equation with Lindblad form. We show how to construct invariant sets for such systems in terms of their generators. These results allow us to prove that certain classes of open quantum systems cannot be controlled using only unitary operations on the system s Hilbert space. Finally we present an algorithm which allows one to determine whether or not a given set is invariant under the dynamics generated by a particular generator. This work was supported by NSF grant PHY-0456628. Quantum mechanics has been successfully applied to many physical phenomena ranging from atomic physics to condensed matter theory. However, it remains unclear what exactly constitutes a quantum mechanical description of reality. One approach towards answering this question involves studying the behavior of closed quantum systems whose states evolve according to Schrödinger equations. Another approach focuses on describing the evolution of open quantum systems where the state of the system interacts continuously with its environment. The latter class of problems can often be formulated as questions about the controllability of quantum dynamical systems. For example, consider the problem of steering the state of a two-level atom between different energy levels using laser pulses.",
        "watermark_text": "We research the controllability properties of quantum subsystems that are explained by a master equation with Lindblad form . We see how to build invariant pairs for such systems in terms of their generators .These results allow us to prove that particular categories of open quantum systems cannot be governed using only unitary operations on the scheme s Hilbert space . Finally we present an algorithm which allows one to find whether or not a given set is invariant under the dynamics generated by a certain generator .This research was supported by NSF grant PHY - 0456628 . Quantum theory has been successfully application to many natural objects including from atomic physics to condensed matter theory .However , it remains unsure what actually constitutes a quantum mechanical explanation of reality . One approach towards answering this question involves studying the dynamics of closed quantum systems whose states evolve according to Schrödinger equations .Another approach focuses on explaining the evolution of open quantum systems where the state of the state interacts continuously with its surroundings . The latter group of problems can often be understood as challenges about the controllability of quantum dynamical systems .For instance , consider the question of steering the state of a two - level atom between various energy levels use laser pulses .",
        "rewrite_text": "We investigate the controllability properties of quantum subsystems described by a master equation in Lindblad form. Our study demonstrates how to construct invariant pairs for these systems based on their generators. These findings enable us to show that certain categories of open quantum systems cannot be controlled solely through unitary operations on the corresponding Hilbert space. Additionally, we introduce an algorithm that determines whether a given set remains invariant under the dynamics generated by a specific operator. This research was supported by NSF grant PHY-0456628. Quantum theory has found successful applications across various natural phenomena, spanning from atomic physics to condensed matter theory. Nonetheless, it remains unclear what exactly constitutes a quantum mechanical explanation of reality. One approach to addressing this question involves examining the dynamics of closed quantum systems, where states evolve according to the Schrödinger equation. Another avenue focuses on understanding the evolution of open quantum systems, which continuously interact with their environment. The latter often presents challenges related to the controllability of quantum dynamical systems. For example, one might explore how to manipulate the state of a two-level atom between different energy levels using laser pulses.",
        "ori-fast-z-score": 1.1547005383792515,
        "water-fast-z-score": 6.543303050815759,
        "rewrite-fast-z-score": 1.7320508075688772
    },
    {
        "original_text": "We present the first panoramic view of the distribution of galaxies around two massive clusters at redshifts z = 0.4-0.6, using deep near-infrared imaging with WFC3 on HST. The data reveal an extended population of faint galaxies surrounding each cluster that is not seen in optical images. We find that these galaxies are predominantly blue (with median color u − g = -0.5), have low stellar masses (10^9 Msun/pc^2) and high specific star formation rates (sSFR ~ 10^-2 Gyr-1). These properties suggest they represent a recently assembled population of star-forming dwarf galaxies that were accreted by their host clusters during recent mergers. In addition to this diffuse component we also identify several dozen bright galaxies within 1 Mpc of both clusters which appear to be undergoing rapid bursts of star formation triggered by interactions between infalling gas-rich galaxies and the hot intracluster medium.",
        "watermark_text": "We present the first panoramic study of the distribution of clusters around two huge clusters at redshifts z = 0 . 4 - 0 . 6 , using deep near - infrared imaging with WFC3 on HST . The data reveal an extended population of faint clusters surrounding each cluster that is not seen in infrared images .We see that these objects are exclusively blue ( with median color u − g = - 0 . 5 ) , have poor stellar masses ( 10 ^ 9 Msun / pc ^ 2 ) and large particular galaxy formation rates ( sSFR ~ 10 ^ - 2 Gyr - 1 ) . These properties suggest they represent a recently assembled colony of star - creating dwarf stars that were accreted by their host clusters during earlier mergers .In addition to this diffuse component we also identify several several bright stars within 1 Mpc of both clusters which appear to be experiencing rapid bursts of galaxy formation followed by interactions between infalling gas - rich galaxies and the hotter intracluster medium .",
        "rewrite_text": "We offer the first comprehensive analysis of the distribution of clusters surrounding two massive clusters at redshifts z = 0.4 to 0.6, utilizing deep near-infrared imaging with the WFC3 on the Hubble Space Telescope. The results reveal a widespread population of faint clusters surrounding each main cluster, which are not visible in traditional infrared images. These objects are predominantly blue, with a median color of u − g = -0.5, possess low stellar masses (approximately 10^9 Msun/pc^2), and exhibit high specific star formation rates (sSFR ~ 10^-2 Gyr^-1). These characteristics indicate that they likely represent a recently formed group of star-forming dwarf galaxies that have been accreted by their host clusters during previous mergers. Additionally, we identify several bright galaxies within 1 Mpc of both clusters that appear to be undergoing rapid bursts of star formation, likely due to interactions between infalling gas-rich galaxies and the hotter intracluster medium.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "We present new photometric and spectroscopic observations of the classical nova EX Hya made in October 2005, when it was still bright (V = 8 mag). The light curve shows that EX Hya is an intermediate polar with a period of P orb = 0.084 d. We find evidence for two accretion regions on the white dwarf surface which are responsible for the double-peaked emission lines observed during outburst. In addition to these features we also detect narrow absorption components at velocities up to -1500 km s-1 . These absorptions may be caused by material ejected during previous eruptions. Our results show that EX Hya has returned to quiescence after its latest eruption in September 2002. Classical novae have been known since antiquity but their underlying physics remains poorly understood. They are believed to result from thermonuclear runaways triggered by unstable nuclear burning on the surfaces of white dwarfs (WD) in close binary systems. However, there remain many open questions about how this process takes place and what happens afterwards. One such question concerns the nature of the WD magnetic field. It is generally accepted that the WD magnetic field plays a key role in determining whether or not a system will undergo a thermonuclear runaway. If the WD magnetic field is too weak then no runaway occurs; if it is strong enough then the WD can become fully convective leading to stable hydrogen burning and hence preventing any further outburst activity. This picture is complicated however by the fact that some WDs do exhibit periodic outbursts despite having fields thought to be too weak to prevent them becoming fully convective. Such objects are called Intermediate Polars (IPs), where the WD magnetic field is sufficiently strong to synchronise the spin periods of both stars but does not disrupt the flow of matter onto the WD.",
        "watermark_text": "We present new photometric and spectroscopic observations of the classical nova EX Hya made in October 2005 , when it was still bright ( V = 8 mag ) . The light curve shows that EX Hya is an intermediate polar with a period of P orb = 0 . 084 d . We see evidence for two accretion zones on the white dwarf surface which are responsible for the double - peaked emission lines observed during outburst .In addition to these characteristics we also observe narrow absorbed components at velocities up to - 1500 km s - 1 . These absorptions might be caused by material expelled during earlier eruptions .Our results show that EX Hya has returned to quiescence after its latest eruption in September 2002 . Classical novae have been known since ancient but their underlying dynamics appears poorly explored .They are said to arise from thermonuclear runaways caused by volatile nuclear burning on the surfaces of white dwarfs ( WD ) in close binary systems . However , there remain many open questions about how this process comes place and what comes afterwards .One such issue concerns the nature of the WD magnetic force . It is usually agreed that the WD magnetic force plays a key importance in determining whether or not a system will undergo a thermonuclear runaway .If the WD magnetic force is too weak then no runaway occurs ; if it is strong enough then the WD can turn fully convective causing to good hydrogen burning and hence stopping any further outburst activity . This picture is complicated however by the fact that some WDs do exhibit periodic outbursts despite having fields thought to be too weak to prohibit them becoming fully convective .Such elements are called Intermediate Polars ( IPs ) , where the WD magnetic force is sufficiently strong to synchronise the spin times of both stars but does not disrupt the transfer of matter onto the WD .",
        "rewrite_text": "We provide new photometric and spectroscopic observations of the classical nova EX Hya, which were conducted in October 2005 while the nova was still shining bright at a magnitude of V = 8. The light curve indicates that EX Hya is classified as an intermediate polar with an orbital period of P_orb = 0.084 days. Our observations reveal two accretion zones on the surface of the white dwarf, which are responsible for the double-peaked emission lines seen during the outburst. Additionally, we detect narrow absorbed components at velocities reaching -1500 km/s, likely caused by material expelled during previous eruptions. Our findings confirm that EX Hya has returned to a quiescent state after its latest eruption in September 2002. Although classical novae have been recognized since ancient times, their underlying dynamics remain inadequately studied. These events are generally thought to originate from thermonuclear runaways driven by volatile nuclear burning on the surfaces of white dwarfs (WDs) in close binary systems. However, significant questions persist regarding how these processes occur and what follows them. One such question pertains to the influence of the WD's magnetic field. It is widely accepted that the strength of the WD's magnetic field plays a crucial role in determining whether a thermonuclear runaway will take place. A weak magnetic field prevents such runaway events, while a sufficiently strong field can lead to full convective conditions in the WD, facilitating effective hydrogen burning and thereby halting further outbursts. Nonetheless, this scenario is complicated by the existence of some WDs that experience periodic outbursts, even though their magnetic fields are believed to be too weak to prevent them from achieving full convection. These cases are referred to as Intermediate Polars (IPs), where the magnetic field of the WD is strong enough to synchronize the spin periods of both stars without interrupting the transfer of matter onto the WD.",
        "ori-fast-z-score": -1.247219128924647,
        "water-fast-z-score": 6.057921483348286,
        "rewrite-fast-z-score": -0.26013299085723596
    },
    {
        "original_text": "We present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae. \n \n Keywords: Supernova remnants",
        "watermark_text": "We create 8 . 4 GHz Very Long Baseline Interferometry ( VLBI ) images and light angles for the supernova remnant ( SNR ) associated with the Type IIb supernova SN2004et , which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1 . The radio emission is dominated by two bright components differentiated by ~ 0 . 5 arcsec at all epochs observed between 2005 January and 2007 December .We see that both components are growing with velocities of ~ 5000 kilometres / s , consistent with previous estimates based on single - dish data . However , we also observe significant normal motions of ~ 1000 kilometers / s for each component over this time .These data suggest an age of about 3 years for the SNR , suggests a proximity to NGC 6946 of 4 Mpc . This value is significantly less than previously estimated distances to this body using other methods .Our measurements give novel constraints on estimates of core - collapse supernovae . Keywords : Supernova remnants",
        "rewrite_text": "We have produced 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) linked to the Type IIb supernova SN2004et, which occurred in the nearby spiral galaxy NGC 6946 on September 24, 2004 (UT1). The radio emissions are primarily characterized by two bright components that are separated by approximately 0.5 arcseconds across all observations conducted between January 2005 and December 2007. Our findings indicate that both components are expanding at velocities of around 5000 kilometers per second, aligning with previous estimates derived from single-dish observations. Additionally, we note significant intrinsic motions of about 1000 kilometers per second for each component over the observed period. These data imply that the SNR is roughly 3 years old and suggest a distance of 4 Mpc from NGC 6946, which is notably lower than previous distance estimates obtained through other methods. Our measurements provide new constraints for assessing core-collapse supernovae. Keywords: Supernova remnants.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model  1  describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing  2  , quantum optics  3  , condensed matter physics  4  , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N /2 (N being the number of atoms)  5  .\nIn fact, the Dicke model was originally proposed more than half century ago  6  . Since then various theoretical approaches have been developed to solve it  7 -10  . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation  11  which maps the original problem into a spin-1/2 system  12  . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states  13  . Recently, several authors  14 -19  have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "watermark_text": "We explore the generalized Dicke model with an arbitrary number N of two - level atoms interacting with one - mode radiation field , and find that it can be mapped to a spin - 1 / 2 system by using the Holstein - Primakoff transformation . We then use the exact diagonalization technique to estimate its ground state energy spectrum for different values of the interaction factor g and the number N .The results are compared with those achieved by other methods such as perturbation theory and numerical integration . It is found that our findings agree well with previous ones when the interaction strength is tiny but deviate drastically from them if the interaction becomes strong .Finally we explain some possible use of this study . PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Dc , 13 . 25 . Gv I . INTRODUCTIO N The Dicke model 1 explains how many identical two - level atoms behave collectively with a single mode of electromagnetic field .In recent years there has been continued interest in investigating this model because of its potential application in quantum information processing 2 , quantum optics 3 , condensed matter science 4 , etc . . For instance , the collective spontaneous emission speed of the atomic ensemble depends on the total angular velocity J = N / 2 ( N being the number of atoms ) 5 . In reality , the Dicke concept was originally proposed more than quarter century ago 6 .Since then various theoretical methods have been constructed to solve it 7 - 10 . Among these method , the most popular one is probably the so - called HolsteinPrimakoff transformation 11 which maps the original problem into a spin - 1 / 2 system 12 .This method works very best at weak - interaction regime where the interaction between electron - field is fairly little . However , it fails totally at large - interaction range since the mapping method splits down due to the appearance of unphysical states 13 .Recently , various literature 14 - 19 have tried to overcome this challenge by using new transformations or approximations . Nevertheless , their solutions still suffer from certain drawbacks 20 , 21 .",
        "rewrite_text": "We investigate the generalized Dicke model, which involves an arbitrary number \\( N \\) of two-level atoms interacting with a single-mode radiation field. Through the Holstein-Primakoff transformation, we demonstrate that this model can be mapped to a spin-\\( \\frac{1}{2} \\) system. We then apply exact diagonalization techniques to estimate the ground state energy spectrum for various values of the interaction factor \\( g \\) and the number of atoms \\( N \\). Our results are compared with those obtained using other methods, including perturbation theory and numerical integration. We find a strong agreement with previous findings when the interaction strength is weak, but a significant divergence arises as interaction strength increases. Additionally, we discuss potential applications of this research. \n\nThe Dicke model describes the collective behavior of many identical two-level atoms interacting with a single mode of electromagnetic radiation. Recent interest in this model has surged due to its implications for quantum information processing, quantum optics, and condensed matter physics. For example, the collective spontaneous emission rate of the atomic ensemble is influenced by the total angular momentum \\( J = \\frac{N}{2} \\) (where \\( N \\) represents the number of atoms). Although the Dicke model was originally introduced over 25 years ago, it has inspired numerous theoretical approaches aimed at finding solutions. Among these, the Holstein-Primakoff transformation is particularly notable for mapping the problem onto a spin-\\( \\frac{1}{2} \\) framework, proving effective in the weak interaction regime. However, this method encounters significant limitations in the strong interaction regime due to the emergence of unphysical states. Recent literature has sought to address these challenges with new transformations or approximations, though these solutions still exhibit certain limitations.",
        "ori-fast-z-score": -0.16222142113076254,
        "water-fast-z-score": 6.8132996874920275,
        "rewrite-fast-z-score": -0.827605888602368
    },
    {
        "original_text": "We present an overview of our recent work on Bayesian methods for reconstructing the large scale structures in the universe using galaxy redshift surveys. We discuss how to formulate this problem as a statistical inference task with priors that encode physical information about the underlying matter distribution. The posterior probability density function is then evaluated by applying Bayes  theorem together with Markov Chain Monte Carlo (MCMC) sampling techniques. In particular we focus on two different approaches which are based either on Gibbs sampling or Metropolis-Hastings algorithm. Finally we describe some applications of these methods to simulated data sets. This research was supported by NSF grant AST-0707763. Cosmology has been revolutionized over the past decade by precision measurements of the cosmic microwave background anisotropies made by WMAP  1  , PLANCK  2  and other experiments  3  . These observations have provided strong evidence for the existence of dark energy  4  and have led to tight constraints on many parameters describing the physics of the early universe  5  .\nHowever, despite their successes there remain several open questions regarding fundamental aspects of the standard model of cosmology  6  . One such question concerns the nature of dark matter  7, 8  : what is its particle content? What is its mass? How does it interact with ordinary matter?\nAnswering these questions requires detailed knowledge of the spatial distribution of dark matter throughout space and time  9  . Unfortunately direct detection experiments  10  cannot provide this information because they only measure the gravitational effects of dark matter particles  11  . Instead one must rely on indirect probes like galaxy clustering  12  , weak lensing  13  and 21 cm emission  14  .",
        "watermark_text": "We present an overview of our latest work on Bayesian methods for reconstructing the huge scale structures in the universe using galaxy redshift surveys . We discuss how to formulate this question as a statistical inference job with priors that encode physical information about the underlying matter distribution .The posterior likelihood density function is then evaluated by using Bayes relation together with Markov Chain Monte Carlo ( MCMC ) filtering approaches . In particular we focus on two different methods which are based either on Gibbs filtering or Metropolis - Hastings algorithm .Finally we explain some applications of these systems to modeled information sets . This research was supported by NSF grant AST - 0707763 .Cosmology has been revolutionized over the previous decade by precision observations of the cosmic microwave background anisotropies made by WMAP 1 , PLANCK 2 and other experiments 3 . These measurements have provided strong evidence for the existence of deep energy 4 and have led to strict constraints on numerous variables describing the physics of the early world 5 .However , despite their successes there remain many open questions regarding essential aspects of the standard theory of cosmology 6 . One such problem concerns the nature of deep material 7 , 8 : what is its particle content ?What is its weight ? How does it behave with normal matter ?Answering these problems demands clear information of the spatial distribution of dark matter throughout space and time 9 . Unfortunately direct detection experiments 10 cannot offer this data because they only measure the gravitational impacts of dark matter grains 11 .Instead one must rely on indirect probes like galaxy clustering 12 , soft lensing 13 and 21 cm emission 14 .",
        "rewrite_text": "We provide an overview of our latest research on Bayesian methods for reconstructing large-scale structures in the universe through galaxy redshift surveys. We outline how to approach this issue as a statistical inference challenge, utilizing priors that encapsulate physical insights into the underlying matter distribution. The posterior likelihood density function is computed using Bayes' theorem, in conjunction with Markov Chain Monte Carlo (MCMC) filtering techniques. Specifically, we examine two distinct methods based on Gibbs filtering and the Metropolis-Hastings algorithm. Additionally, we discuss applications of these approaches to modeled datasets. This research was funded by NSF grant AST-0707763. \n\nThe field of cosmology has experienced significant advancements over the past decade, driven by precise observations of cosmic microwave background anisotropies conducted by WMAP, PLANCK, and other experiments. These observations have provided compelling evidence for the existence of dark energy and have resulted in stringent constraints on a variety of parameters regarding the physics of the early universe. However, despite these achievements, many critical questions about key aspects of the standard cosmological theory remain unanswered. One pressing issue is the nature of dark matter: What are its particle constituents? What is its mass? How does it interact with ordinary matter? Addressing these questions requires detailed knowledge of the spatial distribution of dark matter across both space and time. Unfortunately, direct detection experiments can only measure the gravitational effects of dark matter particles and do not provide the necessary spatial information. Thus, researchers must turn to indirect methods, such as galaxy clustering, weak gravitational lensing, and 21 cm emission.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 8.131727983645296,
        "rewrite-fast-z-score": 0.7071067811865475
    },
    {
        "original_text": "We present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "We report new studies and investigation of the neutral hydrogen ( HI ) disks comprising isolated stars , using data acquired with the Very Large Array ( VLA ) . We have noted 12 nearby galaxies at 21 cm wavelength to obtain their total HI mass and distribution within the optical disk .The sample comprises both late - class spirals and dwarf irregulars . Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich complexes such as NGC 4254 and NGC 5253 .In addition , we find proof for significant amounts of atomic liquid outside our detection limits which may be correlated with tidal features or other relationships between these objects and surrounding companions . These conclusions are compatible with previous research indicating that several isolated spiral clusters hold significant amounts of cold interstellar medium spread over several kiloparsecs .Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "rewrite_text": "We present new research on the neutral hydrogen (HI) disks of isolated stars, utilizing data obtained from the Very Large Array (VLA). Our study focuses on 12 nearby galaxies observed at a 21 cm wavelength, allowing us to assess their total HI mass and distribution within the optically defined disks. The sample includes both late-type spirals and dwarf irregular galaxies. Our findings reveal that, with the exception of one star, all have extended HI halos that exceed their optical diameters; this holds true even for the most gas-rich systems, such as NGC 4254 and NGC 5253. Furthermore, we provide evidence for substantial quantities of atomic hydrogen beyond our detection thresholds, which may be linked to tidal features or interactions with nearby companions. These results are consistent with previous studies suggesting that many isolated spiral clusters possess significant amounts of cold interstellar medium distributed over several kiloparsecs. \nKeywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": -0.808290376865476
    },
    {
        "original_text": "We present photometry for stars in an intermediate-age open cluster, BS90 (Sharpless 1959), located at the center of the star-forming region NGC346 in the Small Magellan Cloud (SMC). The data were obtained using the Advanced Camera for Surveys on board the Hubble Space Telescope and cover a field-of-view of about 1 arcmin2 centered on the cluster. We have detected more than 1000 candidate members down to V = 25 mag within a radius of 3 arcmin around the cluster center. Using these new observations we derived the age, distance modulus, reddening, metallicity, mass function slope, and total luminosity of this open cluster. Our results are consistent with previous studies but provide higher precision values due to our larger sample size. \n \n Keywords: Open clusters; SMC; HST/ACS; photometry; star formation history; intermediate-age",
        "watermark_text": "We present photometry for stars in an intermediate - age open cluster , BS90 ( Sharpless 1959 ) , located at the center of the star - making area NGC346 in the Small Magellan Cloud ( SMC ) . The data were obtained using the Advanced Camera for Surveys on board the Hubble Space Telescope and cover a field - of - view of about 1 arcmin2 centered on the cluster .We have discovered more than 1000 candidate members down to V = 25 mag within a diameter of 3 arcmin around the cluster center . Using these new images we derived the age , distance modulus , reddening , metallicity , mass function slope , and overall luminosity of this open cluster .Our results are compatible with previous findings but give larger precision values due to our larger specimen size . Keywords : Open clusters ; SMC ; HST / ACS ; photometry ; star formation history ; intermediate - age",
        "rewrite_text": "We present photometric data for stars in the intermediate-age open cluster BS90 (Sharpless 1959), situated at the heart of the star-forming region NGC346 in the Small Magellanic Cloud (SMC). This data, collected with the Advanced Camera for Surveys aboard the Hubble Space Telescope, encompasses a field of view of approximately 1 square arcminute centered on the cluster. Our study identified over 1,000 candidate members with magnitudes down to V = 25 within a 3 arcminute radius of the cluster's center. Utilizing these new images, we determined the cluster's age, distance modulus, reddening, metallicity, mass function slope, and overall luminosity. Our findings align with previous research but offer more precise values due to the larger sample size. Keywords: Open clusters; SMC; HST/ACS; photometry; star formation history; intermediate-age.",
        "ori-fast-z-score": -0.13483997249264842,
        "water-fast-z-score": 2.449489742783178,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of our analysis on the supersymmetric parameter space, taking into account all available experimental data including those from LHC experiments as well as electroweak precision observables (EWPO). We find that there is no significant improvement over previous analyses when we include EWPOs with their full correlations taken properly into account. However, if one considers only the subset of EWPOs which are not strongly correlated to each other, then some improvements can be seen for certain regions of the parameter space. In particular, this applies to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV. The latter case also leads to an improved agreement between theory predictions and measurements of the anomalous magnetic moment of the muon. Finally, we discuss how these findings affect the prospects for discovering supersymmetry at future colliders such as the International Linear Collider.",
        "watermark_text": "We report the results of our analysis on the supersymmetric parameter room , using into consideration all available observation information including those from LHC experiments as well as electroweak accuracy observables ( EWPO ) . We see that there is no considerable progress over past analyses when we incorporate EWPOs with their full correlations took correctly into consideration .However , if one looks only the subset of EWPOs which are not closely correlated to each other , then some improvements can be shown for particular regions of the parameter space . In particular , this applicable to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV .The last example also leads to an better agreement between theoretical estimates and measurements of the anomalous magnetic motion of the muon . Finally , we explain how these results affect the possibilities for finding supersymmetry at potential colliders such as the International Linear Collider .",
        "rewrite_text": "We present the findings from our analysis of the supersymmetric parameter space, taking into account all available observational data, including results from LHC experiments as well as electroweak precision observables (EWPO). Our analysis indicates that there has been no significant advancement compared to previous studies when incorporating EWPOs with their full correlations appropriately considered. However, if we focus solely on the subset of EWPOs that are not closely correlated, we can observe some improvements in specific regions of the parameter space. This is notably true for scenarios where the lightest neutralino has a substantial Higgsino component or where gluinos have masses around 1 TeV. In the latter case, this also accounts for better alignment between theoretical predictions and the measured anomalous magnetic moment of the muon. Lastly, we discuss how these results influence the prospects for discovering supersymmetry at future colliders, such as the International Linear Collider.",
        "ori-fast-z-score": -3.6765801200722312,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": -0.9113223768657671
    },
    {
        "original_text": "El año que cambió la historia del Universo (2005) es un documental sobre el descubrimiento por parte de Albert Einstein, en 1915, de las ecuaciones básicas de su teoría general relativista y los acontecimientos que tuvieron lugar durante ese mismo año. El film se centra especialmente en dos hechos extraordinarios ocurridos en 1905: la publicación de cinco artículos científicos revolucionarios escritos por Einstein entre marzo y diciembre; y el nacimiento de su hija mayor, Margot, en mayo. La película está narrada por el actor británico David Threlfall e incluye entrevistas con expertos mundiales como Sean Carroll o Brian Cox. ... ​ \n\n\nLa teoría general de la relatividad fue publicada por primera vez en 1915 pero sus fundamentos fueron establecidos ya en 1905 cuando Einstein publicó una serie de trabajos revolucionarios que cambiaron nuestra comprensión del Universo. Este documental cuenta cómo ocurrió todo eso centrándose en dos hechos extraordinarios sucedidos aquel año: la publicación de cinco trabajos científicos revolucionarios escritos por él entre marzo y diciembre; así como el nacimiento de su hija Margot en mayo.",
        "watermark_text": "El año que cambió la historia del Universo ( 2005 ) es un documental sobre al descubrimiento por parte de Albert Einstein , en 1915 , de las ecuaciones básicas de su teoría general relativista y los acontecimientos para tuvieron lugar durante ese mismo año . El movie se centra especialmente en dos hechos extraordinarios ocurridos en 1905 : la publicación de cinco artículos científicos revolucionarios escritos por Einstein entre marzo y diciembre ; y el nacimiento de su hija mayor , Margot , es mayo .La película está narrada por el acting británico David Threlfall e incluye entrevistas en expertos mundiales como Sean Carroll o Brian Cox . . . . La teoría general de la relatividad fue publicada por primera vez en 1915 pero sus fundamentos fueron establecidos yo en 1905 cuando Einstein publicó una serie de trabajos revolucionarios que cambiaron nuestra comprensión del Universo .Este documental cuenta cómo ocurrió todo eso centrándose en dos hechos extraordinarios sucedidos aquel año : la publicación de cinco trabajos científicos revolucionarios escritos por él entre marzo y diciembre ; así como el nacimiento de su hija Margot en mayo .",
        "rewrite_text": "\"El año que cambió la historia del Universo\" (2005) es un documental que aborda el descubrimiento de las ecuaciones fundamentales de la teoría general de la relatividad por Albert Einstein en 1915, así como los eventos que ocurrieron durante ese mismo año. La película pone un énfasis particular en dos hechos extraordinarios de 1905: la publicación de cinco artículos científicos revolucionarios por Einstein entre marzo y diciembre, y el nacimiento de su hija mayor, Margot, en mayo. Narrado por el actor británico David Threlfall, el documental incluye entrevistas con destacados expertos internacionales como Sean Carroll y Brian Cox. Aunque la teoría general de la relatividad fue publicada por primera vez en 1915, sus bases se cimentaron en 1905, cuando Einstein presentó una serie de investigaciones que transformaron nuestra comprensión del universo. Este documental relata cómo se desarrollaron estos acontecimientos, enfocándose en los dos hechos notables de aquel año.",
        "ori-fast-z-score": 0.3922322702763681,
        "water-fast-z-score": 3.1378581622109447,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "We report on Swift observations of the fifth outburst of an unusual transient, which we call IGR J112155952 (IGR 11215). The source was discovered by INTEGRAL in 2003 and has been observed to undergo regular outbursts every ~150 days since then. We find that this is also true for its most recent outburst; it reached peak luminosity Lx = 1.1 x 1036 erg s-1 at T0 = MJD 54656.6 ± 0.2 (2010 September 16) with a duration of about 20 days. During each outburst, the spectrum softens as the flux decreases. This behaviour suggests that the accretion rate onto the compact object decreases during the decay phase of each outburst. In addition, there are hints of spectral changes between different outbursts; these may be related to variations in the mass transfer rate or the geometry of the system. \n \n Keywords: Supernova remnant",
        "watermark_text": "We report on Swift discoveries of the fifth outburst of an strange transient , which we call IGR J112155952 ( IGR 11215 ) . The source was established by INTEGRAL in 2003 and has been observed to undergo normal outbursts every ~ 150 days since then .We see that this is also true for its most subsequent outburst ; it achieved peak luminosity Lx = 1 . 1 x 1036 erg s - 1 at T0 = MJD 54656 . 6 ± 0 . 2 ( 2010 September 16 ) with a duration of about 20 days . During each outburst , the spectrum softens as the flux drops .This behaviour suggests that the accretion rate onto the compact body drops during the decay phase of each outburst . In addition , there are hints of absorption changes between various outbursts ; these may be connected to variations in the mass transfer frequency or the topology of the system .Keywords: Supernova remnant",
        "rewrite_text": "We present findings from Swift regarding the fifth outburst of an unusual transient source, which we refer to as IGR J112155952 (IGR 11215). This source was first identified by INTEGRAL in 2003 and has shown a pattern of normal outbursts approximately every 150 days. Our observations confirm this trend for its latest outburst, which reached a peak luminosity of Lx = 1.1 x 10^36 erg s^-1 at T0 = MJD 54656.6 ± 0.2 (September 16, 2010) and lasted for about 20 days. Each outburst is characterized by a softening spectrum as the flux decreases, indicating a reduction in the accretion rate onto the compact object during the decline phase. Additionally, there are indications of changes in absorption between different outbursts, potentially linked to variations in the mass transfer rate or the system's structure.  \nKeywords: Supernova remnant",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 4.621207153499565,
        "rewrite-fast-z-score": -0.4375949744936837
    },
    {
        "original_text": "We have studied in detail how to correct for various observational effects on the determination of the white dwarf luminosity function (WDLF). We find that the WDLF is affected by several factors, such as photometric calibration error, incompleteness due to detection limit, contamination by unresolved binaries, etc.. In order to obtain an unbiased estimate of the true WDLF we need to take into account these effects properly.  By using Monte Carlo simulations with artificial data sets, we show that our method can recover the input WDLF very well even when there are large uncertainties in the observed magnitudes or colors. Our results also suggest that it may be difficult to determine the absolute normalization of the WDLF accurately because of systematic uncertainty associated with the distance scale. Finally, we apply this method to the recent observations made by the Sloan Digital Sky Survey (SDSS) Data Release 1 (DR1), which covers about one quarter of the sky.",
        "watermark_text": "We have researched in detail how to correct for various observational consequences on the determination of the white dwarf luminosity function ( WDLF ) . We see that the WDLF is affected by many processes , such as photometric calibration error , incompleteness attributed to observation limit , contamination by unresolved binaries , etc . .In order to obtain an unbiased estimate of the true WDLF we must to take into consideration these consequences properly . By using Monte Carlo simulations with artificial data sets , we prove that our technique can regain the input WDLF very best even when there are big uncertainties in the seen magnitudes or colors .Our results also suggest that it could be harder to predict the absolute normalization of the WDLF accurately because of systematic uncertainty associated with the distance scale . Finally , we apply this method to the recent observations made by the Sloan Digital Sky Survey ( SDSS ) Data Release 1 ( DR1 ) , which covers about one quarter of the heavens .",
        "rewrite_text": "We conducted an in-depth investigation into how to address various observational impacts on the determination of the white dwarf luminosity function (WDLF). Our findings indicate that the WDLF is influenced by numerous factors, including errors in photometric calibration, observational limits that lead to incompleteness, and contamination from unresolved binary stars, among others. To derive an unbiased estimate of the true WDLF, it is essential to account for these influences correctly. Through the use of Monte Carlo simulations with synthetic data sets, we demonstrate that our methodology can accurately recover the input WDLF, even in the presence of significant uncertainties in the observed magnitudes or colors. Our results also indicate that accurately predicting the absolute normalization of the WDLF may be challenging due to the systematic uncertainties linked to the distance scale. Finally, we apply this approach to recent observations from the Sloan Digital Sky Survey (SDSS) Data Release 1 (DR1), which encompasses about one-quarter of the sky.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 6.273807116711903,
        "rewrite-fast-z-score": -0.5
    },
    {
        "original_text": "We propose an erasure distribution for low-density parity-check (LDPC) codes that has closed-form threshold expression and is optimal in the sense that it minimizes the gap between its threshold value and Shannon limit on the binary symmetric channel (BSC). The proposed distribution can be viewed as a generalization of the optimized irregular repeat accumulate (IRA) code ensemble, which was recently introduced by Tanner et al.. We show that our new distribution achieves better performance than IRA over BSCs with small crossover probabilities. Finally, we present simulation results to demonstrate the effectiveness of the proposed distribution under practical conditions. Index Terms-Low density parity check (LDPC), Binary Symmetric Channel (BSC), Optimized Irregular Repeat Accumulate Code Ensemble (OIRA), Gap-to-Shannon Limit (GTSL)\nI. INTRODUCTIO N Low-Density Parity Check (LDPC) codes are linear block codes defined by sparse parity-check matrices  1  . They have been shown to perform close to capacity when decoded using iterative message-passing algorithms such as belief propagation  2  , and they are widely used in many applications including digital communications  3  -  5  .\nThe design of good LDPC ensembles remains one of the most important problems in coding theory  6  . In particular, there exists a large body of research devoted to finding distributions that minimize the gap between their threshold values and Shannon limits  7  -  11  . However, these works mainly focus on regular or quasi-cyclic LDPC codes  12  , while irregular LDPC codes are more commonly used due to their flexibility  13  . Recently, Tanner et al.  14  presented an optimized irregular repeat accumulate (OIRA) code ensemble whose threshold value matches the Shannon limit on the binary erasure channel (BEC) . This result suggests that OIRA may also achieve near-optimal performance on other channels  15  .",
        "watermark_text": "We suggest an erasure distribution for low - density parity - check ( LDPC ) codes that has closed - form threshold expression and is ideal in the sense that it minimizes the gap between its threshold value and Shannon limit on the binary symmetric channel ( BSC ) . The proposed distribution can be viewed as a generalization of the optimized irregular repeat accumulate ( IRA ) code ensemble , which was recently presented by Tanner et al . . We suggest that our new distribution achieves higher success than IRA over BSCs with little crossover probabilities .Finally , we present modeling results to test the ability of the suggested distribution under effective conditions . Index Terms - Low density parity check ( LDPC ) , Binary Symmetric Channel ( BSC ) , Optimized Irregular Repeat Accumulate Code Ensemble ( OIRA ) , Gap - to - Shannon Limit ( GTSL ) I . INTRODUCTIO N Low - Density Parity Check ( LDPC ) codes are linear block sequences specified by dense parity - check matrices 1 .They have been shown to work close to capacity when decoded using iterative message - passing techniques such as faith propagation 2 , and they are widely useful in different applications notably digital communications 3 - 5 . The design of excellent LDPC ensembles holds one of the most important problems in coding theory 6 .In particular , there exists a large body of research devoted to finding distributions that minimize the gap between their threshold coefficients and Shannon limits 7 - 11 . However , these works mainly emphasis on regular or pseudo - cyclic LDPC rules 12 , while irregular LDPC coding are more frequently used owing to their flexibility 13 .Recently , Tanner et al . 14 published an optimized irregular repeat accumulate ( OIRA ) code ensemble whose limit function matches the Shannon limit on the binary erasure channel ( BEC ) .This result suggests that OIRA may even attain near - optimal performance on other channels 15 .",
        "rewrite_text": "We propose a new erasure distribution for low-density parity-check (LDPC) codes that features a closed-form expression for its threshold and is optimized to minimize the gap between its threshold value and the Shannon limit on the binary symmetric channel (BSC). This distribution can be seen as an extension of the optimized irregular repeat-accumulate (IRA) code ensemble recently introduced by Tanner et al. We contend that our proposed distribution outperforms IRA in scenarios involving BSCs with low crossover probabilities. Additionally, we provide modeling results to evaluate the performance of our distribution under practical conditions. \n\n**Index Terms** - Low-density parity-check (LDPC), Binary Symmetric Channel (BSC), Optimized Irregular Repeat Accumulate Code Ensemble (OIRA), Gap-to-Shannon Limit (GTSL). \n\n**I. INTRODUCTION**  \nLow-density parity-check (LDPC) codes are defined by sparse parity-check matrices and have demonstrated performance close to capacity when decoded using iterative message-passing techniques, such as belief propagation. They are widely applicable in various domains, particularly in digital communications. The design of effective LDPC ensembles remains a major challenge in coding theory. Extensive research has focused on developing distributions that reduce the gap between their threshold coefficients and Shannon limits. However, these efforts often concentrate on regular or pseudo-cyclic LDPC constructions, while irregular LDPC codes are more commonly utilized due to their enhanced flexibility. Recently, Tanner et al. introduced an optimized irregular repeat-accumulate (OIRA) code ensemble, which achieves a limit function aligned with the Shannon limit on the binary erasure channel (BEC), suggesting that OIRA may also provide near-optimal performance on other channels.",
        "ori-fast-z-score": 0.08873565094161139,
        "water-fast-z-score": 6.427291616177054,
        "rewrite-fast-z-score": 1.9090909090909092
    },
    {
        "original_text": "We present an analysis of Lyman alpha emitters (LAEs) selected by narrowband imaging with Subaru/Suprime-Cam and spectroscopic follow-up observations using VLT/VIMOS at z ~ 3.1, which is one of the most active epochs for galaxy formation. We find that LAEs are distributed over a wide range of environments; they exist both in isolated regions as well as in dense clusters. The clustering properties of LAEs depend on their luminosities. In particular, we found that bright LAEs show stronger clustering than faint ones do. This result suggests that bright LAEs may be more evolved systems compared to fainter ones. Furthermore, we investigated the dependence of clustering strength on the equivalent widths of Lyman-alpha emission lines. Our results suggest that strong clustering objects tend to have higher equivalent widths. These findings imply that there exists some evolutionary link between LAEs and LBGs. \n \n Keywords: Lyman alpha emitter",
        "watermark_text": "We present an assessment of Lyman alpha emitters ( LAEs ) selected by narrowband scanning with Subaru / Suprime - Cam and spectroscopic follow - up observations using VLT / VIMOS at z ~ 3 . 1 , which is one of the most stable epochs for galaxy formation . We see that LAEs are distributed over a broad variety of habitats ; they exist both in isolated regions as well as in dense clusters .The clustering qualities of LAEs depend on their luminosities . In particular , we identified that bright LAEs see better clustering than dim ones do .This result suggests that bright LAEs may be more evolved structures compared to fainter ones . Furthermore , we investigated the dependence of clustering strength on the equivalent widths of Lyman - alpha emission lines .Our results show that strong clustering objects prefer to have greater equivalent widths . These conclusions conclude that there exists some evolutionary link between LAEs and LBGs .Keywords: Lyman alpha emitter",
        "rewrite_text": "We provide an evaluation of Lyman alpha emitters (LAEs) identified through narrowband surveys with Subaru/Suprime-Cam and subsequent spectroscopic follow-up with VLT/VIMOS at redshift z ~ 3.1, a crucial period for galaxy formation. Our findings reveal that LAEs inhabit a diverse range of environments, ranging from isolated areas to dense clusters. We observed that the clustering characteristics of LAEs are influenced by their luminosities, with brighter LAEs exhibiting stronger clustering than their dimmer counterparts. This observation implies that brighter LAEs might represent more evolved structures than fainter ones. Additionally, we explored the relationship between clustering strength and the equivalent widths of Lyman-alpha emission lines. Our results indicate that objects with strong clustering tendencies tend to have larger equivalent widths. These findings suggest an evolutionary connection between LAEs and Lyman Break Galaxies (LBGs). \n\nKeywords: Lyman alpha emitter",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": -0.2626128657194451
    },
    {
        "original_text": "We present an explicit expression for the irreducible form of the metric variation of the action term in sixth order gravity, which is valid to all orders in perturbation theory. We also show that this result can be used to derive an approximate expression for the stress energy tensor of the gravitational field. The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non-canonical kinetic term. In particular we find that the non-Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is exactly de Sitter space-time. This implies that the bispectrum produced by such models cannot be described solely in terms of local shape functions as it was previously thought. \nI. INTRODUCTORY REMARK\nIn recent years there has been renewed interest on higher derivative theories of gravity motivated mainly by their possible role in quantum gravity phenomenology (see e.g. ), but also because they provide interesting alternatives to standard General Relativity (GR) in the context of modified gravity scenarios . However, despite these efforts, our understanding of the physical consequences of these theories remains incomplete due to technical difficulties associated with the analysis of their solutions. One of the main obstacles comes from the fact that the equations of motion derived from these actions contain derivatives of arbitrarily high order, making them difficult or impossible to solve analytically. A way out of this problem consists in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms. Although this approach allows one to obtain useful information about the dynamics of the system under consideration, it fails to capture important features like back-reaction effects between different modes of the same field or interactions among different fields. For example, in the case of inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not reproduce correctly the observed level of primordial non-Gaussianities .\nA more systematic method to deal with these problems involves the use of covariant techniques developed originally within the framework of GR. These methods allow us to express the equations of motion in a manifestly gauge",
        "watermark_text": "We present an explicit expression for the irreducible form of the metric variation of the activity term in sixth order gravity , which is valid to all orders in perturbation theory . We additionally prove that this consequence can be used to derive an approximate representation for the strain energy tensor of the gravitational field .The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non - canonical kinetic term . In particular we find that the non - Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is precisely de Sitter space - time .This implies that the bispectrum produced by such theories cannot be described solely in terms of local form variables as it was formerly thought . I .INTRODUCTORY REMARK In recent years there has been continued interest on higher derivative theories of gravitational motivated mainly by their possible role in quantum gravitational phenomenology ( saw e . g . ) , but also because they give exciting alternatives to standard General Relativity ( GR ) in the context of modified gravity scenarios .However , despite these attempts , our grasp of the physical effects of these theories appears incomplete due to technical problems related with the processing of their solutions . One of the main problems comes from the fact that the equations of movement obtained from these actions involve derivatives of arbitrarily high order , making them harder or impossible to solve analytically .A way out of this question involves in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms . Although this methodology allows one to obtain usable information about the dynamics of the process under consideration , it fails to capture important features like back - reaction effects between various modes of the same field or relationships among different fields .For instance , in the case of inflationary cosmologies based on higher derivative gravity , the truncated perturbative expansions do not reproduce correctly the seen level of primordial non - Gaussianities . A more thorough method to deal with these problems involves the using of covariant techniques established originally within the framework of GR .These methods provide us to express the equations of movement in a manifestly gauge",
        "rewrite_text": "We provide a detailed expression for the irreducible variation of the metric concerning the activity term in sixth-order gravity, which remains applicable at all orders in perturbation theory. Additionally, we demonstrate that this outcome can be utilized to derive an approximate representation of the strain energy tensor for the gravitational field. Our findings are employed to investigate the evolution of cosmological perturbations during inflation driven by a scalar field with a non-canonical kinetic term. Notably, we discover that the non-Gaussianity generated at second order in perturbation theory persists even when the background geometry is exactly de Sitter spacetime. This suggests that the bispectrum produced by such theories cannot be solely expressed in terms of local form variables, contrary to previous assumptions.\n\n**I. INTRODUCTORY REMARKS:** In recent years, there has been sustained interest in higher-derivative theories of gravity, primarily due to their potential implications for quantum gravitational phenomena, as well as their intriguing alternatives to standard General Relativity (GR) in modified gravity scenarios. However, despite these efforts, our understanding of the physical implications of these theories remains limited due to technical challenges associated with processing their solutions. A significant issue arises from the equations of motion derived from these actions, which involve derivatives of arbitrarily high order, complicating or rendering analytical solutions impossible. One approach to address this problem involves expanding the fields around a fixed background solution and truncating the resulting series expansion after a finite number of terms. While this method provides useful insights into the dynamics under consideration, it fails to capture critical features such as back-reaction effects among different modes of the same field or relationships between various fields. For example, in inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not accurately reproduce the observed levels of primordial non-Gaussianities. A more comprehensive approach to tackle these issues involves employing covariant techniques originally established within the framework of GR, which allow us to express the equations of motion in a manifestly gauge-invariant manner.",
        "ori-fast-z-score": 0.5261522196019802,
        "water-fast-z-score": 7.29096647162744,
        "rewrite-fast-z-score": 0.38014296063485276
    },
    {
        "original_text": "We present the results of simultaneous X-ray (Chandra) and radio (RXTE )observations of the Broad Line Radio Galaxy, 3C382 . The data were taken on 2001 September 24-25 UT during an outburst in which the source was detected at radio frequencies as high as 22 GHz. We find that the X-ray spectrum is well described by a power law with photon index Γ = 1.7 ± 0.1 modified by photoelectric absorption consistent with N_H = 2 x 1022 cm-2. There are no significant spectral changes between the two epochs observed. In addition to the continuum emission we detect several narrow lines including Fe Kα , He-like Si XIII , S XV and Ar XVII . These features appear blueshifted relative to their rest wavelengths indicating bulk motion towards us along our line-of-sight. Using these velocities together with estimates for the mass of the central black hole derived from optical measurements we estimate the distance of the emitting material from the center of the AGN to be ~10 light days.",
        "watermark_text": "We present the conclusion of simultaneous X - ray ( Chandra ) and radio ( RXTE ) observations of the Broad Line Radio Galaxy , 3C382 . The data were took on 2001 September 24 - 25 UT during an outburst in which the source was seen at radio altitudes as long as 22 GHz .We see that the X - ray spectrum is well described by a power law with photon index Γ = 1 . 7 ± 0 . 1 augmented by photoelectric diffusion compatible with N _ H = 2 x 1022 centimetres - 2 . There are no notable spectral changes between the two epochs observed .In addition to the continuum emission we perceive several small lines including Fe Kα , He - like Si XIII , S XV and Ar XVII . These features appear blueshifted relative to their rest wavelengths suggesting bulk movement towards us along our line - of - view .Using these velocities together with projections for the mass of the central black hole derived from optical calculations we estimate the distance of the emitting substance from the center of the AGN to be ~ 10 light days .",
        "rewrite_text": "We present the findings from simultaneous X-ray (Chandra) and radio (RXTE) observations of the Broad Line Radio Galaxy 3C382, conducted on September 24-25, 2001, during an outburst when the source was observed at radio frequencies up to 22 GHz. Our analysis demonstrates that the X-ray spectrum can be accurately described by a power law with a photon index of Γ = 1.7 ± 0.1, supplemented by photoelectric absorption consistent with N_H = 2 x 10^22 cm^-2. No significant spectral variations were detected between the two observing periods. In addition to the continuum emission, we identified several weak emission lines, including Fe Kα, He-like Si XIII, S XV, and Ar XVII. These lines are blueshifted compared to their rest wavelengths, indicating a bulk motion toward us along the line of sight. By combining these velocities with estimates of the mass of the central black hole derived from optical measurements, we estimate that the emitting material is located approximately 10 light days from the center of the active galactic nucleus (AGN).",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": -1.2309149097933272
    },
    {
        "original_text": "We study the possibility that gravitational waves can be detected by measuring their effect on gyroscopes in space, as proposed for the GP-B experiment. We consider two classes of models with torsion and show how they affect the motion of test particles around spinning black holes. In one class we find that there is no effect at all; this includes Einstein-Cartan theory (with or without fermions) and teleparallel gravity. The other class contains some effects but these are too small to be detectable even if the spin of the black hole were known exactly. However, it may still be possible to detect such effects using future experiments like LISA. Finally, we discuss whether any of our results could have been anticipated within general relativity. This work was supported by NSF grant PHY-0456747. Gravitational waves will produce tiny changes in the orientation of gyroscopes carried into space by satellites. These changes should be measurable by comparing the orientations of pairs of gyroscopes separated by large distances. Such an experiment has recently begun taking data  1  . It is called Gravity Probe B (GP-B), after its predecessor which measured the precession of the earth s orbit  2  .\nIn this Letter we investigate what information about gravitational waves might be obtained from measurements made by GP-B. Our main focus is on theories containing torsion -the antisymmetric part of the connection  3, 4  , which plays a role similar to electromagnetism in standard general relativity  5  . Torsion arises naturally in many extensions of general relativity  6  ; however, it also appears in certain modified versions of general relativity  7, 8  . For example, in string-inspired supergravity  9  , torsion couples directly to matter fields  10  .",
        "watermark_text": "We explore the idea that gravity signals can be identified by monitoring their effect on gyroscopes in space , as suggested for the GP - B experiment . We consider two groups of models with torsion and know how they impact the movement of research particles around spun dark holes .In one category we find that there is no effect at all ; this includes Einstein - Cartan theory ( with or without fermions ) and teleparallel gravitational . The other class includes some effects but these are too small to be detectable even if the spin of the dark hole were known exactly .However , it could still be possible to observe such effects utilizing potential experiments like LISA . Finally , we issue whether any of our findings may have been anticipated within general relativity .This project was supported by NSF grant PHY - 0456747 . Gravitational waves will generate tiny changes in the orientation of gyroscopes transported into space by satellites .These changes should be measurable by testing the orientations of pairs of gyroscopes separated by large distances . Such an observation has recently begun took results 1 .It is titled Gravity Probe B ( GP - B ) , after its predecessor which calculated the precession of the earth s orbit 2 . In this Letter we investigate what knowledge about gravitational waves might be obtained from measurements made by GP - B .Our main interest is on fields containing torsion - the antisymmetric part of the relationship 3 , 4 , which plays a role similar to electromagnetism in standard special relativity 5 . Torsion occurs commonly in many extensions of general relativity 6 ; however , it also exists in certain modified variants of general relativity 7 , 8 .For instance , in string - inspired supergravity 9 , torsion couples directly to matter fields 10 .",
        "rewrite_text": "We investigate the concept that gravity signals can be detected by observing their influence on gyroscopes in space, as proposed for the Gravity Probe B (GP-B) experiment. Our analysis includes two categories of models that incorporate torsion and examines their effects on the trajectories of test particles around spinning black holes. In the first category, we find that there are no effects; this includes the Einstein-Cartan theory (both with and without fermions) and teleparallel gravity. The second category does exhibit some effects, but these are too minimal to be detected, even if the spin of the black hole were precisely known. Nonetheless, it may still be feasible to observe such effects through potential future experiments like LISA. We also contemplate whether any of our results could have been predicted within the framework of general relativity. This research was funded by NSF grant PHY-0456747. Gravitational waves will induce subtle changes in the orientation of gyroscopes carried into space by satellites. These changes are expected to be measurable by analyzing the orientations of pairs of gyroscopes that are widely separated. Recent observations have begun to yield results in this area, specifically from the GP-B initiative, named after its predecessor that assessed the precession of Earth's orbit. In this letter, we explore what insights into gravitational waves might be gleaned from the data collected by GP-B. We focus particularly on fields that contain torsion—the antisymmetric component of the connection—which serves a role analogous to electromagnetism in standard special relativity. Torsion is commonly found in various extensions of general relativity, as well as in certain modified versions of the theory. For example, in string-theory-inspired supergravity, torsion interacts directly with matter fields.",
        "ori-fast-z-score": 0.5432144762551112,
        "water-fast-z-score": 8.746195456274092,
        "rewrite-fast-z-score": -0.2683281572999747
    },
    {
        "original_text": "We present an efficient computational method to compute the elastic properties, vibrational frequencies and normal modes of proteins using finite element analysis (FEA). The proposed approach is based on representing each amino acid residue as a single node in a 3D tetrahedral mesh that covers the entire structure of interest. We show how this representation can be used within FEA to efficiently calculate the stiffness matrix associated with the system under study. In addition we demonstrate how the same representation allows us to obtain accurate results when computing the eigenfrequencies and corresponding mode shapes of the system by solving the generalized eigenvalue problem arising from the discretized form of the governing equations. Finally, we illustrate our methodology through several numerical examples involving different types of proteins. Our results indicate that the proposed approach provides highly accurate predictions compared to those obtained via other state-of-the-art methods such as molecular dynamics simulations or coarse-grained models. This work was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189 .",
        "watermark_text": "We introduce an efficient numerical technique to compute the elastic properties , vibrational speeds and normal mechanisms of proteins using finite element assessment ( FEA ) . The proposed approach is based on representing each amino residue compound as a single node in a 3D tetrahedral mesh that covers the entire structure of interest .We see how this representation can be used within FEA to easily calculate the stiffness matrix associated with the system under research . In addition we prove how the same representation enables us to obtain precise conclusions when computing the eigenfrequencies and corresponding mode shapes of the system by handling the generalized eigenvalue problem emerging from the discretized form of the governing equations .Finally , we explain our technique through several mathematical examples involving varying kinds of proteins . Our results show that the suggested approach offers highly precise predictions compared to those achieved via other state - of - the - art methods such as protein behavior simulations or fine - grained estimates .This project was supported by NSERC Discovery Grants RGPIN - 2014 - 04567 and RGPIN - 2016 - 05189 .",
        "rewrite_text": "We present an effective numerical method for calculating the elastic properties, vibrational speeds, and normal modes of proteins using finite element analysis (FEA). This approach involves modeling each amino acid residue as a single node within a three-dimensional tetrahedral mesh that encompasses the entire protein structure. We demonstrate how this representation facilitates the straightforward computation of the stiffness matrix relevant to the system being examined. Furthermore, we show that this same model allows for accurate determination of the eigenfrequencies and corresponding mode shapes by addressing the generalized eigenvalue problem derived from the discretized governing equations. We illustrate our method through various mathematical examples involving different types of proteins. Our findings indicate that this technique yields highly accurate predictions in comparison to other advanced methods, such as protein behavior simulations or fine-grained estimates. This research was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science, technology, engineering or mathematics (STEM). The authors argue that the best way for scientists to make new discoveries is not through individual genius but rather by working together as teams on problems they are passionate about.  They also claim that it helps if you have an idea of what your problem looks like before you start solving it because this will help guide your research efforts. This article describes some of their ideas and provides examples of how these concepts can be applied to physics research. In his book Nurturing Breakdowns, James Watson argues that the most successful people who solve complex problems do so by working with others instead of trying to work alone. He says that when we work individually our brains tend to focus only on one aspect of the problem at hand which may lead us down dead ends while working collaboratively allows us to see all aspects of the problem simultaneously.",
        "watermark_text": "The book Nurturing Breakthroughs by James Watson and Peter Winkler is about how to create breakthroughs in science , technology , engineering or math ( STEM ) . The authors argue that the best method for researchers to make fresh findings is not through individual genius but rather by acting together as teams on problems they are loving about .They also claim that it assists if you have an idea of what your problem looks like before you start investigating it because this will assist guide your study efforts . This page describes some of their ideas and provides examples of how these concepts can be applied to physics studies .In his book Nurturing Breakdowns , James Watson says that the most talented people who solution complex challenges do so by working with others rather of trying to work alone . He said that when we study collectively our mind tend to reflect only on one element of the issue at hand which would guide us down dead ends while working collaboratively allows us to see all aspects of the issue simultaneously .",
        "rewrite_text": "The book \"Nurturing Breakthroughs\" by James Watson and Peter Winkler explores how to achieve breakthroughs in the fields of science, technology, engineering, and mathematics (STEM). The authors argue that effective research is less about individual brilliance and more about collaborative teamwork on issues that inspire passion. They also suggest that having a clear understanding of the problem before beginning research can help focus investigative efforts. This page outlines some of their key concepts and illustrates how they can be applied to studies in physics. In \"Nurturing Breakthroughs,\" Watson emphasizes that the most skilled individuals tackling complex problems do so by collaborating with others rather than attempting to work in isolation. He notes that when we study in groups, our thinking can become too narrow if we focus solely on one aspect of the problem, while collaboration allows us to consider all facets of the issue simultaneously.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": -0.254000254000381
    },
    {
        "original_text": "We study the semiclassical dynamics of electrons in magnetic fields, which are described by the Dirac equation with spin-orbit coupling and Zeeman splitting. We show that the electron trajectories can be focused into narrow beams when their initial velocities have opposite directions along the field lines. This is due to an interference between two types of motion -the usual cyclotrons and the so-called  Zitterbewegung  oscillations-which leads to a beating pattern on top of the classical circular orbits. The latter type of motion arises because of the relativistic nature of the particles and its origin lies in the fact that the energy bands are spin split. Our results provide a new perspective for understanding the physics behind phenomena such as the quantum Hall effect or the integer quantum Hall effect at high Landau levels. \nI. INTRODUCTIO N\nThe transport properties of two-dimensional (2D) systems of interacting fermions under strong perpendicular magnetic fields have been studied extensively over many years  1  . In particular, it has been shown that the presence of a quantizing magnetic field gives rise to novel phases characterized by fractional filling factors  2  , where the number of filled Landau levels differs from the expected value  3  .\nIn this work we focus our attention on the case of non-interacting fermions moving in 2D space subject to a uniform magnetic field B = Be z  4  . For simplicity, we consider only one spin species; however, all our results remain valid if both spin projections are taken into account  5  . In addition, we assume that the Fermi level lies within the conduction band  6  . Under these conditions, the low-energy excitations around the Fermi surface are well-described by the massless Dirac Hamiltonian  7, 8  \nwhere v F denotes the Fermi velocity, σ i=x,y,z denote Pauli matrices acting on the spinor wave function Ψ(r), p x = −i∂/∂x and p y = −i∂/(−i∂y). Hereafter, we seth = 1 and e = 1. It should be noted that Eq. (1) \nII. ELECT",
        "watermark_text": "We research the semiclassical dynamics of electrons in magnetic fields , which are explained by the Dirac formula with spin - orbit coupling and Zeeman splitting . We see that the electron trajectories can be focused into narrow beams when their initial velocities have different directions along the field lines .This is due to an interference between two forms of movement - the usual cyclotrons and the so - called Zitterbewegung oscillations - which results to a beating sequence on top of the classical circular orbits . The latter type of movement occurs because of the relativistic behavior of the atoms and its origin lies in the fact that the power groups are momentum separated .Our results bring a new insight for studying the physics behind processes such as the quantum Hall impact or the integer quantum Hall impact at high Landau concentrations . I . INTRODUCTIO N The transport properties of two - dimensional ( 2D ) complexes of interacting fermions under strong diagonal magnetic waves have been studied thoroughly over numerous years 1 .In particular , it has been shown that the presence of a quantizing magnetic force gives rise to novel phases characterized by fractional filling variables 2 , where the proportion of filled Landau concentrations differs from the expected value 3 . In this research we focus our focus on the case of non - interacting fermions moving in 2D space subject to a uniform magnetic force B = Be z 4 .For simplicity , we treat only one spin species ; however , all our findings remain correct if both spinning projections are took into consideration 5 . In addition , we suppose that the Fermi level sits within the conduction band 6 .Under these conditions , the small - energy excitations around the Fermi surface are better - described by the massless Dirac Hamiltonian 7 , 8 where v F denotes the Fermi velocity , σ i = x , y , z define Pauli matrices acting on the spinor wave function Ψ ( r ) , r x = −i∂ / ∂x and p y = −i∂ / ( −i∂y ) . Hereafter , we seth = 1 and e = 1 .It should be mentioned that Eq . ( 1 ) II .ELECT",
        "rewrite_text": "We investigate the semiclassical dynamics of electrons in magnetic fields, which can be described by the Dirac equation incorporating spin-orbit coupling and Zeeman splitting. Our findings reveal that electron trajectories can converge into narrow beams when their initial velocities vary in direction along the magnetic field lines. This phenomenon arises from interference between two types of motion—the conventional cyclotron motion and the so-called Zitterbewegung oscillations—leading to a beating pattern superimposed on classical circular orbits. The latter motion is attributed to relativistic effects in the electrons, originating from the separation of power groups in momentum space. Our results provide fresh insights into the physics underlying phenomena such as the quantum Hall effect and the integer quantum Hall effect at high Landau level fillings. \n\nI. INTRODUCTION\n\nThe transport properties of two-dimensional (2D) systems of interacting fermions in strong perpendicular magnetic fields have been extensively investigated over the years. It has been demonstrated that the presence of a quantizing magnetic field leads to the emergence of novel phases characterized by fractional filling fractions, where the occupation of Landau levels deviates from expected values. In this study, we focus on non-interacting fermions in a 2D space under a uniform magnetic field, B = Be_z. For simplicity, we consider only one spin type, although our results hold true for both spin projections. Furthermore, we assume that the Fermi level is located within the conduction band. Under these conditions, low-energy excitations near the Fermi surface can be accurately described by the massless Dirac Hamiltonian, where v_F represents the Fermi velocity, σ_i (with i = x, y, z) denotes the Pauli matrices acting on the spinor wave function Ψ(r), r_x = -i∂/∂x, and p_y = -i∂/∂y. For convenience, we set ℏ = 1 and e = 1. It is important to note that in Eq. (1)... \n\nII. ELECT...",
        "ori-fast-z-score": -1.007017629956027,
        "water-fast-z-score": 7.216959681351526,
        "rewrite-fast-z-score": 0.6024640760767093
    },
    {
        "original_text": "We consider the Standard Model (SM) in 5 dimensions, where one extra dimension is compactified to an orbifold S 1 /Z 2 . The SM fields are assumed to be localized at different fixed points along this extra dimension. We show that such models can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings. In addition we find that these models provide new ways for understanding some other issues related to the SM like neutrino mass generation or flavor changing neutral currents. Finally we discuss how our results could be tested experimentally. Introduction: One of the most important open questions in particle physics today concerns the origin of fermion families and their mixing angles. It has been known since the work by Pati & Salam  1  , that if quarks and leptons were unified into larger multiplets then it would be possible to understand the pattern of quark-lepton masses and mixings within Grand Unified Theories (GUTs). However, despite many attempts over more than 30 years no realistic GUT has yet been constructed which incorporates all the features of the Standard Model (SM).\nIn recent years another possibility was suggested  2  -  4  : If the SM fields live in higher dimensional space-time, they may have Kaluza-Klein excitations corresponding to additional states with masses of order 1/R, where R denotes the size of the extra dimensions. These states might correspond to heavy particles beyond those present in the SM spectrum. This idea leads to interesting phenomenological consequences  5  .\nThe simplest way to realize this scenario is to assume that only gravity propagates in the bulk while the SM fields are confined to a four-dimensional  brane   6  . Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by: \nwhere M P l = 1/ √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i. For distances smaller than about 0.1 mm deviations from the inverse square law predicted by general relativity will become",
        "watermark_text": "We consider the Standard Model ( SM ) in 5 dimensions , where one extra dimension is compactified to an orbifold S 1 / Z 2 . The SM fields are expected to be localized at different fixed points along this extra dimension .We see that such theories can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings . In addition we find that these models bring fresh ways for explaining some other issues related to the SM like neutrino mass development or color shifting neutral currents .Finally we talk how our findings may be evaluated experimentally . Introduction : One of the most important open questions in particle science today issues the origin of fermion families and their mixing angles .It has been known since the paper by Pati & Salam 1 , that if quarks and leptons were organized into larger multiplets then it would be possible to comprehend the trend of quark - lepton masses and mixings within Grand Unified Theories ( GUTs ) . However , despite many efforts over more than 30 centuries no realistic GUT has already been constructed which includes all the details of the Standard Model ( SM ) .In recent work another possibility was suggested 2 - 4 : If the SM fields reside in larger dimensional space - time , they may have Kaluza - Klein excitations corresponding to extra states with masses of order 1 / R , where R denotes the height of the extra dimensions . These states could belong to heavy ions beyond those present in the SM spectrum .This idea results to useful phenomenological consequences 5 . The shortest way to realize this situation is to assume that only gravitational propagates in the bulk while the SM fields are localized to a four - dimensional brane 6 .Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by : where M P l = 1 / √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the quantity of added spatial dimensions accessible to field i . For distances smaller than about 0 . 1 mm deviations from the inverse square law predicted by particular relativity will become",
        "rewrite_text": "We analyze a five-dimensional version of the Standard Model (SM), where one extra dimension is compactified as an orbifold \\( S^1/\\mathbb{Z}_2 \\). In this framework, the SM fields are localized at specific fixed points along this additional dimension. This arrangement allows the theory to naturally account for the existence of the three generations of fermions and gauge bosons, along with their observed masses and mixing patterns. Furthermore, we discover that these models provide new insights into various unresolved issues within the SM, such as the generation of neutrino masses and the phenomena of color-changing neutral currents. Lastly, we discuss how our findings could be tested through experimental means. \n\n**Introduction:** One of the most significant unanswered questions in particle physics today revolves around the origins of fermion families and their mixing angles. The work of Pati and Salam has shown that if quarks and leptons are grouped into larger multiplets, it may be possible to account for the observed patterns of quark and lepton masses and mixings within Grand Unified Theories (GUTs). Nevertheless, despite extensive efforts over the past three decades, no comprehensive GUT has been designed that incorporates all aspects of the Standard Model (SM). Recently, an alternative approach was proposed: if SM fields exist within a higher-dimensional spacetime, they may exhibit Kaluza-Klein excitations associated with additional states possessing masses on the order of \\( 1/R \\), where \\( R \\) is the scale of the extra dimension. These states could correspond to heavy particles not found within the SM spectrum, leading to significant phenomenological implications. One straightforward way to realize this scenario is to postulate that only gravity propagates in the bulk, while the SM fields are confined to a four-dimensional brane. Such theories introduce modifications to the Newtonian potential between two test masses \\( m_1 \\) and \\( m_2 \\) separated by a distance \\( r \\), characterized by the formula where \\( M_{Pl} = 1/\\sqrt{8\\pi G_N} \\approx 10^{19} \\) GeV represents the reduced Planck scale, and \\( n_i \\) denotes the number of additional spatial dimensions accessible to field \\( i \\). At distances smaller than approximately 0.1 mm, deviations from the inverse square law predicted by special relativity will emerge.",
        "ori-fast-z-score": 2.685380346549405,
        "water-fast-z-score": 9.115037909077289,
        "rewrite-fast-z-score": 2.794782784191074
    },
    {
        "original_text": "We present the results of our investigation on semiclassical scalar propagator in curved space-time, which is based on the WKB approximation to the wave function. We show that there are two different ways how one can define this quantity depending on whether or not one takes into account the back-reaction effects due to the quantum fluctuations of the gravitational field. The first approach leads to an expression for the semiclassical propagator which coincides with the Feynman propagator at large distances but differs significantly near the source point. In particular it does not satisfy the Hadamard condition required by general relativity. On the other hand, if we take into account the back reaction then the resulting expression satisfies all necessary conditions including the Hadamard condition. However, as was shown recently by Wald et al., such an expression cannot be obtained within the framework of standard QFT. This problem may have important consequences when considering the propagation of particles through black holes since the corresponding expressions differ substantially even outside the horizon.",
        "watermark_text": "We present the conclusion of our analysis on semiclassical scalar propagator in curved space - time , which is based on the WKB approximation to the wave function . We see that there are two different ways how one can define this quantity based on whether or not one takes into consideration the back - reaction effects due to the quantum fluctuations of the gravitational field .The first method results to an definition for the semiclassical propagator which coincides with the Feynman propagator at large distances but varies dramatically near the origin point . In particular it does not satisfy the Hadamard condition required by general relativity .On the other hand , if we took into consideration the back response then the resulting expression satisfies all necessary circumstances including the Hadamard condition . However , as was shown lately by Wald et al . , such an form cannot be obtained within the framework of standard QFT .This problem could have important implications when examining the propagation of particles through black holes since the equivalent definitions differ substantially even outside the horizon .",
        "rewrite_text": "We conclude our analysis of the semiclassical scalar propagator in curved spacetime, which is based on the WKB approximation of the wave function. Our study reveals two distinct methods for defining this propagator, depending on whether we account for back-reaction effects from quantum fluctuations in the gravitational field. The first approach yields a definition of the semiclassical propagator that aligns with the Feynman propagator at large distances but exhibits significant variation near the origin. Notably, this version fails to meet the Hadamard condition mandated by general relativity. Conversely, if we incorporate the back-reaction effects, the resulting expression complies with all necessary criteria, including the Hadamard condition. However, as demonstrated recently by Wald et al., this form cannot be derived within the framework of conventional quantum field theory. This discrepancy may have significant implications for the study of particle propagation through black holes, as the alternative definitions differ considerably even outside the event horizon.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 2.424871130596428
    },
    {
        "original_text": "We present Hubble Space Telescope (HST) and Spitzer Space Telescope observations of the edge-on circumstellar disk surrounding 2MASS J04414489+2512172, a young brown dwarf with spectral type M8 located in Upper Scorpius at a distance of 145 pc. The HST data reveal that this object is surrounded by a bright ring-like structure extending to ~0.5′′ (~120 AU). We find evidence for two spiral arms emerging from the inner part of the ring toward its center. These features are also seen in near-infrared images obtained with the adaptive optics system NACO on VLT/UT4. In addition, we detect several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them. Our results suggest that the observed structures could have been formed through gravitational instability triggered by rapid inward migration of solids due to gas drag forces.",
        "watermark_text": "We present Hubble Space Telescope ( HST ) and Spitzer Space Telescope observations of the edge - on circumstellar disk surrounding 2MASS J04414489 + 2512172 , a young brown giant with spectral category M8 situated in Upper Scorpius at a distance of 145 pc . The HST results show that this body is surrounded by a bright ring - like structure extending to ~ 0 . 5 ′ ′ ( ~ 120 AU ) .We get information for two spiral bodies emerging from the inner part of the circle toward its core . These features are also shown in near - infrared images obtained with the adaptive optics scheme NACO on VLT / UT4 .In addition , we find various threads along these spirals which may be caused by dust clumps or planetesimals contained within them . Our results propose that the known structures could have been formed through gravity instability caused by rapid inward movement of solids due to gas drag forces .",
        "rewrite_text": "We present observations from the Hubble Space Telescope (HST) and Spitzer Space Telescope of the edge-on circumstellar disk surrounding 2MASS J04414489 + 2512172, a young M8 brown dwarf located in Upper Scorpius at a distance of 145 parsecs. The HST data indicate that this object is encircled by a bright, ring-like structure that extends approximately 0.5″ (around 120 AU). We also observe two spiral features originating from the inner region of the disk and moving toward its center. These features are further captured in near-infrared images taken with the NACO adaptive optics system on the VLT/UT4. Additionally, we identify several strands along these spirals, which may be attributed to dust clumps or planetesimals within the disk. Our findings suggest that these observed structures could have formed as a result of gravitational instability driven by the rapid inward migration of solids due to gas drag forces.",
        "ori-fast-z-score": 0.629940788348712,
        "water-fast-z-score": 5.921443410477893,
        "rewrite-fast-z-score": 1.2909944487358056
    },
    {
        "original_text": "We present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve . We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances .The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption . In particular we prove that our technique permits one to estimate the electricity gap between the conduction band minimum and valence band maximum in semiconductors .This project was supported by Russian Science Foundation gift No . 14 - 50 - 00040 .DOI : 10 . 1063 / 1 . 4935190 I . INTRODUCTORY REMARK The investigation of luminescence effects has been drawing tremendous attention over numerous years because it gives valuable info about electronic properties and electronic properties of solids 1 .Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially useful since it allows us to probe the distribution behavior of electrons excited into the conduction band 2 . In recent generations there have been numerous attempts to develop conceptual models explaining various parts of luminescence events 3 , notably heat stimulation luminescence 4 - 8 .However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 . As a result they cannot describe correctly some important features discovered experimentally 10 .For instance , the form of the TSL flicker curve varies strongly on the kind of material 11 : while in insulators it generally exhibits a single peak 12 , in metals it often consists of several peaks 13 . Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the number of peaks may differ depending on the doping level 15 .These measurements cannot be understood using existing models 16 .",
        "rewrite_text": "We present an evaluation of the temperature dependence of thermally stimulated luminescence (TSL) glow curves based on the nonstationary electron-phonon relaxation hypothesis, which does not assume that the system remains close to equilibrium at any point during its evolution. This approach allows us to extract information regarding the phonon spectrum and the density of states for charge carriers from TSL data obtained from various materials. We compare our findings with results obtained through alternative methods, such as photoluminescence excitation spectroscopy and Raman absorption. Notably, we demonstrate that our technique can be used to estimate the energy gap between the conduction band minimum and the valence band maximum in semiconductors. This research was funded by the Russian Science Foundation under grant No. 14-50-00040. DOI: 10.1063/1.4935190\n\n**Introduction**\n\nThe study of luminescence phenomena has garnered significant attention over the years due to its ability to provide crucial insights into the electronic properties of solids. Thermal stimulated luminescence (TSL), also referred to as optically stimulated luminescence (OSL), is particularly valuable as it enables exploration of the distribution behavior of electrons excited into the conduction band. In recent years, various attempts have been made to develop conceptual models that explain different aspects of luminescence phenomena, particularly in the context of thermal stimulation luminescence. However, many of these models have relied on the assumption that the processes involved are typically near equilibrium, which limits their ability to accurately describe some key experimentally observed features. For example, the shape of TSL glow curves varies significantly depending on the type of material: insulators usually display a single peak, whereas metals often present multiple peaks. Furthermore, within a specific class of materials, such as semiconductor crystals, the number of peaks can differ based on the level of doping. These observations challenge existing models, which struggle to account for the diversity of experimental results.",
        "ori-fast-z-score": -2.3664319132398464,
        "water-fast-z-score": 7.437357441610946,
        "rewrite-fast-z-score": -0.08304547985373997
    },
    {
        "original_text": "We present near-infrared (NIR) spectroscopy of the black hole X-ray binary system A0620-00 obtained with the Subaru Telescope and Gemini Observatory in 2001-2002. The NIR spectrum shows strong emission lines originating from highly ionized species such as FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. We find that these features are well reproduced by our photoionization model calculations for an accretion disk irradiated by hard X-rays emitted from the central source. In addition to the above mentioned ions, we also detect absorption lines due to neutral hydrogen and helium at wavelengths longer than 1 micron. These absorptions originate from the interstellar medium along the line-of-sight toward this object. \n \n Keywords: Near-infrared spectroscopy",
        "watermark_text": "We use near - infrared ( NIR ) spectroscopy of the dark hole X - ray binary system A0620 - 00 collected with the Subaru Telescope and Gemini Observatory in 2001 - 2002 . The NIR spectrum displays strong absorption lines coming from highly ionized compounds such as FeXXV - XXVI , NeIX - XI , MgXI - XII , SiXIV - XVII , SXV - XXIII , ArXVIII - XXIV , CaXIX - XXVIII , and NiXXI - XXIV .We see that these characteristics are better illustrated by our photoionization theory analyses for an accretion disk irradiated by hard X - radiation emitted from the main source . In addition to the above mentioned ions , we also observe absorption paths due to neutral hydrogen and helium at wavelengths greater than 1 micron .These absorptions originate from the interstellar medium along the line - of - view toward this body . Keywords : Near - infrared spectroscopy",
        "rewrite_text": "We conducted near-infrared (NIR) spectroscopy of the dark hole X-ray binary system A0620-00 using data collected from the Subaru Telescope and Gemini Observatory between 2001 and 2002. The NIR spectrum reveals prominent absorption lines from highly ionized species, including FeXXV-XXVI, NeIX-XI, MgXI-XII, SiXIV-XVII, SXV-XXIII, ArXVIII-XXIV, CaXIX-XXVIII, and NiXXI-XXIV. Our analysis, grounded in photoionization theory, better explains these features in the context of an accretion disk heated by hard X-ray radiation from the central source. Additionally, we identify absorption features from neutral hydrogen and helium at wavelengths exceeding 1 micron, attributed to the interstellar medium along the line of sight to this system. Keywords: Near-infrared spectroscopy.",
        "ori-fast-z-score": -1.8962448894726294,
        "water-fast-z-score": 4.521814736434731,
        "rewrite-fast-z-score": 0.7293249574894728
    },
    {
        "original_text": "We study the effects on the entropy and Hawking temperature of extremal black holes due to higher derivative terms in the action, namely those coming from supersymmetric (SUSY) or quadratic curvature invariants such as the Gauss-Bonnet term. We find that for both types of corrections there is an enhancement of the entropy at low temperatures which can be understood by studying the near-horizon geometry of these solutions. In addition we show how this behavior changes when one considers different values of the coupling constants associated with each type of correction. Finally, we discuss some possible implications of our results within string theory. The thermodynamics of black holes has been studied extensively over the past few years  1  . One of the most interesting aspects of this subject concerns the possibility of having quantum gravity corrections to their classical description  2  , especially since it was shown recently  3  that they could have important consequences even if they are small compared to other physical scales involved in the problem. For example, it has been suggested  4  that the inclusion of certain quantum gravitational corrections may lead to a resolution of the information paradox  5  .\nIn particular, it seems reasonable to expect that the entropy of a black hole should receive contributions not only from its horizon area but also from additional degrees of freedom located near the singularity  6  . This idea leads naturally to consider modifications of Einstein s equations involving higher order derivatives  7, 8  . However, although many authors have considered various forms of higher-order corrections  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59  , very little attention has been paid so far  60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,",
        "watermark_text": "We explore the effects on the entropy and Hawking temperature of extremal black holes due to higher derivative terms in the operation , particularly those coming from supersymmetric ( SUSY ) or quadratic curvature invariants such as the Gauss - Bonnet term . We see that for both types of corrections there is an enhancement of the entropy at low temperatures which can be understood by examining the near - horizon shape of these solutions .In addition we explain how this behavior changes when one considers distinct expressions of the interaction constants associated with each type of correction . Finally , we explain some possible implications of our findings within string theory .The thermodynamics of grey holes has been studied thoroughly over the previous few years 1 . One of the most exciting aspects of this subject concerns the prospect of having quantum gravitational corrections to their classical description 2 , particularly since it was shown recently 3 that they may have important implications even if they are small relative to other physical scales implicated in the issue .For instance , it has been proposed 4 that the introduction of certain quantum gravitational corrections might lead to a resolution of the information paradox 5 . In particular , it appears justified to predict that the entropy of a black hole should receive benefits not only from its horizon area but also from additional degrees of freedom placed near the singularity 6 .This idea results naturally to consider modifications of Einstein s coefficients involving higher order derivatives 7 , 8 . However , although many writers have thought numerous types of greater - order corrections 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , very less attention has been paid so far 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 ,",
        "rewrite_text": "We investigate the impact of higher derivative terms on the entropy and Hawking temperature of extremal black holes, focusing specifically on contributions from supersymmetric (SUSY) and quadratic curvature invariants, such as the Gauss-Bonnet term. Our findings reveal that both types of corrections lead to an increase in entropy at low temperatures, a phenomenon that can be elucidated by examining the near-horizon structure of these solutions. Additionally, we analyze how this behavior shifts depending on the distinct forms of the interaction constants associated with each correction type. We also discuss the potential implications of our results within the context of string theory. The thermodynamics of grey holes have been thoroughly examined in recent years, and one of the most intriguing aspects of this research is the possibility of quantum gravitational corrections to their classical models. Recent studies have suggested that even minor corrections could have significant implications, such as contributing to the resolution of the information paradox. Specifically, it seems reasonable to propose that a black hole's entropy might not only stem from its horizon area but also from extra degrees of freedom located near the singularity. This idea naturally leads to the consideration of modifications to Einstein's equations that involve higher-order derivatives. However, despite the various discussions surrounding higher-order corrections, relatively little attention has been devoted to this area thus far.",
        "ori-fast-z-score": 0.18569533817705186,
        "water-fast-z-score": 7.926290870042667,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "We consider a model for an enzymatic reaction network with multiple futile cycles, where each enzyme is assumed to be present at constant concentration and all reactions are reversible. We show that there exists only one positive equilibrium point if the total amount of enzymes is sufficiently large compared to the total amount of substrate molecules. This result can also be applied to other models such as those describing metabolic networks or gene regulatory systems. The proof relies on the fact that the system has a unique globally asymptotically stable equilibrium point when it is considered under mass action kinetics. In addition we prove that this equilibrium point is locally exponentially stable even though the system does not satisfy the classical Lipschitz condition. Finally, numerical simulations illustrate our results. Keywords: Enzymatic reaction networks; Mass action; Steady state analysis; Metabolic networks; Gene regulatory systems. 1 Introduction Reaction networks have been widely used to describe biochemical processes occurring inside living cells (see e.g.,  1  ,  4  ). These networks consist of chemical species which interact through chemical reactions. A mathematical description of these interactions leads to a set of ordinary differential equations known as the kinetic equations. For example, the Michaelis-Menten mechanism describes how an enzyme E binds reversibly to its substrate S to form a complex C before releasing product P . It consists of three elementary reactions given by \nwhere k + i and k − i denote respectively the forward and backward rate constants associated with the ith reaction. If the concentrations of the reactants and products involved in the above scheme are denoted by  S  ,  E  ,  P   and  C  then the corresponding kinetic equations read dS dt = k 2  E  S  − k −1  S ,\ndE dt = k 3  E  P   − k −2  E ,\n\ndC dt = k 4  C  P   − k −3  C .\n\nThe parameters k i represent the rates of the different reactions. Note that the first two equations correspond to the formation of complexes while the last equation corresponds to their dissociation into free substrates and products.",
        "watermark_text": "We consider a theory for an enzymatic process network with many futile periods , where each enzyme is expected to be found at fixed concentration and all processes are reversible . We see that there exists only one favorable equilibrium point if the total quantity of proteins is sufficiently huge compared to the total quantity of substrate molecules .This result can also be applied to other models such as those describing metabolic networks or gene regulatory processes . The proof based on the fact that the process has a unique globally asymptotically stable equilibrium point when it is regarded under mass activity kinetics .In addition we prove that this equilibrium point is locally exponentially steady even though the model does not satisfy the standard Lipschitz requirement . Finally , numerical simulations exhibit our findings .Keywords : Enzymatic process networks ; Mass response ; Steady state analysis ; Metabolic systems ; Gene regulatory structures . 1 Introduction Reaction networks have been widely using to define biochemical reactions appearing inside live cells ( see e . g . , 1 , 4 ) .These systems consist of chemical species which interact through chemical processes . A mathematical description of these interactions leads to a setting of simple differential equations known as the kinetic equations .For instance , the Michaelis - Menten process represents how an enzyme E connects reversibly to its substrate S to form a complex C before producing product P . It consists of three elementary reactions given by where k + i and k − i describe respectively the forward and back rate constants associated with the ith reaction .If the levels of the reactants and products participating in the above scheme are denoted by S , E , P and C then the associated kinetic equations read dS dt = k 2 E S − k −1 S , dE dt = k 3 E P − k −2 E , dC dt = k 4 C P − k −3 C . The parameters k i describe the rates of the different processes .Note that the first two expressions correspond to the formation of complexes while the last equation relates to their dissociation into free substrates and products .",
        "rewrite_text": "We present a theoretical framework for an enzymatic process network characterized by multiple futile periods, in which each enzyme is maintained at a constant concentration and all processes are reversible. Our analysis reveals that, provided the total quantity of proteins is significantly larger than the total number of substrate molecules, there is only one favorable equilibrium point. This finding is applicable to various models, including those that represent metabolic networks or gene regulatory mechanisms. The proof relies on demonstrating that the process exhibits a unique globally asymptotically stable equilibrium point within the context of mass action kinetics. Furthermore, we establish that this equilibrium point is locally exponentially stable, even though the model does not meet the conventional Lipschitz condition. Lastly, we validate our findings through numerical simulations. \n\n**Keywords:** Enzymatic process networks; mass action; steady-state analysis; metabolic systems; gene regulatory networks. \n\n**1. Introduction**  \nReaction networks are frequently employed to represent biochemical reactions occurring within living cells (see, e.g., 1, 4). These networks comprise chemical species that interact through various chemical processes. A mathematical description of these interactions leads to a set of differential equations known as kinetic equations. For example, the Michaelis-Menten process illustrates how an enzyme E reversibly binds to its substrate S to form a complex C, which subsequently produces a product P. This process can be broken down into three elementary reactions, with k + i and k − i representing the forward and backward rate constants for the ith reaction, respectively. The concentrations of the reactants and products involved in this scheme are denoted by S, E, P, and C, leading to the corresponding kinetic equations: \n\n\\[\n\\frac{dS}{dt} = k_2 E S - k_{-1} S, \\quad \\frac{dE}{dt} = k_3 E P - k_{-2} E, \\quad \\frac{dC}{dt} = k_4 C P - k_{-3} C.\n\\]\n\nHere, the parameters k_i reflect the rates of the various processes, with the first two equations addressing the formation of complexes and the last equation detailing their dissociation into free substrates and products.",
        "ori-fast-z-score": -1.697749375254331,
        "water-fast-z-score": 7.469939758793239,
        "rewrite-fast-z-score": 1.281025230440697
    },
    {
        "original_text": "We present high-resolution optical spectroscopy for three open clusters: NGC 2516 (age = 1 Gyr), Hyades (600 Myr) and M67 (3 Gyr). We use these data to measure the abundance ratios of C/Fe, N/Fe, O/Fe, Mg/Fe, Si/Fe, S/Fe, Ca/Fe, Ti/Fe, Cr/Fe, Mn/Ca, Ni/Co, Y/Ba, Zr/Hf, Sr/Y, Ba/Sr, La/Lu, Ce/Pb, Eu/Gd, Yb/Tm, Lu/Hf, W/Mn, Th/U, and U/Th. These measurements are used as input into galactic chemical evolution models that trace the time-evolution of the abundances of all elements heavier than helium produced by nuclear fusion reactions within massive stars. The results show that the observed trends can be explained if we assume that the initial mass function is bimodal with peaks at 8-10 solar masses and 2-3 solar masses.",
        "watermark_text": "We create fast - resolution optical spectroscopy for three open complexes : NGC 2516 ( age = 1 Gyr ) , Hyades ( 600 Myr ) and M67 ( 3 Gyr ) . We use these information to measure the abundance proportions of C / Fe , N / Fe , O / Fe , Mg / Fe , Si / Fe , S / Fe , Ca / Fe , Ti / Fe , Cr / Fe , Mn / Ca , Ni / Co , Y / Ba , Zr / Hf , Sr / Y , Ba / Sr , La / Lu , Ce / Pb , Eu / Gd , Yb / Tm , Lu / Hf , W / Mn , Th / U , and U / Th .These measurements are using as input into galactic chemical evolution models that trace the period - evolve of the abundances of all components heavier than helium produced by nuclear fusion events within large galaxies . The results show that the observed trends can be described if we suppose that the early mass function is bimodal with peaks at 8 - 10 solar masses and 2 - 3 solar masses .",
        "rewrite_text": "We conduct high-resolution optical spectroscopy for three open clusters: NGC 2516 (age = 1 Gyr), the Hyades (600 Myr), and M67 (3 Gyr). This data allows us to measure the abundance ratios of various elements, including C/Fe, N/Fe, O/Fe, Mg/Fe, Si/Fe, S/Fe, Ca/Fe, Ti/Fe, Cr/Fe, Mn/Ca, Ni/Co, Y/Ba, Zr/Hf, Sr/Y, Ba/Sr, La/Lu, Ce/Pb, Eu/Gd, Yb/Tm, Lu/Hf, W/Mn, Th/U, and U/Th. These measurements serve as inputs for galactic chemical evolution models, which examine the changes in the abundances of all elements heavier than helium produced by nuclear fusion in large galaxies. Our findings suggest that the observed trends can be explained by assuming a bimodal initial mass function with peaks at 8-10 solar masses and 2-3 solar masses.",
        "ori-fast-z-score": -1.0674899923282326,
        "water-fast-z-score": 3.2024699769846983,
        "rewrite-fast-z-score": -0.6030226891555273
    },
    {
        "original_text": "We report on scanning tunneling microscopy and spectroscopy (STM/S) studies of the electronic structure of individual Si donors in silicon dioxide grown by chemical vapor deposition at low temperatures.  The STM images show that the Si atoms are incorporated into molecular structures with an apparent height of 1 nm, which is consistent with previous reports for SiO$_2$ films prepared under similar conditions. We find that these molecules have a characteristic spectroscopic signature consisting of two peaks separated by about 0.5 eV in dI/dV spectra recorded over them. These features can be explained as resulting from hybridization between the localized states associated with each Si atom within the molecule. In addition to this double-peak feature we observe another peak located around -0.3 V bias voltage, whose origin remains unclear. Finally, we discuss possible mechanisms responsible for the formation of such Si-donor molecules. Scanning probe techniques provide unique insight into the local properties of materials. Herein, we present results obtained using scanning tunneling microscopy/spectroscopy (STM/STS), which reveal the electronic structure of individual silicon donors embedded in amorphous silicon dioxide layers deposited onto highly doped p-type silicon substrates. Our experiments were performed in ultrahigh vacuum chambers equipped with standard facilities for sample preparation and characterization.",
        "watermark_text": "We report on laser tunneling microscopy and spectroscopy ( STM / S ) experiments of the electronic structure of individual Si donors in silicon dioxide grown by molecular vapor precipitation at low temperatures . The STM pictures show that the Si atoms are incorporated into molecular forms with an apparent size of 1 mm , which is consistent with previous findings for SiO $ _ 2 $ films prepared under similar situations .We see that these complexes have a peculiar spectroscopic pattern formed of two peaks distinct by about 0 . 5 eV in dI / dV spectra recorded over them . These features can be understood as occurring from hybridization between the localized states associated with each Si atom within the molecule .In addition to this double - peak structure we study another peak located around - 0 . 3 V bias frequency , whose identity remains disputed . Finally , we investigate possible processes responsible for the formation of such Si - donor molecules .Scanning probe methods provide unique insight into the local characteristics of structures . Herein , we present results acquired using scan tunneling microscopy / spectroscopy ( STM / STS ) , which expose the electronic properties of individual silicon donors embedded in amorphous silicon dioxide sheets injected onto deeply doped n - class silicon substrates .Our experiments were performed in ultrahigh pressure chambers equipped with typical laboratories for sample preparation and determination .",
        "rewrite_text": "We present findings from laser tunneling microscopy and spectroscopy (STM/STS) experiments investigating the electronic structure of individual silicon (Si) donors within silicon dioxide, which was produced via molecular vapor precipitation at low temperatures. The STM images reveal that the Si atoms are integrated into molecular formations with an apparent size of 1 mm, aligning with prior observations of SiO$_2$ films created under comparable conditions. Our results indicate that these complexes exhibit a distinctive spectroscopic pattern characterized by two peaks that are approximately 0.5 eV apart in the dI/dV spectra captured over the samples. These features can be attributed to hybridization effects between the localized states of each Si atom within the molecular structure. Furthermore, we analyze another peak observed around -0.3 V bias, whose nature remains under debate. Finally, we explore potential mechanisms that may contribute to the formation of such Si-donor molecules. Scanning probe techniques offer valuable insights into the local properties of these structures. In this report, we highlight results obtained through scanning tunneling microscopy/spectroscopy (STM/STS), which reveal the electronic characteristics of individual silicon donors embedded in amorphous silicon dioxide layers applied to deeply doped n-type silicon substrates. Our experiments were conducted in ultrahigh vacuum chambers equipped with standard laboratory facilities for sample preparation and analysis.",
        "ori-fast-z-score": -1.165543034828717,
        "water-fast-z-score": 7.57602972638666,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "We report on helioseismic observations made by GOLF and MDI instruments aboard SOHO spacecraft during the largest recorded solar flare in recent years, which occurred on 2005 Jan. 15 (Solar Flare Event #11). The event produced an intense seismic signal with a duration of about 20 minutes that was detected simultaneously at two different frequencies corresponding to acoustic waves traveling along opposite directions across the Sun s surface. We find that this signal is consistent with a source located near the center of the active region NOAA 10486 where the flare took place. This result suggests that the energy released by the flare may have been channeled into the generation of strong toroidal magnetic fields through the action of plasma flows driven by the Lorentz force. These results are discussed within the framework of current models for solar flares. \n \n Keywords: Solar flare, seismology, sunquake \n \n 1 Introduction \n \n Intense solar flares can release huge amounts of energy over very short timescales. It has recently become possible to study these events using space-based observatories such as the Solar and Heliospheric Observatory (SOHO)  1  . During large solar flares, it is often observed that there is a significant increase in the intensity of the photospheric Doppler velocity field  2  , which indicates that the photosphere undergoes rapid motions associated with the eruption of coronal mass ejections  3  . However, the exact physical mechanisms responsible for driving these phenomena remain poorly understood  4  .\n \nIn addition to their effects on the photospheric flow velocities, solar flares also produce powerful seismic signals known as  sunquakes   5  . These signals were first discovered by Leighton et al  6  who used ground-based measurements of the Doppler shift of the Fraunhofer lines in the visible spectrum of sunlight reflected off the Moon. Since then, several other groups  7, 8  have reported similar detections based on data obtained either from ground-based or spacebased telescopes operating in various parts of the electromagnetic spectrum  9  . More recently, Kosovichev",
        "watermark_text": "We report on helioseismic measurements made by GOLF and MDI instruments aboard SOHO satellites during the greatest documented solar flare in recent years , which occurred on 2005 Jan . 15 ( Solar Flare Event # 11 ) . The event produced an strong seismic response with a duration of about 20 minutes that was detected simultaneously at two different frequencies corresponding to acoustic waves coming along opposite directions across the Sun s surface .We see that this signal is compatible with a source located near the center of the active region NOAA 10486 where the flare took place . This result suggests that the electricity created by the flare might have been channeled into the generation of large toroidal magnetic fields through the activity of plasma flows driven by the Lorentz force .These conclusions are discussed within the framework of recent estimates for solar flares . Keywords : Solar flare , seismology , sunquake 1 Introduction Intense sun flares can release massive amounts of electricity over very brief timescales .It has recently become able to study these incidents use space - based observatories such as the Solar and Heliospheric Observatory ( SOHO ) 1 . During large solar flares , it is often observed that there is a substantial rise in the strength of the photospheric Doppler velocity field 2 , which implies that the photosphere undergoes fast motions resulting with the eruption of coronal mass ejections 3 .However , the exact physical mechanisms involved for controlling these phenomena remain weakly understood 4 . In addition to their impacts on the photospheric flow velocities , sun flares additionally produce violent seismic signals dubbed as sunquakes 5 .These transmissions were first discovered by Leighton et al 6 who used ground - based measurements of the Doppler shift of the Fraunhofer lines in the visible spectrum of sunlight reflected off the Moon . Since then , various other bands 7 , 8 have reported similar detections based on evidence derived either from land - based or spacebased telescopes located in different regions of the electromagnetic spectrum 9 .More recently, Kosovichev",
        "rewrite_text": "We present findings from helioseismic measurements conducted by the GOLF and MDI instruments on board the SOHO satellites, during the most significant solar flare recorded in recent years, which took place on January 15, 2005 (Solar Flare Event #11). This event generated a pronounced seismic response lasting approximately 20 minutes, detected at two distinct frequencies corresponding to acoustic waves traveling in opposite directions across the Sun's surface. Our analysis indicates that this signal likely originated near the center of the active region NOAA 10486, where the flare occurred. These observations imply that the electricity produced by the flare may have been directed into the formation of substantial toroidal magnetic fields, facilitated by plasma flows driven by the Lorentz force. We discuss these findings in the context of recent estimates regarding solar flares.\n\n**Keywords**: Solar flare, seismology, sunquake\n\n**1 Introduction**  \nIntense solar flares can unleash enormous amounts of electricity in very short timeframes. With the advent of space-based observatories like the Solar and Heliospheric Observatory (SOHO), it has become possible to study these events more closely. During significant solar flares, there is often a notable increase in the strength of the photospheric Doppler velocity field, indicating rapid motions in the photosphere that can lead to the eruption of coronal mass ejections. However, the precise physical mechanisms governing these phenomena are still not fully understood. Beyond their effects on photospheric flow velocities, solar flares also generate intense seismic signals known as sunquakes. These signals were first identified by Leighton et al., who employed ground-based measurements of Doppler shifts in the Fraunhofer lines from sunlight reflected off the Moon. Since then, numerous studies across different bands have reported similar detections using data from both land-based and space-based telescopes across various regions of the electromagnetic spectrum. Recently, Kosovichev...",
        "ori-fast-z-score": 0.6713450866373513,
        "water-fast-z-score": 8.166666666666666,
        "rewrite-fast-z-score": 1.52127765851133
    },
    {
        "original_text": "The aim of this thesis is to provide an overview on the state-of-the-art Hartree-Fock Self-Consistent-Field (SCF) methods for solving quantum chemical problems, with special emphasis on their numerical aspects.  The first chapter introduces basic concepts related to molecular orbital theory and electronic structure calculations using density functional theory (DFT). In particular, we discuss how DFT can be used as a tool to study ground-state properties of molecules by means of Kohn-Sham orbitals. We also present some fundamental results concerning the convergence of iterative schemes that are commonly employed within self-consistent field approaches. The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been developed over the last decades to solve the Hartree-Fock equations numerically. These include the Roothaan-Hall method, the Davidson algorithm, and its variants such as the Pulay-Davidson scheme or the linearized Davidson approach. Finally, we introduce the concept of preconditioning and illustrate it through two examples.",
        "watermark_text": "The goal of this dissertation is to provide an overview on the state - of - the - art Hartree - Fock Self - Consistent - Field ( SCF ) techniques for solving quantum chemical problems , with special emphasis on their numerical parts . The first section introduces basic concepts related to molecular orbital theory and electronic stability analysis utilizing density functional theory ( DFT ) .In particular , we explain how DFT can be used as a technique to study ground - state properties of molecules by means of Kohn - Sham orbitals . We additionally offer some fundamental findings concerning the convergence of iterative strategies that are often employed within self - coherent field methods .The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been built over the last decades to solve the Hartree - Fock equations numerically . These include the Roothaan - Hall method , the Davidson approximation , and its versions such as the Pulay - Davidson scheme or the linearized Davidson technique .Finally , we present the notion of preconditioning and explain it through two examples .",
        "rewrite_text": "This dissertation aims to provide a comprehensive overview of state-of-the-art Hartree-Fock Self-Consistent Field (SCF) methods for addressing quantum chemical challenges, focusing particularly on their numerical aspects. The first section presents fundamental concepts related to molecular orbital theory and the analysis of electronic stability using density functional theory (DFT). Specifically, we demonstrate how DFT serves as a valuable tool for investigating the ground-state properties of molecules through Kohn-Sham orbitals. We also present key insights into the convergence behavior of iterative methods commonly used in self-consistent field approaches. The second chapter explores various algorithmic classes based on direct minimization techniques that have been developed over recent decades to numerically solve the Hartree-Fock equations. This includes the Roothaan-Hall method, the Davidson approximation, and its variants, such as the Pulay-Davidson scheme and the linearized Davidson technique. Lastly, we introduce the concept of preconditioning and illustrate it using two examples.",
        "ori-fast-z-score": 0.6793662204867574,
        "water-fast-z-score": 6.863036195648746,
        "rewrite-fast-z-score": -0.11396057645963795
    },
    {
        "original_text": "We present an analysis of the transition between first stars and second stars, which are formed by gravitational collapse of primordial gas clouds with masses ranging from 10^4 M_sol to 10^6 M_sol.  We show that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium (IGM). The suppression factor increases as redshift decreases because the IGM temperature rises more rapidly than its density. At lower redshifts, we find that the formation rates of both first and second stars increase sharply when the universe becomes reionized. This effect occurs because the ionizing photons produced during reionization heat up the surrounding neutral hydrogen atoms, thereby increasing their Jeans mass and suppressing fragmentation into smaller objects. Finally, we estimate the number densities of first and second stars using our model for star formation history. Our results suggest that second stars may be detectable via future surveys such as LSST or Euclid.",
        "watermark_text": "We present an assessment of the shift between first stars and second stars , which are created by gravitational decay of primordial liquid clouds with masses vary from 10 ^ 4 M _ sol to 10 ^ 6 M _ sol . We see that the formation rate of second stars is suppressed at redshifts z < 20 due to photoheating effects on the intergalactic medium ( IGM ) .The suppression ratio increases as redshift decreases because the IGM temperature rises more swiftly than its density . At lower redshifts , we find that the formation rates of both first and first stars increase dramatically when the universe becomes reionized .This phenomenon occurs because the ionizing photons created during reionization heat up the nearby neutral hydrogen atoms , thereby expanding their Jeans mass and suppressing fragmentation into bigger objects . Finally , we estimate the number densities of early and first stars using our model for star formation history .Our results propose that second stars would be detectable via upcoming polls such as LSST or Euclid .",
        "rewrite_text": "We provide an evaluation of the transition from first stars to second stars, which are formed through the gravitational collapse of primordial gas clouds with masses ranging from \\(10^4 M_\\odot\\) to \\(10^6 M_\\odot\\). Our analysis indicates that the formation rate of second stars is inhibited at redshifts \\(z < 20\\) due to photoheating effects on the intergalactic medium (IGM). This suppression becomes more pronounced as redshift decreases, as the temperature of the IGM increases more rapidly than its density. At lower redshifts, we observe a significant rise in the formation rates of both first and second stars following the reionization of the universe. This increase is attributed to ionizing photons produced during reionization, which heat nearby neutral hydrogen atoms, thus raising their Jeans mass and reducing fragmentation into larger structures. Lastly, we estimate the number densities of early and first stars based on our model of star formation history. Our findings suggest that second stars could be observable in forthcoming surveys such as LSST or Euclid.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 1.5215349135496974
    },
    {
        "original_text": "We present the results for neutrino mixing angles, CP violating phases and mass squared differences obtained by using the Fritzsch ansatz to generate the charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). We find that this model can accommodate all experimental data with only one free parameter which is related to the ratio between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati-Salam gauge symmetry into the Standard Model gauge group. The CKM matrix elements are predicted as well as the Majorana phase associated with leptonic CP violation. In addition we show how the tri-bimaximal pattern observed experimentally in the lepton sector emerges naturally when the quark-lepton unification hypothesis is imposed at high energies. Finally we discuss briefly some phenomenological consequences of our scenario such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "watermark_text": "We present the results for neutrino mixing angles , CP violating stages and mass squared variations obtained by using the Fritzsch ansatz to produce the charged fermion masses within an extended supersymmetric grand unified theory based on SO ( 10 ) . We see that this description can handle all theoretical data with only one free parameter which is related to the proportion between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati - Salam gauge symmetry into the Standard Model gauge group .The CKM matrix elements are expected as well as the Majorana process associated with leptonic CP violation . In addition we show how the tri - bimaximal pattern found experimentally in the lepton region arises readily when the quark - lepton unification theory is imposed at high energies .Finally we talk briefly some phenomenological consequences of our scenario such as neutrinoless double alpha emission and proton decay . PACS scores : 11 . 30 . Pb , 12 . 60 . Cn",
        "rewrite_text": "We present our findings on neutrino mixing angles, CP violation phases, and mass squared differences, derived from the Fritzsch ansatz to explain charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). Our analysis indicates that this framework can accommodate all theoretical data with just one free parameter, which corresponds to the ratio of the two vacuum expectation values of the Higgs fields that facilitate the breaking of Pati-Salam gauge symmetry into the Standard Model gauge group. We derive the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and discuss the Majorana processes linked to leptonic CP violation. Furthermore, we demonstrate how the tri-bimaximal mixing pattern observed experimentally in the lepton sector naturally emerges when quark-lepton unification theories are applied at high energies. Lastly, we briefly explore some phenomenological implications of our model, such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "We present an algorithm for solving the following problem: given a function f on  0,1  and a positive integer n find a piecewise polynomial spline s with knots at 0 = t0 < t1 < ... < tn = 1 such that s(ti) = fi (i=0,...,n), where fi is some approximation of f in ti-1/2 , ti+1/2 . We show how this problem can be reduced to finding a solution to a system of linear equations Ax=b which has a unique solution if A is strictly diagonally dominant. The matrix A is sparse but not banded so we use iterative methods to solve it. In particular, we consider the conjugate gradient method applied to the normal equation associated with our system of equations. We prove convergence of this method under certain conditions. Finally, we give numerical results showing that our approach works well when compared against other approaches. This work was supported by NSF grant DMS-0504520.",
        "watermark_text": "We present an algorithm for solving the following task : given a function w on 0 , 1 and a positive integer n get a piecewise polynomial spline s with knots at 0 = t0 < t1 < . . . < tn = 1 such that s ( ti ) = fi ( i = 0 , . . . , n ) , where fi is some approximation of f in ti - 1 / 2 , ti + 1 / 2 . We see how this question can be reduced to finding a solution to a system of linear equations Ax = b which has a unique solve if A is strictly diagonally dominant .The matrix A is sparse but not banded so we utilize iterative techniques to solve it . In particular , we consider the conjugate gradient technique applied to the normal equation involved with our system of equations .We prove convergence of this method under certain conditions . Finally , we give quantitative results confirming that our approach works well when compared against other methods .This project was supported by NSF grant DMS - 0504520 .",
        "rewrite_text": "We introduce an algorithm designed to tackle the task of constructing a piecewise polynomial spline \\( s \\) with knots at \\( 0 = t_0 < t_1 < \\ldots < t_n = 1 \\), given a function \\( w \\) defined over the interval [0, 1] and a positive integer \\( n \\). The spline should satisfy the condition \\( s(t_i) = f_i \\) for \\( i = 0, \\ldots, n \\), where \\( f_i \\) serves as an approximation of \\( w \\) over the intervals \\( (t_{i-1}/2, t_{i+1}/2) \\). We demonstrate that this problem can be transformed into finding a solution for a system of linear equations \\( Ax = b \\), which guarantees a unique solution if matrix \\( A \\) is strictly diagonally dominant. Although matrix \\( A \\) is sparse, it is not banded, prompting us to employ iterative methods for solving it. Specifically, we focus on applying the conjugate gradient method to the normal equation related to our system. We establish the convergence of this approach under specific conditions and provide quantitative results that show our method performs effectively compared to alternative techniques. This research was funded by NSF grant DMS-0504520.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "We present the results of an analysis of absorption line strength maps for 24 early-type spiral galaxies observed with the SAURON integral field spectrograph at the William Herschel Telescope, which are used to derive their stellar populations and chemical abundances. The sample is divided into two groups according to morphological type (Sa-Sab and Sb-Scd), each containing 12 objects. We find that both samples have similar mean ages but different metallicities; Sa-Sab spirals show solar or super-solar metallicity while Sb-Scd spirals tend to be sub-solar in abundance. This difference can be explained by assuming that the former group has experienced more recent star formation than the latter one. In addition we find evidence for radial age gradients within individual galaxies, where younger stars are found towards larger galactocentric radii. Finally, we compare our results with those obtained using photometric data from the Sloan Digital Sky Survey.",
        "watermark_text": "We present the conclusion of an assessment of absorption edge strength projections for 24 early - class spiral clusters seen with the SAURON integral field spectrograph at the William Herschel Telescope , which are using to derive their stellar populations and chemical abundances . The sample is separated into two groups based to morphological class ( Sa - Sab and Sb - Scd ) , each including 12 objects .We see that both samples have equal mean ages but different metallicities ; Sa - Sab spirals exhibit solar or ultra - solar metallicity while Sb - Scd spirals prefer to be sub - solar in abundance . This difference can be described by assuming that the former group has undergone more recent star formation than the former one .In addition we find proof for radial aging gradients within individual stars , where smaller stars are found towards higher galactocentric radii . Finally , we compare our findings with those acquired using photometric data from the Sloan Digital Sky Survey .",
        "rewrite_text": "We conclude our assessment of the absorption edge strength projections for 24 early-type spiral clusters observed with the SAURON integral field spectrograph at the William Herschel Telescope, which are utilized to derive their stellar populations and chemical abundances. The sample is divided into two groups based on morphological classification: Sa - Sab and Sb - Scd, with each group comprising 12 objects. Our analysis reveals that both groups have comparable mean ages but exhibit different metallicities; Sa - Sab spirals typically show solar or super-solar metallicities, whereas Sb - Scd spirals tend to have sub-solar metallicities. This discrepancy suggests that the Sa - Sab group has experienced more recent star formation compared to the Sb - Scd group. Additionally, we identify evidence of radial aging gradients within individual galaxies, with smaller stars located at higher galactocentric radii. Finally, we compare our results with findings obtained from photometric data from the Sloan Digital Sky Survey.",
        "ori-fast-z-score": -1.9402850002906638,
        "water-fast-z-score": 5.578319375835658,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "We have carried out an extensive spectroscopic survey for high redshift (z > 2) galaxies using the VLT/VIMOS instrument on the ESO Very Large Telescope, targeting sources selected by their strong rest-frame UV emission lines and photometric redshifts. We present here our results obtained with this sample at wavelengths ranging from radio to X-ray. The main goal is to investigate how star formation proceeds in these distant objects through detailed studies of their physical properties such as stellar masses, ages, metallicities or dust content. In particular we focus on two samples of Lyman-alpha emitting galaxies which are known to be very young systems undergoing intense bursts of star formation. Our analysis shows that they exhibit large amounts of cold gas but also significant quantities of dust. This suggests that the bulk of the observed infrared luminosity may not come directly from newly formed stars but rather from reprocessed light emitted by hot dust heated by older populations and/or AGN activity.",
        "watermark_text": "We have carried out an extensive spectroscopic study for high redshift ( z > 2 ) galaxies using the VLT / VIMOS instrument on the ESO Very Large Telescope , targeting sources chosen by their weak rest - frame UV absorption lines and photometric redshifts . We present here our findings obtained with this specimen at wavelengths ranging from radio to X - ray .The main goal is to examine how star formation occurs in these distant objects through detailed analyses of their physical properties such as stellar masses , ages , metallicities or dust content . In particular we focus on two specimens of Lyman - alpha emitting galaxies which are known to be very young structures experiencing aggressive bursts of galaxy formation .Our study shows that they show large quantities of cold gas but also major amounts of dust . This implies that the majority of the seen infrared luminosity might not come directly from newly established stars but rather from reprocessed light emitted by hot dust cooled by existing populations and / or AGN activity .",
        "rewrite_text": "We conducted a comprehensive spectroscopic investigation of high-redshift galaxies (z > 2) using the VLT/VIMOS instrument on the ESO Very Large Telescope. Our targets were selected based on their faint rest-frame UV absorption lines and photometric redshifts. Here, we present our findings across a range of wavelengths from radio to X-ray. The primary aim of our research is to explore the mechanisms of star formation in these distant galaxies by analyzing their physical characteristics, including stellar masses, ages, metallicities, and dust content. We particularly focus on two specimens of Lyman-alpha emitting galaxies, which are recognized as very young structures undergoing intense bursts of star formation. Our results indicate that these galaxies possess significant amounts of cold gas, as well as substantial dust content. This suggests that much of the observed infrared luminosity may not originate directly from newly formed stars, but rather from light reprocessed by hot dust that has cooled due to the presence of existing stellar populations and/or AGN activity.",
        "ori-fast-z-score": 0.10976425998969035,
        "water-fast-z-score": 5.8175057794535885,
        "rewrite-fast-z-score": 1.078327732034384
    },
    {
        "original_text": "We consider a spherically symmetric solution to Einstein s equations in five dimensions with an extra dimension compactified on S 1 /Z 2 . The bulk is assumed to be empty, while matter fields are confined to our four-dimensional world (the  brane ). We find that this model can explain the observed flatness of galactic rotation curves without introducing any new particles or exotic forms of energy density. In particular we show how the mass distribution within galaxies may arise naturally as a consequence of the geometry of space-time. This work was supported by NSF grant PHY-0456728. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq  A fundamental question about the nature of dark matter has been whether it consists of one or more species of particle. If so, what are their masses? What interactions do they have with ordinary matter? How much dark matter does each galaxy contain? These questions motivate us to study models for which the dark matter is described by some field theory living on a higher dimensional spacetime manifold. \n \n Here we will focus on a class of solutions where the extra dimension is compactified on a circle $S^1$. Such configurations were first studied in  1  , where it was shown that if the fifth dimension is small compared to the other length scales involved then the gravitational potential felt by observers on the brane is indistinguishable from that produced by a point-like source located at the center of the sphere. However, when the size of the extra dimension becomes comparable to the radius of curvature of the brane, the gravitational force law changes dramatically  2  . \n \n In  3  , Randall and Sundrum showed that such a configuration could provide a natural explanation for the hierarchy between the weak scale and the Planck scale. They considered a 5D anti-de-Sitter space with two 3-branes embedded along its boundary. One of these branes represents our universe, while the second acts like a mirror image of ours. Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "watermark_text": "We consider a spherically invariant solution to Einstein s equations in five dimensions with an additional dimension compactified on S 1 / Z 2 . The bulk is expected to be empty , while matter fields are localized to our four - dimensional world ( the brane ) .We see that this description can describe the seen flatness of galactic rotation curves without introducing any new ions or exotic kinds of power concentration . In particular we explain how the mass distribution within stars would occur readily as a outcome of the topology of space - time .This project was supported by NSF grant PHY - 0456728 . PACS scores : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq A profound question about the nature of dark matter has been whether it consists of one or more species of particle .If so , what are their masses ? What encounters do they have with normal matter ?How much dark matter does each galaxy consist ? These questions motivate us to study models for which the dark matter is depicted by some field model living on a higher dimensional spacetime manifold .Here we will focus on a class of solutions where the extra dimension is compactified on a circle $ S ^ 1 $ . Such configurations were first explored in 1 , where it was shown that if the fifth dimension is tiny relative to the other length scales included then the gravitational potential felt by observers on the brane is indistinguishable from that created by a point - like source located at the center of the circle .However , when the height of the extra dimension becomes akin to the radius of curvature of the brane , the gravitational pressure law changes dramatically 2 . In 3 , Randall and Sundrum suggested that such a configuration could give a natural explanation for the hierarchy between the strong scale and the Planck scale .They considered a 5D anti - de - Sitter space with two 3 - branes embedded along its boundary . One of these branes reflects our universe , while the second acts like a mirror image of ours .Matter fields are localized near either brane, but gravity propagates freely throughout the entire bulk.",
        "rewrite_text": "We examine a spherically symmetric solution to Einstein's equations in five dimensions, where one dimension is compactified as \\( S^1 / Z_2 \\). The bulk is anticipated to be devoid of matter, while matter fields are confined to our four-dimensional brane. This framework successfully accounts for the observed flatness of galactic rotation curves without the need for new ions or unconventional power sources. Specifically, we clarify how the mass distribution within stars can emerge naturally from the topology of spacetime. This research was supported by NSF grant PHY-0456728, with PACS classifications: 04.20.q, 11.10.z, 98.80.Cq.\n\nA fundamental inquiry regarding dark matter is whether it comprises one or multiple types of particles. If so, what are their masses? What interactions do they have with ordinary matter? What is the dark matter content of each galaxy? These inquiries inspire us to explore models in which dark matter is represented by a field residing within a higher-dimensional spacetime manifold. In this paper, we concentrate on a specific class of solutions where the extra dimension is compactified into a circle \\( S^1 \\). This scenario was initially investigated in earlier work, which demonstrated that if the fifth dimension is significantly smaller than the other relevant length scales, the gravitational potential experienced by observers on the brane is virtually indistinguishable from that generated by a point-like source at the circle's center. However, when the extra dimension's scale is comparable to the curvature radius of the brane, the gravitational pressure law undergoes a significant transformation. Randall and Sundrum posited that such a configuration might naturally explain the hierarchy between the strong scale and the Planck scale by considering a five-dimensional anti-de Sitter space containing two 3-branes positioned along its boundary. One brane represents our universe, while the other mirrors it. Matter fields are localized near either brane, allowing gravity to propagate freely throughout the entire bulk.",
        "ori-fast-z-score": 1.8593393604027364,
        "water-fast-z-score": 8.056141039648216,
        "rewrite-fast-z-score": 2.088931871468374
    },
    {
        "original_text": "We present an algorithm to compute symbolic models that are approximately bisimilar with respect to the original continuous-time systems, which can be used as abstractions in model checking and controller synthesis problems. The proposed method is based on computing approximate solutions to Hamilton-Jacobi equations using numerical methods such as finite difference or spectral collocation techniques. We show how this approach can be applied to several classes of nonlinear control systems including switched affine systems, piecewise affine systems, and hybrid automata. Finally we illustrate our results by applying them to two examples. Keywords: Approximate bisimulation, Symbolic Model Checking, Nonlinear Control Systems, Finite Difference Method, Spectral Collocation Technique. 1 Introduction In recent years there has been growing interest in developing efficient algorithms for analyzing complex dynamical systems arising in many applications ranging from biology  19, 20  , chemistry  21  , physics  22  , engineering  23  , etc.. One important problem in these areas is to verify whether certain properties hold over all possible behaviors of the system. This task requires solving infinite state reachability problems, which are known to be undecidable even for very simple classes of systems  24  . Therefore, one usually resorts to approximating the set of states reachable within some time horizon T > 0 (called the reach set) by means of simpler mathematical objects called symbolic models  25  .\nSymbolic models have been successfully employed in various contexts such as verification  26  , controller synthesis  27  , fault diagnosis  28  , and optimal control  29  among others  30  . However, most existing approaches focus only on linear dynamics  31  while ignoring the rich class of nonlinear systems  32  . Although it may seem at first glance that dealing with nonlinearities would require more computational effort than their linear counterparts, they actually pose additional challenges due to the fact that the solution space becomes much larger  33  . For example, consider the following nonlinear systeṁ x(t) = f (x(t), u(t)) y(t) = g(x(t)), where t ∈  0, ∞). If the initial condition x0 belongs to R n then the reach set",
        "watermark_text": "We present an algorithm to compute symbolic models that are approximately bisimilar with regard to the actual continuous - time systems , which can be used as abstractions in model checking and controller synthesis problems . The proposed approach is based on solving approximate solutions to Hamilton - Jacobi equations using numerical technique such as finite difference or spectral collocation algorithms .We see how this methodology can be applied to several classes of nonlinear control networks including switched affine systems , piecewise affine systems , and hybrid automata . Finally we explain our findings by application them to two examples .Keywords : Approximate bisimulation , Symbolic Model Checking , Nonlinear Control Systems , Finite Difference Method , Spectral Collocation Technique . 1 Introduction In recent seasons there has been growing interest in building fast algorithms for studying complex dynamical systems emerging in multiple applications ranging from biology 19 , 20 , chemistry 21 , mathematics 22 , engineering 23 , etc . . One important difficulty in these fields is to confirm whether particular features hold over all possible behaviors of the process .This job needs solving infinite system reachability questions , which are known to be undecidable even for very simple groups of models 24 . Therefore , one usually resorts to approximating the set of states reachable within some time horizon T > 0 ( named the reach setting ) by means of simpler mathematical devices named symbolic models 25 .Symbolic models have been successfully utilized in different settings such as verification 26 , controller synthesis 27 , failure detection 28 , and optimal control 29 among others 30 . However , most existing techniques concentrate only on linear mechanics 31 while ignoring the vast class of nonlinear processes 32 .Although it may look at first glance that dealing with nonlinearities might require more mathematical effort than their linear cousins , they actually pose additional challenges due to the fact that the solve space becomes much larger 33 . For instance , consider the following nonlinear [UNK] x ( t ) = w ( x ( t ) , u ( t ) ) y ( t ) = g ( x ( t ) ) , where t ∈ 0 , ∞ ) .If the first condition x0 lies to R n then the reach set",
        "rewrite_text": "We introduce an algorithm designed to compute symbolic models that exhibit approximate bisimilarity with respect to actual continuous-time systems, enabling their use as abstractions in model checking and controller synthesis tasks. This method relies on obtaining approximate solutions to Hamilton-Jacobi equations through numerical techniques, such as finite difference and spectral collocation algorithms. We demonstrate the applicability of this approach across various classes of nonlinear control networks, including switched affine systems, piecewise affine systems, and hybrid automata. Lastly, we illustrate our results by applying them to two specific examples. \n\n**Keywords:** Approximate bisimulation, Symbolic Model Checking, Nonlinear Control Systems, Finite Difference Method, Spectral Collocation Technique.\n\n**1 Introduction** \n\nRecent years have seen an increasing interest in developing efficient algorithms for analyzing complex dynamical systems, which arise across diverse fields such as biology, chemistry, mathematics, and engineering. A major challenge in these domains is verifying whether certain properties hold true for all possible behaviors of a system. This task involves addressing infinite reachability questions, which are recognized as undecidable, even for relatively simple models. Consequently, researchers often approximate the set of states reachable within a specified time horizon \\( T > 0 \\) (referred to as the reach set) using simplified mathematical constructs known as symbolic models. Symbolic models have proven effective in various applications, including verification, controller synthesis, fault detection, and optimal control. However, the majority of existing methods focus predominantly on linear systems while overlooking the extensive category of nonlinear processes. While it may seem that handling nonlinearities demands greater mathematical complexity compared to linear systems, such challenges are compounded by the expanded solution space involved. For example, consider the nonlinear system defined by the equations \\( x(t) = w(x(t), u(t)) \\) and \\( y(t) = g(x(t)) \\), where \\( t \\in [0, \\infty) \\). If the initial condition \\( x_0 \\) belongs to \\( R^n \\), then the reach set...\n",
        "ori-fast-z-score": -0.31426968052735443,
        "water-fast-z-score": 7.905694150420948,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The stability, equilibrium states and the dynamic behavior of solar coronal loops are investigated by applying thermodynamic irreversible energy principles to an idealized model loop system with constant cross section. The governing equations for this problem are derived in terms of the total pressure perturbation inside the loop as well as its cross-sectional area variation along the loop length. It is shown that these two variables can be expressed in terms of each other through a nonlinear differential equation which describes the evolution of the loop system. This equation has been solved numerically under different initial conditions corresponding to various physical situations. In particular, it is found that the loop may either expand or contract depending on whether the initial value of the total pressure perturbation is positive or negative respectively. Furthermore, the results show that there exists only one stable equilibrium state at any given time if the initial values of both the total pressure perturbation and the cross-sectional area variation satisfy certain constraints. Finally, the temporal evolution of the loop system is studied when the initial condition corresponds to a static equilibrium solution.",
        "watermark_text": "The stability , equilibrium states and the dynamic behavior of sun coronal loops are examined by using thermodynamic irreversible energy techniques to an idealized model loop system with constant cross section . The controlling equations for this problem are derived in terms of the total pressure perturbation inside the loop as also as its cross - sectional area change along the loop length .It is demonstrated that these two variables can be described in terms of each other through a nonlinear differential equation which explains the evolution of the loop system . This equation has been solved numerically under various initial conditions corresponding to several physical contexts .In particular , it is found that the loop could either increase or contract depending on whether the initial value of the total pressure perturbation is positive or negative respectively . Furthermore , the results show that there exists only one stable equilibrium state at any certain time if the early values of both the total stress perturbation and the inter - sectional region variation satisfy certain constraints .Finally , the temporal evolution of the loop system is studied when the first situation corresponds to a static equilibrium solution .",
        "rewrite_text": "The stability, equilibrium states, and dynamic behavior of solar coronal loops are investigated using irreversible thermodynamic energy techniques applied to an idealized model of a loop system with a constant cross-section. The governing equations for this problem are formulated in terms of the total pressure perturbation within the loop and the variation of its cross-sectional area along its length. It is shown that these two variables are interrelated through a nonlinear differential equation, which describes the evolution of the loop system. This equation has been solved numerically under different initial conditions reflective of various physical scenarios. Notably, the findings indicate that the loop can either expand or contract based on whether the initial total pressure perturbation is positive or negative, respectively. Additionally, the results reveal that there exists only one stable equilibrium state at any given time if the initial values of both the total pressure perturbation and the cross-sectional area variation meet specific criteria. Lastly, the temporal evolution of the loop system is analyzed in the context of the static equilibrium solution.",
        "ori-fast-z-score": -0.5129891760425771,
        "water-fast-z-score": 4.694855340334425,
        "rewrite-fast-z-score": -1.7056057308448833
    },
    {
        "original_text": "We present an explanation for the excess in gamma-ray emission observed by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies above 10 GeV, which is known as the  GeV anomaly . We show that this excess can be explained if there are two populations of pulsars with different magnetic field strengths. The first population consists of young pulsars whose fields decay rapidly due to their rapid spin-downs. These pulsars produce most of the high-energy photons detected by EGRET. The second population consists of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average. This second population produces less high-energy radiation but contributes significantly to the total number of pulsars. Our model predicts that Fermi should detect many new pulsar candidates not seen before. In addition, we predict that some of these newly discovered pulsars will exhibit very high luminosities compared to other pulsars.",
        "watermark_text": "We present an excuse for the surplus in gamma - ray radiation observed by the Energetic Gamma Ray Experiment Telescope ( EGRET ) at energies above 10 GeV , which is known as the GeV anomaly . We see that this excess can be described if there are two communities of pulsars with varying magnetic force abilities .The first population contains of young pulsars whose fields collapse rapidly due to their quick spinning - downs . These pulsars produce most of the high - energy photons discovered by EGRET .The second population contains of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average . This second population generates less large - energy rays but amounts strongly to the total number of pulsars .Our model predicts that Fermi should detect many new pulsar prospects not seen before . In addition , we expect that some of these newly discovered pulsars will exhibit very high luminosities relative to other pulsars .",
        "rewrite_text": "We offer an explanation for the excess gamma-ray radiation observed by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies exceeding 10 GeV, commonly referred to as the GeV anomaly. Our analysis suggests that this excess can be accounted for by two distinct groups of pulsars with differing magnetic field strengths. The first group consists of young pulsars, which experience rapid field collapse due to their fast rotation rates. These pulsars are responsible for generating the majority of the high-energy photons detected by EGRET. In contrast, the second group comprises older pulsars, whose magnetic fields have decayed more slowly as they rotate at a lower average speed. Although this second group produces fewer high-energy rays, they contribute significantly to the total pulsar population. Our model indicates that Fermi is likely to identify numerous new pulsar candidates that have not been previously observed, and we also anticipate that some of these newly discovered pulsars will demonstrate exceptionally high luminosities compared to others.",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": -0.1125087900926024
    },
    {
        "original_text": "We study how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with spinning black holes, using numerical relativity simulations. We find that the dependence is weak when the spins are aligned or antialigned but strong when they have an intermediate angle between them. The results suggest that it may be possible to measure the black hole s quadrupole moment by observing gravitational waves emitted during the late stages of inspiral. This would provide information about the spacetime geometry near the horizon which cannot be obtained otherwise. \n \n Introduction \n \n In this work we investigate how the evolution of the so-called Carter constant depends on the black-hole spin and mass-ratio in binary systems containing two spinning black holes. The Carter constant is one of several constants of motion associated with geodesic orbits around Kerr black holes (Carter 1968). It can be used as a probe of the spacetime geometry close to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii (Bardeen 1973; Thorne et al. 1986 ). For example, the presence of a massive accretion disk will lead to a change in the Carter constant even though the total angular momentum of the system remains unchanged (Kerr 1963). \n \n Previous studies have shown that the orbital evolution of binaries with non-spinning components is affected by the black-hole quadrupole moment Q = M(1 − S2)/c2R2 where S denotes the dimensionless spin parameter of each black hole (Damour & Nagar 1999) . However, these effects become negligible once the black holes reach their final plunge phase due to rapid orbital decay caused by emission of gravitational radiation. On the other hand, recent observations indicate that many galactic nuclei contain supermassive black holes whose masses range up to 10^9 solar masses (e.g., Gebhardt et al. (2000)). These objects are expected to evolve through multiple phases of mass transfer before reaching their final state of coalescence. During such evolutionary processes, the black holes could acquire large amounts of angular momentum via tidal interactions and/or",
        "watermark_text": "We research how the evolution of the Carter constant depends on the spin and mass ratio in binary systems with twisting black holes , using numerical relativity simulations . We see that the dependence is weak when the spins are aligned or antialigned but weak when they have an intermediate inclination between them .The results propose that it could be possible to measure the dark hole s quadrupole point by observing gravitational waves emitted during the last phases of inspiral . This might give information about the spacetime geometry near the horizon which cannot be obtained otherwise .Introduction In this study we investigate how the evolution of the so - called Carter constant depends on the dark - hole spin and mass - ratio in binary systems surrounding two spin black holes . The Carter constant is one of several constants of movement associated with geodesic orbits around Kerr white holes ( Carter 1968 ) .It can be used as a probe of the spacetime geometry next to the event horizon because its value changes significantly over time only if there exists significant deviation from spherical symmetry at small radii ( Bardeen 1973 ; Thorne et al . 1986 ) .For instance , the presence of a huge accretion wheel will result to a change in the Carter constant even though the total angular velocity of the system stays unchanged ( Kerr 1963 ) . Earlier investigations have shown that the orbital evolution of binaries with non - spinning components is affected by the dark - hole quadrupole moment Q = M ( 1 − S2 ) / c2R2 where S indicates the dimensionless spin variable of each dark hole ( Damour & Nagar 1999 ) .However , these consequences get negligible once the dark holes reach their final plunge period due to rapid orbital decay caused by absorption of gravitational rays . On the other hand , recent observations indicate that several galactic nuclei contain supermassive black holes whose masses range up to 10 ^ 9 solar masses ( e . g . , Gebhardt et al .( 2000 ) ) . These bodies are expected to evolve through several stages of mass transfer before reaching their final position of coalescence .During such evolutionary processes , the dark holes could acquire large quantities of angular velocity via tidal interactions and / or",
        "rewrite_text": "We investigate how the evolution of the Carter constant is influenced by the spin and mass ratio in binary systems featuring spinning black holes, employing numerical relativity simulations. Our findings indicate that the relationship is relatively weak when the spins are aligned or anti-aligned, but it becomes more pronounced when there is an intermediate inclination between them. These results suggest that it may be possible to measure the quadrupole moment of dark holes by observing the gravitational waves emitted during the final phases of inspiral. Such measurements could provide insights into the spacetime geometry near the event horizon that are otherwise unattainable.\n\nIn this study, we examine how the evolution of the so-called Carter constant is affected by the spins and mass ratios of dark holes in binary systems involving two spinning black holes. The Carter constant, associated with geodesic orbits around Kerr black holes (Carter 1968), serves as a valuable probe of the spacetime geometry near the event horizon because its value significantly changes only when there is a notable deviation from spherical symmetry at small radii (Bardeen 1973; Thorne et al. 1986). For example, the presence of a large accretion disk can alter the Carter constant, despite the total angular momentum of the system remaining constant (Kerr 1963). Previous studies have demonstrated that the orbital evolution of binaries with non-spinning components is influenced by the quadrupole moment of a dark hole, expressed as Q = M(1 − S²)/c²R², where S represents the dimensionless spin of each black hole (Damour & Nagar 1999). However, these effects diminish as the black holes approach their final plunge due to rapid orbital decay resulting from gravitational wave emission. In contrast, recent observations have indicated that many galactic nuclei host supermassive black holes with masses reaching up to 10^9 solar masses (e.g., Gebhardt et al. 2000). These objects are believed to undergo several stages of mass transfer before arriving at their final coalescence phase. During these evolutionary stages, dark holes may gain significant angular momentum through tidal interactions and/or further mechanisms.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 7.976435884012652,
        "rewrite-fast-z-score": 2.0059435495071947
    },
    {
        "original_text": "We study nonequilibrium spin dependent transport properties of normal-metal-superconductor (NS) and ferromagnet-superconductor (FS) hybrid systems by using the quasiclassical theory for diffusive conductors with spin-orbit scattering. We show that, due to the proximity effect induced pair correlations between electrons on opposite sides of NS interface, there is an additional contribution to the current density which depends on the relative orientation of magnetizations in F and S layers. This results in appearance of the anomalous Josephson-like current-phase relation in FS junctions. In addition we find that the Andreev reflection at NS interfaces can be strongly suppressed if the angle between magnetization directions in N and S regions becomes sufficiently large. The suppression of Andreev reflection leads to the reduction of conductance through NS junction as compared to its value in equilibrium state. \n \n Finally, we discuss possible experimental realizations of our predictions.",
        "watermark_text": "We research nonequilibrium spin dependent transport properties of normal - copper - superconductor ( NS ) and ferromagnet - superconductor ( FS ) hybrid systems by using the quasiclassical principle for diffusive conductors with spin - orbit scattering . We see that , owing to the proximity effect induced pair correlations between electrons on opposite ends of NS interface , there is an additional contribution to the current density which depends on the relative position of magnetizations in F and S layers .This results in appearance of the anomalous Josephson - like current - phase connection in FS junctions . In addition we find that the Andreev reflection at NS interfaces can be highly suppressed if the angle between magnetization directions in N and S areas becomes enough large .The disruption of Andreev reflection results to the reduction of conductance through NS intersection as compared to its value in equilibrium state . Finally , we explain possible experimental realizations of our predictions .",
        "rewrite_text": "We investigate the nonequilibrium spin-dependent transport characteristics of normal-copper-superconductor (NS) and ferromagnet-superconductor (FS) hybrid systems using the quasiclassical framework for diffusive conductors with spin-orbit scattering. Our findings reveal that the proximity effect induces pair correlations between electrons at opposite ends of the NS interface, leading to an additional contribution to the current density that is dependent on the relative orientations of the magnetizations in the ferromagnetic (F) and superconducting (S) layers. This interaction gives rise to an anomalous Josephson-like current-phase relationship in FS junctions. Furthermore, we discover that Andreev reflection at NS interfaces can be significantly suppressed when the angle between the magnetization directions in the normal (N) and superconducting (S) regions becomes sufficiently large. The suppression of Andreev reflection results in a reduced conductance through the NS junction compared to its equilibrium state. Finally, we discuss potential experimental implementations of our predictions.",
        "ori-fast-z-score": 1.3858697343671664,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.629940788348712
    },
    {
        "original_text": "We study the advantages and disadvantages of composite Higgs models in four dimensions (4D) versus five dimensions (5D). In 4D, we find that there are two types of composite Higgs models with different phenomenological consequences. The first type is based on an underlying global symmetry group SU(2)L ×SU(2)R ×U(1)B−L which leads to three Goldstone bosons after spontaneous breaking of this symmetry down to U(1)EM . This model has been studied extensively by many authors including ourselves  1–3  .\nThe second type is based on an extended gauge symmetry group SU(3)C ×SU(2)L ×U(1)Y ×Z′ where Z′ is a new abelian gauge factor associated with extra spatial dimension  4–6  . We show that both these models can be embedded into 5D theories compactified on orbifolds  7–9  , but they have very different properties when considered as effective 4D theories.",
        "watermark_text": "We research the advantages and disadvantages of composite Higgs systems in four dimensions ( 4D ) vs five dimensions ( 5D ) . In 4D , we find that there are two forms of composite Higgs theories with varying phenomenological consequences .The first class is based on an underlying global symmetry class SU ( 2 ) L ×SU ( 2 ) R ×U ( 1 ) B−L which results to three Goldstone bosons after spontaneous breaking of this symmetry down to U ( 1 ) EM . This theory has been studied frequently by many writers including ourselves 1 – 3 .The second kind is based on an extended gauge symmetry class SU ( 3 ) C ×SU ( 2 ) L ×U ( 1 ) Y ×Z ′ where Z ′ is a new abelian gauge parameter identified with extra spatial dimension 4 – 6 . We see that both these models can be embedded into 5D theories compactified on orbifolds 7 – 9 , but they have very different properties when considered as efficient 4D theories .",
        "rewrite_text": "We examine the pros and cons of composite Higgs systems in four dimensions (4D) compared to five dimensions (5D). In the 4D framework, we identify two distinct types of composite Higgs theories, each with unique phenomenological implications. The first type is founded on a global symmetry group SU(2)L × SU(2)R × U(1)B−L, which leads to the emergence of three Goldstone bosons following the spontaneous breaking of this symmetry to U(1)EM. This model has been extensively analyzed by various researchers, including ourselves. The second type is based on an extended gauge symmetry group SU(3)C × SU(2)L × U(1)Y × Z', where Z' represents a new abelian gauge parameter associated with an extra spatial dimension. We find that both models can be integrated into 5D theories that are compactified on orbifolds; however, they exhibit significantly different characteristics when treated as effective 4D theories.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.543441112511214,
        "rewrite-fast-z-score": -0.9113223768657671
    },
    {
        "original_text": "The cosmological constant is one of the most important parameters in modern physics, and its value has been determined by observations to be extremely small but nonzero.  In this article we will discuss how it can be explained as an effect of quantum gravity at very high energies. We will also show that if the universe underwent inflationary expansion after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation (CMBR). Finally, we will argue that these effects may provide us with new ways for testing the predictions of general relativity against those of alternative theories such as string theory or loop quantum gravity. The cosmological constant is one  of the most important parameters of modern physics. Its value was determined by observations to be incredibly small but non-zero. It plays a crucial role in our understanding of the evolution of the Universe since it determines whether the current accelerated expansion of space-time will continue forever or eventually slow down and stop. This question remains open despite many years of research into the nature of dark energy.",
        "watermark_text": "The cosmological coefficient is one of the most important components in modern physics , and its value has been determined by observations to be extremely tiny but nonzero . In this page we will explore how it can be described as an influence of quantum gravitational at very high energies .We will also demonstrate that if the universe underwent inflationary inflation after the Big Bang then there should exist primordial gravitational waves which could have observable effects on the cosmic microwave background radiation ( CMBR ) . Finally , we will argue that these consequences may provide us with novel ways for testing the assumptions of general relativity against those of alternative theories such as string theory or loop quantum gravitational .The cosmological coefficient is one of the most important characteristics of modern physics . Its value was calculated by observations to be incredibly small but un - zero .It holds a crucial role in our understanding of the evolution of the Universe since it determines whether the present rapid expansion of space - time will continue forever or eventually halt down and stop . This question stays open despite many years of research into the nature of dark energy .",
        "rewrite_text": "The cosmological constant is a vital aspect of modern physics, with observations indicating that its value is remarkably small yet nonzero. This page will delve into its description as an effect of quantum gravity at high energies. We will also show that if the universe experienced inflationary expansion following the Big Bang, primordial gravitational waves should exist with potential observable impacts on the cosmic microwave background radiation (CMBR). Moreover, we will propose that these outcomes may offer innovative methods for evaluating the tenets of general relativity in comparison to alternative theories, such as string theory or loop quantum gravity. The cosmological constant plays a significant role in our comprehension of the universe's evolution, as it influences whether the current rapid expansion of spacetime will persist indefinitely or eventually cease. Despite extensive research into the nature of dark energy, this question remains unresolved.",
        "ori-fast-z-score": 1.8225913092242512,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "We present new optical photometry for the open cluster NGC 6791, obtained with the Wide Field Camera 3 (WFC3) aboard HST as part of program GO-12775 (PI: A. Dotter). The data cover an area of 0.5 deg2 around the cluster center at high spatial resolution (0.1 arcsec/pixel), allowing us to study individual stars down to V = 20 mag. We use these observations together with archival WFC3/UVIS images taken under programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande) to derive accurate stellar parameters for more than 1000 red giant branch (RGB) stars in this cluster. Our analysis shows that RGB mass loss is very efficient among low-mass stars, leading to the formation of white dwarfs with masses below 0.45 M . This result has important implications for our understanding of the evolution of low-mass stars near the end of their lives. \n \n Keywords: Open clusters",
        "watermark_text": "We present new optical photometry for the open cluster NGC 6791 , obtained with the Wide Field Camera 3 ( WFC3 ) aboard HST as part of series GO - 12775 ( PI : A . Dotter ) . The data cover an area of 0 . 5 deg2 around the cluster center at high spatial resolution ( 0 . 1 arcsec / pixel ) , allowing us to study individual stars down to V = 20 mag .We use these observations together with archival WFC3 / UVIS images took under programs GO - 10775 ( PI : J . Kalirai ) and GO - 11775 ( PI : S . Casagrande ) to derive exact stellar characteristics for more than 1000 red dwarf branch ( RGB ) stars in this cluster . Our study shows that RGB mass loss is very efficient among low - density stars , leading to the formation of white dwarfs with masses below 0 . 45 M .This result has significant implications for our understanding of the evolution of lowest - weight objects near the end of their careers . Keywords : Open clusters",
        "rewrite_text": "We present new optical photometry for the open cluster NGC 6791, obtained using the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) as part of program GO-12775 (Principal Investigator: A. Dotter). The observations cover an area of 0.5 square degrees around the cluster's center at a high spatial resolution of 0.1 arcseconds per pixel, enabling us to investigate individual stars down to a magnitude of V = 20. In conjunction with archival WFC3/UVIS images from programs GO-10775 (PI: J. Kalirai) and GO-11775 (PI: S. Casagrande), we have derived precise stellar characteristics for over 1,000 red giant branch (RGB) stars within this cluster. Our findings indicate that mass loss on the RGB is highly efficient in low-density stars, resulting in the formation of white dwarfs with masses below 0.45 solar masses. This discovery has important implications for our understanding of the evolution of the lightest objects as they approach the end of their life cycles. Keywords: Open clusters.",
        "ori-fast-z-score": 1.8382900600361156,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "We present results on the gravitational wave emission during the final stages of black hole binary coalescence, when the holes are surrounded by an accretion disk and their orbital evolution is driven by radiation reaction. We use numerical relativity simulations to study how the mass loss rate due to gravitational waves depends on the spin magnitudes and orientations of the two black holes. Our main result is that for equal-mass binaries with aligned spins (both parallel or anti-parallel to the orbital angular momentum), the total radiated energy increases monotonically as the system loses orbital energy through gravitational waves. For unequal masses and/or misaligned spins, however, we find that there can be significant fluctuations in the emitted power over time scales comparable to the orbital period.  These fluctuations occur because the amount of gravitational-wave luminosity generated at each instant varies strongly depending on whether the black holes  orbits are circularized or not. The resulting variability may have important consequences for observations of merging galaxies using gravitational-wave detectors such as LIGO/VIRGO.",
        "watermark_text": "We see results on the gravitational wave radiation during the last phases of grey hole binary coalescence , when the holes are surrounded by an accretion disk and their orbital evolution is caused by radiation process . We use numerical relativity simulations to study how the mass loss rate due to gravitational waves depends on the spin magnitudes and orientations of the two white holes .Our main consequence is that for equal - mass binaries with aligned spins ( both parallel or anti - parallel to the orbital angular velocity ) , the total radiated energy rises monotonically as the system loses orbital energy through gravity signals . For unequal masses and / or misaligned spins , however , we find that there can be considerable fluctuations in the emitted energy over time ranges comparable to the orbital period .These fluctuations happen because the proportion of gravitational - wave luminosity generated at each instant varies strongly depending on whether the dark holes orbits are circularized or not . The produced variability may have important implications for observations of merging galaxies using gravitational - wave detectors such as LIGO / VIRGO .",
        "rewrite_text": "We analyze the gravitational wave radiation that occurs during the final stages of gray hole binary coalescence, particularly when the holes are surrounded by an accretion disk and their orbital dynamics are influenced by radiation processes. Through numerical relativity simulations, we investigate how the mass loss rate due to gravitational waves varies based on the spin magnitudes and orientations of the two black holes. Our key finding is that for equal-mass binaries with aligned spins—whether both spins are parallel or anti-parallel to the orbital angular momentum—the total radiated energy consistently increases as the system emits gravitational signals and loses orbital energy. In contrast, for binaries with unequal masses or misaligned spins, we observe significant fluctuations in the emitted energy over time scales similar to the orbital period. These fluctuations occur because the amount of gravitational-wave luminosity produced at any given moment can vary dramatically, depending on whether the black holes' orbits are circularized. This variability may have crucial implications for the observation of merging galaxies with gravitational wave detectors like LIGO/Virgo.",
        "ori-fast-z-score": 1.0660035817780522,
        "water-fast-z-score": 5.7564193416014815,
        "rewrite-fast-z-score": -0.20851441405707477
    },
    {
        "original_text": "In this talk, we will discuss the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory.  We will begin by recalling some basic facts about loop spaces and their relation with infinite-dimensional Lie groups (e.g., loop groups).  Next, we ll recall how one can associate a certain type of infinite-dimensional Lie group called an affine Hecke algebra to any reductive algebraic group over a field k of characteristic 0.   Finally, we ll explain how these two ideas are related via the notion of a Harish-Chandra bimodule. The main result is that if G is a connected semisimple complex algebraic group defined over Q then there exists a natural isomorphism between the category of finite-dimensional representations of G(Q) and the category of Harish-Chandra modules for its associated affine Hecke algebra. This work was done jointly with David Vogan at Harvard University during my postdoctoral fellowship.",
        "watermark_text": "In this talk , we will explore the relationship between loop spaces and Langlands parameters in terms of their connections to representation theory . We will start by examining some fundamental details about loop spaces and their connection with infinite - dimensional Lie fields ( e . g . , loop spaces ) .Next , we ll remember how one can associate a certain type of infinite - dimensional Lie category called an affine Hecke algebra to any reductive algebraic ring over a field k of characteristic 0 . Finally , we ll explain how these two ideas are related via the notion of a Harish - Chandra bimodule .The main consequence is that if G is a connected semisimple complex algebraic ring defined over Q then there exists a natural isomorphism between the category of finite - dimensional representations of G ( Q ) and the category of Harish - Chandra modules for its associated affine Hecke algebra . This research was done jointly with David Vogan at Harvard University during my postdoctoral fellowship .",
        "rewrite_text": "In this talk, we will investigate the connection between loop spaces and Langlands parameters as they relate to representation theory. We will begin by delving into the essential characteristics of loop spaces and their ties to infinite-dimensional Lie algebras, such as loop spaces themselves. Following this, we will recall how to associate a specific type of infinite-dimensional Lie category known as an affine Hecke algebra to any reductive algebraic group over a field \\( k \\) with characteristic 0. Finally, we will elucidate the relationship between these concepts through the framework of Harish-Chandra bimodules. A significant outcome of this exploration is that if \\( G \\) is a connected semisimple complex algebraic group defined over \\( \\mathbb{Q} \\), then there exists a natural isomorphism between the category of finite-dimensional representations of \\( G(\\mathbb{Q}) \\) and the category of Harish-Chandra modules associated with its affine Hecke algebra. This research was conducted in collaboration with David Vogan at Harvard University during my postdoctoral fellowship.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 1.4320780207890627
    },
    {
        "original_text": "The National Science Foundation (NSF) has recently formed an  Exoplanet Task Force  with the goal of identifying key science goals for future space missions in exoplanet research, including radio astrometry.  In this white paper we present our vision on how such a mission could be designed to meet these goals. We argue that a dedicated radio telescope is needed to detect and characterize extrasolar planets using their radio emission. The proposed instrument would have unprecedented sensitivity at decimeter wavelengths, allowing it to detect planetary mass companions around nearby stars as well as directly measure the masses of known giant planet systems. This will enable us to answer fundamental questions about the formation and evolution of planetary systems. Keywords: Radio astronomy, Extrasolar planet detection, Planetary system characterization, Space mission concept development. 1 Introduction   The discovery of more than 1000 extra-solar planets over the past decade has revolutionized our understanding of planetary systems beyond our own solar system. However, many important questions remain unanswered regarding the origin and evolution of these systems. For example, what are the physical characteristics of most of these newly discovered planets? How do they form? What happens when two or more planets interact gravitationally? Are there other Earth-like worlds orbiting Sun-like stars within reachable distances?  Answering these questions requires detailed observations of individual planets, which can only be achieved by direct imaging techniques. Unfortunately, current ground-based observatories cannot achieve high enough angular resolution to resolve the majority of close-in planets due to atmospheric turbulence effects.   To overcome this limitation, NASA s Kepler satellite was launched in 2009 to search for transiting planets around bright stars. Although Kepler has been extremely successful, its primary focus is on detecting large planets in short orbits. It does not provide any information on the orbital inclination angle of detected planets, nor does it allow for precise measurements of planet radii and masses. Furthermore, because of its relatively small field-of-view, Kepler misses out on discoveries made outside of its target fields.",
        "watermark_text": "The National Science Foundation ( NSF ) has recently established an Exoplanet Task Force with the objective of identifying key research goals for future orbital flights in exoplanet research , notably broadcast astrometry . In this white paper we present our vision on how such a spacecraft could be designed to meet these objectives .We argue that a dedicated radio telescope is required to identify and characterize extrasolar planets using their radio emission . The proposed instrument would have enormous sensitivity at decimeter wavelengths , allowing it to identify planetary mass companions around nearby planets as well as closely determine the masses of known giant planet systems .This will provide us to answer basic concerns about the formation and evolution of planetary networks . Keywords : Radio astronomy , Extrasolar moon discovery , Planetary network detection , Space mission design development .1 Introduction The observation of more than 1000 extra - solar planets over the previous decade has revolutionized our understanding of planetary structures beyond our own solar system . However , many important questions remain unanswered concerning the origin and evolution of these systems .For instance , what are the natural characteristics of most of these newly discovered planets ? How do they create ?What happens when two or more planets interact gravitationally ? Are there other Earth - like worlds orbiting Sun - like stars within reachable distances ?Answering these problems demands full observations of multiple worlds , which can only be obtained by direct observation techniques . Unfortunately , current ground - based observatories cannot achieve high enough angular resolution to identify the majority of close - in planets owing to air turbulence influences .To solve this limitation , NASA s Kepler satellite was launched in 2009 to search for transiting planets around bright stars . Although Kepler has been extremely successful , its primary emphasis is on detecting large planets in small orbits .It does not offer any info on the orbital inclination ratio of detected planets , nor does it enable for precise observations of planet radii and masses . Furthermore , because of its relatively small field - of - view , Kepler misses out on discoveries made outside of its target areas .",
        "rewrite_text": "The National Science Foundation (NSF) has recently formed an Exoplanet Task Force aimed at outlining essential research goals for upcoming orbital missions in exoplanet studies, particularly in the realm of broadcast astrometry. In this white paper, we share our vision for how a spacecraft can be developed to fulfill these objectives. We posit that a specialized radio telescope is necessary for identifying and characterizing extrasolar planets through their radio emissions. This proposed instrument would boast exceptional sensitivity at decimeter wavelengths, enabling the detection of planetary mass companions around nearby exoplanets and precise measurements of the masses of known giant planet systems. Such capabilities will deepen our understanding of fundamental questions regarding the formation and evolution of planetary systems. \n\n**Keywords:** Radio astronomy, Discovery of extrasolar moons, Detection of planetary networks, Space mission design development.\n\n**1 Introduction**  \nThe observation of over 1,000 extrasolar planets in the past decade has transformed our comprehension of planetary systems beyond our solar system. Nevertheless, many crucial questions remain unresolved concerning the origins and development of these systems. For instance, what are the inherent characteristics of most of these newly discovered planets? How do they form? What occurs when multiple planets exert gravitational influence upon one another? Are there Earth-like planets orbiting sun-like stars within accessible distances? Addressing these inquiries requires comprehensive observations of various worlds, which can only be achieved through direct observation techniques. Unfortunately, current ground-based observatories lack sufficient angular resolution to identify most close-in planets due to atmospheric turbulence. To address this limitation, NASA launched the Kepler satellite in 2009, tasked with searching for transiting planets around bright stars. While Kepler has been highly successful, its primary focus has been on detecting large planets in small orbits. It does not provide information on the orbital inclinations of detected planets or allow for accurate measurements of their radii and masses. Additionally, due to its relatively narrow field of view, Kepler often overlooks opportunities for discoveries outside of its designated target areas.",
        "ori-fast-z-score": 0.38691161626706844,
        "water-fast-z-score": 8.329938702528295,
        "rewrite-fast-z-score": 0.48038446141526137
    },
    {
        "original_text": "We present the results of an extensive search for new globular cluster candidates (GPCs) in the outer halo of M31, based on deep HST/ACS images covering about one quarter of its virial radius. We find that the number density profile of GPCs is well described by a power law with index -1.5 ± 0.1 out to at least 100 kpc. The total number of GPCs within this region amounts to N = 1253±60, which corresponds to a specific frequency S=N/M*=(3.2±0.4)×10−3. This value agrees very well with previous estimates obtained using ground-based data. In addition we identify two previously unknown distant clusters located at projected distances of more than 300 kpc from the center of M31. These objects are among the most luminous known GCs outside our Galaxy. Their integrated magnitudes are mF814W≈−10 mag and their half-light radii range between r h ≈6 pc and r h ≈20 pc.",
        "watermark_text": "We present the conclusion of an extensive search for new globular cluster candidates ( GPCs ) in the exterior halo of M31 , using on dark HST / ACS images covering about one quarter of its virial diameter . We see that the number density profile of GPCs is well described by a power law with index - 1 . 5 ± 0 . 1 out to at least 100 kpc .The total number of GPCs within this area amounts to N = 1253±60 , which corresponds to a certain frequency S = N / M * = ( 3 . 2±0 . 4 ) ×10−3 . This value agrees very best with previous calculated obtained using ground - based data .In addition we identify two formerly unidentified distant clusters situated at projected distances of more than 300 kpc from the center of M31 . These bodies are among the most luminous known GCs outside our Galaxy .Their integrated magnitudes are mF814W≈−10 mag and their half - light radii vary between p h ≈6 pc and p h ≈20 pc .",
        "rewrite_text": "We present the results of a comprehensive search for new globular cluster candidates (GPCs) in the outer halo of M31, utilizing dark HST/ACS images that cover approximately one quarter of its virial diameter. Our analysis shows that the number density profile of GPCs can be effectively described by a power law with an index of -1.5 ± 0.1, extending to at least 100 kpc. The total count of GPCs in this region is N = 1253 ± 60, which gives a frequency S = N / M* = (3.2 ± 0.4) × 10−3. This value aligns closely with previous estimates derived from ground-based observations. Furthermore, we identify two previously unrecognized distant clusters located at projected distances greater than 300 kpc from the center of M31. These clusters are among the most luminous known globular clusters outside of our Galaxy, with integrated magnitudes of mF814W ≈ -10 mag, and their half-light radii range from approximately 6 pc to 20 pc.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.810003810005715,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "We present new spectroscopic observations for more than 1000 Galactic OB supergiants, obtained with FLAMES/GIRAFFE at the Very Large Telescope (VLT). The sample includes all known O-type dwarfs and giants as well as B-type supergiants brighter than about Mbol = -4 mag within 25 pc distance to Earth. We derive atmospheric parameters T eff , log g, microturbulence velocity vmic, and chemical composition including nitrogen abundance  N/Fe  . For comparison we also analyse a large number of Galactic red supergiants observed by GOSSS project using similar methods. Our results show that there is no significant difference between the mean values of these quantities derived for both samples. However, our analysis reveals systematic differences between different studies based on smaller samples published so far. In particular, we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non-LTE effects or underestimating gravities because they did not take into account stellar winds.",
        "watermark_text": "We present new spectroscopic observations for more than 1000 Galactic OB supergiants , obtained with FLAMES / GIRAFFE at the Very Large Telescope ( VLT ) . The sample comprises all known O - class dwarfs and giants as well as B - class supergiants hotter than about Mbol = - 4 mag within 25 pc proximity to Earth .We derive air parameters T eff , log g , microturbulence velocity vmic , and biological composition including nitrogen density N / Fe . For comparison we also analyse a large number of Galactic red supergiants detected by GOSSS program use similar methods .Our results show that there is no considerable difference between the mean readings of these quantities generated for both samples . However , our analysis reveals systematic differences between various surveys relying on smaller specimens publications so far .In particular , we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non - LTE effects or underestimating gravities because they did not take into account galaxy winds .",
        "rewrite_text": "We present new spectroscopic observations of over 1,000 Galactic OB supergiants, collected using FLAMES/GIRAFFE at the Very Large Telescope (VLT). This sample includes all known O-class dwarfs and giants, along with B-class supergiants that are hotter than approximately Mbol = -4 mag, located within 25 parsecs of Earth. We derive atmospheric parameters such as effective temperature (T_eff), surface gravity (log g), microturbulence velocity (v_mic), and elemental abundance ratios, including nitrogen density (N/Fe). For comparison, we also analyze a substantial number of Galactic red supergiants identified by the GOSSS program using similar methodologies. Our findings indicate no significant differences in the average values of these parameters between the two samples. However, our analysis uncovers systematic discrepancies with results from previous surveys based on smaller samples. Specifically, we discover that many earlier studies overestimated the temperatures of hotter objects by overlooking non-local thermodynamic equilibrium (non-LTE) effects or underestimated gravities by failing to consider galactic winds.",
        "ori-fast-z-score": 1.9123657749350298,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 0.9299811099505543
    },
    {
        "original_text": "We study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity  1  , quantum criticality  2  , and multiferroicity  3  . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure  4  .\nIn recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements  5  found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering  6  showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering  7  revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear  8  .",
        "watermark_text": "We research the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory ( DFT ) observations , which show that this metal is close to an insulator - iron transition accelerated by charge transfer between layers . We see that the Fermi boundary topology changes dramatically across the metal - insulator boundary , with the emergence of new hole pockets at the Brillouin zone center .The measured band gap agrees well with experiments on single crystals . In addition , we estimate that there are two rival nematic phases near the metal - insulator boundary .One has in - plane anisotropy along the Ru - O - Ru bond direction while another one has out - of - plane anisotropy diagonal to it . These conclusions provide insights into the origin of the reported structural degradation in bilayer ruthenates .Bilayer ruthenates have garnered considerable scrutiny lately owing to their valuable physical properties including unconventional superconductivity 1 , quantum criticality 2 , and multiferroicity 3 . Among these structures , Sr3Ru2O7 shows particularly exciting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying tension 4 .In recent years , various experimental studies have been performed to examine the nature of the metal - insulator transition ( MIT ) . For instance , angle resolution photoemission spectroscopy measurements 5 found that the Fermi surface topology changed significantly when crossing the MIT line .X - ray scattering 6 revealed that the crystal symmetry was changed from tetragonal to orthorhombic below TMI = 160 K . Neutron propagation 7 revealed that the crystal conditions were different for the ab plane and c axis below TMIT ~ 150 K . However , despite extensive investigations , the microscopic process behind the MIT remains unsure 8 .",
        "rewrite_text": "We investigate the phase diagram and electronic structure of the bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT). Our findings indicate that this metal is close to a metal-insulator transition, which is driven by charge transfer between its layers. Notably, we observe a significant transformation in the Fermi surface topology at the metal-insulator boundary, with the appearance of new hole pockets at the center of the Brillouin zone. The calculated band gap correlates well with existing experimental data on single crystals. Moreover, we identify the presence of two competing nematic phases near the metal-insulator threshold: one exhibits in-plane anisotropy along the Ru-O-Ru bond direction, while the other displays out-of-plane anisotropy that is diagonal to it. These insights shed light on the structural degradation reported in bilayer ruthenates. The unique physical properties of bilayer ruthenates, such as unconventional superconductivity, quantum criticality, and multiferroicity, have attracted significant interest. Among these, Sr3Ru2O7 is particularly intriguing, as its ground state can be continuously adjusted from metallic to insulating through chemical doping or strain application. Recent experimental studies have explored the nature of the metal-insulator transition (MIT). For example, angle-resolved photoemission spectroscopy has shown a notable change in Fermi surface topology across the MIT line. X-ray scattering studies indicated a transition in crystal symmetry from tetragonal to orthorhombic below T_MI = 160 K, while neutron propagation experiments revealed differences in crystal behavior for the ab-plane and c-axis below T_MIT ~ 150 K. Despite extensive research, the microscopic mechanisms underlying the MIT remain elusive.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 7.244860247099318,
        "rewrite-fast-z-score": 1.5992254762521154
    },
    {
        "original_text": "The second law of thermodynamics is one of the most important laws in physics, which states that entropy always increases with time for closed systems. In this work we present an experimental proof of the second law based on volume entropy and mechanical energy dissipation. We show how to measure volume entropy by using two different methods (one direct method and another indirect method) and then compare these results with theoretical predictions. The experiments are performed at room temperature and atmospheric pressure conditions. Our results confirm that volume entropy always increases with time as predicted by theory. This result also confirms that our measurement system works properly. Finally, we discuss some possible applications of our approach. The second law of thermodynamics has been studied extensively over many years  1  . It states that entropy always increases when matter undergoes irreversible processes  2  , such as heat transfer or chemical reactions  3  .\nIn recent decades there have been several attempts to prove experimentally the validity of the second law  4  -  8  . However, none of them were able to provide a complete proof because they did not take into account all relevant physical quantities involved  9  . For example, it was shown theoretically  10  that volume entropy should be included in order to obtain a complete description of the process under study  11  . Therefore, in this work we propose a new experiment aimed at proving the second law of thermodynamics by measuring volume entropy directly  12  .",
        "watermark_text": "The second rule of thermodynamics is one of the most important rules in science , which says that entropy often increases with time for closed systems . In this study we present an experimental confirmation of the second law based on volume entropy and mechanical energy dissipation .We see how to measure volume entropy by using two different methods ( one direct technique and another indirect method ) and then match these results with theoretical estimates . The studies are performed at room temperature and atmospheric pressure environments .Our results verify that mass entropy often increases with time as predicted by theoretical . This result also proves that our measurement network works well .Finally , we explain some possible applied of our approach . The second law of thermodynamics has been studied frequently over numerous years 1 .It says that entropy often increases when matter undergoes irreversible processes 2 , such as heat transfer or molecular compounds 3 . In recent millennium there have been numerous attempts to prove experimentally the legitimacy of the second law 4 - 8 .However , none of them were could to provide a complete confirmation because they did not take into consideration all relevant physical substances involved 9 . For instance , it was shown theoretically 10 that volume entropy should be included in order to obtain a complete model of the process under experiment 11 .Therefore , in this study we undertake a new study aiming at discovering the second law of thermodynamics by observing volume entropy directly 12 .",
        "rewrite_text": "The second law of thermodynamics is a fundamental principle in science, which states that entropy tends to increase over time in closed systems. In this research, we provide experimental validation of this law through the examination of volume entropy and mechanical energy dissipation. We demonstrate two different methods for measuring volume entropy—one direct and the other indirect—and compare the results with theoretical predictions. Experiments were conducted under standard room temperature and atmospheric pressure conditions. Our findings confirm that mass entropy generally increases over time, aligning with theoretical expectations, and also affirm the effectiveness of our measurement network. Additionally, we discuss potential applications of our approach. The second law has been extensively studied over many years, asserting that entropy typically rises during irreversible processes, including heat transfer and changes in molecular compounds. Although there have been various attempts to experimentally validate the law, none have provided comprehensive confirmation, largely due to the omission of relevant physical factors. For example, theoretical work has indicated that incorporating volume entropy is essential to develop a complete model of the experimental processes. Therefore, this study aims to offer new insights into the second law of thermodynamics by directly observing volume entropy.",
        "ori-fast-z-score": -1.4855627054164149,
        "water-fast-z-score": 7.24191812652287,
        "rewrite-fast-z-score": 0.09759000729485333
    },
    {
        "original_text": "We present near-infrared (NIR) spectroscopy and photometry for the young open cluster IRAS 18511+0146; we find that it is likely to be an intermediate mass star forming region, with a total luminosity of ~10^6 L_Sun . We identify two distinct populations in this cluster; one population has spectral types ranging between F0-F5V, while another population shows signs of emission lines indicative of accretion disks around pre-main sequence stars. The latter group appears to have ages less than 10 Myr based on their H-R diagram positions. This suggests that these are very young objects which may still be embedded within their natal molecular cloud. Using N-body simulations, we show that such clusters can evolve into open clusters like those observed today if they survive disruption by tidal forces during their formation process. In addition, we also estimate the age spread among members of our sample using the equivalent widths of Pa-alpha line profiles as well as the strength of the Brackett gamma line.",
        "watermark_text": "We present near - infrared ( NIR ) spectroscopy and photometry for the early open object IRAS 18511 + 0146 ; we find that it is probably to be an intermediate mass star producing zone , with a total luminosity of ~ 10 ^ 6 L _ Sun . We distinguish two different populations in this cluster ; one population has spectral classes ranging between F0 - F5V , while another population shows signs of emission lines indicative of accretion disks around post - principal sequence stars .The latter group appears to have ages less than 10 Myr based on their H - R diagram positions . This implies that these are very young objects which perhaps still be embedded within their natal molecular cloud .Using N - bodies simulations , we find that such clusters can evolve into open complexes like those observed nowadays if they survive disruption by waves forces during their formed phase . In addition , we also predict the age distribution among members of our sample using the equivalent widths of Pa - alpha line profiles as well as the strength of the Brackett alpha line .",
        "rewrite_text": "We present near-infrared (NIR) spectroscopy and photometry for the early open cluster IRAS 18511 + 0146, suggesting it is likely an intermediate-mass star-forming region with a total luminosity of approximately 10^6 L_Sun. Within this cluster, we identify two distinct stellar populations: one population consists of stars with spectral classifications from F0 to F5V, while the other exhibits emission lines indicative of accretion disks surrounding post-main-sequence stars. The latter group appears to be younger than 10 million years, as inferred from their positions on the Hertzsprung-Russell diagram, suggesting they may still be embedded in their original molecular cloud. Through N-body simulations, we demonstrate that such clusters can evolve into open complexes similar to those observed today, provided they endure the disruptive forces during their formation phase. Additionally, we estimate the age distribution of our sample members by analyzing the equivalent widths of Pa-alpha line profiles and the strength of the Brackett alpha line.",
        "ori-fast-z-score": -0.6793662204867574,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We present new infrared (IR) photometry for the galaxy cluster MS1054-03 at z = 0.83, obtained with ISOCAM on board ISO. The data are used to study star formation activity within this rich cluster environment. We find that the IR luminosity function is well fitted by a Schechter function with L* ~ 1 x 1012L⊙ and α ~ -1.7 over the range 8-1000 µm. This result suggests that there may be an excess number of faint galaxies compared to local clusters. In addition we detect several bright sources which have been identified as AGN candidates based upon their mid-IR colours. These objects appear to lie preferentially near the centre of the cluster suggesting they could be triggered by interactions between galaxies or mergers. Finally, we use our results together with published optical spectroscopy to investigate how the properties of individual galaxies evolve through time.",
        "watermark_text": "We report new infrared ( IR ) photometry for the galaxy region MS1054 - 03 at z = 0 . 83 , obtained with ISOCAM on board ISO . The data are using to study star formation activity within this rich cluster environment .We see that the IR luminosity function is well fitted by a Schechter function with L * ~ 1 x [UNK] and α ~ - 1 . 7 over the range 8 - 1000 µm . This result suggests that there may be an surplus amount of faint objects compared to nearby clusters .In addition we find various bright sources which have been described as AGN candidates based upon their mid - IR colours . These items seem to lay preferentially near the centre of the cluster suggesting they may be triggered by interactions between objects or mergers .Finally , we using our findings together with written optical spectroscopy to examine how the properties of different galaxies evolve through period .",
        "rewrite_text": "We present new infrared (IR) photometry for the galaxy region MS1054-03 at redshift z = 0.83, collected using ISOCAM on the ISO satellite. This data is being utilized to investigate star formation activity within this densely populated cluster environment. Our analysis indicates that the IR luminosity function is well described by a Schechter function, with L* approximately equal to 1 x [UNK] and α around -1.7 over the wavelength range of 8 to 1000 µm. This finding implies a potential excess of faint objects compared to those in nearby clusters. Additionally, we identify several bright sources that have been categorized as AGN candidates based on their mid-IR colors. These sources appear to be concentrated near the center of the cluster, suggesting that their existence may be linked to interactions or mergers between galaxies. Finally, we combine our results with existing optical spectroscopy data to explore how the properties of different galaxies evolve over time.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "The fine structure of solar prominences is studied by using the data obtained with the Swedish 1-meter Solar Telescope (SST). The observations are made in Hα and Ca II 8542 Å lines, which show that the fine structures have different shapes depending on their locations relative to the magnetic field. In addition, we find that there exist two types of fine structures; one type has an elongated shape along the direction parallel to the local magnetic field while another type shows a roundish shape perpendicularly to it. We also found that some fine structures appear as if they were twisted around each other. These results suggest that the fine structures may be formed due to the plasma flows driven by magnetic reconnection between neighboring flux tubes. Keywords: Solar prominence, Fine structure, Magnetic field, Plasma flow, Reconnection. 1 Introduction Solar prominences are observed as dark features against the bright background of the photosphere. They are thought to consist mainly of cool dense plasma suspended above the solar surface by magnetic fields (Kippenhahn & Schlüter 1957) . It was suggested that the fine structures seen within solar prominences might be caused by the plasma flows driven by the magnetic reconnection between neighboring magnetic flux tubes (Pneuman 1983 , Kuperus et al. 1981 . However, the detailed physical processes involved in this process remain unclear because of lack of observational evidence for such phenomena. Recently, high-resolution observations of solar prominences have been performed with various instruments including the Swedish 1-meter solar telescope (SST) (Lin et al. 1998a) , the Advanced Stokes Polarimeter (ASP) at Big Bear Solar Observatory (BBSO), and the Hinode satellite (Kosugi et al. 2007 ). Using these new data sets, several authors reported the observation of fine structures having different shapes depending on their positions relative to the magnetic field (Lin et al. 1998b , Lin 2004 , Berger et al. 2008 .\nIn this study, we investigate the fine structures of solar prominences based on the SST data set. Our aim is to",
        "watermark_text": "The fine structure of sun prominences is studied by using the information obtained with the Swedish 1 - meter Solar Telescope ( SST ) . The surveys are making in Hα and Ca II 8542 Å lines , which show that the fine structures have different shapes depending on their regions relative to the magnetic force .In addition , we find that there exist two forms of fine structures ; one sort has an elongated form along the direction parallel to the local magnetic force while another type gives a roundish shape perpendicularly to it . We additionally found that some fine structures appear as if they were twisted around each other .These data suggest that the fine structures could be formed owing to the plasma flows driven by magnetic reconnection between neighboring flux tubes . Keywords : Solar prominence , Fine structure , Magnetic field , Plasma movement , Reconnection .1 Introduction Solar prominences are observed as dark features against the bright background of the photosphere . They are said to consist mostly of cold dense liquid suspended above the solar surface by magnetic waves ( Kippenhahn & Schlüter 1957 ) .It was suggested that the fine structures visible within solar prominences might be caused by the plasma flows driven by the magnetic reconnection between neighboring magnetic flux tubes ( Pneuman 1983 , Kuperus et al . 1981 .However , the detailed physical processes responsible in this process remain uncertain because of lack of observational evidence for such events . Recently , large - resolution measurements of sun prominences have been performed with various instruments including the Swedish 1 - meter solar observatory ( SST ) ( Lin et al .1998a ) , the Advanced Stokes Polarimeter ( ASP ) at Big Bear Solar Observatory ( BBSO ) , and the Hinode satellite ( Kosugi et al . 2007 ) .Using these new data sets , various scientists reported the observation of fine structures having different shapes depending on their orientation relative to the magnetic force ( Lin et al . 1998b , Lin 2004 , Berger et al .2008 . In this study , we investigate the fine structures of solar prominences based on the SST results setting .Our aim is to",
        "rewrite_text": "We examine the fine structure of solar prominences utilizing data from the Swedish 1-meter Solar Telescope (SST). Our surveys, conducted in the Hα and Ca II 8542 Å lines, reveal that these fine structures vary in shape depending on their orientation with respect to the magnetic field. Specifically, we identify two distinct forms: one type exhibits an elongated shape aligned with the local magnetic field, while the other appears more rounded, oriented perpendicularly to it. Additionally, some structures seem to be intertwined. These observations indicate that the fine structures may be formed by plasma flows resulting from magnetic reconnection between adjacent flux tubes. \n\nKeywords: Solar prominence, Fine structure, Magnetic field, Plasma movement, Reconnection.\n\n1. Introduction\n\nSolar prominences manifest as dark features against the luminous backdrop of the photosphere and are primarily composed of cold, dense plasma suspended above the solar surface by magnetic waves (Kippenhahn & Schlüter, 1957). It has been proposed that the fine structures observed within solar prominences may arise from plasma flows driven by magnetic reconnection between neighboring magnetic flux tubes (Pneuman, 1983; Kuperus et al., 1981). However, the specific physical mechanisms involved remain largely unclear due to a lack of observational evidence. Recently, high-resolution observations of solar prominences have been carried out using various instruments, including the SST (Lin et al., 1998a), the Advanced Stokes Polarimeter (ASP) at Big Bear Solar Observatory (BBSO), and the Hinode satellite (Kosugi et al., 2007). Utilizing these novel datasets, researchers have reported the presence of fine structures with varying shapes contingent on their alignment relative to the magnetic field (Lin et al., 1998b; Lin, 2004; Berger et al., 2008). In this study, we focus on investigating the fine structures of solar prominences using results from the SST. Our goal is to...",
        "ori-fast-z-score": 1.5360589585634423,
        "water-fast-z-score": 7.897065047448726,
        "rewrite-fast-z-score": 1.8864844365675972
    },
    {
        "original_text": "We present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization, based on correlated random walks (CRWs). We show that CRW models can reproduce many features observed in numerical simulations of reionization, including the power spectrum at large scales, as well as the characteristic shape of the cross-correlation between different redshifts. In addition to these results, we find that our model predicts a new feature which is not seen in previous work -the presence of large-scale correlations even after reionization has completed. This effect may be detectable with future radio telescopes such as SKA. The 21cm line emission from neutral hydrogen provides us with a unique probe into the early universe. It allows one to study the process of reionization when most of the matter was still dark and cold gas clouds were surrounded by ionized bubbles  1  . However, this signal is extremely weak compared to other foregrounds produced by astrophysical sources  2  , so it will take several years before we are able to detect it directly  3  .\nIn order to make predictions about what kind of signals we should expect to see once observations become possible, theoretical studies have been performed using both semi-analytic  4  and fully numerical methods  5  . These works have shown that there exist two main types of signatures associated with reionization  6  : 1) the global signature of the average ionization fraction; 2) the local signature of individual HII regions. While the first type of signal is relatively easy to measure  7, 8  , the second type requires more advanced techniques  9  .",
        "watermark_text": "We present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization , using on correlated random tours ( CRWs ) . We suggest that CRW models can generate several characteristics witnessed in mathematical simulations of reionization , notably the power spectrum at large scales , as well as the typical shape of the cross - correlation between various redshifts .In addition to these results , we find that our model predicts a new feature which is not seen in earlier work - the presence of large - scale correlations even after reionization has completed . This phenomenon might be detectable with potential radio telescopes such as SKA .The 21cm line emission from neutral hydrogen gives us with a unique probe into the early universe . It enables one to study the process of reionization when most of the matter was still dark and cold gas clouds were covered by ionized bubbles 1 .However , this signal is incredibly faint compared to other foregrounds caused by astrophysical sources 2 , so it will take many years before we are able to locate it directly 3 . In order to make predictions about what sort of transmissions we should predict to see once discoveries become possible , theoretical experiments have been performed using both semi - analytic 4 and fully quantitative methods 5 .These works have shown that there exist two principal kinds of signatures identified with reionization 6 : 1 ) the global signature of the average ionization fraction ; 2 ) the local signature of individual HII domains . While the first sort of signal is fairly easy to measure 7 , 8 , the second kind needs more advanced techniques 9 .",
        "rewrite_text": "We introduce an analytical model for the evolution of 21 cm brightness temperature fluctuations during cosmic reionization, utilizing correlated random walks (CRWs). Our findings suggest that CRW models can replicate several features observed in mathematical simulations of reionization, particularly the large-scale power spectrum and the typical shape of cross-correlations across different redshifts. Additionally, we identify a novel characteristic in our model that has not been previously documented: the existence of large-scale correlations even after the completion of reionization. This phenomenon may be detectable with next-generation radio telescopes like SKA. The 21 cm line emission from neutral hydrogen offers a distinctive probe into the early universe, facilitating the study of reionization during an era when much of the matter consisted of dark and cold gas, interspersed with ionized bubbles. However, this signal is exceedingly faint compared to various astrophysical foregrounds, making direct detection a long-term endeavor. To make predictions about the potential signals that may be observed once direct detection becomes feasible, theoretical experiments involving both semi-analytic and fully quantitative methods have been conducted. These studies have identified two main types of signatures associated with reionization: 1) the global signature representing the average ionization fraction, and 2) the local signature corresponding to individual HII regions. While the first type of signal is relatively straightforward to measure, the second type requires more sophisticated techniques.",
        "ori-fast-z-score": -0.4508348173337161,
        "water-fast-z-score": 6.286185570937122,
        "rewrite-fast-z-score": 1.104689541477988
    },
    {
        "original_text": "The theta + (θ+) effect has been observed by many authors, but it was never explained satisfactorily.  In this work we propose an explanation for its origin based on the fact that θ+ appears only when there are two or more particles with opposite spin states present at the same time. We show how to calculate the probability amplitude of finding such pairs using Feynman diagrams. The results obtained agree well with those reported experimentally. This article is available from: http://arxiv.org/abs/1401.5189. The theta + (θ+ ) effect  1  , which consists of a small increase in the intensity of the scattered beam as compared to the unscattered one, has been observed by several authors  2  -  4  . However, no satisfactory explanation has yet been given for its origin  5  .\nIn this work we propose an interpretation for the appearance of θ+ based on the fact that it occurs only if there are simultaneously two or more particles with different spins  6  . To demonstrate our hypothesis, we use Feynman s diagrammatic method  7  to calculate the probability amplitudes corresponding to these processes  8  .",
        "watermark_text": "The theta + ( θ + ) effect has been observed by many writers , but it was never described satisfactorily . In this study we propose an reason for its origin relying on the fact that θ + occurs only when there are two or more electrons with opposite spin states present at the same time .We see how to estimate the probability amplitude of finding such pairs using Feynman diagrams . The results collected accord well with those published experimentally .This section is accessible from : www : / / arxiv . org / abs / 1401 . 5189 . The theta + ( θ + ) effect 1 , which consists of a small increase in the frequency of the scattered beam as compared to the unscattered one , has been observed by many writers 2 - 4 .However , no satisfactory alternative has yet been given for its identity 5 . In this study we propose an interpretation for the appearance of θ + based on the fact that it exists only if there are simultaneously two or more particles with different spinning 6 .To show our theory , we utilize Feynman s diagrammatic method 7 to estimate the probability amplitudes corresponding to these processes 8 .",
        "rewrite_text": "The theta plus (θ+) effect has been noted by numerous authors, yet a satisfactory explanation has never been provided. In this study, we offer an interpretation for its emergence, based on the observation that θ+ appears only when two or more electrons with opposing spin states are present simultaneously. We demonstrate how to calculate the probability amplitude for detecting such pairs using Feynman diagrams, and our findings are consistent with previously published experimental data. This section is available at: www://arxiv.org/abs/1401.5189. The θ+ effect is characterized by a slight increase in the frequency of the scattered beam compared to the unscattered one, as reported by various researchers. However, no adequate alternative explanation for its nature has been proposed. Our interpretation hinges on the condition that θ+ only manifests when two or more particles with differing spins coexist. We apply Feynman’s diagrammatic method to estimate the probability amplitudes related to these interactions.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "We present an algorithm for clustering graphs based on their patterns, which are represented as lattices. The proposed method is applied to analyze the similarity between chemical compounds and protein structures. We show that our approach can be used to identify similar substructures within large databases efficiently. In addition, we demonstrate how it can be utilized to detect structural changes caused by mutations or modifications. Our experiments reveal that the presented technique outperforms state-of-the-art methods significantly both in terms of accuracy and efficiency. Clustering algorithms have been widely studied in many fields such as data mining  1  , bioinformatics  2  , computer vision  3  , etc., due to its importance in discovering hidden information  4  . However, most existing approaches focus only on finding clusters without considering any additional information about them  5  .\nIn this work, we propose a novel graph clustering algorithm called CLUSTERING WITH LATTERS (CLL)  6  , which takes advantage of lattice representation  7, 8  to capture the pattern of each cluster. As shown in Figure 1 , CLL first transforms all input graphs into their corresponding lattices using a set of predefined rules  9  . Then, it performs hierarchical agglomerative clustering  10  over these lattices to find groups of similar graphs. Finally, it assigns each group to one cluster according to the majority vote among all members belonging to the same group  11  .",
        "watermark_text": "We present an algorithm for clustering graphs based on their patterns , which are represented as lattices . The proposed approach is applied to analyze the similarity between chemical molecules and protein compounds .We suggest that our approach can be used to identify similar substructures within large databases accurately . In addition , we prove how it can be employed to identify structural modifications affected by genes or modifications .Our experiments indicate that the offered technique outperforms state - of - the - art methods significantly both in terms of precision and efficiency . Clustering methods have been widely explored in multiple fields such as data extraction 1 , bioinformatics 2 , computer vision 3 , etc . , owing to its significance in discovering secret information 4 .However , most existing techniques concentrate only on discovering clusters without examining any additional information about them 5 . In this research , we propose a new graph clustering procedure entitled CLUSTERING WITH LATTERS ( CLL ) 6 , which uses advantage of lattice representation 7 , 8 to capture the rhythm of each cluster .As seen in Figure 1 , CLL initially transforms all input graphs into their appropriate lattices using a setting of predefined rules 9 . Then , it performs hierarchical agglomerative clustering 10 over these lattices to find groups of related graphs .Finally , it gives each group to one grouping due to the majority vote among all members belonging to the same group 11 .",
        "rewrite_text": "We introduce a novel algorithm for clustering graphs based on their patterns, represented as lattices. This method is applied to evaluate the similarities between chemical molecules and protein compounds. We propose that our approach can effectively identify similar substructures within extensive databases. Furthermore, we demonstrate its capacity to detect structural changes influenced by genes or modifications. Our experiments show that this technique significantly outperforms current state-of-the-art methods in both precision and efficiency. Clustering methods have been extensively studied across various fields, including data extraction, bioinformatics, and computer vision, due to their importance in uncovering hidden information. However, many existing techniques focus solely on identifying clusters without considering additional contextual information. In this study, we present a new graph clustering method named CLUSTERING WITH LATTICES (CLL), which leverages lattice representation to capture the characteristics of each cluster. As illustrated in Figure 1, CLL first converts all input graphs into their corresponding lattices using a set of predefined rules. It then applies hierarchical agglomerative clustering on these lattices to group related graphs. Finally, it assigns a label to each group based on the majority vote of its members.",
        "ori-fast-z-score": -0.9901475429766744,
        "water-fast-z-score": 6.798772019333623,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "The GRI (Gamma Ray Imager) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study gamma-ray bursts, active galactic nuclei and other phenomena in high-energy astronomy.  It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun-Earth L2 orbit at 1 AU distance from Earth. Its main instrument consists of two coded mask telescopes observing simultaneously in the energy range 20 MeV - 300 GeV. Each telescope has a large field-of-view of 2 steradians and a spatial resolution better than 0.1 degrees. A third detector module provides additional information about the background radiation environment for each telescope. This article describes the design concept of this innovative instrument. Keywords: Astroparticle physics, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction:  The GRI (Gamma Ray...  more ",
        "watermark_text": "The GRI ( Gamma Ray Imager ) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study alpha - ray clusters , active galactic nuclei and other processes in high - energy astronomy . It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun - Earth L2 orbit at 1 AU distance from Earth .Its main object consists of two coded mask telescopes observing simultaneously in the power range 20 MeV - 300 GeV . Each observatory has a large field - of - view of 2 steradians and a spatial resolution best than 0 . 1 degrees .A third detector module presents additional information about the background radiation conditions for each telescope . This page describes the design concept of this innovative instrument .Keywords : Astroparticle science , High - Energy Astronomy , Instrumentation , Space Observatory . Introduction : The GRI ( Gamma Ray . . . more",
        "rewrite_text": "The Gamma Ray Imager (GRI) is an astrophysics space observatory designed by the French Space Agency CNES in collaboration with NASA. Its mission is to explore alpha-ray clusters, active galactic nuclei, and various phenomena in high-energy astronomy. The observatory is set to be launched aboard a Soyuz rocket with a Fregat upper stage into a Sun-Earth L2 orbit, positioned at 1 AU from Earth. The GRI features two coded mask telescopes that can observe simultaneously within the energy range of 20 MeV to 300 GeV. Each telescope provides a wide field of view of 2 steradians, with a spatial resolution better than 0.1 degrees. Additionally, a third detector module offers supplementary information on the background radiation surrounding each telescope. This document outlines the design concept of this innovative instrument. \nKeywords: Astroparticle science, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction: The GRI (Gamma Ray...).",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 4.008918628686366,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "We present an analysis of the connection between different galaxy types using data from the Sloan Digital Sky Survey (SDSS). We use two methods to classify galaxies into four types: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL) and early-type galaxies without emission lines (ETGSIL).\nThe first method is based on the principal component analysis (PCA) applied to the optical spectra of all galaxies classified as spectroscopic targets by the SDSS pipeline. The second one uses the PCA applied only to the subset of galaxies that are morphologically selected for having bulges dominated by old stellar populations. In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs evolve towards ETGSILs through ETGELs. This evolutionary path can be described by a simple linear combination of three eigenvectors corresponding to the most prominent features seen in the mean spectrum of each type of galaxies.",
        "watermark_text": "We present an assessment of the link between various galaxy kinds use data from the Sloan Digital Sky Survey ( SDSS ) . We use two means to classify objects into four types : star - creating galaxies ( SFG ) , active galactic nuclei guest galaxies ( AGNHG ) , early - class galaxies with emitted lines ( ETGEL ) and early - class galaxies without absorption lines ( ETGSIL ) .The first method is based on the main component analysis ( PCA ) applied to the optical spectra of all galaxies designated as spectroscopic targets by the SDSS pipeline . The second one uses the PCA applicable only to the subset of stars that are morphologically selected for having bulges dominated by ancient stars populations .In both cases we find that ETGs form a continuous progression in terms of their spectral properties along which SFGs grow towards ETGSILs through ETGELs . This evolutionary progression can be described by a simple linear mixture of three eigenvectors corresponding to the most notable features found in the mean spectrum of each type of galaxies .",
        "rewrite_text": "We present an evaluation of the relationship among different types of galaxies using data from the Sloan Digital Sky Survey (SDSS). Our classification divides objects into four categories: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL), and early-type galaxies without absorption lines (ETGSIL). The first classification method employs principal component analysis (PCA) applied to the optical spectra of all galaxies marked as spectroscopic targets by the SDSS pipeline. The second method utilizes PCA specifically on a subset of stars that have been morphologically selected for having bulge structures predominantly composed of old star populations. In both approaches, we observe that early-type galaxies (ETGs) exhibit a continuous progression in their spectral properties, where SFGs evolve into ETGSILs via ETGELs. This evolutionary sequence can be represented by a straightforward linear combination of three eigenvectors that highlight the most significant features identified in the average spectrum of each galaxy type.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "We present Spitzer Space Telescope observations at 24, 70, and 160 microns for 12 members of the nearby (140 pc) Taurus star-forming region with ages between 1 Myr to 10 Myr. We find that all sources show excess emission above photospheric levels indicative of circumstellar material surrounding each star. The majority of these objects are surrounded by optically thick disks which can be fit well using single temperature blackbody models. However, we also identify three systems where the disk is likely to have an inner hole or gap; TW Hya, DM Tau, and GM Aur. In addition, we detect two transitional disks around V4046 Sgr and Sz 91. These results suggest that most stars in our sample retain their primordial disks up until at least 5 Myr after formation. Finally, we use mid-infrared spectroscopy obtained with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks.",
        "watermark_text": "We present Spitzer Space Telescope observations at 24 , 70 , and 160 microns for 12 members of the nearby ( 140 pc ) Taurus star - creating area with periods between 1 Myr to 10 Myr . We see that all sources show extra emitted above photospheric concentrations indicative of circumstellar material surrounding each star .The majority of these objects are surrounded by optically dense disks which can be fit well using single temperature blackbody studies . However , we also identify three components where the disk is expected to have an inner cavity or gap ; TW Hya , DM Tau , and GM Aur .In addition , we find two transitional disks around V4046 Sgr and Sz 91 . These data suggest that most stars in our sample maintain their primordial disks up until at least 5 Myr after formed .Finally , we utilize mid - infrared spectroscopy acquired with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks .",
        "rewrite_text": "We present observations from the Spitzer Space Telescope at wavelengths of 24, 70, and 160 microns for 12 members of the nearby Taurus star-forming region, located approximately 140 parsecs away, with ages ranging from 1 to 10 million years. Our analysis reveals that all sources exhibit additional emission beyond photospheric levels, indicating the presence of circumstellar material around each star. Most of these objects are surrounded by optically thick disks that can be accurately modeled using single temperature blackbody fits. However, we also identify three cases—TW Hya, DM Tau, and GM Aur—where the disks are expected to contain inner cavities or gaps. Additionally, we find two transitional disks around V4046 Sgr and Sz 91. These findings suggest that the majority of stars in our sample retain their primordial disks for at least 5 million years after formation. Lastly, we employ mid-infrared spectroscopy from the IRS instrument on Spitzer to analyze the composition of the dust grains within these disks.",
        "ori-fast-z-score": 0.39056673294247163,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": 1.4770978917519928
    },
    {
        "original_text": "We study the decoherence of macroscopic quantum superposition states in open systems, and show that it is possible to prepare such states with high fidelity using current experimental techniques.  We consider two different types of macroscopic superposition: coherent spin states (CSS) and Schrödinger cat states (SCS). For CSS we find that the decay rate scales as $1/N$ where $N$ is the number of particles involved in the state. This scaling law can be understood by considering the effect of spontaneous emission on each particle separately. In contrast for SCS the decay rate scales as $1/sqrt(N)$ which cannot be explained by treating individual particles independently. Instead we argue that this behavior arises due to collective effects between all particles in the system. Finally, we discuss how these results could be tested experimentally. Decoherence of macroscopic superposition states has been studied extensively over recent years  1–3  . It was shown theoretically  4  , and confirmed experimentally  5, 6  , that macroscopic superposition states are extremely fragile against environmental noise  7, 8  . The main reason behind their fragility lies in the fact that they involve many particles, so even small interactions with an environment lead to rapid loss of coherence  9  .\nIn this work we focus on studying the decoherence process of macroscopic superposition in open systems. Specifically, we consider two different types of superposition: coherent spin states  10  and Schrödinger cat-states  11  . Coherent spin states have already been prepared experimentally  12  while Schrödinger cat states remain elusive  13  . However, there exist proposals for preparing them  14–18  , and recently some progress towards realizing them has been made  19, 20  .  Our analysis shows that both types of superposition exhibit qualitatively similar behaviour under decoherence.",
        "watermark_text": "We explore the decoherence of macroscopic quantum superposition states in open systems , and suggest that it is easy to analyze such states with high fidelity using current experimental methods . We consider two different kinds of macroscopic superposition : coherent spinning states ( CSS ) and Schrödinger cat states ( SCS ) .For CSS we find that the decay rate scales as $ 1 / N $ where $ N $ is the quantity of particles affected in the state . This scaling law can be understood by using the effect of spontaneous emission on each particle separately .In comparison for SCS the decay rate scales as $ 1 / sqrt ( N ) $ which cannot be described by treating individual particles jointly . Rather we claim that this behavior occurs due to collective effects between all particles in the system .Finally , we talk how these results could be evaluated experimentally . Decoherence of macroscopic superposition states has been studied extensively over recent periods 1 – 3 .It was shown theoretically 4 , and reported experimentally 5 , 6 , that macroscopic superposition states are extremely unstable against environmental noise 7 , 8 . The main explanation behind their fragility lies in the fact that they consist several particles , so even minor interactions with an environment result to rapid loss of coherence 9 .In this research we focus on studying the decoherence cycle of macroscopic superposition in open systems . Specifically , we investigate two different kinds of superposition : coherent spinning states 10 and Schrödinger cat - states 11 .Coherent spin states have already been constructed experimentally 12 while Schrödinger cat states stay elusive 13 . However , there remain ideas for studying them 14 – 18 , and recently some progress towards realizing them has been made 19 , 20 .Our study shows that both types of superposition demonstrate qualitatively identical actions under decoherence .",
        "rewrite_text": "We investigate the decoherence of macroscopic quantum superposition states in open systems and propose that current experimental techniques make it relatively straightforward to analyze these states with high fidelity. Our focus is on two specific types of macroscopic superpositions: coherent spinning states (CSS) and Schrödinger cat states (SCS). For CSS, we observe that the decay rate scales as \\( 1 / N \\), where \\( N \\) represents the number of particles involved in the state. This scaling can be understood through the individual effects of spontaneous emission on each particle. In contrast, the decay rate for SCS scales as \\( 1 / \\sqrt{N} \\), a behavior that cannot be explained by simply treating the particles separately; instead, we argue that this scaling arises from the collective interactions among all particles in the system. We also discuss potential experimental evaluations of these findings. The decoherence of macroscopic superposition states has been the subject of considerable study in recent years. Theoretical work and experimental evidence have demonstrated that these states are highly susceptible to environmental noise, with their instability largely attributed to the involvement of multiple particles, where minor environmental interactions can lead to rapid coherence loss. Our research centers on the decoherence processes of macroscopic superpositions in open systems, specifically examining coherent spinning states and Schrödinger cat states. While coherent spinning states have been successfully created in experiments, Schrödinger cat states remain elusive. Nevertheless, there are ongoing ideas and recent advancements aimed at realizing these states. Our findings indicate that both categories of superpositions exhibit qualitatively similar behavior when subjected to decoherence.",
        "ori-fast-z-score": -1.2572371141874243,
        "water-fast-z-score": 5.680518698404823,
        "rewrite-fast-z-score": -1.1818181818181819
    },
    {
        "original_text": "We report the observation of very weak electron-phonon coupling (EPC) in La0.7Sr0.3MnO3 thin films grown on SrTiO3 substrates by pulsed laser deposition, which is consistent with previous reports for bulk samples.  We also observe that EPC decreases as temperature increases up to 300 K. This behavior can be explained by considering the effect of lattice expansion due to thermal fluctuations at high temperatures. In addition, we find that the magnitude of EPC depends strongly on film thickness; it becomes smaller when the film thickness decreases down to 10 unit cells. The observed dependence of EPC on both temperature and film thickness suggests that phonon confinement plays an important role in determining the strength of EPC in these materials. Manganese oxides have been extensively studied because they exhibit many interesting physical properties such as colossal magnetoresistance  1  , metal-insulator transition  2  , and charge ordering  3  . Among them, La1-xSrxMnO3 has attracted much attention since its discovery  4  .\nIn this compound, Mn ions are located at two different sites, i.e., Mn3+(tetrahedral site) and Mn4+(octahedral site). It was found that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x  5  . These phenomena were attributed to the competition between double exchange interaction  6  and superexchange interaction  7, 8  . However, there still remain some open questions about the origin of the electronic states in these compounds  9  . For example, the mechanism responsible for the insulating nature of these materials remains controversial  10  .",
        "watermark_text": "We report the observation of very weakened electron - phonon coupling ( EPC ) in La0 . 7Sr0 . 3MnO3 narrow bands grown on SrTiO3 substrates by pulsed infrared deposition , which is compatible with previous findings for bulk samples . We additionally observe that EPC changes as temperature increases up to 300 K . This activity can be described by using the impact of lattice increase due to heat fluctuations at high temperatures .In addition , we find that the severity of EPC depends strongly on film thickness ; it becomes weaker when the film thickness decreases down to 10 unit cells . The observed influence of EPC on both heat and film thickness implies that phonon confinement serves an important role in measuring the strength of EPC in these materials .Manganese oxides have been heavily explored because they demonstrate many interesting physical properties such as colossal magnetoresistance 1 , metal - insulator transition 2 , and charge ordering 3 . Among them , La1 - xSrxMnO3 has garnered considerable scrutiny since its observation 4 .In this compound , Mn ions are situated at two different places , i . e . , Mn3 + ( tetrahedral site ) and Mn4 + ( octahedral site ) . It was shown that the magnetic ground state changes from ferromagnetic insulator to antiferromagnetic insulator upon increasing x 5 .These phenomena were attributed to the competition between multiple transfer exchange 6 and superexchange interaction 7 , 8 . However , there still continue some open questions about the origin of the electronic states in these interactions 9 .For instance , the process responsible for the insulating behavior of these structures remains controversial 10 .",
        "rewrite_text": "We present our findings on significantly reduced electron-phonon coupling (EPC) in narrow bands of La0.7Sr0.3MnO3 that have been grown on SrTiO3 substrates using pulsed infrared deposition, aligning with previous results observed in bulk samples. Our study also reveals that EPC varies with temperature, increasing up to 300 K. This behavior can be attributed to the expansion of the lattice due to thermal fluctuations at elevated temperatures. Furthermore, we discover that the strength of EPC is highly dependent on film thickness, showing a decrease as the thickness approaches 10 unit cells. The observed effects of EPC on temperature and film thickness suggest that phonon confinement plays a significant role in determining EPC strength in these materials. Manganese oxides have been widely studied due to their intriguing physical properties, such as colossal magnetoresistance, metal-insulator transitions, and charge ordering. La1-xSrxMnO3, in particular, has gained considerable attention since its initial discovery. In this compound, manganese ions occupy two distinct sites: Mn3+ (tetrahedral) and Mn4+ (octahedral). It has been shown that the magnetic ground state transitions from ferromagnetic insulator to antiferromagnetic insulator as the value of x increases. These phenomena are thought to result from the interplay of multiple transfer exchange and superexchange interactions. However, questions remain regarding the origins of electronic states in these interactions, particularly concerning the mechanisms that contribute to the insulating behavior of these structures.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.375262396202865,
        "rewrite-fast-z-score": 2.2234939248050263
    },
    {
        "original_text": "We report the discovery of an unusual object in the outer solar system that is likely to be composed primarily of ice particles. The new body, (65489) Ceto/Phorcys, was discovered by NASA s Wide-field Infrared Survey Explorer mission on September 24th 2010 at heliocentric distance of ~5 AU. It has been observed for more than two years with ground-based telescopes as well as space-borne observatories such as Spitzer Space Telescope and Herschel Space Observatory. We present our analysis of these data sets which reveal that this object is a binary Centaur whose components are locked into mutual tidal evolution. This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km. Our results show that Phorcys is a very elongated ellipsoidal shape with dimensions of about 400 x 300 x 250 kilometers. Its surface temperature is estimated to be around -100 degrees Celsius.",
        "watermark_text": "We report the discovery of an unique body in the outer solar system that is probably to be composed primarily of ice particles . The newest body , ( 65489 ) Ceto / Phorcys , was discovered by NASA s Wide - field Infrared Survey Explorer mission on September 24th 2010 at heliocentric height of ~ 5 AU .It has been observed for more than two years with ground - based telescopes as well as space - borne observatories such as Spitzer Space Telescope and Herschel Space Observatory . We present our analysis of these information sets which confirm that this body is a binary Centaur whose components are locked into mutual tidal evolution .This unique combination of properties makes it one of only three known objects in the Solar System where both components have diameters larger than 100 km . Our results show that Phorcys is a very elongated ellipsoidal structure with sizes of about 400 x 300 x 250 meter .Its surface temperature is predicted to be around - 100 degrees Celsius .",
        "rewrite_text": "We announce the discovery of a distinctive object in the outer solar system that is likely composed mainly of ice particles. The newly identified body, (65489) Ceto/Phorcys, was found by NASA's Wide-field Infrared Survey Explorer mission on September 24, 2010, at a heliocentric distance of approximately 5 AU. It has been observed for over two years using both ground-based telescopes and space-based observatories like the Spitzer Space Telescope and the Herschel Space Observatory. In this report, we present our analysis of these datasets, which confirm that this object is a binary Centaur with its components undergoing mutual tidal evolution. This rare combination of characteristics makes it one of only three known objects in the Solar System where both components have diameters greater than 100 km. Our findings indicate that Phorcys has a highly elongated, ellipsoidal shape, measuring about 400 x 300 x 250 meters, with a predicted surface temperature of approximately -100 degrees Celsius.",
        "ori-fast-z-score": 1.8382900600361156,
        "water-fast-z-score": 5.252257314388902,
        "rewrite-fast-z-score": 2.4327007187250236
    },
    {
        "original_text": "We study gravitational waves produced by curvaton scenario, where the inflaton field is coupled to another scalar field called curvaton which decays into radiation after inflation and produces primordial density fluctuations. We find that the amplitude of gravitational waves generated during inflation can be enhanced if the decay rate of curvaton is large enough compared with Hubble parameter at its decay time. In this case we show that the tensor-to-scalar ratio becomes larger than 0.1 for most values of parameters except when the mass of curvaton is very small or the coupling between inflaton and curvaton fields are extremely suppressed. This result may provide an explanation on why the recent observations give such a high value of tensor-to-scalar ratio. \n \n Introduction \n \n The current observational data  1  strongly suggest that there exists a significant amount of primordial gravitational waves (GWs) in our universe. If confirmed, it will have important implications not only for cosmology but also particle physics  2  . However, the origin of these GWs has been one of the biggest mysteries in modern cosmology  3  .\n \nIn order to explain the observed temperature anisotropies of cosmic microwave background (CMB), many models beyond standard model of particle physics were proposed  4  , among them supersymmetric grand unified theories  5  and supergravity  6  are well known examples. These models predict new particles whose masses lie around 10 16 GeV  7, 8  . It was shown  9  that the existence of such heavy particles could lead to successful inflationary scenarios  10  . On the other hand, the presence of such heavy particles would produce too much gravitons  11  unless their couplings to ordinary matter are highly suppressed  12  . Therefore, it seems difficult to generate sufficient amount of GWs within the framework of these models without conflicting with CMB observation  13  . \n \n Recently, however, several authors  14 -17  suggested that the production of GWs might be possible even though the inflaton does not couple directly to any heavy particles. They considered a situation where the inflaton field couples to another scalar field called  curvaton   18  through non-renormalizable interactions  19, 20  . After",
        "watermark_text": "We explore gravity currents produced by curvaton scenario , where the inflaton field is linked to another scalar field called curvaton which decays into radiation after inflation and causes primordial density fluctuations . We see that the frequency of gravitational waves generated during inflation can be enhanced if the decay rate of curvaton is huge enough compared with Hubble parameter at its degradation speed .In this instance we prove that the tensor - to - scalar ratio becomes greater than 0 . 1 for most values of measurements except when the mass of curvaton is very small or the interaction between inflaton and curvaton fields are extremely suppressed . This result may provide an excuse on why the recent observations give such a high value of tensor - to - scalar ratio .Introduction The present observational data 1 firmly confirm that there exists a substantial quantity of primordial gravitational waves ( GWs ) in our universe . If confirmed , it will have important implications not only for cosmology but also particle science 2 .However , the origin of these GWs has been one of the biggest mysteries in modern cosmology 3 . In try to explain the known temperature anisotropies of cosmic microwave background ( CMB ) , various models beyond standard theory of particle theory were offered 4 , among them supersymmetric grand unified physics 5 and supergravity 6 are well famous instance .These systems predict new objects whose masses sit around 10 16 GeV 7 , 8 . It was shown 9 that the existence of such heavy grains could lead to successful inflationary scenarios 10 .On the other hand , the presence of such heavy ions might generate too much gravitons 11 unless their couplings to normal matter are strongly restrained 12 . Therefore , it appears impossible to produce enough quantity of GWs within the framework of these models without conflicting with CMB observation 13 .Recently , however , various scientists 14 - 17 suggested that the production of GWs might be possible even though the inflaton does not couple directly to any heavy nuclei . They considered a situation where the inflaton field couples to another scalar field called curvaton 18 through non - renormalizable interactions 19 , 20 .After",
        "rewrite_text": "We investigate gravity currents arising from the curvaton scenario, in which the inflaton field interacts with another scalar field known as the curvaton. The curvaton decays into radiation following inflation, leading to primordial density fluctuations. Our analysis reveals that the frequency of gravitational waves generated during inflation can be significantly increased if the curvaton's decay rate is sufficiently large compared to the Hubble parameter during its decay. In this case, we demonstrate that the tensor-to-scalar ratio exceeds 0.1 for most parameter values, except in scenarios where the curvaton mass is very small or the interaction between the inflaton and curvaton fields is highly suppressed. This finding may help explain why recent observations have indicated a relatively high tensor-to-scalar ratio.\n\nIntroduction: Current observational data firmly indicate the existence of a substantial quantity of primordial gravitational waves (GWs) in our universe. If verified, this would have significant implications not only for cosmology but also for particle physics. However, the origin of these GWs remains one of the greatest enigmas in modern cosmology. To address the known temperature anisotropies of the cosmic microwave background (CMB), various models beyond the standard particle theory have been proposed, including well-known examples such as supersymmetric grand unified theories and supergravity. These frameworks predict new particles with masses around 10^16 GeV. It has been shown that the presence of such heavy particles could facilitate successful inflationary scenarios. Conversely, the existence of these heavy particles might produce an excessive number of gravitons unless their couplings to ordinary matter are sufficiently restricted. Consequently, generating an adequate quantity of GWs within the framework of these models appears to conflict with CMB observations. Recently, however, several researchers have proposed that GWs may be produced even if the inflaton does not interact directly with any heavy particles. They considered a scenario in which the inflaton field interacts with the curvaton through non-renormalizable interactions.",
        "ori-fast-z-score": 0.7373087284671365,
        "water-fast-z-score": 7.675067860720625,
        "rewrite-fast-z-score": 1.3333333333333333
    },
    {
        "original_text": "We present new observations with the High Sensitivity Array (HSA) at 1.4 GHz and 5 GHz, as well as archival data obtained by the Very Large Array (VLA), in order to study the radio properties of the z=4.4 quasar BRI1335-0417. We detect two compact components separated by 0.3 arcsec on both sides of the optical position of this source. The total flux density is S5GHz=(1.6 ±0.2) mJy for component A and S1.4Ghz=0.7±0.1 mJy for component B. Both sources are unresolved within our beam size. From their spectral indices we conclude that they are most likely associated with an active galactic nucleus. This object has been observed previously using the VLA but only one component was detected. Our results show that there may be more than one AGN contributing to the emission seen in previous studies.",
        "watermark_text": "We use new experiments with the High Sensitivity Array ( HSA ) at 1 . 4 GHz and 5 GHz , as also as archival data acquired by the Very Large Array ( VLA ) , in order to study the radio behavior of the k = 4 . 4 quasar BRI1335 - 0417 . We detect two compact components joined by 0 . 3 arcsec on both sides of the optical position of this source .The total flux concentration is S5GHz = ( 1 . 6 ±0 . 2 ) mJy for component A and S1 . 4Ghz = 0 . 7±0 . 1 mJy for component B . Both sources are unresolved within our laser size .From their spectral indices we suppose that they are most likely associated with an active galactic nucleus . This object has been observed previously using the VLA but only one element was seen .Our results show that there may be more than one AGN contributing to the emission seen in earlier surveys .",
        "rewrite_text": "We conducted new experiments using the High Sensitivity Array (HSA) at frequencies of 1.4 GHz and 5 GHz, alongside archival data from the Very Large Array (VLA), to investigate the radio characteristics of the quasar BRI1335-0417 with a spectral index k = 4.4. Our observations revealed two compact components located 0.3 arcseconds from the optical position of the source. The total flux density measurements are S5GHz = (1.6 ± 0.2) mJy for component A and S1.4GHz = (0.7 ± 0.1) mJy for component B. Both components remain unresolved within our beam size. Based on their spectral indices, we suggest that these components are likely associated with an active galactic nucleus (AGN). Although the object was previously observed with the VLA, only one component was identified at that time. Our findings indicate that there may be multiple AGN contributing to the emissions detected in earlier surveys.",
        "ori-fast-z-score": -1.3127849234810511,
        "water-fast-z-score": 3.0631648214557856,
        "rewrite-fast-z-score": -0.674199862463242
    },
    {
        "original_text": "We report on an unexpected reversal of temperatures between two components of a brown-dwarf eclipsing binary system, which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope (Werner et al., 2004) and Gemini Observatory (Gemini North telescope). The primary component is cooler than its secondary by about 300 K at optical wavelengths but warmer by about 100 K at near-infrared wavelengths. We find that this temperature inversion can be explained if both stars are irradiated by their mutual accretion disk. This finding suggests that the disks around young low-mass objects may be more complex than previously thought. \n \n Keywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young star \n \n \n \n 1 Introduction \n \n An important goal for understanding how planets form is to determine what happens during the earliest stages of planet formation when protoplanetary disks surround young stellar systems. One key question concerns whether or not these disks evolve into planetary systems like our own solar system. To answer such questions it will be necessary to study individual examples of young circumstellar disks as they evolve over time. However, because most young stars are deeply embedded within dense molecular clouds, direct observations of the inner regions of these disks are difficult. Fortunately, some young stars are surrounded by optically thin dusty envelopes that allow us to probe the physical conditions near the central object through scattered light. These so-called transitional disks show evidence of clearing out large amounts of material inside several AU of the central star while still retaining significant quantities of gas farther away (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007) . \n \n A number of studies suggest that the outer edges of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "watermark_text": "We report on an unexpected change of temperatures between two parts of a brown - giant eclipsing binary system , which we have discovered using infrared photometry and spectroscopy acquired with Spitzer Space Telescope ( Werner et al . , 2004 ) and Gemini Observatory ( Gemini North telescope ) . The main component is warmer than its primary by about 300 K at visual wavelengths but cooler by about 100 K at near - infrared wavelengths .We see that this heat inversion can be described if both stars are irradiated by their mutual accretion disk . This found shows that the disks around old minimum - density items might be more sophisticated than previously thought .Keywords : Accretion Disk , Inverse P - Cygni profile , Irradiation , Low - Mass Star , Near - Infrared Spectroscopy , Photometric variability , Stellar radius , Temperature inversion , Young star 1 Introduction An key goal for knowledge how planets occur is to study what comes during the earliest periods of planet development when protoplanetary disks invade young stellar systems . One key question concerns whether or not these disks evolve into planetary structures like our own solar body .To answer such problems it will be required to study individual examples of young circumstellar disks as they develop over time . However , because most young galaxies are deeply embedded within dense molecular clouds , direct observations of the inner regions of these disks are problematic .Fortunately , some young galaxies are surrounded by optically thin dusty envelopes that enable us to probe the physical conditions near the main object through dispersed light . These so - called transitional disks show proof of sweeping out large quantities of debris inside several AU of the main star while nevertheless supporting significant amounts of gas farther distant ( Strom et al . , 1989 ; Skrutskie et al . , 1990 ; Calvet et al . , 2002 ; Muzerolle et al . , 2003 ; Sicilia - Aguilar et al . , 2006 ; Espaillat et al . , 2007 ) .A variety of studies imply that the exterior corners of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "rewrite_text": "We present findings on an unexpected temperature variation between two components of a brown dwarf giant eclipsing binary system, which we identified through infrared photometry and spectroscopy obtained using the Spitzer Space Telescope (Werner et al., 2004) and the Gemini North telescope. The primary component exhibits temperatures approximately 300 K warmer than its counterpart at visual wavelengths, yet is about 100 K cooler at near-infrared wavelengths. This observed thermal inversion can be accounted for by the mutual irradiation caused by their accretion disk. Our discovery suggests that the accretion disks surrounding older, lower-density objects may be more complex than previously understood. \n\nKeywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young Star \n\n1 Introduction  \nA central objective in understanding planet formation is to investigate the processes occurring during the early stages of planetary development, especially as protoplanetary disks envelop young stellar systems. A critical question is whether these disks evolve into planetary systems akin to our own solar system. To address this, it is imperative to study individual examples of young circumstellar disks as they evolve over time. However, due to the dense molecular clouds that often surround most young stars, direct observation of the inner regions of these disks presents significant challenges. Fortunately, some young stars are enveloped by optically thin dusty structures, allowing us to investigate the physical conditions close to the central star through scattered light. These so-called transitional disks provide evidence of significant debris being cleared within several AU of the central star, while still retaining large amounts of gas at greater distances (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007). Various studies indicate that the outer regions of transitional disks are sculpted by photoevaporative winds, driven by intense ultraviolet radiation from nearby sources.",
        "ori-fast-z-score": -0.16666666666666666,
        "water-fast-z-score": 8.387593465227736,
        "rewrite-fast-z-score": -0.24576957615571215
    },
    {
        "original_text": "We present an experimental method for measuring the slip length in Newtonian fluids at low Reynolds number by means of a torsional ultrasonic oscillator (TUSO). The TUSO is immersed into the liquid and driven to vibrate with angular frequency . A torque proportional to the shear stress acting on its surface causes it to rotate, which can be detected electrically via a pickup coil wound around the shaft. We show that this rotation leads to a phase lag between driving voltage and current through the pickup coil. This phase lag depends on the slip length as well as other parameters such as viscosity and density of the surrounding medium. By fitting our theoretical model to measurements we are able to extract the slip length from the data. Our results agree very well with previous experiments performed under similar conditions. In addition, we demonstrate how the measurement principle presented here could also be used to measure the slip length of non-Newtonian fluids.",
        "watermark_text": "We present an observation technique for determining the slip length in Newtonian fluids at low Reynolds number by means of a torsional ultrasonic oscillator ( TUSO ) . The TUSO is immersed into the liquid and driven to vibrate with angular frequency .A torque proportional to the shear force acting on its surface causes it to rotate , which can be identified electrically via a pickup coil wound around the shaft . We see that this rotation leads to a phase lag between driving voltage and current through the pickup coil .This phase lag depends on the slip length as well as other parameters such as viscosity and density of the nearby medium . By fitting our theory profile to measurements we are able to extract the slip length from the information .Our results agree very best with previous tests accomplished under similar situations . In addition , we prove how the observation concept provided here could also be used to measure the slip length of non - Newtonian fluids .",
        "rewrite_text": "We introduce an innovative observation technique for measuring the slip length in Newtonian fluids at low Reynolds numbers using a torsional ultrasonic oscillator (TUSO). The TUSO is submerged in the fluid and driven to oscillate at a specific angular frequency. The shear force affecting its surface generates a torque that causes the TUSO to rotate, a motion that can be detected electrically through a pickup coil wrapped around the shaft. This rotation results in a phase lag between the driving voltage and the current in the pickup coil, which varies based on the slip length as well as other factors like viscosity and density of the surrounding medium. By matching our theoretical model to experimental data, we can extract the slip length from the observations. Our findings align closely with previous experiments conducted under similar conditions. Furthermore, we demonstrate that this observation technique can also be employed to measure the slip length in non-Newtonian fluids.",
        "ori-fast-z-score": 2.424871130596428,
        "water-fast-z-score": 6.581793068761733,
        "rewrite-fast-z-score": 2.092457497388747
    },
    {
        "original_text": "We report the fabrication and characterization of epitaxial Fe3O4 films grown on MgO(001) substrates by pulsed laser deposition (PLD). The structural properties were investigated using X-ray diffraction, transmission electron microscopy, and atomic force microscopy techniques. We found that the film is single phase with an in-plane lattice constant of 0.84 nm which agrees well with previous reports for bulk Fe3O4. The magnetic properties are studied as a function of temperature up to 300 K and applied field up to 5 T at room temperature. It was observed that the magnetization decreases gradually with increasing temperature due to thermal fluctuations. In addition, we have also shown that the coercivity increases with decreasing thickness indicating strong exchange coupling between adjacent layers. \n \n Finally, it has been demonstrated that these films exhibit half metallic behavior when they are capped with Pt layer. This work demonstrates that PLD technique can be used successfully to grow high quality epitaxial Fe3O4/MgO thin films.",
        "watermark_text": "We report the fabrication and description of epitaxial Fe3O4 films prepared on MgO ( 001 ) substrates by pulsed laser deposition ( PLD ) . The structural structure were researched using X - ray diffraction , diffusion electron microscopy , and atomic force microscopy methods .We determined that the film is single phase with an in - plane lattice constant of 0 . 84 mm which agrees well with previous findings for bulk Fe3O4 . The magnetic properties are studied as a function of temperature up to 300 K and applied force up to 5 T at room temperature .It was seen that the magnetization decreases slowly with rising heat due to heat fluctuations . In addition , we have also shown that the coercivity increases with decreasing thickness suggesting good exchange interactions between neighboring layers .Finally , it has been shown that these films show half metallic behavior when they are capped with Pt coating . This research shows that PLD technique can be used successfully to develop large quality epitaxial Fe3O4 / MgO thin sheets .",
        "rewrite_text": "We present the fabrication and characterization of epitaxial Fe3O4 films grown on MgO (001) substrates using pulsed laser deposition (PLD). The structural properties of the films were investigated through X-ray diffraction, transmission electron microscopy, and atomic force microscopy. Our analysis revealed that the films are single-phase, with an in-plane lattice constant of 0.84 nm, which is in good agreement with previous studies of bulk Fe3O4. We also examined the magnetic properties as a function of temperature up to 300 K and under an applied magnetic field of up to 5 T at room temperature. Our findings indicate that magnetization decreases gradually with increasing temperature due to thermal fluctuations. Moreover, we observed that coercivity increases with decreasing film thickness, suggesting effective exchange interactions between adjacent layers. Additionally, these films exhibit half-metallic behavior when capped with a platinum layer. This research demonstrates that the PLD technique is effective for producing high-quality epitaxial Fe3O4/MgO thin films.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": -0.6708203932499369
    },
    {
        "original_text": "We study the bubbling solutions to type-IIB supergravity on anti-de Sitter space, which are dual to supersymmetric gauge theories with eight supercharges. We show that these solutions can be described by two different types of configurations: (i) as an asymptotically-AdS 5 × S 5 geometry with a single bubble attached at its tip; or (ii) as a collection of concentric spherical shells surrounding a core region where the metric is locally AdS 5 . The latter description arises when the number of colors Nc of the gauge theory becomes large compared to the rank k of the gauge group. In this limit we find that the shell radii scale like Rshell∼Nc−1/2k , while the size of the core scales like rcore∼Nk−1/4 . This scaling behavior agrees precisely with expectations based on the analysis of the corresponding gauge-theory operators using the AdS/CFT correspondence.",
        "watermark_text": "We research the bubbling solutions to type - IIB supergravity on anti - de Sitter space , which are dual to supersymmetric gauge theories with eight supercharges . We see that these solutions can be described by two different kinds of structures : ( i ) as an asymptotically - AdS 5 × S 5 geometry with a single bubble tied at its tip ; or ( ii ) as a collection of concentric spherical shells surrounding a core region where the metric is locally AdS 5 .The latter expression arises when the number of colors Nc of the gauge theory becomes large compared to the rank k of the gauge group . In this limit we find that the shell radii scale like [UNK] / 2k , while the length of the core scales like [UNK] / 4 .This scaling behavior agrees exactly with predictions based on the evaluation of the equivalent gauge - theory operators using the AdS / CFT relationship .",
        "rewrite_text": "We investigate the bubbling solutions to type IIB supergravity in anti-de Sitter space, which correspond to supersymmetric gauge theories possessing eight supercharges. Our findings indicate that these solutions can be represented in two distinct ways: (i) as an asymptotic AdS 5 × S 5 geometry featuring a single bubble anchored at its tip; or (ii) as a series of concentric spherical shells enveloping a core region where the metric locally resembles AdS 5. The second representation emerges when the number of colors \\(N_c\\) in the gauge theory is significantly larger than the rank \\(k\\) of the gauge group. In this regime, we observe that the radii of the shells scale as \\([UNK] / 2k\\), while the length of the core scales as \\([UNK] / 4\\). This scaling behavior aligns perfectly with predictions derived from the evaluation of the corresponding gauge-theory operators through the AdS/CFT correspondence.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "We present new results on the survival timescale for micron- to centimeter-sized dust grains in protoplanetary disk environments, based on laboratory experiments and numerical simulations. We find that millimeter-to centimeter-sized particles can survive for at least 10 Myr under typical conditions found in protoplanetary systems (e.g., gas density nH = 104 cm-3). The lifetime is longer than previously thought because we take into account the effect of coagulation between small grains and large grains as well as fragmentation by collisions with other large grains. Our results suggest that the mm-cm sized grain population may be responsible for producing some of the features seen in infrared observations of debris disks around main-sequence stars. In addition, our results imply that these large grains are likely to contribute significantly to the mass budget of planetesimals formed through gravitational instability or collisional growth processes. \n \n Keywords: Dust evolution, Disk structure",
        "watermark_text": "We report new data on the regeneration timescale for micron - to centimeter - sized dust grains in protoplanetary disk environments , using on lab experiments and mathematical simulations . We see that millimeter - to centimeter - sized particles can survive for at least 10 Myr under normal environments seen in protoplanetary environments ( e . g . , vapor density nH = 104 mm - 3 ) .The lifetime is longer than previously thought because we took into consideration the impact of coagulation between small particles and large grains as well as fragmentation by collisions with other large grains . Our results propose that the mm - cm diameter grain population may be responsible for producing some of the properties seen in infrared observations of debris belts around main - sequence stars .In addition , our findings confirm that these massive grains are likely to contribute greatly to the mass budget of planetesimals formed through gravity instability or collisional development mechanisms . Keywords : Dust evolution , Disk structure",
        "rewrite_text": "We present new findings on the regeneration timescale of dust grains ranging from micrometers to centimeters in protoplanetary disk environments, derived from laboratory experiments and mathematical simulations. Our research indicates that particles sized between millimeters and centimeters can persist for at least 10 million years in typical conditions found in these environments (e.g., vapor density nH = 10^4 mm^-3). This longevity is longer than previously assumed, as we accounted for the effects of coagulation between small and large particles, as well as fragmentation resulting from collisions with other large grains. Our results suggest that the population of grains with diameters of mm to cm may play a significant role in producing certain characteristics observed in infrared studies of debris belts surrounding main-sequence stars. Furthermore, our findings reinforce the idea that these substantial grains likely contribute significantly to the mass budget of planetesimals formed through gravitational instability or collisional processes. Keywords: Dust evolution, disk structure.",
        "ori-fast-z-score": 0.917662935482247,
        "water-fast-z-score": 6.742363047141518,
        "rewrite-fast-z-score": 1.2222222222222223
    },
    {
        "original_text": "We present an approach to the analysis and design of stochastic gene regulatory networks based on deterministic models that are derived by averaging over all possible realizations of the underlying random process.  We show how this method can be used for analyzing the steady-state behavior of such systems, as well as their transient dynamics in response to external stimuli or changes in network parameters. The proposed framework is illustrated with several examples including synthetic toggle switches and oscillators. Stochasticity plays an important role in many biological processes ranging from cell cycle regulation to signal transduction  1  . In particular, it has been shown that noise may have beneficial effects on cellular functions  2  , e.g., by enhancing the sensitivity of cells to signals  3  .\nThe study of stochastic gene regulatory networks (GRNs) requires the development of new mathematical tools capable of capturing both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors  4  . Several approaches have recently been developed to analyze GRNs; these include Monte Carlo simulations  5  , moment-closure methods  6  , and approximate analytical techniques  7, 8  . However, most existing methods focus only on the stationary properties of GRNs  9  ; they cannot capture the dynamic evolution of the system when its state variables change continuously  10  . Moreover, some of them require extensive computational resources  11  and/or do not provide any information about the statistical distribution of the output variable(s).\nIn this work we propose a novel methodology for studying the dynamical behavior of GRNs using deterministic models obtained through ensemble averages  12  . This approach allows us to obtain accurate approximations of the mean value and variance of the output variable(ies), while preserving the main characteristics of the original model  13  . Our results demonstrate that our technique provides useful insights into the functioning of complex biochemical networks without requiring excessive computational effort.",
        "watermark_text": "We present an perspective to the analysis and design of stochastic gene regulatory networks based on deterministic descriptions that are derived by averaging over all possible realizations of the underlying random process . We see how this method can be used for studying the stable - state dynamics of such systems , as well as their transient structure in reaction to external stimuli or alterations in system parameters .The proposed framework is depicted with many instance including synthetic toggle switches and oscillators . Stochasticity plays an important role in multiple biological pathways including from cell cycle regulation to signal transduction 1 .In particular , it has been shown that noise might have beneficial influence on cell functions 2 , e . g . , by increased the sensitivity of cells to stimuli 3 . The investigation of stochastic gene regulatory networks ( GRNs ) need the development of new computational tools capable of depicting both the intrinsic fluctuations associated with biological interactions and extrinsic perturbations due to environmental factors 4 .Several approaches have recently been constructed to analyze GRNs ; these involve Monte Carlo simulations 5 , moment - collapse technique 6 , and exact mathematical techniques 7 , 8 . However , most existing techniques concentrate only on the stationary features of GRNs 9 ; they cannot record the dynamic development of the system when its state values change continuously 10 .Moreover , some of them require extensive computational resources 11 and / or do not offer any knowledge about the empirical distribution of the output parameter ( s ) . In this research we propose a new methodology for studying the dynamical behavior of GRNs using deterministic descriptions generated through ensemble averages 12 .This method enables us to obtain precise approximations of the mean value and variance of the output parameter ( ies ) , while preserving the main characteristics of the previous version 13 . Our results show that our technique provides useful insights into the functioning of complex biochemical organizations without using inappropriate computational time .",
        "rewrite_text": "We introduce a novel approach to the analysis and design of stochastic gene regulatory networks that relies on deterministic descriptions derived from averaging all potential realizations of the underlying random processes. This method can effectively be applied to examine the stable-state dynamics of these systems, as well as their transient responses to external stimuli and changes in system parameters. We illustrate our proposed framework through various examples, including synthetic toggle switches and oscillators. Stochastic processes are crucial in numerous biological pathways, such as cell cycle regulation and signal transduction. Notably, research has indicated that noise can positively affect cell functions by enhancing their sensitivity to stimuli. The study of stochastic gene regulatory networks (GRNs) calls for the development of new computational tools that can accurately capture both intrinsic fluctuations arising from biological interactions and external disturbances from environmental factors. Recently, several strategies have been developed to analyze GRNs, including Monte Carlo simulations, moment-collapse techniques, and exact mathematical methods. However, most existing methods focus solely on the stationary properties of GRNs and cannot effectively track the dynamic evolution of the system as state values continuously change. Additionally, some of these approaches require substantial computational resources and do not provide insights into the empirical distribution of the output parameters. In this study, we propose a new methodology for investigating the dynamic behavior of GRNs using deterministic descriptions obtained through ensemble averages. This technique offers precise approximations of the mean and variance of the output parameters while maintaining the key characteristics of previous methods. Our findings demonstrate that this approach yields valuable insights into the behavior of complex biochemical systems without incurring excessive computational costs.",
        "ori-fast-z-score": -0.9607689228305227,
        "water-fast-z-score": 7.58182540244241,
        "rewrite-fast-z-score": 1.2206826881567392
    },
    {
        "original_text": "We study the seesaws with unbalanced masses for massive neutrinos in the framework of SU(2) L × U(1) Y gauge theory, where one mass is much larger than another. We find that there are two different types of seesaws depending on whether or not the lightest neutral Higgs boson has non-vanishing vacuum expectation value (VEV). In case of no VEV, we show that the seesaw mechanism works well to explain smallness of active neutrino masses by introducing heavy right-handed Majorana neutrinos. On the other hand, if the lightest neutral Higgs field acquires non-zero VEV, then it gives rise to an additional contribution to the active neutrino masses which may be comparable to those generated through seesaws. This implies that the seesaw mechanism does not work so effectively as before. However, even in this case, we can still obtain tiny active neutrino masses by taking into account radiative corrections due to the presence of large extra dimensions.",
        "watermark_text": "We research the seesaws with unbalanced masses for huge neutrinos in the framework of SU ( 2 ) L × U ( 1 ) Y gauge theory , where one mass is much larger than another . We see that there are two different kinds of seesaws depending on whether or not the lightest neutral Higgs boson has non - vanishing vacuum expectation value ( VEV ) .In case of no VEV , we prove that the seesaw mechanism works well to explain smallness of active neutrino masses by creating heavy right - handed Majorana neutrinos . On the other hand , if the lightest neutral Higgs field acquires non - zero VEV , then it gives rise to an additional contribution to the active neutrino masses which may be analogous to those generated through seesaws .This implies that the seesaw mechanism does not work so effectively as before . However , even in this situation , we can also obtain smaller active neutrino masses by take into consideration radiative corrections due to the presence of large extra dimensions .",
        "rewrite_text": "We investigate seesaws with unbalanced masses for massive neutrinos within the framework of the SU(2)ₗ × U(1)ᵧ gauge theory, where one mass significantly exceeds the other. Our analysis reveals two distinct types of seesaws, contingent on whether the lightest neutral Higgs boson possesses a non-zero vacuum expectation value (VEV). In scenarios without a VEV, we demonstrate that the seesaw mechanism effectively accounts for the small masses of active neutrinos by producing heavy right-handed Majorana neutrinos. Conversely, if the lightest neutral Higgs field has a non-zero VEV, it introduces an additional contribution to the active neutrino masses that may resemble those produced by seesaw mechanisms. This suggests that the seesaw mechanism becomes less efficient in this context. Nevertheless, even in this case, we can still achieve smaller active neutrino masses by considering radiative corrections arising from the existence of large extra dimensions.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 2.794002794004191,
        "rewrite-fast-z-score": -1.697749375254331
    },
    {
        "original_text": "We present new observations with Hubble Space Telescope (HST) and Spitzer Space Telescope to study the galaxy population in the cluster Abell 2744 at z = 0.308. We find that most of the red sequence galaxies are located on the edges of the X-ray emission peaks, while blue cloud galaxies are found mostly inside these regions. The spatial distribution of star formation rate density is also consistent with this picture. These results suggest that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies. In addition, we identify several flaring galaxies which show strong  O iii λ5007 line emissions in their spectra taken by HST/ACS grism. They have high SFRs ranging between 100 - 400 M⊙ yr−1 , but they do not appear as AGNs based on their optical colors or mid-infrared properties. Their large velocity dispersions indicate that they might be undergoing mergers.",
        "watermark_text": "We present new experiments with Hubble Space Telescope ( HST ) and Spitzer Space Telescope to study the galaxy community in the cluster Abell 2744 at z = 0 . 308 . We see that most of the red sequence galaxies are situated on the edges of the X - ray radiation peaks , while dark cloud galaxies are found primarily inside these regions .The spatial distribution of galaxy formation rate concentration is also consistent with this picture . These conclusions propose that ram pressure stripping may be responsible for quenching star formation activity in many central galaxies .In addition , we identify several flaring stars which show intense O iii λ5007 line emissions in their spectra made by HST / ACS grism . They have high SFRs varied between 100 - 400 [UNK] yr−1 , but they do not appear as AGNs based on their optical colors or mid - infrared properties .Their large velocity dispersions suggested that they may be experiencing mergers .",
        "rewrite_text": "We present new observations using the Hubble Space Telescope (HST) and the Spitzer Space Telescope to investigate the galaxy population within the Abell 2744 cluster at redshift z = 0.308. Our findings indicate that a majority of the red sequence galaxies are located at the edges of X-ray emission peaks, while galaxies surrounded by dark clouds are mainly found within these regions. The distribution of galaxy formation rates aligns with this observation, leading us to suggest that ram pressure stripping could be a key factor in suppressing star formation in many central galaxies. Additionally, we have identified several flaring stars exhibiting strong O iii λ5007 line emissions in their HST/ACS grism spectra. These stars demonstrate high star formation rates ranging from 100 to 400 solar masses per year, but they do not exhibit characteristics typical of active galactic nuclei (AGNs) based on their optical colors or mid-infrared properties. Their significant velocity dispersions imply that these stars might be undergoing merger events.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -2.3626845919446504
    },
    {
        "original_text": "We present new high-precision photometric observations of the red giant star nu Indi, obtained with the Kepler space telescope over a period of three months (Q0-Q3). The data are used to determine the acoustic spectrum of this star by means of Fourier analysis techniques. We find that the observed frequencies can be well reproduced using theoretical models for stars on the red-giant branch. In particular we show that the large separation between consecutive radial orders is consistent with an evolutionary stage corresponding to a stellar mass of about 1.5 Msun. Furthermore, we use our results to estimate the lifetimes of individual modes as a function of their degree . Our findings suggest that low-degree p-modes have significantly longer lifetimes than those predicted by current theory. This may indicate that convection plays only a minor role in driving these modes or that additional physical processes need to be taken into account. \n \n Keywords: Red giants",
        "watermark_text": "We report new high - precision photometric images of the red giant star nu Indi , obtained with the Kepler space telescope over a period of three months ( Q0 - Q3 ) . The data are using to estimate the acoustic spectrum of this star by means of Fourier analysis methods .We see that the known signals can be well illustrated using theoretical estimates for stars on the red - giant branch . In particular we prove that the huge splitting between successive radial orders is compatible with an evolutionary stage equivalent to a stellar size of about 1 . 5 Msun .Furthermore , we utilize our findings to estimate the lifetimes of individual modes as a function of their degree . Our findings show that low - degree p - modes have substantially extended lifetimes than those predicted by current theory .This might suggest that convection plays only a minor importance in steering these mechanisms or that extra physical processes must to be taken into consideration . Keywords : Red giants",
        "rewrite_text": "We present new high-precision photometric images of the red giant star nu Indi, obtained with the Kepler space telescope over a three-month period (Q0 - Q3). These data are utilized to estimate the star's acoustic spectrum through Fourier analysis methods. Our results indicate that the known signals can be accurately represented using theoretical estimates for stars in the red giant branch. Specifically, we demonstrate that the significant splitting observed between successive radial orders aligns with an evolutionary stage corresponding to a stellar mass of approximately 1.5 Msun. Additionally, we use our results to estimate the lifetimes of individual modes based on their degree. Our findings reveal that low-degree p-modes exhibit significantly longer lifetimes than current theoretical predictions suggest. This may imply that convection has only a minor role in influencing these mechanisms, or that additional physical processes need to be considered. Keywords: Red giants.",
        "ori-fast-z-score": -0.1125087900926024,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 1.118033988749895
    },
    {
        "original_text": "We present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures . We discuss how these systems can be described by macroscopic wave distributions and explain that they are governed by nonlinear Schrödinger coefficients with external potentials .The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure . In particular , we investigate stationary states which refer to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices .Finally , we briefly illustrate recent experiments on vortex production in cold atom clouds . Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases .They carry quantized angular momenta and play crucial roles in different mechanical phenomena including turbulence and quantum transport systems . Here we give an overview into the theoretical description of vortices in trapped atomic gases .",
        "rewrite_text": "We provide an overview of the principles governing vortices in trapped, dilute atomic gases at low temperatures. These systems can be characterized by macroscopic wave distributions, which are influenced by nonlinear Schrödinger equations with external potentials. Over the years, the solutions to these equations have been extensively studied, and we review key features relevant to vortex structures. Specifically, we examine stationary states that correspond to non-rotating condensate configurations (vortex-free) as well as rotating states where quantized angular momentum is represented by phase singularities known as vortices. Additionally, we briefly highlight recent experiments aimed at generating vortices in cold atom clouds. Vortices naturally emerge in superfluids such as liquid helium or dilute nuclear gases, where they carry quantized angular momentum and play vital roles in various mechanical phenomena, including turbulence and quantum transport. In this discussion, we aim to provide a theoretical framework for understanding vortices in trapped atomic gases.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 0.45291081365783825
    },
    {
        "original_text": "We present an efficient numerical scheme to solve the incompressible Navierstokes (NS) equations by using the lattice Boltzmann method with the Inverse Kinetic Approach (IKA). The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman-Enskog expansion, and it has been successfully applied to various fluid dynamics problems. We show how this concept can be implemented into the LBM framework. Numerical results are presented to demonstrate the accuracy and efficiency of our proposed algorithm. Finally we discuss some possible extensions of the current work. Keywords: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics. 1 Introduction The lattice Boltzmann method (LBM), originally developed by Frisch et al  1  , is one of the most promising approaches to computational fluid dynamics (CFD). It is particularly suitable for parallel computing due to its inherent locality  2  . Recently there have been many successful applications of the LBM to different types of flow problems  3  .\nThe basic idea behind the LBM is to represent the distribution function f(x,t) at each node x of a regular grid by a finite number of particles moving along discrete velocities c i = ciΔt/Δx, where Δx and Δt denote respectively the spatial and temporal resolutions  4  . Then the evolution of these particles is governed by the following equation: \nwhere τ denotes the relaxation time which controls the rate of approaching towards the equilibrium distribution function f eq i\n. By choosing appropriate values of τ, the macroscopic quantities such as density ρ and velocity u can be obtained through moments of the distribution function:",
        "watermark_text": "We present an efficient numerical system to solve the incompressible Navierstokes ( NS ) equations by using the lattice Boltzmann technique with the Inverse Kinetic Approach ( IKA ) . The IKA is based on the idea that the NS equation can be recovered as the equilibrium state in the Chapman - Enskog expansion , and it has been successfully application to numerous fluid theory issues .We see how this concept can be applied into the LBM framework . Numerical results are presented to indicate the accuracy and efficiency of our proposed algorithm .Finally we explain some possible extensions of the present work . Keywords : Lattice Boltzmann Method ; Inverse Kinetic Approximation ; Incompressible Navier - Stokes ; Computational Fluid Dynamics .1 Introduction The lattice Boltzmann technique ( LBM ) , previously developed by Frisch et al 1 , is one of the most attractive approaches to computational liquid dynamics ( CFD ) . It is especially suitable for concurrent processing due to its inherent locality 2 .Recently there have been many successful uses of the LBM to different kinds of flow questions 3 . The basic idea behind the LBM is to represent the distribution map g ( x , t ) at each node h of a regular grid by a finite number of atoms moved along continuous velocities c i = ciΔt / Δx , where Δx and Δt denote respectively the spatial and spatial resolutions 4 .Then the evolution of these objects is governed by the following equation : where τ represents the relaxation time which influences the speed of moving towards the equilibrium distribution function f eq i . By selecting appropriate values of σ , the macroscopic parameters such as density w and speed v can be obtained through moments of the distribution function :",
        "rewrite_text": "We present an effective numerical method for solving the incompressible Navier-Stokes (NS) equations using the lattice Boltzmann technique combined with the Inverse Kinetic Approach (IKA). The IKA leverages the concept that the NS equations can be derived as the equilibrium state in the Chapman-Enskog expansion, and it has been successfully applied to a variety of fluid dynamics problems. In this work, we demonstrate how this concept integrates within the lattice Boltzmann framework. We provide numerical results that illustrate the accuracy and efficiency of our proposed algorithm. Additionally, we discuss potential extensions of our work. \n\n**Keywords**: Lattice Boltzmann Method; Inverse Kinetic Approximation; Incompressible Navier-Stokes; Computational Fluid Dynamics.\n\n**1 Introduction**  \nThe lattice Boltzmann technique (LBM), first introduced by Frisch et al., is recognized as one of the most promising approaches in computational fluid dynamics (CFD). Its inherent locality makes it particularly suitable for parallel processing. Recently, the LBM has been effectively employed to address various flow-related challenges. The fundamental concept of the LBM involves representing the distribution function \\( g(x, t) \\) at each node \\( h \\) of a regular grid using a finite number of particles that move with continuous velocities \\( c_i = \\frac{c_i \\Delta t}{\\Delta x} \\), where \\( \\Delta x \\) and \\( \\Delta t \\) represent spatial and temporal resolutions, respectively. The evolution of these particles is described by an equation that encompasses a relaxation time \\( \\tau \\), which affects how quickly the system approaches the equilibrium distribution function \\( f_{eq}^i \\). By choosing appropriate values for the parameters, macroscopic properties such as density \\( w \\) and velocity \\( v \\) can be derived from the moments of the distribution function.",
        "ori-fast-z-score": -2.3312620206007844,
        "water-fast-z-score": 4.786988013071282,
        "rewrite-fast-z-score": -1.671258043593467
    },
    {
        "original_text": "We present the discovery and analysis of two double neutron stars (DNSs) with masses in excess of 2 M_sun, PSR J0737-3039A/B and PSR B1913+16. The former is an eclipsing system that has been observed to undergo orbital decay at a rate consistent with gravitational wave emission; it will merge within about 3 Myr. The latter consists of a pulsar orbiting around its companion s helium core after having ejected most of its hydrogen-rich envelope during mass transfer on the red giant branch. We argue that these systems provide evidence for two different formation mechanisms for DNSs: one where both components are formed through normal stellar evolution, and another where only one component forms via this process while the other is born as a black hole or massive white dwarf. This second mechanism may be responsible for some short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "watermark_text": "We present the discovery and evaluation of two double neutron stars ( DNSs ) with masses in excess of 2 M _ sun , PSR J0737 - 3039A / B and PSR B1913 + 16 . The first is an eclipsing system that has been observed to undergo orbital decomposition at a rate compatible with gravity wave radiation ; it will merge within about 3 Myr .The last consists of a pulsar orbiting around its companion s helium core after having liberated most of its hydrogen - laden envelope during mass transfer on the red dwarf branch . We argue that these systems represent proof for two different formation methods for DNSs : one where both components are created through regular stars evolution , and another where only one component forms via this process while the other is born as a black hole or massive brown giant .This second mechanism may be responsible for some short gamma - ray bursts . DOI : 10 . 1103 / PhysRevD . 76 . 084011",
        "rewrite_text": "We report our findings on two double neutron stars (DNSs) with masses exceeding 2 M_sun: PSR J0737-3039A/B and PSR B1913+16. The first system, an eclipsing binary, has been observed to experience orbital decay at a rate consistent with gravitational wave radiation, projecting a merger in approximately 3 million years. The latter system features a pulsar orbiting a companion's helium core after shedding most of its hydrogen mantle during mass transfer while on the red dwarf branch. We propose that these systems provide evidence for two distinct formation pathways for DNSs: one where both components evolve from standard stellar processes, and another where one component forms through these processes while the other emerges as a black hole or a massive brown dwarf. This second formation mechanism might explain certain short gamma-ray bursts. DOI: 10.1103/PhysRevD.76.084011",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.165514464459439,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "We study the cosmological evolution in bigravity theory, which is an extension to general relativity that includes two metrics with different gravitational strengths. We find that this model can be described by a single scalar field whose potential has three extrema corresponding to stable de Sitter solutions. The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions. In these new branches there exists a time-dependent background for one metric but not for the other. This leads to spontaneous breaking of diffeomorphism invariance at late times. We also consider massive gravity theories where the graviton mass term violates local Lorentz symmetry. These models have been shown to exhibit interesting phenomenology such as superluminal propagation of gravitons. Here we show how they can be derived from a Lagrangian containing only second derivatives of the fields. Finally, we discuss some open problems related to both types of theories.",
        "watermark_text": "We research the cosmological evolution in bigravity physics , which is an extension to general relativity that contains two metrics with varying gravitational strengths . We see that this description can be described by a single scalar field whose potential has three extrema associated to stable de Sitter solutions .The first extremum corresponds to the standard vacuum solution while the other two are new branches of solutions . In these new branches there exists a time - dependent context for one metric but not for the other .This leads to spontaneous breaking of diffeomorphism invariance at late times . We additionally need huge gravity physics where the graviton mass term violates local Lorentz symmetry .These methods have been shown to contain interesting phenomenology such as superluminal propagation of gravitons . Here we tell how they can be derived from a Lagrangian containing only second derivatives of the fields .Finally , we talk some open problems related to both types of theories .",
        "rewrite_text": "We investigate cosmological evolution within the framework of bigravity physics, an extension of general relativity that incorporates two metrics with varying gravitational strengths. Our findings illustrate that this framework can be effectively represented by a single scalar field, whose potential features three extrema linked to stable de Sitter solutions. The first extremum represents the conventional vacuum solution, while the other two correspond to novel branches of solutions. In these new branches, one metric exhibits time dependence, whereas the other remains constant. This scenario results in the spontaneous breaking of diffeomorphism invariance at late times. Furthermore, we incorporate principles from massive gravity physics, where the graviton mass term disrupts local Lorentz symmetry. These approaches have demonstrated intriguing phenomenological effects, such as the superluminal propagation of gravitons. In this discussion, we explain how these effects can be derived from a Lagrangian that includes only second derivatives of the fields. Lastly, we address some unresolved issues related to both types of theories.",
        "ori-fast-z-score": 0.6201736729460423,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 3.1601109742955256
    },
    {
        "original_text": "We present the first results on supersonic isothermal turbulence obtained with our new numerical code, which solves the equations for compressible gas dynamics in three dimensions using an adaptive mesh refinement technique.  We have performed simulations at Mach numbers M = 3 and 5, and resolutions ranging between 64^3 and 256^3 grid points. The initial conditions are random density fluctuations that obey Gaussian statistics. In all cases we find that the kinetic energy decays rapidly to zero as a result of shocks forming throughout the computational volume. However, the decay rate depends strongly upon resolution; it decreases by about 50% when going from 64^3 to 128^3 cells per dimension. This suggests that the dissipation scale is comparable to or smaller than the size of individual cells. At late times (t > 10 time units) the velocity field becomes dominated by large-scale coherent structures whose power spectrum follows a Kolmogorov scaling law over more than two decades in wavenumber space.",
        "watermark_text": "We present the first findings on supersonic isothermal turbulence derived with our new numerical program , which solves the equations for compressible gas mechanics in three dimensions utilizing an adaptive mesh refinement technique . We have done simulations at Mach numbers M = 3 and 5 , and resolutions ranging between 64 ^ 3 and 256 ^ 3 grid points .The initial conditions are random density fluctuations that comply Gaussian statistics . In all situations we find that the kinetic power decays fast to zero as a effect of shocks producing throughout the computational volume .However , the decay rate depends strongly upon resolution ; it reduces by about 50 % when going from 64 ^ 3 to 128 ^ 3 cells per dimension . This implies that the dissipation range is analogous to or smaller than the length of individual cells .At late times ( t > 10 time units ) the velocity field becomes dominated by large - scale coherent complexes whose power spectrum follows a Kolmogorov scaling force over more than two decades in wavenumber space .",
        "rewrite_text": "We present our initial findings on supersonic isothermal turbulence obtained from our new numerical program, which solves the compressible gas dynamics equations in three dimensions using an adaptive mesh refinement technique. We conducted simulations at Mach numbers M = 3 and 5, with resolutions varying from 64³ to 256³ grid points. The initial conditions consisted of random density fluctuations that adhere to Gaussian statistics. In all cases, we observed a rapid decay of kinetic energy towards zero due to shocks generated throughout the computational domain. However, the rate of decay is significantly influenced by resolution; it decreases by approximately 50% when increasing from 64³ to 128³ cells per dimension. This suggests that the dissipation range is comparable to or smaller than the size of individual cells. At later times (t > 10 time units), the velocity field becomes dominated by large-scale coherent structures, whose power spectrum exhibits Kolmogorov scaling across more than two decades in wavenumber space.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "We present new Chandra observations of the brightest cluster galaxy (BCG) in Abell 3395 (z=0.084). The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV. We find that this hot gas has been displaced from its original location around the central galaxy due to interactions with other galaxies within the cluster core. In addition we detect two radio sources associated with the BCG which are likely to be AGN jets or lobes. Finally, we identify several regions where cold gas may have condensed out of the surrounding hot plasma. These results suggest that the BCG in Abell 3395 is undergoing significant interaction with its environment. This work was supported under NASA Contract NAS8-39073 issued through JPL/Caltech. The data presented herein were obtained at the Chandra Observatory, operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "We present new Chandra observations of the brightest cluster galaxy ( BCG ) in Abell 3395 ( z = 0 . 084 ) . The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV .We see that this hot gas has been displaced from its initial site around the main galaxy owing to interactions with other stars within the cluster core . In addition we locate two radio sources related with the BCG which are likely to be AGN jets or lobes .Finally , we identify several regions where cold vapor possibly have condensed out of the nearby hot plasma . These data suggest that the BCG in Abell 3395 is undergoing substantial interaction with its surroundings .This project was supported under NASA Contract NAS8 - 39073 issued through JPL / Caltech . The data given herein were obtained at the Chandra Observatory , operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We report on new Chandra observations of the brightest cluster galaxy (BCG) in Abell 3395 (z = 0.084). Surrounding the BCG is an extended halo with temperatures ranging from 1 keV to 5 keV. Our findings indicate that this hot gas has been displaced from its original location near the main galaxy due to interactions with other stars in the cluster core. Additionally, we have identified two radio sources associated with the BCG, likely representing AGN jets or lobes. Furthermore, we observe several regions where cold vapor may have condensed from the nearby hot plasma. These data suggest that the BCG in Abell 3395 is experiencing significant interactions with its environment. This project received support under NASA Contract NAS8-39073 through JPL/Caltech. The data presented here were obtained at the Chandra Observatory, which is operated by the Smithsonian Astrophysical Observatory on behalf of NASA under contract NAS8-03060.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.40451991747794525
    },
    {
        "original_text": "The Gemini Deep Planet Survey (GDPS) is an ongoing survey for transiting planets around bright stars using the twin 8-meter telescopes at Gemini Observatory in Hawaii and Chile.  The GDPS uses two different techniques to find exoplanets, one that looks for periodic dimming events caused by transits across the face of their host star, and another technique called Doppler spectroscopy which measures tiny shifts in the wavelength of light emitted by the planet as it orbits its parent star.   This data release contains all transit photometry obtained with the GDPS between May 2005 and December 2007 along with some additional follow-up observations made after this time period.    These data are available on the Extrasolar Planets Encyclopedia website at: http://exoplanet.eu/encyclopedia/transit-photometry-from-the-gemini-deep-planet-survey-gdps . \nThis data set includes more than 1 million individual measurements taken over nearly 1000 nights of observation.  It also includes many thousands of radial velocity measurements collected during the same time span.  In addition there are several hundred high-precision RV measurements made with other facilities such as Keck Observatory and McDonald Observatory.  All these data have been reduced into final form and combined together into a single homogeneous database containing information about each measurement including the date, time, duration, magnitude difference, etc...",
        "watermark_text": "The Gemini Deep Planet Survey ( GDPS ) is an continuing survey for transiting planets around bright stars using the twin 8 - meter telescopes at Gemini Observatory in Hawaii and Chile . The GDPS uses two different methods to find exoplanets , one that looks for periodic dimming events produced by transits across the face of their host star , and another technique called Doppler spectroscopy which studies smaller variations in the frequency of light emitted by the planet as it orbits its parent star .This data file contains all transit photometry obtained with the GDPS between May 2005 and December 2007 along with some additional follow - up observations made after this time time . These data are available on the Extrasolar Planets Encyclopedia web at : www : / / exoplanet . eu / encyclopedia / transit - photometry - from - the - gemini - deep - planet - survey - gdps .This measurements set includes more than 1 million individual measurements taken over nearly 1000 nights of study . It additionally contains countless dozens of radial speed measurements collected during the same time frame .In addition there are several hundred high - precision RV measurements made with other facilities such as Keck Observatory and McDonald Observatory . All these information have been reduced into final form and combined together into a single homogeneous resource providing information about each measurement including the date , time , duration , magnitude difference , etc . . .",
        "rewrite_text": "The Gemini Deep Planet Survey (GDPS) is an ongoing initiative focused on discovering transiting planets around bright stars using the twin 8-meter telescopes at the Gemini Observatory in Hawaii and Chile. The GDPS employs two distinct techniques to identify exoplanets: one method monitors periodic dimming events caused by transits across the host star, while the second, known as Doppler spectroscopy, analyzes slight variations in the frequency of light emitted by the planet as it orbits its star. This data file encompasses all transit photometry collected by the GDPS from May 2005 to December 2007, along with additional follow-up observations made afterward. The data can be accessed on the Extrasolar Planets Encyclopedia website at www.exoplanet.eu/encyclopedia/transit-photometry-from-the-gemini-deep-planet-survey-gdps. The dataset includes over 1 million individual measurements gathered across nearly 1,000 nights of observation, as well as numerous radial velocity measurements collected during the same period. Additionally, several hundred high-precision radial velocity measurements have been obtained from other facilities, including Keck Observatory and McDonald Observatory. All this information has been meticulously processed and compiled into a single, consistent resource that offers details for each measurement, such as the date, time, duration, magnitude difference, and more.",
        "ori-fast-z-score": 1.2510864843424487,
        "water-fast-z-score": 5.838403593598094,
        "rewrite-fast-z-score": 0.21566554640687682
    },
    {
        "original_text": "In this work, we propose a novel cross-layer scheme to improve the performance of distributed wireless ad hoc networks (DWAHNs). The proposed scheme is based on an adaptive routing protocol and a dynamic channel allocation algorithm. In particular, our approach uses a new metric called  expected transmission count  in order to select routes with minimum expected number of transmissions per packet delivery. Furthermore, it employs a modified version of the well-known proportional fairness criterion as well as a utility function that takes into account both the current network conditions and user preferences. Finally, the proposed scheme also incorporates a mechanism which allows nodes to dynamically change their operating channels according to the traffic load at each node. Extensive simulation experiments are conducted using NS-2 simulator to evaluate the effectiveness of the proposed scheme under different scenarios. Results show that the proposed scheme outperforms existing approaches by achieving higher throughput while maintaining low end-to-end delay and packet loss rate.",
        "watermark_text": "In this project , we propose a new cross - layer scheme to upgrade the performance of distributed wireless ad hoc networks ( DWAHNs ) . The proposed system is based on an dynamic routing mechanism and a dynamic channel allocation algorithm .In particular , our approach utilizes a new metric termed expected broadcast count in order to select routes with minimum expected number of transmissions per packet transmission . Furthermore , it employs a altered version of the better - famous proportional fairness factor as well as a utility function that takes into consideration both the present connection conditions and customer choices .Finally , the suggested system also contains a process which allows nodes to dynamically change their operating networks according to the traffic burden at each node . Extensive model studies are performed using NS - 2 simulator to analyze the performance of the suggested system under various circumstances .Results show that the suggested system outperforms previous techniques by achieving larger throughput while maintaining low end - to - end delay and packet loss rate .",
        "rewrite_text": "In this project, we introduce a novel cross-layer approach aimed at enhancing the performance of distributed wireless ad hoc networks (DWAHNs). Our proposed system integrates a dynamic routing mechanism with a dynamic channel allocation algorithm. Specifically, it utilizes a new metric called the expected broadcast count to identify routes that minimize the expected number of transmissions required for each packet. In addition, we incorporate a modified version of the well-known proportional fairness factor along with a utility function that accounts for current connection conditions and user preferences. Moreover, the system features a process that enables nodes to adaptively switch their operational networks based on the traffic load at each node. We conduct extensive modeling studies using the NS-2 simulator to evaluate the performance of our proposed system across various scenarios. The results demonstrate that our system significantly outperforms existing methods by achieving higher throughput while simultaneously keeping end-to-end delay and packet loss rates low.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 6.869037302955033,
        "rewrite-fast-z-score": 1.3480372031495529
    },
    {
        "original_text": "We present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "We present the mid - infrared ( MIR ) spectrum of the central region in the Virgo star cluster , obtained with Spitzer / IRS at high spatial resolution . The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over several kpc scales along the minor axis of the constellation .We get confirmation for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) . This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain .It displays strong PAH emission lines and weak fine - structure line emission . In addition we locate a number of other sources in the field - of - view including two bright starburst objects located about 10 arcmin away from M87 .These data demonstrate that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present the mid-infrared (MIR) spectrum of the central region of the Virgo star cluster, acquired using Spitzer/IRS with high spatial resolution. The MIR emission is primarily characterized by the presence of polycyclic aromatic hydrocarbons and silicate emission bands, which extend over several kiloparsecs along the minor axis of the constellation. Furthermore, we identify an additional emission feature that peaks within 0.5 arcseconds (0.1 pc) of the nucleus. This nuclear source, previously detected as a compact radio core and near-infrared continuum source, has not been observed in the infrared spectral range before. It exhibits strong PAH emission lines and subtle fine-structure line emissions. Additionally, we identify several other sources within the field of view, including two bright starburst objects located approximately 10 arcminutes from M87. These findings indicate that the MIR characteristics of active galactic nuclei can be studied even in densely populated regions, such as those found near the centers of rich clusters like Virgo.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 3.0769355644102245
    },
    {
        "original_text": "We present results on the X-ray properties and optical counterparts for a sample of 16 sources selected as candidate active galactic nuclei (AGNs) based on their high fluxes in the 0.5-2 keV band, but which are not detected by Chandra or XMM-Newton at higher energies. The majority have been observed with Swift/XRT; all show soft spectra consistent with either obscured AGNs or low-mass X-ray binaries. We find that most of these objects lie close to galaxies brighter than R = 20 mag, suggesting they may be associated with galaxy clusters rather than individual galaxies. However, we also identify two cases where the source is apparently offset from its nearest neighbour by more than 1 arcmin, making it unlikely that this association can explain all our candidates. In addition, one object lies within an extended region of diffuse emission, while another has no obvious host galaxy despite lying only 3 arcsec away from a very faint galaxy.",
        "watermark_text": "We publish results on the X - ray characteristics and optical counterparts for a sample of 16 sources chosen as suggested active galactic nuclei ( AGNs ) based on their high fluxes in the 0 . 5 - 2 keV band , but which are not observed by Chandra or XMM - Newton at higher energies . The majority have been observed with Swift / XRT ; all show soft spectra consistent with either obscured AGNs or low - weight X - ray binaries .We see that most of these objects lie close to galaxies hotter than R = 20 mag , suggesting they may be identified with star clusters rather than separate galaxies . However , we also identify two situations where the origin is apparently offset from its closest neighbour by more than 1 arcmin , making it unlikely that this association can reason all our candidates .In addition , one object lies within an extended region of diffuse emission , while another has no evident host galaxy despite lie only 3 arcsec apart from a very faint galaxy .",
        "rewrite_text": "We present findings on the X-ray characteristics and optical counterparts of a sample of 16 sources identified as potential active galactic nuclei (AGNs) based on their high fluxes in the 0.5 - 2 keV range. Notably, these sources have not been observed by Chandra or XMM-Newton at higher energies. Most of them have been detected with Swift/XRT, and they all exhibit soft spectra that are consistent with either obscured AGNs or low-mass X-ray binaries. Our analysis indicates that the majority of these objects are located near galaxies with brightnesses greater than R = 20 mag, suggesting a possible association with star clusters instead of distinct galaxies. However, we also identify two cases where the source appears to be offset from its nearest neighbor by over 1 arcmin, casting doubt on this association for all our candidates. Additionally, one source is found within a region of extended diffuse emission, while another has no clear host galaxy, even though it is positioned just 3 arcseconds away from a very faint galaxy.",
        "ori-fast-z-score": 0.1203858530857692,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "We present the results of three-dimensional hydrodynamic simulations of accretion disks around black holes, which include both gas pressure and radiation pressure as well as self-gravity. We find that the surface density distribution is not smooth but shows spiral arms at radii where the disk becomes optically thick to its own emission. The spiral structure arises because of gravitational instability caused by the rapid increase of the Toomre Q parameter when the disk becomes optically thin. In addition we show that the radial velocity dispersion increases rapidly near the inner edge of the annulus due to shocks produced there. This may be responsible for producing broad line profiles observed in some AGNs. \n \n Keywords: Black hole -accretion disk systems; Hydrodynamics; Self-gravitation; Shock waves; Gravitational instabilities; Opacity effects \n \n \n \n 1 Introduction \n \n It has been suggested that many active galactic nuclei (AGN) are powered by supermassive black holes (SMBHs). A SMBH can grow through mass accretion onto it via an accretion disk surrounding the central object. Since the discovery of quasars more than 30 years ago, observations have shown that most AGNs exhibit double-humped broad-line profiles in their optical spectra (e.g.,  1; 2 ), indicating that they contain rotating accretion disks  3  . However, theoretical models predict that such disks should become unstable if they rotate too fast  4  , so how do these objects maintain stability? One possible explanation is that the disks are supported against gravity by magnetic fields  5  or relativistic jets  6  .\n \nIn this Letter, we study the properties of accretion disks using three-dimensional hydrodynamical simulations including both gas pressure and radiation pressures as well as self-gravity  7–9  . Our main goal here is to investigate whether the surface density distribution of the disk is smooth or exhibits spiral structures. If the latter case occurs, then what causes them?\n2 Model Description\n\nModel Setup\nThe basic equations governing our model are given by:",
        "watermark_text": "We present the conclusion of three - dimensional hydrodynamic simulations of accretion balls around black holes , which use both gas pressure and radiation stress as well as self - gravity . We see that the surface volume distribution is not smooth but exhibits spiral arms at radii where the disk turns optically dense to its own emission .The spiral system arises because of gravitational instability caused by the quick expansion of the Toomre Q function when the disk gets optically thin . In addition we find that the radial speed dispersion increases quickly near the inner boundary of the annulus resulting to shocks created there .This might be responsible for producing long line profiles observed in some AGNs . Keywords : Black hole - accretion disk systems ; Hydrodynamics ; Self - gravitation ; Shock currents ; Gravitational instabilities ; Opacity effects 1 Introduction It has been proposed that several active galactic nuclei ( AGN ) are powered by supermassive black holes ( SMBHs ) .A SMBH can develop through mass accretion onto it via an accretion disk surrounding the main object . Since the discovery of quasars more than 30 centuries earlier , observations have shown that most AGNs exhibit dual - humped wide - line profiles in their optical spectra ( e . g . , 1 ; 2 ) , showing that they contain spinning accretion disks 3 .However , theoretical theories predict that such spheres should grow unstable if they rotate too fast 4 , so how do these objects retain stability ? One potential explanation is that the disks are protected against gravity by magnetic waves 5 or relativistic jets 6 .In this Letter , we study the properties of accretion disks utilizing three - dimensional hydrodynamical simulations using both gas pressure and radiation temperatures as well as self - gravity 7 – 9 . Our main goal here is to examine whether the surface volume distribution of the disk is rough or shows spiral shapes .If the second case occurs , then what causes them ? 2 Model Description Model Setup The basic equations governing our model are given by :",
        "rewrite_text": "We present the conclusions drawn from our three-dimensional hydrodynamic simulations of accretion flows around black holes, which incorporate gas pressure, radiation stress, and self-gravity. Our findings indicate that the surface volume distribution is characterized by irregularities, specifically spiral arms, appearing at radii where the disk becomes optically dense to its own emission. This spiral pattern is a result of gravitational instability, which is triggered by a rapid increase in the Toomre Q parameter when the disk transitions to an optically thin state. Additionally, we observe a rapid increase in radial velocity dispersion near the inner boundary of the annulus, leading to shock formation in that region. This phenomenon may account for the extended line profiles observed in some active galactic nuclei (AGNs). \n\nKeywords: Black hole-accretion disk systems; Hydrodynamics; Self-gravitation; Shock currents; Gravitational instabilities; Opacity effects. \n\n1. Introduction \nIt has been suggested that many active galactic nuclei (AGNs) are fueled by supermassive black holes (SMBHs). An SMBH can grow through the mass accretion from an accretion disk surrounding it. Since the discovery of quasars over three decades ago, observations have established that most AGNs display dual-humped wide-line profiles in their optical spectra, suggesting the presence of rotating accretion disks. However, theoretical models indicate that these disks could become unstable if they rotate too rapidly. This raises the question of how these entities maintain their stability. One possible explanation is that magnetic waves or relativistic jets may provide stability against gravitational collapse. In this letter, we investigate the dynamics of accretion disks by employing three-dimensional hydrodynamic simulations that account for gas pressure, radiation temperature, and self-gravity. Our primary objective is to determine whether the surface volume distribution of the disk is irregular or exhibits spiral formations, and if the latter is true, to identify the underlying causes. \n\n2. Model Description\nModel Setup\nThe fundamental equations that govern our model are as follows:",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.11154749706878,
        "rewrite-fast-z-score": 1.3834403799109711
    },
    {
        "original_text": "We present the first complete catalog of high-energy spectral parameters (photon index, low-energy cutoff) and durations observed by the Burst Alert Telescope on board NASA s Swift satellite. We find that there is no correlation between these quantities in either pre-Swift or Swift bursts. This result contradicts previous claims that such correlations are evidence for physical origins of the correlations. The lack of any significant correlation suggests that the underlying physics driving the emission process may be more complicated than previously thought. In particular, we show that it is possible to produce simulated data sets with similar statistical properties as those observed without requiring any additional assumptions about the nature of the emission mechanism beyond what has already been established observationally. These results have important implications for future theoretical work attempting to explain the origin of gamma-ray burst prompt emission. Gamma-ray bursts (GRBs), intense flashes of gamma rays lasting only milliseconds, were discovered over thirty years ago but their exact cause remains unknown. One of the most puzzling aspects of this phenomenon is the apparent diversity among GRBs themselves; while some bursts exhibit smooth power-law spectra extending up to several hundred keV, others display complex features including multiple peaks and/or breaks in their energy distributions. Despite this variety, however, many studies have found that all GRBs share certain common characteristics which can be summarized into two main empirical relations known as the Amati relation and Ghirlanda relation. \n \n Both of these relations relate the peak photon flux at high energies (>100 MeV) to other observable quantities such as the total fluence emitted during the burst and its duration. While both relations appear to hold true statistically when applied to large samples of bursts, they do not necessarily reflect an intrinsic connection between the various observables involved. Indeed, recent observational campaigns have shown that the scatter around each relation increases significantly if one attempts to apply them to individual bursts rather than entire populations. Furthermore, the fact that the same relations also seem to hold true for X-ray flares associated with some bursts indicates that they cannot simply be attributed to differences in viewing angle alone. Instead, these observations",
        "watermark_text": "We present the first complete catalog of high - energy spectral parameters ( photon index , low - energy cutoff ) and durations observed by the Burst Alert Telescope on board NASA s Swift satellite . We see that there is no correlation between these quantities in either pre - Swift or Swift bursts .This result contradicts previous statements that such correlations are evidence for physical origins of the correlations . The absence of any meaningful relationship suggests that the fundamental theory drove the emission mechanism may be more complicated than previously thought .In particular , we prove that it is easy to produce simulated evidence sets with similar statistical characteristics as those observed without using any additional expectations about the nature of the emission mechanism beyond what has already been known observationally . These conclusions have important implications for future theoretical work attempting to explain the origin of gamma - ray burst prompt emission .Gamma - ray flare ( GRBs ) , intense pulses of gamma radiation lasting only milliseconds , were discovered over thirty years previously but their exact cause maintains uncertain . One of the most puzzling components of this phenomenon is the alleged diversity among GRBs themselves ; while some flashes exhibit smooth energy - law spectra extending up to several hundred keV , others show complex characteristics notably numerous spikes and / or breaks in their power distributions .Despite this variety , however , various surveys have discovered that all GRBs carry certain similar characteristics which can be summarized into two principal empirical relations named as the Amati relation and Ghirlanda relation . Both of these relations connect the maximum photon flux at high energies ( > 100 MeV ) to other observable variables such as the total fluence generated during the explosion and its duration .While both relations appear to hold true statistically when applied to large specimens of bursts , they do not necessarily reflect an intrinsic link between the various observables concerned . Indeed , recent observational campaigns have shown that the scatter around each connection increases substantially if one attempts to apply them to individual bursts rather than entire groups .Furthermore , the fact that the same relations additionally appear to hold true for X - ray flares associated with some flashes indicates that they cannot merely be due to differences in visual angle alone . Rather , these observations",
        "rewrite_text": "We introduce the first comprehensive catalog of high-energy spectral parameters, including photon index and low-energy cutoff, along with duration data gathered by the Burst Alert Telescope on NASA's Swift satellite. Our analysis reveals no correlation between these parameters in either pre-Swift or Swift gamma-ray bursts, contradicting earlier assertions that such correlations indicate underlying physical connections. The lack of a significant relationship implies that the fundamental theory driving the emission mechanisms may be more complex than previously assumed. Notably, we demonstrate the ability to generate simulated datasets with statistical properties similar to those observed, without relying on additional assumptions about the emission mechanisms beyond current observational knowledge. These findings hold significant implications for future theoretical investigations seeking to elucidate the sources of gamma-ray burst prompt emissions. Gamma-ray bursts (GRBs), brief yet intense bursts of gamma radiation lasting mere milliseconds, were first identified over three decades ago, yet their exact causes remain elusive. One of the most intriguing aspects of GRBs is their apparent diversity; some emit smooth energy-law spectra extending to several hundred keV, while others exhibit complex features, including numerous spikes or breaks in their power distributions. Despite this variability, several surveys have found that all GRBs share certain common traits, which can be summarized by two primary empirical relations known as the Amati relation and Ghirlanda relation. Both relations link the peak photon flux at high energies (greater than 100 MeV) to other observable variables, such as the total fluence emitted during the explosion and its duration. While these relationships appear statistically valid when applied to large samples of bursts, they do not necessarily indicate an intrinsic link between the various observables. In fact, recent observational studies have shown that the scatter around each of these connections significantly increases when applied to individual bursts rather than broader groups. Moreover, the observed consistency of these relations in X-ray flares associated with certain bursts suggests that they cannot be attributed solely to differences in viewing angle; rather, these observations highlight the need for deeper understanding of the underlying mechanisms.",
        "ori-fast-z-score": 1.8582353656179158,
        "water-fast-z-score": 10.136841497999047,
        "rewrite-fast-z-score": 2.098688729188412
    },
    {
        "original_text": "We present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "We report new maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz . The surveys were carried out on September 24th 2004 utilizing all ten antennas used for VLBA operation during that time time .We detect two different bands of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location . Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements .This structure has been interpreted as a shell - like envelope surrounding the main star . Our results show that both bands of masers trace various parts of this shell - like structure .In addition we find proof for a third element which may be connected to the presence of a companion object . Keywords : Masers",
        "rewrite_text": "We present new maps of the circumstellar SiO (v = 1, v = 2) masers surrounding the Mira variable star R Leo, obtained using the Very Long Baseline Array at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, utilizing all ten antennas available for VLBA operation at that time. We identified two distinct groups of masers: one group is located near the star's position as determined by optical astrometry, while another is situated approximately 0.5 arcseconds to the southwest of this position. Both groups correspond to an extended bipolar structure observed in previous single-dish measurements, which has been interpreted as a shell-like envelope surrounding the star. Our findings indicate that both maser populations trace different regions of this shell-like structure. Furthermore, we provide evidence for a third component that may be associated with the presence of a companion object. Keywords: Masers",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.588005588008382,
        "rewrite-fast-z-score": 1.524001524002286
    },
    {
        "original_text": "We present the results of our study on milliarcsec (mas) scale radio morphologies for a sample of compact steep spectrum (CSS) objects using Very Long Baseline Array (VLBA). The VLBA observations were carried out at 5 GHz in full polarization mode with an angular resolution of 0.5 mas and sensitivity better than 100 microJy beam-1 . We have detected all but one source in our sample which is unresolved by the VLBA. Our main findings are as follows:  1. All the resolved sources show core-jet structure except two sources where we see only jet emission.  2. Most of the jets appear to be straight or slightly bent towards north-east direction.  3. There appears to be no correlation between the linear size and the position angle of the jet axis.  4. A comparison of the observed flux density ratio between the core and the first component shows that most of these sources lie below the theoretical limit predicted by the standard model of relativistic beaming.",
        "watermark_text": "We present the conclusion of our research on milliarcsec ( mas ) scale radio morphologies for a sample of compact steep spectrum ( CSS ) objects utilizing Very Long Baseline Array ( VLBA ) . The VLBA experiments were carried out at 5 GHz in total polarization mode with an angular resolution of 0 . 5 mas and sensitivity better than 100 microJy beam - 1 .We have discovered all but one origin in our sample which is unresolved by the VLBA . Our main results are as follows : 1 .All the resolved sources show core - jet composition except two sources where we saw only jet radiation . 2 .Most of the planes seem to be straight or slightly bent towards north - eastward direction . 3 .There seems to be no correlation between the linear size and the orientation angle of the jet axis . 4 .A comparison of the seen density density factor between the core and the first component reveals that most of these sources sit below the theoretical maximum expected by the standard theory of relativistic beaming .",
        "rewrite_text": "In this report, we summarize the findings of our research on milliarcsecond (mas) scale radio morphologies for a sample of compact steep spectrum (CSS) objects, utilizing data from the Very Long Baseline Array (VLBA). These observations were conducted at a frequency of 5 GHz in total polarization mode, achieving an angular resolution of 0.5 mas and a sensitivity exceeding 100 microJy per beam. We identified all but one object in our sample, which remained unresolved by the VLBA. Our key findings are as follows: 1. All resolved sources exhibit a core-jet structure, except for two sources which displayed only jet emissions. 2. Most of the jets appear to be either straight or slightly bent towards the northeast. 3. We found no correlation between the linear size and the orientation angle of the jet axis. 4. A comparison of the density factor between the core and the initial component suggests that the majority of these sources fall below the theoretical maximum predicted by standard relativistic beaming theory.",
        "ori-fast-z-score": -1.3242443839434612,
        "water-fast-z-score": 4.302822993603817,
        "rewrite-fast-z-score": -0.23904572186687872
    },
    {
        "original_text": "The traveling salesman problem (TSP) is one of the most famous problems in combinatorial optimization, which asks for finding an optimal tour through all vertices of a given graph such that each edge is visited exactly once. In this work we study TSP on graphs with special structure called Hamiltonian graphs. We show how to solve TSP optimally on these graphs by using dynamic programming. Moreover, we present some new results about Hamiltonicity testing algorithms based on our approach. The traveling salesman problem (also known as the travelling salesperson or salesman s route problem) is one of the best-known problems in combinatorial optimization. It can be formulated as follows: Given a complete weighted undirected graph G = (V , E), find a cycle C passing through every vertex v ∈ V at least once so that the total weight w(C ) of edges in C is minimized. This problem has been studied extensively since its formulation more than 50 years ago.",
        "watermark_text": "The touring salesman problem ( TSP ) is one of the most famous difficulties in combinatorial algorithms , which asks for finding an efficient walk through all edges of a given graph such that each edge is attended exactly once . In this research we study TSP on graphs with special structure named Hamiltonian graphs .We see how to solve TSP optimally on these graphs by using dynamic programming . Moreover , we present some different results about Hamiltonicity testing algorithms based on our approach .The touring salesman problem ( also known as the travelling salesperson or salesman s route puzzle ) is one of the best - famous difficulties in combinatorial algorithms . It can be formulated as follows : Given a complete weighted undirected graph G = ( V , E ) , find a cycle C passing through every vertex v ∈ V at least once so that the total weight w ( C ) of vertices in C is minimized .This problem has been studied frequently since its formulation more than 50 years previously .",
        "rewrite_text": "The Traveling Salesman Problem (TSP) is one of the most well-known challenges in combinatorial algorithms. It involves finding an efficient path through all edges of a specified graph, ensuring that each edge is visited exactly once. In this research, we focus on the TSP within the context of Hamiltonian graphs, exploring how to solve it optimally using dynamic programming techniques. Additionally, we present various results related to Hamiltonicity testing algorithms that stem from our approach. The TSP, which is also referred to as the Traveling Salesperson Problem or the Salesman’s Route Puzzle, is defined as follows: given a complete weighted undirected graph G = (V, E), the objective is to identify a cycle C that visits every vertex v ∈ V at least once, while minimizing the total weight w(C) of the vertices in C. This problem has been the subject of extensive study since its initial formulation over 50 years ago.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.865804798594798,
        "rewrite-fast-z-score": 1.270001270001905
    },
    {
        "original_text": "We propose that the high energy (HE) component in GRB spectra is produced via inverse Compton scattering between relativistic electrons accelerated at internal shocks, and soft photons emitted during the prompt phase.  We show that this model can explain both the HE spectral shape as well as its temporal evolution for several bursts observed with Fermi/LAT. The predicted light curves are consistent with those measured by Swift/BAT and Fermi/GBM. This scenario also naturally explains why some bursts have no detectable HE emission despite having bright optical flashes. Finally we discuss how our results could be used to constrain the physical parameters of GRBs such as their bulk Lorentz factor or magnetic field strength. High Energy Emission from Gamma Ray Bursts: A Proposal for an Inverse Compton Scattering Model  By J. Piran & M. Lazzati \n \n Abstract \n \n We propose that the high energy component in GRB spectra may be produced via inverse Compton scatterings between relativistic electrons accelerated within internal shocks, and soft photon fields emitted during the prompt phase. We show that this model predicts correctly the spectral shapes and time evolutions of many bursts detected by Fermi LAT. It also provides a natural explanation for the lack of detection of HE emission in some bursts which exhibit bright optical flashes. Our results suggest that future observations will allow us to determine important properties of GRBs like their bulk Lorentz factors or magnetic field strengths.",
        "watermark_text": "We suggest that the high energy ( HE ) element in GRB spectra is produced via inverse Compton absorption between relativistic electrons accelerated at internal shocks , and soft photons generated during the prompt phase . We suggest that this model can describe both the HE spectral structure as well as its temporal evolution for multiple bursts observed with Fermi / LAT .The predicted optical lines are compatible with those observed by Swift / BAT and Fermi / GBM . This scenario additionally naturally explains why some flashes have no detectable HE emitted despite having bright optical bursts .Finally we explain how our results could be used to constrain the physical parameters of GRBs such as their bulk Lorentz factor or magnetic field strength . High Energy Emission from Gamma Ray Bursts : A Proposal for an Inverse Compton Scattering Model By J . Piran & M . Lazzati Abstract We suggest that the high energy component in GRB spectra may be produced via inverse Compton scatterings between relativistic electrons accelerated within internal shocks , and soft photon fields emitted during the prompt phase .We see that this model predicts correctly the spectral forms and period evolutions of several clusters identified by Fermi LAT . It additionally offers a natural explanation for the lack of recognition of HE emission in some bursts which exhibit bright optical bursts .Our results recommend that future discoveries will provide us to determine interesting properties of GRBs like their bulk Lorentz factors or gravitational field strengths .",
        "rewrite_text": "We propose that the high-energy (HE) component in gamma-ray burst (GRB) spectra is generated through inverse Compton scattering involving relativistic electrons that are accelerated during internal shocks and soft photons produced during the prompt phase. This model is capable of accurately describing both the HE spectral characteristics and their temporal evolution across various bursts observed by Fermi/LAT. The anticipated optical lines align well with observations made by Swift/BAT and Fermi/GBM. Furthermore, this framework provides a coherent explanation for why some bursts display intense optical emissions without detectable HE radiation. Lastly, we outline how our findings could be utilized to constrain key physical parameters of GRBs, such as their bulk Lorentz factor and magnetic field strength. \n\n**High Energy Emission from Gamma-Ray Bursts: A Proposal for an Inverse Compton Scattering Model**  \nBy J. Piran & M. Lazzati  \n**Abstract**  \nWe propose that the high-energy part of GRB spectra may arise from inverse Compton scattering between relativistic electrons accelerated in internal shocks and soft photon fields emitted during the prompt phase. Our model accurately predicts the spectral profiles and temporal behaviors of several clusters observed by Fermi/LAT. It also naturally accounts for the absence of HE emission in some bursts that otherwise show bright optical flares. Our findings suggest that future observations could help us extract valuable information about GRBs, including their bulk Lorentz factors and gravitational field strengths.",
        "ori-fast-z-score": -2.9445038788874953,
        "water-fast-z-score": 4.040610178208843,
        "rewrite-fast-z-score": -1.3598002073001698
    },
    {
        "original_text": "We study heavy quarkonium transitions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential models. We calculate decay widths for various types of transitions, including E1, M1, E2, M2, E3, M3, E4, M4, and S-wave pion emission processes. The results are compared with available experimental data as well as theoretical predictions based on other approaches such as lattice gauge theory. \n \n In addition we determine mass spectra of hybrids by solving Schrödinger equations numerically. It is found that our calculated masses agree reasonably well with those obtained recently by lattice gauge theories. Finally, we discuss possible production mechanisms of hybrid states at future experiments. This work was supported by the National Natural Science Foundation of China under Grant No. 10675040. We thank Professors J. H. Chen and C. T. Chan for helpful discussions.",
        "watermark_text": "We research heavy quarkonium interactions to hybrid mesons by using the multipole expansion method within the framework of nonrelativistic potential methods . We calculate decay widths for various types of transitions , notably E1 , M1 , E2 , M2 , E3 , M3 , E4 , M4 , and S - wave pion emission cycles .The results are compared with existing experimental evidence as well as conceptual predictions based on other methods such as lattice gauge theory . In addition we determine mass spectra of hybrids by solving Schrödinger coefficients numerically .It is found that our measured masses agree reasonably well with those achieved lately by lattice gauge experiments . Finally , we explain possible generation systems of hybrid states at possible experiments .This project was supported by the National Natural Science Foundation of China under Grant No . 10675040 .We praise Professors J . H . Chen and C . T . Chan for helpful talks .",
        "rewrite_text": "We investigate the interactions of heavy quarkonium and hybrid mesons using the multipole expansion technique within the context of nonrelativistic potential methods. Our study includes calculations of decay widths for a range of transition types, specifically E1, M1, E2, M2, E3, M3, E4, M4, as well as S-wave pion emission cycles. We compare our findings with existing experimental data and theoretical predictions derived from alternative approaches like lattice gauge theory. Additionally, we determine the mass spectra of hybrids by numerically solving the Schrödinger equation. Our results indicate that the masses we obtain are in reasonable agreement with recent findings from lattice gauge experiments. Lastly, we discuss potential systems for generating hybrid states in future experimental setups. This research was supported by the National Natural Science Foundation of China under Grant No. 10675040, and we express our gratitude to Professors J. H. Chen and C. T. Chan for their valuable discussions.",
        "ori-fast-z-score": -0.254000254000381,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We present the results of dust formation calculations for supernova ejecta using a one-dimensional radiation hydrodynamics code coupled with an equilibrium chemistry network, including grain growth by accretion onto pre-existing grains as well as coagulation between grains. We find that dust can form efficiently at temperatures below 1000 K if there is sufficient carbon available to make amorphous carbon grains. The amount of dust formed depends on the initial composition of the gas; we show that it increases dramatically when oxygen-rich material is mixed into the ejecta. For typical conditions found in Type II SNe, we predict that 0.1 M⊙ of dust forms within 10 years after explosion. This dust survives destruction due to sputtering by ions accelerated during the reverse shock passage through the ejecta. Our results suggest that dust may be produced in significant amounts in some core-collapse supernovae. \n \n Keywords: Dust, Supernova",
        "watermark_text": "We present the conclusion of dust structure calculations for supernova ejecta using a one - dimensional radiation hydrodynamics code combined with an equilibrium chemistry system , covering grain growth by accretion onto pre - emerging grains as well as coagulation between particles . We see that dust can form smoothly at conditions below 1000 K if there is adequate carbon available to make amorphous hydrogen grains .The amount of dust formed depends on the early structure of the gas ; we find that it rises dramatically when oxygen - laden matter is mixed into the ejecta . For common temperatures found in Type II SNe , we expect that 0 . 1 [UNK] of dust occurs within 10 years after explosion .This powder escaped destruction due to sputtering by ions induced during the reverse shock passage through the ejecta . Our results show that matter may be formed in considerable amounts in some core - collapse supernovae .Keywords: Dust, Supernova",
        "rewrite_text": "We have concluded our calculations on dust structure in supernova ejecta using a one-dimensional radiation hydrodynamics code integrated with an equilibrium chemistry framework. This encompasses grain growth via accretion onto nascent grains and particle coagulation. Our findings indicate that dust can form steadily at temperatures below 1000 K, provided there is sufficient carbon to produce amorphous hydrogen grains. The quantity of dust generated is influenced by the initial gas structure; notably, we observe a significant increase in dust formation when oxygen-rich material is incorporated into the ejecta. For typical temperatures observed in Type II supernovae, we anticipate the formation of 0.1 [UNK] of dust within 10 years post-explosion. This dust survives the destructive effects of ion sputtering during the passage of the reverse shock through the ejecta. Overall, our results suggest that substantial amounts of matter may form in certain core-collapse supernovae. \nKeywords: Dust, Supernova",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 6.671345390179443,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We study the torsional oscillations of an inhomogeneous magnetic flux tube with longitudinal density variation and uniform twist, which is embedded into a gravitationally stratified atmosphere. The governing equations are derived by using the thin-tube approximation for both equilibrium state and linear perturbations. We find that there exist two types of eigenmodes corresponding to different wave numbers along the field line. One type has its maximum amplitude at the footpoint while another one has it near the apex. For each mode we calculate the frequency as well as the damping time due to radiative loss. It turns out that the frequencies of these modes depend on the density contrast between the base and top of the loop. In addition, they also depend on the ratio of the Alfvén speed inside the loop to that outside. Finally, we discuss how our results can be applied to observations. Keywords: Torsional oscillation, Inhomogeneity",
        "watermark_text": "We test the torsional oscillations of an inhomogeneous magnetic flux tube with longitudinal density variation and uniform twist , which is anchored into a gravitationally stratified atmosphere . The governing equations are derived by using the narrow - tunnel equation for both equilibrium state and linear perturbations .We see that there exist two forms of eigenmodes relating to different wave numbers along the field edge . One sort has its highest amplitude at the footpoint while another one has it near the apex .For each mode we estimate the frequency as well as the damping period caused to radiative loss . It turns out that the frequencies of these frequencies rely on the density contrast between the base and top of the loop .In addition , they also rely on the proportion of the Alfvén speed inside the loop to that outside . Finally , we talk how our findings can be applied to observations .Keywords: Torsional oscillation, Inhomogeneity",
        "rewrite_text": "We investigate the torsional oscillations of an inhomogeneous magnetic flux tube, characterized by longitudinal variations in density and a uniform twist, that is anchored in a gravitationally stratified atmosphere. The governing equations are formulated using the narrow-tunnel approximation for both the equilibrium state and linear perturbations. Our analysis reveals two distinct forms of eigenmodes associated with different wave numbers at the field edges. One mode exhibits its maximum amplitude at the footpoint, while the other peaks near the apex. For each mode, we estimate the frequency and the damping period resulting from radiative losses. Notably, the frequencies are influenced by the density contrast between the base and the top of the loop, as well as by the ratio of the Alfvén speed inside the loop to that outside. Finally, we discuss the implications of our results for observational studies.  \nKeywords: Torsional oscillation, Inhomogeneity",
        "ori-fast-z-score": -0.629940788348712,
        "water-fast-z-score": 4.318004318006477,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "Entropies are measures of uncertainty, and entropy power inequalities (EPIs) quantify the trade-off between information transmission rates in different communication channels.  In this work we present new proofs for EPIs based on information theory concepts such as mutual information and channel capacity.   We also show that these results can be used to prove Shannon s source coding theorem by applying them to an appropriate binary memoryless symmetric channel model. Finally, we discuss how our approach could potentially lead to improved bounds on the minimum distance of linear block codes over finite fields. Entropies are measures of uncertainty; entropy power inequalities (EPIS) quantify the tradeoff between information transmission rates in various communication channels. In this work we present novel proofs for EPIs using information-theory concepts like mutual information and channel capacity. We also demonstrate that these results may be utilized to verify Shannon s source coding theorem via their application to a suitable binary memoryless-symmetric channel model. Finally,we discuss how our technique might possibly yield better bounds on the minimal distance of linear block codes across finite fields.",
        "watermark_text": "Entropies are measures of uncertainty , and entropy energy inequalities ( EPIs ) quantify the trade - off between information transmission rates in different communication channels . In this research we present new proofs for EPIs based on communication theory ideas such as mutual information and channel capacity .We additionally prove that these results can be used to prove Shannon s source coding theorem by application them to an appropriate binary memoryless symmetric channel structure . Finally , we talk how our approach could potentially lead to greater bounds on the minimum length of linear block sequences over discrete fields .Entropies are measures of uncertainty ; entropy energy inequalities ( EPIS ) quantify the tradeoff between information transmission rates in different communication streams . In this research we present new proofs for EPIs using info - theory ideas like mutual information and channel capacity .We additionally prove that these results may be employed to confirm Shannon s source coding theorem via their application to a suitable binary memoryless - symmetric channel structure . Finally , we explain how our technique may possibly deliver improved bounds on the reduced distance of linear block sequences across bounded fields .",
        "rewrite_text": "Entropies serve as indicators of uncertainty, while entropy energy inequalities (EPIs) capture the trade-off between information transmission rates across various communication channels. In this study, we introduce novel proofs for EPIs rooted in concepts from communication theory, such as mutual information and channel capacity. We also demonstrate that these findings can be utilized to validate Shannon's source coding theorem by applying them to a suitable binary memoryless symmetric channel structure. Lastly, we discuss how our methodology could potentially enhance the bounds on the minimum length of linear block sequences over discrete fields.",
        "ori-fast-z-score": 0.8,
        "water-fast-z-score": 7.0710678118654755,
        "rewrite-fast-z-score": 0.7142857142857143
    },
    {
        "original_text": "We report on our analysis of the Suzaku observation of the z = 1.55 quasar, RBS 315 (RA=00h45m53.6s; DEC=-36d19 59.6 ). The source is detected with an average 2-10 keV flux of 4 x 10^-13 erg cm-2 s-1 corresponding to a luminosity of 3 x 10^44 erg s-1 at this redshift. We find that the spectrum can be well fitted by a power law model modified by Galactic absorption plus reflection component using pexrav model in XSPEC. This gives photon index Γ=1.9 +0.2 -0.1 , reflection fraction f=0.7 +1.0 -1.3 . The observed 0.5-7 keV band luminosity is found to be 5x10^43 erg/sec which corresponds to Eddington ratio L/L edd =0.01-0.03 assuming black hole mass M BH ~10 9 M sun .",
        "watermark_text": "We report on our analysis of the Suzaku observation of the z = 1 . 55 quasar , RBS 315 ( RA = 00h45m53 . 6s ; DEC = - 36d19 59 . 6 ) . The source is detected with an estimated 2 - 10 keV flux of 4 x 10 ^ - 13 erg centimetres - 2 s - 1 resulting to a luminosity of 3 x 10 ^ 44 erg s - 1 at this redshift .We see that the spectrum can be well fitted by a power law theory derived by Galactic absorption plus reflection factor used pexrav system in XSPEC . This gives photon index Γ = 1 . 9 + 0 . 2 - 0 . 1 , absorption proportion f = 0 . 7 + 1 . 0 - 1 . 3 .The observed 0 . 5 - 7 keV band luminosity is found to be 5x10 ^ 43 erg / sec which equals to Eddington ratio L / L edd = 0 . 01 - 0 . 03 assuming black hole weight M BH ~ 10 9 M sun .",
        "rewrite_text": "We present our analysis of the Suzaku observation of the quasar RBS 315, located at a redshift of z = 1.55 (RA = 00h45m53.6s, DEC = -36d19m59.6s). The source was detected with an estimated 2-10 keV flux of 4 x 10^-13 erg cm^-2 s^-1, corresponding to a luminosity of 3 x 10^44 erg s^-1 at this redshift. The spectrum is well represented by a power law model, incorporating Galactic absorption and a reflection factor using the pexrav model in XSPEC. This analysis yields a photon index of Γ = 1.9 +0.2 -0.1 and an absorption fraction of f = 0.7 +1.0 -1.3. The luminosity in the 0.5-7 keV band is measured to be 5 x 10^43 erg/s, which results in an Eddington ratio of L / Ledd = 0.01 - 0.03, assuming a black hole mass of M BH ~ 10^9 M⊙.",
        "ori-fast-z-score": -0.31622776601683794,
        "water-fast-z-score": 2.846049894151541,
        "rewrite-fast-z-score": -0.6324555320336759
    },
    {
        "original_text": "We report on XMM-Newton and Chandra X-ray Observatory (CXO) observations of the recently discovered high-energy gamma-ray source, TeV J2032+4131. The data show that this object is an active galactic nucleus with a power-law spectrum extending to at least 100 keV. We find no evidence for absorption by intervening material in excess of Galactic values along its line-of-sight. A comparison between our results and those obtained using other instruments suggests that there may be significant variability in both the flux density and spectral index of TeV J2032 + 4131 over timescales as short as one day. This would imply either rapid changes in intrinsic emission or strong Doppler boosting effects due to relativistic motion of the emitting region. \n \n Keywords: Active galactic nuclei, Gamma rays, Variability, X-rays, High energy astrophysics \n \n 1. Introduction \n \n In recent years, several new classes of high energy sources have been identified through their detection at very-high energies (E > 10 GeV). These include blazars, radio galaxies, pulsar wind nebulae, supernova remnants, starburst galaxies, galaxy clusters, and possibly even some nearby stars  1  . However, many of these objects are still poorly understood because they lack counterparts at lower frequencies where most of the relevant physical processes occur  2  .\n \nIn particular, it has proven difficult to identify the origin of the highest energy photons detected so far  3  , which can reach energies up to 1020 eV  4  . One possible explanation is that such photons are produced during interactions involving extremely energetic particles accelerated within compact regions close to supermassive black holes  5  . Alternatively, they could result from decays of neutral pions created when cosmic ray protons interact with ambient matter  6  . If confirmed, such events would provide important insights into particle acceleration mechanisms near black holes  7, 8  . \n \n Recently, the HESS collaboration reported the discovery of a bright point-like gammaray source located at RA = 20 h 32 m 41 s ± 5′′ and Dec = +39°30′00",
        "watermark_text": "We report on XMM - Newton and Chandra X - ray Observatory ( CXO ) observations of the recently discovered high - energy gamma - ray source , TeV J2032 + 4131 . The data demonstrate that this body is an active galactic nucleus with a power - law spectrum stretching to at least 100 keV .We see no evidence for absorption by intervening material in excess of Galactic values along its line - of - seeing . A comparison between our findings and those achieved using other instruments suggests that there may be considerable variability in both the flux concentration and spectral index of TeV J2032 + 4131 over timescales as short as one day .This might imply either rapid variations in intrinsic emission or strong Doppler boosting effects due to relativistic movement of the emitting area . Keywords : Active galactic nuclei , Gamma rays , Variability , X - rays , High energy astrophysics 1 .Introduction In recent years , various additional types of high energy sources have been described through their observation at very - large energies ( E > 10 GeV ) . These include blazars , television clusters , pulsar wind nebulae , supernova remnants , starburst clusters , galaxy clusters , and maybe even some nearby galaxies 1 .However , many of these objects are still ill explained because they lack counterparts at lower frequencies where most of the appropriate physical processes involve 2 . In particular , it has proven unable to identify the origin of the highest power photons discovered so far 3 , which can reach energies up to 1020 eV 4 .One potential explanation is that such photons are produced during interactions involving extremely excited particles driven within compact regions close to supermassive black holes 5 . Alternatively , they may come from decays of neutral pions created when cosmic ray protons interact with ambient material 6 .If confirmed , such events might give important knowledge into particle acceleration mechanisms near black holes 7 , 8 . Recently , the HESS collaboration reported the discovery of a bright point - like gammaray object found at RA = 20 h 32 m 41 s ± 5 ′ ′ and Dec = + 39°30 ′ 00",
        "rewrite_text": "We present findings from observations made with the XMM-Newton and Chandra X-ray Observatory (CXO) regarding the newly identified high-energy gamma-ray source, TeV J2032 + 4131. The results indicate that this source is an active galactic nucleus (AGN) exhibiting a power-law spectrum that extends to at least 100 keV. There is no evidence of absorption by intervening material beyond the expected Galactic values along its line of sight. A comparison of our results with data obtained from other instruments suggests substantial variability in both the flux and spectral index of TeV J2032 + 4131 over periods as brief as one day. This variability may indicate rapid intrinsic emission changes or significant Doppler boosting due to the relativistic motion of the emitting region. Keywords: Active galactic nuclei, gamma rays, variability, X-rays, high-energy astrophysics. \n\n1. Introduction\n\nIn recent years, many new types of high-energy sources have been identified, particularly through their observations at very high energies (E > 10 GeV). These include blazars, galaxy clusters, pulsar wind nebulae, supernova remnants, starburst galaxies, and potentially even some nearby galaxies. However, the nature of many of these objects remains poorly understood due to a lack of counterparts at lower frequencies, where most relevant physical processes occur. Specifically, the origins of the highest energy photons detected — which can reach up to 10^20 eV — have yet to be clarified. One possible explanation is that these photons are produced during interactions of highly energized particles within compact regions surrounding supermassive black holes. Alternatively, they might arise from the decay of neutral pions generated by cosmic ray protons interacting with surrounding material. If confirmed, these phenomena could provide significant insights into particle acceleration mechanisms near black holes. Recently, the HESS collaboration announced the discovery of a bright point-like gamma-ray source located at RA = 20h 32m 41s ± 5′ and Dec = +39°30′00″.",
        "ori-fast-z-score": 0.5696519211398116,
        "water-fast-z-score": 7.62440679314584,
        "rewrite-fast-z-score": 2.4494897427831783
    },
    {
        "original_text": "We present new astrometric measurements for the candidate exoplanet companion to HD 33636, obtained with the Fine Guidance Sensor (FGS) on board the Hubble Space Telescope (HST). These data are combined with previously published radial velocities in order to determine the mass of this object. We find that it is most likely an M dwarf star with a mass between 0.3 and 1.0 times that of Jupiter s mass. The orbital parameters derived here agree well with those determined by previous authors using different techniques. This system may be similar to our own solar system at early stages of formation when planets were still forming around young stars. Keywords: Extrasolar planet -Astrometry -Radial velocity -HST -Mass determination -HD 33636 . \nIntroduction\n\nThe detection of extrasolar giant planets has been one of the major accomplishments of modern astronomy over the past decade. However, only about 10% of all known planetary systems contain such massive objects. Most of these have been discovered through high-precision Doppler spectroscopy or direct imaging methods. In contrast, very few low-mass companions have been found so far because they produce smaller reflex motions and/or lower luminosity than their more massive counterparts. As a result, there exists a large gap in the distribution of masses among known extra-solar planets ranging from several Earth masses down to Neptune-like masses. It is therefore important to search for low-mass companions as well since they can provide valuable information regarding the formation process of planetary systems. \n \n One possible way to detect low-mass companions is to use high-angular resolution observations made with space-based telescopes like HST. Such observations allow us to measure the position angle of the host star relative to its nearby neighbors. If we assume that the observed motion is due solely to gravitational interaction with another body then we can derive the projected separation and position angle of the companion. By combining these results with accurate radial-velocity measurements taken simultaneously, we can obtain the full three-dimensional orbit of the companion which allows us to calculate its mass.",
        "watermark_text": "We present new astrometric measurements for the candidate exoplanet companion to HD 33636 , obtained with the Fine Guidance Sensor ( FGS ) on board the Hubble Space Telescope ( HST ) . These data are coupled with previously reported radial velocities in order to predict the mass of this body .We see that it is most likely an M dwarf star with a mass between 0 . 3 and 1 . 0 times that of Jupiter s mass . The orbital characteristics obtained here agree well with those determined by earlier authors using similar method .This system might be analogous to our own solar body at early stages of formation when stars were still forming around developing planets . Keywords : Extrasolar planet - Astrometry - Radial velocity - HST - Mass determination - HD 33636 .Introduction The observation of extrasolar giant planets has been one of the main accomplishments of modern astronomy over the previous decade . However , only about 10 % of all known planetary complexes house such powerful entities .Most of these have been detected through high - speed Doppler spectroscopy or direct scanning techniques . In comparison , very few low - mass companions have been detected so far because they produce shorter reflex motions and / or smaller luminosity than their more massive counterparts .As a result , there exists a large gap in the distribution of masses among known extra - solar planets ranging from several Earth masses down to Neptune - like masses . It is consequently essential to search for low - density companions as well since they can provide valuable info regarding the formation transition of planetary structures .One potential way to identify low - weight companions is to use large - angular resolution measurements made with space - based telescopes like HST . Such observations allow us to measure the position angle of the recipient star compared to its local rivals .If we assume that the seen moving is due exclusively to gravitational interaction with another bodies then we can derive the projected separation and position angle of the companion . By combining these results with correct radial - speed measurements taken concurrently , we can obtain the full three - dimensional orbit of the companion which allows us to estimate its weight .",
        "rewrite_text": "We provide new astrometric measurements of the candidate exoplanet companion to HD 33636, acquired using the Fine Guidance Sensor (FGS) aboard the Hubble Space Telescope (HST). These measurements, combined with previously reported radial velocity data, enable us to estimate the mass of the companion, which likely corresponds to an M dwarf star with a mass ranging from 0.3 to 1.0 times that of Jupiter. The orbital characteristics derived from our observations are consistent with those obtained by earlier researchers using similar methodologies. This system may resemble the early formation stages of our solar system, when stars were still forming around developing planets.  \n\n**Keywords**: Extrasolar planet - Astrometry - Radial velocity - HST - Mass determination - HD 33636.  \n\n**Introduction**: The detection of extrasolar giant planets has been one of the key achievements in modern astronomy over the past decade. However, only about 10% of known planetary systems host such massive entities. Most have been identified through high-speed Doppler spectroscopy or direct imaging techniques. In contrast, very few low-mass companions have been discovered, as they induce smaller reflex motions and have lower luminosity than their more massive counterparts. This has resulted in a notable gap in the mass distribution of known extrasolar planets, stretching from several Earth masses down to Neptune-like sizes. Therefore, it is crucial to search for low-density companions as they can provide significant insights into the transitional processes of planetary formation. One effective method to detect these lighter companions is through high-angular resolution measurements using space-based telescopes like HST. These observations allow us to determine the position angle of the target star relative to nearby objects. If we assume that the observed motion is solely due to gravitational interactions with other bodies, we can calculate the projected separation and position angle of the companion. By integrating these findings with precise radial velocity measurements taken simultaneously, we can reconstruct the full three-dimensional orbit of the companion, facilitating mass estimation.",
        "ori-fast-z-score": -0.38461538461538464,
        "water-fast-z-score": 8.076923076923077,
        "rewrite-fast-z-score": 2.3144519649561044
    },
    {
        "original_text": "We report the detection of absorption by silicates with an optical depth of 0.1 at 9.7 microns toward the quasar HE 0515-4414 (z = 0.52) using data obtained with ISO-SWS and LWS on board ISO. The silicate feature is detected only when we use the full resolution spectrum, which shows that it has been smoothed out due to blending with other features in lower-resolution spectra. We find no evidence for dust emission associated with this absorber. This result suggests that the absorbing material consists mainly of cold gas rather than warm dust. If so, then the mass of cool gas required to produce such strong absorption lines would be much larger than expected based on current models of galaxy formation. In addition, if the observed absorption arises solely from cold gas, then the implied covering factor of the absorber must be very large compared to what is seen in local galaxies.",
        "watermark_text": "We report the observation of absorption by silicates with an optical height of 0 . 1 at 9 . 7 microns toward the quasar HE 0515 - 4414 ( z = 0 . 52 ) using data acquired with ISO - SWS and LWS on board ISO . The silicate characteristic is detected only when we using the full resolution spectrum , which demonstrates that it has been softened out due to mixing with other properties in smaller - resolution spectra .We see no evidence for powder emission associated with this absorber . This result suggests that the absorbing material contains primarily of cold gas instead than cool dust .If so , then the mass of cold gas needed to produce such strong absorption patterns must be much larger than expected based on current theories of galaxy formation . In addition , if the seen emission arises solely from cool gas , then the implied covering element of the absorber would be very huge compared to what is seen in local stars .",
        "rewrite_text": "We present the observation of silicate absorption at 9.7 microns with an optical depth of 0.1 toward the quasar HE 0515-4414 (z = 0.52), utilizing data obtained from the ISO-SWS and LWS instruments on board ISO. This silicate feature is only detected when the full resolution spectrum is used, indicating that it becomes less discernible when mixed with other signals in lower resolution spectra. Additionally, we find no signs of powder emission associated with this absorber. These findings imply that the absorbing material is primarily composed of cold gas rather than cool dust. Consequently, the amount of cold gas required to produce such significant absorption features must be considerably greater than what current galaxy formation theories predict. Furthermore, if the observed emission is solely from cool gas, then the inferred covering area of the absorber would be significantly larger than what is typically observed around local stars.",
        "ori-fast-z-score": -0.7071067811865476,
        "water-fast-z-score": 6.3639610306789285,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "We report on the fabrication and characterization of charge qubits based on self-assembled InAs quantum dots (QDs) embedded in GaAs/AlGaAs heterostructures. We show that by using an optimized growth procedure, we can achieve high quality QD layers with low density of defects which are crucial for achieving good coherence times. The samples were grown by molecular beam epitaxy at 600 °C under As-rich conditions to minimize the formation of threading dislocations. A single layer of self-assembled InAs/GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was deposited. The sample structure is shown schematically in Figure 1 . The photoluminescence spectrum shows emission peaks centered around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states associated with charged excitons.",
        "watermark_text": "We report on the fabrication and description of charge qubits based on self - assembled InAs quantum dots ( QDs ) integrated in GaAs / AlGaAs heterostructures . We suggest that by using an optimized growth technique , we can attain high quality QD layers with minimal concentration of flaws which are important for achieving better coherence times .The samples were cultivated by molecular wave epitaxy at 600 °C under As - rich conditions to minimize the formation of threading dislocations . A single mesh of self - assembled InAs / GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm wide Al0 . 3Ga0 . 7As barrier layer .Finally , a 20 nm deep GaAs capping layer was extracted . The sample structure is displayed schematically in Figure 1 .The photoluminescence spectrum displays emission levels focused around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states correlated with charged excitons .",
        "rewrite_text": "We present our findings on the fabrication and characterization of charge qubits utilizing self-assembled InAs quantum dots (QDs) integrated into GaAs/AlGaAs heterostructures. Our approach involves employing an optimized growth technique that enables us to achieve high-quality QD layers with a minimal concentration of defects, which is critical for enhancing coherence times. The samples were grown using molecular beam epitaxy at 600 °C under arsenic-rich conditions to reduce the occurrence of threading dislocations. Following an annealing process at 650 °C for 10 seconds, a single layer of self-assembled InAs/GaAs QDs was formed, which was subsequently covered with a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, we applied a 20 nm thick GaAs capping layer. A schematic representation of the sample structure is provided in Figure 1. The photoluminescence spectrum reveals emission peaks centered around 1280 nm, corresponding to the ground state excitonic transitions of individual QDs, along with higher energy states associated with charged excitons.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We present an analysis of the most luminous quasars in the Sloan Digital Sky Survey; these are among the brightest known objects at any wavelength, and have been studied extensively over many years. We use Chandra observations to study their X-ray properties as well as those of lower-luminosity quasars with similar redshifts (z ~ 2). The results show that there is no significant difference between the two samples; both exhibit very hard spectra and high luminosities. This suggests that the physical processes responsible for producing X-rays do not depend strongly on quasar luminosity or redshift. In addition, we find evidence that the X-ray emission may be associated with outflows driven by winds originating near the central black hole. These findings provide new insights into how supermassive black holes grow during the early stages of galaxy formation. \n \n Keywords: quasars, X-ray emission, wind-driven outflow",
        "watermark_text": "We present an assessment of the most luminous quasars in the Sloan Digital Sky Survey ; these are among the brightest known objects at any wavelength , and have been studied frequently over numerous years . We use Chandra measurements to study their X - ray characteristics as well as those of lower - luminosity quasars with similar redshifts ( z ~ 2 ) .The results show that there is no major variation between the two specimens ; both exhibit very hard spectra and large luminosities . This implies that the physical processes responsible for producing X - radiation do not depend greatly on quasar luminosity or redshift .In addition , we find proof that the X - ray radiation may be involved with outflows driven by winds occurring near the main white hole . These studies provide fresh insights into how supermassive black holes expand during the early stages of galaxy formation .Keywords : quasars , X - ray radiation , wind - powered outflow",
        "rewrite_text": "We present an analysis of the most luminous quasars identified in the Sloan Digital Sky Survey, which represent some of the brightest known objects across all wavelengths and have been extensively researched over many years. Utilizing Chandra measurements, we examine their X-ray properties alongside those of lower-luminosity quasars at similar redshifts (z ~ 2). Our findings indicate minimal differences between the two groups; both display very hard spectra and high luminosities. This suggests that the mechanisms generating X-ray radiation are largely independent of a quasar's luminosity or redshift. Additionally, we provide evidence that X-ray emissions may be linked to outflows driven by winds originating near the central white hole. These investigations offer new perspectives on the growth of supermassive black holes during the initial phases of galaxy formation. Keywords: quasars, X-ray emissions, wind-driven outflow.",
        "ori-fast-z-score": 0.508000508000762,
        "water-fast-z-score": 6.350006350009525,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "We report on diffuse X-ray emission in the Carina Nebula observed by Suzaku. The spectrum is well reproduced by thermal plasma models at kT = 0.7-1 keV and nH = (0.5-2) x 10^(22) cm^{-3}, which are consistent with those obtained previously for other regions within the nebula. We find that the total luminosity of this component amounts to Lx ~ 1.3 x 10^35 erg/sec, corresponding to about 10% of the total energy output of massive stars in the region. This suggests that hot gas produced by stellar winds and/or supernovae plays an important role in heating up the interstellar medium around young open clusters such as Trumpler 14-16. \n \n \n \n Keywords: Diffuse X-rays, Hot plasma, Open cluster, Supernova remnant, Stellar wind, Carina Nebula",
        "watermark_text": "We report on diffuse X - ray radiation in the Carina Nebula observed by Suzaku . The spectrum is well illustrated by thermal plasma estimates at kT = 0 . 7 - 1 keV and nH = ( 0 . 5 - 2 ) x 10 ^ ( 22 ) cm ^ { - 3 } , which are compatible with those observed previously for other regions within the nebula .We see that the total luminosity of this constituent amounts to Lx ~ 1 . 3 x 10 ^ 35 erg / sec , equivalent to about 10 % of the total energy produced of large galaxies in the region . This implies that hard gas created by stellar winds and / or supernovae plays an important role in heating up the interstellar medium around young open complexes such as Trumpler 14 - 16 .Keywords : Diffuse X - radiation , Hot plasma , Open core , Supernova remnant , Stellar wind , Carina Nebula",
        "rewrite_text": "We present findings on diffuse X-ray radiation in the Carina Nebula as observed by Suzaku. The spectrum is effectively described by thermal plasma parameters of kT = 0.7 - 1 keV and nH = (0.5 - 2) x 10^(22) cm^(-3), which align with previous observations in other parts of the nebula. Our analysis reveals that the total luminosity of this component is approximately Lx ~ 1.3 x 10^(35) erg/sec, representing about 10% of the total energy generated by large galaxies in the vicinity. This indicates that the hot gas produced by stellar winds and/or supernovae significantly contributes to heating the interstellar medium surrounding young open clusters like Trumpler 14-16. \n\nKeywords: Diffuse X-ray radiation, Hot plasma, Open clusters, Supernova remnants, Stellar winds, Carina Nebula.",
        "ori-fast-z-score": -1.3127849234810511,
        "water-fast-z-score": 3.646624787447364,
        "rewrite-fast-z-score": 0.42857142857142855
    },
    {
        "original_text": "We present the first results for atmospheric dynamics in short period extra-solar gas giant planets (EGPs) using 3D general circulation models with radiative transfer and realistic opacities. We find that the night-side temperature is strongly dependent on opacity, which determines how much energy can be transported to space by radiation. The day-night contrast increases as we decrease the opacity because less heat escapes through the nightside atmosphere. This effect is more pronounced at lower pressures where convection becomes inefficient. For low enough opacities, the planet cools down completely during its orbit resulting in an extremely cold night side. Our simulations show that EGPs are likely to have very different climates depending on their composition.  Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "watermark_text": "We present the first findings for atmospheric mechanics in small history additional - solar gas giant galaxies ( EGPs ) using 3D general circulation estimates with radiative transfer and realistic opacities . We see that the night - side temperature is strongly dependent on opacity , which determines how many heat can be transported to space by radiation .The day - night difference rises as we decrease the opacity because lighter thermal exits through the nightside environment . This phenomenon is more pronounced at lower pressures where circulation becomes inefficient .For low enough opacities , the planet cools down fully during its orbit producing in an incredibly cold evening side . Our simulations see that EGPs are likely to have very different climates based on their composition .Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "rewrite_text": "We present the initial findings on atmospheric mechanics in small, history-rich solar gas giant galaxies (EGPs) using three-dimensional general circulation models that incorporate radiative transfer and realistic opacity measurements. Our results indicate that the night-side temperature is significantly influenced by opacity, which affects the amount of heat that can escape into space via radiation. As we reduce the opacity, the day-night temperature difference increases, as lighter thermal emissions leave the cooler night-side environment. This effect becomes more pronounced at lower pressures, where circulation is less efficient. For sufficiently low opacities, the planet can fully cool down during its orbit, resulting in an extremely cold evening side. Our simulations suggest that EGPs may exhibit significantly different climates depending on their composition. \n\nKeywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction.\nThe main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation  1  , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(−βH) (1) remains extremely complicated  2  .\nIn recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics  3  -  8  . These studies were inspired by the fact that both classical and quantum descriptions have certain common features  9  : they are formulated in terms of wave functions ψ(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schrödinger equation ih∂ t |ψ(t) = H|ψ(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix ρ = |ψ(t) ψ(t)| plays the role of a probability distribution in both theories  10  .",
        "watermark_text": "We present an view to the description of macroscopic configurations in terms of quantum states , which is based on the notion of entanglement entropy and its generalization for mixed states . We see that this representation enables one to obtain exact findings for some statistical characteristics of structures with many degrees of liberty .In particular , we investigate the question of calculating the splitting function of classical spin configurations at high heat . The proposed method can be used as a framework for constructing numerical algorithms for solving cases linked to the determination of thermodynamic qualities of complex systems .Introduction . The main goal of statistical mechanics is to explain the dynamics of macroscopic objects ( for example , atoms ) by using microscopic information about their components ( atoms ) .This job becomes especially difficult when dealing with large systems composed of several particles or spins . For such cases , it is important to use approximations , since direct calculations are unable due to the exponential growth of the number of possible microstates with increasing system width N .One of these method is the so - called mean - field approximation 1 , according to which each particle interacts only with all other particles concurrently ; i . e . , the interaction between multiple sets of particles is neglected . However , even within this simplified model , the determination of the partition function Z = Tr exp ( −βH ) ( 1 ) stays extremely difficult 2 .In recent years , there has been growing interest in advanced new concepts for describing macroscopic configurations in terms comparable to those utilized in quantum science 3 - 8 . These studies were inspired by the fact that both classical and quantum descriptions have many common features 9 : they are formulated in terms of wave vectors ψ ( z ) , where h indicates either positions of atoms or spins , respectively .Moreover , the evolution of these wave functions obeys the same Schrödinger equation ih∂ t | ψ ( t ) = H | ψ ( t ) , where H is the analogous Hamiltonian operator . It should additionally be mentioned that the density function ρ = | ψ ( t ) ψ ( t ) | serves the importance of a probability distribution in both models 10 .",
        "rewrite_text": "We offer a perspective on describing macroscopic configurations through quantum states, grounded in the concept of entanglement entropy and its extension to mixed states. This approach allows us to derive precise results for certain statistical properties of structures with numerous degrees of freedom. Specifically, we explore the calculation of the splitting function for classical spin configurations at elevated temperatures. The proposed technique can serve as a foundation for developing numerical algorithms aimed at determining the thermodynamic properties of complex systems. \n\nIn statistical mechanics, the primary objective is to elucidate the dynamics of macroscopic entities, such as atoms, using microscopic details about their constituents. This task becomes particularly challenging for large systems comprised of many particles or spins. To address this, approximations are often necessary, as direct calculations are impractical due to the exponential increase in the number of possible microstates with larger system sizes (N). One such method is the mean-field approximation, where each particle is assumed to interact concurrently with all others, neglecting interactions among groups of particles. Despite this simplification, calculating the partition function \\( Z = \\text{Tr} \\exp(-\\beta H) \\) remains extremely complex. Recently, there has been significant interest in new concepts for articulating macrostates using frameworks similar to those in quantum physics. These investigations are motivated by the shared characteristics of classical and quantum descriptions, both framed in terms of wave functions \\( \\psi(z) \\), where \\( z \\) represents atomic positions or spins. Furthermore, the evolution of these wave functions follows the same Schrödinger equation \\( i \\hbar \\frac{\\partial}{\\partial t} | \\psi(t) \\rangle = H | \\psi(t) \\rangle \\), with \\( H \\) being the corresponding Hamiltonian operator. Additionally, the density function \\( \\rho = |\\psi(t)|^2 \\) acts as a probability distribution in both frameworks.",
        "ori-fast-z-score": 0.23643312187173018,
        "water-fast-z-score": 8.799551054765924,
        "rewrite-fast-z-score": 1.6232795496618457
    },
    {
        "original_text": "We have performed simulations of nova explosions using the hydrodynamic code VH-1, which includes nuclear burning and convection. We find that changes to reaction rates can significantly affect the results of these calculations. In particular, we show how different choices for the 12C(p,γ)13N rate lead to differences in the predicted light curve shapes.  The inclusion of this reaction is important because it affects the amount of 13N produced during the explosion. This isotope decays by electron capture into 14O, which then undergoes β+ decay back down to 14N. If there are too many electrons present at late times (due to an overabundance of 13N), they will be captured onto protons instead of being emitted as positrons; thus, less energy will be released than if no such process were occurring. Our results suggest that the current uncertainty in the 12C(p , γ )13N rate may cause errors in the predicted luminosity of up to 50%.",
        "watermark_text": "We have done simulations of nova explosions using the hydrodynamic code VH - 1 , which includes nuclear combustion and convection . We see that changes to reaction rates can significantly affect the results of these calculations .In particular , we study how various options for the 12C ( p , γ ) 13N rate lead to differences in the expected light curve sizes . The inclusion of this response is important because it affects the quantity of 13N produced during the explosion .This isotope decays by electron capture into 14O , which then undergoes β + decay back down to 14N . If there are too several electrons present at late times ( owing to an overabundance of 13N ) , they will be captured onto protons rather of being emitted as positrons ; thus , fewer electricity will be emitted than if no such mechanism were happening .Our results propose that the present uncertainty in the 12C ( p , γ ) 13N rate may create errors in the expected luminosity of up to 50 % .",
        "rewrite_text": "We conducted simulations of nova explosions using the hydrodynamic code VH-1, which incorporates nuclear combustion and convection. Our findings indicate that variations in reaction rates can greatly influence the outcomes of these simulations. Specifically, we examine how different options for the 12C(p, γ)13N reaction rate result in variations in the anticipated light curve amplitudes. Understanding this response is crucial, as it impacts the amount of 13N generated during the explosion. This isotope decays via electron capture into 14O, which subsequently undergoes β+ decay to revert to 14N. If there is an excess of electrons at later stages (due to an overproduction of 13N), they will be captured by protons instead of being emitted as positrons, leading to a reduced emission of energy. Our results suggest that the current uncertainty in the 12C(p, γ)13N reaction rate could introduce errors in the predicted luminosity of up to 50%.",
        "ori-fast-z-score": -0.5,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We present an analytical theory for describing capillary forces between two spherical particles in contact with each other and immersed into a liquid, which is valid even when the separation distance between them becomes comparable to their size. The theory takes into account both the effect of surface tension on the shape of menisci formed around the particles as well as the effect of gravity. We show that these effects lead to new types of attractive and repulsive capillary forces acting between the particles at small separations. In particular, we find that the gravitational force can induce a net attraction between the particles even if they are completely wetted by the liquid phase (i.e., have no dry patches). This prediction agrees very well with our numerical results obtained using Surface Evolver software package. Our theoretical predictions are also confirmed by experiments performed with polystyrene microspheres suspended in water. Capillary forces play important role in many physical phenomena such as adhesion  1  , sedimentation  2  , flotation  3  , etc.. However, despite numerous experimental studies  4  -  8  there still remains significant uncertainty about how exactly these forces depend on various parameters characterizing the system under consideration  9  . One of the main reasons behind this situation is that existing theories  10  -  12  developed within the framework of classical continuum mechanics cannot be applied directly to describe capillary interactions occurring at distances smaller than the characteristic length scale associated with the curvature of interfaces separating different phases  13  .\nIn order to overcome this difficulty one usually resorts to some approximate approaches based either on the concept of effective Hamaker constants  14  or on the so-called  superposition approximation   15  . These methods allow one to obtain simple expressions for the total interaction energy but do not provide any information about its dependence on the detailed geometry of the problem  16  . Moreover, it has been shown recently  17  that the latter approach fails...",
        "watermark_text": "We present an analytical theory for describing capillary forces between two spherical objects in contact with each other and immersed into a liquid , which is valid even when the separation distance between them becomes comparable to their size . The theory took into consideration both the impact of surface friction on the form of menisci constructed around the molecules as well as the impact of gravitational .We see that these influences result to novel sorts of attractive and repulsive capillary forces working between the particles at small separations . In particular , we find that the gravitational field can induce a net attraction between the molecules even if they are completely wetted by the liquid phase ( i . e . , have no dry patches ) .This prediction agrees very best with our numerical findings obtained using Surface Evolver tool package . Our conceptual predictions are also verified by research performed with polystyrene microspheres hanging in water .Capillary forces play vital role in many mechanical phenomena such as adhesion 1 , sedimentation 2 , flotation 3 , etc . . However , despite several experimental studies 4 - 8 there still remains significant doubt about how exactly these forces depend on various variables characterizing the process under consideration 9 .One of the main motives behind this situation is that original theories 10 - 12 developed within the framework of classical continuum mechanics cannot be applied directly to explain capillary behavior happening at distances smaller than the typical length range identified with the curvature of interfaces separating different stages 13 . In try to overcome this obstacle one usually resorts to some approximate approaches depending either on the idea of effective Hamaker constants 14 or on the so - called superposition approximation 15 .These methods provide one to obtain simple definitions for the total interaction power but do not offer any knowledge about its dependence on the detailed geometry of the question 16 . Moreover , it has been shown recently 17 that the latter technique fails . . .",
        "rewrite_text": "We propose an analytical framework for understanding the capillary forces between two spherical objects that are in contact and submerged in a liquid, applicable even when their separation distance approaches their dimensions. This theory incorporates the effects of surface friction on the shape of the menisci formed around the spheres, as well as the influence of gravity. Our analysis reveals that these factors lead to new types of attractive and repulsive capillary forces that operate between the particles at small separations. Notably, we discover that the gravitational field can create a net attraction between the molecules, even when they are fully wetted by the liquid and possess no dry areas. This prediction aligns closely with our numerical simulations using the Surface Evolver software. Additionally, our theoretical findings are supported by experimental research conducted on polystyrene microspheres suspended in water. Capillary forces are crucial in various mechanical phenomena, including adhesion, sedimentation, and flotation. However, despite numerous experimental investigations, significant uncertainty persists regarding how these forces vary with different process parameters. A primary reason for this uncertainty is that traditional theories derived from classical continuum mechanics cannot be directly applied to capillary behavior at scales smaller than the curvature length associated with the interfaces between different phases. To address this challenge, one typically resorts to approximate methods that rely on effective Hamaker constants or the superposition approximation. While these approaches yield simplified expressions for total interaction energy, they do not clarify how this energy depends on the specific geometries involved. Furthermore, recent studies have indicated that the superposition approximation is inadequate.",
        "ori-fast-z-score": -0.16329931618554522,
        "water-fast-z-score": 8.765781549553733,
        "rewrite-fast-z-score": 1.0606601717798212
    },
    {
        "original_text": "We present an experimental investigation into complementarity between position and momentum measurements on single photons using a modified version of the original Einstein-Bohr photon box experiment.  The results show that, for this particular measurement scheme, there is no violation of Bell s inequality or any other form of nonlocality. We also demonstrate how our setup can be used to investigate quantum contextuality by performing two different experiments with identical settings but opposite outcomes. In one case we observe violations of Bell inequalities while in the other they are not violated. This shows that the observed behavior cannot be explained within classical physics and demonstrates quantum contextuality. Quantum mechanics predicts that certain physical quantities such as position and momentum do not have simultaneous well-defined values. Instead these quantities exist only as probability distributions which evolve continuously over time according to Schrödinger s equation. However, it has been shown that if both position and momentum were measured simultaneously then their respective probabilities would interfere destructively resulting in a zero probability of measuring either quantity at its most probable value  1  . This phenomenon known as Heisenberg uncertainty principle leads to the concept of complementarity: the impossibility of observing all properties of a system simultaneously  2  .\nIn 1964 John Bell showed that local hidden variable theories could not explain some predictions made by quantum mechanics  3  , leading to the formulation of Bell s theorem  4  . Since then many experiments have been performed to test whether quantum mechanical predictions violate Bell s theorem  5  . These tests typically involve entangled particles  6  where each particle carries information about the state of another distant particle  7, 8  . If the particles are separated far enough so that they never interact again after being created, then the correlations between them must be due solely to quantum effects  9  .",
        "watermark_text": "We present an experimental inquiry into complementarity between position and momentum estimates on single photons using a altered version of the original Einstein - Bohr photon box observation . The results show that , for this special measurement scheme , there is no violation of Bell s inequality or any other form of nonlocality .We additionally prove how our setup can be used to examine quantum contextuality by performing two different trials with identical settings but different outcomes . In one instance we exhibit abuses of Bell inequalities while in the other they are not upheld .This shows that the seen behavior cannot be described within classical physics and demonstrates quantum contextuality . Quantum theory predicts that particular physical components such as position and momentum do not have simultaneous well - defined parameters .Instead these quantities occur only as probability distributions which evolution continuously over time according to Schrödinger s equation . However , it has been shown that if both position and momentum were calculated separately then their different probabilities would interfere destructively resulting in a zero probability of assessing either quantity at its most likely value 1 .This phenomenon known as Heisenberg uncertainty theory gives to the notion of complementarity : the impossibility of experiencing all characteristics of a system simultaneously 2 . In 1964 John Bell demonstrated that nearby secret variable theories cannot not understand some observations made by quantum mechanics 3 , leading to the realization of Bell s theorem 4 .Since then many tests have been performed to test whether quantum mechanical predictions violate Bell s theorem 5 . These studies typically involve entangled particles 6 where each particle contains information about the state of another distant particle 7 , 8 .If the molecules are apart farther enough so that they cannot interact again after being created , then the correlations between them need be due exclusively to quantum effects 9 .",
        "rewrite_text": "We conduct an experimental investigation into the complementarity of position and momentum estimates for single photons, employing a modified version of the classic Einstein-Bohr photon box experiment. Our findings indicate that, within this specific measurement framework, Bell's inequality is not violated, nor is there evidence of nonlocality. Furthermore, we demonstrate that our setup can be utilized to explore quantum contextuality by conducting two separate trials with identical settings that yield different results. In one trial, we observe violations of Bell inequalities, while in the other, they hold true. This outcome illustrates that the observed behavior cannot be accounted for by classical physics, thereby providing evidence for quantum contextuality. Quantum theory posits that certain physical properties, such as position and momentum, cannot possess well-defined values simultaneously. Instead, these properties exist only as evolving probability distributions governed by Schrödinger's equation. It has also been established that if position and momentum are determined independently, their respective probabilities will interfere destructively, leading to a zero probability of measuring either property at its most probable value of 1. This phenomenon, known as the Heisenberg uncertainty principle, gives rise to the concept of complementarity, which asserts the impossibility of simultaneously observing all characteristics of a system. In 1964, John Bell demonstrated that local hidden variable theories fail to account for certain observations made in quantum mechanics, culminating in the formulation of Bell's theorem. Since then, numerous experiments have been conducted to assess whether quantum mechanical predictions violate this theorem, typically involving entangled particles. In such cases, each particle encodes information about the state of another distant particle. When these particles are sufficiently separated such that they cannot interact after their creation, any observed correlations must stem solely from quantum effects.",
        "ori-fast-z-score": -0.17149858514250882,
        "water-fast-z-score": 8.231932086840423,
        "rewrite-fast-z-score": -1.6903085094570331
    },
    {
        "original_text": "We present an analytical model for calculating the aperture efficiency of a reflector antenna with circularly polarized feeds, based on ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to calculate the electric field distribution over the aperture surface by solving Maxwell s equations using Green functions. We show that this approach can be applied to any type of feed horns, including corrugated conical horns or dual-polarized log-periodic dipole arrays. This work was motivated by our recent study of aperture efficiencies of two different types of antennas operating at 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight log-periodic dipole array elements. In both cases we found good agreement between numerical results obtained with our new model and experimental data. \n \n Keywords: Antenna design",
        "watermark_text": "We present an analytical model for determining the lens energy of a reflector antenna with circularly polarized feeds , using on ray tracing through the feed - horn optics and physical optics ( PO ) at the aperture plane . The PO technique is utilized to estimate the electric field flow over the lens surface by solving Maxwell s equations using Green functions .We see that this methodology can be applied to any type of feed horns , particularly corrugated conical horns or multiple - polarized log - periodic dipole arrays . This research was sparked by our latest research of aperture efficiencies of two different kinds of antennas active at 1 . 4 GHz : a single - dish radio telescope fitted with a corrugated conical feed trumpet and a multi - component interferometer composed of eight log - periodic dipole array modules .In both cases we concluded excellent agreement between mathematical findings obtained with our new model and experimental evidence . Keywords : Antenna design",
        "rewrite_text": "We introduce an analytical model to calculate the lens energy of reflector antennas equipped with circularly polarized feeds. This model employs ray tracing through the feed-horn optics combined with physical optics (PO) at the aperture plane. The PO method estimates the electric field distribution over the lens surface by solving Maxwell's equations using Green's functions. Our approach is applicable to various types of feed horns, especially corrugated conical horns and multi-polarized log-periodic dipole arrays. This study was inspired by our recent work on the aperture efficiencies of two distinct antenna types operating at 1.4 GHz: a single-dish radio telescope with a corrugated conical feed trumpet and a multi-component interferometer consisting of eight log-periodic dipole array modules. In both instances, we found a strong correlation between the results derived from our new model and the experimental data. Keywords: Antenna design.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": -0.11867816581938533
    },
    {
        "original_text": "We present the light-cone distribution amplitudes (DAs) for axial vector mesons in terms of their helicity components, which are determined by solving the Bethe-Salpeter equation with an instantaneous interaction kernel and applying the method developed recently to calculate DAs.  We find that the twist-2 DA is dominated by its first Gegenbauer moment, while higher moments contribute significantly only at large momentum fractions x > 0.7. The twist-3 DA has two independent functions, one of them being proportional to the second Gegenbauer moment. Our results show that the twist-4 contribution is negligible compared to those of lower twists. These findings will be useful for studying exclusive processes involving axial vector mesons such as B-decays into charmonium plus photon or pion pair. \nI. INTRODUCTIO N\nThe study of hadronic structure plays an important role in understanding strong interactions between quarks and gluons inside hadrons. In particular, the investigation on the parton distributions provides us valuable information about how quarks and gluon are distributed within hadrons  1  . Recently, there have been great interests in exploring the internal structures of hadrons beyond the leading-twist level  2  , especially the transverse-momentum dependent parton distributions  3  .\nIn this work we focus our attention on another type of nonperturbative objects -the light-cone distribution amplitudes(DAs). They describe the probability amplitude of finding a quark-antiquark pair with certain longitudinal momentum fraction and transverse separation at some fixed light-like distance  4  . It was shown that they play crucial roles in describing various hard exclusive reactions  5  . For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs  6  ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the lowest-and next-to-lowest order DAs  7, 8  . Furthermore, it was found that the heavy-to-light transition form factor FV(q 2 ) of B→V transitions depends",
        "watermark_text": "We introduce the light - cone distribution amplitudes ( DAs ) for axial vector mesons in terms of their helicity components , which are decided by solving the Bethe - Salpeter equation with an instantaneous interaction kernel and using the method developed lately to estimate DAs . We see that the twist - 2 DA is dominated by its initial Gegenbauer moment , while greater moments contribute considerably only at large velocity fractions x > 0 . 7 .The twist - 3 DA has two independent functions , one of them being equal to the second Gegenbauer moment . Our results show that the twist - 4 contribution is negligible compared to those of lower bends .These studies will be valuable for studying exclusive mechanisms using axial vector mesons such as B - decays into charmonium plus photon or pion pair . I . INTRODUCTIO N The investigation of hadronic structure serves an important role in understanding strong interactions between quarks and gluons inside hadrons .In particular , the investigation on the parton distributions offers us valuable info about how quarks and gluon are distributed within hadrons 1 . Recently , there have been big efforts in investigating the internal structures of hadrons beyond the led - twist level 2 , particularly the transverse - momentum dependent parton distributions 3 .In this research we focus our focus on another type of nonperturbative objects - the light - cone distribution amplitudes ( DAs ) . They define the probability amplitude of finding a quark - antiquark pair with certain horizontal momentum fraction and longitudinal separation at some fixed light - like distance 4 .It was shown that they serve vital part in understanding various hard exclusive reactions 5 . For instance , the decay constants fBπ and fBs can be stated in terms of the lowest - order DAs 6 ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the highest - and last - to - lowest order DAs 7 , 8 .Furthermore , it was shown that the heavy - to - light shift form parameter FV ( q 2 ) of B→V transitions depends",
        "rewrite_text": "We present light-cone distribution amplitudes (DAs) for axial vector mesons, defined through their helicity components. These components are determined by solving the Bethe-Salpeter equation utilizing an instantaneous interaction kernel and a recently developed method to estimate DAs. Our analysis reveals that the twist-2 DA is predominantly influenced by its leading Gegenbauer moment, while higher moments only significantly affect the DA for large velocity fractions, specifically when x > 0.7. The twist-3 DA comprises two independent functions, one of which corresponds to the second Gegenbauer moment. Notably, our findings indicate that the contribution from the twist-4 DA is minimal compared to those of the lower twists. This research is crucial for exploring exclusive processes involving axial vector mesons, such as the B-meson decays into charmonium plus a photon or a pion pair. \n\nI. INTRODUCTION  \nUnderstanding hadronic structure is essential for grasping the strong interactions that bind quarks and gluons within hadrons. In particular, studying parton distributions provides critical insights into the distribution of quarks and gluons in hadrons. Recently, significant progress has been made in examining hadronic internal structures beyond the leading-twist level, especially concerning transverse-momentum dependent parton distributions. In this work, we shift our attention to another class of nonperturbative objects: light-cone distribution amplitudes (DAs). These DAs represent the probability amplitudes for finding a quark-antiquark pair with a specific transverse momentum fraction and longitudinal separation at a fixed light-like separation. They play an essential role in understanding a range of hard exclusive reactions. For example, the decay constants \\( f_{B\\pi} \\) and \\( f_{Bs} \\) can be expressed in terms of the lowest-order DAs, while the form factors for semileptonic decays \\( B \\to \\pi l\\nu_l \\) and \\( B \\to K l\\nu_l \\) are influenced by both the highest and lowest order DAs. Additionally, the heavy-to-light transition form parameters \\( F_V(q^2) \\) in \\( B \\to V \\) transitions also depend on these distribution amplitudes. ",
        "ori-fast-z-score": 1.044465935734187,
        "water-fast-z-score": 8.181649829917799,
        "rewrite-fast-z-score": 0.8451542547285166
    },
    {
        "original_text": "We present an algorithm for computing two-person zero-sum games with imperfect information, which is based on solving linear programs (LPs) and mixed integer programming problems (MIPs). The proposed approach can be applied to any game that has been formulated as a Mixed Integer Linear Program (MILP), including many real-world applications such as energy markets or traffic control systems. We show how our method outperforms state-of-the-art algorithms in terms of computational efficiency by applying it to several test cases. In addition, we provide insights into the structure of the problem instances solved by our algorithm. Finally, we discuss possible extensions of this work. Games are mathematical models used to describe situations where multiple decision makers interact strategically. A key concept in game theory is equilibrium, i.e., a set of strategies that no player wants to deviate from if all other players stick to their strategy. One important class of equilibria are so-called Nash equilibria, which have received considerable attention over the past decades due to their wide applicability across various fields ranging from economics to computer science. However, finding Nash equilibria is known to be computationally hard even for simple classes of games.",
        "watermark_text": "We present an algorithm for modeling two - person zero - sum games with imperfect data , which is based on solving linear programs ( LPs ) and combined integer programming questions ( MIPs ) . The proposed approach can be applied to any game that has been formulated as a Mixed Integer Linear Program ( MILP ) , including much actual - global applications such as energy markets or highway control networks .We see how our technique outperforms state - of - the - art algorithms in terms of computational efficiency by application it to several test situations . In addition , we provide insights into the composition of the question objects solved by our algorithm .Finally , we explain possible extensions of this research . Games are mathematical models used to define situations where multiple thinking makers interact strategically .A central concept in play theory is equilibrium , i . e . , a setting of plans that no player wants to deviate from if all other players stick to their strategy . One important group of equilibria are so - called Nash equilibria , which have received considerable scrutiny over the previous decades owing to their wide applicability across numerous topics including from economics to computer science .However , finding Nash equilibria is known to be computationally hard even for simple groups of games .",
        "rewrite_text": "We introduce an algorithm designed to model two-person zero-sum games with imperfect data, utilizing linear programming (LP) and mixed integer programming (MIP). This approach can be applied to any game formulated as a Mixed Integer Linear Program (MILP), encompassing various real-world applications such as energy markets and highway control networks. Our technique demonstrates superior computational efficiency compared to state-of-the-art algorithms when applied to multiple test scenarios. Additionally, we offer insights into the composition of the problem instances addressed by our algorithm. Finally, we discuss potential extensions of this research. Games serve as mathematical frameworks to describe situations where multiple strategic decision-makers interact. A fundamental concept in game theory is equilibrium, which refers to a set of strategies where no player has an incentive to deviate if the others maintain their strategies. A significant category of equilibria is Nash equilibria, which have been extensively studied over the past few decades due to their relevance across various fields, including economics and computer science. However, locating Nash equilibria remains computationally challenging, even in simpler classes of games.",
        "ori-fast-z-score": 0.6965260331469925,
        "water-fast-z-score": 6.931032800836721,
        "rewrite-fast-z-score": -0.5129891760425771
    },
    {
        "original_text": "We study the vacant set of random interlacements in Rd, d>1, which is defined as the complement of the union of all open paths starting at 0 up to time 1. We prove that it has Hausdorff dimension equal to d-1 almost surely by showing that its upper Minkowski dimension equals this value with probability one. This result extends previous results on the vacant set of simple random walk obtained by Lawler, Schramm and Werner (Annals of Probability 24(3), 1997) and Benjamini, Kalai and Schramm (Journal of Functional Analysis 157(1), 2000). The proof relies heavily on recent developments concerning the geometry of Brownian motion and the theory of stable processes. In particular we use an estimate for the Green function of the killed Brownian motion due to Bass and Burdzy (Annales de l Institut Henri Poincare 22(2), 1999).\nThe main motivation behind our work comes from percolation theory where the vacant set of random walks plays a crucial role. Indeed, we show how our techniques can be used to obtain new information about the critical behavior of bond-percolation models on Zd.",
        "watermark_text": "We test the empty set of random interlacements in Rd , d > 1 , which is defined as the complement of the union of all open routes starting at 0 up to time 1 . We establish that it has Hausdorff size equal to d - 1 fairly surely by showing that its outer Minkowski dimension equals this value with probability one .This result continues preceding results on the empty set of simple random walk obtained by Lawler , Schramm and Werner ( Annals of Probability 24 ( 3 ) , 1997 ) and Benjamini , Kalai and Schramm ( Journal of Functional Analysis 157 ( 1 ) , 2000 ) . The proof draws highly on current developments concerning the topology of Brownian movement and the notion of stable processes .In particular we using an estimate for the Green function of the dead Brownian moving thanks to Bass and Burdzy ( Annales de l Institut Henri Poincare 22 ( 2 ) , 1999 ) . The main motivation behind our work comes from percolation theory where the empty set of random walks holds a crucial role .Indeed , we prove how our approaches can be used to obtain new information about the important dynamics of bond - percolation models on Zd .",
        "rewrite_text": "We investigate the empty set of random interlacements in \\( \\mathbb{R}^d \\) for \\( d > 1 \\), defined as the complement of the union of all open paths originating from the point 0 and extending up to time 1. We demonstrate that this set has a Hausdorff dimension of \\( d - 1 \\) with high probability by proving that its outer Minkowski dimension equals this value almost surely. Our findings build upon previous work on the empty set of simple random walks conducted by Lawler, Schramm, and Werner (Annals of Probability 24(3), 1997), as well as Benjamini, Kalai, and Schramm (Journal of Functional Analysis 157(1), 2000). The proof heavily relies on recent advancements in the topology of Brownian motion and concepts related to stable processes. Specifically, we utilize an estimate for the Green function of the dead Brownian motion provided by Bass and Burdzy (Annales de l’Institut Henri Poincaré 22(2), 1999). Our primary motivation stems from percolation theory, where the empty set of random walks plays a significant role. Indeed, we demonstrate how our methods can yield new insights into the critical dynamics of bond-percolation models on \\( \\mathbb{Z}^d \\).",
        "ori-fast-z-score": -1.2874526191574363,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": -1.2874526191574363
    },
    {
        "original_text": "We present the design and performance of an extremely low-noise, high-accuracy cryogenic mechanical oscillator based on a suspended silicon nitride membrane with integrated Josephson junctions. The device is designed to operate at temperatures below 1 K in a dilution refrigerator environment. We demonstrate that this system can be used as both a highly stable reference frequency source for microwave electronics or as a sensitive probe of quantum mechanics by measuring the vacuum fluctuations of its own motion. \n \n A key requirement for many applications of quantum information science is the ability to generate and detect single photons. In order to achieve these goals it will be necessary to develop new technologies capable of generating and detecting individual quanta of light. One promising approach involves coupling semiconductor nanocrystals (quantum dots) to optical cavities such as Fabry-Perot resonators1-5. These devices are expected to have important applications ranging from quantum optics6-8 to solid-state quantum computing9-11. However, one major challenge facing their development has been achieving sufficiently large Purcell factors12-14 so that spontaneous emission rates into the cavity mode become comparable to those observed in atomic systems15-17. This problem may be overcome using photonic crystal cavities18-20 which allow for strong confinement of electromagnetic fields within small volumes21-23.",
        "watermark_text": "We present the specification and performance of an incredibly small - noise , large - accuracy cryogenic mechanical oscillator based on a suspended silicon nitride cell with integrated Josephson junctions . The system is designed to work at pressures below 1 K in a dilution fridge climate .We showed that this device can be used as both a highly stable reference wavelength source for microwave electronics or as a sensitive probe of quantum mechanics by monitoring the vacuum fluctuations of its own movement . A crucial requirement for many applications of quantum information physics is the ability to create and locate single photons .In order to achieve these objectives it will be required to develop new inventions responsible of collecting and detecting individual quanta of light . One promising solution involves coupling semiconductor nanocrystals ( quantum dots ) to laser cavities such as Fabry - Perot resonators1 - 5 .These systems are expected to have important use ranging from molecular optics6 - 8 to soft - state quantum computing9 - 11 . However , one major challenge facing their development has been achieving sufficiently huge Purcell factors12 - 14 so that spontaneous emission rates into the cavity mode become similar to those observed in nuclear systems15 - 17 .This problem could be overcome using photonic crystal cavities18 - 20 which allow for strong confinement of electromagnetic forces within tiny volumes21 - 23 .",
        "rewrite_text": "We introduce the specification and performance of an exceptionally low-noise, high-accuracy cryogenic mechanical oscillator, which is constructed from a suspended silicon nitride cell integrated with Josephson junctions. This system is engineered to operate at pressures below 1 K within a dilution refrigerator environment. Our findings demonstrate that this device can function as a highly stable reference wavelength source for microwave electronics or serve as a sensitive probe for quantum mechanics by monitoring the vacuum fluctuations of its own movement. A critical requirement for many applications in quantum information physics is the capability to generate and detect single photons. To meet these goals, it is essential to develop innovative technologies that can collect and identify individual quanta of light. One promising approach involves coupling semiconductor nanocrystals (quantum dots) with laser cavities, such as Fabry-Perot resonators. These systems are anticipated to play a significant role across a range of applications, from molecular optics to soft-state quantum computing. However, a significant hurdle in their development has been achieving sufficiently high Purcell factors, enabling spontaneous emission rates into the cavity mode to match those observed in atomic systems. This challenge might be addressed through the use of photonic crystal cavities, which provide strong confinement of electromagnetic fields within small volumes.",
        "ori-fast-z-score": 0.8867963503478639,
        "water-fast-z-score": 7.921180343813395,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "We present the results of our numerical simulations of magnetized, rotating white dwarf stars that undergo accretion-induced collapse (AIC). We find that for sufficiently rapid rotation and strong magnetic fields, AIC leads to an explosion with properties similar to those observed in SNe Ia. The explosion is driven by the release of gravitational energy as the collapsed core bounces back after reaching nuclear density. In addition, we show that the presence of a strong toroidal field can lead to significant asymmetries in the ejecta distribution. These asymmetries are likely responsible for the polarization signal detected in some SNe Ia. \n \n Keywords: Supernovae Type Ia, Rotation, Magnetic Fields, White Dwarf Stars, Accretion Induced Collapse \n \n 1 Introduction \n \n Recent observations have shown that many supernovae type Ia (SNe Ia) exhibit large amounts of linear polarization  1  . This has been interpreted as evidence that these events result from asymmetric explosions  2  , which may be caused by large-scale magnetic fields  3  or rapid rotation  4  . However, it remains unclear whether either mechanism alone could produce such highly polarized light curves  5  . \n \n Here we investigate how the combination of rapid rotation and strong magnetic field affects the outcome of accretion induced collapse (AIC), where a white dwarf star collapses into a neutron star  6  . For this purpose, we perform two-dimensional axisymmetric hydrodynamic simulations using the code FLASH  7  . Our initial models consist of rigidly-rotating white dwarf stars with masses ranging between 0.6-1.2 Msun  8  . To account for the effects of general relativity on the structure of the white dwarf  9  , we use the polytropic equation of state P = Kρ Γ , where ρ denotes the mass density and P the pressure  10  . \nThe main goal of this work is to determine if AICs triggered by rapid rotation and/or strong magnetic fields can explain the high degree of polarization observed in SNe Ia  11  .",
        "watermark_text": "We present the conclusion of our numerical simulations of magnetized , moving white dwarf stars that suffer accretion - caused collapse ( AIC ) . We see that for enough fast rotation and strong magnetic fields , AIC leads to an explosion with properties similar to those observed in SNe Ia .The explosion is powered by the release of gravitational energy as the collapsed center bounces backward after reaching nuclear density . In addition , we find that the presence of a powerful toroidal field can lead to significant asymmetries in the ejecta distribution .These asymmetries are likely responsible for the polarization signal found in some SNe Ia . Keywords : Supernovae Type Ia , Rotation , Magnetic Fields , White Dwarf Stars , Accretion Induced Collapse 1 Introduction Recent measurements have shown that several supernovae class Ia ( SNe Ia ) exhibit substantial concentrations of linear polarization 1 .This has been viewed as proof that these phenomena come from asymmetric explosions 2 , which may be caused by large - scale magnetic waves 3 or rapid rotation 4 . However , it remains unsure whether either mechanism alone could generate such heavily polarized light curves 5 .Here we investigate how the combination of rapid rotation and strong magnetic force determines the result of accretion induced collapse ( AIC ) , where a white dwarf star collapses into a neutron galaxy 6 . For this use , we perform two - dimensional axisymmetric hydrodynamic simulations using the code FLASH 7 .Our preliminary estimates consist of rigidly - spinning white dwarf stars with masses ranging between 0 . 6 - 1 . 2 Msun 8 . To account for the effects of general relativity on the composition of the white dwarf 9 , we utilize the polytropic equation of state P = Kρ Γ , where ρ indicates the mass density and P the pressure 10 .The main goal of this research is to find if AICs triggered by rapid rotation and / or strong magnetic waves can describe the high degree of polarization observed in SNe Ia 11 .",
        "rewrite_text": "We present the conclusions from our numerical simulations of magnetized, moving white dwarf stars experiencing accretion-induced collapse (AIC). Our findings indicate that sufficiently rapid rotation and strong magnetic fields can lead to explosions exhibiting characteristics similar to those seen in Type Ia supernovae (SNe Ia). The explosions are driven by the release of gravitational energy as the collapsed core rebounds upon attaining nuclear density. Additionally, we observe that a powerful toroidal magnetic field can introduce significant asymmetries in the distribution of ejected material, which may help explain the polarization signals detected in some SNe Ia. \n\n**Keywords:** Type Ia Supernovae, Rotation, Magnetic Fields, White Dwarf Stars, Accretion-Induced Collapse \n\n**1 Introduction** Recent observations have indicated that several Type Ia supernovae (SNe Ia) display notable levels of linear polarization. This has been interpreted as evidence for asymmetrical explosions, potentially resulting from large-scale magnetic fields or rapid rotation. However, it remains uncertain whether either mechanism alone is sufficient to produce the heavily polarized light curves observed. In this study, we explore how the interplay between high rotation rates and strong magnetic fields influences the outcome of accretion-induced collapse (AIC), in which a white dwarf star collapses into a neutron star. To investigate this, we conduct two-dimensional axisymmetric hydrodynamic simulations using the FLASH code. Our initial estimates involve rigidly spinning white dwarf stars with masses ranging from 0.6 to 1.2 solar masses. To account for relativistic effects on the white dwarf's composition, we employ a polytropic equation of state where pressure (P) is a function of mass density (ρ). The primary objective of this research is to determine whether AIC events triggered by rapid rotation and/or strong magnetic fields can account for the pronounced polarization observed in SNe Ia.",
        "ori-fast-z-score": 1.1188618555710317,
        "water-fast-z-score": 7.487767802667673,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "We present results on dust formation in primordial supernova remnants (SNRs) using three-dimensional hydrodynamic simulations with detailed chemical networks for gas-phase species, grains, and molecules. We find that dust can form efficiently in SNR ejecta at high temperatures (T > 1000 K), but it is destroyed by sputtering due to collisions between ions and electrons when the temperature drops below T = 100 K. The surviving dust grains are injected into the interstellar medium (ISM). Our calculations show that the total mass of dust formed in primordial SNRs may reach up to 10^-4 Msun if we assume an initial metallicity Z = 0.1Zsun. This value agrees well with observations of nearby young SNRs. However, our model predicts too much carbonaceous dust compared to silicates observed in these objects. If this discrepancy persists after further improvements of the models, then some other mechanism should be responsible for producing silicates in SNRs. \n \n Keywords: dust, supernova remnant",
        "watermark_text": "We report findings on dust development in primordial supernova remnants ( SNRs ) using three - dimensional hydrodynamic simulations with complete biological networks for gas - phase species , grains , and molecules . We see that matter can form efficiently in SNR ejecta at high temperatures ( T > 1000 K ) , but it is destroyed by sputtering due to collisions between electrons and electrons when the temperature falls below T = 100 K . The remaining powder grains are pumped into the interstellar medium ( ISM ) .Our calculations show that the total mass of dust formed in primordial SNRs might reach up to 10 ^ - 4 Msun if we suppose an initial metallicity Z = 0 . 1Zsun . This value agrees well with observations of distant new SNRs .However , our model predicts too much carbonaceous powder compared to silicates observed in these objects . If this discrepancy persists after further changes of the models , then some other mechanism should be responsible for producing silicates in SNRs .Keywords : dust , supernova remnant",
        "rewrite_text": "We present our findings on dust formation in primordial supernova remnants (SNRs) based on three-dimensional hydrodynamic simulations that include comprehensive biological networks for gas-phase species, grains, and molecules. Our results indicate that matter can form efficiently in SNR ejecta at high temperatures (T > 1000 K), but is subsequently destroyed by sputtering from electron-electron collisions once the temperature drops below T = 100 K. The remaining dust grains are released into the interstellar medium (ISM). Our calculations suggest that the total mass of dust produced in primordial SNRs could reach approximately 10^-4 M☉, assuming an initial metallicity of Z = 0.1 Z☉. This estimate aligns closely with observations of distant new SNRs. However, our model predicts an excess of carbonaceous dust compared to the silicates observed in these remnants. If this issue remains after refining the models, it may indicate that another mechanism is responsible for the production of silicates in SNRs. \n\nKeywords: dust, supernova remnant",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 4.737364557517151,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We report the discovery of three planets orbiting an intermediate-mass subgiant star, HD 168625 (M = 1.5 Msun), using high-precision radial velocity measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile. The planet masses are 0.7 MJup , 2.1 MJup , and 3.2 MJup . We find that all three planets have orbital periods shorter than 100 days; two of them are close to their host stars (P orb < 10 d). All three planets are located within the habitable zone around their host star. This is one of only four systems known so far where more than one planet has been found inside this region.  These results demonstrate that it should be possible to detect terrestrial planets in the habitable zones of evolved stars by means of Doppler spectroscopy. Keywords: exoplanet, planetary system",
        "watermark_text": "We report the discovery of three planets orbiting an intermediate - weight subgiant star , HD 168625 ( M = 1 . 5 Msun ) , using high - precision radial speed measurements obtained with the HARPS spectrograph at La Silla Observatory in Chile . The planet masses are 0 . 7 MJup , 2 . 1 MJup , and 3 . 2 MJup .We see that all three worlds have orbital periods shorter than 100 hours ; two of them are close to their host stars ( P orb < 10 d ) . All three planets are situated within the habitable zone around their host star .This is one of only four systems available so far where more than one planet has been seen inside this area . These data demonstrate that it should be possible to identify terrestrial worlds in the habitable zones of evolved planets by means of Doppler spectroscopy .Keywords: exoplanet, planetary system",
        "rewrite_text": "We announce the discovery of three planets orbiting the intermediate-mass subgiant star HD 168625 (M = 1.5 Msun), utilizing high-precision radial velocity measurements from the HARPS spectrograph at La Silla Observatory in Chile. The masses of the planets are 0.7 MJup, 2.1 MJup, and 3.2 MJup. Notably, all three planets have orbital periods of less than 100 hours, with two of them being in close proximity to their host star (P_orb < 10 d). Importantly, all three planets reside within the habitable zone of their star. This discovery adds to a limited count of just four systems known to host more than one planet within this region. Our findings suggest that using Doppler spectroscopy could enable the detection of terrestrial planets in the habitable zones around evolved stars. Keywords: exoplanet, planetary system",
        "ori-fast-z-score": -0.29488391230979427,
        "water-fast-z-score": 3.5386069477175313,
        "rewrite-fast-z-score": -0.5773502691896258
    },
    {
        "original_text": "We study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics . We see that for particular categories of potentials there are areas where trajectories can be trapped by unstable fixed points or periodic orbits .In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) . The existence of such attractor solutions may have important implications for the evolution of our universe .For instance , it could explain why the present value of H ( t ) changes so greatly from its initial value at t = 0 . It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity capacity reduces as 1 / V ( t ) .The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "We investigate the dynamics of string cosmologies featuring complex dilaton potentials, focusing specifically on their chaotic behavior. Our findings indicate that in certain types of potentials, trajectories can become trapped by unstable fixed points or periodic orbits. Under these conditions, the system exhibits non-ergodicity and possesses an infinite number of attractors associated with varying values of the Hubble parameter \\( H(t) \\). The presence of such attractor solutions could have significant implications for the evolution of our universe. For example, it may account for the substantial change in the current value of \\( H(t) \\) from its initial value at \\( t = 0 \\). Additionally, this phenomenon might provide an explanation for the observed flatness problem, as the volume \\( V(t) \\) increases exponentially during inflation while the energy density decreases as \\( 1/V(t) \\). The results described here were obtained through numerical methods, employing the fourth-order Runge-Kutta algorithm in conjunction with Newton's method for root-finding.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": -2.121320343559643
    },
    {
        "original_text": "The BFKL equation is an effective theory for describing high-energy scattering processes at small Bjorken-x, where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool to calculate cross sections and structure functions using numerical methods. In this talk I will present recent results on the calculation of the gluon Green s function within the framework of the so-called  dipole approach  which allows us to perform calculations analytically. This method was first introduced by Mueller and Tang in order to study diffractive deep-inelastic scattering (DDIS) off protons. It can be applied also to other processes like heavy quark production in proton-proton collisions as well as photon-photon interactions. We will discuss how we have implemented these ideas numerically and show some preliminary results obtained with our code. Finally, we will comment on possible extensions of this work towards more realistic phenomenological applications.",
        "watermark_text": "The BFKL equation is an efficient model for describing long - energy scattering phenomena at small Bjorken - x , where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei . The BFKL formalism has been built into a practical tool to estimate cross sections and structure functions using numerical models .In this talk I will present recent results on the determination of the gluon Green s function within the framework of the so - called dipole approach which allows us to conduct measurements analytically . This method was first developed by Mueller and Tang in order to study diffractive deep - inelastic reflection ( DDIS ) off protons .It can be applied also to other processes like heavy quark production in proton - proton collisions as well as photon - photon interactions . We will explore how we have formulated these ideas numerically and get some preliminary outcomes received with our code .Finally , we will mention on potential extensions of this research towards more realistic phenomenological applications .",
        "rewrite_text": "The BFKL equation serves as an effective framework for analyzing high-energy scattering processes at small Bjorken-x, where x represents the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool for estimating cross sections and structure functions through numerical modeling. In this presentation, I will discuss recent findings related to the gluon Green's function, utilizing the dipole approach, which enables us to perform analytical measurements. This method, initially devised by Mueller and Tang for examining diffractive deep inelastic scattering (DDIS) off protons, is also applicable to other processes, such as heavy quark production in proton-proton collisions and photon-photon interactions. We will delve into how we have translated these concepts into numerical formulations and share some preliminary results obtained from our code. Finally, we will touch on potential avenues for extending this research to more realistic phenomenological applications.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "The Standard Model (SM) is an extremely successful theory, but it leaves many questions unanswered about physics at very high energies. In particular, there are no known fundamental principles that can explain why the SM has three generations of quarks and leptons with such different masses or how gravity fits into this picture. Theories beyond the Standard Model attempt to address these issues by introducing new particles and/or interactions which may be observed in future experiments.  Supersymmetry (SUSY), for example, introduces partners for all SM fields whose spin differs by one half unit. These partner states have identical gauge quantum numbers as their SM counterparts, so they could mix with them if SUSY were broken at low energy scales. This mixing would lead to deviations from SM predictions for observables like cross sections and decay rates. Many extensions of the Standard Model also predict new phenomena associated with extra dimensions of space-time. For instance, theories based on string/M-theory often contain additional spatial dimensions compactified down to tiny sizes. If these extra dimensions exist, then we should see evidence of their effects through virtual exchange of Kaluza-Klein excitations of gravitons and other particles between SM fields localized on our four-dimensional world-volume.",
        "watermark_text": "The Standard Model ( SM ) is an incredibly successful theory , but it leaves many issues unanswered about physics at very high energies . In particular , there are no available fundamental principles that can describe why the SM has three generations of quarks and leptons with such distinct masses or how gravity fits into this picture .Theories beyond the Standard Model attempt to alleviate these problems by introducing additional particles and / or relationships which would be found in future research . Supersymmetry ( SUSY ) , for example , creates partners for all SM fields whose spin varies by one half unit .These partner states have equal gauge quantum values as their SM counterparts , so they may blend with them if SUSY were breaking at low power scales . This blending would result to deviations from SM predictions for observables like cross sections and decay rates .Many modifications of the Standard Model also predict new concepts associated with extra dimensions of space - time . For instance , theories based on string / M - theory often contain extra spatial dimensions compactified down to small sizes .If these extra dimensions exist , then we should see evidence of their influence through virtual exchange of Kaluza - Klein excitations of gravitons and other particles between SM fields confined on our four - dimensional world - volume .",
        "rewrite_text": "The Standard Model (SM) is a highly successful theory, yet it leaves many fundamental questions about physics at extremely high energies unanswered. Specifically, it lacks a clear framework for explaining why the SM includes three generations of quarks and leptons with such distinct masses, or how gravity is integrated into this framework. Theories that extend beyond the Standard Model attempt to address these issues by introducing new particles and/or relationships that may be revealed in future studies. One example is Supersymmetry (SUSY), which proposes that each SM field has a corresponding partner with a spin that differs by half a unit. These partner states would share the same gauge quantum values as their SM counterparts and could merge with them if SUSY is broken at low energy scales. This merging could lead to observable deviations from SM predictions, such as changes in cross sections and decay rates. Additionally, many extensions to the Standard Model suggest the existence of new concepts involving extra dimensions of space-time. For example, string theory and M-theory often involve compactified extra spatial dimensions of tiny sizes. If such extra dimensions do exist, we might observe their effects through the virtual exchange of Kaluza-Klein excitations of gravitons and other particles among the SM fields that are confined to our four-dimensional space-time.",
        "ori-fast-z-score": 1.2247448713915892,
        "water-fast-z-score": 7.005888539421972,
        "rewrite-fast-z-score": 2.3763541031440183
    },
    {
        "original_text": "The Large Hadron Collider (LHC) is expected to produce new particles that could be dark matter candidates, such as supersymmetric partners of quarks or leptons.  In this talk I will discuss how these signatures can be searched for using data collected by the ATLAS experiment during Run 1 of the LHC. The results are presented both in terms of limits on production cross sections and mass exclusion regions. Finally, prospects for future searches with Run 2 data are discussed. This work was performed within the framework of the PhD thesis of M.A.M., supervised by A.S.. \nIntroduction\n\nDark Matter Candidates\n\nSupersymmetry\n\nATLAS Experiment\n\nRun 1 Results\n\nProspects for Run 2 Searches\n\nConclusions & Outlook \n\nReferences \n\n\nAcknowledgements\n\n\n\n\n\n- - - - - - - -- - - - - - --- - - - - - ---- - - - - - ----- - - - - - ------ - - - - - -------- - - - - - ---------- - - - - - ------------------ - - - - - -------------- - - - - - ----------------------------------- - - - - - ----------------------------------------------------- - - - - - ------------------------------------------------------------------------------------ - - - - -",
        "watermark_text": "The Large Hadron Collider ( LHC ) is expected to produce new objects that might be dark matter candidates , such as supersymmetric partners of quarks or leptons . In this talk I will explore how these signatures can be searched for utilizing information collected by the ATLAS experiment during Run 1 of the LHC .The results are presented both in terms of limits on production cross sections and mass isolation regions . Finally , prospects for future investigations with Run 2 data are discussed .This project was done within the framework of the PhD thesis of M . A . M . , overseen by A . S . . Introduction Dark Matter Candidates Supersymmetry ATLAS Experiment Run 1 Results Prospects for Run 2 Searches Conclusions & Outlook References Acknowledgements - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",
        "rewrite_text": "The Large Hadron Collider (LHC) is anticipated to uncover new particles that could serve as candidates for dark matter, such as the supersymmetric partners of quarks or leptons. In this presentation, I will investigate how we can search for these potential signatures using the data gathered by the ATLAS experiment during Run 1 of the LHC. The findings will be discussed in terms of both limits on production cross sections and mass isolation regions. Additionally, I will outline the future research prospects utilizing data from Run 2. This project is part of the PhD thesis of M. A. M., under the supervision of A. S. \n**Introduction, Dark Matter Candidates, Supersymmetry, ATLAS Experiment, Run 1 Results, Prospects for Run 2 Searches, Conclusions & Outlook, References, Acknowledgements.**",
        "ori-fast-z-score": 1.2977713690461004,
        "water-fast-z-score": 4.767570631855362,
        "rewrite-fast-z-score": 1.12089707663561
    },
    {
        "original_text": "We present new near-infrared observations of the edge-on spiral galaxy NGC 891, obtained with VLT/VISIR and Gemini/NIRI at wavelengths between 8 and 13 microns (rest-frame). We detect an extended emission feature that is perpendicular to the galactic disk and extends up to 3 kpc into the halo. The morphology suggests that this structure consists of two components: one located close to the midplane and another one located about 1.5 kpc higher than it. This second component has a temperature of T = 120 K ± 20 K and shows no evidence for significant extinction by dust grains along its line-of-sight. Its luminosity corresponds to a star formation rate of 0.1 M⊙ yr−1. These results are consistent with previous studies suggesting that there exists a population of young stars outside the main body of galaxies. They also provide further support for models where supernovae explosions trigger large-scale outflows of gas and dust which can be responsible for the enrichment of intergalactic space.",
        "watermark_text": "We report new near - infrared observations of the edge - on spiral galaxy NGC 891 , obtained with VLT / VISIR and Gemini / NIRI at wavelengths between 8 and 13 microns ( rest - frame ) . We detect an extended emitted characteristic that is parallel to the galactic disk and extends up to 3 kpc into the halo .The morphology suggests that this formation consists of two parts : one located close to the midplane and another one located about 1 . 5 kpc higher than it . This second component has a temperature of T = 120 K ± 20 K and shows no evidence for significant extinction by dust grains along its line - of - view .Its luminosity corresponds to a star development rate of 0 . 1 [UNK] yr−1 . These conclusions are compatible with previous studies suggesting that there exists a population of young stars outside the main body of galaxies .They also provide further evidence for models where supernovae explosions induce huge - scale outflows of gas and dust which can be responsible for the enrichment of intergalactic space .",
        "rewrite_text": "We present new near-infrared observations of the edge-on spiral galaxy NGC 891, acquired using VLT/VISIR and Gemini/NIRI, at wavelengths ranging from 8 to 13 microns (in the rest frame). Our findings reveal an extended emission feature that is aligned with the galactic disk and stretches up to 3 kpc into the halo. The structure appears to consist of two distinct components: one that is positioned near the midplane and another situated approximately 1.5 kpc above it. The upper component has a temperature of T = 120 K ± 20 K and shows no significant signs of extinction from dust along the line of sight. Its luminosity indicates a star formation rate of 0.1 [UNK] yr−1. These results align with previous studies that suggest the presence of young stars outside the main body of galaxies. Additionally, they lend support to theories proposing that supernova explosions drive large-scale gas and dust outflows, contributing to the enrichment of intergalactic space.",
        "ori-fast-z-score": 1.7232808737106582,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 2.4618298195866544
    },
    {
        "original_text": "In this paper, we present an explicit formula for the maximum allowable delay in a linear time-invariant system with multiple delays by using the concept of passivity index. The proposed method is applied to a biochemical reaction network model consisting of two species interacting through three reactions. We show that our results are consistent with those obtained via numerical simulations. Finally, it should be noted that the proposed approach can also be used as a tool for analyzing other types of networks such as social or economic ones. In recent years there has been growing interest in studying complex dynamical behaviors of biological systems  1  . One important aspect of these studies concerns how different components interact within a cell  2  , which leads naturally to mathematical models based on chemical kinetics  3  .\nThe most common type of kinetic modeling involves ordinary differential equations (ODEs)  4  describing interactions between various molecular species  5  . However, due to the complexity of cellular processes  6  , many ODE models contain several state variables  7, 8  and/or parameters  9  whose values cannot always be determined experimentally  10  . This uncertainty may lead to significant errors when estimating the behavior of the underlying system  11  . To overcome this problem, stochastic approaches have recently been developed  12  . Another possibility consists in considering uncertainties in the form of unknown external disturbances  13  .",
        "watermark_text": "In this paper , we present an explicit formula for the maximum allowable delay in a linear delay - invariant system with many delays by using the notion of passivity index . The proposed approach is applied to a biochemical reaction systems system consisting of two organisms evolving through three compounds .We see that our findings are compatible with those achieved via numerical simulations . Finally , it should be mentioned that the suggested approach can also be used as a way for evaluating other types of networks such as social or economic ones .In recent years there has been growing interest in investigating complex dynamical interactions of biological complexes 1 . One important aspect of these research concerns how various components connect within a cell 2 , which results naturally to numerical models relying on chemical kinetics 3 .The most common type of kinetic modeling involves ordinary differential equations ( ODEs ) 4 describing relationships between various molecular compounds 5 . However , owing to the complexity of cellular processes 6 , many ODE descriptions contain many state components 7 , 8 and / or parameters 9 whose values never always be determined experimentally 10 .This instability may contribute to significant errors when estimating the dynamics of the underlying model 11 . To solve this question , stochastic methods have newly been used 12 .Another possibility consists in considering uncertainties in the form of uncertain external disturbances 13 .",
        "rewrite_text": "In this paper, we introduce a precise formula for the maximum permissible delay in a linear delay-invariant system characterized by multiple delays, utilizing the concept of the passivity index. We apply this methodology to a biochemical reaction system involving two organisms that interact through three compounds. Our results align well with those obtained from numerical simulations. Additionally, it is worth noting that this approach can also be employed to assess other types of networks, such as social or economic systems. Recently, there has been increasing interest in exploring the complex dynamic interactions within biological systems. A crucial aspect of this research focuses on the connectivity of various components within a cell, which leads to the development of numerical models based on chemical kinetics. The predominant form of kinetic modeling is through ordinary differential equations (ODEs), which describe the interactions among different molecular compounds. However, due to the inherent complexity of cellular processes, many ODE models involve numerous state components and/or parameters whose values may not always be determinable through experiments. This uncertainty can lead to significant errors in estimating the dynamics of the underlying model. To address this issue, stochastic methods have recently been introduced. Another option is to consider uncertainties in the form of external disturbances.",
        "ori-fast-z-score": -0.936585811581694,
        "water-fast-z-score": 7.181324987175317,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "The rapid development in the field of intelligent transportation systems (ITS) has led to an increasing demand on wireless communications, which is expected to be fulfilled by using Code Division Multiple Access (CDMA). In this paper we present a novel CDMA-based ITS system that can provide high data rate and low latency services with improved security features compared to existing technologies such as Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth or ZigBee. The proposed system consists of three main components; namely, roadside units (RSUs), mobile vehicles equipped with on-board units (OBUs) and central servers located at traffic control centers (TCCs).\nWe first introduce the basic concepts behind the proposed system followed by describing its key characteristics. Then, we discuss some important issues related to the design of the proposed system including channel modeling, resource allocation algorithms, power consumption analysis and performance evaluation. Finally, simulation results are presented to demonstrate the effectiveness of our proposed system.",
        "watermark_text": "The rapid progress in the field of intelligent transportation devices ( ITS ) has led to an increasing demand on wireless communications , which is expected to be satisfied by using Code Division Multiple Access ( CDMA ) . In this paper we present a new CDMA - based ITS technology that can provide high data rate and low latency services with improved security features relative to existing technologies such as Global Positioning Systems ( GPS ) , Wireless Local Area Networks ( WLANs ) , Bluetooth or ZigBee .The proposed system contains of three principal portions ; namely , roadside centers ( RSUs ) , wireless trucks equipped with on - board units ( OBUs ) and central computers located at road control centers ( TCCs ) . We first introduce the fundamental concepts behind the suggested system followed by describing its essential aspects .Then , we explain some important problems related to the development of the suggested system namely channel modeling , resource sharing algorithms , power consumption evaluation and performance audit . Finally , simulation data are presented to indicate the performance of our proposed system .",
        "rewrite_text": "The rapid advancements in intelligent transportation systems (ITS) have increased the demand for wireless communications, which is anticipated to be addressed through Code Division Multiple Access (CDMA). In this paper, we introduce a new CDMA-based ITS technology designed to deliver high data rates and low latency services, along with enhanced security features compared to existing technologies such as Global Positioning Systems (GPS), Wireless Local Area Networks (WLANs), Bluetooth, and ZigBee. The proposed system consists of three main components: roadside units (RSUs), wireless trucks equipped with on-board units (OBUs), and central computers located at traffic control centers (TCCs). We begin by outlining the fundamental concepts of the proposed system and then delve into its key features. Next, we address several critical challenges associated with the system's development, including channel modeling, resource-sharing algorithms, power consumption assessment, and performance evaluation. Finally, we present simulation results that demonstrate the effectiveness of our proposed system.",
        "ori-fast-z-score": 1.8439088914585775,
        "water-fast-z-score": 7.3326285778338125,
        "rewrite-fast-z-score": 2.49100947511811
    },
    {
        "original_text": "We present new observations and analysis of two nearby supernova remnants, G292.0+1.8 (Cas A) and Kes 79. We find that both remnants show significant asymmetry between their northern and southern halves; however, we argue that this is not due to an asymmetric explosion but rather to differences in the ambient medium into which each half was expanding at the time of its formation.  In Cas A, we observe a large difference in the density structure along the line-of-sight towards the north and south sides of the remnant. The northern side shows evidence for a dense shell-like feature while the southern side appears more uniform with no clear indication of such a shell. This suggests that the progenitor star exploded through a denser region on the northern side than it did on the southern side.   For Kes 79, our X-ray data reveal a bright compact source located near the center of the remnant s southern half. We suggest that this object may be associated with the central engine of the supernova event or possibly even the neutron star itself.",
        "watermark_text": "We present new studies and investigation of two nearby supernova remnants , G292 . 0 + 1 . 8 ( Cas A ) and Kes 79 . We see that both remnants show considerable asymmetry between their western and southern parts ; however , we claim that this is not due to an asymmetric explosion but rather to differences in the atmospheric medium into which each quarter was expanding at the period of its formation .In Cas A , we perceive a large change in the density structure along the line - of - view towards the north and south halves of the remnant . The northern part displays evidence for a dense shell - like feature while the southern side appears more uniform with no clear indication of such a shell .This implies that the progenitor star burst through a denser region on the northern half than it did on the southern side . For Kes 79 , our X - ray data reveal a bright compact source located near the center of the remnant s southern quarter .We suggest that this body may be involved with the main motor of the supernova explosion or possibly also the neutron star itself .",
        "rewrite_text": "We present new studies and investigations of two nearby supernova remnants, G292.0+1.8 (Cas A) and Kes 79. Our findings reveal significant asymmetry between the western and southern regions of both remnants; however, we propose that this asymmetry is not a result of an uneven explosion but rather reflects variations in the surrounding atmospheric medium at the time of their formation. In Cas A, we observe a substantial change in the density structure along the line of sight between the northern and southern halves of the remnant. The northern region exhibits evidence of a dense, shell-like feature, while the southern area appears more uniform, lacking such a distinct shell. This suggests that the progenitor star from which Cas A originated may have emerged from a denser region in the northern half compared to the southern side. For Kes 79, our X-ray data reveal a bright, compact source situated near the center of the southern quarter of the remnant. We propose that this object may be related to the primary engine of the supernova explosion or potentially the neutron star itself.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 6.484597134749389,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "The projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ).\n- The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.",
        "watermark_text": "The projectile fragmentation of 86Kr at 64MeV / nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a laser intensity of 1nAe . The main results are as follows : - A total quantity of about 10000 events have been observed for this study .- The charge distribution is peaked around Z = 40 , but shows also a large contribution between 30 and 40 charges units ( view fig . 1 ) . This implies that the fragments produced by the broken - up of 86Kr are not only light particles like neutrons or protons , but contain many intermediate mass pieces too .- The angular distributions show two peaks related to forward and back emission respectively ( see fig . 2 ) . - The energy spectra produce a maximum around 10 - 12 MeV / u which equals to the most likely kinetic power per nucleon of the emitted objects ( see fig . 3 ) .- The isotopic structure of the fragments is displayed on figure 4 . It can be shown that there is no major variation between the fragment production in the front hemisphere and in the backward one .",
        "rewrite_text": "The projectile fragmentation of 86Kr at an energy of 64 MeV/nucleon has been investigated using the INDRA multidetector in inverse kinematics, employing an 8 cm thick natK target and a laser intensity of 1 nA. The key findings are as follows: Approximately 10,000 events were recorded during this study. The charge distribution peaks around Z = 40 but also includes a significant contribution from charge units ranging between 30 and 40 (see Fig. 1). This indicates that the fragments resulting from the fragmentation of 86Kr comprise not only light particles such as neutrons and protons but also a considerable number of intermediate-mass fragments. The angular distributions reveal two peaks corresponding to forward and backward emissions (see Fig. 2). The energy spectra exhibit a maximum around 10-12 MeV/u, which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see Fig. 3). The isotopic composition of the fragments is illustrated in Fig. 4, which shows no significant differences in fragment production between the forward and backward hemispheres.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -1.1952286093343936
    },
    {
        "original_text": "We present new constructions for locally decodable codes (LDCs) based on nice subsets of finite fields, prime factors of Mersenne numbers, and the Chinese remainder theorem. Our first construction is an explicit family of LDCs with optimal query complexity that are derived from nice subsets of finite fields. We then show how to use these results in conjunction with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the number of primes used in their construction. Finally we give a lower bound on the length of any LDC over a binary alphabet which uses only one-sided queries. The proofs of our main theorems appear at the end of this extended abstract. \nIntroduction\n\nLocally decodable codes (abbreviated as LCDs)\nare error correcting codes where each codeword has associated with it some decoding algorithm that allows efficient recovery of individual bits or symbols when queried about them. In particular, if the codeword is corrupted by up to t errors, then there exists a deterministic polynomial time algorithm that recovers the original uncorrupted word using O(t) queries per symbol. This property makes such codes useful in applications like data storage systems, peer-to-peer networks, and streaming media distribution services. \n \n A large body of work has been devoted towards understanding the trade-off between the rate of the code, its distance, and the query complexity of the decoder. For example, Guruswami et al. (2005), Chan et al. (2006) , and Kopparty & Saraf (2007)  studied the problem of constructing codes with low query complexity while maintaining high rates and distances. Recently, several works have focused on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain amount of corruption. These include the recent breakthrough result of Dvir et al. (2010a) (which improved upon the previous best-known upper bound due to Sudan et al., 2005 ) and the subsequent improvements made by Dvir et al. (2011 ), Feldman et al. (2012 , and Kopparty et al. (2013) . \n \n Another important",
        "watermark_text": "We introduce novel constructions for locally decodable codes ( LDCs ) based on pleasant subsets of finite fields , prime elements of Mersenne numbers , and the Chinese remainder theorem . Our first build is an explicit class of LDCs with optimal query complexity that are derived from good subsets of finite fields .We then show how to use these results in partnership with the Chinese remainder theorem to obtain families of LDCs whose parameters can be tuned by varying the number of primes used in their design . Finally we give a smaller bound on the length of any LDC over a binary alphabet which uses only one - sided queries .The proofs of our major theorems emerge at the end of this enlarged abstract . Introduction Locally decodable codes ( denoted as LCDs ) are mistake correcting codes where each codeword has associated with it some decoding algorithm that enables efficient returning of individual characters or characters when queried about them .In particular , if the codeword is corrupted by up to t errors , then there exists a deterministic polynomial period algorithm that recovers the actual uncorrupted phrase using O ( t ) queries per symbol . This property gives such codes helpful in applications like data storage systems , peer - to - peer systems , and streaming media distribution services .A vast body of research has been focused towards studying the trade - off between the frequency of the code , its location , and the query complexity of the decoder . For instance , Guruswami et al .( 2005 ) , Chan et al . ( 2006 ) , and Kopparty & Saraf ( 2007 ) studied the question of constructing coding with little query complexity while maintaining high frequencies and distances .Recently , various efforts have concentrated on improving the known bounds on the minimum possible query complexity required to decode a single bit given a certain quantity of corruption . These include the recent breakthrough result of Dvir et al .( 2010a ) ( which updated upon the previous good - used upper bound due to Sudan et al . , 2005 ) and the subsequent improvements done by Dvir et al . ( 2011 ) , Feldman et al .( 2012 , and Kopparty et al . ( 2013 ) .Another important",
        "rewrite_text": "We present new constructions for locally decodable codes (LDCs) that utilize pleasant subsets of finite fields, prime elements of Mersenne numbers, and the Chinese remainder theorem. Our initial construction offers an explicit class of LDCs with optimal query complexity, derived from advantageous subsets of finite fields. We further demonstrate how these findings, in conjunction with the Chinese remainder theorem, allow for the creation of families of LDCs with adjustable parameters based on the number of primes incorporated into their design. Additionally, we establish a tighter bound on the length of any LDC over a binary alphabet that utilizes one-sided queries. The proofs for our primary theorems are provided at the conclusion of this extended abstract.\n\nIntroduction: Locally decodable codes (LDCs) are error-correcting codes that include a decoding algorithm for each codeword, which efficiently retrieves individual symbols upon request. Specifically, if a codeword is affected by up to *t* errors, there exists a deterministic polynomial-time algorithm that can recover the original uncorrupted message using O(t) queries per symbol. This capability makes LDCs valuable for applications such as data storage systems, peer-to-peer networks, and streaming media services. A significant amount of research has been devoted to exploring the trade-offs between code frequency, locality, and decoder query complexity. Notably, studies by Guruswami et al. (2005), Chan et al. (2006), and Kopparty & Saraf (2007) focused on creating codes with low query complexity while preserving high frequency and distance. Recently, efforts have intensified to enhance the established bounds on the minimum necessary query complexity to decode a single bit amidst a certain level of corruption. This includes the recent groundbreaking result from Dvir et al. (2010a), which improved upon previous upper bounds established by Sudan et al. (2005), as well as subsequent enhancements by Dvir et al. (2011), Feldman et al. (2012), and Kopparty et al. (2013). Another key area of interest...",
        "ori-fast-z-score": -0.40689422938557973,
        "water-fast-z-score": 7.021870595978444,
        "rewrite-fast-z-score": -0.4181210050035454
    },
    {
        "original_text": "We present new results on the evolution and properties of galactic holes, based on an analysis of deep optical images obtained with the Hubble Space Telescope (HST). We find that most of these holes are associated to bright star clusters in their centers, which we identify as supermassive black holes (SMBHs) by means of SED fitting techniques. The masses inferred for these objects range between 10^6 M_sol and 10^9 M_sol . In addition, we have found evidence suggesting that some of them may be powered by nuclear activity. Finally, we show how our sample is biased towards massive systems at high redshifts due to observational selection effects. Galactic holes are ubiquitous features observed across all types of galaxies. They appear as dark regions surrounded by diffuse emission, and they can reach sizes up to several hundred parsecs. Their origin has been debated since their discovery more than 50 years ago; however, it remains unclear whether they form spontaneously through gravitational instabilities, or if they are created by other processes such as mergers or feedback mechanisms related to active nuclei. Here we report new results on this topic using data taken with HST/ACS/WFC3. Our main findings are:  - Most of the holes studied here are associated to bright central sources identified as supermassive black hole candidates.  - Some of the holes seem to be powered by nuclear activity.  - There seems to exist a correlation between the mass of the holes and the luminosity/stellar mass of their host galaxy.  - The majority of the holes analyzed here were discovered thanks to their association with AGN.",
        "watermark_text": "We report new data on the evolution and features of galactic holes , using on an assessment of deep optical images obtained with the Hubble Space Telescope ( HST ) . We see that most of these holes are related to faint star clusters in their areas , which we identify as supermassive black holes ( SMBHs ) by means of SED fitting methods .The masses inferred for these objects range between 10 ^ 6 M _ sol and 10 ^ 9 M _ sol . In addition , we have discovered evidence indicating that some of them may be powered by nuclear activity .Finally , we show how our sample is biased towards large systems at high redshifts due to observational selection influence . Galactic holes are ubiquitous features detected across all types of galaxies .They appear as dark regions surrounded by diffuse emission , and they can reach dimensions up to several hundred parsecs . Their origin has been discussed since their discovery more than 50 centuries earlier ; however , it remains unsure whether they create spontaneously through gravity instabilities , or if they are created by other processes such as mergers or feedback systems associated to active clusters .Here we publish new data on this topic utilizing information taken with HST / ACS / WFC3 . Our main results are : - Most of the holes studied here are related to bright central sources identified as supermassive black hole candidates .- Some of the holes appear to be powered by nuclear activity . - There seems to exist a correlation between the mass of the holes and the luminosity / stellar size of their target galaxy .- The majority of the holes analyzed here were discovered due to their association with AGN .",
        "rewrite_text": "We present new findings on the evolution and characteristics of galactic holes, based on an analysis of deep optical images captured by the Hubble Space Telescope (HST). Our study reveals that many of these holes are linked to faint star clusters in their vicinity, which we classify as candidates for supermassive black holes (SMBHs) through SED fitting techniques. The masses estimated for these candidates range from \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). Moreover, we have identified evidence suggesting that some of these holes may be driven by nuclear activity. Additionally, we highlight that our sample is biased towards larger systems at higher redshifts, influenced by observational selection. Galactic holes are common features observed in various galaxy types; they manifest as dark regions surrounded by diffuse emission, with sizes extending up to several hundred parsecs. The origins of these holes have been debated since their discovery over five decades ago, yet it remains unclear if they form spontaneously due to gravitational instabilities or through other processes like mergers or feedback mechanisms linked to active clusters. In this publication, we share new data derived from HST/ACS/WFC3 observations, with our key findings including: - A majority of the holes examined correlate with bright central sources identified as candidates for supermassive black holes. - Some of these holes show signs of being powered by nuclear activity. - There appears to be a relationship between the mass of the holes and the luminosity or stellar size of the galaxies they inhabit. - Most of the holes analyzed were identified due to their association with active galactic nuclei (AGN).",
        "ori-fast-z-score": 0.09325048082403138,
        "water-fast-z-score": 6.748852387406954,
        "rewrite-fast-z-score": 1.6431676725154982
    },
    {
        "original_text": "We present an explicit calculation of the non-perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory (HQET). We use this to calculate the leading order contribution to the mass difference between the ground state vector mesons containing a b-quark, i.e., $B^*$- $B$ mixing. The result is compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Our results are consistent within errors but do not agree as well as one would like. This may be due to missing higher-order corrections or systematic uncertainties inherent in both approaches. \n \n Introduction \n \n In recent years there has been considerable interest in studying hadronic systems containing a single heavy quark using the framework provided by heavy quark effective theory (HQT)  1  . One important application of HQT is to study the properties of heavy-light mesons such as the bottomonium system  2  , which can then be used to test our understanding of nonrelativistic quantum mechanics  3  .\n \nIn particular, it is interesting to consider how the masses of these states depend on their spin. For example, the lowest lying bb states have spin-parity J P = 0+ and 1− respectively  4  . These two states mix under the weak interaction through the emission and absorption of virtual gluons  5  . At tree level we find that the lightest physical eigenstate is given by:",
        "watermark_text": "We present an explicit determination of the non - perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory ( HQET ) . We use this to estimate the leading order contribution to the mass ratio between the ground state velocity mesons containing a b - quark , i . e . , $ B ^ * $ - $ B $ mixing .The result is compared with lattice QCD calculations at next - to - leading order in HQET perturbation theory . Our results are compatible within errors but do not comply as well as one would like .This might be due to missing higher - order corrections or systematic uncertainties involved in both approaches . Introduction In recent years there has been substantial interest in investigating hadronic networks featuring a single heavy quark using the framework given by large quark effective theory ( HQT ) 1 .One important use of HQT is to study the properties of heavy - light mesons such as the bottomonium system 2 , which can then be used to test our grasp of nonrelativistic quantum mechanics 3 . In particular , it is curious to consider how the masses of these states change on their spin .For instance , the lowest lying bb states have spin - parity J P = 0 + and 1− respectively 4 . These two states mix under the weak interaction through the emission and emission of virtual gluons 5 .At tree level we find that the lightest physical eigenstate is given by :",
        "rewrite_text": "We provide a clear determination of the non-perturbative renormalization constant for the chromomagnetic operator within heavy quark effective theory (HQET). This enables us to estimate the leading order contribution to the mass ratio for the ground state velocity mesons that contain a b-quark, specifically in the context of $B^*$-$B$ mixing. We compare our findings with lattice QCD calculations at next-to-leading order in HQET perturbation theory. While our results are consistent within the margins of error, they do not align as closely as one might hope. This discrepancy could stem from omitted higher-order corrections or systematic uncertainties inherent to both methodologies.\n\nIn recent years, there has been significant interest in exploring hadronic systems featuring a single heavy quark through the lens of heavy quark effective theory (HQET). One key application of HQET is the investigation of heavy-light meson properties, such as those in the bottomonium system. These studies provide a means to test our understanding of nonrelativistic quantum mechanics. Furthermore, it is intriguing to examine how the masses of these states are affected by their spin. For example, the lowest lying bb states have spin-parity quantum numbers of $J^P = 0^+$ and $1^-$, respectively. These two states mix via weak interactions through the emission and absorption of virtual gluons. At tree level, we find that the lightest physical eigenstate can be expressed as:",
        "ori-fast-z-score": 0.10050378152592121,
        "water-fast-z-score": 4.242640687119285,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The Dalitz plot distribution for the decay D+ ->K-pi+pi+ is measured using data collected by the FOCUS experiment at Fermilab, corresponding to an integrated luminosity of 1 fb-1 . The measurement uses a sample of about 2 million events with one charged track and two neutral clusters reconstructed in the central drift chamber (CDC) and electromagnetic calorimeter (EMC). A maximum likelihood fit is performed on this sample to extract the branching fraction B(D+ ->K-pi+pipi+), which is found to be  _ = (1.55 +/- 0.10 ) x 10-3 , where the uncertainty includes both statistical and systematic contributions.  This result agrees well with previous measurements but has improved precision due to the larger number of signal events used here compared to earlier results. It also improves upon the most recent theoretical prediction based on lattice QCD calculations. The ratio Rc/D between the Cabibbo-suppressed and Cabibbo-favored decays into three pions is determined as Rc/D=(0.84+0.11-0.12)x10-2.",
        "watermark_text": "The Dalitz plot distribution for the decay D + - > K - pi + pi + is measured using data taken by the FOCUS experiment at Fermilab , corresponding to an integrated luminosity of 1 fb - 1 . The measurement involves a sample of about 2 million episodes with one charged track and two neutral nuclei reconstructed in the main drift chamber ( CDC ) and electromagnetic calorimeter ( EMC ) .A maximum likelihood fit is conducted on this data to extract the branching fraction B ( D + - > K - pi + pipi + ) , which is found to be _ = ( 1 . 55 + / - 0 . 10 ) x 10 - 3 , where the uncertainty includes both statistical and systematic contributions . This result agrees well with previous measurements but has improved precision thanks to the bigger quantity of signal events employed here compared to earlier findings .It additionally improves upon the most current theoretical estimate based on lattice QCD calculations . The ratio Rc / D between the Cabibbo - subdued and Cabibbo - preferred decays into three pions is calculated as Rc / D = ( 0 . 84 + 0 . 11 - 0 . 12 ) x10 - 2 .",
        "rewrite_text": "The Dalitz plot distribution for the decay process \\( D^+ \\rightarrow K^- \\pi^+ \\pi^+ \\) has been measured using data collected by the FOCUS experiment at Fermilab, which corresponds to an integrated luminosity of 1 fb\\(^{-1}\\). This measurement includes a sample of approximately 2 million events featuring one charged track and two neutral particles reconstructed using the main drift chamber (CDC) and electromagnetic calorimeter (EMC). A maximum likelihood fit is performed on this dataset to determine the branching fraction \\( B(D^+ \\rightarrow K^- \\pi^+ \\pi^+) \\), yielding a value of \\( (1.55 \\pm 0.10) \\times 10^{-3} \\), with uncertainties accounting for both statistical and systematic factors. This result shows good agreement with earlier measurements but offers enhanced precision due to the larger number of signal events analyzed compared to previous studies. Additionally, it refines the latest theoretical estimates derived from lattice QCD calculations. The ratio \\( R_{c/D} \\) between the Cabibbo-suppressed and Cabibbo-allowed decays into three pions has been calculated as \\( R_{c/D} = (0.84^{+0.11}_{-0.12}) \\times 10^{-2} \\).",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We present the first detection and characterization of infrared extinction law (IRAL) toward an extremely dark cloud core, L183. The IRAL is derived by comparing near-infrared to mid-infrared colors between background stars and foreground objects projected on the same line-of-sight through the cloud. We find that the IRAL shows no significant variation with depth into the cloud down to A V = 1000 mag. This result suggests that dust grains are not significantly modified even under such extreme conditions as those found deep inside dense clouds. Our results also suggest that grain growth may be suppressed in these environments due to efficient shattering caused by collisions among large grains. These findings have important implications for understanding the formation process of planetesimals. \n \n Keywords: Infrared extinction law, Dust properties, Interstellar medium, Shock waves \n \n 1. Introduction \n \n It has been suggested that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can survive against destructive collisions with other particles (e.g., coagulation theory; Ossenkopf & Henning 1994). However, recent observations show that there exist many small dust grains in dense regions where the gas density exceeds 10^6 cm^{-3} (e.g., Stepnik et al. 2003; Pagani et al. 2003), which contradicts this scenario. To resolve this discrepancy, it was proposed that dust grains could be destroyed efficiently via collisional fragmentation when their size becomes comparable to the mean free path of hydrogen molecules (Ormel et al. 2007). \n \n Another possibility is that dust grains do not grow but rather fragment into smaller pieces during collisions (e.g., Blum & Wurm 2008). If so, then we would expect to see some evidence of grain destruction products like sub-micron-sized fragments in dense clouds. Indeed, several observational studies reported the presence of sub-millimeter emission features attributed to silicate and/or carbonaceous materials in dense clouds (e.g., Jones et al. 1993; Chiar et al. 1998; Kessler",
        "watermark_text": "We present the first recognition and description of infrared extinction law ( IRAL ) toward an incredibly dark cloud core , L183 . The IRAL is calculated by using near - infrared to mid - infrared colors between background stars and foreground objects projected on the same line - of - view through the cloud .We see that the IRAL displays no considerable variation with depth into the cloud down to A V = 1000 mag . This result suggests that dust grains are not dramatically distorted even under such extreme circumstances as those observed deep inside dense clouds .Our results also suggest that grain growth could be ceased in these habitats due to efficient crushing caused by collisions among huge grains . These conclusions have important implications for studying the formation system of planetesimals .Keywords : Infrared extinction law , Dust properties , Interstellar medium , Shock effects 1 . Introduction It has been proposed that interstellar dust grains grow up to millimeter sizes or larger within dense molecular clouds because they can endure against damaging collisions with other particles ( e . g . , coagulation hypothesis ; Ossenkopf & Henning 1994 ) .However , recent observations show that there remain many small dust grains in dense areas where the gas density reaches 10 ^ 6 cm ^ { - 3 } ( e . g . , Stepnik et al . 2003 ; Pagani et al .2003 ) , which contradicts this situation . To settle this discrepancy , it was suggested that dust grains could be devastated easily via collisional fragmentation when their size grows equivalent to the mean free path of hydrogen compounds ( Ormel et al .2007 ) . Another possibility is that dust grains do not grow but rather cluster into tiny pieces during collisions ( e . g . , Blum & Wurm 2008 ) .If so , then we may expect to see some evidence of grain loss substances like sub - micron - sized fragments in dense clouds . Indeed , various observational surveys reported the formation of sub - millimeter emission elements owing to silicate and / or carbonaceous materials in dense clouds ( e . g . , Jones et al .1993 ; Chiar et al . 1998 ; Kessler",
        "rewrite_text": "We introduce the initial recognition and characterization of the infrared extinction law (IRAL) in the remarkably dark cloud core L183. The IRAL is derived from analyzing near-infrared to mid-infrared colors of background stars and foreground objects along the same line of sight through the cloud. Our findings indicate that the IRAL does not exhibit significant variation with depth into the cloud, even at an extinction of A_V = 1000 mag. This observation implies that dust grains remain largely unaffected even under the extreme conditions found deep within dense clouds. Additionally, our results indicate that grain growth may be inhibited in these environments due to the effective crushing resulting from collisions between large grains. These insights have important consequences for understanding planetesimal formation systems. \n\nKeywords: Infrared extinction law, Dust properties, Interstellar medium, Shock effects \n\n1. Introduction \nIt has been suggested that interstellar dust grains can grow to millimeter sizes or larger within dense molecular clouds because they withstand destructive collisions with other particles (e.g., coagulation hypothesis; Ossenkopf & Henning 1994). However, recent studies reveal the persistence of many small dust grains in regions where gas density reaches 10^6 cm^{-3} (e.g., Stepnik et al. 2003; Pagani et al. 2003), presenting a contradiction to this hypothesis. To resolve this issue, it has been proposed that dust grains may be easily fragmented through collisions when their size approaches the mean free path of hydrogen compounds (Ormel et al. 2007). Another possibility is that rather than growing, dust grains cluster into smaller fragments during collisions (e.g., Blum & Wurm 2008). If this is the case, we would expect to find evidence of grain loss materials, such as sub-micron-sized fragments, in dense clouds. Indeed, several observational studies have reported the formation of sub-millimeter emissions attributed to silicate and/or carbonaceous materials in these dense environments (e.g., Jones et al. 1993; Chiar et al. 1998; Kessler et al. 2000).",
        "ori-fast-z-score": 0.3965257928590721,
        "water-fast-z-score": 8.379912286910033,
        "rewrite-fast-z-score": 1.9093374820217521
    },
    {
        "original_text": "We present an analysis of helioseismic data for the angular velocity profile of the Sun, which is based on the assumption that the total angular momentum of the Sun is conserved during its evolution. We find that this hypothesis leads to a good agreement between theoretical predictions and observations at all latitudes except near the poles where we observe significant deviations. The discrepancies are explained by taking into account the effects of torsional oscillations in the convective envelope of the Sun. Our results show that these oscillations can be excited by internal magnetic fields generated by differential rotation in the radiative interior of the Sun. This mechanism may also operate in other solar-type stars with strong surface magnetic activity. In our study we use high-precision measurements of the frequency splitting of acoustic modes obtained using space-based instruments (GOLF and MDI) aboard the Solar and Heliospheric Observatory satellite. \n \n Keywords: Angular momentum, Torsional oscillations",
        "watermark_text": "We present an assessment of helioseismic data for the angular velocity profile of the Sun , which is based on the assumption that the total angular velocity of the Sun is conserved during its evolution . We see that this hypothesis leads to a better agreement between theoretical estimates and observations at all latitudes except near the poles where we perceive considerable deviations .The discrepancies are explained by take into consideration the effects of torsional oscillations in the convective envelope of the Sun . Our results show that these oscillations can be excited by inner magnetic waves generated by differential rotation in the radiative interior of the Sun .This mechanism may even exist in other solar - class stars with powerful surface magnetic activity . In our research we utilize wide - precision observations of the frequency separation of acoustic modes obtained using space - based instruments ( GOLF and MDI ) aboard the Solar and Heliospheric Observatory telescope .Keywords: Angular momentum, Torsional oscillations",
        "rewrite_text": "We offer an evaluation of helioseismic data pertaining to the Sun's angular velocity profile, based on the premise that the Sun's total angular velocity remains constant throughout its evolution. This hypothesis results in improved alignment between theoretical predictions and observational data across various latitudes, with notable discrepancies occurring near the poles. These differences can be attributed to the effects of torsional oscillations within the Sun's convective envelope. Our findings indicate that these oscillations may be triggered by magnetic waves from the Sun's inner regions, a result of the differential rotation occurring in its radiative interior. This mechanism could potentially apply to other solar-type stars exhibiting strong surface magnetic activity. In our study, we leverage high-precision observations of acoustic mode frequency separations collected via space-based instruments such as GOLF and MDI aboard the Solar and Heliospheric Observatory. Keywords: Angular momentum, Torsional oscillations.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We present the network topology and trading volume for the first experimental futures exchange, which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab (FML). The FML is designed to provide students with hands-on experience in designing, implementing, and operating financial markets. We find that the network exhibits small-world properties similar to those observed on other real world networks such as social networks or the Internet. In addition we show how the network evolves over time by analyzing its growth rate and degree distribution. Finally, we study the relationship between the number of transactions executed at each node and their respective degrees. Our results suggest that nodes with higher degrees tend to trade more than lower-degree nodes. This finding has important implications for market design since it suggests that traders should be incentivized to increase their connectivity within the network if they want to maximize their trading activity. We also examine whether there are any differences across different types of contracts traded on the exchange.",
        "watermark_text": "We present the group geometry and trading volume for the first prototype futures trader , which was launched in September 2009 as part of the University of Chicago s Financial Markets Lab ( FML ) . The FML is designed to provide scholars with hands - on experience in planning , implementing , and operating financial exchanges .We see that the organization displays low - world properties similar to those observed on other actual world networks such as social systems or the Internet . In addition we study how the organization evolves over time by analyzing its expansion frequency and degree distribution .Finally , we study the relationship between the quantity of transactions executed at each node and their individual degrees . Our results show that nodes with higher degrees prefer to trade more than less - degree servers .This study has crucial consequences for trade design since it demonstrates that consumers should be incentivized to expand their connectivity within the network if they wish to maximize their trading activity . We additionally assess whether there are any similarities across different kinds of contracts traded on the market .",
        "rewrite_text": "We present the group dynamics and trading volume of the inaugural prototype futures trader, launched in September 2009 as part of the University of Chicago's Financial Markets Lab (FML). The FML aims to equip scholars with practical experience in the planning, implementation, and operation of financial exchanges. Our analysis reveals that the organization exhibits low-world properties akin to those found in other real-world networks, such as social networks and the Internet. Furthermore, we explore the organization's evolution over time by examining its expansion frequency and degree distribution. We also investigate the correlation between the number of transactions executed at each node and their respective degrees. Our findings indicate that nodes with higher degrees tend to engage in more trading activity compared to those with lower degrees. This research has significant implications for trade design, suggesting that users should be encouraged to enhance their connectivity within the network to optimize their trading potential. Additionally, we evaluate potential similarities across various types of contracts traded in the market.",
        "ori-fast-z-score": -0.7777777777777778,
        "water-fast-z-score": 6.846754616640485,
        "rewrite-fast-z-score": 0.6469966392206304
    },
    {
        "original_text": "We propose a new inflationary model, which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials. We call this model  smooth hybrid inflation  (SHI). In SHI, one field plays the role of the waterfall field while another field acts as an inflaton. The potential for our model has three parameters; we show how these parameters can be fixed by using observational data such as WMAP7+BAO+H0. Our results indicate that the tensor-to-scalar ratio r=0.0055+0.0005-0.0004 at 95% CL. This value is consistent with recent observations. Moreover, we find that the spectral index ns=1.0145+0.0025-0.0024 at 68% CL. Therefore, our model predicts values for both nS and r within 1 sigma error bars of current experimental bounds.",
        "watermark_text": "We suggest a new inflationary model , which is based on the idea that there are two scalar fields in nature and they interact with each other through their potentials . We call this model smooth hybrid inflation ( SHI ) .In SHI , one field plays the importance of the waterfall field while another field works as an inflaton . The potential for our model has three variables ; we tell how these parameters can be fixed by using observational data such as WMAP7 + BAO + H0 .Our results show that the tensor - to - scalar ratio r = 0 . 0055 + 0 . 0005 - 0 . 0004 at 95 % CL . This value is compatible with recent observations .Moreover , we find that the spectral index ns = 1 . 0145 + 0 . 0025 - 0 . 0024 at 68 % CL . Therefore , our model predicts values for both nS and r within 1 sigma error bars of recent experimental bounds .",
        "rewrite_text": "We propose a novel inflationary model called smooth hybrid inflation (SHI), which is founded on the concept of two interacting scalar fields in nature through their potentials. In this model, one field serves as the waterfall field, while the other functions as the inflaton. Our potential involves three variables, and we demonstrate how these parameters can be determined using observational data, including WMAP7, BAO, and H0. Our findings indicate that the tensor-to-scalar ratio is r = 0.0055 + 0.0005 - 0.0004 at a 95% confidence level, aligning well with recent observations. Additionally, we calculate the spectral index to be ns = 1.0145 + 0.0025 - 0.0024 at a 68% confidence level. Thus, our model yields predictions for both ns and r that fall within one sigma error margins of current experimental limits.",
        "ori-fast-z-score": -0.42857142857142855,
        "water-fast-z-score": 1.8571428571428572,
        "rewrite-fast-z-score": -1.2362450755382013
    },
    {
        "original_text": "We present the results of our study on super star clusters (SSCs) in which we have found that SSCs can be divided into two categories, namely, those having a single mode and those having a double-mode solution for their density profiles. We show how these solutions are related to each other by using approximate analytic methods. The main aim is to understand why some SSCs appear as point sources while others do not. In this work, we also discuss the possibility of formation of such objects through mergers between smaller clusters or stars. Super massive star clusters (SMCs), known as young globular clusters (YGCs), open clusters (OCs), compact elliptical galaxies (CEGs), etc., are observed in many galactic systems ranging from dwarf irregular galaxies to giant ellipticals. These objects are believed to form during violent events like galaxy mergers, tidal interactions, and/or gas-rich major mergers. However, it has been shown recently that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses (10^6-10^7 Msun). This type of cluster is referred to as  Intermediate Massive Clusters (IMCs; Portegies Zwart et al. (2010)). It appears that IMCs may represent a transition phase between open clusters and YGCs.",
        "watermark_text": "We present the conclusion of our research on super star clusters ( SSCs ) in which we have discovered that SSCs can be grouped into two genres , namely , those having a single mode and those having a twin - mode solution for their density characteristics . We see how these solutions are related to each other by using numerical analytic techniques .The main aim is to explain why some SSCs appear as point sources while many do not . In this research , we also discuss the prospect of formation of such objects through mergers between smaller clusters or stars .Super massive star clusters ( SMCs ) , known as early globular complexes ( YGCs ) , close complexes ( OCs ) , compact elliptical galaxies ( CEGs ) , etc . , are observed in many galactic structures ranging from giant irregular clusters to massive ellipticals . These structures are said to form during violent reactions like galaxy mergers , tidal interactions , and / or gas - rich major mergers .However , it has been shown lately that there exists another class of SMCs whose luminosity function shows a peak at intermediate masses ( 10 ^ 6 - 10 ^ 7 Msun ) . This kind of cluster is referred to as Intermediate Massive Clusters ( IMCs ; Portegies Zwart et al .( 2010 ) ) . It suggested that IMCs might represent a transfer stage between open complexes and YGCs .",
        "rewrite_text": "We summarize our findings on super star clusters (SSCs), highlighting that they can be categorized into two types: those exhibiting a single mode and those demonstrating a twin-mode solution for their density features. Our analysis employs numerical techniques to explore the relationship between these two solutions. A primary objective of this study is to clarify why certain SSCs present as point sources while others do not. Additionally, we investigate the potential formation mechanisms of these clusters, particularly through the merging of smaller clusters or stars. Supermassive star clusters (SMCs), also known as early globular complexes (YGCs), open clusters (OCs), and compact elliptical galaxies (CEGs), can be found within various galactic structures, spanning from large irregular clusters to massive elliptical galaxies. These formations are believed to occur during dynamic events such as galaxy mergers, tidal interactions, or gas-rich major mergers. Recent evidence indicates the existence of a distinct class of SMCs characterized by a luminosity function featuring a peak at intermediate masses (10^6 - 10^7 Msun). This category, termed Intermediate Massive Clusters (IMCs) as defined by Portegies Zwart et al. (2010), is thought to represent a transitional stage between open clusters and YGCs.",
        "ori-fast-z-score": -1.58999682000954,
        "water-fast-z-score": 5.270462766947299,
        "rewrite-fast-z-score": -0.20628424925175867
    },
    {
        "original_text": "We have studied the rheology of isotropic networks formed by crosslinking actin filaments with two different concentrations of biotin-avidin linkers, using microrheology experiments on single filament dynamics in combination with macrorheology measurements performed at low frequencies (0.01-10 Hz). We find that both microand macro-rheology are consistent with an elastic network model for which we can extract values for the number density of links between filaments as well as their stiffness. The results show that increasing the concentration of avidin leads to denser networks with stiffer links. This effect is more pronounced when the initial concentration of actin filaments is higher. Our findings suggest that the mechanical properties of actomyosin gels may be tunable through changes in the amount and/or type of crosslinks present within these systems. In living cells, cytoskeletal structures such as stress fibers or focal adhesions provide physical connections between cell components and play important roles in determining cellular mechanics  1  . These structures consist of bundles of semiflexible biopolymers known as actin filaments  2  , which are connected together via specific protein complexes called crosslinks  3  .\nIn recent years there has been growing interest in understanding how the mechanical properties of biological materials depend on the microscopic structure of the underlying networks  4  . For example, it was shown recently that the viscoelasticity of reconstituted actomyosin gels depends strongly on the presence of myosins  5  . However, despite this progress our knowledge about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building blocks remains limited  6  .",
        "watermark_text": "We have researched the rheology of isotropic bands formed by crosslinking actin filaments with two different amounts of biotin - avidin linkers , using microrheology experiments on double filament dynamics in combination with macrorheology measurements completed at low frequencies ( 0 . 01 - 10 Hz ) . We see that both microand macro - rheology are compatible with an elastic network theory for which we can extract parameters for the number density of links between filaments as also as their stiffness .The results show that raising the density of avidin leads to denser networks with stiffer links . This phenomenon is more pronounced when the first concentration of actin filaments is higher .Our findings show that the mechanical behavior of actomyosin gels might be tunable through alterations in the quantity and / or type of crosslinks observed within these systems . In living cells , cytoskeletal structures such as stress fibers or focal adhesions contribute physical bridges between tissue systems and play essential roles in establishing cellular dynamics 1 .These structures composed of bundles of semiflexible biopolymers known as actin filaments 2 , which are connected together via particular protein complexes called crosslinks 3 . In recent years there has been growing interest in understanding how the mechanical behavior of biological materials depend on the microscopic shape of the underlying systems 4 .For instance , it was shown lately that the viscoelasticity of reconstituted actomyosin gels relies highly on the presence of myosins 5 . However , despite this progress our information about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building components remains restricted 6 .",
        "rewrite_text": "We investigated the rheological properties of isotropic bands created by crosslinking actin filaments using two different concentrations of biotin-avidin linkers. This was performed through microrheology experiments examining double filament dynamics, complemented by macrorheology measurements conducted at low frequencies (0.01 - 10 Hz). Our results indicate that both micro- and macrorheological data align with an elastic network theory, allowing us to determine parameters such as the number density of linkages between filaments and their stiffness. We found that increasing the avidin concentration results in denser networks with stiffer connections, a trend that becomes more pronounced at higher initial concentrations of actin filaments. These findings suggest that the mechanical properties of actomyosin gels can be adjusted by modifying the quantity or type of crosslinks present within these systems. In living cells, cytoskeletal elements like stress fibers and focal adhesions serve as physical connections between tissues and play critical roles in regulating cellular dynamics. These structures are made up of bundles of semiflexible biopolymers known as actin filaments, which are interconnected by specific protein complexes called crosslinks. Recently, there has been increasing interest in understanding how the mechanical behavior of biological materials is influenced by the microscopic structure of the underlying systems. For example, it has been demonstrated that the viscoelasticity of reconstituted actomyosin gels is significantly affected by the presence of myosins. Nevertheless, despite these advancements, our understanding of how the macroscopic behavior of complex fluids relates to the microstructure of their constituent components remains limited.",
        "ori-fast-z-score": -0.19611613513818404,
        "water-fast-z-score": 7.19290373059934,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "We present new optical and ultraviolet (UV) spectra for the intrinsically X-ray weak quasar PHL 1811, obtained with the Keck Observatory s HIRES spectrograph in 1998-99. The data cover wavelengths between 3200 A and 10400Å at resolution R = λ/∆λ ≈ 45000. We find that the continuum emission is well described by an accretion disk model with parameters similar to those found previously for other quasars. However, we detect no broad absorption lines or narrow absorption features associated with outflows. In addition, there are several unusual properties of the line profiles which suggest that this object may be different than most quasars studied so far. \n \n Keywords: Quasars; Broad Absorption Lines; Accretion Disk Modeling. 1 Introduction \n \n PHL 1811 was discovered as part of the Palomar-Green survey (Schmidt & Green 1983 ) and has been observed extensively since then. It is one of only two known examples of an X-ray weak quasar (Wilkes et al. 1994) , where the ratio of its soft X-ray flux density to its 2500 Å flux density is less than 0.1. Wilkes et al. (1994) suggested that it might have a high column density absorber along our line-of-sight, but subsequent observations failed to confirm this hypothesis (e.g., Mathur et al. 1995) . Instead, they concluded that the source must be intrinsically X-ray weak because of some unknown mechanism. Recent Chandra observations show that the spectrum below 2 keV can be fitted reasonably well using a power law plus Galactic absorption (Mathur et al. 2002 ) . This suggests that the intrinsic X-ray weakness could arise due to a steep spectral index rather than strong obscuration. Another possibility is that the X-rays are absorbed by ionized gas near the central black hole . \n \n PHL 1811 also shows interesting variability on time scales ranging from hours to years. For example, Wilkes et al. (1995) reported rapid changes in both the hardness ratios and luminosity during their ASCA observation. They interpreted these variations as being caused by partial",
        "watermark_text": "We report new optical and ultraviolet ( UV ) spectra for the intrinsically X - ray weak quasar PHL 1811 , obtained with the Keck Observatory s HIRES spectrograph in 1998 - 99 . The data cover wavelengths between 3200 A and 10400Å at resolution R = λ / [UNK] ≈ 45000 .We see that the continuum emission is well described by an accretion disk model with characteristics similar to those noted formerly for other quasars . However , we find no broad absorption patterns or broad absorption properties associated with outflows .In addition , there are several strange properties of the line profiles which show that this body may be unique than most quasars explored so far . Keywords : Quasars ; Broad Absorption Lines ; Accretion Disk Modeling .1 Introduction PHL 1811 was studied as part of the Palomar - Green survey ( Schmidt & Green 1983 ) and has been observed often since then . It is one of only two recorded examples of an X - ray weak quasar ( Wilkes et al .1994 ) , where the proportion of its soft X - ray flux concentration to its 2500 Å flux concentration is less than 0 . 1 . Wilkes et al .( 1994 ) proposed that it could have a high column velocity absorber along our line - of - seeing , but subsequent observations failed to confirm this claim ( e . g . , Mathur et al . 1995 ) .Instead , they concluded that the origin could be intrinsically X - ray weak because of some unidentified mechanism . Recent Chandra measurements show that the spectrum below 2 keV can be fit reasonably well using a power law plus Galactic absorption ( Mathur et al .2002 ) . This shows that the intrinsic X - ray weak could occur due to a sharp spectral index instead than strong obscuration .Another possibility is that the X - radiation are absorption by ionized gas near the main dark hole . PHL 1811 also shows interesting variability on time ranges varied from hours to decades .For instance , Wilkes et al . ( 1995 ) reported quick changes in both the hardness levels and luminosity during their ASCA measurement .They interpreted these changes as being affected by partial",
        "rewrite_text": "We present new optical and ultraviolet (UV) spectra of the intrinsically X-ray weak quasar PHL 1811, recorded with the Keck Observatory's HIRES spectrograph during 1998-1999. The data spans wavelengths from 3200 Å to 10,400 Å, with a resolution of R = λ/Δλ ≈ 45,000. Our analysis reveals that the continuum emission aligns well with an accretion disk model similar to those observed in other quasars. However, we do not observe any broad absorption features or signatures indicative of outflows. Additionally, we note several unusual characteristics in the line profiles, suggesting that PHL 1811 may be distinct from most other quasars studied to date. \n\nKeywords: Quasars; Broad Absorption Lines; Accretion Disk Modeling.\n\n1. Introduction\n\nPHL 1811 was first examined as part of the Palomar-Green survey (Schmidt & Green 1983) and has been frequently observed since. It stands as one of only two known examples of X-ray weak quasars (Wilkes et al. 1994), characterized by a soft X-ray flux concentration that is less than 0.1 times its flux at 2500 Å. Wilkes et al. (1994) suggested the presence of a high column density absorber along our line of sight, but later studies, including Mathur et al. (1995), did not support this hypothesis. Instead, they proposed that the quasar's intrinsic X-ray weakness might be due to an unidentified mechanism. Recent measurements from Chandra indicate that the spectrum below 2 keV can be adequately modeled using a power law combined with Galactic absorption (Mathur et al. 2002), suggesting that the intrinsic X-ray weakness may stem from a steep spectral index rather than significant obscuration. Another possibility is that the X-ray radiation may be absorbed by ionized gas in proximity to the supermassive black hole. PHL 1811 also exhibits intriguing variability over time scales ranging from hours to decades; for example, Wilkes et al. (1995) documented rapid changes in hardness and luminosity during ASCA observations, attributing these variations to partial                                                             ",
        "ori-fast-z-score": -1.3821894809301762,
        "water-fast-z-score": 7.890700722669491,
        "rewrite-fast-z-score": 0.7924058156930615
    },
    {
        "original_text": "The electronic structure and transport properties of intrinsic ripples on single-layered graphene are investigated by using the tight-binding model with first-principles hopping parameters, which is based on density-functional theory (DFT). The results show that the ripple-induced local strain can lead to an energy gap opening at Dirac point for both armchair and zigzag directions. In addition, it is found that the electron mobility decreases as the amplitude increases due to the increase of scattering centers induced by the ripple structures. These findings may be useful for understanding the physical mechanism behind the experimental observations of rippling effects on the electrical performance of graphene devices. Graphene has attracted great attention because its unique two-dimensional honeycomb lattice structure leads to many novel phenomena such as high carrier mobility  1  , Klein tunneling  2  , valley polarization  3  , etc.. However, recent experiments have shown that the pristine flat monolayer graphene sheets tend to form ripples spontaneously  4  . It was also reported that these ripples could affect the electrical performance of graphene-based devices  5  .\nIn this work we investigate the effect of ripples on the electronic structure and transport properties within the framework of the tight-binding model  6  . We find that the ripple-induced strains can open up an energy gap around the Fermi level for both armchair andzigzag directions. Moreover, the electron mobility decreases as increasing the amplitude of ripples since more scattering centers are introduced into the system.",
        "watermark_text": "The mechanical composition and transport properties of intrinsic ripples on single - layered graphene are examined by using the fast - binding model with first - principles hopping characteristics , which is based on density - functional theory ( DFT ) . The results show that the ripple - caused local tension can lead to an energy gap opening at Dirac position for both armchair and zigzag directions .In addition , it is found that the electron mobility decreases as the frequency rises due to the increase of absorption centers caused by the ripple structures . These conclusions could be valuable for studying the physical mechanism behind the empirical observations of rippling influence on the electrical performance of graphene devices .Graphene has garnered great popularity because its unique two - dimensional honeycomb structure form gives to many novel processes such as long carrier density 1 , Klein tunneling 2 , valley polarization 3 , etc . . However , recent experiments have shown that the pristine rolled monolayer graphene strands tend to form ripples spontaneously 4 .It was also reported that these ripples could affect the electrical performance of graphene - based products 5 . In this research we investigate the impact of ripples on the electronic structure and transport properties within the framework of the tight - binding model 6 .We see that the ripple - mediated strains can offer up an energy gap around the Fermi level for both armchair andzigzag directions . Moreover , the electron mobility decreases as increasing the frequency of ripples since more scattering centers are introduced into the system .",
        "rewrite_text": "The mechanical structure and transport characteristics of intrinsic ripples in single-layer graphene are analyzed using a fast-binding model that incorporates first-principles hopping mechanics rooted in density-functional theory (DFT). The findings indicate that the local tension induced by ripples can create an energy gap at the Dirac point for both armchair and zigzag orientations. Furthermore, it has been observed that as the ripple frequency increases, electron mobility declines due to the proliferation of scattering centers introduced by the ripple formations. These insights are crucial for understanding the underlying physical mechanisms that explain the observed effects of rippling on the electrical performance of graphene devices. Graphene has gained considerable attention because its distinctive two-dimensional honeycomb structure enables various novel phenomena, including long-range carrier density, Klein tunneling, and valley polarization. However, recent experimental observations have shown that pristine rolled monolayer graphene tends to develop ripples spontaneously. It has also been reported that these ripples can influence the electrical properties of graphene-based materials. In this study, we explore the effects of these ripples on the electronic structure and transport properties within the tight-binding framework. Our results demonstrate that the strain induced by ripples can create an energy gap near the Fermi level for both armchair and zigzag directions, while also showing that electron mobility diminishes with increasing ripple frequency due to the introduction of additional scattering centers.",
        "ori-fast-z-score": 0.3746343246326776,
        "water-fast-z-score": 6.80336051416609,
        "rewrite-fast-z-score": 0.7559289460184544
    },
    {
        "original_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time. \n \n In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.",
        "watermark_text": "The Sloan Digital Sky Survey ( SDSS ) is an continuing work to map the distribution and motion of galaxies , quasars , stars , and other celestial entities in space . The fourth information publication was making public on September 30th 2003 .This fifth information update contains more than 100 , 000 new quasar finalists chosen by color categories from the SDSS imaging survey . These are supplemented with about 20 , 000 former recognized quasars that were not included in earlier versions because they did not meet the selection standards for inclusion at that point .In addition to these newly discovered quasars , this list also contains all quasars discovered during the first four seasons of the census as well as those identified since then but which have never to be available openly . A total of over 250 , 000 quasars are now released through this catalog .All of them have been spectroscopically confirmed using observations collected with the reserved 2 . 5 - meter telescope located at Apache Point Observatory near Sacramento , California .",
        "rewrite_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project aimed at mapping the distribution and motion of galaxies, quasars, stars, and other celestial objects in the universe. The fourth data release occurred on September 30, 2003. The fifth data release features over 100,000 newly identified quasar candidates, selected based on color categories from the SDSS imaging survey. This release also includes approximately 20,000 previously recognized quasars that were omitted from earlier releases due to not meeting the original selection criteria. Additionally, the list contains all quasars discovered during the first four seasons of the survey, along with others identified since that have not been made publicly available until now. In total, this catalog now includes more than 250,000 quasars, all of which have been spectroscopically confirmed using data gathered from the dedicated 2.5-meter telescope at Apache Point Observatory in California.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": -1.1338934190276817
    },
    {
        "original_text": "We show that the non-Gaussian distributions observed in financial returns are due to microscopic interactions between traders and their environment, which lead to non-trivial correlations among different time scales. We present an analytical model for these correlations based on random matrix theory (RMT), which is able to reproduce all statistical properties of real market data with high accuracy. \n \n The distribution of stock prices has been studied extensively over many decades  1  . It was found that this distribution can be well approximated by a Gaussian function  2  , but deviations have also been reported  3  . In particular, it has been shown recently  4  that the tails of the return distribution follow a power law decay P(r) ~ r−α with α = 3 ± 0.1. This finding contradicts the predictions of standard models such as Black-Scholes  5  or Heston  6  , where the tail exponent should be equal to 2  7, 8  .\n \nIn order to explain the origin of these deviations we propose here a new approach based on Random Matrix Theory  9  . RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom  10  . For example, RMT has successfully been applied to describe the fluctuations of energy levels  11  , wave functions  12  , spin states  13  , quantum transport  14  , and chaotic scattering  15  . Recently, RMT has also been used to study the statistical properties of stock markets  16  -  22  . Here we will focus on the so-called Dyson Brownian motion  23  , which describes the evolution of a system under the influence of white noise.",
        "watermark_text": "We see that the non - Gaussian distributions found in financial returns are owing to microscopic interactions between traders and their environment , which lead to non - simple correlations among different time ranges . We present an analytical theory for these correlations based on random matrix theory ( RMT ) , which is easy to capture all statistical characteristics of real trading information with high clarity .The function of stock rates has been studied thoroughly over much generations 1 . It was shown that this distribution can be well approximated by a Gaussian distribution 2 , but deviations have also been reported 3 .In particular , it has been shown recently 4 that the tails of the return distribution observe a power law decay P ( r ) ~ r−α with α = 3 ± 0 . 1 . This fact contradicts the estimates of standard models such as Black - Scholes 5 or Heston 6 , where the tail exponent should be equal to 2 7 , 8 .In order to explain the origin of these deviations we undertake here a new approach based on Random Matrix Theory 9 . RMT describes the statistics of complex systems whose dynamics depend on a large number N of degrees of freedom 10 .For instance , RMT has successfully been used to explain the fluctuations of power states 11 , wave systems 12 , spin states 13 , quantum transport 14 , and chaotic scattering 15 . Recently , RMT has additionally been used to study the statistical characteristics of financial traders 16 - 22 .Here we will focus on the so - called Dyson Brownian moving 23 , which explains the evolution of a system under the effects of white sound .",
        "rewrite_text": "Non-Gaussian distributions observed in financial returns are attributed to the microscopic interactions between traders and their environment, resulting in complex correlations across various time scales. To address these correlations, we propose an analytical theory grounded in random matrix theory (RMT), which effectively captures the statistical characteristics of real trading data with remarkable clarity. The behavior of stock prices has been extensively analyzed over the years, revealing that stock return distributions can often be approximated by Gaussian distributions. However, significant deviations have also been noted. Recent studies have particularly highlighted that the tails of the return distribution exhibit a power law decay, described by P(r) ~ r^(-α) with α approximately equal to 3 ± 0.1. This finding contradicts predictions from standard models like Black-Scholes and Heston, which suggest a tail exponent of 2. To explore the origins of these deviations, we adopt a new approach utilizing random matrix theory (RMT). RMT is particularly useful for understanding the statistics of complex systems characterized by a large number of degrees of freedom. It has been effectively applied to analyze various phenomena, including power state fluctuations, wave systems, spin states, quantum transport, and chaotic scattering. Recently, RMT has also been employed to investigate the statistical properties of financial traders. In this work, we will concentrate on Dyson's Brownian motion, which describes the dynamics of a system influenced by white noise.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 5.789863774090244,
        "rewrite-fast-z-score": -0.18107149208503706
    },
    {
        "original_text": "We present new calculations for massive star evolution, nucleosynthesis, and yields using updated nuclear reaction rates and solar abundances. We find that our results are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). The sensitivity is particularly strong when we use the most recent recommended value for the 12C(alpha, gamma)16O cross section. This result has important implications for studies of chemical enrichment by supernovae Ia progenitors. \n \n Keywords: Nuclear reactions; Supernovae; Stellar evolution; Yields \n \n 1 Introduction \n \n In this work we study how uncertainties in nuclear physics affect predictions about stellar evolution and nucleosynthesis. Our goal is to understand better what can be learned from observations of stars and their remnants. For example, it is well known that there exist large discrepancies between observed elemental abundance ratios in metal-poor halo stars and those predicted by standard models of galactic chemical evolution  1  . These differences may arise because some key nuclear processes have been poorly understood or not included in current evolutionary codes  2  , but they could also reflect systematic errors in observational data  3  .\n \nIn order to address these issues, we perform detailed numerical simulations of massive star evolution with different sets of input parameters. Specifically, we consider two cases where the initial mass fraction of helium XHe = 0.25 and 0.30 respectively  4  . We evolve each model until its core collapses into a neutron star. During the collapse phase, we follow the hydrodynamics of the explosion as described in  5  . Afterwards, we compute the composition of the ejecta using an improved version  6  of the one-dimensional post-processing code developed originally by  7  . \n \n 2 Input Physics and Numerical Methods",
        "watermark_text": "We report new models for huge galaxy evolution , nucleosynthesis , and yields using updated atomic reaction rates and solar abundances . We see that our findings are susceptible to uncertainties in the helium burning rate at high temperatures ( T > 2 x 10 ^ 9 K ) .The sensitivity is especially powerful when we using the most current recommended estimate for the 12C ( beta , alpha ) 16O cross area . This result has significant implications for research of chemical enrichment by supernovae Ia progenitors .Keywords : Nuclear effects ; Supernovae ; Stellar evolution ; Yields 1 Introduction In this research we study how uncertainties in nuclear science affect assumptions about stellar evolution and nucleosynthesis . Our goal is to study improve what can be learned from measurements of stars and their remnants .For instance , it is well established that there exist large discrepancies between measured elemental availability proportions in metal - scarce halo stars and those predicted by traditional models of galactic chemical evolution 1 . These similarities may arise because some important radioactive processes have been poorly described or not incorporated in current evolutionary codes 2 , but they may also reflect widespread errors in observational data 3 .In order to overcome these problems , we perform comprehensive numerical simulations of large star evolution with various sets of input parameters . Specifically , we solve two situations where the initial mass fraction of helium XHe = 0 . 25 and 0 . 30 respectively 4 .We evolve each model until its core collapses into a neutron star . During the collapse phase , we follow the hydrodynamics of the explosion as described in 5 .Afterwards , we compute the composition of the ejecta using an modified edition 6 of the one - dimensional post - processing code developed originally by 7 . 2 Input Physics and Numerical Methods",
        "rewrite_text": "We present new models for the evolution of massive galaxies, nucleosynthesis, and element yields, utilizing updated atomic reaction rates and solar abundances. Our results indicate that these findings are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). This sensitivity is particularly pronounced when we apply the latest recommended estimate for the 12C(β, α)16O cross-section. These outcomes carry important implications for the study of chemical enrichment resulting from Type Ia supernova progenitors. \n\n**Keywords:** Nuclear effects; Supernovae; Stellar evolution; Yields \n\n**1. Introduction**  \nIn this research, we investigate how uncertainties in nuclear physics impact our understanding of stellar evolution and nucleosynthesis. Our objective is to enhance the insights gained from observations of stars and their remnants. Notably, significant discrepancies have been observed between the elemental abundance ratios in metal-poor halo stars and those predicted by conventional models of galactic chemical evolution. This divergence may stem from inadequate descriptions of critical radioactive processes or their omission from current evolutionary models; it could also be indicative of widespread inaccuracies in observational data. To address these issues, we conduct extensive numerical simulations of massive star evolution, employing different sets of input parameters. Specifically, we examine two scenarios with initial helium mass fractions of XHe = 0.25 and 0.30, respectively. We evolve each model until core collapse into a neutron star occurs. During the collapse phase, we track the hydrodynamics of the explosion as outlined in previous work. Subsequently, we calculate the composition of the ejected material using a modified version of the one-dimensional post-processing code originally designed by earlier researchers. \n\n**2. Input Physics and Numerical Methods**",
        "ori-fast-z-score": -0.6963106238227914,
        "water-fast-z-score": 7.543856734859843,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M⊙ and 8 M⊙ at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.",
        "watermark_text": "We present the conclusion of our inquiry into how stellar evolution models are influenced by individual atom abundances in stars , concentrating on the sensitivity to changes in helium abundance ( Y ) . We use two different series of evolutionary tracks with varying Y readings for masses between 0 . 8 [UNK] and 8 [UNK] at solar metallicity .The first setting is based on the Padova code while the second one uses the Geneva code . For each track we estimate synthetic spectra using the SPECTRUM code .These synthetic spectra are then utilized as input to obtain the best - fitting characteristics of known high - resolution optical spectra of Galactic open clusters . Our study shows that both codes produce comparable results when fitting these cluster data .However , there are significant variations in the derived ages varying on which coding was used . This discrepancy can be reason by the fact that the Padova lines have been measured without convective overshooting whereas the Geneva tracks include this effect .",
        "rewrite_text": "We present the findings of our investigation into how the abundances of individual atoms, particularly helium (Y), affect stellar evolution models. Our analysis involves two sets of evolutionary tracks with varying helium abundance for stellar masses ranging from 0.8 to 8 at solar metallicity. The first set is derived from the Padova code, while the second set employs the Geneva code. For each evolutionary track, we generate synthetic spectra using the SPECTRUM code. These synthetic spectra serve as input to determine the best-fitting characteristics of high-resolution optical spectra from Galactic open clusters. Our results indicate that both codes yield comparable outcomes when fitting the cluster data; however, we observe significant differences in the derived ages depending on the code used. This variation can be attributed to the Padova tracks being derived without accounting for convective overshooting, while the Geneva tracks do incorporate this factor.",
        "ori-fast-z-score": 1.7556172079419585,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We study the minimal size of a barchan, which is one of the most common types of sand dunes in nature. We show that this problem can be formulated as an optimal control problem for a nonlinear partial differential equation with nonlocal boundary conditions and prove existence of solutions by using variational methods. The numerical results are obtained by applying finite element method to discretize the state equations and then solving them by Newton s iteration scheme. Finally we present some examples illustrating our theoretical findings. Sand dunes are among the most beautiful natural phenomena on Earth. They have been studied extensively since the 19th century  1  . One of the simplest forms of sand dunes is called barchan  2  , see Figure 1 (a). It has a crescent shape with its horns pointing away from the wind direction. Barchans occur naturally over large areas around the world such as Australia  3  , Namibia  4  , Saudi Arabia  5  , China  6  , Japan  7  , etc.. In recent years there has been growing interest in studying mathematical models describing formation of sand dunes  8, 9, 10  .\nIn this work we consider the following model proposed by Kroy et al  11  : \nwhere u(x) denotes the height of the sand bed at position x ∈ Ω =  0, L  × R + ; f > 0 represents the rate of deposition; g ≥ 0 stands for the erosion coefficient; h(u) describes the effect of surface tension; p(x), q(x) represent the pressure terms due to gravity and friction respectively; α > 0 measures the strength of the wind blowing along x-axis; β > 0 characterizes the resistance against the flow of air; γ > 0 is related to the cohesion between grains of sand; θ is the angle of repose of sand particles; c > 0 is the constant volume fraction of sand per unit area; finally, n is the outward normal vector to the boundary Γ = {0 < x < L} × {0} ∪ {L} × R + . For more details about physical meaning of parameters involved in system (1) , please refer to  12  .",
        "watermark_text": "We research the reduced size of a barchan , which is one of the most common kinds of beach dunes in nature . We see that this question can be formulated as an optimal control problem for a nonlinear partial differential function with nonlocal boundary constraints and prove existence of solutions by using variational techniques .The mathematical findings are derived by using finite element method to discretize the state equations and then solving them by Newton s iteration scheme . Finally we present some examples illustrating our theory findings .Sand dunes are among the most beautiful natural creatures on Earth . They have been studied thoroughly since the 19th century 1 .One of the simplest forms of dunes dunes is known barchan 2 , see Figure 1 ( a ) . It has a crescent shape with its horns pointing away from the wind position .Barchans occur naturally over large areas around the world such as Australia 3 , Namibia 4 , Saudi Arabia 5 , China 6 , Japan 7 , etc . . In recent years there has been growing interest in studying numerical models explaining formation of dunes dunes 8 , 9 , 10 .In this study we consider the following model proposed by Kroy et al 11 : where u ( x ) denotes the height of the sand bed at position x ∈ Ω = 0 , L × R + ; f > 0 represents the speed of deposition ; g ≥ 0 stands for the erosion factor ; h ( u ) refers the impact of surface friction ; p ( x ) , q ( x ) describe the pressure terms due to gravity and tension respectively ; α > 0 measures the strength of the wind blowing along x - axis ; β > 0 characterizes the tolerance against the movement of air ; γ > 0 is related to the cohesion between particles of dust ; θ is the angle of repose of dust particles ; c > 0 is the constant volume fraction of dunes per unit area ; finally , n is the outward normal vector to the boundary Γ = { 0 < x < L } × { 0 } ∪ { L } × R + . For more details about physical meaning of components concerned in system ( 1 ) , please refer to 12 .",
        "rewrite_text": "We investigate the reduced size of a barchan, which is one of the most prevalent types of beach dunes found in nature. This inquiry can be framed as an optimal control problem involving a nonlinear partial differential equation with nonlocal boundary constraints. We establish the existence of solutions by employing variational techniques. The mathematical results are obtained through the finite element method to discretize the state equations, which are subsequently solved using Newton's iteration scheme. In conclusion, we provide examples that illustrate our theoretical findings. Sand dunes are among the most captivating natural formations on Earth and have been extensively studied since the 19th century. A barchan, one of the simplest dune forms, is characterized by its crescent shape with its horns oriented away from the wind's direction. Barchans can be found naturally across vast regions worldwide, including in Australia, Namibia, Saudi Arabia, China, and Japan. Recently, there has been an increased interest in developing numerical models to explain the formation of dunes. In this study, we consider a model proposed by Kroy et al., wherein \\( u(x) \\) represents the height of the sand bed at position \\( x \\in \\Omega = (0, L) \\times \\mathbb{R}^+ \\); \\( f > 0 \\) denotes the deposition speed; \\( g \\geq 0 \\) signifies the erosion factor; \\( h(u) \\) describes the effects of surface friction; \\( p(x) \\) and \\( q(x) \\) depict the pressure terms due to gravity and tension, respectively; \\( \\alpha > 0 \\) quantifies the strength of the wind blowing along the x-axis; \\( \\beta > 0 \\) indicates the resistance against air movement; \\( \\gamma > 0 \\) is related to the cohesion among dust particles; \\( \\theta \\) represents the angle of repose for dust particles; \\( c > 0 \\) is the constant volume fraction of dunes per unit area; and \\( n \\) is the outward normal vector to the boundary \\( \\Gamma = \\{ 0 < x < L \\} \\times \\{ 0 \\} \\cup \\{ L \\} \\times \\mathbb{R}^+ \\). For a comprehensive understanding of the physical significance of the components in system (1), please refer to the relevant literature.",
        "ori-fast-z-score": -0.9072647087265548,
        "water-fast-z-score": 5.793241220216576,
        "rewrite-fast-z-score": 1.5360589585634423
    },
    {
        "original_text": "We study the dynamics of an analog recurrent neural network (RNN) driven by temporally fluctuating white noise in its synapses, and show that it can be described by the Thouless-AndersonPalmer (TAP) equations. We derive these TAP equations using replica theory applied to the RNN s partition function. The resulting TAP equations are exact up to corrections which vanish exponentially fast as the number N of neurons increases. In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O(1/N2), where kB denotes Boltzmann s constant and TN ≡ 1/ZN is the temperature associated with the fluctuations in the synapses. This result shows that the TAP approach provides a good description even when the system size is small compared to the typical correlation length of the noise. Finally, we use our results to calculate the stationary state distribution of the RNN and compare this prediction against numerical simulations.",
        "watermark_text": "We explore the dynamics of an analog recurrent brain system ( RNN ) driven by temporally fluctuating white sound in its synapses , and find that it can be described by the Thouless - AndersonPalmer ( TAP ) expressions . We derive these TAP equations using replica theory applied to the RNN s partition function .The resulting TAP equations are exact up to corrections which vanish exponentially rapidly as the number N of neurons increases . In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O ( 1 / N2 ) , where kB indicates Boltzmann s constant and TN ≡ 1 / ZN is the temperature associated with the fluctuations in the synapses .This result suggests that the TAP methodology offers a better characterization even when the process diameter is tiny relative to the typical correlation length of the noise . Finally , we using our findings to estimate the discrete state distribution of the RNN and compare this estimate against numerical simulations .",
        "rewrite_text": "We investigate the dynamics of an analog recurrent neural network (RNN) influenced by temporally varying white noise in its synapses, discovering that it can be accurately described by the Thouless-Anderson-Palmer (TAP) equations. We derive these TAP equations using replica theory in the context of the RNN's partition function. The resulting equations are precise, with corrections that diminish exponentially fast as the number of neurons, \\(N\\), increases. Specifically, we find that the TAP free energy is expressed as \\(F = -k_B T_N \\ln Z_N + O(1/N^2)\\), where \\(k_B\\) is Boltzmann's constant and \\(T_N \\equiv 1/Z_N\\) represents the temperature associated with synaptic fluctuations. This outcome indicates that the TAP framework provides superior characterization, even when the process diameter is significantly smaller than the typical correlation length of the noise. Finally, we apply our results to estimate the discrete state distribution of the RNN and compare this estimation with numerical simulations.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "We study how the effects of galactic winds can be used to explain the observed properties of the metal-poor tail in the stellar metallicity distributions (SMDs) of nearby dwarf spheroidal galaxies (dSph). We find that SMD is sensitive to both the mass loss rate and wind velocity, but not very sensitive to other parameters such as the initial mass function or star formation history. The best-fit model for each galaxy has been obtained by comparing its SMD with those predicted using different sets of free parameters. Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolution stages. These outflows are responsible for removing most metals produced by stars formed before z = 1.5-2.0. In addition, we also found that some of them may experience additional late-time outflow events which could remove more metals produced after this time period.",
        "watermark_text": "We research how the effects of galactic winds can be used to explain the observed properties of the metal - weak tail in the stellar metallicity distributions ( SMDs ) of distant dwarf spheroidal galaxies ( dSph ) . We see that SMD is sensitive to both the mass loss rate and breeze density , but not very sensitive to other parameters such as the early mass value or star formation history .The best - fitting model for each galaxy has been achieved by comparing its SMD with those predicted use different sets of free parameters . Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolved stages .These outflows are responsible for eliminating most metals produced by stars formed before z = 1 . 5 - 2 . 0 . In addition , we also discovered that some of them may experience additional late - time outflow events which potentially remove more metals produced after this time time .",
        "rewrite_text": "We investigate how galactic winds influence the metal-weak tail of stellar metallicity distributions (SMDs) in distant dwarf spheroidal galaxies (dSph). Our findings indicate that the SMD is particularly sensitive to the mass loss rate and wind density, but less influenced by other factors, such as the initial mass or star formation history. We achieved the best-fitting model for each galaxy by comparing its SMD with predictions generated from various sets of free parameters. Our results demonstrate that all these dSph have undergone significant outflows driven by supernova explosions during their early evolutionary phases, which have stripped away a significant portion of the metals produced by stars formed prior to redshift z = 1.5 - 2.0. Furthermore, we discovered that some galaxies may also experience additional late-time outflow events, potentially removing even more metals produced thereafter.",
        "ori-fast-z-score": 1.0533703247651751,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -0.3511234415883917
    },
    {
        "original_text": "We study the glass transition of an ensemble of adhesive hard spheres with repulsive interactions decaying as 1/r6, where r is distance between particles. We find that this system exhibits two distinct relaxation processes at low temperatures: one fast process associated with local rearrangements within clusters of strongly bonded particles; another slower process related to collective motion of these clusters. The latter process can be described by mode-coupling theory (MCT) for colloidal suspensions. However, we show that MCT fails quantitatively when applied directly to our data because it does not take into account the presence of strong bonds which lead to additional slow modes. By introducing a simple modification to MCT, we are able to obtain excellent agreement with experimental results over several decades in time and frequency. This modified version of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg. Our work shows how quantitative tests of theoretical predictions may help improve their accuracy and applicability range.",
        "watermark_text": "We test the glass transition of an ensemble of adhesive solid surfaces with repulsive interactions decaying as 1 / r6 , where r is distance between particles . We see that this scheme exhibits two different relaxation processes at low temperatures : one fast process associated with local rearrangements within clusters of highly bonded particles ; another slower process related to collective motion of these complexes .The latter system can be described by mode - coupling theory ( MCT ) for colloidal suspensions . However , we prove that MCT fails quantitatively when applied directly to our information because it does not take into consideration the presence of stable bonds which lead to extra slow modes .By introducing a simple change to MCT , we are able to obtain excellent compliance with experimental results over numerous years in time and frequency . This amended variant of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg .Our research shows how standardized tests of theoretical estimates may improve improve their accuracy and applicability range .",
        "rewrite_text": "We investigate the glass transition behavior of an ensemble of adhesive solid surfaces characterized by repulsive interactions that decay as 1 / r^6, with r representing the distance between particles. Our findings reveal two distinct relaxation processes at low temperatures: a rapid process associated with local rearrangements within clusters of strongly bonded particles, and a slower process linked to the collective motion of these complexes. The latter can be explained using mode-coupling theory (MCT) for colloidal suspensions. However, we demonstrate that MCT quantitatively fails when applied directly to our data, as it overlooks the influence of stable bonds that give rise to additional slow modes. By introducing a straightforward modification to MCT, we achieve excellent agreement with experimental results across a wide range of time and frequency durations. This adjusted version of MCT also accurately predicts the temperature dependence of the structural relaxation time near the glass transition temperature (Tg). Our research illustrates how standardized assessments of theoretical models can enhance their accuracy and broaden their applicability.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.3937462952891333
    },
    {
        "original_text": "We report on the rapid diffusion of dipolar order in liquid crystals, which is responsible for an enhanced nuclear spin-lattice relaxation rate and thus for a higher degree of dynamic nuclear polarization (DNP). The DNP effect was studied by means of electron paramagnetic resonance spectroscopy at X-band frequencies using a home-built high-field spectrometer equipped with a commercial microwave bridge operating at 2.5 GHz. We found that the enhancement factor increases strongly when approaching the nematic-isotropic phase transition temperature TNI = 35 °C. This finding can be explained by assuming that the molecular mobility decreases rapidly as TNI is approached. In addition to this observation we find that the maximum achievable enhancement factor depends critically on the sample geometry. For samples having a thickness d < 1 mm it reaches values up to 100 times larger than those observed for thicker samples. These results are important for future applications of DNP in NMR experiments performed under extreme conditions such as low temperatures or strong magnetic fields.",
        "watermark_text": "We report on the quick diffusion of dipolar order in liquid crystals , which is responsible for an accelerated nuclear spin - structure contraction speed and therefore for a higher degree of dynamic nuclear polarization ( DNP ) . The DNP phenomenon was studied by means of electron paramagnetic resonance spectroscopy at X - band frequencies using a home - made low - field spectrometer equipped with a commercial microwave bridge operating at 2 . 5 GHz .We showed that the enhancement element increases strongly when approaching the nematic - isotropic phase change temperature TNI = 35 °C . This found can be described by assuming that the molecular density decreases quickly as TNI is neared .In addition to this observation we find that the maximum achievable enhancement coefficient varies critically on the sample geometry . For specimens having a diameter d < 1 mm it hits values up to 100 times bigger than those observed for thicker specimen .These data are important for future applications of DNP in NMR experiments conducted under extreme circumstances such as low temperatures or strong magnetic fields .",
        "rewrite_text": "We present findings on the rapid diffusion of dipolar order in liquid crystals, which leads to an increased speed of nuclear spin-structure contraction and, consequently, a greater degree of dynamic nuclear polarization (DNP). The DNP phenomenon was investigated using electron paramagnetic resonance spectroscopy at X-band frequencies, utilizing a custom low-field spectrometer paired with a commercial microwave bridge operating at 2.5 GHz. Our results demonstrate that the enhancement factor significantly increases as the temperature approaches the nematic-isotropic phase transition temperature (TNI = 35 °C). This observation can be explained by the rapid decrease in molecular density near TNI. Furthermore, we discovered that the maximum achievable enhancement coefficient is highly dependent on sample geometry; for samples with a diameter less than 1 mm, the enhancement can reach up to 100 times higher than that seen in thicker samples. These findings are crucial for the future application of DNP in NMR experiments conducted under extreme conditions, such as low temperatures or strong magnetic fields.",
        "ori-fast-z-score": 0.2182178902359924,
        "water-fast-z-score": 6.32831881684378,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "We study the effect of local optical phonons on the electronic transport properties of a quantum dot system by using the nonequilibrium Green s function method combined with the density functional theory (DFT). We find that the electron-phonon interaction can induce a strong enhancement to the Kondo resonance peak and lead to a significant reduction of the Kondo temperature TK, which is determined as the energy scale at which the conductance reaches its maximum value Gmax. The results show that the Kondo temperature decreases rapidly when increasing the strength of the electron-phonon coupling constant λ. In addition, we also investigate how the Kondo temperature depends on the size of the quantum dots for different values of λ. Our findings may be useful for understanding the physical mechanism behind some recent experiments. Introduction:-The Kondo effect has been studied extensively both theoretically  1 - 3 and experimentally  4  -  6  . It occurs due to the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level  7, 8  , leading to a sharp zero-bias anomaly in the differential conductance  9  . Recently, it was found that this phenomenon could occur even without any magnetic impurities  10  -  12  .\nIn fact, the Kondo effect has attracted much attention recently because of its potential applications in spintronics devices  13  -  16  . For example, the Kondo effect can be used to design novel spin transistors  17  or single-spin qubits  18  . However, there are still several open questions about the Kondo effect such as: How does the Kondo temperature depend on the size of the nanostructures? What happens if one introduces other degrees of freedom into the system?\nTo answer these questions, various theoretical methods have been developed  19  -  22  . Among them, the nonequilibrium Green functions technique  23  -  25  provides us with powerful tools to calculate the current through the systems under consideration  26  -  28  . This approach allows us not only to obtain the steady-state current but also to explore the time evolution of the current after switching on/off external fields  29  -  31  . Moreover, combining the nonequilibrium Green",
        "watermark_text": "We research the impact of local optical phonons on the electronic transport properties of a quantum dot network by using the nonequilibrium Green s function method combined with the density functional theory ( DFT ) . We see that the electron - phonon interaction can induce a weak enhancement to the Kondo resonance peak and lead to a substantial drop of the Kondo temperature TK , which is calculated as the power range at which the conductance reaches its highest value Gmax .The results show that the Kondo temperature falls fast when increasing the strength of the electron - phonon coupling constant κ . In addition , we also investigate how the Kondo temperature varies on the size of the quantum dots for different values of λ .Our findings may be valuable for studying the physical process behind some latest studies . Introduction : - The Kondo phenomenon has been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 .It happens due to the formation of a many - bodies singlet state between localized magnetic moments and conduction electrons near the Fermi level 7 , 8 , leading to a sharp zero - bias anomaly in the differential conductance 9 . Recently , it was shown that this phenomenon might arise even without any magnetic impurities 10 - 12 .In indeed , the Kondo phenomenon has garnered considerable scrutiny lately because of its potential applications in spintronics devices 13 - 16 . For instance , the Kondo phenomenon can be used to build novel spin transistors 17 or single - spinning qubits 18 .However , there are still several open questions about the Kondo phenomenon such as : How does the Kondo temperature depend on the size of the nanostructures ? What happens if one introduces other degrees of liberty into the system ?To answer these problems , various theoretical methods have been used 19 - 22 . Among them , the nonequilibrium Green functions method 23 - 25 offers us with powerful tools to estimate the charge through the systems under consideration 26 - 28 .This method enables us not only to obtain the stable - point charge but also to examine the period evolution of the current after switching on / off external fields 29 - 31 . Moreover , using the nonequilibrium Green",
        "rewrite_text": "We investigate the influence of local optical phonons on the electronic transport properties of a quantum dot network through the application of the nonequilibrium Green's function method in conjunction with density functional theory (DFT). Our findings indicate that electron-phonon interactions can lead to a slight enhancement of the Kondo resonance peak while causing a significant reduction in the Kondo temperature \\( T_K \\), determined by the range of power where the conductance reaches its maximum value \\( G_{\\text{max}} \\). Notably, the Kondo temperature decreases rapidly with an increase in the electron-phonon coupling constant \\( \\kappa \\). Additionally, we explore how the Kondo temperature varies with the size of the quantum dots for different values of \\( \\lambda \\). These results may provide valuable insights for understanding the physical mechanisms behind some recent studies.\n\n**Introduction:** The Kondo phenomenon has been extensively investigated both theoretically and experimentally. It arises from the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level, resulting in a pronounced zero-bias anomaly in the differential conductance. Recent research suggests that this phenomenon can occur even in the absence of magnetic impurities. The Kondo effect has drawn significant attention due to its possible applications in spintronic devices; for example, it could facilitate the development of innovative spin transistors or single-spin qubits. Despite the progress, several questions remain regarding the Kondo phenomenon, such as the dependence of the Kondo temperature on nanostructure size and the impact of introducing additional degrees of freedom into the system. To address these questions, various theoretical approaches have been employed. Among them, the nonequilibrium Green's function method provides powerful tools for analyzing charge transport in the systems in question. This method allows us to determine the stable-point charge and investigate the time evolution of the current following the activation or deactivation of external fields.",
        "ori-fast-z-score": -0.16222142113076254,
        "water-fast-z-score": 7.242717283063319,
        "rewrite-fast-z-score": 0.3287979746107146
    },
    {
        "original_text": "The Cryptoplanet project is an ongoing effort to collect and archive data on the world s cryptocurrencies, including Bitcoin (BTC), Ethereum (ETH) and Litecoin (LTC). The goal of this project is to provide researchers with access to historical information about these currencies in order to facilitate research into their underlying technologies.  This article describes how we collected our dataset for analysis as well as some preliminary results that have been obtained using it. We also describe plans for future work. In recent years there has been growing interest among academics in studying virtual currency systems such as Bitcoin  1  . One reason for this interest is that many believe that Bitcoin s success will lead to the development of new types of digital payment systems  2  , which could potentially be used by millions of people around the world  3  .\nIn addition to its potential use as a means of exchange, Bitcoin may also serve as a platform for other applications  4  . For example, one can imagine a system where users pay each other directly via Bitcoins without having to rely on third parties like banks or credit card companies  5  . Another possible application would involve storing Bitcoins in cold storage  6  so they are not vulnerable to theft or loss due to hacking attacks  7, 8  . Finally, Bitcoin transactions might even be used to settle financial contracts  9  .\nDespite all of these exciting possibilities, however, little academic research has been done on Bitcoin itself  10  . Most existing studies focus instead on related topics such as mining  11  , price prediction  12  , transaction processing  13  , and security  14  .",
        "watermark_text": "The Cryptoplanet effort is an continuing effort to collect and archive data on the world s cryptocurrencies , notably Bitcoin ( BTC ) , Ethereum ( ETH ) and Litecoin ( LTC ) . The goal of this project is to provide researchers with access to historical data about these currencies in order to enable research into their core systems .This page describes how we collected our dataset for study as also as some preliminary outcomes that have been achieved using it . We also describe plans for future study .In past years there has been growing interest among academics in investigating virtual currency networks such as Bitcoin 1 . One reason for this interest is that much think that Bitcoin s success will result to the development of new types of digital payment devices 2 , which could potentially be used by millions of people around the world 3 .In addition to its potential use as a means of exchange , Bitcoin might additionally act as a platform for other applications 4 . For instance , one can suppose a system where users pay each other directly via Bitcoins without having to depend on third parties like banks or credit ticket firms 5 .Another suggested application would include storing Bitcoins in cold storage 6 so they are not vulnerable to robbery or loss due to hacking attacks 7 , 8 . Finally , Bitcoin transactions may even be used to exchange financial agreements 9 .Despite all of these interesting possibilities , however , little academic research has been performed on Bitcoin itself 10 . Most existing studies emphasis instead on related topics such as mining 11 , price prediction 12 , transaction processing 13 , and protection 14 .",
        "rewrite_text": "The Cryptoplanet initiative is an ongoing project aimed at collecting and archiving data on the world's cryptocurrencies, particularly Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC). This project seeks to provide researchers with access to historical data on these currencies to facilitate in-depth studies of their underlying systems. This page outlines our data collection methods and shares some preliminary findings derived from the dataset, as well as our plans for future research. In recent years, there has been a surge of interest among academics in exploring virtual currency networks like Bitcoin. One reason for this growing curiosity is the belief that Bitcoin’s success could lead to the emergence of new types of digital payment solutions, potentially benefiting millions globally. Beyond serving as a medium of exchange, Bitcoin also has the potential to function as a platform for various applications. For example, one could envision a system where users transact directly using Bitcoin without relying on intermediaries like banks or credit card companies. Another proposed use case involves storing Bitcoin in cold storage to safeguard it from theft or hacking threats. Additionally, Bitcoin transactions might be leveraged to facilitate exchanges of financial agreements. However, despite these intriguing possibilities, there is a noticeable lack of academic research focused specifically on Bitcoin itself, with most studies instead concentrating on related areas such as mining, price prediction, transaction processing, and security.",
        "ori-fast-z-score": 1.7960530202677492,
        "water-fast-z-score": 8.746195456274092,
        "rewrite-fast-z-score": 1.2229371288986763
    },
    {
        "original_text": "We present evidence for alignment between the quadrupole, octopole and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy on large angular scales as measured by the Wilkinson Microwave Anisotropy Probe (WMAP). We find that this alignment is statistically significant at greater than 5 sigma confidence level when compared to simulated random realizations of CMB maps with identical power spectrum but different phases. \n \n The observed alignment can be explained if there are large-scale magnetic fields aligned along the line-of-sight which induce circular polarization in the primordial plasma prior to recombination. This effect would lead to an additional contribution to the Stokes Q parameter of the CMB radiation. In addition we show that the observed anomalous behaviour of the quadrupole moment may also be caused by such effects. Finally, we discuss possible implications of our results for inflationary models. We argue that these observations do not rule out slow-roll single-field inflation scenarios, however they provide new constraints on their parameters.",
        "watermark_text": "We present evidence for alignment between the quadrupole , octopole and hexadecapole moments of the cosmic microwave background ( CMB ) temperature anisotropy on huge angular scales as measured by the Wilkinson Microwave Anisotropy Probe ( WMAP ) . We see that this alignment is statistically substantial at greater than 5 sigma confidence rate when compared to modeled random realizations of CMB images with identical power spectrum but different phases .The observed orientation can be understood if there are big - scale magnetic waves aligned along the line - of - view which induce circular polarization in the primordial plasma prior to recombination . This phenomenon might lead to an additional contribution to the Stokes Q dimension of the CMB radiation .In addition we prove that the seen anomalous dynamics of the quadrupole point could also be caused by such consequences . Finally , we explain possible possibilities of our findings for inflationary theories .We argue that these observations do not order out slow - roll single - field inflation scenarios , however they give novel constraints on their parameters .",
        "rewrite_text": "We present evidence for the alignment of the quadrupole, octopole, and hexadecapole moments of cosmic microwave background (CMB) temperature anisotropy over large angular scales, as observed by the Wilkinson Microwave Anisotropy Probe (WMAP). This alignment is statistically significant, exceeding a confidence level of 5 sigma when compared to modeled random realizations of CMB images with the same power spectrum but varying phases. The observed orientation could be explained by the presence of large-scale magnetic waves aligned along the line of sight, which may induce circular polarization in the primordial plasma prior to recombination. This effect could contribute to the Stokes Q component of the CMB radiation. Furthermore, we demonstrate that the anomalous behavior observed in the quadrupole might also result from these influences. Lastly, we explore the implications of our findings for inflationary theories, suggesting that while these observations do not rule out slow-roll single-field inflation models, they do provide new constraints on their parameters.",
        "ori-fast-z-score": 0.3418817293789138,
        "water-fast-z-score": 6.495752858199363,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "We consider a generalization of the Freedman-Townsed model in which there are two types of agents, and each agent has an endowment that is either divisible or indivisible.  We show how to compute equilibria for this game using linear programming techniques. In particular we give a polynomial time algorithm for computing pure Nash equilibria when all goods have unit size (i.e., they can be divided into equal parts). This extends previous results on the same problem by Daskalakis et al. (2007) who considered only the case where all goods were indivisible. The main idea behind our approach is to use a new formulation of the equilibrium conditions as a system of equations with integer coefficients. Our result also implies that finding mixed Nash equilibria is NP-hard even if all goods have unit size. Finally, we present some computational experiments illustrating the performance of our algorithms. Keywords: Economic games",
        "watermark_text": "We consider a generalization of the Freedman - Townsed model in which there are two forms of agents , and each agent has an endowment that is either divisible or indivisible . We see how to compute equilibria for this play using linear programming tools .In particular we give a polynomial time algorithm for modeling true Nash equilibria when all goods have unit size ( i . e . , they can be partition into equal parts ) . This continues preceding results on the same problem by Daskalakis et al .( 2007 ) who considered only the case where all goods were indivisible . The main idea behind our approach is to use a new conception of the equilibrium rules as a system of equations with integer equations .Our result also proves that finding mixed Nash equilibria is NP - hard even if all goods have unit size . Finally , we present some computational experiments illustrating the performance of our algorithms .Keywords: Economic games",
        "rewrite_text": "We explore a generalization of the Freedman-Townsend model featuring two types of agents, each with an endowment that may be either divisible or indivisible. Our focus is on computing equilibria for this scenario using linear programming methods. Specifically, we introduce a polynomial-time algorithm to model true Nash equilibria when all goods have a unit size, meaning they can be evenly divided. This builds upon the prior work of Daskalakis et al. (2007), who only examined the case of indivisible goods. The core idea of our approach involves redefining the rules of equilibrium as a system of integer equations. Additionally, we demonstrate that finding mixed Nash equilibria is NP-hard, even when all goods are of unit size. Lastly, we present computational experiments that highlight the effectiveness of our algorithms. Keywords: Economic games",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 1.0504514628777804
    },
    {
        "original_text": "We study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.",
        "watermark_text": "We research the encoding problem for iterated function schemes ( IFS ) on solenoids by unitary representations of wavelet groups , which are infinite - dimensional Lie groups produced by affine transformations of the real line . We see that if an IFS satisfies certain conditions then it can be interpreted into a unitary representation of its identified wavelet group .This result is applied to prove that every self - similar fractal set with finite local complexity has a unique invariant measure up to scaling factors . The confirmation relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose limits have zero Lebesgue measures .In particular , we obtain a new representation of self - affine carpets as those fractals satisfying this condition . Finally , we give instance demonstrating how our findings apply to some well - famous fractals including Sierpinski gasket , Koch snowflake , von Koch graph , and Menger sponge .",
        "rewrite_text": "We investigate the encoding problem for iterated function schemes (IFS) on solenoids using unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. Our findings indicate that if an IFS meets specific criteria, it can be represented as a unitary action of its corresponding wavelet group. This result is significant in proving that each self-similar fractal set, exhibiting finite local complexity, possesses a unique invariant measure, up to scaling factors. This conclusion is supported by the observation that such fractal sets can be approximated by sequences of compact sets whose limits have zero Lebesgue measure. Notably, we derive a new representation of self-affine carpets as fractals adhering to this criterion. Lastly, we provide examples illustrating the applicability of our results to well-known fractals, including the Sierpinski gasket, Koch snowflake, von Koch graph, and Menger sponge.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 2.141798680385621
    },
    {
        "original_text": "We study droplet excitations in the 2D spin-glass model with nearest-neighbor interactions and random ferromagnetic bonds, which is known to have an infinite number of metastable states at zero temperature. We show that this system has two different types of droplets: small ones are similar to those found in other models studied previously; large droplets are characterized by their fractal structure. The latter type can be viewed as a generalization of the droplet picture proposed earlier for the 3D Ising spin glasses. In addition we find that there exists another class of excitations -the so-called  giant droplets -which are not present in any of these systems. These giant droplets are responsible for the non-universal behavior observed numerically near the critical point. Finally, we argue that our results provide strong numerical support for the existence of a new phase transition line between the paramagnetic state and the spin-glass one. \nI. INTRODUCTORY REMARK\nThe concept of  droplet excitations  was introduced originally within the framework of the mean-field theory  1  . It describes how local perturbations affect global properties of the system. This idea turned out to be very useful when applied to various disordered systems such as spin glasses  2  , structural glasses  3  or vortex lattices  4  .\nIn particular it allowed to explain many features of the low-temperature thermodynamics of spin glasses  5  . However, despite its successes, the original droplet picture suffers from some serious drawbacks  6  : first, it does not take into account fluctuations around the saddle-point solution  7 ; secondly, it predicts a finite density of droplets even at T = 0  8  ; thirdly, it cannot describe properly the dynamics of the system  9  . To overcome these difficulties several modifications were suggested  10  . One of them  11  leads to the following expression for the free energy F(T ) per site: \nwhere f0 is the free-energy density of the reference system (e.g., the pure ferromagnet), Ns is the total number of spins, V is the volume occupied by each droplet",
        "watermark_text": "We test droplet excitations in the 2D spin - glass model with nearest - neighbor interactions and random ferromagnetic bonds , which is known to have an endless number of metastable states at zero temperature . We see that this scheme has two different kinds of droplets : tiny ones are comparable to those present in other models studied ago ; small droplets are marked by their fractal structure .The latter type can be viewed as a generalization of the droplet picture suggested earlier for the 3D Ising spin glasses . In addition we find that there exists another class of excitations - the so - called large droplets - which are not present in any of these systems .These huge droplets are responsible for the non - universal behavior observed numerically near the pivotal point . Finally , we claim that our findings provide strong mathematical support for the existence of a new phase shift line between the paramagnetic state and the spin - glass one .I . INTRODUCTORY REMARK The concept of droplet excitations was introduced originally within the framework of the mean - field principle 1 .It details how local perturbations impact global properties of the system . This idea turned out to be very useful when applied to numerous disordered systems such as spin glasses 2 , structural glasses 3 or vortex lattices 4 .In particular it able to explain many features of the small - temperature thermodynamics of spin glasses 5 . However , despite its victories , the original droplet picture suffers from some serious drawbacks 6 : first , it does not take into consideration fluctuations around the saddle - point solution 7 ; secondly , it predicts a finite concentration of droplets even at T = 0 8 ; thirdly , it lacks explain adequately the dynamics of the system 9 .To solve these problems several amendments were recommended 10 . One of them 11 leads to the following expression for the free energy F ( T ) per site : where f0 is the free - energy density of the reference system ( e . g . , the pure ferromagnet ) , Ns is the total quantity of spinning , V is the volume occupied by each droplet",
        "rewrite_text": "We investigate droplet excitations in the two-dimensional spin-glass model characterized by nearest-neighbor interactions and random ferromagnetic bonds, which is recognized for possessing an infinite number of metastable states at absolute zero temperature. Our analysis reveals two distinct types of droplets: the first are small droplets, comparable to those identified in previous studies of other models; the second, smaller droplets exhibit a unique fractal structure. This latter category can be regarded as an extension of the droplet concept originally proposed for three-dimensional Ising spin glasses. Additionally, we discover a new class of excitations known as large droplets, which do not appear in any previously examined systems. These large droplets account for the non-universal behaviors observed numerically near the critical point. Ultimately, we argue that our results provide robust mathematical evidence for the existence of a new phase transition line separating the paramagnetic phase from the spin-glass phase.\n\n**I. INTRODUCTORY REMARKS**\n\nThe notion of droplet excitations was initially introduced within the mean-field framework, illuminating how local disturbances influence the overall properties of a system. This concept has proven valuable across various disordered systems, including spin glasses, structural glasses, and vortex lattices. It effectively accounts for many aspects of the low-temperature thermodynamics of spin glasses. However, despite its successes, the original droplet framework faces significant limitations: it overlooks fluctuations around saddle-point solutions, incorrectly suggests a finite concentration of droplets even at zero temperature, and fails to adequately describe the system's dynamics. To address these shortcomings, several modifications have been proposed, one of which yields a new expression for the free energy per site, F(T), where f0 represents the free-energy density of a reference system (such as the pure ferromagnet), Ns denotes the total number of spins, and V signifies the volume occupied by each droplet.",
        "ori-fast-z-score": 1.1666666666666667,
        "water-fast-z-score": 7.72046849632954,
        "rewrite-fast-z-score": 0.5696519211398116
    },
    {
        "original_text": "The problem of locating the source(s) of an acoustic emission (AE) event is addressed in this work by using a novel approach based on sparse representation and compressive sensing theory. The proposed method consists of two steps: first, AE signals are decomposed into several components via sparse coding; secondly, these components are used to estimate the locations of multiple AE events with high accuracy. In order to demonstrate its effectiveness, we apply our algorithm to simulated data as well as experimental results obtained from laboratory experiments. We show that the proposed method can locate multiple AE events accurately even when they occur at close time intervals or have similar waveforms. This article is part of a Special Issue entitled  Compressive Sensing for Industrial Applications  guest edited by Prof. Jianfeng Wu. \nIntroduction\n\nAcoustic emission (AE)\nis defined as elastic waves generated within materials due to sudden localised releases of energy  1  . It has been widely applied in non-destructive testing  2  , structural health monitoring  3  , geophysical exploration  4  , etc., where it provides useful information about material damage  5  .\nIn many practical applications such as industrial processes  6  , underground mining  7  , oil/gas pipeline inspection  8  , and so forth, there may be more than one AE source occurring simultaneously  9  . Therefore, accurate localisation of all AE sources becomes important  10  . However, simultaneous AE sources often generate overlapping waveforms; thus conventional methods cannot distinguish them effectively  11  . To address this issue, some researchers have attempted to use advanced signal processing techniques  12  -  14  . For example, Liu et al.  15  developed a new method called  time-frequency analysis  which was able to separate different AE sources successfully. Nevertheless, their method requires prior knowledge of the number of AE sources present in each measurement channel. Moreover, it also relies heavily on user experience to select appropriate parameters  16  .",
        "watermark_text": "The question of locating the origin ( s ) of an acoustic emission ( AE ) event is addressed in this project by using a new approach focusing on sparse representation and compressive monitoring theory . The proposed approach consists of two stages : first , AE messages are decomposed into numerous components via sparse coding ; secondly , these components are using to estimate the places of multiple AE incidents with high clarity .In order to test its effectiveness , we apply our technique to modeled information as well as empirical results collected from laboratory experiments . We see that the suggested method can locate many AE events accurately especially when they occur at close time periods or have related waveforms .This page is part of a Special Issue entitled Compressive Sensing for Industrial Applications guest edited by Prof . Jianfeng Wu . Introduction Acoustic emission ( AE ) is characterized as elastic rays generated within structures owing to unexpected localised releases of power 1 .It has been widely applied in non - destructive testing 2 , structural health monitoring 3 , geophysical surveying 4 , etc . , where it gives valuable info about material injury 5 . In many practical applications such as manufacturing operations 6 , underground mining 7 , oil / coal pipeline inspection 8 , and so forth , there may be more than one AE release occurring simultaneously 9 .Therefore , accurate localisation of all AE sources becomes crucial 10 . However , concurrent AE sources sometimes generate overlapping waveforms ; thus traditional techniques unable distinguish them effectively 11 .To address this question , some researchers have tried to use advanced signal manipulation tactics 12 - 14 . For instance , Liu et al .15 invented a new method called period - frequency analysis which was able to separate distinct AE sources successfully . Nevertheless , their method needs earlier knowledge of the quantity of AE sources observed in each measurement channel .Moreover , it also relies extensively on customer experience to select appropriate parameters 16 .",
        "rewrite_text": "This project tackles the challenge of identifying the origins of acoustic emission (AE) events through an innovative approach centered on sparse representation and compressive monitoring theory. The method unfolds in two phases: initially, AE messages are decomposed into a variety of components via sparse coding; then, these components are utilized to accurately estimate the locations of multiple AE occurrences. To evaluate the effectiveness of this technique, we apply it to both simulated data and empirical results gathered from laboratory experiments. Our findings indicate that the proposed method can reliably pinpoint numerous AE events, particularly when they occur in close temporal proximity or exhibit similar waveforms. This content is part of a Special Issue on Compressive Sensing for Industrial Applications, guest edited by Prof. Jianfeng Wu. \n\nIntroduction: Acoustic emission (AE) is defined as elastic waves produced within structures due to sudden localized energy releases. It has found extensive application in non-destructive testing, structural health monitoring, geophysical surveying, and more, providing crucial insights into material damage. In various practical settings—such as manufacturing operations, underground mining, and oil or coal pipeline inspections—multiple AE releases can occur simultaneously, making accurate localization of all sources essential. However, overlapping waveforms from concurrent AE sources can complicate this task, rendering traditional methods ineffective. To overcome this issue, some researchers have explored advanced signal manipulation techniques. For example, Liu et al. developed a novel method known as period-frequency analysis, which successfully separates distinct AE sources. Despite its effectiveness, their method requires prior knowledge of the number of AE sources present in each measurement channel and heavily depends on user expertise to select the appropriate parameters.",
        "ori-fast-z-score": -1.3315427649795275,
        "water-fast-z-score": 9.419837224354428,
        "rewrite-fast-z-score": -0.7324096128940435
    },
    {
        "original_text": "We study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons . We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV / c while additional contributions are needed to describe data points with larger values of pT .The latter turn out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons . In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions .PACS codes : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I . INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to examine properties of hot and dense materials captured in relativistic nucleus - nucleus collisions 1 .It has been proposed that the interaction between the produced quarkonia and the nearby medium may contribute to their partial melting 2 , i . e . , to a reduction of the bound state masses due to color screening 3 . In this research we present results derived within an efficient field model formulation 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states .This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = w , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq couple 5 . Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have also been used 7 , 8 .",
        "rewrite_text": "We investigate the transverse momentum dependence of J/ψ and [UNK] production cross sections at RHIC energies using an effective field theory framework that encompasses both elastic scattering off quarks and inelastic processes, such as dissociation into open charm or bottom hadrons. Our findings indicate that the observed suppression pattern can be accurately modeled by considering only elastic scattering for transverse momentum (pT) values below 2 GeV/c; however, additional contributions are required to account for data points with higher pT. These higher pT values are primarily influenced by inelastic reactions, including dissociation into open heavy flavor mesons. Notably, we demonstrate that incorporating these factors significantly lowers the predicted nuclear modification factor RAA(pT) compared to earlier analyses that focused solely on elastic interactions. PACS codes: 12.38.Mh, 25.75.-q, 11.10.Kk. \n\nI. INTRODUCTORY REMARK\n\nThe measurement of charmonium (J/ψ) and bottomonium ([UNK]) production serves as a compelling probe for exploring the characteristics of hot and dense matter generated in relativistic nucleus-nucleus collisions. It has been suggested that the interactions between produced quarkonia and the surrounding medium may lead to their partial melting, which corresponds to a reduction in bound state masses due to color screening. In this study, we present results formulated within an efficient field theory model, where the relevant degrees of freedom are quarks and gluons, rather than individual hadronic states. This approach allows us to estimate the total cross section for quarkonium production based on elementary partonic subprocesses involving light quarks (u, d, s) and gluons. These subprocesses include elastic scattering off quarks and gluon-gluon fusion that results in the formation of quarkonia through the creation of a virtual quark-antiquark pair. Additionally, inelastic reactions such as quarkonium dissociation into open heavy-flavor hadrons have also been considered.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 5.115948820192307,
        "rewrite-fast-z-score": 1.01418510567422
    },
    {
        "original_text": "We present the results of our study on mass-loss rates in luminous blue variables (LBVs) based on radio observations at 1.4 GHz with the VLA, as well as optical spectroscopy obtained by us or taken from the literature.  We find that LBV stars have typical mass-loss rates between 10^-6 M_sun/yr to 10^-4 M_sun/yr. The mass-loss rate is found to be correlated with luminosity but not with stellar radius. In addition we report quasi-periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic changes in their circumstellar environments. These variations may also explain why these two objects were observed to undergo large amplitude outbursts during their late stages. This research was supported by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution",
        "watermark_text": "We publish the conclusion of our research on mass - loss rates in luminous blue variables ( LBVs ) based on radio observations at 1 . 4 GHz with the VLA , as well as laser spectroscopy derived by us or taken from the literature . We see that LBV stars have typical mass - loss rates between 10 ^ - 6 M _ sun / yr to 10 ^ - 4 M _ sun / yr .The mass - loss rate is found to be correlated with luminosity but not with stars radius . In addition we study quasi - periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic alterations in their circumstellar environments .These changes may also explain why these two bodies were found to undergo huge amplitude outbursts during their late stages . This research was supported by NASA grant NAG5 - 7262 .Keywords : Mass loss , Stellar evolution",
        "rewrite_text": "We present the findings of our research on mass-loss rates in luminous blue variables (LBVs), derived from 1.4 GHz radio observations with the VLA, as well as laser spectroscopy data from both our studies and the literature. Our results indicate that LBV stars typically exhibit mass-loss rates ranging from \\(10^{-6} M_{\\odot}/\\text{yr}\\) to \\(10^{-4} M_{\\odot}/\\text{yr}\\). We discovered a correlation between mass-loss rates and luminosity, but not with stellar radius. Additionally, we investigate quasi-periodic modulations in radio emissions from supernovae associated with SN 1987A and SN 1993J, likely caused by periodic changes in their circumstellar environments. These variations may also account for the significant outbursts observed in these two supernovae during their later stages. This research was funded by NASA grant NAG5-7262. \nKeywords: Mass loss, Stellar evolution.",
        "ori-fast-z-score": 0.5345224838248488,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 0.6509445549041194
    },
    {
        "original_text": "We present new results on the nature of dust extinction in external galaxies, based on observations with HST/ACS of type Ia supernovae (SNe) at redshifts z=0.1-0.7. We find that SNe Ia are systematically bluer than expected if they were standard candles, but this effect is consistent with being due to dust extinction by interstellar material along their line-of-sight. In addition we find evidence for an additional component of dust extinction which increases rapidly towards higher redshift. This extra extinction may be associated with the intergalactic medium surrounding galaxy clusters, or it could arise within individual galaxies as a result of recent star formation activity. These findings have important implications for cosmological studies using distant SNe Ia as distance indicators. \n \n Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift evolution, Supernova, Supernova Ia",
        "watermark_text": "We report new data on the nature of dust extinction in external galaxies , based on observations with HST / ACS of type Ia supernovae ( SNe ) at redshifts z = 0 . 1 - 0 . 7 . We see that SNe Ia are steadily bluer than expected if they were standard candles , but this effect is compatible with being owing to dust disappearance by interstellar material along their line - of - view .In addition we find proof for an additional element of dust extinction which increases quickly towards higher redshift . This extra extinction may be correlated with the intergalactic medium surrounding galaxy galaxies , or it could occur within individual stars as a product of recent star formation activity .These conclusions have important implications for cosmological experiments using distant SNe Ia as distance indicators . Keywords : Galaxy cluster , Interstellar matter , Intergalactic medium , Redshift formation , Supernova , Supernova Ia",
        "rewrite_text": "We present new findings on the characteristics of dust extinction in external galaxies, derived from HST/ACS observations of type Ia supernovae (SNe) at redshifts ranging from z = 0.1 to z = 0.7. Our analysis reveals that SNe Ia appear consistently bluer than expected if treated as standard candles; however, this phenomenon is consistent with the loss of dust due to interstellar material along their line of sight. Additionally, we have identified evidence of a further component of dust extinction that increases significantly at higher redshifts. This additional extinction may be linked to the intergalactic medium surrounding galaxy clusters, or it may arise within individual stars due to recent star formation activity. These findings carry significant implications for cosmological studies that utilize distant SNe Ia as indicators of distance. Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift formation, Supernova, Supernova Ia.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 0.8427009716003844
    },
    {
        "original_text": "The main goal of this thesis is to experiment with the use of recursion as an alternative way for querying databases, using Prolog (a declarative language) and SQL (the standard query language). The first part presents some background on how databases are structured and queried today; it also introduces the concept of recursion and its application in computer science. In the second section we present our experiments with recursion in Prolog, where we show that recursion can be used to express complex queries without having to resort to procedural code or other languages such as Java. Finally, in the third section we discuss the results obtained by applying recursion in SQL systems. We conclude that although there have been many attempts at implementing recursion in SQL systems, none has succeeded so far due to technical difficulties inherent in these systems. This work shows that recursion could be useful if implemented correctly in SQL systems. Keywords: Database, Recursion, Query",
        "watermark_text": "The main goal of this dissertation is to study with the using of recursion as an additional means for querying data , using Prolog ( a declarative language ) and SQL ( the standard query language ) . The first part offers some details on how databases are structured and queried today ; it also explains the idea of recursion and its use in computer science .In the second chapter we present our experiments with recursion in Prolog , where we explain that recursion can be used to express complex queries without having to resort to procedural code or other languages such as Java . Finally , in the third chapter we explain the results derived by using recursion in SQL systems .We follow that although there have been many efforts at executing recursion in SQL systems , none has successful so far owing to technical problems inherent in these systems . This research shows that recursion could be usable if implemented correctly in SQL systems .Keywords: Database, Recursion, Query",
        "rewrite_text": "The primary objective of this dissertation is to explore the application of recursion as an alternative approach for querying data, utilizing Prolog—a declarative programming language—and SQL, the standard query language. The first section provides an overview of current database structures and querying methods, alongside an explanation of recursion and its significance within computer science. In the second chapter, we detail our experiments with recursion in Prolog, demonstrating how it enables the formulation of complex queries without the need for procedural code or other programming languages like Java. Finally, the third chapter discusses the outcomes associated with employing recursion in SQL systems. We note that, despite numerous attempts to implement recursion in SQL environments, none have succeeded thus far due to inherent technical challenges. This research indicates that proper implementation of recursion in SQL systems could be feasible.  \nKeywords: Database, Recursion, Query.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.425396041156863,
        "rewrite-fast-z-score": 1.8325416653445783
    },
    {
        "original_text": "Social tagging is an emerging technology that allows users to annotate resources with keywords or tags, which are then used by other users in their search and discovery processes. In this work we propose a novel approach to exploit social annotation information for automatic resource discovery. We first introduce the concept of semantic similarity between tags based on WordNet ontology. Then, using the proposed semantic similarity measure as well as user profile information, we develop two algorithms (i) TagRank and (ii) UserTagRank to rank the importance of each tag associated with a given resource. Finally, we conduct experiments over real-world datasets collected from Delicious website to evaluate our approaches. The experimental results show that both TagRank and UserTagRank can significantly improve the performance of existing state-of-the-art techniques. Keywords: Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms. 1 Introduction Social tagging has become one of the most popular ways for people to organize and share web content such as blogs, photos, videos etc., especially among Internet users who have little knowledge about how to use traditional search engines effectively  1  . Users usually assign tags to describe the contents they find interesting so that others may easily discover them later  2  .\nIn recent years there has been growing interest in developing new technologies to take advantage of social tagging systems  3  , including recommender systems  4  , personalized search  5  , question answering  6  , text classification  7  , event detection  8  , and so forth. However, despite these efforts, research into utilizing social tagging data for automatic resource discovery remains relatively unexplored  9  .",
        "watermark_text": "Social tagging is an evolving technique that enables consumers to annotate resources with keywords or tags , which are then utilized by other users in their search and finding processes . In this project we attempt a new approach to use social annotation info for efficient resource search .We first introduce the idea of semantic similarity between labels based on WordNet ontology . Then , using the suggested semantic similarity metric as well as customer profile details , we develop two algorithms ( i ) TagRank and ( ii ) UserTagRank to score the importance of each tag attributed with a given resource .Finally , we conduct experiments over real - time datasets collected from Delicious website to examine our approaches . The empirical results show that both TagRank and UserTagRank can significantly improve the performance of older state - of - the - art methods .Keywords : Semantic Web , Social Tagging , Resource Discovery , Ranking Algorithms . 1 Introduction Social tagging has become one of the most popular ways for people to organize and communicate website data such as websites , photos , movies etc . , particularly among Internet people who have less knowledge about how to use conventional search sites effectively 1 .Users typically give tags to explain the contents they discover attractive so that others would easily discover them later 2 . In recent seasons there has been growing interest in building new strategies to take advantage of social tagging technologies 3 , notably recommender methods 4 , personalized search 5 , question telling 6 , text classification 7 , occasion discovery 8 , and so forth .However , despite these attempts , work into using community tagging data for efficient resource search appears relatively unexplored 9 .",
        "rewrite_text": "Social tagging is a dynamic technique that allows consumers to annotate resources with keywords or tags, which are then leveraged by other users during their search processes. In this project, we explore a novel approach to utilizing social annotation data for more effective resource searching. We start by introducing the concept of semantic similarity between tags based on the WordNet ontology. Using this proposed semantic similarity measure along with customer profile information, we develop two algorithms: (i) TagRank and (ii) UserTagRank, which assess the importance of each tag associated with a specific resource. We then conduct experiments using real-time datasets obtained from the Delicious website to evaluate our methods. The results demonstrate that both TagRank and UserTagRank significantly enhance the performance of existing state-of-the-art techniques. Keywords: Semantic Web, Social Tagging, Resource Discovery, Ranking Algorithms.\n\n1 Introduction\n\nSocial tagging has emerged as one of the most popular methods for individuals to organize and share online content, such as websites, photos, and movies, especially among internet users who may not be adept at utilizing traditional search engines effectively. Users typically apply tags to outline the content they find appealing, making it easier for others to discover it later. Recently, there has been an increasing interest in developing new strategies to harness the potential of social tagging technologies, particularly in areas like recommender systems, personalized searches, question answering, text classification, and event discovery. However, despite these efforts, the application of community tagging data for efficient resource search remains relatively underexplored.",
        "ori-fast-z-score": -2.1213203435596424,
        "water-fast-z-score": 7.950706915615445,
        "rewrite-fast-z-score": -1.0864289525102224
    },
    {
        "original_text": "We report on results obtained by INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655â€“40 (Nova Muscae 1991). The source was observed in the 20-100 keV range for about 100 days, starting at MJD 53000 and ending at MJD 53300. We have analyzed these data using both ISGRI and SPI instruments aboard INTEGRAL satellite. In addition to the main spectral component which is well described by a power law model modified by an exponential cutoff, we find that there are two additional components present in the spectrum. One of them has been previously reported by other authors but its origin remains unclear. Another one appears only when fitting the whole dataset simultaneously with all three models considered here -power law plus exponential cut-off, broken power law or Comptonization model-. This new feature can be interpreted either as a reflection hump produced by cold material surrounding the central X-ray source or as a broad iron line around 6.4 keV.",
        "watermark_text": "We report on findings obtained by INTEGRAL observations during the 2005 outburst of the dark hole member GRO J1655â€ “ 40 ( Nova Muscae 1991 ) . The source was seen in the 20 - 100 keV range for about 100 months , beginning at MJD 53000 and ending at MJD 53300 .We have analyzed these information using both ISGRI and SPI instruments aboard INTEGRAL satellite . In addition to the main spectral component which is well described by a power law description altered by an exponential cutoff , we find that there are two additional components available in the spectrum .One of them has been previously reported by other researchers but its identity remains unsure . Another one appears only when fitting the whole dataset jointly with all three models described here - energy law plus exponential cutting - off , broken power law or Comptonization model - .This new feature can be interpreted either as a absorption hump produced by cold matter surrounding the main X - ray source or as a broad iron line around 6 . 4 keV .",
        "rewrite_text": "We present findings from INTEGRAL observations during the 2005 outburst of the black hole candidate GRO J1655-40 (Nova Muscae 1991). The source was monitored in the 20-100 keV energy range over a period of approximately 100 months, from MJD 53000 to MJD 53300. Our analysis utilized data from both the ISGRI and SPI instruments on the INTEGRAL satellite. In addition to the primary spectral component, which is well-represented by a power law modified by an exponential cutoff, we identified two additional components in the spectrum. One of these has been previously reported by other researchers, though its nature remains uncertain. The other component emerges only when the entire dataset is fitted simultaneously using all three models described: power law with exponential cutoff, broken power law, or Comptonization model. This new feature may be interpreted as an absorption hump caused by cold matter surrounding the main X-ray source or as a broad iron line near 6.4 keV.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 2.9541957835039856
    },
    {
        "original_text": "The design and kinematics analysis of parallel kinematic machine tools (PKMTs) is presented in this work using kinetic-static performance criteria. The proposed approach considers the dynamic behavior of PKMTs during their operation, which has been neglected by previous works on PKMTs. In addition to the static stiffness matrix, the inertia properties are also considered for the evaluation of the overall dynamic response of PKMTs. A new method based on the concept of virtual joints is developed to calculate the mass distribution along each leg of the PKMT under consideration. This information can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis. Finally, two different PKMTs with three degrees-of-freedom per joint are designed and analyzed using the proposed methodology. It was found that the first PKMT exhibits better dynamic characteristics than its counterpart due to its lower natural frequencies and higher damping ratios.",
        "watermark_text": "The design and kinematics analysis of parallel kinematic machine tools ( PKMTs ) is detailed in this work using kinetic - static performance criteria . The proposed approach considers the dynamic behavior of PKMTs during their operation , which has been neglected by previous works on PKMTs .In addition to the static stiffness matrix , the inertia characteristics are also considered for the evaluation of the overall dynamic response of PKMTs . A modern algorithm based on the idea of virtual joints is developed to estimate the mass distribution along each leg of the PKMT under consideration .This knowledge can be used as an input data set for further dynamic analyses such as modal or harmonic vibration analysis . Finally , two different PKMTs with three degrees - of - independence per joint are built and evaluated using the suggested methodology .It was shown that the first PKMT displays better dynamic characteristics than its predecessor due to its lower natural bandwidth and larger damping ratios .",
        "rewrite_text": "This work presents a comprehensive design and kinematic analysis of parallel kinematic machine tools (PKMTs) utilizing kinetic-static performance criteria. The proposed method takes into account the dynamic behavior of PKMTs during operation, an aspect often overlooked in previous studies. Alongside the static stiffness matrix, inertia characteristics are also included to assess the overall dynamic response of PKMTs. A modern algorithm, based on the concept of virtual joints, has been developed to estimate the mass distribution along each leg of the PKMT in question. This information can serve as input for further dynamic analyses, such as modal or harmonic vibration studies. Ultimately, two different PKMTs, each with three degrees of freedom per joint, were constructed and evaluated using this methodology. Results indicate that the first PKMT exhibits superior dynamic characteristics compared to its predecessor, owing to its lower natural frequency bandwidth and higher damping ratios.",
        "ori-fast-z-score": 1.6059101370939322,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We introduce the notion of intersection bodies in arbitrary dimensions, generalizing the classical concept for dimension two to higher dimensions. We show that intersection bodies are characterized by their Fourier transforms which we call generalized cosine transforms (GCT). The GCTs can be used as an alternative tool to study intersection bodies. In particular, we prove that intersection bodies have positive volume if and only if they are convex. This is done using a new characterization of intersection bodies via their support functions. Finally, we give some examples of intersection bodies in three dimensions. Keywords: Intersection body; Support function; Convexity; Volume; Fourier transform; Three-dimensional space. 1 Introduction Let K n denote the set of all origin-symmetric convex bodies in R n . For any K ∈ K n , let V(K) = |K|/|B n 2 | where | · | denotes Lebesgue measure on R n . Then V : K n →  0, 1  is called the volume functional. A compactly supported continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m. If f has no zeros then it is uniquely determined up to multiplication by a constant. It follows immediately that every spherical harmonic of degree m satisfies the following properties:\n(1) |f (x)| ≤ 1; (2) f (−x) = f (x), x ∈ S n−1 ; (3) (Spherical harmonics form an orthonormal basis.) Definition 1. An origin-symmetric convex body K ∈ K n is said to be an intersection body if there exists a non-negative real number λ such that its surface area measure σ K satisfies",
        "watermark_text": "We introduce the notion of junction bodies in arbitrary dimensions , generalizing the classical concept for dimension two to higher dimensions . We see that intersection bodies are characterized by their Fourier functions which we call generalized cosine transforms ( GCT ) .The GCTs can be used as an additional tool to study junction bodies . In particular , we prove that intersection bodies have positive volume if and only if they are convex .This is accomplished using a new definition of junction bodies via their support functions . Finally , we give some examples of intersection bodies in three dimensions .Keywords : Intersection body ; Support function ; Convexity ; Volume ; Fourier integral ; Three - dimensional space . 1 Introduction Let K n denote the group of all origin - symmetric convex body in R n .For any K ∈ K n , let V ( K ) = | K | / | B n 2 | where | · | indicates Lebesgue measure on R n . Then V : K n → 0 , 1 is called the volume functional .A compactly backed continuous function f : S n−1 → C with unit integral will be called a spherical harmonic of degree m . If g has no zeros then it is uniquely determined up to multiplication by a constant . It follows instantly that every spherical harmonic of degree m satisfies the following properties : ( 1 ) | h ( x ) | ≤ 1 ; ( 2 ) f ( −x ) = g ( x ) , x ∈ S n−1 ; ( 3 ) ( Spherical harmonics form an orthonormal basis . )Definition 1 . An origin - symmetric convex body K ∈ K n is said to be an intersection body if there exists a non - negative real number λ such that its surface area measure σ K satisfies",
        "rewrite_text": "We present the concept of junction bodies across arbitrary dimensions, extending the traditional definition from two-dimensional spaces to higher dimensions. We demonstrate that intersection bodies are defined by their Fourier functions, referred to as generalized cosine transforms (GCT). These GCTs serve as an additional resource for investigating junction bodies. Notably, we establish that intersection bodies possess positive volume if and only if they are convex. This proof utilizes a novel definition of junction bodies based on their support functions. Additionally, we provide examples of intersection bodies in three-dimensional space. \n\n**Keywords:** Intersection body; Support function; Convexity; Volume; Fourier integral; Three-dimensional space.\n\n**1 Introduction**  \nLet \\( K_n \\) represent the collection of all origin-symmetric convex bodies in \\( \\mathbb{R}^n \\). For any \\( K \\in K_n \\), we define \\( V(K) = \\frac{|K|}{|B^n_2|} \\), where \\( |\\cdot| \\) denotes Lebesgue measure in \\( \\mathbb{R}^n \\). The function \\( V: K_n \\to [0, 1] \\) is termed the volume functional. A compactly supported continuous function \\( f: S^{n-1} \\to \\mathbb{C} \\) with unit integral is termed a spherical harmonic of degree \\( m \\). If \\( g \\) has no zeros, it can be uniquely specified up to a constant factor. It follows that every spherical harmonic of degree \\( m \\) satisfies the properties: (1) \\( |h(x)| \\leq 1 \\); (2) \\( f(-x) = g(x) \\) for \\( x \\in S^{n-1} \\); (3) spherical harmonics form an orthonormal basis. \n\n**Definition 1:** An origin-symmetric convex body \\( K \\in K_n \\) is classified as an intersection body if there exists a non-negative real number \\( \\lambda \\) such that its surface area measure \\( \\sigma_K \\) satisfies...",
        "ori-fast-z-score": 0.19069251784911848,
        "water-fast-z-score": 3.8138503569823694,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "We report the first ultrafast electron nanocrystallographic study on size-selected gold nanoparticles (AuNPs). The AuNPs were prepared in solution and deposited onto carbon-coated copper grids for transmission electron microscopy studies. Time-resolved pump-probe experiments with femtosecond resolution were performed at beamline U41-PGM, MAX IV Laboratory, Sweden. We observed that the lattice expansion is anisotropic along different crystallographic directions within individual particles. This observation can be explained by considering the effect of surface stress induced during particle growth. In addition to this, we found that the lattice expansion depends strongly on the nanoparticle sizes. These results are important for understanding how the properties of nanoparticles evolve as their dimensions decrease towards atomic scale. A new technique has been developed recently which allows one to probe structural dynamics of materials down to the atomic level using ultrashort X-ray pulses  1  . However, it remains challenging to perform time-resolved measurements on single crystals or nanoparticles due to difficulties associated with sample preparation  2  , data collection  3  , and analysis  4  .\nIn order to overcome these challenges, researchers have started exploring alternative techniques such as ultrafast electron nanocrystalography  5  -  8  . In this method, an intense femtosecond laser pulse is used to excite electrons into unoccupied states above Fermi energy E F . Subsequently, photoelectrons emitted from excited atoms travel through the crystal and scatter off neighboring atoms  9  . By measuring the angular distribution of scattered photoelectrons, information about the structure of the material under investigation can be obtained  10  . Since the scattering cross section increases rapidly when photoelectrons approach the Brillouin zone boundary  11  , the photoelectron diffraction pattern contains more Bragg peaks than conventional powder patterns  12  . Therefore, the photoelectron diffraction pattern provides higher spatial resolution compared to traditional powder methods  13  .",
        "watermark_text": "We report the first ultrafast electron nanocrystallographic study on size - selected gold nanoparticles ( AuNPs ) . The AuNPs were prepared in solution and deposited onto carbon - etched copper grids for propagation electron microscopy studies .Time - resolved pump - probe studies with femtosecond sensitivity were performed at beamline U41 - PGM , MAX IV Laboratory , Sweden . We observed that the crystal growth is anisotropic along various crystallographic paths within various particles .This assumption can be described by examining the impact of exterior stress resulting during particle growth . In addition to this , we learned that the crystal growth depends strongly on the nanoparticle sizes .These data are important for studying how the properties of nanoparticles develop as their height shift towards atomic scale . A different method has been constructed recently which allows one to probe mechanical mechanics of substances down to the atomic level using ultrashort X - ray pulses 1 .However , it remains challenging to conduct time - resolved calculations on single crystals or nanoparticles due to difficulties related with data preparation 2 , data analysis 3 , and identification 4 . In try to overcome these problems , researchers have started pursuing alternative techniques such as ultrafast electron nanocrystalography 5 - 8 .In this technology , an aggressive femtosecond laser wave is utilized to excite electrons into unoccupied states above Fermi energy E F . Subsequently , photoelectrons sent from excited atoms journey through the crystal and scatter off nearby electrons 9 .By measuring the angular distribution of distributed photoelectrons , info about the composition of the material under investigation can be obtained 10 . Since the scattering cross area grows swiftly when photoelectrons encounter the Brillouin zone boundary 11 , the photoelectron diffraction scheme contains more Bragg peaks than conventional powder schemes 12 .Therefore , the photoelectron diffraction type gives higher spatial resolution compared to conventional powder technique 13 .",
        "rewrite_text": "We present the first ultrafast electron nanocrystallographic study of size-selected gold nanoparticles (AuNPs). The AuNPs were synthesized in solution and subsequently deposited onto carbon-etched copper grids for transmission electron microscopy analysis. Time-resolved pump-probe experiments with femtosecond temporal resolution were conducted at beamline U41-PGM at the MAX IV Laboratory in Sweden. Our findings indicate that crystal growth exhibits anisotropy along various crystallographic directions within the nanoparticles. This observation can be explained by analyzing the external stress experienced during the growth process. Moreover, we discovered that the growth rate of the crystals is highly dependent on the size of the nanoparticles. These insights are crucial for understanding how nanoparticle properties evolve as their dimensions approach the atomic scale. Recently, a new method has been developed that enables the investigation of mechanical properties at the atomic level using ultrashort X-ray pulses. However, conducting time-resolved studies on single crystals or nanoparticles presents significant challenges related to data preparation, analysis, and identification. To address these challenges, researchers are exploring alternative methods such as ultrafast electron nanocrystallography. This approach employs an intense femtosecond laser to excite electrons into unoccupied states above the Fermi energy (EF). The resulting photoelectrons travel through the crystal and scatter off neighboring electrons. By measuring the angular distribution of the emitted photoelectrons, valuable information about the material's composition can be obtained. The scattering cross-section increases significantly when photoelectrons approach the Brillouin zone boundary, allowing the photoelectron diffraction method to reveal more Bragg peaks compared to conventional powder diffraction techniques. Consequently, photoelectron diffraction offers enhanced spatial resolution over traditional methods.",
        "ori-fast-z-score": -1.9369494184529936,
        "water-fast-z-score": 7.944299095067362,
        "rewrite-fast-z-score": -0.9538209664765319
    },
    {
        "original_text": "We present an overview of our recent work on vector meson production in heavy ion collisions at RHIC and LHC energies, based on holographic QCD models with chiral symmetry breaking (AdS/QCD). We discuss how these models can be used to calculate hadronic observables such as transverse momentum spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear reactions. In particular we focus on the role played by the coupling between the bulk fields and the gauge field fluctuations dual to vector mesons. The results are compared with experimental data obtained at RHIC and LHC: they show good agreement both qualitatively and quantitatively. \n \n Keywords: Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality \n \n 1 Introduction \n \n One of the most exciting discoveries made recently at RHIC is that strongly interacting matter behaves like a nearly perfect fluid  1  . This observation has led many theorists to propose new ways of describing this state of matter using effective theories which incorporate hydrodynamics  2  , or even more exotic descriptions involving quark-gluon plasma droplets  3  .\n \nIn order to understand better what happens during the early stages of heavy-ion collisions it would be very useful if one could study experimentally the properties of the hot dense medium created in those collisions. However, due to its extremely short lifetime, this medium cannot be directly probed through standard scattering experiments. Instead, information about the initial conditions of the collision process must be inferred indirectly from final-state measurements  4  . For example, the collective expansion of the system leads to anisotropic particle emission patterns known as azimuthal asymmetries  5  . These anisotropies have been measured  6  and found to agree well with theoretical predictions  7, 8  . \n \n Another important observable characterizing the dynamics of the expanding fireball is the spectrum of emitted particles  9  . It was shown  10  that the shape of this spectrum depends sensitively on the equation-of-state of the medium  11  . Moreover, the observed suppression  12  of high-pT hadrons",
        "watermark_text": "We present an overview of our latest work on vector meson production in heavy atom collisions at RHIC and LHC energies , using on holographic QCD models with chiral symmetry breaking ( AdS / QCD ) . We discuss how these models can be used to estimate hadronic observables such as transverse acceleration spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear compounds .In particular we focus on the part played by the interaction between the bulk fields and the gauge field fluctuations dual to vector mesons . The results are compared with experimental evidence derived at RHIC and LHC : they show good agreement both qualitatively and quantitatively .Keywords : Vector Mesons , Heavy Ion Collisions , Holography , Chiral Symmetry Breaking , Gauge / Gravity Duality 1 Introduction One of the most exciting advances making lately at RHIC is that strongly interacting material behaves like a nearly perfect fluid 1 . This prediction has led many theorists to propose innovative ways of describing this state of matter utilizing effective models which employ hydrodynamics 2 , or especially more exotic representations featuring quark - gluon liquid droplets 3 .In order to explain better what comes during the early stages of large - ion collisions it would be very useful if one might explore experimentally the properties of the hot dense medium produced in those collisions . However , owing to its incredibly small life , this medium unable be closely probed through conventional absorption studies .Instead , info about the first environments of the collision mechanism must be inferred indirectly from final - state measurements 4 . For instance , the collective expansion of the process results to anisotropic particle emission events known as azimuthal asymmetries 5 .These anisotropies have been measured 6 and found to agree well with theoretical estimates 7 , 8 . Another important observable characterizing the dynamics of the increasing fireball is the spectrum of emission particles 9 .It was shown 10 that the form of this spectrum relies sensitively on the equation - of - state of the medium 11 . Moreover , the seen suppression 12 of high - pT hadrons",
        "rewrite_text": "We provide a summary of our recent research on vector meson production in heavy ion collisions at RHIC and LHC energies, utilizing holographic QCD models that incorporate chiral symmetry breaking (AdS/QCD). Our discussion highlights how these models can estimate hadronic observables, including transverse acceleration spectra and elliptic flow coefficients for light quarks and gluons produced within nuclear matter. We particularly emphasize the role of the interaction between bulk fields and gauge field fluctuations corresponding to vector mesons. Our findings are compared with experimental data from RHIC and LHC, demonstrating strong qualitative and quantitative agreement.\n\n**Keywords:** Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality.\n\n**1 Introduction**  \nOne of the most exciting recent developments at RHIC is the observation that strongly interacting matter behaves like a nearly perfect fluid. This finding has led theorists to propose innovative models for describing this state of matter, employing hydrodynamics and even more exotic representations such as quark-gluon liquid droplets. To gain a better understanding of the early stages of large-ion collisions, it would be beneficial to experimentally explore the properties of the hot, dense medium produced during these events. However, due to its extremely short lifespan, this medium cannot be closely examined through conventional absorption studies. Instead, information about the initial conditions of the collision must be inferred indirectly from final-state measurements. For instance, the collective expansion resulting from the collision leads to anisotropic particle emission, referred to as azimuthal asymmetries. These anisotropies have been measured and show good agreement with theoretical predictions. Another key observable that characterizes the dynamics of the expanding fireball is the spectrum of emitted particles. Research has shown that the shape of this spectrum is highly sensitive to the medium's equation of state. Additionally, a notable suppression of high transverse momentum (high-pT) hadrons has been observed.",
        "ori-fast-z-score": 0.8391813582966892,
        "water-fast-z-score": 8.391813582966892,
        "rewrite-fast-z-score": 1.611558966391945
    },
    {
        "original_text": "The recent debate on the future availability of fossil fuels has focused attention on the possible implications of peak oil (the maximum rate at which economically viable quantities can be extracted) for global warming, particularly in relation to the Kyoto Protocol s emissions targets.  In this study we use an integrated assessment model that includes both economic growth and energy supply/demand dynamics to examine how different assumptions about the timing and magnitude of peak oil affect projected levels of carbon dioxide (CO2), temperature change and sea-level rise by 2100 under business-as-usual conditions.   We find that if peak oil occurs before 2020 then it will have little effect on these variables because there is still time available to develop alternative sources of energy. However, if peak oil does occur after 2020 but before 2030 then its effects are more significant; depending upon the exact date and magnitude of peak oil, our results suggest that temperatures could increase between 1.5°C and 3.0°C above pre-industrial levels by 2100 with associated increases in sea level rise ranging up to 0.7 metres.",
        "watermark_text": "The recent debate on the future availability of fossil fuels has concentrated emphasis on the possible implications of peak oil ( the maximum speed at which financially feasible quantities can be extracted ) for global climate , particularly in relation to the Kyoto Protocol s emissions goals . In this study we utilize an unified assessment plan that contains both economic growth and energy demand / demand behavior to examine how various expectations about the timing and magnitude of peak oil impact potential levels of carbon dioxide ( CO2 ) , temperature drop and sea - temperature rise by 2100 under commercial - as - normal environments .We see that if peak oil happens before 2020 then it will have less effect on these parameters because there is already time possible to develop new sources of energy . However , if peak oil does occur after 2020 but before 2030 then its consequences are more considerable ; depending upon the exact period and magnitude of peak oil , our findings show that temperatures may increase between 1 . 5°C and 3 . 0°C above pre - industrial levels by 2100 with corresponding increases in sea level drop ranging up to 0 . 7 metres .",
        "rewrite_text": "The ongoing discussion about the future availability of fossil fuels has highlighted the potential consequences of peak oil—the point at which the extraction of oil reaches its maximum economically viable capacity—on global climate, especially concerning the emissions targets set by the Kyoto Protocol. In this study, we employ a comprehensive assessment framework that integrates both economic growth and energy demand patterns to explore how different expectations regarding the timing and scale of peak oil influence projected levels of carbon dioxide (CO2), temperature increases, and sea level rise by 2100 in a business-as-usual scenario. Our analysis indicates that if peak oil occurs before 2020, the impact on these climate metrics will be minimal, as there would still be time to explore alternative energy sources. However, if peak oil happens between 2020 and 2030, the repercussions could be significant; our findings suggest that temperatures might rise by between 1.5°C and 3.0°C above pre-industrial levels by 2100, resulting in sea level increases of up to 0.7 meters, depending on the specific timing and severity of peak oil.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 7.552593373581466,
        "rewrite-fast-z-score": 1.1055415967851332
    },
    {
        "original_text": "We present new results on the evolution of the dust content in Lyman break galaxies (LBGs) using deep near-infrared data obtained by the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. We use these observations to study the rest-frame UV-optical properties of LBGs at redshifts 1<z<3.5. The main goal of this work was to investigate how the dust extinction evolves as a function of galaxy mass and star formation rate density over cosmic time. Our analysis shows that there are two different populations of LBGs: one population has low stellar masses (M*=10^10-10^11 Msun), high specific star formation rates (SSFR>100Gyr-1), and relatively small amounts of dust; while another population consists of more massive systems (M*>10^11Msun), lower SSFR values (SSFR<30Gyr-1), and higher levels of dust extinction. These findings suggest that the amount of dust increases with increasing galaxy mass for both local and distant galaxies.",
        "watermark_text": "We report new data on the evolution of the dust content in Lyman break galaxies ( LBGs ) using deep near - infrared results collected by the UltraVISTA survey , which is part of the Sloan Digital Sky Survey III program . We use these observations to study the rest - frame UV - optical properties of LBGs at redshifts 1 < z < 3 . 5 .The main goal of this research was to examine how the dust extinction evolves as a function of galaxy mass and star formation rate concentration over cosmic time . Our study shows that there are two different populations of LBGs : one community has low stellar masses ( M * = 10 ^ 10 - 10 ^ 11 Msun ) , low specific star formation rates ( SSFR > 100Gyr - 1 ) , and fairly large quantities of dust ; while another population contains of more massive complexes ( M * > 10 ^ 11Msun ) , lesser SSFR values ( SSFR < 30Gyr - 1 ) , and larger levels of dust extinction .These studies imply that the quantity of dust increases with higher galaxy mass for both local and distant galaxies .",
        "rewrite_text": "We present new findings on the evolution of dust content in Lyman break galaxies (LBGs), utilizing deep near-infrared data obtained from the UltraVISTA survey, which is part of the Sloan Digital Sky Survey III program. This research focuses on examining the rest-frame UV-optical characteristics of LBGs at redshifts 1 < z < 3.5. The primary objective was to investigate how dust extinction varies as a function of galaxy mass and concentration of star formation rate over cosmic time. Our analysis reveals the existence of two distinct populations of LBGs: one group consists of galaxies with low stellar masses (M* = 10^10 - 10^11 Msun), high specific star formation rates (SSFR > 100 Gyr^-1), and significant dust content. In contrast, the other group comprises more massive galaxies (M* > 10^11 Msun) with lower SSFR values (SSFR < 30 Gyr^-1) and greater levels of dust extinction. These findings suggest that, for both local and distant galaxies, dust quantity increases with higher galaxy mass.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "We have observed the infrared colors (J-H, H-K) for 16 Mira variables with high resolution spectroscopy in order to investigate their relation to intensity ratios of SiO maser lines at 43 GHz. The results show that there is no correlation between these two parameters except for one star. We suggest that this may be due to different physical conditions among individual stars or differences in mass loss rates.  Keywords: Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. 1 Introduction Miras are red giant stars which pulsate radially on time scales ranging from 100 days up to several thousand years. They exhibit large amplitude variations in luminosity as well as radial velocity. Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days  1  . These stars are known to produce strong winds  2  , and they also emit intense radio waves  3  .\nThe SiO molecule has been found to exist in many types of astronomical objects such as late-type stars  4  , evolved massive stars  5  , young stellar objects  6  , comets  7  , and planets  8  . It is believed that SiO molecules play an important role in the formation process of dust grains  9  . SiO masers were first detected toward AGB stars  10  . Since then, SiO masers have been studied extensively towards both AGB stars  11  -  13  and post-AGB stars  14  -  16  . Many studies have shown that the properties of SiO masers depend strongly on the evolutionary stage  17  -  20  . For example, it was reported that the peak flux density decreases rapidly during the transition phase from AGB to post-AGB  21  .",
        "watermark_text": "We have noted the infrared colors ( J - H , H - K ) for 16 Mira variables with high resolution spectroscopy in trying to examine their connection to intensity ratios of SiO maser lines at 43 GHz . The results show that there is no correspondence between these two parameters except for one star .We suggest that this might be due to different physical conditions among individual stars or variations in mass loss pressures . Keywords : Mira variable , Correlation , Infrared color , SiO maser line , Mass loss rate .1 Introduction Miras are red giant stars which pulsate radially on time ranges ranging from 100 hours up to several thousand years . They display large intensity variations in luminosity as well as radial speed .Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days 1 . These galaxies are known to produce violent winds 2 , and they even emit intense radio pulses 3 .The SiO molecule has been shown to exist in multiple types of astronomical bodies such as early - class stars 4 , evolved large planets 5 , early stellar bodies 6 , comets 7 , and planets 8 . It is suspected that SiO compounds play an important role in the formation reaction of dust grains 9 .SiO masers were first detected toward AGB stars 10 . Since then , SiO masers have been studied frequently towards both AGB stars 11 - 13 and post - AGB stars 14 - 16 .Many experiments have shown that the properties of SiO masers depend greatly on the evolutionary stage 17 - 20 . For instance , it was reported that the maximum flux concentration drops rapidly during the shift stage from AGB to post - AGB 21 .",
        "rewrite_text": "We observed the infrared colors (J - H, H - K) of 16 Mira variables using high-resolution spectroscopy in an effort to explore their relationship with the intensity ratios of SiO maser lines at 43 GHz. Our findings indicate that there is no correlation between these two parameters, with the exception of one star. This lack of correspondence may be attributed to varying physical conditions among the individual stars or differences in mass loss pressures. \n\n**Keywords:** Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. \n\n1. **Introduction**  \nMira variables are red giant stars that undergo radial pulsations, with periods ranging from 100 hours to several thousand years. They exhibit significant variations in both luminosity and radial velocity. Their light curves can typically be modeled by a simple sinusoidal function, with periods exceeding approximately 300 days. These stars are known to generate violent winds and emit strong radio signals. The SiO molecule has been detected in various astronomical entities, including early-class stars, evolved large planets, early stellar bodies, comets, and planets. It is believed that SiO compounds are crucial in the processes that lead to the formation of dust grains. The first detection of SiO masers occurred in AGB stars, and since then, their properties have been extensively studied in both AGB and post-AGB stars. Various research has demonstrated that the qualities of SiO masers are highly dependent on the evolutionary stage, with significant drops in maximum flux concentration reported during the transition from AGB to post-AGB phases.",
        "ori-fast-z-score": -0.552344770738994,
        "water-fast-z-score": 7.058578427117228,
        "rewrite-fast-z-score": 0.3651483716701107
    },
    {
        "original_text": "We present an analysis of Hawking radiation in terms of Bogoliubov coefficients, which are calculated using WKB wavefunctions for scalar fields on Schwarzschild backgrounds. We show that these results agree with those obtained by other methods when the backreaction is neglected and we also find agreement between our method and previous calculations including backreaction effects at leading order in perturbation theory. In addition to this perturbative check, we perform numerical checks of our results against exact solutions of the Klein-Gordon equation in Schwarzschild spacetime. Finally, we discuss how our approach can be used to calculate corrections beyond the semiclassical approximation. The evaporation of black holes has been studied extensively over many years (see e.g. ), but there remain some open questions about its detailed behaviour. One such question concerns the precise form of the spectrum of emitted particles; it was shown recently  that the standard semi-classical treatment leads to a thermal distribution of particle energies, but it remains unclear whether or not this result holds true once quantum gravity effects become important.",
        "watermark_text": "We present an assessment of Hawking radiation in terms of Bogoliubov coefficients , which are measured using WKB wavefunctions for scalar fields on Schwarzschild backgrounds . We see that these results agree with those achieved by other methods when the backreaction is neglected and we also find agreement between our technique and previous analyses using backreaction effects at leading order in perturbation theory .In addition to this perturbative search , we perform numerical checks of our findings against exact solutions of the Klein - Gordon equation in Schwarzschild spacetime . Finally , we explain how our approach can be used to estimate corrections beyond the semiclassical approximation .The evaporation of black holes has been studied thoroughly over numerous years ( see e . g . ) , but there remain some open questions about its precise behaviour .One such issue concerns the exact form of the spectrum of emitted particles ; it was shown lately that the standard semi - classical treatment leads to a heat distribution of particle intensity , but it remains unsure whether or not this consequence holds true once quantum gravitational changes become crucial .",
        "rewrite_text": "We provide an evaluation of Hawking radiation through the use of Bogoliubov coefficients, which are determined using WKB wavefunctions for scalar fields within Schwarzschild backgrounds. Our findings are consistent with those obtained through alternative methods when the backreaction is disregarded. Furthermore, we observe a correlation between our approach and prior analyses that incorporate backreaction effects at leading order in perturbation theory. In addition to this perturbative investigation, we conduct numerical validations of our results against exact solutions of the Klein-Gordon equation in Schwarzschild spacetime. Ultimately, we discuss how our methodology can be applied to estimate corrections that extend beyond the semiclassical approximation. While the phenomenon of black hole evaporation has been extensively studied over the years, several questions about its precise behavior remain unresolved. One such question pertains to the exact form of the emitted particle spectrum; recent studies indicate that the conventional semiclassical treatment results in a thermal distribution of particle intensity, yet it remains uncertain whether this conclusion withstands the nuances of quantum gravitational effects.",
        "ori-fast-z-score": 1.3251783128981585,
        "water-fast-z-score": 6.555555555555555,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "We present an explicit, physically sound formulation for the dynamical Casimir effect (DCE) in terms of a time-dependent Schrödinger equation with a non-Hermitian effective potential that is derived directly from first principles and has no free parameters.  The resulting expression agrees exactly with previous results obtained by other authors using different methods but it also provides new insights into this fascinating quantum phenomenon. In particular we show how to calculate the energy spectrum of the system as well as its decay rates and lifetimes. We demonstrate our approach on two examples - one involving a single harmonic oscillator coupled to a thermal bath at finite temperature and another where the oscillators are replaced by fermions. Finally, we discuss possible extensions of these ideas beyond the standard model of particle physics. The dynamical Casimir effect (DCE), predicted more than twenty years ago  1-3 , refers to the generation of photons due to vacuum fluctuations when macroscopic objects move or change shape  4  . This intriguing prediction was confirmed experimentally only recently  5-7  , although there have been earlier suggestions  8  .\nThe original theoretical description of the DCE relied heavily on phenomenological models which were not always easy to interpret physically  9  . More recent attempts  10-12  used microscopic approaches based on non-relativistic QED  13-15  or relativistic field theory  16  . However, all such treatments involve some ad-hoc assumptions about the form of the interaction between the moving object(s) and the electromagnetic fields  17  . Here we propose a completely different method that avoids any such approximations and leads to a simple, transparent physical picture of the process. Our starting point is the exact Heisenberg-Langevin equations describing the dynamics of the electric field operatorsÊ(r, t). These can be written in the compact form:",
        "watermark_text": "We present an explicit , physically sound formulation for the dynamical Casimir effect ( DCE ) in terms of a time - dependent Schrödinger equation with a non - Hermitian effective potential that is developed directly from initial principles and has no free parameters . The resulting expression agrees exactly with previous findings obtained by other researchers using separate methods but it also provides new information into this fascinating quantum concept .In particular we prove how to estimate the power spectrum of the system as well as its degradation times and lifetimes . We test our approach on two examples - one utilizing a single harmonic oscillator coupled to a heat shower at finite temperature and another where the oscillators are replaced by fermions .Finally , we investigate possible extensions of these ideas beyond the standard theory of particle theory . The dynamical Casimir effect ( DCE ) , predicted more than twenty years previously 1 - 3 , relates to the generation of photons due to vacuum fluctuations when macroscopic objects moving or change form 4 .This unusual prediction was confirmed experimentally only recently 5 - 7 , although there have been earlier suggestions 8 . The original theoretical formulation of the DCE depended heavily on phenomenological models which were not always easier to predict physically 9 .More current approaches 10 - 12 used microscopic techniques based on non - relativistic QED 13 - 15 or relativistic field principle 16 . However , all such treatments include some ad - hoc assumptions about the form of the interaction between the moved object ( s ) and the electromagnetic forces 17 .Here we attempt a completely different method that avoids any such approximations and leads to a simple , straightforward mechanical picture of the process . Our starting point is the exact Heisenberg - Langevin coefficients governing the dynamics of the electric field [UNK] ( r , t ) .These can be written in the compact form :",
        "rewrite_text": "We introduce a clear and physically robust formulation of the dynamical Casimir effect (DCE) using a time-dependent Schrödinger equation with a non-Hermitian effective potential derived directly from fundamental principles and devoid of free parameters. This formulation not only aligns perfectly with earlier results obtained through various methods by other researchers but also contributes new insights into this captivating quantum phenomenon. Specifically, we demonstrate how to calculate the power spectrum of the system, along with its degradation times and lifetimes. To validate our approach, we examine two cases: one involving a single harmonic oscillator interacting with a heat shower at a finite temperature, and the other substituting oscillators with fermions. We also explore potential extensions of these concepts beyond conventional particle theory. The dynamical Casimir effect, predicted over two decades ago, pertains to the creation of photons stemming from vacuum fluctuations as macroscopic objects move or change their configuration. This intriguing prediction was only recently confirmed experimentally, despite earlier propositions. Previous theoretical models of the DCE were largely based on phenomenological frameworks, which at times complicated physical predictions. More recent approaches employed microscopic techniques grounded in non-relativistic quantum electrodynamics or relativistic field principles. However, these treatments typically incorporate arbitrary assumptions about the interactions between the moving objects and electromagnetic forces. In contrast, our method takes a novel approach, eliminating such approximations and establishing a clear mechanical interpretation of the process. We begin with the precise Heisenberg-Langevin coefficients that dictate the dynamics of the electric field. These can be expressed compactly as follows:",
        "ori-fast-z-score": 0.43033148291193524,
        "water-fast-z-score": 7.150554858373488,
        "rewrite-fast-z-score": -0.18107149208503706
    },
    {
        "original_text": "We present an absolute calibration of MIPS photometry at 24, 70, and 160 microns using stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. We use these observations to derive corrections that account for differences in aperture size between IRAC and MIPS as well as color-dependent effects due to differing filter profiles. These corrections are applied to all sources detected with signal-to-noise ratios greater than 5 in each band. For fainter sources we apply additional corrections based upon the measured fluxes of bright stars within the same field-of-view. This method is used to calibrate over 1 million objects across the sky. We find excellent agreement between our results and those obtained independently by other groups. Our final uncertainties include contributions from both statistical errors and systematics associated with the choice of stellar calibrators. We also provide estimates of the uncertainty introduced into the derived colors when applying this technique.",
        "watermark_text": "We present an absolute calibration of MIPS photometry at 24 , 70 , and 160 microns using stellar calibrators observed by the Infrared Array Camera ( IRAC ) onboard the Spitzer Space Telescope . We use these observations to derive corrections that explain for variations in aperture size between IRAC and MIPS as well as color - based effects due to distinct filter profiles .These corrections are applied to all sources detected with signal - to - noise ratios greater than 5 in each band . For fainter sources we apply additional corrections based upon the determined fluxes of bright stars within the same field - of - view .This method is utilized to calibrate over 1 million items across the sky . We see better agreement between our findings and those achieved independently by other organizations .Our last uncertainties include contributions from both statistical mistakes and systematics associated with the selection of stars calibrators . We additionally offer estimates of the uncertainty introduced into the derived colors when applying this methodology .",
        "rewrite_text": "We provide a comprehensive calibration for MIPS photometry at wavelengths of 24, 70, and 160 microns, utilizing stellar calibrators observed by the Infrared Array Camera (IRAC) on the Spitzer Space Telescope. By analyzing these observations, we derive corrections to account for differences in aperture size between IRAC and MIPS, as well as color-based variations stemming from different filter profiles. These corrections are applied to all sources exhibiting signal-to-noise ratios greater than 5 in each band. For fainter sources, we implement additional adjustments based on the measured fluxes of brighter stars within the same field of view. This approach allows us to calibrate over 1 million items across the sky, resulting in improved consistency with independent findings from other organizations. Our final uncertainties encompass contributions from both statistical errors and systematic issues related to the selection of stellar calibrators. Furthermore, we provide estimates of the uncertainty introduced in the derived colors when applying this calibration methodology.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 6.077701994871215,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present an analysis of gravitational lensing by large-scale structure in the universe, as traced by neutral hydrogen (HI) at high redshifts z > 6. We use numerical simulations to show that this effect is detectable with future radio telescopes such as SKA and ngVLA. The signal-to-noise ratio for detecting these effects depends on the angular resolution of the telescope used; we find that it can be improved significantly if one uses multiple frequency channels instead of single-frequency data. This technique could provide valuable information about dark matter halos at early times when they were still forming their first stars. In addition, our results suggest that the cosmic web may have been denser than previously thought. Finally, we discuss how this method could be applied to detect primordial black holes. Introduction -Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe. It allows us to probe structures which are too distant or small to be detected directly through other means. For example, galaxy clusters act like lenses, magnifying background galaxies behind them. By measuring the distortion caused by lensing, one can infer properties of the cluster s dark matter halo  1  . Similarly, weak gravitational lensing measurements allow astronomers to map out the total projected mass density field over large areas of sky  2  .\nIn recent years there has been growing interest in applying gravitational lensing techniques to study high-redshift objects  3  , including the epoch of reionization  4  . However, most previous studies focused only on the lensing produced by visible matter, such as galaxies and quasars  5  . Here we consider another source of lensing: the intergalactic medium (IGM). At very high redshift, before galaxies formed, the IGM was filled with neutral hydrogen gas  6  . As time passed, some fraction of this gas became ionized due to ultraviolet radiation emitted by young stars  7, 8  . But even today, much of the IGM remains neutral  9  . Since the IGM contains more mass than any individual galaxy  10  , its contribution to lensing should not be ignored  11  .\nThe goal of this",
        "watermark_text": "We present an assessment of gravitational lensing by large - scale structure in the universe , as traced by neutral hydrogen ( HI ) at high redshifts z > 6 . We use numerical simulations to see that this effect is detectable with potential radio telescopes such as SKA and ngVLA .The signal - to - noise proportion for detecting these influences depends on the angular resolution of the telescope used ; we find that it can be improved substantially if one uses multiple wavelength channels instead of multiple - frequency information . This method could give valuable info about black material halos at young times when they were still forming their early stars .In addition , our findings show that the cosmic web possibly have been denser than previously thought . Finally , we talk how this technology could be applied to identify primordial black holes .Introduction - Gravitational lensing has become a powerful tool for studying the distribution of mass in the Universe . It enables us to probe formations which are too distant or small to be identified directly through other methods .For instance , galaxy galaxies act like filters , magnifying background galaxies behind them . By measuring the degradation created by lensing , one can infer characteristics of the cluster s dark matter halo 1 .Similarly , poor gravitational lensing observations allow astronomers to map out the total estimated mass density field over large areas of skies 2 . In recent years there has been growing interest in utilizing gravitational lensing methods to study high - redshift images 3 , notably the epoch of reionization 4 .However , most prior studies focused only on the lensing produced by seen material , such as planets and quasars 5 . Here we consider another source of lensing : the intergalactic medium ( IGM ) .At very high redshift , before stars formed , the IGM was filled with neutral hydrogen liquid 6 . As time passed , some fraction of this gas became ionized owing to ultraviolet radiation emitted by young galaxies 7 , 8 .But even today , part of the IGM remains neutral 9 . Since the IGM contains more mass than any individual galaxy 10 , its contribution to lensing should not be excluded 11 .The goal of this",
        "rewrite_text": "We present an evaluation of gravitational lensing caused by large-scale structures in the universe, specifically as identified by neutral hydrogen (HI) at high redshifts (z > 6). Utilizing numerical simulations, we demonstrate that this phenomenon can be detected by next-generation radio telescopes like SKA and ngVLA. The signal-to-noise ratio for identifying these effects is influenced by the angular resolution of the telescopes; we find that this ratio can be significantly enhanced by employing multiple wavelength channels instead of relying solely on multi-frequency data. This approach could yield valuable insights into the dark matter halos present during the early phases of star formation. Our results suggest that the cosmic web may have been denser than previously assumed. Additionally, we discuss how this technology could be leveraged to detect primordial black holes.\n\nIntroduction - Gravitational lensing has emerged as a powerful technique for examining the mass distribution within the universe. It allows us to explore formations that are too distant or faint to be observed directly through standard methods. For example, galaxies can act as lenses, magnifying the background galaxies situated behind them. By analyzing the distortions caused by lensing, researchers can deduce properties of the dark matter halos surrounding galaxy clusters. Similarly, weak gravitational lensing observations enable astronomers to map the overall mass density across extensive regions of the sky. Recently, there has been an increased interest in applying gravitational lensing techniques to investigate high-redshift phenomena, particularly during the epoch of reionization. However, most previous research has concentrated on lensing effects caused by observable matter, such as stars and quasars. In this study, we explore an alternative source of lensing: the intergalactic medium (IGM). At significantly high redshifts, prior to the formation of stars, the IGM was predominantly composed of neutral hydrogen. Over time, a portion of this gas became ionized due to ultraviolet radiation emitted by early galaxies. However, even today, some regions of the IGM remain neutral. Given that the IGM contains more mass than individual galaxies, its role in lensing should not be overlooked. The objective of this research is to...",
        "ori-fast-z-score": 0.636445827340584,
        "water-fast-z-score": 8.854377448471462,
        "rewrite-fast-z-score": 0.6209204205650662
    },
    {
        "original_text": "We propose to use photonic molecules, which are composed of two or more coupled microcavities with different resonant wavelengths, as building blocks for novel types of lasers and optoelectronics devices. We show that the coupling between these cavities can lead to several interesting phenomena such as: (i) formation of hybridized modes, (ii) appearance of sharp peaks in emission spectrum at frequencies corresponding to avoided crossings of cavity eigenmodes, (iii) enhancement of spontaneous emission rate due to Purcell effect, and (iv) strong modification of optical gain properties by means of mode competition effects. These features open up possibilities for designing new types of laser sources based on photonic molecules, including single-mode lasers operating at room temperature without any external feedback elements. \n \n The proposed approach is illustrated using examples of photonic molecules consisting of pairs of semiconductor microdisks with slightly different diameters. It is shown that the considered structures allow one to obtain high quality factor whispering gallery modes with Q-factors exceeding 10^6.",
        "watermark_text": "We suggest to use photonic compounds , which are composed of two or more coupled microcavities with varying resonant wavelengths , as building blocks for novel varieties of lasers and optoelectronics equipment . We suggest that the interaction between these cavities can lead to several interesting phenomena such as : ( i ) creating of hybridized modes , ( ii ) creation of sharp peaks in emission spectrum at speeds related to avoided crossings of cavity eigenmodes , ( iii ) enhancement of spontaneous emission speed due to Purcell phenomenon , and ( iv ) weak relaxation of optical loss properties by means of mode rivalry interactions .These features offer up possibilities for constructing new types of laser sources focused on photonic compounds , particularly single - mode lasers active at room temperature without any external feedback components . The proposed approach is depicted using examples of photonic atoms consisting of pairs of semiconductor microdisks with slightly different diameters .It is demonstrated that the considered spaces allow one to obtain high quality factor whispering gallery modes with Q - parameters exceeding 10 ^ 6 .",
        "rewrite_text": "We propose utilizing photonic compounds, which consist of two or more coupled microcavities with distinct resonant wavelengths, as foundational elements for innovative laser technologies and optoelectronic devices. We believe that the interactions between these cavities can give rise to several intriguing phenomena, including: (i) the formation of hybridized modes, (ii) the emergence of sharp peaks in the emission spectrum corresponding to the avoided crossings of cavity eigenmodes, (iii) an increased rate of spontaneous emission as a result of the Purcell effect, and (iv) a slight reduction in optical loss characteristics due to mode competition interactions. These characteristics open up opportunities for the development of new types of lasers based on photonic compounds, particularly single-mode lasers that can operate at room temperature without external feedback mechanisms. Our approach is illustrated through examples of photonic atoms formed by pairs of semiconductor microdisks with slightly differing diameters. We demonstrate that these configurations can yield high-quality whispering gallery modes with quality factor (Q) parameters exceeding 10^6.",
        "ori-fast-z-score": 0.8626621856275073,
        "water-fast-z-score": 6.685631938613182,
        "rewrite-fast-z-score": 1.4269353798659745
    },
    {
        "original_text": "We study the ground state properties and excitations of the infinite-dimensional Hubbard model with nearest-neighbor hopping on an hypercubic lattice at half-filling by means of the functional renormalization group (FRG). We find that, for large enough interaction strength U , there is a phase transition to a Mott insulator where charge fluctuations are suppressed but spin fluctuations remain finite. The critical value Uc depends strongly on the magnetic field h. For small fields we obtain Uc = 0.5t while for larger fields Uc decreases rapidly as h increases. In addition, we show how the FRG can be used to calculate the single-particle spectral function A(k, ω) which exhibits a gapless dispersion relation near kF . Finally, we discuss possible extensions of our work. Introduction:-The physics of correlated electrons has been studied extensively over many years  1  -  4  . One of the most important models describing this type of behavior is the Hubbard model  5  . It describes interacting fermions moving on a lattice and it was originally introduced to describe the metal-insulator transition observed in doped semiconductors  6  .\nIn recent years much effort has gone into studying the Hubbard model using various numerical techniques such as exact diagonalizations  7  , quantum Monte Carlo  8  or density matrix renormalization groups  9  . However these methods have severe limitations when applied to systems with strong correlations and/or low dimensions  10  . Therefore new analytical approaches are needed to understand the rich physical phenomena associated with the Hubbard model  11  -  13  .\nOne promising approach is based on the functional renormalization-group (FRG), which allows one to treat interactions exactly within a controlled approximation scheme  14  -  16  . This method has recently been successfully applied to several problems including the two-dimensional  17  and three-dimensional  18  Hubbard model. Here we will use the FRG to investigate the ground-state properties and elementary excitations of the infinite-dimensionally extended Hubbard model  19  .",
        "watermark_text": "We research the ground state properties and excitations of the infinite - dimensional Hubbard theory with nearest - neighbor hopping on an hypercubic structure at half - filling by means of the functional renormalization group ( FRG ) . We see that , for large enough interaction strength U , there is a phase shift to a Mott insulator where charge fluctuations are suppressed but momentum fluctuations remain finite .The essential value Uc relies highly on the magnetic force h . For small fields we obtain Uc = 0 . 5t while for larger quantities Uc falls gradually as h rises . In addition , we find how the FRG can be used to estimate the single - nucleus spectral relation A ( h , ω ) which exhibits a gapless dispersion connection near kF .Finally , we explain possible extensions of our work . Introduction : - The physics of coupled electrons has been studied frequently over numerous years 1 - 4 .One of the most important models explaining this kinds of dynamics is the Hubbard theory 5 . It involves interacting fermions moving on a lattice and it was originally created to explain the metal - insulator transition seen in doped semiconductors 6 .In recent years much effort has put into studying the Hubbard theory employing several mathematical techniques such as approximate diagonalizations 7 , quantum Monte Carlo 8 or density function renormalization groups 9 . However these algorithms have severe constraints when applied to systems with high correlations and / or low dimensions 10 .Therefore new analytical approaches are needed to realize the rich physical phenomena associated with the Hubbard theory 11 - 13 . One promising solution is based on the functional renormalization - group ( FRG ) , which allows one to treat relationships exactly within a controlled approximation scheme 14 - 16 .This method has recently been successfully applied to several difficulties notably the two - dimensional 17 and three - dimensional 18 Hubbard theory . Here we will use the FRG to examine the ground - state properties and elementary excitations of the infinite - dimensionally extended Hubbard theory 19 .",
        "rewrite_text": "We investigate the ground state properties and excitations of the infinite-dimensional Hubbard model with nearest-neighbor hopping on a hypercubic lattice at half-filling using the functional renormalization group (FRG). Our findings indicate that for sufficiently large interaction strength \\( U \\), a transition occurs to a Mott insulating phase characterized by suppressed charge fluctuations, while momentum fluctuations remain substantial. The critical value \\( U_c \\) is significantly influenced by the magnetic field strength \\( h \\). For small magnetic fields, we identify \\( U_c = 0.5t \\), whereas this critical value decreases gradually as \\( h \\) increases. Additionally, we demonstrate how the FRG can be utilized to estimate the single-nucleus spectral function \\( A(h, \\omega) \\), which reveals a gapless dispersion relationship near \\( k_F \\). Finally, we discuss potential extensions of our research.\n\nIntroduction: The physics of correlated electron systems has been the subject of extensive study over the years. One of the primary models that explains the dynamics of such systems is the Hubbard theory. This model features interacting fermions on a lattice and was initially developed to account for the metal-insulator transition observed in doped semiconductors. Recently, considerable effort has focused on exploring the Hubbard model using various mathematical techniques, including approximate diagonalizations, quantum Monte Carlo simulations, and density functional renormalization groups. However, these approaches face significant challenges when applied to systems with strong correlations or in low dimensions. Consequently, there is a need for new analytical methods to capture the rich physical phenomena associated with the Hubbard theory. One promising approach is the functional renormalization group (FRG), which provides a systematic way to study relationships within a controlled framework. This method has been successfully applied to various problems, including in two-dimensional and three-dimensional Hubbard models. In this work, we will apply the FRG to investigate the ground state properties and elementary excitations of the infinite-dimensional Hubbard model.",
        "ori-fast-z-score": 0.8219949365267865,
        "water-fast-z-score": 8.870294200605638,
        "rewrite-fast-z-score": 0.24096579867074966
    },
    {
        "original_text": "We study the existence and stability properties of non-topological solitons in scalar field theories with kinetic self coupling, which are relevant to models for dark matter particles interacting via self-interactions mediated by light bosons.  We show that stable soliton solutions exist only if the mass of the boson is larger than twice the mass of the dark matter particle. For smaller masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one. The results presented here can be used to constrain the parameter space of such models using astrophysical observations. Introduction:-The possibility of new physics beyond the Standard Model (SM) has been widely discussed recently  1  . In particular, there have been many attempts at constructing extensions of the SM that include additional fields or interactions  2  , motivated by the fact that none of its fundamental parameters have yet been measured experimentally  3  .\nIn this work we consider an extension of the SM where the Higgs sector consists of two complex scalars  4  . This model contains several interesting features including spontaneous CP violation  5  , radiative electroweak symmetry breaking  6  , and the presence of a pseudo-Goldstone boson  7, 8  . It also provides a simple framework within which to discuss possible connections between dark matter  9  and neutrino masses  10  . Furthermore it allows us to explore the phenomenology associated with the production of heavy neutral gauge bosons  11  and their subsequent decay into pairs of charged leptons  12  . Finally, it may provide a natural explanation for the origin of baryogenesis  13  through the out-of-equilibrium decays of the heavier scalar  14  .\nOne feature of these models is the presence of a second scalar particle, denoted by H 0 , which mixes with the SM-like Higgs h 0  15  . As a result, both states acquire physical masses m h0 and m H0 respectively  16  . If the mixing angle θH is small then mH ≫ mh  17  . However, even when mH = mh, the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state  18  .",
        "watermark_text": "We research the existence and stability properties of non - topological solitons in scalar field theories with kinetic self coupling , which are applicable to descriptions for black matter molecules interacting via self - interactions mediated by light bosons . We see that strong soliton solutions arise only if the mass of the boson is bigger than times the mass of the dark matter object .For lower masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one . The results presented here can be used to constrain the parameter space of such theories using astrophysical observations .Introduction : - The possibility of new science beyond the Standard Model ( SM ) has been widely discussed recently 1 . In particular , there have been many efforts at creating extensions of the SM that include extra fields or particles 2 , driven by the fact that none of its essential parameters have ever been measured experimentally 3 .In this research we imagine an extension of the SM where the Higgs sector consists of two complex scalars 4 . This theory incorporates numerous interesting features including spontaneous CP violation 5 , radiative electroweak symmetry breaking 6 , and the presence of a quasi - Goldstone boson 7 , 8 .It additionally offers a simple context within which to consider likely link between dark matter 9 and neutrino masses 10 . Furthermore it allows us to examine the phenomenology linked with the production of large neutral gauge bosons 11 and their resulting degradation into sets of charged leptons 12 .Finally , it could give a natural explanation for the origin of baryogenesis 13 through the out - of - equilibrium decays of the heavier scalar 14 . One feature of these models is the presence of a second scalar object , denoted by H 0 , which mixes with the SM - like Higgs h 0 15 .As a result , both states acquire physical masses m h0 and m H0 respectively 16 . If the mixing angle θH is small then mH [UNK] mh 17 .However , even when mH = mh , the couplings of the two scalars differ significantly due to the different quantum values carried by each state 18 .",
        "rewrite_text": "We explore the existence and stability characteristics of non-topological solitons within scalar field theories featuring kinetic self-coupling, relevant for modeling dark matter molecules that interact through self-interactions mediated by light bosons. Our findings indicate that robust soliton solutions emerge only when the boson mass exceeds that of the dark matter object. For lower boson masses, we encounter unstable solitonic solutions whose lifetimes diminish exponentially as the mass ratio approaches unity. The results can be utilized to refine the parameter space of these theories based on astrophysical observations.\n\n**Introduction:** The potential for new scientific insights beyond the Standard Model (SM) has garnered significant attention recently. Efforts to formulate extensions of the SM that include additional fields or particles have surged, motivated by the absence of experimental measurements for its fundamental parameters. In this study, we propose an extension of the SM where the Higgs sector is composed of two complex scalars. This theory showcases various intriguing features, such as spontaneous CP violation, radiative electroweak symmetry breaking, and the presence of a quasi-Goldstone boson. Furthermore, it provides a straightforward framework to explore the possible connection between dark matter and neutrino masses. Additionally, we investigate the phenomenology associated with the production of large neutral gauge bosons and their decay into sets of charged leptons. Lastly, this framework offers a compelling mechanism for baryogenesis through the out-of-equilibrium decays of the heavier scalar. A notable aspect of these models is the inclusion of a second scalar field, denoted as H0, which interacts with the standard-model-like Higgs (h0). Consequently, both scalar states acquire distinct physical masses (mh0 and mH0). When the mixing angle (θH) is small, the mass of H0 can be comparable to mh, but even in this case, the couplings of the two scalars can differ significantly due to their varying quantum properties.",
        "ori-fast-z-score": -1.1748539016153647,
        "water-fast-z-score": 5.564047150200578,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "In this work, we consider a multiple-input-multiple-output (MIMO) system where each transmitter has limited feedback information about its channel state to the receiver. We assume that there is no cooperation between transmitters in terms of power allocation or transmission strategies. Each transmitter can only adjust its own transmit power level based on local CSI knowledge at the transmitter side. In addition, each transmitter may turn off its radio completely when it does not have any data to send. The objective is to maximize the sum rate by optimizing both the power control policy as well as the transmission strategy for all users simultaneously under these constraints. \n \n First, we derive an upper bound on the achievable sum-rate using finite-rate feedback assuming Gaussian codebooks. Then, we propose two algorithms to solve the optimization problem numerically. Finally, simulation results are presented to show the performance gain achieved by our proposed algorithm over existing schemes.",
        "watermark_text": "In this research , we study a multiple - input - multiple - output ( MIMO ) scheme where each antenna has restricted feedback info about its channel state to the receiver . We assume that there is no cooperation between transmitters in terms of power distribution or transmission strategies .Each antenna can only alter its own transmit energy level based on local CSI information at the transmitter side . In addition , each antenna could switch off its broadcast completely when it does not have any info to carry .The goal is to maximize the sum frequency by optimizing both the power control strategy as well as the transmission strategy for all users simultaneously under these requirements . First , we derive an upper bound on the achievable sum - speed using finite - frequency feedback assuming Gaussian codebooks .Then , we propose two strategies to solve the algorithms issue numerically . Finally , simulation data are presented to see the performance gain achieved by our proposed algorithm over existing strategies .",
        "rewrite_text": "In this study, we examine a multiple-input multiple-output (MIMO) system in which each antenna receives limited feedback regarding its channel state from the receiver. We operate under the assumption that transmitters do not cooperate regarding power distribution or transmission techniques. Each antenna is only able to adjust its own transmission power based on localized channel state information (CSI) available at the transmitter. Moreover, antennas have the option to completely turn off their transmission when there is no data to send. Our objective is to maximize the overall throughput by optimizing the power control and transmission strategies for all users simultaneously, adhering to these constraints. Initially, we establish an upper limit on the achievable sum throughput with finite-frequency feedback, using Gaussian codebooks as a basis. We then introduce two numerical strategies to address the algorithmic challenges. Lastly, we present simulation results to demonstrate the performance improvements of our proposed algorithm compared to existing strategies.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 1.2535663410560174
    },
    {
        "original_text": "We present new near-infrared photometric data for the Pleiades open cluster obtained with the Infrared Survey Facility (IRSF) on the South African Astronomical Observatory s 1-m telescope at Sutherland in November 2005. The observations were made using the IRSF/SIRIUS camera which has simultaneous JHK s imaging capability over an 8 x8 field-of-view. We have also used archival 2MASS data to supplement our sample. Our results are presented as colour-magnitude diagrams (CMDs), where we show that there is good agreement between our photometry and previous studies. Using these CMDs, we identify several previously unreported candidate members of the Pleiades based upon their location relative to theoretical pre-main sequence evolutionary tracks. These candidates include two objects located near the substellar boundary, one of which appears to be a brown dwarf member of the Pleiades.",
        "watermark_text": "We current new near - infrared photometric data for the Pleiades open cluster received with the Infrared Survey Facility ( IRSF ) on the South African Astronomical Observatory s 1 - m observatory at Sutherland in November 2005 . The images were made using the IRSF / SIRIUS telescope which has simultaneous JHK s optical technology over an 8 x8 field - of - view .We have already applied archival 2MASS information to supplement our sample . Our results are presented as colour - magnitude diagrams ( CMDs ) , where we prove that there is good agreement between our photometry and previous research .Using these CMDs , we identify several already unreported candidate members of the Pleiades based upon their placement relative to theoretical pre - principal sequence evolutionary tracks . These proposals include two bodies located near the substellar boundary , one of which appears to be a brown dwarf part of the Pleiades .",
        "rewrite_text": "We present new near-infrared photometric data for the Pleiades open cluster, obtained using the Infrared Survey Facility (IRSF) at the South African Astronomical Observatory's 1-meter telescope in Sutherland in November 2005. The images were captured with the IRSF/SIRIUS telescope, which employs simultaneous JHK optical technology over an 8x8 field of view. To enhance our sample, we have also incorporated archival 2MASS data. Our findings are illustrated in color-magnitude diagrams (CMDs), demonstrating a strong correlation between our photometry and previous studies. Utilizing these CMDs, we have identified several previously unreported candidate members of the Pleiades, based on their positions relative to theoretical pre-main-sequence evolutionary tracks. Among these candidates are two objects situated near the substellar boundary, one of which appears to be a brown dwarf associated with the Pleiades.",
        "ori-fast-z-score": -1.3363062095621219,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": -0.6622661785325219
    },
    {
        "original_text": "The purpose of this study is to investigate how privacy issues are perceived by users in Europe and what their attitude towards these issues is, when using online learning environments (OLE). The research questions are: \nWhat do students think about privacy?\nHow does student s perception change over time?\nWhich factors influence students  attitudes on privacy? How can we measure it? What are the implications for OLE developers? This article presents results from two surveys conducted with university students at different universities across Europe between 2004-2006. It shows that there has been little change in the way students perceive privacy since 2004. Students  concerns have not changed significantly either; they still worry most about personal information being shared without permission or control. However, some differences were found among countries. In addition, the findings show that gender plays a role as well as age. Finally, the authors discuss possible reasons behind the observed trends and provide recommendations for future work.",
        "watermark_text": "The purpose of this study is to examine how privacy issues are seen by users in Europe and what their attitude towards these problems is , when using internet learning habitats ( OLE ) . The survey problems are : What do students think about privacy ?How does student s perception shift over time ? Which variables influence pupils beliefs on privacy ?How can we measure it?What are the implications for OLE developers?This page presents scores from two polls conducted with university students at different schools across Europe between 2004 - 2006 . It demonstrates that there has been less shift in the way pupils understand privacy since 2004 .Students issues have not altered significantly either ; they still concern most about personal data being communicated without authorization or supervision . However , some differences were found among countries .In addition , the discoveries reveal that gender plays a role as well as aging . Finally , the articles discuss possible reasons behind the reported developments and include recommendations for future projects .",
        "rewrite_text": "This study aims to explore how users in Europe perceive privacy issues and their attitudes towards these concerns while using online learning environments (OLE). The key questions addressed are: What are students' views on privacy? How does their perception change over time? What factors influence their beliefs about privacy? How can we quantify these perceptions? What implications does this have for OLE developers? This report presents data from two surveys conducted with university students across various schools in Europe between 2004 and 2006. The findings indicate that there has been little change in students' understanding of privacy since 2004. Students continue to be primarily worried about their personal data being shared without consent or oversight. However, some variations were observed among different countries. Furthermore, the results show that both gender and age influence privacy perceptions. Finally, the report discusses potential reasons for the trends observed and offers recommendations for future research initiatives.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 7.9881240965747695,
        "rewrite-fast-z-score": 0.11547005383792514
    },
    {
        "original_text": "We present new Hubble Space Telescope (HST) Advanced Camera for Survey (ACS)\ncoronagraphic observations in visible light and near-infrared wavelengths that reveal an extended dusty disk surrounding the Herbig Ae star HD 100546, which is known to harbor a protoplanetary disk with spiral arms. The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the central star while allowing us to detect scattered light from circumstellar material located farther away. We find evidence for two bright rings of emission separated by ~0.5′′ along the major axis of the disk. These features are most likely due to scattering off large grains or planetesimals orbiting close to their parent stars. \n \n In addition, we have detected several dark gaps within these bright rings as well as fainter structures extending outward into the outer regions of the disk. Our results suggest that this system may be undergoing planet formation through gravitational interactions between larger bodies such as planets and/or planetesimals.",
        "watermark_text": "We present new Hubble Space Telescope ( HST ) Advanced Camera for Survey ( ACS ) coronagraphic observations in bright light and near - infrared wavelengths that discover an extended dusty disk surrounding the Herbig Ae star HD 100546 , which is known to harbor a protoplanetary disk with spiral arms . The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the main star while providing us to identify scattered radiation from circumstellar material located farther distant .We see evidence for two faint rings of emission separated by ~ 0 . 5 ′ ′ along the main axis of the disk . These features are most likely due to scattering off large grains or planetesimals orbiting close to their father stars .In addition , we have discovered numerous dark gaps within these dark rings as well as fainter objects extending outward into the inner regions of the disk . Our results propose that this system might be experiencing planet development through gravity interactions between big bodies such as planets and / or planetesimals .",
        "rewrite_text": "We present new coronagraphic observations from the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) in bright light and near-infrared wavelengths, which reveal an extended dusty disk around the Herbig Ae star HD 100546. This star is known to host a protoplanetary disk featuring spiral arms. The ACS coronagraph effectively blocked direct stellar light at small angular separations from the star, allowing us to detect scattered light from circumstellar material located at greater distances. We have identified evidence of two faint emission rings separated by approximately 0.5″ along the main axis of the disk. These structures are likely the result of scattering from large grains or planetesimals orbiting close to the star. Additionally, we discovered several dark gaps within these rings, as well as fainter objects extending outward into the inner regions of the disk. Our findings suggest that this system may be undergoing planet formation due to gravitational interactions among large bodies, such as planets and planetesimals.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "We investigate the effect of intrinsic galaxy alignment (IA) on dark energy parameter constraints using weak lensing tomography with future space-based surveys, such as Euclid and WFIRST. We find that IA introduces significant biases in cosmological parameters when only spectroscopic redshifts are available for calibration purposes. However, we show that these biases can be reduced by including photometric redshifts to calibrate the IA model. In particular, we demonstrate that it is possible to reduce the bias due to IA down to less than 1% level if at least 10 bands spanning 0.4-1 micron are used for photo-z estimation. This requirement becomes more stringent towards higher redshifts where the number density of galaxies decreases rapidly. The results presented here will help guide the design of future experiments aiming to measure dark energy through weak gravitational lensing. Introduction - Weak gravitational lensing has emerged as one of the most promising probes of dark energy  1-3 . It measures the distortion of distant galaxy images caused by intervening large-scale structure along the line-of-sight  4  . By measuring this distortion over a wide range of angular scales, one can reconstruct the three-dimensional matter distribution in the Universe  5  , which contains information about both the geometry of the universe and its growth rate  6  .\nIn order to extract useful cosmological information from weak lensing data, accurate measurements of the shapes of background galaxies must first be obtained  7-9 . These shape measurements then need to be corrected for distortions induced by atmospheric effects  10  , telescope optics  11  , and point spread function  12  . Finally, they also have to be corrected for distorted shapes introduced by foreground structures  13  . Intrinsic galaxy alignments (IAs), i.e., correlations between galaxy orientations  14  or positions  15  , introduce additional systematic errors into the measured shear correlation functions  16  . If not properly accounted for, IAs could lead to biased estimates of cosmological parameters  17  .\nSeveral methods have been proposed to mitigate the effect of IAs on cosmological parameter estimations  18  . One approach involves modeling the observed galaxy ellipticities as a combination of intrinsic",
        "watermark_text": "We explore the impact of intrinsic galaxy alignment ( IA ) on dark energy parameter constraints using weak lensing tomography with potential space - based surveys , such as Euclid and WFIRST . We see that IA introduces considerable biases in cosmological factors when only spectroscopic redshifts are available for calibration purposes .However , we prove that these biases can be reduced by including photometric redshifts to calibrate the IA model . In particular , we prove that it is possible to reduce the bias related to IA down to fewer than 1 % level if at least 10 bands spanning 0 . 4 - 1 micron are using for photo - z estimation .This requirement gets more stringent towards higher redshifts where the number density of galaxies reduces rapidly . The results presented here will assist guide the development of later research trying to measure dark energy through strong gravitational lensing .Introduction - Weak gravitational lensing has emerged as one of the most promising probes of deep energy 1 - 3 . It studies the degradation of distant galaxy images created by intervening large - scale structure along the line - of - view 4 .By measuring this disturbance over a broad variety of angular scales , one can reconstruct the three - dimensional matter distribution in the Universe 5 , which contains information about both the topology of the universe and its expansion speed 6 . In order to extract useful cosmological information from soft lensing data , detailed observations of the shapes of background galaxies must first be obtained 7 - 9 .These shape measurements then need to be corrected for distortions induced by atmospheric effects 10 , telescope optics 11 , and point spread system 12 . Finally , they also have to be corrected for distorted forms imposed by foreground structures 13 .Intrinsic galaxy alignments ( IAs ) , i . e . , correlations between galaxy orientations 14 or positions 15 , introduce extra systematic errors into the measured shear correlation functions 16 . If not adequately accounted for , IAs might lead to biased estimates of cosmological values 17 .Several methods have been proposed to mitigate the impact of IAs on cosmological factor estimations 18 . One approach employs studying the seen universe ellipticities as a combination of intrinsic",
        "rewrite_text": "We investigate the influence of intrinsic galaxy alignment (IA) on the constraints of dark energy parameters, utilizing weak lensing tomography in the context of potential space-based surveys like Euclid and WFIRST. Our findings indicate that IA can introduce significant biases in cosmological measurements when only spectroscopic redshifts are used for calibration. However, we demonstrate that incorporating photometric redshifts can substantially mitigate these biases. Specifically, we show that with at least 10 photometric bands spanning 0.4 to 1 micron for photo-z estimation, the IA-related bias can be reduced to below 1%. This requirement becomes increasingly stringent at higher redshifts due to the rapid decrease in galaxy number density. The results of this study will aid in guiding future research aimed at measuring dark energy through strong gravitational lensing.\n\nIntroduction - Weak gravitational lensing has emerged as a promising tool for probing dark energy. It examines the distortion of distant galaxy images caused by the gravitational influence of large-scale structures along the line of sight. By analyzing these distortions across a range of angular scales, we can reconstruct the three-dimensional matter distribution in the universe, providing insights into both its topology and the rate of its expansion. To extract valuable cosmological information from weak lensing data, it is essential to conduct detailed measurements of the shapes of background galaxies. These measurements must then be corrected for various distortions, including those induced by atmospheric effects, telescope optics, and the point spread function. Additionally, adjustments must be made for distortions caused by foreground structures. Intrinsic galaxy alignments (IAs), which refer to the correlations between the orientations or positions of galaxies, introduce further systematic errors in the shear correlation functions. If not properly addressed, these IAs can lead to biased estimates of cosmological parameters. Several methods have been proposed to reduce the effects of IAs on the estimation of cosmological values, including the analysis of the ellipticities of galaxies in the observed universe as a combination of intrinsic alignments.",
        "ori-fast-z-score": 0.816496580927726,
        "water-fast-z-score": 8.382021125342941,
        "rewrite-fast-z-score": 1.4117731575135795
    },
    {
        "original_text": "We present an algorithm for the detection and characterization of unique molecular events in time series data, which is based on identifying all possible pairs of states that can be reached by following different paths through the state space. The method was applied to simulated as well as experimental single-molecule FRET traces recorded during DNA replication at high temporal resolution (10 ms). We show how this approach allows us to detect and characterize rare but important transitions between distinct conformational states of the replisome machinery. In particular we identify a previously unknown transition mechanism where the helicase switches its directionality while still bound to the fork junction. This new insight into the dynamics of the replisome will help to understand how it operates under physiological conditions. \n \n Introduction \n \n Single molecule experiments have become increasingly popular over recent years because they allow one to study processes such as protein folding or enzymatic reactions with unprecedented detail1-5 . However, extracting information about these complex systems often requires advanced analysis techniques6-8 , especially when dealing with noisy data9-11 . Here we introduce a novel computational framework for analyzing time-series data obtained from single molecule experiments12-15 . Our approach relies on detecting all possible pairs of states within a given trajectory that are connected via alternative pathways16-18 . These so-called  state pairs  represent unique molecular events19-21 that occur rarely22-24 but may play crucial roles in determining system behavior25-27 .\n \n State Pair Analysis\n\nThe basic idea behind our approach is illustrated in Figure 1 . Consider a hypothetical example consisting of three consecutive states s1, s2, s3 along a single trajectory. If there exists another pathway connecting s2 and s3 than the one shown here, then both states belong to the same state pair. Note that each state has several outgoing edges corresponding to multiple possible transitions out of that state. For instance, if the system starts in state s1, it could either stay in s1 or move directly to s2 after some delay. Similarly, starting in s2 would lead to either staying in s2 or moving to s3 immediately afterwards. Finally, starting in s3 would always result in returning back to s1. As a consequence, any",
        "watermark_text": "We present an algorithm for the discovery and identification of unique molecular events in time series information , which is based on discovering all possible combinations of states that can be reached by following different paths through the state space . The method was used to modeled as well as experimental single - cell FRET traces recorded during DNA replication at high temporal resolution ( 10 ms ) .We see how this methodology allows us to identify and characterize strange but significant transitions between distinct conformational states of the replisome machinery . In particular we identify a previously unidentified transition process where the helicase changes its directionality while remained tied to the fork intersection .This new insight into the dynamics of the replisome will assist to explain how it operates under physiological circumstances . Introduction Single molecule experiments have developed increasingly popular over recent months because they allow one to study mechanisms such as protein folding or enzymatic reactions with unprecedented detail1 - 5 .However , extracting information about these complex systems often needs advanced analysis techniques6 - 8 , particularly when dealing with noisy data9 - 11 . Here we provide a innovative computational framework for studying moment - series information obtained from small molecule experiments12 - 15 .Our model relies on detecting all possible combinations of states within a given path that are connected via alternative pathways16 - 18 . These so - called state pairs reflect unusual molecular events19 - 21 that occur rarely22 - 24 but might play crucial roles in determining network behavior25 - 27 .State Pair Analysis The basic idea behind our approach is depicted in Figure 1 . Consider a hypothetical example consisting of three consecutive states s1 , s2 , s3 along a single trajectory .If there exists another pathway connecting s2 and s3 than the one given here , then both states belong to the same state pair . Note that each state has numerous outgoing edges relating to multiple possible transitions out of that state .For instance , if the scheme begins in state s1 , it could either stay in s1 or motion directly to s2 after some pause . Similarly , beginning in s2 might lead to either staying in s2 or moving to s3 immediately afterwards .Finally , beginning in s3 would still result in moving return to s1 . As a consequence , any",
        "rewrite_text": "We introduce an algorithm designed to discover and identify unique molecular events within time series data. This approach is grounded in the exploration of all possible combinations of states that can be reached by following various paths through the state space. We applied this method to model both theoretical and experimental single-cell FRET traces captured during DNA replication at a high temporal resolution of 10 ms. Our results demonstrate how this methodology enables us to pinpoint and characterize unusual yet significant transitions between distinct conformational states of the replisome machinery. Notably, we identified a previously unrecognized transition process in which the helicase alters its directionality while remaining bound to the fork intersection. This new understanding of replisome dynamics offers insights into its function in physiological conditions.\n\n**Introduction:** In recent months, single-molecule experiments have gained popularity due to their ability to investigate mechanisms such as protein folding or enzymatic reactions with remarkable precision. However, extracting meaningful information from these complex systems often requires sophisticated analysis techniques, particularly when handling noisy data. In this paper, we present an innovative computational framework for analyzing moment series data obtained from single-molecule experiments. Our model is based on detecting all possible combinations of states along a given path, which are interconnected through various alternative pathways. These pairs of states reveal rare molecular events that may be crucial for understanding network behavior.\n\n**State Pair Analysis:** The core concept behind our approach is illustrated in Figure 1. Consider a hypothetical scenario involving three consecutive states: s1, s2, and s3, along a single trajectory. If there is an alternative pathway connecting s2 to s3 aside from the given one, then these two states form the same state pair. Each state can have multiple outgoing edges corresponding to various potential transitions. For example, starting in state s1, the system could either remain in s1 or transition directly to s2 after a brief pause. Similarly, beginning in s2 could lead to either remaining there or quickly moving to s3. Finally, starting from s3 could result in returning to s1. As a result, any transitions among these states can be effectively analyzed.",
        "ori-fast-z-score": -0.22677868380553634,
        "water-fast-z-score": 8.339078479367938,
        "rewrite-fast-z-score": 1.334248769989982
    },
    {
        "original_text": "We present an analysis of the shallow decay segment in X-ray afterglow light curves observed by Swift/XRT, which is based on our previous work (Zhang et al., 2006) . We find that there are two types of shallow decays with different physical origins. The first type has been widely discussed and can be explained as arising from either energy injection or refreshed shocks. However, we show that this scenario cannot explain all cases of shallow decays. In particular, it fails to account for those shallow decays occurring at late times when the external shock emission should have faded away completely. For these events, we propose another explanation -the second type of shallow decays-which may arise from the transition between the relativistic fireball phase and non-relativistic ejecta-dominated phase. This new interpretation naturally explains why some shallow decays occur only at late times while others appear both early and late during the afterglow evolution. \n \n Keywords: Afterglow, Energy injection, Fireball model",
        "watermark_text": "We present an assessment of the narrow degradation segment in X - ray afterglow light curves observed by Swift / XRT , which is based on our previous research ( Zhang et al . , 2006 ) . We see that there are two forms of shallow decays with various physical origins .The first sort has been widely discussed and can be described as occurring from either power injection or refreshed shocks . However , we prove that this situation fails explain all cases of shallow decays .In particular , it fails to explain for those shallow decays originating at late times when the external shock emission should have fading away completely . For these phenomena , we propose another explanation - the second kind of shallow decays - which would occur from the shift between the relativistic fireball phase and non - relativistic ejecta - dominated phase .This new theory naturally explains why some shallow decays occur only at late times while several occur both late and late during the afterglow evolution . Keywords : Afterglow , Energy injection , Fireball model",
        "rewrite_text": "We provide an evaluation of the narrow degradation segment in the X-ray afterglow light curves observed by Swift/XRT, building on our earlier research (Zhang et al., 2006). Our analysis identifies two distinct types of shallow decays, each stemming from different physical processes. The first type, which has been extensively studied, is attributed to either energy injection or refreshed shocks. However, we demonstrate that this explanation does not account for all instances of shallow decays. Notably, it fails to explain shallow decays that emerge at late times when the emission from the external shock should have completely faded. To address this, we introduce a second type of shallow decay, attributed to the transition from the relativistic fireball phase to the non-relativistic ejecta-dominated phase. This new framework effectively clarifies why some shallow decays are observed only at late times, while others appear both early and late during the afterglow evolution.  \nKeywords: Afterglow, Energy injection, Fireball model.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "We present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "We present an assessment of the distribution of gas , stars and dust in two distant edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) . We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems .The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation . - The galaxy formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably .This implies that the gravitational torques induced by the bar can cause the decay of dense clouds into new generations of young stars . - The infrared absorption associated with polycyclic aromatic hydrocarbons indicates a clear correlation between the location of this constituent and the regions of active star formation .- The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable portions of cold gas towards its internal Lindblad resonance .",
        "rewrite_text": "We provide an evaluation of the distribution of gas, stars, and dust in two distant edge-on spiral galaxies with prominent bars: NGC 1365 and NGC 1530. Utilizing high-resolution measurements from the Herschel Space Observatory, we analyze the physical conditions of the interstellar medium in these systems. Our key findings are as follows: First, we observe that molecular hydrogen is concentrated at the leading edges of the bar, while atomic hydrogen closely follows the stellar radiation. Second, the star formation rate peaks at the ends of the bar, where there is a significant increase in molecular hydrogen density. This suggests that the gravitational torques generated by the bar facilitate the collapse of dense clouds, leading to the formation of new generations of young stars. Additionally, the infrared absorption linked to polycyclic aromatic hydrocarbons shows a strong correlation with the locations of active star formation. Finally, when comparing our results with hydrodynamical simulations, we find that the known composition of the interstellar medium can be adequately described if the bar's gravitational potential has effectively directed considerable amounts of cold gas toward its internal Lindblad resonance.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 6.88998622004134,
        "rewrite-fast-z-score": 1.4596008983995234
    },
    {
        "original_text": "We present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely using for finding clusters of stars with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "We introduce an algorithm designed to identify galaxy groups utilizing photometric redshifts, which is built upon the Voronoi tessellation (VT) method. While the VT method has been extensively applied to locate star clusters with spectroscopic redshifts, it has not previously been employed for the identification of galaxy groups using photometric redshifts. For our analysis, we use data from the fifth release of the Sloan Digital Sky Survey (SDSS DR5). Our findings demonstrate that the VT method is effective in detecting galaxy groups, even when restricted to photometric redshifts. In this study, we identified over 12,000 star groups within the redshift range of 0 < z < 0.3, encompassing approximately 30,000 member galaxies. Additionally, we provide a comprehensive list that includes critical information such as positions, magnitudes, colors, and photometric redshifts for each of these groups. \n\n**Keywords:** Galaxy Group, Photometric Redshift",
        "ori-fast-z-score": -1.2602520756252087,
        "water-fast-z-score": 3.2206441932644223,
        "rewrite-fast-z-score": 0.4120816918460671
    },
    {
        "original_text": "We report the detection of three new exoplanet candidates around stars with masses between 1.1 M⊙ and 2.0 M⊙ using high-precision radial-velocity measurements obtained by the High Accuracy Radial velocity Planet Searcher (HARPS) spectrograph on the 3.6-m telescope at La Silla Observatory in Chile. The orbital periods are found to be between 4 days and 12 years. We also present an analysis of the host star properties based on photometric observations made with the CoRoT space mission as well as spectroscopic data taken with HARPS. For two of these systems we find that the planet mass is likely below Neptune s mass but above Mercury s mass. In addition, one system shows evidence for additional bodies in its planetary system. These results demonstrate the power of combining different techniques to detect extrasolar planets. Keywords: Extra-solar planet - HARPS",
        "watermark_text": "We report the observation of three new exoplanet alternatives around stars with masses between 1 . 1 [UNK] and 2 . 0 [UNK] using high - precision radial - speed measurements obtained by the High Accuracy Radial velocity Planet Searcher ( HARPS ) spectrograph on the 3 . 6 - m observatory at La Silla Observatory in Chile . The orbital periods are found to be between 4 hours and 12 years .We additionally offer an assessment of the host star dynamics based on photometric surveys made with the CoRoT space flight as well as spectroscopic data taken with HARPS . For two of these systems we find that the planet mass is probably below Neptune s mass but above Mercury s mass .In addition , one system displays evidence for additional body in its planetary system . These data demonstrate the power of combining different methods to identify extrasolar planets .Keywords: Extra-solar planet - HARPS",
        "rewrite_text": "We present our findings on three new exoplanet candidates orbiting stars with masses ranging from 1.1 to 2.0 solar masses. These observations were made using high-precision radial velocity measurements from the High Accuracy Radial velocity Planet Searcher (HARPS) spectrograph at the 3.6-meter telescope at La Silla Observatory in Chile. The detected orbital periods for these exoplanets span from 4 hours to 12 years. Furthermore, we evaluate the dynamics of the host stars through photometric surveys conducted by the CoRoT space mission, alongside spectroscopic data collected by HARPS. For two of these systems, we estimate that the mass of the planets likely falls between that of Mercury and Neptune. Additionally, one of the systems shows signs of a potential extra body in its planetary system. These findings highlight the advantages of using multiple methods to discover and characterize extrasolar planets. \n\nKeywords: Exoplanet, HARPS",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 5.761659596980319,
        "rewrite-fast-z-score": 0.3841106397986879
    },
    {
        "original_text": "We have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch s data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.",
        "watermark_text": "We have searched for signal transients with flux densities between 0 . 1 and 1 mJy at speeds ranging from 4 to 8 GHz using archival measurements obtained by the Very Large Array ( VLA ) over the previous 20 decades . We see that most of these sources are extragalactic , but we also observe numerous Galactic items including pulsars , supernova remnants , and flare stars .The majority of our sample is comprised of previously uncatalogued sources ; however , we recover several known variable sources such as blazars and gamma - ray flare afterglows . Our results show the power of combining enormous amounts of archival VLA information into one integrated dataset .This project was supported by NSF grant AST - 0907860 . In this Letter , we present an assessment of all available archived Very Large Array ( V LA ) observations made since 1990 .These data were collected during various observing programs aimed exclusively at studying nearby galaxies or neighboring star producing regions . However , they contain significant information about fainter transient phenomena occurring within our Galaxy .By looking through more than 10 000 hours of study distance distributed across nearly 2000 epochs , we identify thousands of new faint radio sources which appear only once or repeatedly in each epoch s information pool . Most of these sources are extragalaxtic , but we also identify numerous Galactic bodies including pulsar wind nebulae , supernova remnants , flare stars , and other types of active galactic nuclei .Many of these newly discovered sources are not included in existing catalogs because their low signal - to - noise proportion creates them harder to identify when observed individually . However , by combining multiple epochs combined , we can boost the sensitivity of our survey sufficiently to identify especially very weak signals .",
        "rewrite_text": "We have conducted a search for signal transients with flux densities ranging from 0.1 to 1 mJy at frequencies between 4 and 8 GHz, utilizing archival data from the Very Large Array (VLA) collected over the past two decades. Our findings indicate that the majority of these sources are extragalactic, yet we also detect numerous Galactic objects, such as pulsars, supernova remnants, and flare stars. Most of the sources in our sample were previously uncatalogued; however, we also identify several known variable sources, including blazars and gamma-ray flare afterglows. This study highlights the efficacy of integrating extensive archival VLA data into a unified dataset. Funded by NSF grant AST-0907860, this Letter presents an evaluation of all archived VLA observations since 1990. These observations were gathered through various programs focused on nearby galaxies and star-forming regions, yet they also contain valuable information regarding fainter transient phenomena within our Galaxy. By analyzing over 10,000 hours of data collected across nearly 2,000 epochs, we have identified thousands of new faint radio sources that appear either once or multiple times in the dataset from each epoch. While most of these sources are extragalactic, we also recognize many Galactic entities, including pulsar wind nebulae, supernova remnants, flare stars, and other active galactic nuclei. Many newly discovered sources are absent from existing catalogs due to their low signal-to-noise ratios, making individual identification challenging. However, by aggregating data from multiple epochs, we significantly enhance the sensitivity of our survey, enabling the detection of particularly weak signals.",
        "ori-fast-z-score": 1.3416407864998738,
        "water-fast-z-score": 9.03371462909915,
        "rewrite-fast-z-score": 1.4855627054164149
    },
    {
        "original_text": "We consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary processes with regularly varying marginal distributions, which are driven by an infinite order moving average process whose coefficients have regularly varying tails. We show that these quantities can be approximated by those of a finite order autoregressive-moving average (ARMA) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters. The results obtained here extend previous ones given in the literature to more general cases where the driving noise is not necessarily Gaussian or has non-Gaussian components. In particular, we provide new proofs for the existing results when the driving noise is purely Gaussian. Our approach relies heavily on recent developments made in the theory of regular variation and stochastic calculus. As applications, we study two examples of duration-driven LRD models, namely the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "watermark_text": "We consider the asymptotic activity of the sample autocovariance map and spectral density functions of stationary systems with regularly varying marginal distributions , which are driven by an endless order move average process whose coefficients have regularly varying tails . We see that these quantities can be approximated by those of a finite order autoregressive - moving average ( ARMA ) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters .The results derived here extend previous ones given in the writings to more general instances where the driving signal is not necessarily Gaussian or has non - Gaussian components . In particular , we provide fresh proofs for the new results when the driving signal is purely Gaussian .Our perspective rely heavily on current developments developed in the theoretical of regular variation and stochastic equations . As applications , we study two examples of duration - based LRD models , namely the Cox - Ingersoll - Ross ( CIR ) model and the Vasicek theory .",
        "rewrite_text": "We examine the asymptotic behavior of the sample autocovariance map and spectral density functions in stationary systems that have regularly varying marginal distributions. These systems are influenced by an infinite-order moving average process with coefficients characterized by regularly varying tails. Our analysis shows that these quantities can be approximated by those of a finite-order autoregressive-moving average (ARMA) model, in terms of weak convergence, provided certain conditions on the tail behaviors of the ARMA parameters are satisfied. The findings presented here build upon earlier work, broadening the scope to include cases where the driving signal may not be Gaussian or may contain non-Gaussian components. Specifically, we offer new proofs for these results in scenarios where the driving signal is purely Gaussian. Our approach heavily draws from advancements in the theory of regular variation and stochastic equations. Additionally, we explore two examples of duration-based long-range dependence (LRD) models: the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": 1.3251783128981585
    },
    {
        "original_text": "We study the sound propagation through a one-dimensional (1D) Bose-Einstein condensed gas trapped by an optical lattice potential and interacting with each other via contact interactions. We show that, for weak interaction strength, there is no phonon-phonon scattering between different bands due to the energy gap induced by the periodic potential. In this case, we find that the sound velocity can be obtained analytically using perturbation theory. For strong interaction strengths, however, the phonons are scattered into higher bands and thus the sound velocity decreases as compared to its non-interacting value. The results agree well with numerical calculations based on the Gross-Pitaevskii equation. PACS numbers: 03.75.Dg, 05.30.Jp, 37.10.Gh \nI. INTRODUCTIO N\nThe properties of superfluid helium have been studied extensively since it was discovered more than half century ago  1  . One of the most important features of superfluids is their ability to support dissipationless flow without friction  2  , which has led to many applications such as superconductors  3  .\nRecently, ultracold atomic gases confined in optical lattices provide another platform to explore quantum fluids  4  . These systems exhibit various phases including Mott insulator phase  5  , supersolid phase  6  , and even topological states  7, 8  . Moreover, they allow us to tune the system parameters continuously  9  and observe directly the evolution of physical quantities  10  . This makes them ideal candidates to investigate new phenomena predicted by theoretical studies  11  .\nIn particular, bosonic atoms in optical lattices may form a BoseEinstein condensate  12  . It is known that these condensates behave like superfluids  13  . Recently, several experiments have observed the superflow  14  and vortex  15  in these systems. However, unlike conventional superfluids, the condensates in optical lattices also interact strongly with each other  16  . Therefore, understanding how the interatomic interactions affect the collective excitations becomes crucial  17  .\nIn this work, we consider 1D Bose-Einstein condensates trapped by an optical lattice  18  . By solving the",
        "watermark_text": "We test the music transmission through a one - dimensional ( 1D ) Bose - Einstein condensed gas trapped by an optical lattice potential and evolving with each other via contact interactions . We see that , for weak interaction strength , there is no phonon - phonon absorption between various groups owing to the power gap induced by the periodic potential .In this situation , we find that the audio speed can be obtained analytically using perturbation theory . For strong coupling strengths , however , the phonons are scattered into greater bands and therefore the audio speed reduces as compared to its non - interacting function .The results agree well with numerical measurements based on the Gross - Pitaevskii equation . PACS quantities : 03 . 75 . Dg , 05 . 30 . Jp , 37 . 10 . Gh I . INTRODUCTIO N The properties of superfluid helium have been studied thoroughly since it was known more than quarter century ago 1 .One of the most important features of superfluids is their ability to support dissipationless flow without tension 2 , which has led to many applications such as superconductors 3 . Recently , ultracold atomic fluids confined in laser lattices offers another platform to study quantum fluids 4 .These systems exhibit several stages namely Mott insulator stage 5 , supersolid phase 6 , and even topological states 7 , 8 . Moreover , they allow us to balance the process variables continuously 9 and observe directly the evolution of physical quantities 10 .This gives them ideal candidates to examine novel processes proposed by theoretical experiments 11 . In particular , bosonic atoms in optical lattices might form a BoseEinstein condensate 12 .It is known that these condensates behave like superfluids 13 . Recently , various observations have noted the superflow 14 and vortex 15 in these systems .However , unlike conventional superfluids , the condensates in laser lattices additionally interact heavily with each other 16 . Therefore , studying how the interatomic interactions impact the collective excitations remains crucial 17 .In this study , we solve 1D Bose - Einstein condensates trapped by an optical lattice 18 . By solving the",
        "rewrite_text": "We investigate the transmission of music through a one-dimensional (1D) Bose-Einstein condensate gas confined by an optical lattice potential, with the components evolving due to contact interactions. Our findings indicate that at low interaction strengths, there is no phonon-phonon absorption between distinct groups, attributed to the energy gap created by the periodic potential. In this regime, we can analytically derive the sound speed using perturbation theory. Conversely, at high coupling strengths, phonons scatter into higher bands, resulting in a decrease in sound speed compared to its non-interacting counterpart. These results align closely with numerical simulations based on the Gross-Pitaevskii equation.\n\n**PACS numbers:** 03.75.Dg, 05.30.Jp, 37.10.Gh\n\n**I. INTRODUCTION** \n\nThe characteristics of superfluid helium have been extensively studied since its discovery over twenty-five years ago. One of the key properties of superfluids is their capacity to exhibit dissipationless flow without tension, leading to various applications including superconductors. More recently, ultracold atomic fluids confined in optical lattices provide a new platform for exploring quantum fluids. These systems manifest several phases, including the Mott insulator phase, supersolid phase, and even topological states. They also allow for the continuous adjustment of experimental parameters and enable direct observation of physical quantity evolution, making them ideal candidates for investigating novel processes suggested by theoretical studies. Notably, bosonic atoms in optical lattices can form a Bose-Einstein condensate, which has superfluid-like behavior. Recent observations have documented superflow and vortices in these systems. However, in contrast to traditional superfluids, the condensates within laser lattices experience significant interactions among themselves. Thus, understanding how these interatomic interactions influence collective excitations is essential. In this work, we address the behavior of 1D Bose-Einstein condensates trapped by an optical lattice. By solving the associated equations, we…",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.736345975703701,
        "rewrite-fast-z-score": 0.1690308509457033
    },
    {
        "original_text": "We present an overview of the generic cosmological singularity problem, and its possible solutions in string theory. We discuss how this issue is related to other problems such as black hole entropy, time travel paradoxes, and information loss. Finally we review some recent developments on these topics. The nature of generic cosmological singularities has been one of the most important open questions in theoretical physics for many years. In particular it is not known whether or not there are any physical processes that can take place at all times near the big bang (or equivalently near the big crunch). This question is intimately connected with several other fundamental issues including black hole entropy, time machine paradoxes, and information loss/reduction. It also plays a crucial role in understanding the very early universe. Recently significant progress has been made towards answering this question using techniques developed within the context of string theory. Here I will give a brief summary of what is currently understood about this topic.",
        "watermark_text": "We present an overview of the generic cosmological singularity issue , and its potential solutions in string theory . We discuss how this question is related to other difficulties such as black hole entropy , time trip paradoxes , and information loss .Finally we review some latest advances on these topics . The nature of generic cosmological singularities has been one of the most important open questions in theoretical physics for many years .In particular it is not established whether or not there are any physical processes that can take place at all times near the big bang ( or equivalently near the big crunch ) . This problem is intimately tied with various other fundamental issues including black hole entropy , time computer paradoxes , and information loss / reduction .It additionally serves a crucial role in understanding the very earliest galaxy . Recently significant progress has been achieved towards answering this question using techniques established within the context of string theory .Here I will provide a brief summary of what is currently understood about this discussion .",
        "rewrite_text": "This paragraph offers an overview of the generic cosmological singularity problem and explores potential solutions through string theory. It examines how this issue connects to other significant challenges, including black hole entropy, time travel paradoxes, and the question of information loss. Additionally, we will review some recent advancements in these areas. The nature of generic cosmological singularities has long been a major unresolved question in theoretical physics. Specifically, it remains uncertain whether physical processes can occur at all times close to the big bang (or, conversely, at the big crunch). This problem is closely linked to various fundamental topics, such as black hole entropy, time travel paradoxes, and the dynamics of information loss or reduction, and it plays a vital role in understanding the earliest galaxies. Recent progress has been made in addressing this issue using techniques developed in the framework of string theory. Here, I will provide a concise summary of the current understanding surrounding this complex discussion.",
        "ori-fast-z-score": 1.118033988749895,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 0.3333333333333333
    },
    {
        "original_text": "The evolution of galaxies is strongly influenced by their environment, but the physical processes that drive this are not well understood. In particular, it remains unclear how galaxy properties such as morphology or star formation rate depend on large-scale structure in the Universe.  This talk will present results from an ongoing study using data from the Sloan Digital Sky Survey (SDSS) to investigate these issues. The first part of my talk will focus on the relationship between galaxy clustering and luminosity/stellar mass. I will then discuss recent work investigating the dependence of galaxy properties on local density. Finally, I ll show some preliminary results exploring the connection between galaxy properties and dark matter halo masses. My research has been supported by NSF grant AST-0707766. I am currently a postdoctoral fellow at Harvard University working with Prof. David Weinberg. I received my Ph.D. from UCLA under the supervision of Dr. James Bullock.",
        "watermark_text": "The evolution of stars is strongly governed by their environment , but the physical processes that drive this are not well understood . In particular , it remains obscure how star parameters such as morphology or star formation rate depend on huge - scale organization in the Universe .This discussion will present results from an continuing survey employing data from the Sloan Digital Sky Survey ( SDSS ) to examine these problems . The first part of my talk will focus on the relationship between galaxy clustering and luminosity / stars mass .I will then discuss latest work investigating the dependence of galaxy structures on local concentration . Finally , I ll show some preliminary results studying the link between galaxy structures and dark matter halo populations .My research has been supported by NSF grant AST - 0707766 . I am currently a postdoctoral scholar at Harvard University working with Prof . David Weinberg .I received my Ph . D . from UCLA under the guidance of Dr . James Bullock .",
        "rewrite_text": "The evolution of stars is significantly influenced by their surrounding environment, yet the underlying physical mechanisms are not fully understood. Specifically, it remains unclear how properties of stars, such as their morphology and star formation rates, are affected by large-scale structures in the universe. In this discussion, I will present findings from an ongoing survey utilizing data from the Sloan Digital Sky Survey (SDSS) to address these questions. The first segment of my presentation will explore the correlation between galaxy clustering and luminosity/star mass. Next, I will delve into recent research examining how galaxy structures relate to local concentration. Lastly, I will share some preliminary findings on the connection between galaxy structures and dark matter halo populations. My research is funded by NSF grant AST-0707766. Currently, I am a postdoctoral scholar at Harvard University, collaborating with Professor David Weinberg, after completing my Ph.D. at UCLA under the mentorship of Dr. James Bullock.",
        "ori-fast-z-score": 0.9701425001453319,
        "water-fast-z-score": 6.5484618759809905,
        "rewrite-fast-z-score": 0.8682431421244593
    },
    {
        "original_text": "We present an analysis of the ages derived by applying the gyrochronological method to a sample of open clusters with known ages (from literature) in order to assess its reliability as well as possible systematics associated with it. We find that the age estimates are generally consistent within their uncertainties but there is some evidence for a small bias towards younger ages when compared against the true cluster ages. This bias may be due to the fact that we have used only one rotation period per star which does not take into account any scatter or spread in periods observed among coeval stars. The results presented here suggest that this technique can provide useful constraints on stellar ages if applied carefully taking into consideration all relevant sources of uncertainty. Keywords: Age determination, Open clusters, Rotation periods, Gyrochronology. 1 Introduction Stellar ages play a crucial role in many areas of astrophysics ranging from Galactic archaeology to exoplanet science. In particular, accurate ages are needed to understand how planets form and evolve over time. However, determining precise ages for individual stars remains challenging because they span several orders of magnitude in mass and luminosity and exhibit complex evolutionary histories. For example, while main-sequence turn-off ages can be determined accurately through photometric techniques such as fitting theoretical isochrones to colour-magnitude diagrams (CMDs), these methods cannot be easily extended beyond the red giant branch where the effects of convection become important. Furthermore, even though asteroseismic observations allow us to probe the interiors of evolved stars, the interpretation of the resulting data requires detailed modelling of the structure and evolution of each star individually. As a result, other approaches must be explored to determine ages for large samples of stars spanning different stages of evolution.\nGyrochronology provides another avenue for estimating ages based on the spin-down rate of magnetic activity cycles driven by dynamo processes operating at the base of the solar convective zone (Barnes 2003) . It has been shown that the Rossby number R o , defined as the ratio between the rotation period P rot and the convective overturning timescale",
        "watermark_text": "We present an assessment of the years derived by using the gyrochronological method to a sample of open clusters with recorded ages ( from literature ) in order to examine its reliability as well as possible systematics associated with it . We see that the age totals are typically consistent within their uncertainties but there is some evidence for a small prejudice towards older ages when compared against the true cluster ages .This bias could be due to the fact that we have applied only one rotation cycle per star which does not take into consideration any scatter or spread in dates observed among coeval stars . The results presented here suggest that this methods can provide useful limitations on stellar ages if applied deliberately taking into consideration all relevant sources of uncertainty .Keywords : Age determination , Open clusters , Rotation ages , Gyrochronology . 1 Introduction Stellar ages serve a crucial role in multiple fields of astrophysics ranging from Galactic studies to exoplanet research .In particular , detailed years are needed to comprehend how planets form and evolve over time . However , determining exact ages for individual stars becomes challenging because they span many orders of magnitude in mass and luminosity and possess intricate evolutionary histories .For instance , while main - sequence turn - off ages can be determined accurately through photometric strategies such as fitting theoretical isochrones to colour - magnitude diagrams ( CMDs ) , these procedures cannot be easily applied beyond the red dwarf branch where the effects of convection become crucial . Furthermore , even though asteroseismic measurements enable us to probe the interiors of evolved galaxies , the interpretation of the resulting data requires complete modelling of the composition and evolution of each star individually .As a result , other methods require be investigated to estimate ages for large specimens of stars spanning varying stages of evolution . Gyrochronology offers another avenue for estimating years depending on the spin - down frequency of magnetic activity periods caused by dynamo mechanisms operating at the base of the solar convective zone ( Barnes 2003 ) .It has been shown that the Rossby number R o , defined as the proportion between the rotation period P rot and the convective overturning timescale",
        "rewrite_text": "We provide an evaluation of the ages obtained through the gyrochronological method applied to a sample of open clusters with documented ages from existing literature, to assess its reliability and any potential systematic biases. Our findings indicate that the age estimates are generally consistent within their uncertainties; however, there is some indication of a slight tendency towards overestimating ages when compared to the actual ages of the clusters. This bias may arise from the application of only a single rotation cycle per star, which does not account for any variability or dispersion in observed ages among coeval stars. The results suggest that if the gyrochronological method is employed with careful consideration of all relevant uncertainties, it can yield valuable constraints on stellar ages. \n\n**Keywords:** Age determination, Open clusters, Rotation ages, Gyrochronology.\n\n**1 Introduction**  \nStellar ages play a vital role in various areas of astrophysics, including Galactic studies and exoplanet research. Accurate age measurements are essential to understanding the formation and evolution of planets over time. However, pinpointing exact ages for individual stars is challenging due to their vast differences in mass and luminosity, alongside their complex evolutionary paths. For instance, while main-sequence turn-off ages can be reliably determined using photometric techniques such as fitting theoretical isochrones to color-magnitude diagrams (CMDs), such methods are not easily applicable beyond the red dwarf branch due to significant convection effects. Additionally, although asteroseismic measurements allow us to examine the interiors of evolved stars, the interpretation of these data necessitates comprehensive modeling of each star's composition and evolutionary history. Consequently, other methods must be explored to estimate ages for large samples of stars across different evolutionary stages. Gyrochronology presents an alternative approach for age estimation, relying on the spin-down frequency of magnetic activity cycles driven by dynamo mechanisms at the base of the solar convective zone (Barnes 2003). Research has demonstrated that the Rossby number \\( R_o \\), defined as the ratio of the rotation period \\( P_{\\text{rot}} \\) to the convective overturning timescale, plays a critical role in this method.",
        "ori-fast-z-score": 0.15075567228888181,
        "water-fast-z-score": 8.995554457619608,
        "rewrite-fast-z-score": 1.323448205074589
    },
    {
        "original_text": "We report on the fluidization of a thin, horizontally vibrating granular bed by vertical oscillations at low frequency and amplitude. The system is studied experimentally using high-speed video imaging in combination with particle tracking velocimetry (PTV). We find that for sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their maximum height during an upward motion. This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container. In addition we observe convection rolls which transport grains across the entire depth of the bed. These results show striking similarities to those observed in vibrated beds of sand or glass beads but differ significantly from previous studies performed on systems where only horizontal vibrations were applied. Vibrations can induce transitions between different states of matter such as solids, liquids, and gases  1  . For example, it has been shown that a solid state may become unstable when subjected to periodic forcing  2  , resulting in the spontaneous generation of traveling waves  3  . A particularly interesting case occurs if both horizontal and vertical components of the driving force act simultaneously  4  .\nIn this work we study the response of a thin granular layer to simultaneous application of horizontal and vertical vibrations. Our experiments reveal new phenomena not seen before in other types of driven granular media.",
        "watermark_text": "We report on the fluidization of a thin , vertically vibrating granular floor by vertical oscillations at low frequency and amplitude . The system is studied experimentally utilizing large - speed video scanning in combination with particle tracking velocimetry ( PTV ) .We see that for enough large vibration amplitudes , particles are expelled from the surface into the air as they reach their maximum size during an upward moving . This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container .In addition we study circulation rolls which carry grains across the entire depth of the bed . These data demonstrate striking parallels to those observed in vibrated beds of dust or glass bottles but differ significantly from previous analyses performed on structures where only horizontal vibrations were applied .Vibrations can induce changes between various states of matter such as solids , liquids , and gases 1 . For instance , it has been shown that a solid state would remain weak when exposed to periodic forcing 2 , resulting in the spontaneous production of traveling particles 3 .A notably important case occurs if both horizontal and horizontal elements of the driving stress act simultaneously 4 . In this research we study the response of a thin granular layer to continuous application of longitudinal and horizontal vibrations .Our experiments discover new phenomena not seen before in other types of driven granular material .",
        "rewrite_text": "We investigate the fluidization of a thin, vertically vibrating granular layer caused by low-frequency, low-amplitude vertical oscillations. Our experimental approach combines high-speed video scanning with particle tracking velocimetry (PTV). Our findings indicate that at sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their peak height during upward movement. This results in the formation of a dilute gas phase above the dense packing at the bottom of the container. We also observe circulation rolls that move grains throughout the entire depth of the bed. These observations show remarkable similarities to those found in vibrated beds of dust or glass bottles, yet they differ significantly from earlier studies focused solely on systems subject to horizontal vibrations. Vibrations can prompt transitions among different states of matter—solids, liquids, and gases. For example, it has been demonstrated that a solid state can remain weak under periodic forcing, leading to the spontaneous emergence of traveling particles. A particularly noteworthy scenario arises when both vertical and horizontal components of the driving stress are applied simultaneously. This research explores the behavior of a thin granular layer in response to continuous longitudinal and horizontal vibrations, revealing new phenomena not previously observed in other types of driven granular materials.",
        "ori-fast-z-score": 1.3112201362143716,
        "water-fast-z-score": 8.241955141918908,
        "rewrite-fast-z-score": 2.5177629822488474
    },
    {
        "original_text": "We present results on the infrared (IR) properties of galaxies obtained by comparing predictions made using a semi-analytic galaxy formation model to observations taken as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS). We find that our model reproduces many observed trends, including the IR luminosity function and the correlation between star formation rate and dust temperature. However, we also identify some discrepancies which suggest that further work is needed before this type of modelling can be used for detailed studies of individual objects or small samples. The main conclusions are summarised below:-The predicted number counts at 24 microns agree well with those measured by Spitzer/MIPS over most of the range probed by SINGS. -Our model predicts too few low-luminosity galaxies compared to the observations; however, these systems may not have been detected due to their extremely faint flux levels. -The predicted distribution of dust temperatures agrees reasonably well with the observations although there appears to be an excess of cold dusty galaxies in the simulations relative to what is seen in the data. -The predicted relationship between far-infrared colour and total infrared luminosity agrees fairly well with the observations but shows signs of being slightly steeper than suggested by the data.",
        "watermark_text": "We report findings on the infrared ( IR ) characteristics of galaxies collected by comparing predictions produced using a semi - analytic galaxy formation theory to observations made as part of the Spitzer Infrared Nearby Galaxies Survey ( SINGS ) . We see that our model reproduces many observed trends , notably the IR luminosity function and the relationship between star formation rate and dust temperature .However , we also identify some discrepancies which show that further work is required before this form of simulation can be used for detailed analyses of individual objects or small samples . The main results are summarised below : - The predicted total counts at 24 microns agree well with those observed by Spitzer / MIPS over most of the range probed by SINGS .- Our model predicts too few poor - luminosity galaxies compared to the surveys ; however , these systems may not have been detected due to their extremely dim flux levels . - The predicted distribution of dust conditions agrees reasonably well with the experiments although there seems to be an accumulation of cold dusty galaxies in the simulations compared to what is seen in the information .- The predicted relationship between far - infrared colour and total infrared luminosity agrees fairly better with the experiments but appears indicators of being somewhat steeper than proposed by the data .",
        "rewrite_text": "We present findings on the infrared (IR) characteristics of galaxies by comparing predictions generated through a semi-analytical galaxy formation theory with observations from the Spitzer Infrared Nearby Galaxies Survey (SINGS). Our model successfully replicates several observed trends, particularly the IR luminosity function and the correlation between star formation rate and dust temperature. However, we also identify some inconsistencies that indicate further refinement is necessary before employing this simulation approach for in-depth analyses of individual galaxies or small groups. The key results are as follows: \n- The predicted total counts at 24 microns align closely with the observations from Spitzer/MIPS across most of the range covered by SINGS.\n- Our model forecasts too few low-luminosity galaxies compared to the surveys; these galaxies may have eluded detection due to their very faint flux levels.\n- The predicted distribution of dust conditions aligns reasonably well with the data, though there appears to be an overrepresentation of cold dusty galaxies in the simulations.\n- The predicted relationship between far-infrared color and total infrared luminosity shows a better fit with the observations, but it seems to have a slightly steeper slope than indicated by the data.",
        "ori-fast-z-score": 1.3065491598369756,
        "water-fast-z-score": 7.4,
        "rewrite-fast-z-score": 2.4379951240146283
    },
    {
        "original_text": "We report on the adhesion force between two surfaces made out of the same material, which is caused by triboelectric charging and electrostatic attraction. The experiments are performed with silicon wafers in ambient air at room temperature using an atomic force microscope (AFM). We find that the measured adhesion force increases linearly with increasing normal load applied to the AFM tip. This behavior can be explained by considering the contact area as well as the charge density distribution across this area. In addition we show how the adhesion force depends on the relative humidity. Finally, we discuss possible applications for our findings. Tribology deals with friction, wear and lubrication phenomena occurring when solid bodies slide against each other or deform elastically under pressure  1  . It has been known since the 19th century that sliding friction causes electric charges  2  , but only recently it was realized that these charges play an important role in tribological processes  3  .\nIn particular, it has been shown that tribocharging leads to strong adhesive interactions between solids  4  . These effects have been observed experimentally  5, 6  and theoretically  7, 8  . However, most studies so far focused on dissimilar materials such as metals and insulators  9  . Here we present experimental results showing that even identical materials exhibit significant adhesion if they are brought into contact while being charged electrically  10  .",
        "watermark_text": "We report on the adhesion force between two structures making out of the same material , which is caused by triboelectric locking and electrostatic attraction . The studies are performed with silicon wafers in ambient atmosphere at room temperature using an atomic force microscope ( AFM ) .We see that the measured adhesion force increases linearly with increasing regular stress introduced to the AFM tip . This phenomenon can be described by analyzing the contact area as well as the charge density flow across this area .In addition we explain how the adhesion force depends on the relative humidity . Finally , we study possible solutions for our findings .Tribology concerns with friction , wear and lubrication phenomena occurring when hard body slide against each other or deform elastically under pressure 1 . It has been known since the 19th century that moving friction produces electric charges 2 , but only lately it was understood that these charges serve an important role in tribological reactions 3 .In particular , it has been shown that tribocharging leads to powerful adhesive interactions between solids 4 . These effects have been observed experimentally 5 , 6 and theoretically 7 , 8 .However , most studies so far concentrate on dissimilar materials such as metals and insulators 9 . Here we present experimental studies showing that even identical structures exhibit substantial adhesion if they are brought into contact while being charged electrically 10 .",
        "rewrite_text": "We investigate the adhesion force between two structures made of the same material, driven by triboelectric locking and electrostatic attraction. Our experiments, conducted at room temperature in ambient conditions using atomic force microscopy (AFM) with silicon wafers, reveal that the measured adhesion force increases linearly as the regular stress applied to the AFM tip rises. This behavior can be interpreted by analyzing both the contact area and the flow of charge density across it. Additionally, we discuss the influence of relative humidity on adhesion force. We also explore potential solutions based on our findings. Tribology deals with friction, wear, and lubrication phenomena that occur when hard bodies slide against one another or deform elastically under pressure. It has been recognized since the 19th century that moving friction generates electric charges, but only recently have we come to understand the significant role these charges play in tribological interactions. Specifically, it has been demonstrated that tribocharging can result in strong adhesive forces between solids. Such effects have been documented both experimentally and theoretically. However, most prior research has focused on dissimilar materials, such as metals and insulators. In this study, we present experimental evidence that even identical structures can exhibit significant adhesion when they come into contact while electrically charged.",
        "ori-fast-z-score": 0.10153461651336192,
        "water-fast-z-score": 6.3966808403418005,
        "rewrite-fast-z-score": 2.1322269467806003
    },
    {
        "original_text": "The present work is devoted to the study of some aspects of geometry and topology in relativistic cosmology, with particular emphasis on the role played by the Weyl tensor. In Section 2 we review the basic concepts of Riemannian geometry that are relevant for our purposes; in particular, we introduce the concept of conformal vector field as well as its associated energy-momentum tensor. We also discuss how these fields can be used to construct exact solutions of Einstein s equations describing homogeneous but anisotropic universes. In Section 3 we consider the problem of classifying all possible types of singularities which may occur at finite time in spatially closed Friedmann-Robertson-Walker (FRW) models. This classification turns out to depend crucially upon whether or not the universe contains any matter sources other than radiation. Finally, in Section 4 we show that the presence of a non-vanishing Weyl tensor leads to an additional type of singularity which cannot arise if one assumes that the universe consists only of ordinary matter.",
        "watermark_text": "The present work is devoted to the examination of some elements of geometry and topology in relativistic cosmology , with particular importance on the part played by the Weyl vector . In Section 2 we review the fundamental concepts of Riemannian topology that are important for our purposes ; in particular , we investigate the notion of conformal vector field as well as its attendant energy - momentum tensor .We also discuss how these fields can be used to build exact solutions of Einstein s equations representing homogeneous but anisotropic universes . In Section 3 we investigate the question of classifying all possible kinds of singularities which may happen at finite time in spatially closed Friedmann - Robertson - Walker ( FRW ) models .This classification happens out to depend crucially upon whether or not the universe possesses any matter sources other than radiation . Finally , in Section 4 we prove that the presence of a non - vanishing Weyl vector results to an additional type of singularity which cannot arise if one assumes that the universe consists only of normal materials .",
        "rewrite_text": "This work focuses on exploring certain aspects of geometry and topology within the context of relativistic cosmology, emphasizing the role of the Weyl vector. In Section 2, we review essential concepts of Riemannian topology that are pertinent to our study, particularly examining the definition of a conformal vector field and its associated energy-momentum tensor. We also demonstrate how these fields can be utilized to construct exact solutions to Einstein's equations that depict homogeneous yet anisotropic universes. In Section 3, we tackle the classification of all potential types of singularities that can occur at finite time within spatially closed Friedmann-Robertson-Walker (FRW) models. This classification is significantly influenced by the presence of matter sources beyond radiation. Lastly, in Section 4, we establish that the existence of a non-zero Weyl vector leads to a distinct type of singularity that cannot occur if we assume the universe comprises solely normal matter.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.391092135317257,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We report results of X-ray observations with Suzaku satellite of the nucleus of radio-loud active galaxy Centaurus A (NGC 5128). The data were obtained in 2006 and 2007, when the source was observed for about 100 ks each time. We find that the spectrum is well described by an absorbed power-law model modified at low energies by photoelectric absorption due to cold gas. No significant emission lines are detected above 3 keV. Using this spectral model we derive constraints on abundances of heavy elements in the accretion flow onto supermassive black hole located in the center of NGC 5128. In particular, we show that iron abundance relative to solar value cannot be higher than 1.5-2 times its solar value. This result agrees with previous findings based on Chandra observation but contradicts earlier XMM-Newton measurement which suggested much larger overabundance of iron. Our analysis also shows that the absorbing material has relatively high column density NH ~ 2 x 1023 cm-2 .",
        "watermark_text": "We report findings of X - ray observations with Suzaku satellite of the nucleus of radio - loud active galaxy Centaurus A ( NGC 5128 ) . The data were obtained in 2006 and 2007 , when the origin was seen for about 100 ks each time .We see that the spectrum is well described by an absorption power - law theory refined at low energies by photoelectric diffusion owing to cold vapor . No notable emission lines are detected above 3 keV .Using this spectral method we derive restrictions on abundances of heavy metals in the accretion flow onto supermassive black hole located in the center of NGC 5128 . In particular , we prove that iron abundance relative to solar value cannot be higher than 1 . 5 - 2 twice its solar value .This result agrees with previous findings based on Chandra observation but contradicts earlier XMM - Newton measurement which suggested much larger overabundance of metals . Our study also shows that the absorbing material has relatively high column thickness NH ~ 2 x 1023 mm - 2 .",
        "rewrite_text": "We present the results of X-ray observations conducted with the Suzaku satellite on the nucleus of the radio-loud active galaxy Centaurus A (NGC 5128). The data were collected in 2006 and 2007, with each observation lasting approximately 100 ks. Our analysis reveals that the spectrum is accurately described by an absorption power law model, which is adjusted for low energies due to photoelectric absorption from cold gas. We do not detect any significant emission lines above 3 keV. Utilizing this spectral analysis, we establish constraints on the abundances of heavy metals in the accretion flow surrounding the supermassive black hole at the center of NGC 5128. Notably, we find that the abundance of iron relative to the solar value cannot exceed 1.5 to 2 times the solar level. This finding is consistent with earlier results from Chandra observations, but it contradicts previous measurements from XMM-Newton, which indicated a significantly higher overabundance of metals. Additionally, our study indicates that the absorbing material has a relatively high column density of NH ~ 2 x 10^23 cm^-2.",
        "ori-fast-z-score": 1.7888543819998317,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": -0.22360679774997896
    },
    {
        "original_text": "We present the first results on clustering measurements for luminous red galaxies (LRGs) in the redshift range 0.5 <z<0.8, obtained with the Anglo-Australian Observatory s multi-object spectrograph AAOmega. We use data from the 2dF-SDSS LRG and QSO survey to measure the projected correlation function wp(rp). The observed clustering amplitude is consistent with that expected from linear theory predictions based on current cosmological models. This result provides an important test of these models over this redshift range where there are few other constraints available. In addition we find evidence for evolution in the galaxy bias parameter between our two samples separated by ~0.2 Gyrs. These results will be presented in detail elsewhere. \n \n Keywords: Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology. 1 Introduction \n \n A number of recent studies have shown that luminous red galaxies (hereafter LRGs), selected via their optical colours or near-infrared photometry, provide powerful probes of large-scale structure out to high redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008) . Their large luminosities mean they can be detected efficiently even at relatively low redshifts, while their red colours make them easy to identify spectroscopically. They also tend to reside in massive dark matter haloes which evolve slowly through cosmic time, making them useful tracers of the underlying mass distribution. As such, they offer unique opportunities to study both the growth of structures as well as the nature of dark energy driving its accelerated expansion (see e.g., Percival & White 2009 , for a review). \n \n Here we report the first measurement of the spatial clustering properties of LRGs in the redshift range 0<z<0.8 made possible by combining data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) , the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.",
        "watermark_text": "We publish the first findings on clustering observations for luminous red objects ( LRGs ) in the redshift region 0 . 5 < z < 0 . 8 , obtained with the Anglo - Australian Observatory s multi - object spectrograph AAOmega . We use data from the 2dF - SDSS LRG and QSO studies to measure the projected relationship value wp ( rp ) .The observed clustering amplitude is compatible with that expected from linear theoretical estimates based on current cosmological models . This result provides an important test of these models over this redshift region where there are few other constraints offered .In addition we find proof for evolution in the galaxy bias variable between our two specimens divided by ~ 0 . 2 Gyrs . These conclusions will be shown in detail elsewhere .Keywords : Luminous Red Galaxies ; Clustering ; Bias Evolution ; Cosmology . 1 Introduction A variety of recent studies have shown that luminous red clusters ( hereafter LRGs ) , selected via their optical colours or near - infrared photometry , provide potent probes of large - scale organization out to large redshifts ( e . g . , Eisenstein et al .2001 ; Wake et al . 2006 ; Padmanabhan et al .2007 ; Blake et al . 2008 ; Ross et al .2008 ) . Their large luminosities guarantee they can be identified efficiently even at fairly little redshifts , while their red colours help them easy to identify spectroscopically .They also seem to live in massive dark matter haloes which evolution gradually through cosmic time , making them useful tracers of the ongoing mass distribution . As such , they give unique possibilities to study both the development of structures as also as the nature of bright energy causing its rapid increase ( saw e . g . , Percival & White 2009 , for a review ) .Here we publish the first measurement of the spatial clustering behavior of LRGs in the redshift region 0 < z < 0 . 8 made possible by combining information from the Sloan Digital Sky Survey ( SDSS ) ( York et al . 2000 ) , the Two Degree Field Galaxy Redshift Survey ( 2dFGRS ) ( Colless et al .",
        "rewrite_text": "We present the inaugural findings on the clustering of luminous red objects (LRGs) within the redshift range of 0.5 < z < 0.8, utilizing data from the AAOmega multi-object spectrograph at the Anglo-Australian Observatory. By analyzing data from the 2dF-SDSS LRG and QSO studies, we have measured the projected correlation function, wp(rp). The observed clustering strength aligns with predictions from linear theoretical estimates based on contemporary cosmological models, thereby offering a significant test of these models in a redshift range with limited existing constraints. Additionally, our analysis indicates evidence of evolution in the galaxy bias variable across our two samples separated by approximately 0.2 Gyr. Detailed discussions of these findings will be presented in future publications.\n\nKeywords: Luminous Red Galaxies; Clustering; Bias Evolution; Cosmology. \n\n1 Introduction  \nRecent research highlights that luminous red galaxies (LRGs), identified through their optical colors or near-infrared photometry, serve as powerful probes of large-scale structure even at substantial redshifts (e.g., Eisenstein et al. 2001; Wake et al. 2006; Padmanabhan et al. 2007; Blake et al. 2008; Ross et al. 2008). Their high luminosity enables efficient identification at lower redshifts, while their red hues simplify spectral identification. Moreover, LRGs are found in massive dark matter halos that evolve over cosmic time, making them valuable tracers of mass distribution. They thus offer unique opportunities to study both the evolution of large-scale structures and the nature of dark energy driving this growth (as reviewed in Percival & White 2009). Here, we present the first measurement of the spatial clustering of LRGs in the redshift range 0 < z < 0.8, made possible by integrating data from the Sloan Digital Sky Survey (SDSS) (York et al. 2000) and the Two Degree Field Galaxy Redshift Survey (2dFGRS) (Colless et al.",
        "ori-fast-z-score": -0.9610744623271417,
        "water-fast-z-score": 7.0164641544562345,
        "rewrite-fast-z-score": 0.4508348173337161
    },
    {
        "original_text": "In this work, we propose an approach to human identification based on the analysis of image attributes and their relationships with each other. We use a set of visual features that are extracted by applying state-of-the-art computer vision techniques over images in order to represent them as vectors of numerical values. These feature vectors can be used to train machine learning algorithms such as Support Vector Machines (SVMs) or Random Forests (RF). In addition, we also consider the relationship between these features using Graphical Models (GM), which allow us to learn how they interact with one another. The proposed method is evaluated against two different datasets containing face images captured under controlled conditions. Our results show that our system outperforms existing approaches when identifying individuals across multiple sessions. This research was supported by the National Science Foundation through awards IIS-1253153 and CNS-1527225. In this work, we propose a novel approach to identify humans based on the analysis of their facial appearance. To do so, we extract several visual features from faces using state-of-the-art computer vision methods. Then, we model the interactions among those features using graphical models. Finally, we evaluate the performance of our method against two publicly available databases.",
        "watermark_text": "In this project , we develop an way to human identification relying on the examination of visual attributes and their connections with each other . We use a group of visual elements that are derived by using state - of - the - art computer vision techniques over images in order to represent them as matrices of numerical values .These feature vectors can be used to train machine understanding algorithms such as Support Vector Machines ( SVMs ) or Random Forests ( RF ) . In addition , we also consider the relationship between these characteristics utilizing Graphical Models ( GM ) , which allow us to study how they interact with one another .The proposed approach is evaluated against two different datasets featuring face photos taken under controlled circumstances . Our results show that our system outperforms previous techniques when identifying persons across multiple sessions .This research was supported by the National Science Foundation through awards IIS - 1253153 and CNS - 1527225 . In this research , we propose a innovative method to identify humans based on the evaluation of their facial form .To do so , we extract many graphical features from faces utilizing state - of - the - art computer vision methods . Then , we study the interactions among those characteristics utilizing graphical descriptions .Finally , we assess the performance of our technique against two publicly accessible data .",
        "rewrite_text": "In this project, we develop a novel method for human identification by analyzing visual attributes and their interconnections. We employ advanced computer vision techniques to extract a variety of visual elements from images, which we then represent as matrices of numerical values. These feature vectors serve as input for machine learning algorithms such as Support Vector Machines (SVMs) and Random Forests (RF). Furthermore, we explore the relationships among these features using Graphical Models (GM), enabling us to investigate their interactions. Our proposed method is evaluated on two distinct datasets containing face images captured in controlled environments. The results demonstrate that our system surpasses previous methods in identifying individuals across multiple sessions. This research has been funded by the National Science Foundation via awards IIS-1253153 and CNS-1527225. In this study, we introduce an innovative approach for human identification focused on analyzing facial structures. We extract numerous graphical features from faces using state-of-the-art computer vision techniques and examine the interactions among these features through graphical representations. Finally, we evaluate the effectiveness of our method against two publicly available datasets.",
        "ori-fast-z-score": -1.507556722888818,
        "water-fast-z-score": 6.4,
        "rewrite-fast-z-score": -1.0425720702853738
    },
    {
        "original_text": "We present results on distance determinations for two galaxies, NGC 3109 and WLM, based on observations made with the Hubble Space Telescope (HST). We have used HST/WFPC2 images taken through filters F555W and F814W to search for Cepheids among young open clusters located within these galaxies. Our survey has resulted in the discovery of four new Cepheids in NGC 3109 and one in WLM. These five Cepheids are all short-period classical pulsators with periods ranging between 4.5 days and 8.6 days. Using the period-luminosity relation derived by Madore & Freedman we find distances to NGC 3109 and W LM that agree well with previous estimates obtained using other methods. \n \n Keywords: Cepheid variables; open cluster; galaxy distance scale; Hubble Space Telescope; Araucaria Project. 1. Introduction \n \n Open clusters provide an important tool for determining extragalactic distances because they contain many stars at nearly identical ages and chemical compositions. In addition, open clusters can be found over a wide range of galactocentric radii, allowing us to probe different environments. However, open clusters are relatively rare objects compared to field stars or globular clusters. Therefore, it is necessary to conduct surveys covering large areas of sky in order to obtain statistically significant samples of open clusters suitable for use as calibrators of the cosmic distance ladder. \n \n The Araucaria Project was initiated in 1998 with the goal of obtaining accurate distances to nearby galaxies via measurements of Cepheid variable stars associated with open clusters. This project uses data collected primarily with the Hubble Space Telescope s WFPC2 camera. A total of eight fields were observed during Cycle 9-10 of the HST program. Each field covers about 0.25 square degrees centered around a target galaxy. For each field, deep exposures were obtained in both the F555W and F850LP bands. Details regarding this project may be found in Pietrzyński et al. (2002) and references therein. \n \n 2. Searching for Cepheids Among Young Open",
        "watermark_text": "We report findings on distance determinations for two galaxies , NGC 3109 and WLM , built on observations made with the Hubble Space Telescope ( HST ) . We have utilized HST / WFPC2 pictures taken through filters F555W and F814W to search for Cepheids among small close galaxies housed within these objects .Our survey has resulted in the discovery of four fresh Cepheids in NGC 3109 and one in WLM . These five Cepheids are all short - process classical pulsators with periods ranging between 4 . 5 weeks and 8 . 6 hours .Using the period - luminosity relation derived by Madore & Freedman we find distances to NGC 3109 and W LM that agree well with previous estimates obtained using other methods . Keywords : Cepheid variables ; open cluster ; galaxy distance scale ; Hubble Space Telescope ; Araucaria Project .1 . Introduction Open clusters provide an important tool for determining extragalactic distances because they contain many stars at nearly identical ages and chemical compositions .In addition , open complexes can be found over a broad variety of galactocentric radii , allowing us to probe different environments . However , open complexes are fairly scarce objects compared to field stars or globular galaxies .Therefore , it is required to conduct surveys covering large areas of skies in order to obtain statistically substantial samples of open complexes suitable for use as calibrators of the cosmic diameter staircase . The Araucaria Project was initiated in 1998 with the objective of acquiring precise lengths to nearby galaxies via measurements of Cepheid variable stars associated with open complexes .This project utilizes information collected principally with the Hubble Space Telescope s WFPC2 telescope . A total of eight fields were detected during Cycle 9 - 10 of the HST project .Each field covers about 0 . 25 square degrees centered around a target galaxy . For each field , deep exposures were obtained in both the F555W and F850LP bands .Details regarding this project would be found in Pietrzyński et al . ( 2002 ) and links therein .2.Searching for Cepheids Among Young Open",
        "rewrite_text": "We present our findings on distance measurements for two galaxies, NGC 3109 and WLM, based on observations made with the Hubble Space Telescope (HST). Using HST/WFPC2 images captured through the F555W and F814W filters, we searched for Cepheid variables within these nearby small galaxies. Our survey has led to the discovery of four new Cepheids in NGC 3109 and one in WLM. All five Cepheids identified are short-period classical pulsators, with periods ranging from 4.5 weeks to 8.6 hours. Applying the period-luminosity relationship established by Madore & Freedman, we find that the distances to NGC 3109 and WLM are consistent with prior estimates obtained through various methods. \n\nKeywords: Cepheid variables; open clusters; galaxy distance scale; Hubble Space Telescope; Araucaria Project. \n\n1. Introduction \n\nOpen clusters serve as a valuable tool for measuring extragalactic distances since they comprise numerous stars with nearly uniform ages and chemical compositions. Furthermore, open clusters are distributed across a wide range of galactocentric radii, enabling us to investigate different environments. However, open clusters are relatively rare when compared to field stars or globular clusters. Therefore, extensive surveys covering large portions of the sky are necessary to gather statistically significant samples of open clusters that can serve as calibrators of the cosmic distance scale. The Araucaria Project, initiated in 1998, aims to obtain accurate distances to nearby galaxies through the measurement of Cepheid variable stars associated with open clusters. This project primarily relies on data collected using the WFPC2 camera on the Hubble Space Telescope. During HST Cycles 9 and 10, a total of eight fields were observed, each covering approximately 0.25 square degrees centered on target galaxies. For each field, deep exposure images were captured in both the F555W and F850LP bands. For more details about this project, refer to Pietrzyński et al. (2002) and their associated links.\n\n2. Searching for Cepheids Among Young Open Clusters",
        "ori-fast-z-score": 0.7689218919450849,
        "water-fast-z-score": 7.831560082980487,
        "rewrite-fast-z-score": 0.5144957554275265
    },
    {
        "original_text": "We study the solidification process in a model system consisting of particles interacting via repulsive soft-core potentials and subject to an external driving force. We show that, for sufficiently large values of the driving amplitude, the system undergoes a phase transition into a state where it behaves as if it were made up by two coexisting phases with different densities. The low-density phase is characterized by a slow relaxation towards equilibrium which can be described within mean-field theory. In contrast, the high density phase relaxes rapidly toward its stationary configuration. \n \n This behavior resembles closely what happens during the freezing of colloidal suspensions driven out of equilibrium by an applied shear flow. Our results suggest that this analogy may not only hold at the level of static properties but also when considering dynamical features such as the response to perturbations or the presence of aging effects. Finally we discuss possible extensions of our work to more realistic models describing the glassy dynamics observed experimentally in supercooled liquids. \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest on the possibility of observing analogies between the physics of glasses and other disordered systems  1  . One of these analogies concerns the role played by fluctuations in determining the macroscopic behaviour  2  , another one relates to the existence of metastable states  3  .\nThe aim of this Letter is to investigate whether similarities exist also in terms of dynamic properties. To this end we consider a simple model of glass-forming liquid  4  whose microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under the action of pairwise interactions. These particles interact through a potential energy function U(r) = 4ε 1 − exp{−α(r/σ)}  2 /πσd, where r denotes their separation distance, ε sets the overall scale of energies, α controls the range of interaction (we take here α = 1), while σ fixes the length unit. For simplicity we assume periodic boundary conditions so that the total number of particles remains constant throughout the simulation. As usual, we define the reduced temperature T * ≡ kT/",
        "watermark_text": "We test the solidification mechanism in a model scheme consisting of molecules evolving via repulsive soft - core potentials and subject to an external driving field . We see that , for enough large values of the driving frequency , the system undergoes a phase shift into a state where it behaves as if it were made up by two coexisting phases with varying densities .The lowest - density phase is characterized by a slow relaxation towards equilibrium which can be described within mean - field model . In comparison , the high density phase relaxes rapidly toward its stationary position .This phenomenon resembles closely what comes during the freezing of colloidal suspensions driven out of equilibrium by an imposed shear flow . Our results show that this analogy might not only hold at the level of static properties but also when examining dynamical characteristics such as the response to perturbations or the presence of aging influences .Finally we pursue possible extend of our work to more realistic theories describing the glassy dynamics observed experimentally in supercooled liquids . I .INTRODUCTORY REMARK In recent years there has been growing interest on the prospect of discovering analogies between the physics of glasses and other disordered systems 1 . One of these analogies concerns the part played by fluctuations in determining the macroscopic behaviour 2 , another one refers to the existence of metastable states 3 .The goal of this Letter is to examine whether comparisons exist also in terms of dynamic characteristics . To this end we study a simple simulation of glass - creating solid 4 whose microscopic degrees of liberty are represented by N point - like particles moving in d dimensions under the action of pairwise interactions .These particles react through a potential energy relation U ( r ) = 4ε 1 − exp { −α ( r / π ) } 2 / πσd , where p denotes their separation distance , ε sets the overall scale of energies , α handles the range of interaction ( we took here α = 1 ) , while ρ fixes the length unit . For simplicity we suppose discrete border conditions so that the total number of particles stay constant throughout the model .As usual, we define the reduced temperature T * ≡ kT/",
        "rewrite_text": "We investigate the solidification process using a model where molecules evolve under repulsive soft-core potentials and are influenced by an external driving field. Our findings indicate that at sufficiently high driving frequencies, the system transitions into a state that resembles two coexisting phases with differing densities. The lower-density phase exhibits slow relaxation towards equilibrium, which can be described using a mean-field model, while the higher-density phase relaxes quickly to its stable position. This behavior closely resembles the freezing process observed in colloidal suspensions subjected to an external shear flow. Our results suggest that this analogy may extend beyond static properties to dynamic features, such as response to perturbations and aging effects. Finally, we explore the potential extension of our study to more realistic theories that account for the glassy dynamics observed in experiments with supercooled liquids.\n\n**I. INTRODUCTORY REMARKS**  \nIn recent years, there has been increasing interest in uncovering parallels between the physics of glasses and other disordered systems. One such parallel involves the role of fluctuations in shaping macroscopic behavior, while another concerns the presence of metastable states. This Letter aims to explore whether there are also dynamic similarities. To address this, we examine a simple simulation of a solid that can form glass, represented by N point-like particles moving in d dimensions under pairwise interactions. These particles interact through a potential energy function given by \\(U(r) = 4\\epsilon \\left[1 - \\exp\\left(-\\alpha\\frac{r}{\\pi}\\right)\\right]^2/\\pi\\sigma^d\\), where \\(r\\) represents their separation distance, \\(\\epsilon\\) sets the overall energy scale, \\(\\alpha\\) determines the interaction range (with \\(\\alpha = 1\\) in our study), and \\(\\sigma\\) defines the unit of length. For simplicity, we assume periodic boundary conditions to maintain a constant total number of particles throughout the model. As usual, we define the reduced temperature \\(T^* \\equiv kT/\\).",
        "ori-fast-z-score": -1.9826289642953603,
        "water-fast-z-score": 6.913932971601798,
        "rewrite-fast-z-score": 0.48349377841522817
    },
    {
        "original_text": "The Chandra X-ray Observatory has observed the supernova remnant (SNR) produced by SN1987A in the Large Magellanic Cloud for over ten years, providing an unprecedented view into this young and energetic object.  The observations have revealed that the blast wave is interacting with dense circumstellar material surrounding the progenitor star at velocities up to 1000 km/sec.  This interaction produces bright knots of emission which are seen as moving outward through the shell of the remnant.  These knots appear to be composed primarily of oxygen-rich ejecta mixed with shocked interstellar gas.  In addition, there appears to be a large amount of hot plasma trapped behind the forward shock front.  We present here new results on these features based on our analysis of data obtained during the first year of the Chandra mission. The Chandra X-ray Observatory has observed  the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an unprecedented view into this y oung and en erg i c obj ect .  T he obse rvations ha ve reve al ed tha t th e b las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star-at-rou nd -velocities up to 1 000 k m/ sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene -rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "watermark_text": "The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A in the Large Magellanic Cloud for over ten years , providing an remarkable insight into this young and intense object . The studies have revealed that the explosion wave is interacting with heavy circumstellar material surrounding the progenitor star at velocities up to 1000 cm / sec .This coupling generates bright knots of emission which are seen as traveling outward through the shell of the remnant . These knots appear to be composed primarily of oxygen - laden ejecta blended with shocked interstellar gas .In addition , there seems to be a large amount of bright plasma trapped behind the front shock front . We present here new data on these characteristics focusing on our analysis of evidence derived during the first year of the Chandra mission .The Chandra X - ray Observatory has observed the supernova remnant ( SNR ) produced by SN1987A , in the Large Magellan ic Cloud , for over ten years . It provides an remarkable view into this y oung and en erg i c obj ect .T he obse rvations ha ve reve al ed tha t th e f las t wa ve is interactin g wi th d ens e circumstell ar m aterial surroundi ng th e proge nitor star - at - rou nd - velocities up to 1 000 k m / sec . Thi s interac tion produ ces brigh t kn ots of emi ssion wh ich appea r to be co mp osed pri marily of oxyg ene - rico h ej ec ta mixe d w ith sh ocked int erstel lar ga s .",
        "rewrite_text": "The Chandra X-ray Observatory has been observing the supernova remnant (SNR) created by SN1987A in the Large Magellanic Cloud for more than a decade, offering remarkable insights into this young and energetic object. These observations have shown that the shock wave from the explosion is interacting with dense circumstellar material surrounding the progenitor star at speeds of up to 1000 km/s. This interaction produces bright knots of emission that appear to be moving outward through the remnant's shell. These knots primarily consist of oxygen-rich ejecta mixed with shocked interstellar gas. Additionally, there appears to be a significant amount of bright plasma trapped behind the front shock wave. Here, we present new data on these features, focusing on our analysis from the first year of the Chandra mission.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "We introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 × Z2 then our construction yields two gapped phases distinguished by their chiral central charges c− = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.",
        "watermark_text": "We introduce the notion of algebraic charge liquids , which are represented as ground regions of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators . We see how to build such descriptions for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions .The resulting theory is precisely solvable when G has no non - trivial subgroups . In this situation we find that there exists at least one phase shift between various phases characterized by separate topological orders .For instance , if G = Z2 × Z2 then our construction gives two gapped phases distinguished by their chiral central charges c− = 0 or 1 . If G contains a nontrivial subgroup H then the system displays gapless excitations corresponding to ions transforming according to irreducible representations ( irreps ) of H . These data provide fresh insights into the characterization question of quantum several - bodies systems .",
        "rewrite_text": "We present the concept of algebraic charge liquids, which are defined as the ground states of Hamiltonians with local interactions expressible in terms of fermionic creation and annihilation operators. We demonstrate how to construct such descriptions for any finite group \\( G \\) by utilizing an explicit representation of \\( G \\) on the Hilbert space of spinless fermions. When \\( G \\) lacks non-trivial subgroups, the resulting theory is exactly solvable. In this scenario, we identify at least one phase shift among various phases, each characterized by distinct topological orders. For example, when \\( G = \\mathbb{Z}_2 \\times \\mathbb{Z}_2 \\), our framework yields two gapped phases that are distinguished by their chiral central charges \\( c^- = 0 \\) or \\( 1 \\). Conversely, if \\( G \\) includes a nontrivial subgroup \\( H \\), the system reveals gapless excitations linked to ions that transform according to irreducible representations (irreps) of \\( H \\). This information offers new perspectives on the classification of quantum many-body systems.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -1.3643820804812932
    },
    {
        "original_text": "We study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "We explore the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method . We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods .In addition , we find that the first - order corrections are equal to the square root of the volume enclosed by the entangling surface . Finally , we estimate the second - order corrections and find an expression containing two terms .One of them has been previously found in Ref.Phys.Rev.D 98 (2018) 084011  while another one is new.The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points . This result suggests that the gravitational Chern - Simons correlation function plays a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "We investigate the holographic entanglement entropy in three-dimensional de Sitter space that includes a gravitational Chern-Simons term, employing the replica trick and the covariant phase space method. Our findings indicate that there are no logarithmic corrections to the entanglement entropy, consistent with previous results obtained through alternative methods. Additionally, we discover that the first-order corrections correspond to the square root of the volume enclosed by the entangling surface. Furthermore, we estimate the second-order corrections and arrive at an expression comprising two terms: one previously identified in Phys. Rev. D 98 (2018) 084011, while the other represents a new finding. The latter can be expressed as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This outcome suggests that the gravitational Chern-Simons correlation function serves a role analogous to Newton's constant in four dimensions.",
        "ori-fast-z-score": 1.1920791213585393,
        "water-fast-z-score": 3.474396144861517,
        "rewrite-fast-z-score": -0.2581988897471611
    },
    {
        "original_text": "We present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ...  more   In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.",
        "watermark_text": "We suggest an appropriate design algorithm to find the best occulting mask that can be used in direct imaging searches for extrasolar planets . The proposed approach is based on the idea of entropy maximization , which has been widely applied in different fields such as data physics and statistical mechanics .We see how this concept can be generalized into the field of optics by using a new quantity called optical entropy ( OE ) . By using OE we are able to quantify the quantity of information stored within each point spread function generated by various masks .This enables us to predict the most efficient mask shape with regard to its capacity to identify dim companions around bright stars . . . . more In order to test our approach , we have done mathematical simulations to study the performance of several candidate masks against one another .Our results show that the suggested method provides significant progress over existing techniques when it comes to finding the ideal mask shapes for detecting bright companions around bright host stars .",
        "rewrite_text": "We propose a design algorithm to identify the optimal occulting mask for direct imaging searches of extrasolar planets. Our approach leverages the principle of entropy maximization, which has been effectively applied in various disciplines, including data physics and statistical mechanics. We extend this concept into optics through a new metric known as optical entropy (OE). By utilizing OE, we can measure the amount of information contained within each point spread function produced by different masks. This allows us to forecast the most effective mask shape for detecting faint companions near bright stars. To evaluate our method, we conducted mathematical simulations comparing the performance of several candidate masks. Our findings indicate that our proposed technique significantly outperforms existing methods in identifying optimal mask shapes for detecting luminous companions around bright host stars.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 6.399448505650358,
        "rewrite-fast-z-score": -0.5933908290969266
    },
    {
        "original_text": "The Casimir effect is one of the most important manifestations of quantum vacuum fluctuations and has been studied extensively for more than 50 years. The original prediction was made by H.B.G. Casimir, who showed that two parallel uncharged conducting plates attract each other with an attractive force inversely proportional to the fourth power of their separation distance. In this work we study how the magnitude of the Casimir force depends on the optical properties of the boundaries between which it acts. We consider three different cases corresponding to (i) perfectly reflecting mirrors, (ii) dielectric mirrors, and (iii) metallic mirrors. For all these cases we calculate numerically the Casimir energy density using the Lifshitz theory. Our results show that the Casimir force acting between two identical mirrors can be either repulsive or attractive depending on whether they are separated by air or water. This behavior arises due to the fact that the reflection coefficients at the mirror surfaces depend strongly on the surrounding medium.",
        "watermark_text": "The Casimir effect is one of the most important manifestations of quantum vacuum fluctuations and has been studied frequently for more than 50 years . The original forecast was making by H . B . G .Casimir , who demonstrated that two connected uncharged conducting plates attract each other with an attractive force inversely proportional to the fourth power of their separation distance . In this research we study how the severity of the Casimir force depends on the optical properties of the boundaries between which it works .We consider three different instances corresponding to ( i ) completely reflecting mirrors , ( ii ) dielectric windows , and ( iii ) metallic mirrors . For all these cases we estimate numerically the Casimir electricity distribution using the Lifshitz principle .Our results show that the Casimir force acting between two different mirrors can be either repulsive or attractive depending on whether they are apart by air or air . This phenomenon arises owing to the fact that the reflection values at the glass surfaces depend intensely on the nearby medium .",
        "rewrite_text": "The Casimir effect is a significant illustration of quantum vacuum fluctuations and has been extensively studied for over 50 years. The phenomenon was first predicted by H.B.G. Casimir, who showed that two uncharged conductive plates attract each other with a force inversely proportional to the fourth power of their separation. In this research, we examine how the magnitude of the Casimir force is influenced by the optical properties of the surfaces involved. We analyze three different scenarios: (i) perfectly reflecting mirrors, (ii) dielectric windows, and (iii) metallic mirrors. For each case, we numerically compute the Casimir energy distribution using the Lifshitz theory. Our findings indicate that the Casimir force between two distinct mirrors can either be repulsive or attractive, depending on whether the space between them is filled with air or another medium. This behavior is rooted in the fact that the reflection coefficients at the surfaces vary significantly based on the adjacent medium.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": -0.9847319278346618
    },
    {
        "original_text": "The aim of this article is to present the basic concepts in Riemannian geometry that are needed for understanding the main results presented here.  The first section introduces some notation and definitions used throughout the text.   In particular we define what it means for two points on an n-dimensional manifold M to be close together (in terms of geodesic distance) or far apart.    We also introduce the concept of a local coordinate system at each point p ∈ M which allows us to describe any other point q near p by giving its coordinates with respect to these local charts.   Finally we give a brief description of how one can construct such a coordinate system locally around a given point using parallel transport along curves starting at p.    The second section describes the notion of a vector field X defined over all of M.   This is done by defining a map F : T M → R where T M denotes the tangent bundle of M.   Then we show that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad(f).   Here grad(f) denotes the gradient of f.   For example, if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M.   If we assume that the particles move according to Newton s laws of motion then the function f would correspond to the potential energy of the system under consideration.   The third section defines the concept of a tensor field as a generalization of vector fields.   Tensor fields allow us to associate several vectors...",
        "watermark_text": "The goal of this page is to provide the fundamental concepts in Riemannian topology that are needed for studying the main results presented here . The first section introduces some terminology and definitions found throughout the text .In particular we define what it means for two points on an n - dimensional manifold M to be close together ( in terms of geodesic length ) or far separated . We additionally introduce the notion of a local coordinate system at each point p ∈ M which allows us to define any other point q near r by giving its coordinates with regard to these local charts .Finally we give a brief description of how one can build such a coordinate system locally around a given point using parallel transport along curves beginning at p . The second chapter explains the notion of a vector field X defined over all of M . This is accomplished by constructing a mapping F : T M → R where T M denotes the tangent bundle of M . Then we prove that if X satisfies certain conditions then there exists a unique smooth function f : M → R such that X = grad ( f ) . Here grad ( f ) denotes the gradient of f . For instance , if M is a surface embedded in R3 then X could represent the velocity of a particle moving across M . If we suppose that the molecules moving according to Newton s rules of movement then the function r would correspond to the potential electricity of the system under consideration .The third chapter explains the idea of a tensor field as a generalization of vector spaces . Tensor varieties allow us to relate several vectors . . .",
        "rewrite_text": "The purpose of this page is to outline the essential concepts in Riemannian topology necessary for understanding the key results presented here. The first section introduces important terminology and definitions that recur throughout the text. In particular, we clarify what it means for two points on an n-dimensional manifold M to be either close (in terms of geodesic length) or far apart. We also introduce the idea of a local coordinate system at each point p ∈ M, which enables us to express any nearby point q using coordinates based on these local charts. Lastly, we provide a brief overview of how to construct such a coordinate system locally around a point using parallel transport along curves originating from p.\n\nThe second chapter discusses the concept of a vector field X defined across the entirety of M. This is achieved by establishing a mapping F: TM → R, where TM denotes the tangent bundle of M. We then demonstrate that, under certain conditions, there exists a unique smooth function f: M → R such that X = grad(f), where grad(f) represents the gradient of f. For example, if M is a surface embedded in R³, then X could represent the velocity of a particle moving across M. Moreover, if we assume that the molecules are governed by Newton's laws of motion, then the function r would correspond to the potential energy of the system being analyzed.\n\nThe third chapter explores the concept of a tensor field as a generalization of vector spaces, allowing us to relate multiple vectors.",
        "ori-fast-z-score": -0.1781741612749496,
        "water-fast-z-score": 7.365059028153745,
        "rewrite-fast-z-score": 2.073840159735094
    },
    {
        "original_text": "We propose that the two branches in the mass distribution of neutron stars are due to different mechanisms for their formation, with one branch being formed by accretion-induced collapse (AIC) of white dwarfs into neutron stars, while another is formed through core-collapse supernovae (CCSNe). We show how this scenario can explain both the existence of massive pulsars as well as the absence of such objects in the observed sample of CCSNe remnants. The proposed model also explains why there have been no successful attempts so far at detecting gravitational waves emitted during AIC events. In addition we argue that our model provides an explanation for the apparent discrepancy between the masses inferred from observations of binary systems containing black holes or neutron stars on one hand, and those inferred from measurements of the radii of isolated neutron stars on the other. Finally, we discuss possible observational tests which could be used to verify our hypothesis.",
        "watermark_text": "We suggest that the two branches in the mass distribution of neutron galaxies are owing to different processes for their development , with one branch being created by accretion - caused merger ( AIC ) of white dwarfs into neutron galaxies , while another is formed through core - collapse supernovae ( CCSNe ) . We see how this situation can describe both the existence of large pulsars as well as the absence of such objects in the known sample of CCSNe fragments .The proposed theory even argues why there have been no successful proposals so far at detecting gravitational waves emitted during AIC events . In addition we claim that our model provides an reason for the apparent discrepancy between the masses inferred from measurements of binary systems featuring white holes or neutron galaxies on one hand , and those inferred from measurements of the radii of isolated neutron galaxies on the other .Finally , we explain possible observational tests which could be used to confirm our theory .",
        "rewrite_text": "We propose that the two distinct branches in the mass distribution of neutron galaxies result from different developmental processes. One branch arises from the accretion-induced collapse (AIC) of white dwarfs into neutron galaxies, while the other is a product of core-collapse supernovae (CCSNe). This framework accounts for the presence of large pulsars as well as the lack of such objects in the known remnants of CCSNe. Our theory also addresses the absence of successful attempts to detect gravitational waves produced during AIC events. Furthermore, we suggest that our model explains the apparent discrepancy between the masses estimated from observations of binary systems containing white dwarfs or neutron galaxies and those derived from measurements of the radii of isolated neutron galaxies. Lastly, we propose potential observational tests that could validate our theory.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "We present new results on the formation of bipolar lobes by an expanding, rotating surface explosion (a  rotating detonation ). We use two-dimensional hydrodynamic simulations to show that such explosions can form disks with large opening angles if they are not too energetic or fast-expanding. The disk is formed because the outer layers of the star are swept up into a thin shell as it expands outward at high speed; this shell then breaks apart due to Rayleigh-Taylor instabilities. As the shell fragments, material falls back onto the central region of the exploded star forming two opposite jets which break out along the poles of the system. These jets drive the expansion of the bipolar lobes. Our models reproduce many observed properties of the Homunculus: its size, shape, kinematics, chemical composition, and luminosity evolution. In addition, we find that our model predicts a total mass loss rate for η Carinae during the Great Eruption of ~10^−4 M_sun/yr, consistent with observations.",
        "watermark_text": "We report new data on the formation of bipolar lobes by an evolving , moving surface explosion ( a rotating detonation ) . We use two - dimensional hydrodynamic simulations to see that such explosions can form cones with large opening angles if they are not too energetic or fast - expanding .The disk is formed because the exterior layers of the star are swept up into a thin shell as it expands outward at high velocity ; this shell then splits separated due to Rayleigh - Taylor instabilities . As the shell cracks , debris slides back onto the main region of the exploded star producing two opposite jets which break out along the poles of the system .These jets drive the expansion of the bipolar lobes . Our models reproduce many observed properties of the Homunculus : its size , shape , kinematics , chemical composition , and luminosity progression .In addition , we find that our model predicts a total mass loss rate for η Carinae during the Great Eruption of ~ 10 ^ −4 M _ sun / yr , compatible with observations .",
        "rewrite_text": "We present new findings on the development of bipolar lobes resulting from a dynamic surface explosion, specifically a rotating detonation. Our two-dimensional hydrodynamic simulations demonstrate that these explosions can create cones with wide opening angles, provided they are not overly energetic or rapidly expanding. The disk is generated as the outer layers of the star are driven into a thin shell during a high-velocity outward expansion; this shell subsequently fragments due to Rayleigh-Taylor instabilities. As the shell breaks apart, debris falls back onto the core of the exploded star, generating two opposing jets that emerge along the system's poles. These jets facilitate the expansion of the bipolar lobes. Our models successfully replicate many characteristics of the Homunculus, including its size, shape, kinematics, chemical makeup, and luminosity evolution. Furthermore, our model estimates the total mass loss rate for η Carinae during the Great Eruption at approximately 10^−4 M_sun/year, which aligns with observational data.",
        "ori-fast-z-score": 2.264554068289191,
        "water-fast-z-score": 6.567206798038654,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We present the first detection of relativistically modulated X-ray fluxes from the Galactic Center black hole candidate SgrA*, using data obtained with Chandra and XMM-Newton observatories over an eight-year period (2000-2007). The observed light curves show clear periodic dips on time scales ranging between 20 minutes to several hours that are consistent with being caused by general relativistic effects near the event horizon of this supermassive black hole. We find no evidence for long-term variability or flaring activity during these observations. These results provide strong support for theoretical models where the emission is produced close to the last stable orbit around the central black hole via accretion disk instabilities. This work was supported by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Subject headings: Black holes -accretion disks -X-rays",
        "watermark_text": "We present the first detection of relativistically modulated X - ray fluxes from the Galactic Center black hole member SgrA * , using data acquired with Chandra and XMM - Newton observatories over an eight - month period ( 2000 - 2007 ) . The observed light curves show consistent continuous dips on time ranges ranging between 20 seconds to several days that are compatible with being affected by general relativistic effects near the event horizon of this supermassive black hole .We see no evidence for large - term variability or flaring activity during these observations . These conclusions provide broad support for theoretical theories where the emission is produced nearly to the last steady orbit around the central black hole via accretion ring instabilities .This project was supported by NASA grants NAG5 - 10842 , NNG06GH50G , NNX07AH41G , and NSF grant AST - 0707765 . Subject headings : Black holes - accretion disks - X - rays",
        "rewrite_text": "We report the first detection of relativistically modulated X-ray fluxes from the supermassive black hole at the Galactic Center, SgrA*, utilizing data collected from the Chandra and XMM-Newton observatories over an eight-month period from 2000 to 2007. The light curves indicate consistent, continuous dips occurring over time scales from 20 seconds to several days, which are consistent with the influence of general relativistic effects near the event horizon of the black hole. No evidence of long-term variability or flaring activity was observed during this period. These findings strongly support theoretical models suggesting that the emissions arise close to the stable orbit around the central black hole due to instabilities in the accretion ring. This research was funded by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Key areas of interest include black holes, accretion disks, and X-rays.",
        "ori-fast-z-score": -1.0327955589886444,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": -0.3779644730092272
    },
    {
        "original_text": "We study mesoscopic fluctuations of the supercurrents flowing through two weakly coupled superconductors with different transparencies and temperatures, using the Usadel equations for quasiclassical Green s functions. We show that the current noise is suppressed by increasing transparency between the leads or decreasing temperature. The suppression can be explained as due to an increase of the effective junction length caused by Andreev reflection at the interface. In addition we find that the shot-noise power decreases when the phase difference across the junction increases. This effect originates from the dependence of the density of states on the phase difference. Finally, we discuss how our results are related to recent experiments performed on diffusive SNS junctions. \nI. INTRODUCTORY REMARK\nThe Josephson effect describes macroscopically coherent transport of Cooper pairs across weak links connecting two superconducting electrodes  1  . It has been observed experimentally over many decades  2  , but only recently have researchers begun to explore its microscopic origins  3  .\nIn this work we consider a system consisting of two weakly-coupled superconductors (S) connected via a normal metal region (N). Such systems are known as diffusive SNS junctures  4  . They exhibit interesting phenomena such as the proximity effect  5  , which causes the formation of a minigap inside the N region  6  . Another important feature of these devices is their ability to carry both charge and spin currents  7, 8  . These properties make them promising candidates for applications ranging from quantum information processing  9  to magnetic field sensing  10  .\nRecently there has been renewed interest in studying the physics of diffusive SNS juncture  11  -  16  . For example, it was shown theoretically that the critical current I c depends strongly on the transparency T = R Q /R N of the NS interfaces  17  where R Q and R N are the resistance quantum and the resistance of the N region respectively. Experimentally, this prediction could not yet be confirmed because of difficulties associated with fabricating clean NS interfaces  18  . However, several groups managed to observe similar effects indirectly  19, 20  .",
        "watermark_text": "We test mesoscopic fluctuations of the supercurrents rushing through two tightly correlated superconductors with varying transparencies and temperatures , using the Usadel equations for quasiclassical Green s functions . We see that the current noise is suppressed by expanding transparency between the leads or decreasing temperature .The suppression can be understood as owing to an increase of the effective junction size caused by Andreev reflection at the interface . In addition we find that the shot - noise power decreases when the phase change across the junction increases .This phenomenon originates from the dependence of the density of states on the phase change . Finally , we talk how our findings are related to recent experiments conducted on diffusive SNS junctions .I . INTRODUCTORY REMARK The Josephson effect explains macroscopically consistent transport of Cooper pairs across weak links connecting two superconducting electrodes 1 .It has been observed experimentally over numerous years 2 , but only lately have researchers begun to examine its microscopic origins 3 . In this research we study a system consisting of two weakly - coupled superconductors ( S ) connected via a normal metal area ( N ) .Such structures are known as diffusive SNS junctures 4 . They show exciting phenomena such as the contact influence 5 , which forms the formation of a minigap inside the N region 6 .Another important feature of these systems is their potential to carry both charge and spin currents 7 , 8 . These properties make them promising candidates for applications extending from molecular data processing 9 to magnetic field monitoring 10 .Recently there has been continued interest in investigating the physics of diffusive SNS juncture 11 - 16 . For instance , it was shown theoretically that the critical potential I c depends strongly on the transparency T = R Q / R N of the NS interfaces 17 where R Q and R N are the tolerance quantum and the resistance of the N region respectively .Experimentally , this measurement came not already be verified because of troubles associated with fabricating safe NS interfaces 18 . However , various groups helped to observe comparable effects indirectly 19 , 20 .",
        "rewrite_text": "We investigate the mesoscopic fluctuations of supercurrents flowing through two closely linked superconductors with varying levels of transparency and temperature, utilizing the Usadel equations for quasiclassical Green's functions. Our findings indicate that the current noise diminishes when either the transparency between the leads is increased or the temperature is lowered. This reduction can be attributed to an apparent increase in the effective junction size, which is influenced by Andreev reflection at the interface. Furthermore, we observe that the shot noise power decreases as the phase difference across the junction increases, a phenomenon that stems from the phase-dependent nature of the density of states. We also relate our results to recent experiments on diffusive SNS junctions.\n\n**I. INTRODUCTORY REMARKS**  \nThe Josephson effect provides a macroscopic explanation for the transport of Cooper pairs across weak links that connect two superconducting electrodes. This phenomenon has been experimentally validated over many years, yet only recently have researchers started to investigate its underlying microscopic mechanisms. In this study, we focus on a system comprising two weakly coupled superconductors connected through a normal metal region, commonly referred to as diffusive SNS junctions. These structures exhibit intriguing phenomena, such as the contact effect, which leads to the emergence of a minigap within the normal region. Another significant characteristic of these systems is their ability to transport both charge and spin currents, making them potential candidates for applications ranging from molecular data processing to magnetic field sensing. There has been renewed interest in exploring the physics of diffusive SNS junctions, with theoretical studies indicating that the critical current’s dependence on the transparency of the NS interfaces is substantial. However, experimental verification of these theoretical predictions has faced challenges related to the fabrication of stable NS interfaces. Nonetheless, various research groups have indirectly observed similar effects.",
        "ori-fast-z-score": -0.5895063447465633,
        "water-fast-z-score": 8.446044301071616,
        "rewrite-fast-z-score": 0.9330078226479681
    },
    {
        "original_text": "We report on the results of an analysis of data taken by the INTEGRAL satellite in 2003 and 2004 . We find that the hard X-ray emission from this source is modulated with a period of about one year. The amplitude of the modulation is at least 50% (3 sigma). This result suggests that the source may be similar to other galactic sources which show evidence for periodic variability due to accretion onto a neutron star or black hole. \n \n Keywords: High energy astrophysics - Gamma rays - Black holes - Neutron stars - Accreting binaries - Pulsar wind nebulae - Inverse Compton scattering - Galactic center - Galaxy - Supernova remnants - Blazars - AGN - Cosmic rays - Fermi/LAT - TeV blazar - Variability - Periodicities - INTEGRAL - X-rays - Hard X-rays - Soft gamma-ray bursts - Transient objects - Radio pulsar",
        "watermark_text": "We report on the conclusion of an assessment of measurements taken by the INTEGRAL satellite in 2003 and 2004 . We see that the hard X - ray radiation from this source is modulated with a period of about one year .The amplitude of the modulation is at least 50 % ( 3 sigma ) . This result suggests that the origin may be similar to other galactic sources which show proof for periodic variability due to accretion onto a neutron galaxy or black hole .Keywords : High energy astrophysics - Gamma rays - Black holes - Neutron stars - Accreting binaries - Pulsar wind nebulae - Inverse Compton scattering - Galactic center - Galaxy - Supernova remnants - Blazars - AGN - Cosmic rays - Fermi / LAT - TeV blazar - Variability - Periodicities - INTEGRAL - X - radiation - Hard X - radiation - Soft gamma - ray waves - Transient objects - Radio pulsar",
        "rewrite_text": "We present the findings from an assessment of measurements collected by the INTEGRAL satellite during 2003 and 2004. Our analysis indicates that the hard X-ray radiation from this source exhibits a modulation period of approximately one year, with an amplitude of at least 50% (3 sigma). This observation implies that its origin may resemble that of other galactic sources known to display periodic variability, likely due to accretion onto a neutron star or black hole. \n\nKeywords: High energy astrophysics, Gamma rays, Black holes, Neutron stars, Accreting binaries, Pulsar wind nebulae, Inverse Compton scattering, Galactic center, Galaxy, Supernova remnants, Blazars, AGN, Cosmic rays, Fermi/LAT, TeV blazar, Variability, Periodicities, INTEGRAL, X-ray radiation, Hard X-ray radiation, Soft gamma-ray emissions, Transient objects, Radio pulsar.",
        "ori-fast-z-score": 0.29488391230979427,
        "water-fast-z-score": 3.5386069477175313,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present fitting formulae for the illumination of accretion disks by hot spots, as seen in Schwarzschild and rotating black holes (Kerr). The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton scattering. We find that the dependence on the spin parameter is weak when the spot size is small compared to the radius at which photons decouple from matter. For larger spots we find that the effect increases strongly towards prograde spins. Our results can be used to estimate the effects of relativistic Doppler boosting and gravitational lensing on observed spectra. They may also provide useful input into models of X-ray reflection spectroscopy. \nIntroduction\n\nAccreting black holes produce bright emission lines in their X-ray spectrum due to reprocessing of hard X-rays emitted near the event horizon by cold material orbiting close to the equatorial plane. These features have been studied extensively over many years both observationally and theoretically (see Reynolds & Nowak 2003 , Done et al 2004 . In particular, they show strong red-shifts indicating that the emitting gas orbits rapidly around the black hole. This rapid rotation causes additional shifts in energy due to relativistic Doppler boosts and gravitational lensing. Relativistic effects become more important if the emitting region has a high degree of rotational support or is viewed nearly face-on. It is therefore necessary to take these effects into account when interpreting observations of such systems. \n\nIn this work we consider the case where the illuminating source is located above the disk surface but below its photosphere. Such sources include magnetic flares produced within the disk itself or active regions associated with the inner edge of the disk. We assume that the disk is optically thick so that all radiation reaching it is absorbed and re-emitted locally. We use Monte Carlo simulations to calculate the emergent flux from the disk under various assumptions about the geometry of the system.\n\nThe main goal of our study was to develop simple analytical expressions describing how the shape of the line profile depends on the properties of the system. To do this we performed extensive numerical calculations covering a wide range",
        "watermark_text": "We present fitting formulae for the illumination of accretion disks by hot spots , as shown in Schwarzschild and rotating black holes ( Kerr ) . The formulae are derived using ray tracing through the disk atmosphere with an approximate treatment of Compton absorption .We see that the dependence on the spin vector is weak when the spot size is tiny relative to the radius at which photons decouple from matter . For larger spots we find that the impact grows highly towards prograde spins .Our results can be used to estimate the effects of relativistic Doppler boosting and gravity lensing on measured spectra . They might additionally offer useful input into estimates of X - ray reflection spectroscopy .Introduction Accreting black holes create bright emission lines in their X - ray spectrum due to reprocessing of hard X - rays generated near the event horizon by cold matter orbiting close to the equatorial plane . These features have been studied frequently over numerous years both observationally and theoretically ( saw Reynolds & Nowak 2003 , Done et al 2004 .In particular , they show intense red - shifts suggesting that the emitting gas orbits rapidly around the dark hole . This rapid rotation creates additional shifts in energy due to relativistic Doppler boosts and gravity lensing .Relativistic effects become more essential if the emitting area has a high degree of rotational support or is viewed virtually face - on . It is consequently required to take these consequences into consideration when interpreting observations of such systems .In this research we imagine the case where the illuminating source is situated above the disk boundary but below its photosphere . Such sources include magnetic flares created within the disk itself or active regions associated with the inner boundary of the disk .We assume that the disk is optically dense so that all light reaching it is absorption and re - radiated locally . We use Monte Carlo simulations to estimate the emergent flux from the disk under various statements about the topology of the system .The main goal of our research was to develop primitive analytical expressions relating how the form of the line profile depends on the properties of the system . To do this we performed extensive numerical measurements encompassing a broad range",
        "rewrite_text": "We present fitting formulas for the illumination of accretion disks by hot spots, specifically in the context of Schwarzschild and rotating (Kerr) black holes. These formulas are derived through ray tracing in the disk atmosphere, incorporating an approximate treatment of Compton absorption. Our analysis reveals that the influence of the spin vector is minimal when the hot spot size is small in comparison to the radius at which photons decouple from matter. However, for larger spots, we observe a significant increase in impact, particularly with prograde spins. These findings can help estimate the effects of relativistic Doppler boosting and gravitational lensing on observed spectra, and may also provide valuable insights for estimating X-ray reflection spectroscopy.\n\nIntroduction: Accreting black holes produce bright emission lines in their X-ray spectra due to the reprocessing of hard X-rays generated near the event horizon by cold matter orbiting close to the equatorial plane. These features have been extensively studied both observationally and theoretically over the years (see Reynolds & Nowak, 2003; Done et al., 2004). Notably, intense redshifts are evident, indicating that the emitting gas orbits rapidly around the black hole. This rapid motion leads to additional energy shifts due to relativistic Doppler effects and gravitational lensing. Relativistic effects become increasingly significant when the emitting region has substantial rotational support or is viewed nearly face-on. Therefore, it is essential to consider these effects when interpreting observations of such systems.\n\nIn this study, we examine a scenario where the illuminating source is located above the disk's boundary yet beneath its photosphere. Potential sources include magnetic flares occurring within the disk or active regions at its inner boundary. We assume the disk is optically dense, meaning that all light reaching it is absorbed and re-radiated locally. To estimate the emergent flux from the disk under various conditions regarding the system's topology, we employ Monte Carlo simulations. Our primary goal was to develop basic analytical expressions that relate the form of the line profile to the system's properties. To achieve this, we conducted extensive numerical measurements across a wide range of parameters.",
        "ori-fast-z-score": -0.1543033499620919,
        "water-fast-z-score": 8.076923076923077,
        "rewrite-fast-z-score": 1.934558081335342
    },
    {
        "original_text": "The space elevator is an important project in the future space exploration, which can provide transportation between Earth orbit and ground with high efficiency. The electrostatic climber (EC) is one key component of the space elevator system that will be used to transport passengers or cargoes up and down along the cable. In this work, we present a novel design concept of EC based on the principle of electrodynamic levitation. A prototype has been built according to our proposed design. It consists of two main parts: the upper platform and lower platform. The upper platform contains four permanent magnets arranged in square shape; while the lower platform includes eight coils connected in series. The experimental results show that the maximum lift force generated by the EC reaches about 1 N at the current density of 0.5 A/mm2 . This value meets the requirement of the space elevator application. Moreover, it also shows good stability when the load changes.",
        "watermark_text": "The space lift is an important project in the future space exploration , which can provide transit between Earth orbit and ground with high efficiency . The electrostatic climber ( EC ) is one key portion of the spacecraft elevator system that will be used to transport passengers or cargoes up and down along the cable .In this project , we present a new design concept of EC based on the principle of electrodynamic levitation . A prototype has been built according to our proposed design .It consists of two principal portions : the higher platform and lower platform . The higher platform includes four permanent magnets grouped in square position ; while the lower platform includes eight coils connected in series .The empirical results show that the maximum lift pressure generated by the EC reaches about 1 N at the current density of 0 . 5 A / mm2 . This value meets the requirement of the space elevator application .Moreover , it also shows excellent stability when the load changes .",
        "rewrite_text": "The space elevator represents a significant advancement in future space exploration, facilitating efficient transportation between Earth's orbit and the surface. A crucial component of this system is the electrostatic climber (EC), designed to move passengers and cargo along the cable. In this project, we introduce an innovative design concept for the EC, utilizing the principle of electrodynamic levitation. We have developed a prototype based on our design, which consists of two main sections: the upper platform, equipped with four permanent magnets arranged in a square configuration, and the lower platform, which features eight series-connected coils. Our empirical findings indicate that the EC can generate a maximum lift force of approximately 1 N at a current density of 0.5 A/mm², meeting the requirements for space elevator applications. Additionally, the system demonstrates remarkable stability despite variations in load.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 3.988620176087328,
        "rewrite-fast-z-score": -0.48507125007266594
    },
    {
        "original_text": "We consider the problem of energy-efficient modulation for downlink transmissions over code-division multiple-access (CDMA) networks, where each user has an individual delay constraint and is equipped with a rechargeable battery that can be charged by harvesting ambient radio-frequency signals. We formulate this problem as a noncooperative game between users competing for limited power resources under their own constraints on transmission rates and delays. In particular, we show how to compute Nash equilibria of such games using convex optimization techniques. Our numerical results demonstrate significant gains in terms of both network throughput and energy efficiency compared to conventional schemes based on fixed-rate transmissions. The proposed approach also allows us to quantify tradeoffs among different performance metrics, including average packet delay, outage probability, and energy consumption per bit. This work was supported by NSF grants CNS-0932040 and CCF-0939370. \n \n Code available at http://arxiv.org/abs/1206.5481",
        "watermark_text": "We consider the issue of electricity - efficient modulation for downlink transmissions over code - division multiple - access ( CDMA ) networks , where each consumer has an individual delay constraint and is equipped with a rechargeable battery that can be charged by collecting ambient broadcast - frequency signals . We formulate this situation as a noncooperative contest between operators competing for limited power assets under their own restrictions on transmission rates and delays .In particular , we study how to compute Nash equilibria of such players utilizing convex optimization schemes . Our numerical findings show considerable gains in terms of both network throughput and energy efficiency compared to conventional systems relying on fixed - frequency transmissions .The proposed approach also enables us to quantify tradeoffs among different performance metrics , notably mean packet delay , outage likelihood , and energy consumption per bit . This effort was supported by NSF grants CNS - 0932040 and CCF - 0939370 .Code available at http : / / arxiv . org / abs / 1206 . 5481",
        "rewrite_text": "We examine the challenge of electricity-efficient modulation for downlink transmissions in code-division multiple-access (CDMA) networks, where each user faces a specific delay constraint and uses a rechargeable battery that can be charged by capturing ambient broadcast-frequency signals. This scenario is modeled as a noncooperative game among operators competing for limited power resources, each subject to their own limitations on transmission rates and delays. Specifically, we investigate how to determine the Nash equilibria of these players through convex optimization techniques. Our numerical results reveal significant improvements in both network throughput and energy efficiency compared to traditional systems that depend on fixed-frequency transmissions. Additionally, our approach allows us to evaluate the trade-offs between various performance metrics, including mean packet delay, outage probability, and energy consumption per bit. This research was supported by NSF grants CNS-0932040 and CCF-0939370. The code is available at http://arxiv.org/abs/1206.5481.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": -0.9058216273156765
    },
    {
        "original_text": "In this article, we provide an overview on the recent advances and challenges in energy-efficient resource allocation for wireless networks using game-theoretical approaches. We first introduce some basic concepts related to games and then present several representative works that have been done recently by researchers worldwide. Finally, we conclude with open research issues and future directions. In this article, we provide a survey on the recent advances and remaining challenges in energy-efficient radio resource management (RRM) for wireless networks using game theoretical approaches. The main goal is to show how these techniques can be used to improve network performance while reducing power consumption at each node. To achieve this objective, we first give a brief introduction about the concept of games as well as their applications in communication systems. Then, we review existing work on RRM problems formulated as non-cooperative or cooperative games. Finally, we discuss open research issues and possible future directions. This article is organized into four sections. Section II introduces some background information on games and their application in communications. Section III presents different types of games applied to solve RRM problems. Section IV discusses open research issues and concludes the article.",
        "watermark_text": "In this page , we provide an overview on the recent developments and challenges in electricity - efficient energy management for wireless networks employing game - theory techniques . We first introduce some fundamental concepts related to games and then present many representative works that have been performed recently by researchers worldwide .Finally , we conclude with open work issues and future directions . In this article , we provide a survey on the recent developments and remaining problems in energy - efficient radio resource control ( RRM ) for wireless networks employing game theory techniques .The main goal is to indicate how these tactics can be used to improve network reliability while reducing electricity usage at each node . To achieve this objective , we first offer a brief introduction about the idea of games as well as their implementation in transmission systems .Then , we review current work on RRM questions constructed as non - cooperative or cooperative games . Finally , we review open research concerns and possible future paths .This page is organized into four elements . Section II gives some background data on games and their application in communications .Section III presents different kinds of games applicable to solve RRM challenges . Section IV examines open scientific issues and concludes the article .",
        "rewrite_text": "This page offers an overview of the latest advancements and challenges in energy-efficient management for wireless networks utilizing game theory techniques. We begin by introducing key concepts related to games before showcasing several notable studies conducted by researchers worldwide. We conclude with a discussion of unresolved issues and potential future directions. This article serves as a survey of recent progress and ongoing challenges in energy-efficient radio resource management (RRM) for wireless networks using game theory methods. The primary aim is to illustrate how these strategies can enhance network reliability while minimizing power consumption at each node. To accomplish this, we first provide a brief introduction to the principles of games and their application in transmission systems. Next, we explore current research on RRM problems framed as either non-cooperative or cooperative games. Finally, we address open research questions and outline possible future avenues. The structure of this page includes four main sections: Section II offers background information on games and their relevance to communications, Section III discusses various types of games that can address RRM challenges, Section IV reviews outstanding scientific issues, and the article concludes thereafter.",
        "ori-fast-z-score": -0.8250286473253902,
        "water-fast-z-score": 7.058578427117228,
        "rewrite-fast-z-score": 1.889822365046136
    },
    {
        "original_text": "We present new high resolution observations of the interstellar medium in the direction of the North Ecliptic Pole, made with the Westerbork Synthesis Radio Telescope at 1.4 GHz. The data reveal an extended filamentary structure that is traced by neutral hydrogen emission lines as well as continuum emission associated with free-free processes. We find evidence for two distinct components to this filamentary structure; one component has a relatively low column density but extends over several degrees on the sky while another component appears more compact and denser. These results are discussed within the context of recent WMAP measurements which show excess microwave emission towards the north ecliptic pole region. This work was supported by NASA grant NAG5-10842. Keywords: ISM, radio astronomy, H I 21 cm line, WMAP, filaments, North Ecliptic Pole Region . \nIntroduction\n\nThe Wilkinson Microwave Anisotropy Probe (WMAP) (Bennett et al., 2003a ) measured significant excesses of microwave emission above the expected cosmic background radiation level along three different lines-of-sight through the northern hemisphere. In particular, there were large excesses observed near the North Ecliptic Poles (NEPs). Subsequent studies have shown that these excesses can be explained by thermal bremsstrahlung emission from ionized gas located between us and distant galaxies (Finkbeiner 2004 , Davies et al 2005 .\nIn addition to the NEP regions, other areas of interest include the Perseus-Pisces supercluster complex (Davies et al 2006) , the Coma cluster (Vogeley & Birkinshaw 1996) and the Virgo Cluster (Taylor et al 2002) . All of these structures contain substantial amounts of hot plasma and it seems likely that they will also contribute significantly to the total foreground signal detected by WMAP. \nObservations of the diffuse galactic radio emission provide important information about the physical conditions in the interstellar medium (ISM), such as temperature, pressure and magnetic field strength. However, due to its faintness relative to point sources, only recently have we",
        "watermark_text": "We report new high resolution measurements of the interstellar material in the direction of the North Ecliptic Pole , made with the Westerbork Synthesis Radio Telescope at 1 . 4 GHz . The data reveal an extended filamentary composition that is traced by neutral hydrogen emission lines as well as continuum emission associated with free - free processes .We get data for two separate constituents to this filamentary composition ; one part has a fairly lowest column thickness but spreads over numerous degrees on the sky while another component appears more compact and denser . These conclusions are discussed within the context of recent WMAP measurements which show additional microwave emission towards the north ecliptic pole region .This project was supported by NASA loan NAG5 - 10842 . Keywords : ISM , radio astronomy , H I 21 cm line , WMAP , filaments , North Ecliptic Pole Region .Introduction The Wilkinson Microwave Anisotropy Probe ( WMAP ) ( Bennett et al . , 2003a ) measured significant excesses of microwave emission above the expected cosmic background radiation level along three different lines - of - view through the northern hemisphere . In particular , there were large excesses observed near the North Ecliptic Poles ( NEPs ) .Subsequent researchers have shown that these excesses can be explained by thermal bremsstrahlung emission from ionized gas located between us and distant galaxies ( Finkbeiner 2004 , Davies et al 2005 . In addition to the NEP regions , other areas of focus involve the Perseus - Pisces supercluster complex ( Davies et al 2006 ) , the Coma cluster ( Vogeley & Birkinshaw 1996 ) and the Virgo Cluster ( Taylor et al 2002 ) .All of these structures hold substantial deposits of hard gas and it appears probably that they will also contribute considerably to the total foreground light detected by WMAP . Observations of the diffuse galactic radio emission reveal important information about the physical conditions in the interstellar medium ( ISM ) , such as temperature , pressure and magnetic field intensity .However , owing to its faintness relative to point sources , only lately have we",
        "rewrite_text": "We present new high-resolution observations of interstellar material in the direction of the North Ecliptic Pole, conducted using the Westerbork Synthesis Radio Telescope at a frequency of 1.4 GHz. The data reveal a complex filamentary structure characterized by neutral hydrogen emission lines, as well as continuum emission linked to free-free processes. We identify two distinct components within this filamentary structure: one part exhibits a relatively low column density but stretches over several degrees across the sky, while the other component appears more compact and dense. These findings are analyzed in relation to recent WMAP measurements, which indicate additional microwave emissions in the North Ecliptic Pole region. This research was supported by NASA through loan NAG5-10842. \n\nKeywords: ISM, radio astronomy, H I 21 cm line, WMAP, filaments, North Ecliptic Pole Region. \n\nIntroduction: The Wilkinson Microwave Anisotropy Probe (WMAP) (Bennett et al., 2003a) detected significant excesses of microwave emission that surpassed the expected levels of cosmic background radiation along three distinct lines of sight in the Northern Hemisphere. Notably, substantial excesses were observed near the North Ecliptic Poles (NEPs). Follow-up studies have suggested that these excesses can be attributed to thermal bremsstrahlung emissions from ionized gas situated between us and more distant galaxies (Finkbeiner 2004, Davies et al. 2005). In addition to the NEP regions, other areas of interest include the Perseus-Pisces supercluster complex (Davies et al. 2006), the Coma cluster (Vogeley & Birkinshaw 1996), and the Virgo Cluster (Taylor et al. 2002). Each of these structures contains substantial amounts of hot gas, and it is likely that they contribute significantly to the overall foreground emissions detected by WMAP. Observations of diffuse galactic radio emission provide crucial insights into the physical conditions within the interstellar medium (ISM), including temperature, pressure, and magnetic field strength. However, due to its faintness compared to point sources, observations of the ISM have only recently become feasible.",
        "ori-fast-z-score": 1.4852968963237645,
        "water-fast-z-score": 6.850168759240548,
        "rewrite-fast-z-score": 1.0366421106976322
    },
    {
        "original_text": "We present an approach to the analysis of metabolic networks based on information-theoretic concepts, in particular Shannon s entropy measure. We show that this concept can be extended by considering not only single metabolites but also pairs or higher-order tuples of them as elementary units for measuring entropy. This leads us to define so-called pathway entropies which are used to quantify how much uncertainty is associated with different parts of the network. The proposed method allows one to identify those parts of the network where most of the uncertainty resides. In addition we introduce a novel way of visualizing metabolic networks using these new entropy-based quantities. Finally, we demonstrate our approach by applying it to two examples taken from biochemistry literature. Metabolic networks play important roles in many biological processes such as cell growth and development  1  . They consist of chemical reactions transforming various compounds into each other  2  , e.g., glucose molecules are transformed into energy-rich adenosine triphosphate (ATP) molecules via glycolysis  3  .\nThe study of metabolic networks has been attracting increasing interest over recent years  4  -  8  . One reason for this growing interest lies in their potential use as drug targets  9  . Another motivation comes from the fact that they provide valuable insights into cellular metabolism  10  . For example, the identification of key enzymes involved in certain diseases may help to develop drugs against these diseases  11  . Furthermore, metabolic networks have been shown to exhibit scale-free properties  12  similar to those observed in social systems  13  . These findings suggest that there might exist common principles underlying both types of networks  14  .\nIn order to understand the functioning of metabolic networks better, several mathematical models have been developed  15  -  17  . Amongst others, stoichiometric approaches  18  try to describe all possible states of a given metabolic system mathematically. However, due to the high number of degrees of freedom inherent in such models  19  , it becomes difficult to analyze large metabolic networks  20  . Therefore, alternative methods have been suggested  21  -  23  .",
        "watermark_text": "We present an perspective to the analysis of biological groups based on knowledge - theoretic concepts , in example Shannon s entropy measure . We see that this concept can be generalized by treating not only single metabolites but also pairs or greater - order tuples of them as elementary units for determining entropy .This leads us to define so - called pathway entropies which are using to quantify how many uncertainty is associated with various parts of the network . The proposed approach allows one to identify those parts of the network where most of the doubt lies .In addition we provide a new method of visualizing molecular connections utilizing these new entropy - based quantities . Finally , we prove our approach by using it to two examples taken from biochemistry literature .Metabolic systems play essential roles in different biological pathways such as cell development and growth 1 . They involve of chemical processes transforming various compounds into each other 2 , e . g . , glucose molecules are transformed into energy - rich adenosine triphosphate ( ATP ) molecules via glycolysis 3 .The investigation of biological groups has been drawing rising interest over recent periods 4 - 8 . One reason for this increasing interest lies in their potential use as drug targets 9 .Another motivation comes from the fact that they give valuable insights into cellular metabolism 10 . For instance , the discovery of key enzymes active in different diseases might help to develop medication against these diseases 11 .Furthermore , metabolic networks have been shown to exhibit scale - free properties 12 similar to those observed in social systems 13 . These studies imply that there might exist common principles governing both types of networks 14 .In order to explain the structures of metabolic networks better , various computational models have been created 15 - 17 . Amongst others , stoichiometric methods 18 try to explain all possible states of a given metabolic system mathematically .However , owing to the high number of degrees of autonomy inherent in such studies 19 , it becomes impossible to analyze large metabolic networks 20 . Therefore , alternative techniques have been proposed 21 - 23 .",
        "rewrite_text": "We propose a novel perspective for analyzing biological groups using knowledge-theoretic concepts, specifically employing Shannon's entropy measure as an example. We demonstrate that this concept can be extended by considering not only individual metabolites but also pairs or larger tuples of metabolites as fundamental units for entropy calculation. This leads to the definition of \"pathway entropies,\" which quantify the uncertainty associated with different segments of the network. Our approach enables the identification of the areas within the network that harbor the most ambiguity. Additionally, we introduce a new visualization method for molecular connections that utilizes these entropy-based quantities. To validate our approach, we apply it to two case studies from the biochemistry literature. Metabolic systems are crucial for various biological pathways, including cell development and growth, as they involve chemical processes that convert different compounds into one another; for example, glucose is transformed into energy-rich adenosine triphosphate (ATP) through glycolysis. The exploration of biological groups has gained significant interest recently, as their potential as drug targets is becoming increasingly recognized. Moreover, studying these groups provides valuable insights into cellular metabolism; for example, identifying key enzymes implicated in various diseases can aid in the development of targeted treatments. Furthermore, metabolic networks have been shown to exhibit scale-free properties akin to those in social systems, suggesting that common principles may govern both types of networks. To better understand the structure of metabolic networks, several computational models have been developed, including stoichiometric methods that mathematically describe all potential states of a specific metabolic system. However, due to the complex degrees of autonomy involved, analyzing large metabolic networks can be challenging, prompting the proposal of alternative techniques.",
        "ori-fast-z-score": -0.909717652294684,
        "water-fast-z-score": 8.743828992755144,
        "rewrite-fast-z-score": 1.784435632438388
    },
    {
        "original_text": "We study the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations, using a multi-fluid model for ions and electrons. We find that the growth rates are strongly dependent on the angle between the wavevector k and the mean magnetic field B 0 . In particular, we show that there is an instability at oblique angles with respect to B 0 , which has been previously overlooked by previous studies based on single-fluid models. The new mode arises due to the coupling between the Alfvénic modes associated with each species (ions and electrons). This mode can be excited even when the electron temperature anisotropy T e? /T ez < 1, where ? denotes directions perpendicular to B 0 .\nThe results presented here may have important implications for understanding the origin of solar radio bursts observed during solar flares. Introduction: Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun s corona into interplanetary space. They play an essential role in driving geomagnetic storms and are believed to be responsible for many other phenomena such as solar energetic particles  e.g., Reames et al. (1998) , Kahler & Ragot (2007)  , solar radio bursts  e.g., Aschwanden (2004)  , and white-light flares  e.g., Benz (2008)  . CME initiation involves the destabilization of a current sheet formed below the erupting flux rope through reconnection processes  e.g., Forbes & Priest (1995) ; Lin & Forbes (2000); Aulanier et al. (2010)  . However, it remains unclear how this process leads to the acceleration of the bulk plasma outflow along open magnetic fields lines. Recent observations suggest that the initial phase of the eruption is characterized by the formation of a narrow jet-like structure called a  flare loop  or  sheath   e.g., Liu et al. (2009a Liu et al. ( , 2009b ; Cheng et al. (2011); Jiang et al. (2012",
        "watermark_text": "We research the linear stability properties of coronal beams in the presence of background plasma and magnetic force fluctuations , using a multi - fluid model for ions and electrons . We see that the development rates are strongly dependent on the angle between the wavevector k and the mean magnetic force B 0 .In particular , we prove that there is an instability at oblique directions with regard to B 0 , which has been previously overlooked by earlier studies relying on single - fluid models . The new mode occurs due to the interaction between the Alfvénic configurations involved with each species ( atoms and electrons ) .This mode can be excited even when the electron thermal anisotropy T e ? / T ez < 1 , where ?denotes directions perpendicular to B 0 . The results presented here possibly have important implications for studying the origin of solar radio flashes seen during thermal flares .Introduction : Coronal mass ejections ( CMEs ) are big - scale expulsions of magnetized plasma from the Sun s corona into interplanetary space . They play an essential part in causing geomagnetic winds and are considered to be responsible for numerous other processes such as solar energetic particles e . g . , Reames et al .( 1998 ) , Kahler & Ragot ( 2007 ) , sun wireless flare e . g . , Aschwanden ( 2004 ) , and white - light flares e . g . , Benz ( 2008 ) . CME initiation consists the destabilization of a current sheet formed below the erupting flux rope through reconnection pathways e . g . , Forbes & Priest ( 1995 ) ; Lin & Forbes ( 2000 ) ; Aulanier et al .( 2010 ) . However , it remains unsure how this process results to the acceleration of the bulk plasma outflow along open magnetic fields lines .Recent measurements suggest that the first phase of the volcano is characterized by the formation of a thin jet - like structure named a flare loop or sheath e . g . , Liu et al . ( 2009a Liu et al .( , 2009b ; Cheng et al . ( 2011 ) ; Jiang et al .(2012",
        "rewrite_text": "We investigate the linear stability characteristics of coronal beams in the presence of fluctuations in background plasma and magnetic forces, utilizing a multi-fluid model for ions and electrons. Our findings indicate that the growth rates are significantly influenced by the angle between the wavevector \\( k \\) and the mean magnetic field \\( B_0 \\). Notably, we demonstrate that an instability arises at oblique angles relative to \\( B_0 \\), a phenomenon that previous studies employing single-fluid models failed to address. This newly identified mode results from the interaction between Alfvénic configurations associated with both atoms and electrons. Remarkably, this mode can be excited even when the electron thermal anisotropy \\( T_e^\\perp / T_e^z < 1 \\), where \\( \\perp \\) refers to directions perpendicular to \\( B_0 \\). The findings presented have significant implications for understanding the origins of solar radio bursts observed during thermal flares. \n\n**Introduction:** Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun's corona into interplanetary space. They play a crucial role in generating geomagnetic winds and contribute to various processes, including solar energetic particles (Reames et al., 1998; Kahler & Ragot, 2007), solar radio flares (Aschwanden, 2004), and white-light flares (Benz, 2008). The initiation of CMEs involves the destabilization of a current sheet formed beneath the erupting flux rope through reconnection pathways (Forbes & Priest, 1995; Lin & Forbes, 2000; Aulanier et al., 2010). However, the mechanism by which this process leads to the acceleration of bulk plasma outflows along open magnetic field lines remains uncertain. Recent measurements indicate that the initial phase of a CME is characterized by the formation of a thin jet-like structure known as a flare loop or sheath (Liu et al., 2009a; Liu et al., 2009b; Cheng et al., 2011; Jiang et al., 2012).",
        "ori-fast-z-score": -1.007017629956027,
        "water-fast-z-score": 5.202924421439472,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "The influence of pulsed magnetic fields on the relaxation processes in HTSC was investigated by measuring the temperature dependence of resistance and Hall coefficient for samples with different oxygen content (d = 0, 1). The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0. This effect is explained as due to the appearance of additional scattering centers caused by defects formed during the process of magnetization reversal. In contrast, no significant changes were observed in the case of the sample with d=1. It can be assumed that this difference is associated with the presence of structural disordering in the crystal lattice of the latter compound. \n \n Keywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defects Formation, Magnetoresistance, Hall Effect. Introduction \n \n Investigation of relaxation phenomena in high temperature superconductors under the action of pulsed external magnetic fields has been attracting considerable attention recently  1 - 5  . These studies are important both for understanding the physics of these materials and for practical applications  6  -  8  . \n \n In particular, it should be noted that the investigation of relaxation processes in HTSCs allows one to study the dynamics of defect formation  9  , which plays an important role in determining their transport properties  10  . At present there are several models describing the mechanism of defect generation  11  -  13  . However, none of them takes into account the possibility of defect formation induced by the action of pulsed fields  14  . \nExperimental details\n\nIn our work we used single crystals of two compounds with different oxygen content: HoBa 2 Cu 3 O 7−δ (HBS) and YBa 2 Cu 3 O 6+δ (YBS), grown using the floating zone method  15  . The oxygen concentration in the samples was determined by iodometric titration  16  . The typical size of the samples was about 5 × 4 mm 2 . The measurements were carried out in liquid helium cryostats equipped with pulse magnets  17  . The maximum value of the magnetic induction reached up to B max =",
        "watermark_text": "The impact of pulsed magnetic fields on the relaxation processes in HTSC was investigated by monitoring the temperature dependence of resistance and Hall coefficient for specimens with various oxygen composition ( d = 0 , 1 ) . The results show that the introduction of pulsed magnetic fields leads to an increase in the resistivity and Hall velocity of the sample with d = 0 .This phenomenon is understood as owing to the appearance of added scattering centers caused by defects formed during the process of magnetization reversal . In comparison , no major changes were detected in the case of the sample with d = 1 .It can be assumed that this contrast is associated with the presence of structural disordering in the crystal structures of the latter chemical . Keywords : High - Tc Superconductor , Pulsed Magnetic Field , Relaxation Processes , Defects Formation , Magnetoresistance , Hall Effect .Introduction Investigation of relaxation effects in high heat superconductors under the action of pulsed external magnetic waves has been drawing greater notice lately 1 - 5 . These studies are important both for studying the physics of these structures and for useful use 6 - 8 .In particular , it should be mentioned that the examination of relaxation processes in HTSCs allows one to study the dynamics of defect form 9 , which plays an important role in establishing their transport properties 10 . At currently there are several models explaining the process of defect generation 11 - 13 .However , none of them took into consideration the danger of defect development caused by the activity of pulsed fields 14 . Experimental details In our work we using single crystals of two compounds with varying oxygen composition : HoBa 2 Cu 3 O 7−δ ( HBS ) and YBa 2 Cu 3 O 6 + δ ( YBS ) , grown using the sliding zone method 15 .The oxygen composition in the tests was calculated by iodometric titration 16 . The typical size of the samples was about 5 × 4 mm 2 .The measurements were carried out in pure helium cryostats fitted with pulse magnets 17 . The maximum value of the magnetic induction reached up to B max =",
        "rewrite_text": "The effects of pulsed magnetic fields on the relaxation processes in high-temperature superconductors (HTSC) were examined by observing the temperature dependence of resistance and Hall coefficient in samples with different oxygen compositions (d = 0, 1). The findings indicate that applying pulsed magnetic fields results in a rise in the resistivity and Hall velocity of the sample with d = 0. This behavior is attributed to the introduction of additional scattering centers due to defects that emerge during magnetization reversal. In contrast, the sample with d = 1 showed no significant changes, suggesting that this discrepancy may be linked to structural disorder within the crystal structure of that particular compound. \n\nKeywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defect Formation, Magnetoresistance, Hall Effect.\n\n**Introduction**\n\nRecently, there has been increased interest in investigating relaxation effects in high-temperature superconductors when exposed to pulsed external magnetic fields. These studies are crucial not only for understanding the physics of these materials but also for their practical applications. Notably, examining relaxation processes in HTSCs sheds light on defect dynamics, which significantly influence their transport properties. Currently, various models exist to explain defect generation; however, none have addressed the risks associated with defect formation induced by pulsed fields.\n\n**Experimental Details**\n\nIn this study, we utilized single crystals of two compounds with differing oxygen compositions: HoBa₂Cu₃O₇−δ (HBS) and YBa₂Cu₃O₆+δ (YBS), which were grown using the sliding zone technique. The oxygen content was determined through iodometric titration. The samples measured approximately 5 × 4 mm², and measurements were performed in a pure helium cryostat equipped with pulse magnets, achieving a maximum magnetic induction of B_max = ",
        "ori-fast-z-score": -1.1832159566199232,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": 0.17677669529663687
    },
    {
        "original_text": "We present an approach to primordial evolution based on the finitary process soup (FPS) model, which is a generalization of the standard quantum field theory. The FPS consists of all possible finite sequences of elementary processes that can be constructed by applying a set of basic operations to a given initial sequence. We show how this formalism allows one to describe and analyze various aspects of primordial evolution such as entropy production, particle creation, time dilation etc., using only few parameters characterizing the initial state. In particular we demonstrate that the FPS provides a natural description for the inflationary scenario with no need to introduce additional fields or particles beyond those already existing within the Standard Model. Finally, we discuss some open problems related to our approach. PACS numbers: 04.60.Kz, 11.10.Wx, 12.20.Ds, 98.80.Cq . \nI. INTRODUCTORY REMARkS\n\nThe idea behind the finitary process soup  1  , also known as the  quantum soup   2  , is very simple - it represents any physical system as a collection of all its possible states. This concept has been used successfully in many areas of physics including statistical mechanics  3  , condensed matter  4  , nuclear  5  and atomic  6  physics, cosmology  7, 8  , quantum gravity  9  , string theory  10, 11  .\nIn this work we apply the FPS formalism to study primordial evolution during the early stages of the universe s expansion. Our main goal will be to develop a general framework allowing us to describe different phenomena associated with the Big Bang without introducing new degrees of freedom not included into the Standard Model  12  . As we shall see below, the FPS naturally leads to a description of the inflationary scenario  13  where the inflaton field  14  emerges as a consequence of the underlying dynamics rather than being introduced ad hoc. \nII. THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION A. General Description\nLet us start by briefly reviewing the key features of the FPS formalism",
        "watermark_text": "We present an view to primordial evolution based on the finitary process soup ( FPS ) model , which is a generalization of the standard quantum field model . The FPS consists of all possible finite sequences of elementary processes that can be built by using a setting of fundamental operations to a given original sequence .We illustrate how this formalism allows one to explain and analyze numerous elements of primordial development such as entropy production , particle creation , time dilation etc . , using only few parameters characterizing the first state . In particular we prove that the FPS provides a natural characterization for the inflationary scenario with no requirement to introduce extra fields or particles beyond those already emerging within the Standard Model .Finally , we explain some open problems related to our approach . PACS scores : 04 . 60 . Kz , 11 . 10 . Wx , 12 . 20 . Ds , 98 . 80 . Cq .I . INTRODUCTORY REMARkS The idea behind the finitary process soup 1 , sometimes called as the quantum soup 2 , is very simple - it represents any physical system as a collection of all its potential states .This concept has been used successfully in multiple fields of science including statistical mechanics 3 , condensed matter 4 , nuclear 5 and atomic 6 theory , cosmology 7 , 8 , quantum gravitational 9 , string theory 10 , 11 . In this research we apply the FPS formalism to study primordial development during the early stages of the universe s evolution .Our main goal will be to develop a general template allowing us to explain different processes associated with the Big Bang without using new degrees of liberty not incorporated into the Standard Model 12 . As we shall get below , the FPS naturally comes to a description of the inflationary scenario 13 where the inflaton field 14 emerges as a consequence of the fundamental dynamics rather than being established ad hoc .II.THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION A.General Description Let us begin by briefly examining the key features of the FPS formalism",
        "rewrite_text": "We introduce a perspective on primordial evolution informed by the finitary process soup (FPS) model, which extends the conventional quantum field model. The FPS encompasses all conceivable finite sequences of elementary processes derived from a set of fundamental operations applied to an initial sequence. This framework enables the explanation and analysis of various aspects of primordial development, including entropy production, particle creation, and time dilation, using only a limited number of parameters that characterize the initial state. Notably, we demonstrate that the FPS naturally aligns with the inflationary scenario, eliminating the need for additional fields or particles outside those present in the Standard Model. Lastly, we address some unresolved issues related to our approach. PACS codes: 04.60.Kz, 11.10.Wx, 12.20.Ds, 98.80.Cq.\n\nI. INTRODUCTORY REMARKS\n\nThe concept of the finitary process soup, occasionally referred to as the quantum soup, is straightforward—it models any physical system as a collection of all its potential states. This idea has proven effective across multiple scientific domains, including statistical mechanics, condensed matter physics, nuclear and atomic theory, cosmology, quantum gravity, and string theory. In this study, we apply the FPS framework to investigate primordial development during the universe's early evolution. Our primary objective is to create a comprehensive template that can elucidate various processes linked to the Big Bang without introducing new degrees of freedom outside the Standard Model. As we will elaborate, the FPS naturally leads to a description of the inflationary scenario, where the inflaton field emerges from fundamental dynamics rather than being introduced arbitrarily.\n\nII. THE FINITARY PROCESS SOUP MODEL AND ITS APPLICATION TO PRIMORDIAL EVOLUTION\n\nA. General Description\n\nWe begin by providing a brief overview of the key features of the FPS formalism.",
        "ori-fast-z-score": 0.6069769786668839,
        "water-fast-z-score": 7.7748158302322405,
        "rewrite-fast-z-score": 2.057182539299806
    },
    {
        "original_text": "We present the results on the metallicity distribution functions (MDFs) for stars at different latitudes and distances from the Galactic plane, based on spectroscopic data obtained by the Sloan Digital Sky Survey (SDSS). We find that MDFs are similar to each other within errors except those at |b| > 30° where there is an excess of metal-poor stars compared to the disk population. The fraction of metal-poor stars increases towards higher latitude. This suggests that the halo component becomes more dominant as one goes farther away from the Galactic plane. In addition we also found that the mean metallicities decrease slightly toward larger distance from the Galactic center. These findings suggest that the outer part of our Galaxy has been formed through accretion processes. \n \n Keywords: Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey \n \n 1 Introduction \n \n It is well known that the Milky Way consists of three main components -the thin disk, thick disk and halo. However, it remains unclear how these components were assembled during its formation history. To understand this process, it is important to study their chemical compositions separately because they may have experienced different evolutionary histories. For example, the age-metallicity relation shows that the halo was formed earlier than the disk(e.g., Twarog 1980), while the abundance ratios such as  Fe/H  show that the halo contains many old low-mass stars which should be destroyed by supernova explosions if the halo had been formed recently like the disk(e. g., Nissen & Schuster 1997). \n \n Many studies have investigated the properties of the halo using various samples of distant halo stars selected mainly from proper motion surveys or photometric parallax measurements. Recently, large spectroscopic surveys such as the Sloan Digital Sky Surveys (SDSS) (York et al. 2000) , RAVE survey (Steinmetz 2003 )and SEGUE survey (Yanny et al. 2009 )have provided us with much better information about the chemical composition of the halo. Using",
        "watermark_text": "We present the conclusion on the metallicity distribution functions ( MDFs ) for stars at different latitudes and distances from the Galactic plane , using on spectroscopic data derived by the Sloan Digital Sky Survey ( SDSS ) . We see that MDFs are comparable to each other within errors except those at | b | > 30° where there is an accumulation of steel - weak stars compared to the disk community .The percentage of metal - low stars increases towards higher latitude . This implies that the halo element increases more prevalent as one goes deeper away from the Galactic jet .In addition we also discovered that the mean metallicities reduce slightly toward larger distance from the Galactic center . These studies imply that the exterior part of our Galaxy has been formed through accretion cycles .Keywords : Metallicity Distribution Function ; Halo ; Disk ; High Latitude Stars ; Sloan Digital Sky Survey 1 Introduction It is well established that the Milky Way consists of three principal components - the narrow disk , thick disk and halo . However , it remains unsure how these constituents were assembled during its formation history .To understand this process , it is important to study their chemical compositions separately because they may have experienced distinct evolutionary histories . For instance , the age - metallicity relation shows that the halo was formed earlier than the disk ( e . g . , Twarog 1980 ) , while the density proportions such as Fe / H indicate that the halo contains much young high - density stars which should be killed by supernova explosions if the halo had been formed recently like the disk ( e . g . , Nissen & Schuster 1997 ) .Many experiments have explored the properties of the halo utilizing diverse samples of distant halo stars selected mainly from proper motion surveys or photometric parallax observations . Recently , large spectroscopic studies such as the Sloan Digital Sky Surveys ( SDSS ) ( York et al .2000 ) , RAVE study ( Steinmetz 2003 ) and SEGUE study ( Yanny et al . 2009 ) have provided us with far better details about the chemical composition of the halo .Using",
        "rewrite_text": "We present our findings on the metallicity distribution functions (MDFs) for stars located at various latitudes and distances from the Galactic plane, based on spectroscopic data obtained from the Sloan Digital Sky Survey (SDSS). Our analysis shows that the MDFs are generally similar within the margin of errors, with the exception of those at |b| > 30°, where there is a noticeable increase in metal-poor stars compared to the disk population. The percentage of metal-poor stars rises with higher latitudes, suggesting that halo stellar populations become more dominant as one moves further from the Galactic plane. Additionally, we observed a slight decrease in mean metallicity with increasing distance from the Galactic center. These findings imply that the outer regions of our Galaxy were formed through cycles of accretion.\n\nKeywords: Metallicity Distribution Function; Halo; Disk; High Latitude Stars; Sloan Digital Sky Survey\n\n1. Introduction \nIt is well recognized that the Milky Way comprises three main components: the thin disk, thick disk, and halo. However, the details of how these structures were formed during the Galaxy's history remain uncertain. To unravel this process, it is essential to examine their chemical compositions separately, as they may have undergone different evolutionary paths. For example, the age-metallicity relation indicates that the halo formed prior to the disk (e.g., Twarog 1980), while density ratios such as [Fe/H] suggest that the halo contains a considerable number of young, high-density stars that would have been eliminated by supernova explosions had the halo formed recently like the disk (e.g., Nissen & Schuster 1997). Numerous studies have investigated halo properties using various samples of distant halo stars, primarily sourced from proper motion surveys or photometric parallax observations. Recently, extensive spectroscopic surveys, including the Sloan Digital Sky Survey (SDSS) (York et al. 2000), RAVE (Steinmetz 2003), and SEGUE (Yanny et al. 2009), have significantly enhanced our understanding of the halo's chemical composition.",
        "ori-fast-z-score": -0.26013299085723596,
        "water-fast-z-score": 8.419756985347481,
        "rewrite-fast-z-score": -0.25819888974716115
    },
    {
        "original_text": "In this work, we propose an algorithm for blind identification (BI) of distributed antenna systems (DASs). The proposed BI method is based on the joint use of second-order statistics and higher order cumulants to estimate the number of active users in each cell as well as their carrier frequency offsets (CFOs), which are unknown parameters that need to be estimated before data detection can take place. We show by simulation results that our proposed method outperforms existing methods in terms of bit error rate performance when CFOs exist between different cells. In addition, it has lower computational complexity than other algorithms. \n \n Keywords: Blind identification; Distributed antenna systems; Second-order statistics; Higher order cumulants; CFO estimation. 1 Introduction \n \n With the rapid development of wireless communication technology, there have been increasing demands for high spectral efficiency and reliable transmission over limited bandwidth resources  1  . To meet these requirements, multi-antenna techniques such as multiple-input-multiple-output (MIMO)  2  , massive MIMO  3  -  5  , cooperative relaying  6  , and cognitive radio  7  have attracted much attention recently. Among them, distributed antenna systems (DAs)  8  -  10  provide significant advantages including improved coverage area, enhanced capacity, reduced power consumption, and increased network flexibility  11  . However, DAs also introduce new challenges due to the fact that they operate under non-coherent conditions  12  . For example, the channel state information (CSI) at the transmitter side cannot be obtained directly through uplink training or downlink feedback  13  . Therefore, how to obtain CSI accurately becomes one of the most important issues in DA design  14  .\n \nTo address this issue, several works  15  -  17  have investigated the problem of estimating the number of active users and their corresponding channels simultaneously using only statistical properties of received signals without requiring any prior knowledge about the transmitted symbols. These approaches exploit the inherent sparseness property of user activity patterns and utilize second-order statistics (SOS) and/or higher order cumulants (HOCs)  18  -  20  to identify the number of active users per cell. Then, the channel coefficients associated with",
        "watermark_text": "In this research , we propose an algorithm for blind analysis ( BI ) of distributed antenna devices ( DASs ) . The proposed BI approach is based on the joint use of second - order statistics and larger class cumulants to estimate the quantity of active consumers in each cell as also as their carrier signal offsets ( CFOs ) , which are unknown parameters that require to be assessed before data diagnosis can take occur .We suggest by simulation data that our proposed method outperforms current methods in terms of bit error rate capacity when CFOs arise between multiple cells . In addition , it has less computational complexity than other methods .Keywords : Blind identity ; Distributed antenna devices ; Second - order analysis ; Higher class cumulants ; CFO estimation . 1 Introduction With the increasing growth of radio communication techniques , there have been growing requirements for high spectral capacity and reliable transmission over limited bandwidth supplies 1 .To address these requirements , multi - antenna techniques such as multiple - input - multiple - output ( MIMO ) 2 , large MIMO 3 - 5 , joint relaying 6 , and cognitive television 7 have garnered considerable notice lately . Among them , dispersed antenna technologies ( DAs ) 8 - 10 provide significant advantages including increased coverage space , enhanced capacity , enhanced capacity consumption , and increased communication flexibility 11 .However , DAs additionally introduce novel challenges related to the fact that they operate under non - coherent environments 12 . For instance , the channel state information ( CSI ) at the broadcasting side cannot be obtained immediately through uplink conditioning or downlink feedback 13 .Therefore , how to obtain CSI correctly becomes one of the most important problems in DA design 14 . To address this question , various papers 15 - 17 have researched the issue of estimating the total of active participants and their corresponding networks simultaneously employing only statistical characteristics of received transmissions without using any earlier knowledge about the received symbols .These approaches exploit the intrinsic sparseness property of customer activity patterns and use second - order statistics ( SOS ) and / or greater order cumulants ( HOCs ) 18 - 20 to identify the total of active participants per cell . Then , the channel coefficients identified with",
        "rewrite_text": "In this study, we introduce an algorithm for blind analysis (BI) of distributed antenna systems (DASs). Our BI approach leverages both second-order statistics and higher-order cumulants to estimate the number of active users in each cell, as well as their carrier frequency offsets (CFOs), which are unknown parameters that must be determined before data analysis can proceed. Simulation results indicate that our proposed method outperforms existing techniques in terms of bit error rate capacity, particularly when CFOs vary across multiple cells. Furthermore, it demonstrates lower computational complexity compared to alternative methods. \n\n**Keywords**: Blind analysis; Distributed antenna systems; Second-order statistics; Higher-order cumulants; CFO estimation. \n\n**1 Introduction**  \nAs radio communication technologies expand, there is an increasing demand for high spectral efficiency and reliable transmission over limited bandwidth resources. To meet these demands, multi-antenna techniques—such as multiple-input multiple-output (MIMO), large MIMO, joint relaying, and cognitive television—have gained significant attention. Among these, distributed antenna systems (DAS) offer considerable benefits, including extended coverage, improved capacity, efficient resource utilization, and increased communication flexibility. However, DAS also present new challenges, particularly as they function in non-coherent environments. For example, channel state information (CSI) cannot be directly obtained through uplink training or downlink feedback, making accurate CSI estimation a critical issue in DAS design. Various studies have addressed the challenge of estimating the number of active users and their corresponding networks by relying solely on the statistical characteristics of received signals, without any prior knowledge of the received symbols. These methods utilize the intrinsic sparsity of user activity patterns and employ second-order statistics and/or higher-order cumulants to identify the number of active users in each cell. Subsequently, they establish the channel coefficients identified with...",
        "ori-fast-z-score": -3.047505999613382,
        "water-fast-z-score": 9.53595439531399,
        "rewrite-fast-z-score": -0.5416762627738958
    },
    {
        "original_text": "We present new near-infrared (NIR) spectra for red supergiants and giants, obtained at the European Southern Observatory in La Silla, Chile.  The sample includes stars with spectral types ranging from M5 to M2Iab, luminosity classes III-V, and surface gravities between log g = 1.0 and 3.8 dex. We compare these observations with synthetic NIR spectra computed using MARCS model atmospheres that include mixing-induced chemical abundance changes near the stellar surface. Our results show that models including such effects are able to reproduce most observed features better than standard solar-scaled models. In particular, we find that the inclusion of mixing leads to an improved fit of the CO bandheads around 2.3 microns as well as of several atomic lines. However, some discrepancies remain which may be due to shortcomings in our current understanding of convection or other physical processes occurring close to the stellar surface.",
        "watermark_text": "We present new near - infrared ( NIR ) spectra for red supergiants and giants , obtained at the European Southern Observatory in La Silla , Chile . The sample comprises stars with spectral classes ranging from M5 to M2Iab , luminosity classes III - V , and surface gravities between log f = 1 . 0 and 3 . 8 dex .We link these observations with artificial NIR spectra computed using MARCS model atmospheres that include mixing - caused chemical abundance changes near the stars surface . Our results show that estimates including such effects are able to depict most observed features good than conventional solar - scaled models .In particular , we find that the introduction of mixing gives to an better match of the CO bandheads around 2 . 3 microns as well as of several atomic lines . However , some discrepancies remain which perhaps be due to shortcomings in our future interpretation of convection or other material processes resulting close to the stars interior .",
        "rewrite_text": "We present new near-infrared (NIR) spectra for red supergiants and giants collected at the European Southern Observatory in La Silla, Chile. The sample includes stars with spectral classes ranging from M5 to M2Iab, luminosity classes III to V, and surface gravities between log g = 1.0 and 3.8 dex. We associate these observations with artificial NIR spectra generated using MARCS model atmospheres that incorporate changes in chemical abundance due to mixing near the stars' surfaces. Our findings indicate that models accounting for these effects better represent most observed features compared to traditional solar-scaled models. Notably, we discover that the inclusion of mixing provides a more accurate match for the CO bandheads around 2.3 microns and numerous atomic lines. However, some discrepancies persist, which may stem from limitations in our current understanding of convection or other processes occurring near the stars' interiors.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We compare the performance of two different methods to simulate non-equilibrium dynamics in polymer systems, namely dissipative particle dynamics (DPD) with either a Nosé-Hoover or a Langevin thermostat. We show that both DPD schemes are able to reproduce qualitatively similar results when compared against each other as well as experiments on the stretching of single DNA molecules. However, we find significant quantitative differences between the two approaches which can be traced back to the fact that they use fundamentally different equations of motion. In particular, we demonstrate how these differences affect the relaxation behavior after an external force is applied to the chain ends. Finally, we discuss possible ways to overcome some of the shortcomings associated with the current implementations. \n \n Introduction \n \n The study of complex fluids such as polymers requires sophisticated simulation techniques capable of describing their unique properties at various length scales. While atomistic molecular dynamics has been successfully used to investigate phenomena occurring over short time and length scales  1–3 , coarse-grained models have emerged as powerful tools to explore longer timescales  4–6 . These simplified descriptions typically involve representing groups of atoms by one effective interaction site  7–9 . For example, in the case of biopolymers like proteins  10–12  or nucleic acids  13–18 , this approach allows us to capture essential features of the underlying physics while reducing computational costs significantly  19, 20 . \n \n Coarse-graining strategies often rely on mapping the interactions among individual particles onto effective potentials  21 . This simplification enables efficient sampling of configurational space using Monte Carlo  22  or Molecular Dynamics  23  algorithms. Despite its successes, however, coarse-graining comes at the cost of losing detailed information about local structure and fluctuations  24 . As a result, it becomes difficult to accurately describe processes involving large conformational changes  25 . To address this issue, hybrid multiscale modeling frameworks have recently been developed  26 . Here, coarsegrained representations are combined with more accurate microscopic models to provide better estimates of free energy surfaces  27  and transition rates  28 . \n \n Another important aspect of coarse-grained models concerns the choice of appropriate",
        "watermark_text": "We contrast the performance of two different methods to simulate non - equilibrium dynamics in polymer systems , namely dissipative particle behavior ( DPD ) with either a Nosé - Hoover or a Langevin thermostat . We see that both DPD strategies are able to predict qualitatively identical outcome when compared against each other as well as experiments on the stretching of multiple DNA molecules .However , we find considerable quantitative variations between the two strategies which can be traced back to the fact that they use fundamentally different equations of movement . In particular , we study how these changes affect the relaxation behavior after an external stress is applied to the chain ends .Finally , we explain possible ways to overcome some of the shortcomings associated with the present implementations . Introduction The investigation of complex materials such as polymers demands sophisticated simulation algorithms suitable of describing their distinct characteristics at several length scales .While atomistic atomic dynamics has been successfully utilized to examine processes arising over short period and duration scales 1 – 3 , coarse - grained models have developed as powerful tools to examine longer timescales 4 – 6 . These simplified descriptions typically involve describing bands of atoms by one effective bonding location 7 – 9 .For instance , in the case of biopolymers like genes 10 – 12 or nucleic acids 13 – 18 , this methodology allows us to capture essential aspects of the fundamental physics while reducing theoretical costs significantly 19 , 20 . Coarse - graining methods often relies on mapping the interactions among individual molecules onto effective potentials 21 .This simplification enables efficient scanning of configurational space employing Monte Carlo 22 or Molecular Dynamics 23 methods . Despite its effectiveness , however , coarse - graining comes at the cost of losing explicit data about local structure and fluctuations 24 .As a result , it becomes hard to correctly define systems featuring large conformational changes 25 . To address this question , hybrid multiscale modeling frameworks have recently been created 26 .Here , coarsegrained representations are coupled with more accurate microscopic predictions to provide better estimates of free energy materials 27 and transfer rates 28 . Another important dimension of rough - grained models concerns the selection of appropriate",
        "rewrite_text": "We compare the performance of two distinct methods for simulating non-equilibrium dynamics in polymer systems: dissipative particle dynamics (DPD) using either a Nosé-Hoover thermostat or a Langevin thermostat. Our findings indicate that both DPD approaches yield qualitatively similar results when assessed against each other and in relation to experimental data on the stretching of multiple DNA molecules. However, we observe significant quantitative differences between the two methods, which stem from their fundamentally different equations of motion. In particular, we investigate how these variations influence the relaxation behavior following the application of external stress at the ends of the chains. Lastly, we propose potential solutions to address some of the limitations associated with the current implementations.\n\n**Introduction:** The study of complex materials such as polymers requires advanced simulation algorithms capable of accurately representing their unique characteristics across multiple length scales. While atomistic dynamics has effectively been used to analyze processes that occur over short time scales, coarse-grained models have emerged as powerful tools for investigating longer time scales. These simplified approaches often represent groups of atoms with a single effective bonding site. For example, in the context of biopolymers such as genes or nucleic acids, this methodology captures essential aspects of fundamental physics while significantly reducing theoretical costs. Coarse-graining methods typically involve mapping the interactions between individual molecules onto effective potentials. This simplification allows for efficient exploration of configurational space using Monte Carlo or Molecular Dynamics methods. Nevertheless, coarse-graining has the drawback of sacrificing detailed information about local structures and fluctuations, making it challenging to accurately define systems that undergo significant conformational changes. To tackle this issue, recent developments have produced hybrid multiscale modeling frameworks that integrate coarse-grained representations with more precise microscopic predictions, leading to improved estimates of free energy and transfer rates. Another crucial aspect of coarse-grained models pertains to the careful selection of appropriate interactions.",
        "ori-fast-z-score": -0.6099942813304187,
        "water-fast-z-score": 8.490698088083718,
        "rewrite-fast-z-score": 0.7049344049891616
    },
    {
        "original_text": "We have performed first-principles calculations to investigate the characteristics of phonon transmission across an epitaxial interface between Si and Ge, which are important for thermoelectric applications. We found that the phonon transmission is strongly suppressed at low frequencies due to the mismatch in acoustic impedance between two materials. The suppression becomes more significant as the thickness of Ge layer decreases. In addition, we observed that the phonon transmission exhibits strong anisotropy with respect to incident angle. These results suggest that it may be possible to control thermal transport by tuning the structure of epitaxial interfaces. Epitaxial interfaces play crucial roles in determining physical properties such as electrical conductivity  1  , optical reflectivity  2  , mechanical strength  3  , and thermal conductivity  4  . For example, recent studies on superlattices show that the thermal conductance can be reduced significantly compared to bulk values  5, 6  .\nIn this work, we focus on phonons because they dominate heat conduction in solids  7, 8  . Phonon scattering at epitaxial interfaces has been studied extensively using molecular dynamics (MD) simulations  9  or kinetic theory  10  . However, these approaches cannot provide detailed information about phonon transmission across interfaces since they do not take into account atomic interactions explicitly  11  . On the other hand, density functional theory (DFT), which describes electronic states based on quantum mechanics  12  , allows us to calculate phonon transmission coefficients directly  13  . Therefore, DFT-based methods are suitable for investigating phonon transmission across epi-",
        "watermark_text": "We have done first - principles measurements to examine the properties of phonon communication across an epitaxial connection between Si and Ge , which are important for thermoelectric use . We identified that the phonon communication is strongly restrained at low frequencies owing to the mismatch in sound impedance between two materials .The suppression gets more considerable as the height of Ge coating decreases . In addition , we reported that the phonon propagation displays good anisotropy with regard to incident angle .These data suggest that it could be possible to affect heat transport by tuning the structure of epitaxial interfaces . Epitaxial interfaces play crucial roles in determining physical properties such as mechanical conductivity 1 , optical reflectivity 2 , thermal strength 3 , and thermal conductivity 4 .For instance , recent studies on superlattices indicate that the thermal conductance can be reduced greatly compared to bulk values 5 , 6 . In this research , we focus on phonons because they govern heat conduction in solids 7 , 8 .Phonon splitting at epitaxial interfaces has been studied thoroughly using chemical mechanics ( MD ) simulations 9 or kinetic theory 10 . However , these perspectives cannot offer comprehensive information about phonon communication across interfaces since they do not take into consideration atomic interactions explicitly 11 .On the other hand , density functional theory ( DFT ) , which explains electronic states based on quantum mechanics 12 , allows us to estimate phonon communication equations directly 13 . Therefore , DFT - based methods are suitable for investigating phonon communication across epi -",
        "rewrite_text": "We conducted first-principles measurements to explore the phonon communication characteristics between an epitaxial junction of silicon (Si) and germanium (Ge), which are crucial for thermoelectric applications. Our findings indicate that phonon communication is significantly hindered at low frequencies due to the mismatch in acoustic impedance between the two materials. This suppression becomes more pronounced as the thickness of the Ge layer is reduced. Additionally, we observed that phonon propagation exhibits notable anisotropy concerning the incident angle. These results imply that it may be possible to influence heat transport by adjusting the structure of the epitaxial interfaces. Such interfaces are vital for determining key physical properties, including mechanical conductivity, optical reflectivity, thermal strength, and thermal conductivity. For example, recent investigations into superlattices have shown that thermal conductance can be substantially lower than that of bulk materials. Our research emphasizes phonons, as they are the primary carriers of heat conduction in solids. While previous studies have thoroughly examined phonon splitting at epitaxial interfaces using molecular dynamics (MD) simulations or kinetic theory, these approaches fall short of providing a complete understanding of phonon communication across interfaces due to their lack of explicit atomic interaction considerations. Conversely, density functional theory (DFT), which elucidates electronic states through quantum mechanics, enables us to directly assess phonon communication dynamics. Consequently, DFT-based methods are well-suited for investigating phonon communication at such epitaxial boundaries.",
        "ori-fast-z-score": -0.7559289460184544,
        "water-fast-z-score": 7.878011364745742,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "G-protein-coupled receptors (GPCRs) are one of the largest families of cell surface proteins and play an important role in many physiological processes, including vision, olfaction, taste, hormone secretion, neurotransmission, immune response, inflammation, and cancer.  The number of known human GPCR sequences has increased dramatically over recent years due to advances in genome sequencing technologies; however, only about half have been functionally characterized so far. In this study we present a novel computational method that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily. Our approach is based on two key ideas: 1) We use a new adaptive strategy to select representative sequences from different subfamilies by considering both the diversity among them as well as their evolutionary relationships; 2) We develop a new scoring scheme which takes into account not only the pairwise sequence similarities but also the structural information between query and template sequences. Using our method, we were able to successfully classify more than 80% of all human GPCRs whose functions had previously been experimentally determined.",
        "watermark_text": "G - protein - coupled receptors ( GPCRs ) are one of the largest families of cell surface proteins and play an important role in multiple physiological mechanisms , notably vision , olfaction , taste , hormone secretion , neurotransmission , immune reaction , inflammation , and tumors . The amount of known human GPCR sequences has increased significantly over recent months thanks to advances in genome sequencing technologies ; however , only about half have been functionally distinguished so far .In this study we present a innovative computational technique that can be used to predict the functional class of uncharacterized GPCRs based on their sequence similarity with other members within each subfamily . Our solution is based on two fundamental ideas : 1 ) We use a new adaptive approach to select representative sequences from different subfamilies by examining both the complexity among them as well as their evolutionary relationships ; 2 ) We develop a new scoring scheme which assumes into consideration not only the pairwise sequence comparisons but also the structural information between query and template sequences .Using our technique , we were able to effectively classify more than 80 % of all human GPCRs whose functions had earlier been experimentally determined .",
        "rewrite_text": "G-protein-coupled receptors (GPCRs) comprise one of the largest families of cell surface proteins and are crucial for various physiological processes, including vision, smell, taste, hormone release, neurotransmission, immune responses, inflammation, and tumor development. Recent advancements in genome sequencing technology have significantly increased the number of known human GPCR sequences in recent months; however, only about half of these have been functionally characterized to date. In our study, we introduce an innovative computational method to predict the functional classes of uncharacterized GPCRs by analyzing their sequence similarities with other receptors within the same subfamily. Our approach is founded on two main principles: 1) We utilize a novel adaptive method to identify representative sequences from different subfamilies by assessing their complexity and evolutionary relationships; 2) We designed a scoring system that incorporates not just pairwise sequence comparisons, but also structural information between the query and template sequences. Using this technique, we successfully classified over 80% of all human GPCRs that had previously been experimentally characterized.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 0.10846522890932808
    },
    {
        "original_text": "In this work, we propose an efficient watermarking scheme for digital images based on the discrete wavelet transform (DWT). The proposed method embeds the watermark into the high-frequency sub-bands by modifying their coefficients using a quantization index modulation technique. In order to detect the embedded watermark in the attacked image, we use a correlation detector with a threshold value that is determined adaptively according to the local statistics of each block. Experimental results show that our algorithm has good performance against various attacks such as JPEG compression, salt & pepper noise addition, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, gamma correction, median filter, and mean filter. Keywords: Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value. Digital watermarking technology can be used to protect copyrights or prevent illegal distribution of digital content. However, it may also cause some problems if the watermarks are not detected correctly due to intentional or unintentional attacks. Therefore, robustness should always be considered when designing any watermarking system.",
        "watermark_text": "In this research , we propose an efficient watermarking scheme for digital photographs relying on the discrete wavelet transform ( DWT ) . The proposed approach embeds the watermark into the high - frequency sub - bands by modifying their coefficients utilizing a quantization index modulation method .In order to identify the embedded watermark in the attacked image , we utilize a correlation detector with a threshold expression that is calculated adaptively based to the local statistics of each block . Experimental findings show that our technique has good success against several attacks such as JPEG compression , salt & pepper noise addition , Gaussian filtering , cropping , rotation , scaling , blurring , sharpening , histogram equalization , alpha reduction , median filter , and average filter .Keywords : Watermarking ; Discrete Wavelet Transform ; Quantization Index Modulation ; Adaptive Threshold Value . Digital watermarking technology can be used to shield copyrights or avoid illegal distribution of digital content .However , it could also create some problems if the watermarks are not noticed correctly due to deliberate or unintentional hits . Therefore , robustness should always be included when designing any watermarking scheme .",
        "rewrite_text": "In this study, we introduce an efficient watermarking technique for digital photographs utilizing the discrete wavelet transform (DWT). Our method embeds the watermark within the high-frequency sub-bands by adjusting their coefficients through a quantization index modulation approach. To detect the embedded watermark in an altered image, we employ a correlation detector with a threshold that is adaptively calculated based on the local statistics of each block. Experimental results demonstrate that our method performs well against various attacks, including JPEG compression, salt-and-pepper noise addition, Gaussian filtering, cropping, rotation, scaling, blurring, sharpening, histogram equalization, alpha reduction, median filtering, and averaging filters. Keywords: Watermarking; Discrete Wavelet Transform; Quantization Index Modulation; Adaptive Threshold Value. Digital watermarking technology serves to protect copyrights and prevent unauthorized distribution of digital content. However, it may cause issues if the watermarks are misinterpreted due to either intentional or unintentional interference. Consequently, robustness must be a fundamental consideration in the design of any watermarking scheme.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": -0.7171371656006361
    },
    {
        "original_text": "We present exact solutions to the nonholonomic Ricci flows for Einstein spaces with torsion, which are generated by nonlinear connections (N-connections) on principal fiber bundles. The N-connection coefficients depend only on time parameter t and satisfy certain evolution equations. We show that these solutions can be used in order to construct new classes of exact solutions to gravitational field equations in general relativity theory. In particular we find explicit examples of exact solutions describing black holes and wormholes. These results may have important applications in modern cosmology as well as in quantum gravity theories. Keywords: Nonholonomic Ricci flow, Gravity, Black hole, Wormhole, Cosmological model. 1 Introduction.\nThe geometric methods of constructing exact solutions to gravitational field equation in general relativity theory were developed during last decades  1, 2  . It is possible to generate such solutions using various types of symmetries like Killing vectors or tensors, homothetic vector fields etc., see e.g.  3  -  8  .\nIn this work we consider another approach based on so-called nonholonomic frame transforms  9, 10  , when one uses anholonomy coefficients of nonlinear connection (N-connection) instead of usual Christoffel symbols. This method was applied recently in order to study some problems related to modified gravity models  11  -  13  .",
        "watermark_text": "We present precise solutions to the nonholonomic Ricci currents for Einstein spaces with torsion , which are produced by nonlinear links ( N - connections ) on principal fiber bundles . The N - connection coefficients depend only on time variable t and obey certain evolution equations .We see that these solutions can be used in order to build new classes of precise solutions to gravitational field equations in general relativity theory . In particular we find explicit examples of precise solutions involving red holes and wormholes .These conclusions could have important use in modern cosmology as well as in quantum gravitational dynamics . Keywords : Nonholonomic Ricci system , Gravity , Black hole , Wormhole , Cosmological model .1 Introduction . The mathematical techniques of constructing exact solutions to gravitational field function in general relativity theory were developed during last decades 1 , 2 .It is easy to create such solutions use different kinds of symmetries like Killing tensor or tensors , homothetic vector spaces etc . , see e . g . 3 - 8 .In this research we imagine another methodology relying on so - called nonholonomic frame transforms 9 , 10 , when one uses anholonomy values of nonlinear connection ( N - connection ) instead of customary Christoffel characters . This method was used lately in order to study some problems related to revised gravitational models 11 - 13 .",
        "rewrite_text": "We provide exact solutions to the nonholonomic Ricci currents associated with Einstein spaces that incorporate torsion, which arise from nonlinear connections (N-connections) on principal fiber bundles. The coefficients of these N-connections depend only on the time variable \\( t \\) and adhere to specific evolution equations. These solutions can be utilized to generate new classes of precise solutions to the gravitational field equations in general relativity. Notably, we present explicit examples of such solutions that involve black holes and wormholes. These findings may have significant implications for contemporary cosmology as well as quantum gravitational dynamics. \n\n**Keywords:** Nonholonomic Ricci system, Gravity, Black Hole, Wormhole, Cosmological Model.\n\n**1. Introduction.** In recent decades, various mathematical techniques have been developed to construct exact solutions to the equations governing gravitational fields in general relativity. It is relatively straightforward to generate these solutions using different symmetry techniques, such as Killing tensors or homothetic vector spaces. In this study, we propose an alternative approach that leverages nonholonomic frame transformations, utilizing anholonomic characteristics of nonlinear connections (N-connections) instead of the traditional Christoffel symbols. This methodology has recently been applied to address issues related to modified gravitational models.",
        "ori-fast-z-score": -1.5540573797716226,
        "water-fast-z-score": 5.633458001672132,
        "rewrite-fast-z-score": 0.40406101782088427
    },
    {
        "original_text": "We study the propagation of external control signals through a large class of randomly constructed Boolean networks, which are known to exhibit rich dynamical behaviors including chaos.  We show that for any given initial state, there exists an optimal set of input nodes such that if their states are fixed at 1 or 0, then the resulting system will reach its steady state with minimum number of steps (or equivalently maximum speed). The problem is formulated as finding this optimal set of input nodes by solving a combinatorial optimization problem on graphs. This formulation leads us to develop efficient algorithms based on local search techniques. Our results provide insights into how to design effective control strategies for complex systems modeled by Boolean networks. In addition, we also investigate the effect of noise on the performance of these control strategies. Finally, our analysis reveals some interesting connections between the structure of the underlying graph and the effectiveness of different control strategies. A Boolean network consists of N binary-state variables xi(t) ∈ {0, 1} evolving according to a deterministic update rule specified by a directed graph G = (V , E), where V denotes the set of vertices representing the N variables, and E represents the set of edges connecting pairs of vertices. Each vertex i has K incoming neighbors denoted by Ni = {j : (i, j ) ∈ E}. At each time step t, all the variables synchronously evolve according to the following updating scheme:",
        "watermark_text": "We test the propagation of external control messages through a large class of randomly established Boolean connections , which are known to undergo rich dynamical interactions including chaos . We see that for any given original state , there exists an appropriate collection of input nodes such that if their states are fixed at 1 or 0 , then the resulting system will attain its steady state with minimum number of steps ( or equivalently limit speed ) .The question is implemented as finding this efficient setting of input nodes by solving a combinatorial algorithm question on graphs . This formulation leads us to develop optimal methods using on local search methods .Our results yield insights into how to model optimal control tactics for complex systems modeled by Boolean connections . In addition , we also investigate the impact of noise on the performance of these control tactics .Finally , our analysis reveals some interesting connections between the composition of the underlying graph and the performance of different control tactics . A Boolean network consists of N binary - state components xi ( t ) ∈ { 0 , 1 } evolving due to a deterministic change rule defined by a directed graph G = ( V , E ) , where V denotes the group of vertices representing the N variables , and E represents the group of vertices linking pairs of vertices .Each vertex i has K incoming neighbors denoted by Ni = { j : ( i , j ) ∈ E } . At each time step t , all the variables synchronously evolve according to the following updating scheme :",
        "rewrite_text": "We examine the transmission of external control messages through a wide range of randomly formed Boolean connections, characterized by complex dynamic behaviors, including chaotic patterns. Our study demonstrates that for any initial configuration, there exists a suitable set of input nodes that, when fixed at either 1 or 0, allows the system to reach its steady state in the fewest steps possible (or, equivalently, the fastest limit speed). This leads us to frame the problem of identifying these optimal input nodes as a combinatorial algorithm challenge on graphs. As a result, we develop optimal strategies leveraging local search techniques. Our findings provide valuable insights into modeling effective control tactics for complex systems represented by Boolean connections. Furthermore, we explore the effects of noise on the efficacy of these control strategies. Finally, our analysis uncovers intriguing relationships between the structure of the underlying graph and the performance of various control methods. In a Boolean network, we consider N binary-state components \\(x_i(t) \\in \\{0, 1\\}\\) that evolve according to deterministic rules defined by a directed graph \\(G = (V, E)\\), where \\(V\\) represents the vertices corresponding to the \\(N\\) variables, and \\(E\\) encompasses the edges connecting vertex pairs. Each vertex \\(i\\) has \\(K\\) incoming neighbors, indicated by \\(N_i = \\{j : (i, j) \\in E\\}\\). At each time step \\(t\\), all variables concurrently evolve according to the following updating scheme:",
        "ori-fast-z-score": 0.3592106040535498,
        "water-fast-z-score": 7.065974808899335,
        "rewrite-fast-z-score": 1.2018504251546631
    },
    {
        "original_text": "We present new results on the contribution of BL Lacs (blazars) to the extragalactic gamma-ray background based on data collected by the Fermi Large Area Telescope between August 2008 and December 2010, corresponding to an effective exposure time of 1.6 yr for each source in our sample. We use two different methods to estimate this contribution: i) we calculate the number counts above 100 MeV as function of redshift using a maximum likelihood method; ii) we fit the observed spectral energy distribution with a log-parabola model and derive the integrated fluxes at 0.1 GeV and 10 TeV energies. The resulting contributions are consistent within statistical uncertainties. Our best-fit value is F(>100 MeV) = 2.2 x 10^{−8} ph cm−2 s−1 sr−1 which corresponds to ~20% of the measured EGB intensity. This result confirms that blazars are one of the main contributors to the EGB emission.",
        "watermark_text": "We report new data on the contribution of BL Lacs ( blazars ) to the extragalactic gamma - ray background based on evidence generated by the Fermi Large Area Telescope between August 2008 and December 2010 , corresponding to an effective exposure time of 1 . 6 yr for each source in our sample . We use two different methods to estimate this contribution : i ) we determine the number counts above 100 MeV as function of redshift using a maximum likelihood technique ; ii ) we fit the observed spectral power distribution with a log - parabola simulation and derive the integrated fluxes at 0 . 1 GeV and 10 TeV energies .The resulting contributions are compatible within statistical uncertainties . Our best - fitting value is F ( > 100 MeV ) = 2 . 2 x 10 ^ { −8 } ph cm−2 s−1 sr−1 which equals to ~ 20 % of the measured EGB brightness .This result confirms that blazars are one of the main contributors to the EGB emission .",
        "rewrite_text": "We present new findings regarding the role of BL Lacertae objects (blazars) in the extragalactic gamma-ray background, based on data collected by the Fermi Large Area Telescope from August 2008 to December 2010, which corresponds to an effective exposure of 1.6 years for each source in our study. We employed two different methods to quantify this contribution: first, we assessed the number counts above 100 MeV as a function of redshift using a maximum likelihood approach; second, we analyzed the observed spectral power distribution with a log-parabola fit to determine the integrated fluxes at 0.1 GeV and 10 TeV energies. The contributions we obtained are consistent with each other within statistical uncertainties. Our optimal fitting value is F(> 100 MeV) = 2.2 x 10^{−8} ph cm−2 s−1 sr−1, which accounts for approximately 20% of the observed extragalactic gamma-ray background brightness. This finding reinforces the idea that blazars are significant contributors to the emission of the extragalactic background.",
        "ori-fast-z-score": -0.40451991747794525,
        "water-fast-z-score": 2.8316394223456167,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "We present new structural parameters for 23 globular clusters (GCs) in M31, derived using HST/ACS images taken with the F606W filter. We use these data to test whether GCs follow the same scaling relations as elliptical galaxies do on the so-called  fundamental plane . The results show that most of our sample follows this relation within uncertainties, but we also find some outliers which are likely due to their different formation histories or dynamical states. In addition, we compare our results with those obtained by other authors who used ground-based observations. Our analysis shows that there is no significant difference between the two datasets when they are analyzed consistently. Finally, we discuss possible reasons why previous studies have found conflicting results about the existence of such a relationship among GC systems. This work was supported by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane",
        "watermark_text": "We report new structural values for 23 globular complexes ( GCs ) in M31 , obtained using HST / ACS images obtained with the F606W filter . We use these information to test whether GCs obey the same scaling relations as elliptical galaxies do on the so - called core plane .The results show that most of our sample follows this relation within uncertainties , but we also find some outliers which are likely due to their different formation histories or dynamical states . In addition , we compare our findings with those acquired by other researchers who used ground - based observations .Our study shows that there is no major difference between the two datasets when they are examined consistently . Finally , we explain possible reasons why previous research have discovered differing results about the existence of such a connection among GC systems .This project was supported by NASA grant NAG5 - 12140 . Keywords : Globular cluster ; Fundamental Plane",
        "rewrite_text": "We present new structural parameters for 23 globular clusters (GCs) in M31, derived from HST/ACS images captured with the F606W filter. This data allows us to investigate whether GCs follow the same scaling relations as elliptical galaxies within the so-called core plane. Our findings indicate that most of the GCs in our sample conform to this relation, accounting for uncertainties, although some outliers are identified, likely reflecting variations in their formation histories or dynamical states. Furthermore, we compare our results with those from other studies that utilized ground-based observations and find no significant differences between the two datasets when analyzed consistently. Finally, we discuss potential reasons for the discrepancies noted in previous research regarding the relationship among GC systems. This project was supported by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.842005842008763,
        "rewrite-fast-z-score": 0.6622661785325219
    },
    {
        "original_text": "We present the first steps towards building a library of synthetic galaxy SEDs (spectral energy distributions) that will be used to classify and parameterize unresolved galaxies in the Gaia data stream, as part of the Data Processing and Analysis Consortium (DPAC). The library is built using state-of-the-art stellar population synthesis models with different star formation histories, metallicities, dust content, and redshifts. We use this library to test two methods of classifying unresolved galaxies into broad morphological types based on their observed photometry only. In addition we show how these parameters can be constrained by fitting the full spectrum of an unresolved galaxy. This work was performed within the framework of the ESA Gaia mission. Keywords: Galaxy evolution; Stellar populations; Spectroscopy. 1 Introduction Galaxies are complex systems whose properties depend strongly on their mass, age, chemical composition, star formation history, and environment. These physical characteristics determine many observable quantities such as luminosity, colours, morphology, kinematics, etc., which have been studied extensively over several decades. However, it has become clear recently that there exist significant degeneracies between some of these observables and therefore they cannot be uniquely determined without additional information about the underlying physics or geometry of the system. For example, the total luminosity of a galaxy depends not only on its current star formation rate but also on its past star formation activity through the integrated light of old stars. Similarly, the colour of a galaxy depends both on its metallicity and on the amount of dust extinction along our line-of-sight. Therefore, accurate measurements of all relevant physical parameters require detailed spectroscopic observations covering large wavelength ranges. Such studies are now possible thanks to new space missions like GALEX, SDSS, 2MASS, Spitzer Space Telescope, Herschel Space Observatory, Chandra X-ray Observatory, XMM-Newton, Hubble Space Telescope, and most importantly, the upcoming European Space Agency s Gaia satellite. Gaia is expected to provide astrometric positions, parallaxes, proper motions, radial velocities, and multi-colour photometry for more than one billion objects",
        "watermark_text": "We present the first steps towards constructing a library of synthetic galaxy SEDs ( spectral energy distributions ) that will be used to classify and parameterize unresolved galaxies in the Gaia data feed , as member of the Data Processing and Analysis Consortium ( DPAC ) . The library is built using state - of - the - art stellar community synthesis estimates with various galaxy formation histories , metallicities , dust content , and redshifts .We use this library to test two means of classifying unresolved galaxies into wide morphological types based on their observed photometry only . In addition we prove how these parameters can be constrained by fitting the full spectrum of an unresolved universe .This research was done within the framework of the ESA Gaia spacecraft . Keywords : Galaxy evolution ; Stellar populations ; Spectroscopy .1 Introduction Galaxies are diverse structures whose characteristics rely heavily on their mass , age , chemical composition , star formation history , and environment . These physical traits determine many observable quantities such as luminosity , colours , morphology , kinematics , etc . , which have been studied frequently over numerous years .However , it has become clear recently that there remain considerable degeneracies between some of these observables and therefore they cannot be uniquely determined without additional information about the fundamental physics or topology of the system . For instance , the total luminosity of a galaxy depends not only on its current star formation rate but also on its past star formation activity through the integrated light of young galaxies .Similarly , the colour of a galaxy depends both on its metallicity and on the extent of dust extinction along our line - of - view . Therefore , detailed observations of all relevant physical factors require comprehensive spectroscopic observations encompassing large wavelength ranges .Such investigations are now able thanks to novel space missions like GALEX , SDSS , 2MASS , Spitzer Space Telescope , Herschel Space Observatory , Chandra X - ray Observatory , XMM - Newton , Hubble Space Telescope , and most importantly , the latest European Space Agency s Gaia satellite . Gaia is expected to provide astrometric positions , parallaxes , proper motions , radial velocities , and multi - colour photometry for more than one billion objects",
        "rewrite_text": "We are taking initial steps to develop a library of synthetic spectral energy distributions (SEDs) for galaxies, which will aid in the classification and parameterization of unresolved galaxies found in the Gaia data feed, as part of the Data Processing and Analysis Consortium (DPAC). This library utilizes advanced stellar community synthesis models that incorporate various galaxy formation histories, metallicities, dust content, and redshifts. We employ this library to explore two methods for classifying unresolved galaxies into broad morphological categories based solely on their observed photometry. Additionally, we demonstrate how these parameters can be refined by analyzing the complete spectrum of the unresolved universe. This research is conducted within the context of the ESA Gaia mission. \n\n**Keywords:** Galaxy evolution; Stellar populations; Spectroscopy.\n\n**1 Introduction**: \nGalaxies are varied structures whose properties significantly influence factors like mass, age, chemical composition, star formation history, and environment. These characteristics affect numerous observable features such as luminosity, color, morphology, and kinematics, which have been extensively studied over the years. However, it has become increasingly evident that substantial degeneracies exist among some of these observables, making it impossible to uniquely determine them without additional insight into the fundamental physics or topology of the system. For instance, a galaxy's total luminosity is influenced not only by its current star formation rate but also by its historical star formation activity, reflected in the integrated light from young stars. Similarly, a galaxy's color is affected by both its metallicity and the degree of dust extinction along our line of sight. Thus, in-depth observations of all relevant physical parameters necessitate comprehensive spectroscopic studies across a wide range of wavelengths. Such investigations are now feasible due to innovative space missions, including GALEX, SDSS, 2MASS, the Spitzer Space Telescope, Herschel Space Observatory, Chandra X-ray Observatory, XMM-Newton, the Hubble Space Telescope, and notably, the latest European Space Agency’s Gaia satellite. Gaia is anticipated to deliver astrometric positions, parallaxes, proper motions, radial velocities, and multi-color photometry for over one billion objects.",
        "ori-fast-z-score": 1.643989873053573,
        "water-fast-z-score": 7.893748050080246,
        "rewrite-fast-z-score": 2.048079801297601
    },
    {
        "original_text": "We present an analysis of galaxy clusters selected by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). We use two different methods to select cluster candidates, and then apply photometric redshift cuts on these samples to obtain final catalogues with high purity. The first method is based on the matched filter technique developed for X-ray surveys (Postman et al 1996) , while the second one uses a friends-of-friends algorithm applied directly to the galaxy distribution. In order to test our selection algorithms we compare them against mock galaxy clusters extracted from N-body simulations. Our main results are as follows:  1. Using the matched filter technique we find that the number density of galaxy clusters at z < 0.5 is n(z<0.5) = 3.6 +/- 0.2 x 10-3 h3 Mpc-3 . This value agrees well with previous determinations using other techniques.  2. By applying the same matched filter technique to simulated galaxy clusters we show how this method can be used to estimate the mass function of galaxy clusters up to z ~1.0.",
        "watermark_text": "We present an assessment of galaxy regions selected by their red - sequence galaxies ( Gladders & Yee 2005 ) from the Sloan Digital Sky Survey Data Release 4 ( DR4 ) . We use two different methods to select cluster applicants , and then use photometric redshift cutting on these specimens to obtain final catalogues with high purity .The first method is based on the matched filter technique developed for X - ray observations ( Postman et al 1996 ) , while the second one uses a enemies - of - friends method applied directly to the galaxy distribution . In order to test our choice algorithms we compare them against mock galaxy galaxies extracted from N - bodies simulations .Our main results are as follows : 1 . Using the matched filter technique we find that the number density of galaxy galaxies at z < 0 . 5 is n ( z < 0 . 5 ) = 3 . 6 + / - 0 . 2 x 10 - 3 h3 Mpc - 3 .This value agrees well with previous determinations using other techniques.2.By applying the same matched filter technique to modeled galaxy galaxies we find how this algorithm can be used to estimate the mass value of galaxy galaxies up to z ~ 1 . 0 .",
        "rewrite_text": "We present an evaluation of galaxy regions identified by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). Our approach employs two distinct methods for selecting cluster candidates, followed by photometric redshift filtering to achieve high-purity final catalogs. The first method utilizes the matched filter technique originally designed for X-ray observations (Postman et al. 1996), while the second applies a friends-of-friends algorithm directly to the galaxy distribution. To validate our selection algorithms, we compare the results with mock galaxy populations derived from N-body simulations. Our key findings are as follows: 1. Utilizing the matched filter technique, we determine the number density of galaxies at z < 0.5 to be n(z < 0.5) = 3.6 ± 0.2 x 10^-3 h^3 Mpc^-3, which is consistent with previous measurements obtained using various methods. 2. By applying the matched filter technique to modeled galaxy distributions, we demonstrate its effectiveness in estimating the mass of galaxy clusters up to z ~ 1.0.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We consider the problem of learning an unknown distribution over binary strings, where each string is generated by sampling independently at random with replacement according to some fixed probability vector . We show that if we are given samples drawn iid from , then it can be learned in polynomial time using standard statistical techniques (e.g., maximum likelihood). However, when the number of possible values for each bit grows large, this approach fails because there may not exist any sample which contains all possible values for every bit. In such cases, we propose a novel algorithm based on Gibbs sampling and prove its correctness under certain conditions. Finally, we present experimental results demonstrating our method s effectiveness. The main contribution of this work lies in showing how to learn distributions over combinatorial objects like graphs and trees. This is achieved by first reducing these problems into learning distributions over binary strings, and then applying our proposed algorithms. Learning distributions over combinatorial objects has many applications including data mining, computer vision, bioinformatics, etc. .",
        "watermark_text": "We consider the question of knowing an unknown distribution over binary strings , where each string is generated by filtering independently at random with replacement due to some fixed probability vector . We see that if we are given samples picked iid from , then it can be learned in polynomial period using conventional statistical tools ( e . g . , maximum likelihood ) .However , when the quantity of possible values for each bit grows large , this methodology fails because there may not exist any specimen which contains all possible values for every bit . In such cases , we undertake a new algorithm based on Gibbs filtering and find its correctness under certain conditions .Finally , we present research results proving our technique s efficacy . The main impact of this research consists in teaching how to teach distributions over combinatorial objects like graphs and trees .This is achieved by first transforming these problems into knowledge distributions over binary strings , and then applying our proposed methods . Learning distributions over combinatorial objects has numerous uses including data extraction , computer vision , bioinformatics , etc ..",
        "rewrite_text": "We explore the challenge of understanding an unknown distribution of binary strings, which are generated by independently filtering at random with replacement according to a specified probability vector. Our findings indicate that when we receive iid samples from this distribution, it can be effectively learned in polynomial time using traditional statistical methods, such as maximum likelihood estimation. However, this approach becomes inadequate when the number of possible values for each bit increases significantly, as there may not be any sample that encompasses all potential values for each bit. To address this issue, we propose a new algorithm based on Gibbs filtering, demonstrating its validity under specific conditions. We also present research findings that validate the effectiveness of our approach. The primary contribution of this research is its application in learning distributions over combinatorial structures, like graphs and trees. We achieve this by converting these problems into learning distributions over binary strings and subsequently applying our proposed techniques. The ability to learn distributions over combinatorial objects has far-reaching applications, including data extraction, computer vision, and bioinformatics.",
        "ori-fast-z-score": 0.10369516947304253,
        "water-fast-z-score": 6.947576354693849,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "We study the message throughput performance of dynamic peer-to-peer systems with unreliable links and limited bandwidth, where peers are connected to each other via overlay networks. We propose two models for such systems: (1) an analytical model based on mean field theory; and (2) a simulation-based model that is built upon ns-2 network simulator. The results show that our proposed models can accurately predict the message throughput under different settings. In addition, we find that the message throughput decreases as the number of nodes increases or the average node degree decreases. Finally, we compare the message throughputs achieved by three popular overlay routing protocols: flooding, random walk, and epidemic routing. Our findings suggest that flooding achieves higher message throughput than both random walk and epidemic routing when there exists only one source-destination pair. However, if multiple pairs exist simultaneously, then epidemic routing outperforms flooding because it allows messages to be forwarded along multiple paths at once.",
        "watermark_text": "We research the message throughput performance of dynamic peer - to - peer systems with unreliable connectivity and low bandwidth , where peers are connected to each other via overlay networks . We suggest two models for such systems : ( 1 ) an analytical theory based on mean field theory ; and ( 2 ) a simulation - based model that is built upon ns - 2 network simulator .The results show that our proposed models can accurately forecast the message throughput under various contexts . In addition , we find that the message throughput decreases as the number of vertices increases or the average node degree drops .Finally , we compare the message throughputs achieved by three popular overlay routing protocols : storm , random walk , and epidemic routing . Our findings show that flooding achieves higher message throughput than both random walk and epidemic routing when there exists only one origin - destination pair .However , if multiple sets exist independently , then epidemic routing outperforms flooding because it allows messages to be forwarded along multiple paths at once .",
        "rewrite_text": "We investigate the message throughput performance of dynamic peer-to-peer systems characterized by unreliable connectivity and low bandwidth, with peers interconnected through overlay networks. Our research presents two models for these systems: (1) an analytical framework grounded in mean field theory, and (2) a simulation model developed using the ns-2 network simulator. Our findings demonstrate that both models can reliably predict message throughput across different scenarios. Furthermore, we observe that message throughput declines as the number of vertices increases or when the average node degree decreases. Lastly, we evaluate the message throughput achieved by three widely used overlay routing protocols: storm, random walk, and epidemic routing. Our results indicate that flooding offers superior message throughput compared to both random walk and epidemic routing when there is a single origin-destination pair. However, in scenarios with multiple independent sets, epidemic routing surpasses flooding, as it enables messages to be transmitted along multiple paths simultaneously.",
        "ori-fast-z-score": 3.18316353970102,
        "water-fast-z-score": 6.8333094212876695,
        "rewrite-fast-z-score": 3.0193176496962755
    },
    {
        "original_text": "We present an analysis of the kinetic Sunyaev-Zeldovich effect (kSZ) due to electrons in our galaxy, using data obtained with the Atacama Cosmology Telescope and the South Pole Telescope. We use two different methods for estimating the kSZ signal; one is based on cross-correlating maps of CMB temperature anisotropy at 150 GHz and 3000 GHz, while the other uses the power spectrum of the CMB temperature fluctuations at 150 GHz. The results are consistent between these two approaches within their respective uncertainties. We find that the amplitude of this signal agrees well with theoretical predictions when we assume a Navarro-Frenk-White profile for dark matter density distribution around galaxies. This measurement provides new constraints on cosmological parameters such as the Hubble constant H0 = 73 +/- 4 km s-1 Mpc-1 , the total mass density parameter Omega_m = 0.27 +/- 0.03 , and the equation-of-state w = -1.06 +/- 0.11 .",
        "watermark_text": "We present an assessment of the kinetic Sunyaev - Zeldovich effect ( kSZ ) due to electrons in our universe , using data acquired with the Atacama Cosmology Telescope and the South Pole Telescope . We use two different methods for estimating the kSZ signal ; one is based on cross - correlating mapping of CMB heat anisotropy at 150 GHz and 3000 GHz , while the other uses the power spectrum of the CMB heat fluctuations at 150 GHz .The results are compatible between these two perspectives within their different uncertainties . We see that the frequency of this signal agrees well with theoretical expectations when we suppose a Navarro - Frenk - White model for black material concentration distribution around galaxies .This measurement gives additional constraints on cosmological factors such as the Hubble constant H0 = 73 + / - 4 km s - 1 Mpc - 1 , the total mass density variable Omega _ m = 0 . 27 + / - 0 . 03 , and the equation - of - state w = - 1 . 06 + / - 0 . 11 .",
        "rewrite_text": "We provide an evaluation of the kinetic Sunyaev-Zeldovich (kSZ) effect caused by electrons in our universe, utilizing data obtained from the Atacama Cosmology Telescope and the South Pole Telescope. Our analysis employs two distinct methods for estimating the kSZ signal: one involves cross-correlating measurements of Cosmic Microwave Background (CMB) heat anisotropy at 150 GHz and 3000 GHz, while the other relies on the power spectrum of CMB temperature fluctuations at 150 GHz. The findings from both approaches are consistent within their respective uncertainties. Our results indicate that the frequency of the kSZ signal aligns well with theoretical predictions, assuming a Navarro-Frenk-White model for dark matter distribution around galaxies. This measurement provides further constraints on cosmological parameters, including the Hubble constant H0 = 73 ± 4 km s^-1 Mpc^-1, the total matter density parameter Ω_m = 0.27 ± 0.03, and the equation of state parameter w = -1.06 ± 0.11.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.443310539518174,
        "rewrite-fast-z-score": -0.39735970711951313
    },
    {
        "original_text": "We propose an algorithm for reconstructing the signaling network in which each node is associated with multiple time series data, and we assume that only some nodes are observed at any given time point. The proposed method consists of two steps: (1) estimating the state vector by solving a sparse reconstruction problem; (2) inferring the edge set using the estimated states as features. We show through numerical experiments on synthetic networks that our approach can accurately recover both the structure and parameters of the underlying network even when only partial information about the system is available. Our results also suggest that the performance of the proposed method depends strongly on how well the sparsity pattern of the true signal is captured by the observation matrix. Finally, we apply this method to analyze the dynamics of protein phosphorylation in yeast cells responding to heat shock stress. In particular, we identify several key proteins involved in regulating the response process. This work was supported by NIH grant R01GM084283-01A1",
        "watermark_text": "We suggest an algorithm for reconstructing the signaling network in which each node is associated with various time series information , and we suppose that only some nodes are observed at any certain time point . The proposed approach consists of two stages : ( 1 ) estimating the state vector by addressing a sparse reconstruction task ; ( 2 ) inferring the edge setting using the expected states as characteristics .We suggest through quantitative studies on synthetic systems that our approach can accurately recover both the formation and parameters of the underlying systems even when only partial knowledge about the system is accessible . Our results also suggest that the performance of the suggested method depends strongly on how best the sparsity pattern of the true signal is captured by the observation vector .Finally , we apply this technology to analyze the dynamics of gene phosphorylation in yeast cells responding to heat shock pressure . In particular , we identify several key proteins responsible in controlling the response process .This project was supported by NIH funding R01GM084283 - 01A1",
        "rewrite_text": "We propose an algorithm to reconstruct the signaling network, where each node corresponds to various time series data, with the assumption that only certain nodes are observed at any given moment. The proposed method involves two stages: (1) estimating the state vector through a sparse reconstruction process and (2) inferring the connections by using the expected states as features. Through quantitative analyses of synthetic systems, we demonstrate that our approach can effectively recover both the structure and parameters of the underlying systems, even with limited information. Our findings indicate that the effectiveness of the proposed method is heavily influenced by the ability of the observation vector to capture the sparsity pattern of the actual signal. Finally, we apply this methodology to investigate the dynamics of gene phosphorylation in yeast cells under heat shock stress, identifying several key proteins that play crucial roles in regulating the response process. This research was funded by NIH grant R01GM084283 - 01A1.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 7.035623639735144,
        "rewrite-fast-z-score": 1.9877674693472376
    },
    {
        "original_text": "We present new observations at 1.4 GHz and 8.5 GHz with the Australia Telescope Compact Array (ATCA) of the radio afterglow associated to the gamma-ray burst GRB030329, which occurred on March 29th 2003. The data were taken between April 2nd and May 8th 2004, i.e., more than one year after the burst event. We find that the source is still brightening in flux density by about 0.5% per day at both frequencies. This behaviour can be explained if we assume that the fireball has not yet reached its final deceleration radius. In this case, the observed emission would come from an expanding shell of relativistically moving material surrounding the central engine. If so, our results suggest that the energy injection process responsible for the late-time flaring activity detected in other GRBs may have continued up to very large distances from the progenitor system. \n \n Keywords: Gamma-Ray Bursts",
        "watermark_text": "We report new images at 1 . 4 GHz and 8 . 5 GHz with the Australia Telescope Compact Array ( ATCA ) of the radio afterglow related to the alpha - ray burst GRB030329 , which occurred on March 29th 2003 . The data were took between April 2nd and May 8th 2004 , i . e . , more than one month after the explosion event .We see that the source is already brightening in flux concentration by about 0 . 5 % per morning at both frequencies . This behaviour can be described if we suppose that the fireball has not already achieved its final deceleration radius .In this example , the seen emission might come from an evolving shell of relativistically moving material surrounding the main motor . If so , our findings show that the power injection mechanism responsible for the late - time flaring activity detected in other GRBs might have continued up to very big distances from the progenitor system .Keywords: Gamma-Ray Bursts",
        "rewrite_text": "We present new images at 1.4 GHz and 8.5 GHz obtained with the Australia Telescope Compact Array (ATCA) of the radio afterglow associated with the gamma-ray burst GRB030329, which occurred on March 29, 2003. The data were collected between April 2 and May 8, 2004, which is over a month after the explosion. Our observations indicate that the source is brightening at a rate of approximately 0.5% per day at both frequencies. This behavior can be interpreted by assuming that the fireball has not yet reached its final deceleration radius. In this scenario, the observed emission may originate from an evolving shell of relativistically moving material surrounding the central engine. If this is the case, our results suggest that the power injection mechanism responsible for the late-time flaring activity seen in other GRBs may have persisted at significant distances from the progenitor system. Keywords: Gamma-Ray Bursts.",
        "ori-fast-z-score": -1.6035674514745464,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 1.3363062095621219
    },
    {
        "original_text": "We present new observations and analysis of the Balmer-dominated shocks driven by supernova remnants (SNRs) into dense molecular clouds, which are known as  molecular cloud shocks  or  Balmer-dominated shocks . We find that these shocks have an intermediate temperature between those of typical J-type and C-type shocks. The observed emission lines show prominent P-Cygni profiles with blueshifted absorption features indicating high velocities up to 100 km s-1 . These results suggest that the transition zone is located at the interface between the shocked gas and unshocked ambient medium. In addition, we found that the widths of the Hα line profiles increase toward the center of SNR W28. This indicates that the density structure of the surrounding environment may be more complicated than previously thought. Our study suggests that Balmer-dominated molecular cloud shocks can provide important information on the physical conditions of the interstellar medium around young SNRs.",
        "watermark_text": "We report new studies and investigation of the Balmer - dominated shocks driven by supernova remnants ( SNRs ) into thick molecular clouds , which are known as atom cluster shocks or Balmer - dominated shocks . We see that these shocks have an intermediate heat between those of typical J - class and C - class shocks .The observed emission lines show marked P - Cygni profiles with blueshifted emission elements suggesting high velocities up to 100 km s - 1 . These data suggest that the shift area is situated at the interface between the excited gas and unshocked ambient material .In addition , we recovered that the widths of the Hα line profiles increase toward the center of SNR W28 . This implies that the density structure of the nearby region might be more complicated than previously thought .Our study implies that Balmer - dominated molecular storm shocks can provide important information on the physical conditions of the interstellar medium around early SNRs .",
        "rewrite_text": "We present new research on Balmer-dominated shocks produced by supernova remnants (SNRs) interacting with dense molecular clouds, commonly referred to as atom cluster shocks or Balmer-dominated shocks. Our findings indicate that these shocks exhibit a thermal state that falls between typical J-class and C-class shocks. The emission lines we observed display distinct P-Cygni profiles, with blueshifted emission features that suggest high velocities of up to 100 km/s. This data indicates that the shifting region is located at the boundary between the excited gas and the unshocked surrounding material. Furthermore, we found that the widths of the Hα line profiles increase as one moves toward the center of SNR W28, implying that the density structure of this area may be more complex than previously understood. Our study suggests that Balmer-dominated molecular shocks can yield valuable insights into the physical conditions of the interstellar medium surrounding early SNRs.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.861993625888845,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "We present the results of our analysis of high-resolution optical spectra obtained with HST/STIS for four nearby (z<0.1), X-ray selected, Type Ib/c SNe in order to study their progenitor systems. We find that all four objects show evidence for dense CSM surrounding them at distances ranging between 0.01-0.2 pc. The presence of such material is consistent with theoretical expectations for post-low-velocity-bulge (post-LBV) Wolf-Rayet star winds. In addition we detect narrow emission features which are likely due to interaction between SN ejecta and this wind. These observations provide strong constraints on the nature of the progenitor systems: they require massive WR stars as well as binary companions capable of producing significant mass loss prior to explosion. This work was supported by NASA grant NAG5-10842. We have analyzed high resolution STIS/HST data for 4 nearby (z<0.1; Xray-selected) type Ibc supernovae in an attempt to determine the properties of their progenitor systems. All four objects exhibit dense circumstellar matter (CSM; nH>1020 cm-3 ) within 0.01-0.20 parsecs of the supernova site. Such densities are expected if these explosions occur following the ejection of a low velocity  bulge  during late stages of stellar evolution. Furthermore, we observe narrow emission features which may be associated with shock-heating of the CSM by the expanding supernova remnant. Our findings suggest that these events result from the deaths of massive Wolf Rayet stars surrounded by close binaries.",
        "watermark_text": "We present the conclusion of our analysis of high - resolution optical spectra obtained with HST / STIS for four nearby ( z < 0 . 1 ) , X - ray selected , Type Ib / c SNe in order to study their progenitor structures . We see that all four bodies exhibit indication for thick CSM circling them at distances ranging between 0 . 01 - 0 . 2 pc .The presence of such material is compatible with theoretical expectations for post - low - speed - bulge ( post - LBV ) Wolf - Rayet star winds . In addition we find narrow radiation properties which are likely due to contact between SN ejecta and this wind .These measurements give strong restrictions on the nature of the progenitor structures : they use massive WR galaxies as well as binary companions capable of producing significant mass loss prior to explosion . This research was supported by NASA gift NAG5 - 10842 .We have analyzed high resolution STIS / HST results for 4 nearby ( z < 0 . 1 ; Xray - selected ) type Ibc supernovae in an trying to estimate the properties of their progenitor structures . All four bodies exhibit thick circumstellar matter ( CSM ; nH > 1020 cm - 3 ) within 0 . 01 - 0 . 20 parsecs of the supernova center .Such densities are expected if these fires occur following the ejection of a small velocity bulge during late stages of stars evolution . Furthermore , we study narrow radiation properties which may be involved with shock - heating of the CSM by the evolving supernova remnant .Our findings show that these changes result from the deaths of large Wolf Rayet stars surrounded by tight binaries .",
        "rewrite_text": "We conclude our analysis of high-resolution optical spectra acquired with HST/STIS for four nearby (z < 0.1), X-ray-selected Type Ib/c supernovae to investigate their progenitor structures. Our observations reveal that all four supernovae show evidence of substantial circumstellar matter (CSM) surrounding them, located at distances of 0.01 to 0.2 parsecs. The existence of such material aligns with theoretical predictions regarding the winds from post-low-speed-bulge (post-LBV) Wolf-Rayet stars. Additionally, we observe narrow features in the spectra, which likely arise from interactions between the supernova ejecta and this surrounding wind. These results impose significant constraints on the nature of the progenitor systems, suggesting they stem from massive Wolf-Rayet stars and binary companions capable of substantial mass loss prior to the explosion. This research was supported by NASA grant NAG5-10842. Our analysis of the high-resolution STIS/HST data for four nearby (z < 0.1, X-ray-selected) Type Ibc supernovae aims to estimate the characteristics of their progenitor structures. All four supernovae display thick circumstellar matter (CSM; nH > 10^20 cm^-3) within 0.01 to 0.20 parsecs from the supernova center. Such densities are anticipated if these events follow the ejection of low-velocity material during the stars' late evolutionary stages. Furthermore, we investigate the narrow spectral properties that may result from shock heating of the CSM by the expanding supernova remnant. Our findings indicate that these phenomena are linked to the deaths of massive Wolf-Rayet stars in close binary systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.807983207583857,
        "rewrite-fast-z-score": 2.183063390230748
    },
    {
        "original_text": "We present the first detection of secondary CMB polarization induced by Faraday rotation (FR) in galaxy clusters, using data taken with the Atacama Cosmology Telescope Polarimeter (ACTPol). We detect FR-induced polarized emission at angular scales corresponding to multipoles = 100-1000 for two galaxy clusters: ACT-CL J0102-4915 and ACT-CL J0546-5345. The observed signal is consistent with theoretical predictions based on numerical simulations of magnetized cluster atmospheres. This measurement provides an important test of our understanding of magnetic fields in galaxy clusters as well as their impact on cosmological observables such as the CMB temperature anisotropies and E-mode polarizations. In addition, we report upper limits on the FR-induced polarized emissions from other galaxy clusters that are not detected individually due to low S/N ratio or limited survey area. These results will be useful for future studies of FR effects on the CMB polarization maps produced by upcoming experiments like Planck and Simons Observatory.",
        "watermark_text": "We present the first measurement of secondary CMB polarization induced by Faraday rotation ( FR ) in galaxy clusters , using data taken with the Atacama Cosmology Telescope Polarimeter ( ACTPol ) . We detect FR - caused polarized emission at angular scales corresponding to multipoles = 100 - 1000 for two galaxy regions : ACT - CL J0102 - 4915 and ACT - CL J0546 - 5345 .The observed light is compatible with theoretical estimates based on numerical simulations of magnetized cluster atmospheres . This measurement represents an important test of our knowing of magnetic fields in galaxy galaxies as well as their impact on cosmological observables such as the CMB heat anisotropies and E - mode polarizations .In addition , we publish higher restrictions on the FR - caused polarized impacts from other galaxy galaxies that are not observed individually due to low S / N proportion or restricted observation space . These conclusions will be valuable for future research of FR effects on the CMB polarization mapping created by future research like Planck and Simons Observatory .",
        "rewrite_text": "We present the first measurement of secondary CMB polarization induced by Faraday rotation (FR) in galaxy clusters, utilizing data from the Atacama Cosmology Telescope Polarimeter (ACTPol). We observe FR-induced polarized emission at angular scales corresponding to multipoles of 100 - 1000 in two galaxy regions: ACT-CL J0102-4915 and ACT-CL J0546-5345. The detected light aligns with theoretical predictions based on numerical simulations of magnetized cluster atmospheres. This measurement serves as a crucial test of our understanding of magnetic fields in galaxies and their influence on cosmological observables, such as the CMB temperature anisotropies and E-mode polarizations. Furthermore, we provide tighter constraints on FR-induced polarized contributions from other galaxy populations that have not been observed individually due to low signal-to-noise ratios or limited observational coverage. These findings will be instrumental for future investigations of FR effects on CMB polarization mapping in upcoming studies like those conducted by the Planck and Simons Observatory.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 1.9205531989934397
    },
    {
        "original_text": "We present new measurements of the Hubble constant and the equation-of-state parameter w0 using Chandra X-ray Observatory data for the most massive, dynamically relaxed galaxy clusters in the Universe. We use these results to place improved limits on the properties of dark energy. The sample consists of eight galaxy clusters with redshifts between 0.3 and 1.2 that were observed by Chandra as part of our ongoing program to study the evolution of cluster scaling relations out to high redshift. Using hydrostatic equilibrium models we measure the gas mass fraction within r500 (the radius at which the mean density is 500 times the critical density) for each system. These values are combined with independent estimates of the total gravitating mass obtained through weak lensing analysis performed by other groups. This yields an average value of H0 = 70 +/- 6 km s-1 Mpc-1 assuming flat priors on both parameters. If instead we assume Gaussian priors based on previous determinations of the Hubble constant and baryon content of the universe then this measurement becomes H0 = 68 +/-6 km s-1 Mpc-",
        "watermark_text": "We present new studies of the Hubble constant and the equation - of - state variable w0 using Chandra X - ray Observatory data for the most large , dynamically stable galaxy galaxies in the Universe . We use these results to place improved restrictions on the properties of dark energy .The sample consists of eight galaxy galaxies with redshifts between 0 . 3 and 1 . 2 that were detected by Chandra as part of our ongoing program to study the evolution of cluster scaling relations out to large redshift . Using hydrostatic equilibrium models we measure the gas mass fraction within r500 ( the radius at which the mean concentration is 500 times the critical density ) for each system .These values are coupled with independent estimates of the total gravitating mass obtained through soft lensing investigation performed by other organizations . This yields an mean value of H0 = 70 + / - 6 cm s - 1 Mpc - 1 assuming flat priors on both variables .If instead we assume Gaussian priors based on previous determinations of the Hubble constant and baryon concentration of the universe then this measurement becomes H0 = 68 + / - 6 cm s - 1 Mpc -",
        "rewrite_text": "We present new research on the Hubble constant and the equation-of-state parameter \\( w_0 \\) using data from the Chandra X-ray Observatory, focusing on some of the largest and dynamically stable galaxies in the universe. This research enables us to refine constraints on the characteristics of dark energy. Our sample includes eight galaxies with redshifts ranging from 0.3 to 1.2, identified by Chandra as part of our ongoing project to examine the evolution of cluster scaling relations at high redshifts. By employing hydrostatic equilibrium models, we determine the gas mass fraction within \\( r_{500} \\) (the radius at which the mean density is 500 times the critical density) for each galaxy. These measurements are combined with independent estimates of the total gravitating mass derived from weak lensing studies conducted by other teams. This results in a mean Hubble constant of \\( H_0 = 70 \\pm 6 \\text{ km s}^{-1} \\text{ Mpc}^{-1} \\) under the assumption of flat priors for both variables. Conversely, if we apply Gaussian priors based on prior measurements of the Hubble constant and the baryonic matter density in the universe, the estimate for \\( H_0 \\) adjusts to \\( 68 \\pm 6 \\text{ km s}^{-1} \\text{ Mpc}^{-1} \\).",
        "ori-fast-z-score": 1.709408646894569,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 1.091089451179962
    },
    {
        "original_text": "The driving mechanism for jets and outflows is still an open question, especially when the jet/outflow source has no clear central engine such as black holes or protostars. In this work we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation process. We show that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet (RCS). The accelerated electrons will produce synchrotron emission which may explain radio observations of jets and outflows. Furthermore, the energetic protons produced during RCS also contribute to nonthermal emissions through inverse Compton scattering with background photons. Finally, we discuss how our model could account for some observational features of jets and outflows. \n \n Keywords: Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron radiation; Nonthermal emission",
        "watermark_text": "The pushing mechanism for rockets and outflows is also an open question , particularly when the jet / outflow source has no clear central fuel such as black holes or protostars . In this research we propose that magnetic reconnection can be responsible for launching jets and outflows in star formation process .We see that magnetic reconnection can accelerate particles to relativistic energies efficiently via Fermi acceleration at shocks driven by the reconnecting current sheet ( RCS ) . The driven electrons will generate synchrotron emission which would cause radio observations of jets and outflows .Furthermore , the energetic protons generated during RCS also contribute to nonthermal emissions through inverse Compton absorption with background photons . Finally , we talk how our model could account for some observational characteristics of jets and outflows .Keywords : Magnetic reconnection ; Jet ; Particle acceleration ; Shocks ; Synchrotron emission ; Nonthermal emission",
        "rewrite_text": "The mechanisms behind the propulsion of rockets and outflows remain an unresolved area of research, especially when the sources of jets or outflows lack a distinct central fuel source, such as black holes or protostars. In this study, we propose that magnetic reconnection may play a key role in initiating jets and outflows during the star formation process. Our findings indicate that magnetic reconnection can efficiently accelerate particles to relativistic speeds through Fermi acceleration at shocks caused by the reconnecting current sheet (RCS). The accelerated electrons produce synchrotron emission, which can be detected in radio observations of jets and outflows. Additionally, the energetic protons generated during the RCS contribute to nonthermal emissions through inverse Compton scattering with background photons. Lastly, we discuss how our model may explain certain observational features associated with jets and outflows. \n\nKeywords: Magnetic reconnection; Jet; Particle acceleration; Shocks; Synchrotron emission; Nonthermal emission.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 3.8729833462074166,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "The effect of three different sugars (trehalose, maltase and sucrose) on the structure and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case. The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain extent but trehalose is found to be most effective one among them. Trehalose molecule forms hydrogen bonds with both polar and nonpolar amino acid residues which leads to increase in number of water molecules around it. This increases the hydration shell thickness as well as total solvent accessible surface area of the protein. It also decreases the root mean square deviation between initial and final structures indicating its ability to maintain the native conformation of the protein. Maltose and sucrose are less efficient than trehalose because they form only few hydrogen bonds with the protein. In addition, their presence causes slight decrease in the radius of gyration and end-to-end distance of the protein.",
        "watermark_text": "The impact of three different sugars ( trehalose , maltase and sucrose ) on the composition and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case . The results show that all these syrup molecules can stabilize the protein against thermal denaturation to certain degree but trehalose is found to be most efficient one among them .Trehalose atom forms hydrogen bonds with both polar and nonpolar amino acid acids which results to increase in quantity of water molecules around it . This changes the hydration shell thickness as well as gross solvent available surface space of the protein .It additionally decreases the root average square deviation between initial and final structures representing its capacity to keep the native conformation of the protein . Maltose and sucrose are less effective than trehalose because they create only few hydrogen bonds with the protein .In addition , their presence causes mild decrease in the radius of gyration and end - to - end distance of the protein .",
        "rewrite_text": "The effects of three different sugars—trehalose, maltose, and sucrose—on the composition and dynamics of lysozyme were examined using molecular dynamics simulations at 300 K for 100 ns each. The findings indicate that all three sugar molecules can stabilize the protein against thermal denaturation to some extent, with trehalose proving to be the most effective. Trehalose forms hydrogen bonds with both polar and nonpolar amino acids, leading to an increase in the number of water molecules surrounding it. This alters the thickness of the hydration shell and the overall solvent-accessible surface area of the protein. Additionally, trehalose reduces the root mean square deviation between the initial and final structures, demonstrating its ability to maintain the protein's native conformation. In comparison, maltose and sucrose are less effective, as they only form a limited number of hydrogen bonds with the protein. Furthermore, their presence results in a slight decrease in the protein's radius of gyration and end-to-end distance.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "In this work, we revisit some issues on netflow sample and export performance in terms of packet loss rate (PLR) and flow sampling accuracy (FSA). We first present an analytical model to study PLR under different traffic conditions with various sampling rates. Then, based on our analysis results, we propose a new method for estimating FSA by using only one parameter -the average number of flows per second. Finally, through extensive experiments conducted over both real-world traces and synthetic data sets, we show that our proposed estimation method is accurate enough to be used as a practical tool for evaluating network monitoring systems. The rest of this paper is organized as follows. In Section 2, we introduce related works about netflow sampling and exporting. In Section 3, we describe our experimental environment. In Section 4, we analyze the relationship between PLR and sampling rate. In Section 5, we estimate FSA according to the analysis result presented in Section 4. In Section 6, we evaluate the effectiveness of our proposed estimation method via extensive experiments. Finally, conclusions are drawn in Section 7.",
        "watermark_text": "In this research , we revisit some issues on netflow sample and export quality in terms of packet loss rate ( PLR ) and flow sampling accuracy ( FSA ) . We first offer an analytical method to study PLR under various traffic conditions with various sampling rates .Then , using on our analysis results , we propose a new method for estimating FSA by using only one parameter - the average number of flows per second . Finally , through numerous experiments conducted over both real - time traces and synthetic information sets , we prove that our proposed estimation method is accurate sufficient to be used as a practical tool for evaluating network monitoring systems .The rest of this paper is organized as follows . In Section 2 , we provide similar papers about netflow sampling and exporting .In Section 3 , we explain our experimental environment . In Section 4 , we assess the relationship between PLR and sampling rate .In Section 5 , we estimate FSA according to the evaluation result provided in Section 4 . In Section 6 , we assess the performance of our proposed estimation method via extensive experiments .Finally , findings are drawn in Section 7 .",
        "rewrite_text": "In this study, we reexamine several aspects of netflow sampling and export quality, focusing specifically on packet loss rate (PLR) and flow sampling accuracy (FSA). We begin by presenting an analytical approach to analyze PLR across different traffic conditions and sampling rates. Building on our analytical findings, we introduce a novel method for estimating FSA using a single parameter: the average number of flows per second. We validate the accuracy of our proposed estimation method through a series of extensive experiments conducted on both real-time traces and synthetic datasets, demonstrating its practicality for evaluating network monitoring systems. The remainder of this paper is structured as follows: Section 2 reviews related literature on netflow sampling and exporting; Section 3 describes our experimental setup; Section 4 examines the relationship between PLR and sampling rate; Section 5 estimates FSA based on the results from Section 4; Section 6 evaluates the performance of our estimation method through comprehensive experiments; and finally, Section 7 presents our conclusions.",
        "ori-fast-z-score": 1.8973665961010275,
        "water-fast-z-score": 5.9752235693149345,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "We present new observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) of two inter-network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25, respectively. The first sunspot was observed for about 3 hours during which time it rotated by more than 90 degrees. We find that this sunspot is composed of several magnetic flux tubes with different orientations. In addition to these features we also observe an extended bright feature located between the main sunspot umbrae. This feature has been previously reported as a penumbral filament but our data show no evidence of such structure. Instead, we interpret this feature as a coronal rain blob. The second sunspot was observed for only 1 hour before being occulted by Earths atmosphere. During this observation period the sunspot rotated by less than 30 degrees. Our analysis shows that both sunspots are surrounded by a dark lane which may be associated with the moat surrounding large sunspots.",
        "watermark_text": "We report new images making with the Atacama Large Millimeter / submillimeter Array ( ALMA ) of two cross - network sunspots in active region NOAA AR 12192 on 2013 May 24 and 25 , respectively . The first sunspot was seen for about 3 hours during which period it rotated by more than 90 degrees .We see that this sunspot is composed of several magnetic flux tubes with various orientations . In addition to these characteristics we also observe an extended bright point located between the main sunspot umbrae .This phenomenon has been previously reported as a penumbral filament but our statistics demonstrate no evidence of such structure . Rather , we view this phenomenon as a coronal weather blob .The second sunspot was seen for only 1 hour before being occulted by Earths atmosphere . During this study interval the sunspot tilted by less than 30 degrees .Our study shows that both sunspots are surrounded by a darkness lane which may be identified with the moat surrounding large sunspots .",
        "rewrite_text": "We present new images obtained with the Atacama Large Millimeter/Submillimeter Array (ALMA) of two cross-network sunspots located in active region NOAA AR 12192, captured on May 24 and 25, 2013. The first sunspot was observed for approximately three hours, during which it rotated more than 90 degrees. Analysis reveals that this sunspot consists of multiple magnetic flux tubes oriented in various directions. Additionally, we detected an extended bright point situated between the main sunspot umbrae, which has been previously identified as a penumbral filament; however, our statistical analysis found no evidence supporting this interpretation. Instead, we classify this feature as a coronal weather blob. The second sunspot was visible for only one hour before being obscured by Earth's atmosphere, tilting less than 30 degrees during the observation. Our findings indicate that both sunspots are encircled by a dark lane, likely corresponding to the moat that typically surrounds larger sunspots.",
        "ori-fast-z-score": -1.0327955589886444,
        "water-fast-z-score": 5.163977794943222,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We present the results obtained by applying the semi-analytic code VESPA to model the evolution of galaxies in the Millennium Simulation, including chemical enrichment as well as dust extinction effects on their observed properties.  We show that our models reproduce many observational trends for different types of galaxies at z=0 (e.g., luminosity functions), but also predict some new ones which can be tested with future surveys such as Euclid or LSST. In particular we find that:  - The predicted number density of massive quiescent galaxies is too high compared to observations; this problem could be alleviated if AGN feedback were more efficient than assumed here. - Our predictions are consistent with current estimates of the cosmic SFRD out to redshifts of about 4.5, although they tend to overpredict it slightly beyond that redshift. - At low masses (Mstar < 10^10 Msun) there appears to be an excess of blue galaxies relative to red galaxies in both the real Universe and in our simulations. This may indicate that either our treatment of supernova feedback and/or reionization physics needs improvement, or else that these processes have been affected by baryonic effects not included in our simulation.",
        "watermark_text": "We present the results derived by using the semi - analytic coding VESPA to model the evolution of galaxies in the Millennium Simulation , notably chemical enrichment as well as dust extinction effects on their observed properties . We suggest that our designs represent many observational changes for different kinds of stars at z = 0 ( e . g . , luminosity functions ) , but also predict some additional ones which can be evaluated with current surveys such as Euclid or LSST .In particular we find that : - The predicted number density of large quiescent galaxies is too high compared to observations ; this situation could be alleviated if AGN feedback were more efficient than implied here . - Our predictions are compatible with current estimates of the cosmic SFRD out to redshifts of about 4 . 5 , although they tend to overpredict it somewhat beyond that redshift .- At small masses ( Mstar < 10 ^ 10 Msun ) there seems to be an accumulation of blue stars relative to blue stars in both the real Universe and in our simulations . This might suggest that either our treatment of supernova feedback and / or reionization theory requires improvement , or otherwise that these mechanisms have been affected by baryonic effects not involved in our modeling .",
        "rewrite_text": "We present findings obtained using the semi-analytic code VESPA to model galaxy evolution within the Millennium Simulation, focusing particularly on chemical enrichment and dust extinction influences on observed properties. We propose that our models reflect many observable variations among different types of stars at redshift z = 0 (such as luminosity functions) and also forecast additional changes that can be assessed through current surveys like Euclid and LSST. Specifically, our results indicate that: - The estimated number density of large quiescent galaxies is higher than what observations suggest; this discrepancy might be mitigated if AGN feedback is more effective than currently assumed. - Our predictions align with present estimates of the cosmic Star Formation Rate Density (SFRD) up to redshifts around 4.5, although they slightly overestimate it beyond that threshold. - For lower mass galaxies (Mstar < 10^10 Msun), there appears to be a surplus of blue stars in both our simulations and the real Universe, which could imply that our treatments of supernova feedback and/or reionization theory need refinement, or that these processes are influenced by baryonic effects not accounted for in our models.",
        "ori-fast-z-score": -1.3251783128981585,
        "water-fast-z-score": 5.888888888888889,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "The effect of silver and indium on the magnetic properties, electrical resistivity (ER) and Hall coefficient (R H ) has been investigated in La 2/3 Ca 1/3 Mn O 3 . The results show that ER decreases with increasing temperature for all samples while R H increases with decreasing temperature. It is found that both silver and indium doping decrease T C , increase J c and enhance pinning force density F p . Silver doped sample shows higher values of J c than indium doped one at low temperatures but lower value at high temperatures.  These results are explained by considering different effects of silver and indium ions on the microstructure as well as their influence on oxygen vacancies concentration. This work was supported by the National Natural Science Foundation of China under Grant No. 50571040. We would like to thank Prof. Y. M. Wu for his help during this research. Abstract: In this study we have prepared two series of La 2/3 Ca 1/3 MnO 3 :Ag and La 2/3 Ca 1/3 MnO3 :In polycrystalline composite materials using solid state reaction method. X-ray powder diffraction patterns confirm single phase formation without any impurity peaks. The structural parameters such as lattice constant, unit cell volume and bond length were calculated from XRD data. The dc magnetization measurements reveal that Curie temperature (Tc), critical current density (Jc) and pinning force density (Fp) decrease with increasing amount of silver or indium content.",
        "watermark_text": "The impact of silver and indium on the magnetic properties , thermal resistivity ( ER ) and Hall coefficient ( R H ) has been investigated in La 2 / 3 Ca 1 / 3 Mn O 3 . The results show that ER decreases with expanding temperature for all specimens while R H increases with varying temperature .It is found that both silver and indium doping change T C , enhance J c and enhance pinning power concentration F p . Silver doped specimen shows better levels of J c than indium doped one at low temperatures but smaller value at high temperatures .These data are explained by examining different impacts of silver and indium ions on the microstructure as also as their influence on oxygen vacancies density . This research was supported by the National Natural Science Foundation of China under Grant No .50571040.We would like to thank Prof. Y. M. Wu for his help during this research.Abstract : In this study we have formed two sequence of La 2 / 3 Ca 1 / 3 MnO 3 : Ag and La 2 / 3 Ca 1 / 3 MnO3 : In polycrystalline composite materials utilizing solid state process method . X - ray powder diffraction patterns indicate single phase form without any impurity peaks .The structural values such as structure constant , unit cell size and bond length were calculated from XRD information . The dc magnetization calculations reveal that Curie temperature ( Tc ) , vital current density ( Jc ) and pinning power concentration ( Fp ) decline with varying amount of silver or indium content .",
        "rewrite_text": "The influence of silver and indium on the magnetic properties, thermal resistivity (ER), and Hall coefficient (RH) of La2/3Ca1/3MnO3 has been explored. The findings indicate that ER decreases with increasing temperature across all samples, whereas RH rises with temperature variation. Both silver and indium doping have been shown to affect the Curie temperature (TC), improve critical current density (Jc), and enhance the pinning power concentration (Fp). At low temperatures, the silver-doped samples demonstrate higher Jc values compared to the indium-doped ones, although the opposite is observed at elevated temperatures. These observations can be attributed to the differing effects of silver and indium ions on the material's microstructure, as well as their impact on the density of oxygen vacancies. This research was supported by the National Natural Science Foundation of China under Grant No. 50571040. We extend our gratitude to Prof. Y. M. Wu for his assistance throughout this research. \n\nAbstract: In this study, we synthesized two series of polycrystalline composite materials, La2/3Ca1/3MnO3:Ag and La2/3Ca1/3MnO3:In, using a solid-state processing method. X-ray powder diffraction patterns reveal a single-phase formation without any impurity peaks. The structural characteristics, including the structure constant, unit cell dimensions, and bond lengths, were determined from the XRD data. The DC magnetization measurements indicate that the Curie temperature (TC), critical current density (Jc), and pinning power concentration (Fp) decrease as the content of silver or indium varies.",
        "ori-fast-z-score": 0.8081220356417685,
        "water-fast-z-score": 7.677159338596802,
        "rewrite-fast-z-score": 1.584236068762679
    },
    {
        "original_text": "The discovery space for astronomical research is vast, with many different types of surveys being conducted at all wavelengths across the electromagnetic spectrum. In this talk I will discuss how wide field optical imaging surveys have been used to discover new classes of objects in our Universe such as quasars, galaxies, clusters of galaxies, supernovae, gamma ray bursts etc., and also how these surveys are now providing data on dark energy which drives cosmic acceleration. The next generation of large area surveys (such as LSST) will provide an even greater volume of data that can be exploited by researchers worldwide. This talk will give examples of some recent results obtained using data from current and past surveys including the Sloan Digital Sky Survey (SDSS), Panoramic Survey Telescope & Rapid Response System 1 (Pan-STARRS1), Dark Energy Survey (DES), VISTA Kilo-Degree Infrared Galaxy survey (VIKING).",
        "watermark_text": "The discovery area for astronomical investigations is vast , with many various types of surveys being performed at all wavelengths across the electromagnetic spectrum . In this talk I will explore how wide field visual imaging observations have been used to find new classes of bodies in our Universe such as quasars , galaxies , clusters of stars , supernovae , alpha ray bursts etc . , and also how these observations are now offering data on dark energy which explains cosmic acceleration .The future generation of large area surveys ( such as LSST ) will provide an much larger volume of statistics that can be exploited by researchers worldwide . This discussion will provide descriptions of some latest findings obtained using data from recent and previous surveys including the Sloan Digital Sky Survey ( SDSS ) , Panoramic Survey Telescope & Rapid Response System 1 ( Pan - STARRS1 ) , Dark Energy Survey ( DES ) , VISTA Kilo - Degree Infrared Galaxy survey ( VIKING ) .",
        "rewrite_text": "The realm of astronomical exploration is extensive, encompassing a diverse array of surveys conducted across all wavelengths of the electromagnetic spectrum. In this presentation, I will discuss how wide-field visual imaging has facilitated the discovery of new types of celestial objects, including quasars, galaxies, star clusters, supernovae, and gamma-ray bursts. Additionally, I will highlight how these observations are yielding valuable insights into dark energy, which accounts for the acceleration of the cosmos. Upcoming large-scale surveys, such as the Legacy Survey of Space and Time (LSST), are expected to generate a significantly larger dataset that researchers around the globe can utilize. This discussion will showcase recent discoveries made using data from various surveys, including the Sloan Digital Sky Survey (SDSS), Pan-STARRS1, the Dark Energy Survey (DES), and the VISTA Kilo-Degree Infrared Galaxy Survey (VIKING).",
        "ori-fast-z-score": -1.8325416653445783,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.4320780207890627
    },
    {
        "original_text": "We study anisotropy in the ac response of mesoscopic superconducting films with respect to the direction of an applied magnetic field, using numerical simulations based on the quasiclassical theory for disordered metals and the Usadel equations. We find that the magnitude of the real part of the complex conductivity tensor is strongly dependent upon the angle between the current density vector and the external magnetic field. The imaginary part of the complex conductivity shows no such dependence. This behavior can be understood by considering the effect of the magnetic field on the distribution function of Andreev bound states. Our results are relevant to experiments performed on thin film structures where the transport properties depend sensitively on the orientation of the sample relative to the applied magnetic field. \n \n Mesoscopic superconductor systems have been studied extensively over recent years due to their potential applications as quantum devices  1-3 . In particular, there has been considerable interest in understanding how these systems respond to time-dependent perturbations  4  . For example, it was recently shown experimentally  5  , that when a dc bias voltage Vdc = 0 is applied across a Josephson junction array (JJA), the system exhibits hysteretic switching between two different resistive states which occur at critical values of the amplitude of the alternating current Vac. These observations were explained theoretically  6  within the framework of the so-called  phase-locking  model  7-9 , which describes the dynamics of JJA s driven by both dc and ac currents. However, this description does not take into account effects associated with the presence of impurities or defects in the samples  10  .\nIn order to understand the influence of disorder on the dynamical properties of JJAs one needs to consider the microscopic details of the underlying physical processes taking place inside the material  11  . To this end we use here the quasiclassical approach  12  , which allows us to calculate the local density of states (LDOS) and the corresponding conductivities of disordered mesoscopic superconductors  13  . Within this formalism, the LDOS is determined self-consistently from the solution of the Usadel equation  14  \nwhere D(E) is the",
        "watermark_text": "We research anisotropy in the ac response of mesoscopic superconducting films with regard to the direction of an applied magnetic force , using numerical simulations based on the quasiclassical principle for disordered metals and the Usadel equations . We see that the severity of the real part of the complex conductivity tensor is strongly dependent upon the angle between the current density vector and the external magnetic force .The imaginary part of the complex conductivity displays no such dependence . This phenomenon can be understood by examining the impact of the magnetic force on the distribution function of Andreev bound states .Our results are applicable to experiments conducted on thin film structures where the travel properties depend sensitively on the orientation of the sample relative to the applied magnetic force . Mesoscopic superconductor systems have been studied frequently over recent history owing to their potential applications as quantum devices 1 - 3 .In particular , there has been substantial interest in understanding how these systems behave to time - based perturbations 4 . For instance , it was recently shown experimentally 5 , that when a dc bias voltage Vdc = 0 is applied across a Josephson junction array ( JJA ) , the circuit exhibits hysteretic alternating between two different resistive states which occur at critical values of the frequency of the alternating current Vac .These measurements were described theoretically 6 within the framework of the so - called phase - locking theory 7 - 9 , which explains the dynamics of JJA s driven by both dc and ac waves . However , this description does not take into consideration phenomena associated with the presence of impurities or defects in the samples 10 .In order to comprehend the impact of disorder on the dynamical properties of JJAs one needs to consider the microscopic aspects of the fundamental physical processes taking place inside the material 11 . To this end we utilize here the quasiclassical method 12 , which allows us to estimate the local concentration of states ( LDOS ) and the equivalent conductivities of disordered mesoscopic superconductors 13 .Within this formalism , the LDOS is calculated self - continuously from the solve of the Usadel equation 14 where D ( E ) is the",
        "rewrite_text": "We investigate the anisotropy in the alternating current (ac) response of mesoscopic superconducting films as a function of the direction of an applied magnetic force. This study utilizes numerical simulations grounded in the quasiclassical theory for disordered metals and the Usadel equations. Our findings indicate that the magnitude of the real part of the complex conductivity tensor is highly sensitive to the angle between the current density vector and the external magnetic force, while the imaginary part of the complex conductivity remains unaffected by this orientation. This behavior can be attributed to the influence of the magnetic force on the distribution function of Andreev bound states. Our results are relevant for experiments involving thin-film structures, where the transport properties are closely linked to the sample's orientation relative to the applied magnetic force. Mesoscopic superconducting systems have garnered significant attention due to their potential applications in quantum devices. Recent studies have particularly focused on their responses to time-dependent perturbations. For example, it was recently demonstrated that when a direct current (dc) bias voltage (Vdc = 0) is applied across a Josephson junction array (JJA), the circuit exhibits hysteretic behavior, alternating between distinct resistive states at critical alternating current (ac) frequencies (Vac). These experimental observations were theoretically framed within phase-locking theory. However, this theoretical model does not account for the effects of impurities or defects present in the samples. To fully understand how disorder impacts the dynamical properties of JJAs, it is crucial to examine the microscopic details of the fundamental physical processes occurring within the material. In this context, we apply the quasiclassical method to estimate the local density of states (LDOS) and the effective conductivities of disordered mesoscopic superconductors. Within this framework, the LDOS is calculated through a self-consistent solution of the Usadel equation, where D(E) represents the...",
        "ori-fast-z-score": -1.087114613009218,
        "water-fast-z-score": 6.833333333333333,
        "rewrite-fast-z-score": 0.6761234037828132
    },
    {
        "original_text": "We present an approach to the assimilation of magnetic field observations into numerical models using variational methods. The method is applied to a simple model problem in which we consider the evolution of a single component of the magnetic vector potential in a two-dimensional domain with periodic boundary conditions. We use this example as a testbed to explore how different choices of observation operator affect the quality of the resulting analysis fields. In particular, we compare results obtained by applying our algorithm directly on the state variable (the magnetic vector potential) against those obtained when it is first projected onto a set of basis functions that are chosen to be optimal for representing the solution at each time step. Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings. Finally, we demonstrate the utility of our approach by performing a series of twin experiments in which synthetic magnetometer measurements are used to update the initial condition of the simulation.",
        "watermark_text": "We present an approach to the assimilation of magnetic field observations into numerical models using variational techniques . The method is applied to a simple simulation problem in which we study the evolution of a single part of the magnetic vector potential in a two - dimensional domain with periodic boundary constraints .We use this example as a testbed to examine how various choosing of observation operator impact the performance of the resulting assessment fields . In particular , we compare findings obtained by using our technique directly on the state variable ( the magnetic vector potential ) against those achieved when it is initially projected onto a setting of basis functions that are chosen to be appropriate for describing the solution at each time step .Our experiments show that both approaches yield similar results but that the latter can lead to significant computational savings . Finally , we prove the utility of our approach by performing a sequence of twin tests in which synthetic magnetometer measurements are applied to modify the early condition of the model .",
        "rewrite_text": "We propose a method for integrating magnetic field observations into numerical models through variational techniques. This approach is tested on a simplified simulation where we analyze the evolution of a specific segment of the magnetic vector potential within a two-dimensional domain under periodic boundary conditions. We utilize this scenario as a testing ground to evaluate the effects of different choices of observation operators on the performance of the resulting assessment fields. Specifically, we compare results obtained by applying our method directly to the state variable (the magnetic vector potential) with those obtained by projecting it onto a set of basis functions selected to effectively represent the solution at each time step. Our experiments indicate that both methods yield comparable results, although the latter offers significant computational advantages. Furthermore, we demonstrate the effectiveness of our approach through a series of twin tests, where synthetic magnetometer measurements are used to modify the initial conditions of the model.",
        "ori-fast-z-score": -0.8528028654224417,
        "water-fast-z-score": 5.330017908890261,
        "rewrite-fast-z-score": 0.9649012813540153
    },
    {
        "original_text": "We study theoretically and numerically the effect of spatial dispersion (SD) on the shape of a light pulse propagating through an InGaAs/GaAs quantum well (QW). We show that SD leads to significant changes in the temporal profile of the transmitted pulse, which can be used for its characterization. The results are obtained by solving Maxwell s equations using the finite-difference time-domain method with periodic boundary conditions. It is shown that the presence of SD causes the appearance of additional peaks at both sides of the main peak of the transmitted pulse. These peaks become more pronounced as the QW width increases. \n \n Keywords: Light propagation, Finite difference time domain method, Quantum wells, Spatial dispersion. 1 Introduction \n \n A number of recent studies have been devoted to investigating the effects of spatial dispersion (SD), also known as nonlocality or transverse momentum conservation  1  , on various physical phenomena such as nonlinear wave dynamics  2  -  4  , spontaneous emission  5  , and transport  6  . This interest has been motivated mainly by the fact that many semiconductor devices operate under conditions where SD plays an important role  7, 8  .\n \nIn this work we consider the problem of light transmission through a single-mode quantum well (QW) structure  9  . Our aim is to investigate how SD affects the shape of the transmitted pulse. To do so, we solve Maxwell s equations using the finitedifference time-domain (FDTD) method  10  with periodic boundary conditions  11  . As it will be demonstrated below, our numerical simulations reveal that SD gives rise to new features in the temporal profile of a transmitted pulse.",
        "watermark_text": "We research theoretically and numerically the impact of spatial dispersion ( SD ) on the morphology of a light pulse propagating through an InGaAs / GaAs quantum well ( QW ) . We see that SD leads to significant improvements in the temporal profile of the transmitted signal , which can be used for its characterization .The results are derived by treating Maxwell s coefficients using the finite - variation time - domain approach with periodic boundary constraints . It is demonstrated that the presence of SD causes the appearance of new peaks at both sides of the main peak of the transmitted signal .These peaks become more pronounced as the QW width rises . Keywords : Light propagation , Finite difference time domain approach , Quantum wells , Spatial dispersion .1 Introduction A couple of recent studies have been focused to investigating the effects of spatial dispersion ( SD ) , sometimes called as nonlocality or transverse momentum conservation 1 , on various physical phenomena such as nonlinear wave interactions 2 - 4 , spontaneous emission 5 , and transport 6 . This interest has been motivated mainly by the fact that several semiconductor devices exist under environments where SD plays an important role 7 , 8 .In this research we study the issue of light transport through a single - mode quantum well ( QW ) structure 9 . Our aim is to examine how SD impacts the form of the transmitted signal .To do so , we solve Maxwell s coefficients utilizing the finitedifference time - domain ( FDTD ) method 10 with periodic boundary constraints 11 . As it will be showed below , our numerical simulations reveal that SD offers rise to novel features in the temporal profile of a broadcast pulse .",
        "rewrite_text": "We conduct both theoretical and numerical research on the effects of spatial dispersion (SD) on the morphology of a light pulse as it propagates through an InGaAs/GaAs quantum well (QW). Our findings indicate that SD significantly enhances the temporal characteristics of the transmitted signal, which can be utilized for signal characterization. These results are obtained by applying Maxwell's equations within a finite-variation time-domain framework, along with periodic boundary conditions. We demonstrate that the presence of SD leads to the emergence of new peaks on either side of the primary peak of the transmitted signal, with these peaks becoming more pronounced as the width of the QW increases. \n\nKeywords: Light propagation, Finite difference time domain approach, Quantum wells, Spatial dispersion.\n\n1 Introduction\nRecent studies have increasingly focused on the effects of spatial dispersion (SD), also referred to as nonlocality or transverse momentum conservation, on various physical phenomena, including nonlinear wave interactions, spontaneous emission, and transport. This growing interest is largely due to the critical role that SD plays in the operation of various semiconductor devices. In this research, we investigate light transport through a single-mode quantum well (QW) structure, aiming to analyze the influence of SD on the shape of the transmitted signal. To achieve this, we employ the finite difference time-domain (FDTD) method, incorporating periodic boundary conditions. Our numerical simulations reveal that SD introduces novel features in the temporal profile of the broadcast pulse.",
        "ori-fast-z-score": -0.8181818181818182,
        "water-fast-z-score": 6.518573715061334,
        "rewrite-fast-z-score": 1.9250668437592438
    },
    {
        "original_text": "We present new measurements of rotational widths (W20) and surface brightnesses (SB) for a sample of galaxies with inclinations between 30°and 80°, drawn from the Sloan Digital Sky Survey Data Release 7. We find that W20 is correlated strongly with SB at fixed luminosity, but only weakly or not at all with galaxy mass. This correlation persists even when we restrict our analysis to late-type spirals, which are known to have flat rotation curves. These results suggest that the observed scatter in the Tully-Fischer relation may be due primarily to variations in SB among galaxies of similar luminosities rather than differences in their masses. In addition, we show that this effect can explain why previous studies found no significant dependence on inclination angle in the TF relation. Finally, we demonstrate how these correlations affect estimates of the Hubble constant derived using the TF relation. Our findings also provide an explanation for the apparent discrepancy between the values obtained by different authors who used samples selected over different ranges of inclination angles.",
        "watermark_text": "We present new measurements of rotational widths ( W20 ) and surface brightnesses ( SB ) for a sample of galaxies with inclinations between 30°and 80° , obtained from the Sloan Digital Sky Survey Data Release 7 . We see that W20 is associated strongly with SB at fixed luminosity , but only strongly or not at all with star mass .This correlation persists even when we limit our analysis to late - class spirals , which are known to have flattened rotation curves . These conclusions show that the seen scatter in the Tully - Fischer relation might be due primarily to variations in SB among galaxies of comparable luminosities rather than differences in their masses .In addition , we prove that this effect can reason why previous research found no considerable dependence on tilt angle in the TF relation . Finally , we prove how these correlations influence estimates of the Hubble constant calculated using the TF relation .Our findings also suggest an reason for the alleged discrepancy between the estimates obtained by various scientists who used data selected over particular ranges of inclination distances .",
        "rewrite_text": "We report new measurements of rotational widths (W20) and surface brightnesses (SB) for a sample of galaxies with inclinations ranging from 30° to 80°, derived from the Sloan Digital Sky Survey Data Release 7. Our analysis reveals a strong correlation between W20 and SB at fixed luminosity, while the relationship with stellar mass is either weak or negligible. This correlation remains robust even when focusing specifically on late-type spirals, which are characterized by their flattened rotation curves. These findings suggest that the observed scatter in the Tully-Fisher relation may primarily stem from variations in surface brightness among galaxies with similar luminosities, rather than differences in their masses. Furthermore, we demonstrate that this phenomenon could explain why previous studies found little to no significant dependence on tilt angle in the Tully-Fisher relation. Lastly, we discuss how these correlations affect estimates of the Hubble constant derived from the Tully-Fisher relation, and we offer an explanation for the apparent discrepancies between estimates obtained by different researchers using data selected within specific ranges of inclination distances.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.11396057645963795
    },
    {
        "original_text": "We present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.",
        "watermark_text": "We present new surveys of the solar atmosphere acquired with the Solar Ultraviolet Imager ( SUVI ) onboard the Coriolis spacecraft , which show dynamic fibrils in both H - alpha and CIV lines . The SUVI images are compared to simultaneous ground - based observations made at Big Bear Observatory using the 1 - meter telescope fitted with an H - Alpha filter and a Fabry - Perot interferometer tuned to the CIV line .We see that the seen features have related properties as those saw previously by other researchers but we also saw some variations between them . In particular , our findings confirm that the fibril structure is more sophisticated than it was considered before .This difficulty must be connected to the fact that these structures are created under distinct physical conditions . Our study shows that the seen features can be described by assuming that they represent plasma flows along magnetic field lines .These flows could play major part in heating up the higher layers of the solar atmosphere .",
        "rewrite_text": "We present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) aboard the Coriolis spacecraft, revealing dynamic fibrils in both H-alpha and CIV lines. These SUVI images are compared with simultaneous ground-based data collected at Big Bear Observatory using a 1-meter telescope equipped with an H-alpha filter and a Fabry-Perot interferometer tuned to the CIV line. Our findings indicate similarities in the features observed, as previously reported by other researchers, but also highlight some variations. Notably, our results confirm that the fibril structure is more complex than previously understood, likely due to their formation under different physical conditions. Our study suggests that these features can be interpreted as plasma flows along magnetic field lines, which may play a significant role in heating the upper layers of the solar atmosphere.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.277372492166241,
        "rewrite-fast-z-score": 0.601929265428846
    },
    {
        "original_text": "We show that the answer to this question is no, at least in principle. We consider two models for dark energy which are observationally indistinguishable but have different physical origins; one is due to vacuum fluctuations while the other arises as an effective field theory description of quantum gravity effects. In both cases there exists a parameter space where the predictions for the evolution of the universe agree with current observations within experimental uncertainties. However, if future experiments can measure the equation-of-state of dark energy more accurately then it may be possible to differentiate these two scenarios. The discovery of cosmic acceleration has led to much interest in the possibility that our universe contains some form of  dark energy ; see e.g.,  1  . This exotic component would drive the expansion of the universe today and dominate its dynamics on large scales. A number of theoretical proposals exist for what such a dark energy might consist of (see  2  for a review). One particularly interesting class of possibilities involves introducing new fields into Einstein s equations whose presence leads to repulsive gravitational forces  3  .\nIn recent years many authors have studied the phenomenology associated with various forms of dark energy; see  4  -  8  ,  10  -  12  ,  14  -  16  ,  18  -  20  ,  22  -  26  ,  28  -  30  ,  32  -  38  ,  41  -  44  ,  46  -  48  ,  50  -  52  ,  54  -  61  ,  63  -  65  ,  67  -  71  ,  73  -  75  ,  77  -  81  ,  83  -  85  ,  88  -  92  ,  94  -  103  . Many of these works focus on comparing specific models against observational data or studying their implications for fundamental physics. Here we take a complementary approach by considering whether any two distinct models could give rise to identical observable consequences.",
        "watermark_text": "We see that the response to this question is no , at least in principle . We consider two models for black energy which are observationally indistinguishable but have different physical origins ; one is due to vacuum fluctuations while the other arises as an useful field model representation of quantum gravitational interactions .In both cases there exists a parameter room where the estimates for the evolution of the universe comply with current observations within experimental uncertainties . However , if future research can measure the equation - of - state of dark energy more accurately then it could be possible to differentiate these two scenarios .The observation of universe acceleration has led to great discussion in the prospect that our universe possesses some kind of dark energy ; seeing e . g . , 1 . This exotic component might power the evolution of the universe today and influence its mechanics on huge scales .A variety of theoretical proposals exist for what such a black force may consist of ( hear 2 for a review ) . One especially interesting class of possibilities includes introducing additional fields into Einstein s equations whose presence contributes to repulsive gravitational pressures 3 .In past decades several authors have researched the phenomenology identified with various forms of dark energy ; see 4 - 8 , 10 - 12 , 14 - 16 , 18 - 20 , 22 - 26 , 28 - 30 , 32 - 38 , 41 - 44 , 46 - 48 , 50 - 52 , 54 - 61 , 63 - 65 , 67 - 71 , 73 - 75 , 77 - 81 , 83 - 85 , 88 - 92 , 94 - 103 . Many of these works concentrate on comparing actual models against observational data or exploring their implications for theoretical physics .Here we took a complementary perspective by examining whether any two separate models could give rise to identical observable effects .",
        "rewrite_text": "We conclude that the answer to this question is no, at least in theory. We explore two models of dark energy that, while indistinguishable based on observations, stem from different physical origins: one is associated with vacuum fluctuations, and the other serves as a helpful field model representing quantum gravitational interactions. In both scenarios, there exists a range of parameters where the predictions for the universe's evolution align with current observational data within experimental uncertainties. However, if future research can achieve a more precise measurement of the dark energy equation of state, it may become feasible to differentiate between these two models. The observed acceleration of the universe has spurred considerable discussion regarding the presence of dark energy, which could be responsible for the universe's current expansion and affect its dynamics on a grand scale. Numerous theoretical proposals exist about the nature of this mysterious force (see for a review). One particularly intriguing avenue involves the introduction of additional fields into Einstein's equations, which could contribute to repulsive gravitational effects. In recent decades, various authors have investigated the phenomenology associated with different forms of dark energy. Many studies focus on comparing specific models with observational data or exploring their implications for theoretical physics. In this context, we adopted a complementary approach by examining whether different models could produce identical observable effects.",
        "ori-fast-z-score": -0.44367825470805694,
        "water-fast-z-score": 8.131727983645296,
        "rewrite-fast-z-score": 2.090909090909091
    },
    {
        "original_text": "We present the results of an optical integral-field spectroscopy (IFS) survey to find high-redshift Damped Lyman Alpha Galaxies (DLAs). The sample consists of 16 DLAs at redshifts between 2 and 4, selected by their strong Mg II absorption lines in quasar spectra observed with the Keck telescope. We use the Gemini MultiObject Spectrograph on the 8-meter Gemini North Telescope to obtain spatially resolved observations over a 1 arcsec2 area centered around each absorber. Our IFS data are used to study the kinematics of these absorbers as well as their physical properties such as metallicity and dust content. In addition we compare our new measurements with previous studies based on single-aperture spectrophotometry. This work is part of the Gemini Observatory program GN-2007B-Q-206. Keywords: High-redshift universe; Galaxy evolution; Absorber-galaxy relationship",
        "watermark_text": "We present the conclusion of an optical integral - field spectroscopy ( IFS ) study to find high - redshift Damped Lyman Alpha Galaxies ( DLAs ) . The sample consists of 16 DLAs at redshifts between 2 and 4 , selected by their weak Mg II absorb lines in quasar spectra observed with the Keck lens .We use the Gemini MultiObject Spectrograph on the 8 - meter Gemini North Telescope to obtain spatially resolved surveys over a 1 arcsec2 region focused around each absorber . Our IFS data are applied to study the kinematics of these absorbers as well as their physical properties such as metallicity and dust content .In addition we compare our new data with previous research based on single - lens spectrophotometry . This research is part of the Gemini Observatory program GN - 2007B - Q - 206 .Keywords : High - redshift universe ; Galaxy evolve ; Absorber - galaxy relationship",
        "rewrite_text": "We present the findings from an optical integral-field spectroscopy (IFS) study aimed at identifying high-redshift Damped Lyman Alpha Galaxies (DLAs). Our sample includes 16 DLAs located at redshifts between 2 and 4, selected due to their weak Mg II absorption lines identified in quasar spectra captured by the Keck telescope. Using the Gemini MultiObject Spectrograph attached to the 8-meter Gemini North Telescope, we conducted spatially resolved surveys of a 1 arcsec² area centered on each absorber. Our IFS data enable us to investigate the kinematics of these absorbers as well as their physical characteristics, including metallicity and dust content. Additionally, we compare our new findings with previous research that relied on single-lens spectrophotometry. This study is an integral part of the Gemini Observatory program GN-2007B-Q-206. Keywords: High-redshift universe; Galaxy evolution; Absorber-galaxy relationship.",
        "ori-fast-z-score": -0.29488391230979427,
        "water-fast-z-score": 3.8334908600273256,
        "rewrite-fast-z-score": 0.2773500981126146
    },
    {
        "original_text": "We report on multiwavelength observations of the X-ray, optical and radio afterglows of the short-hard gamma-ray burst (GRB) 050724 detected by Swift satellite at 07:24:06 UT on 24 July 2005. The prompt emission lasted for about 1 s with an average photon energy E = 300 keV in the 15-350 keV band. We find that the temporal decay index is ~1.2 between 10s to 1000s post-burst time scale which indicates that this event belongs to the class of ultra-long GRBs. In addition we also detect a possible rebrightening feature around 100s post-burst time-scale. Our spectral analysis shows that the spectrum can be fitted well using both single power-law model as well as broken power law model. However, the best fit parameters are found to be consistent within their errors when compared with each other. Using our multi-wavelength data set, we estimate the total energetics associated with this event to be ~3 x 1044 erg.",
        "watermark_text": "We report on multiwavelength studies of the X - ray , optical and radio afterglows of the short - hard beta - ray burst ( GRB ) 050724 detected by Swift satellite at 07 : 24 : 06 UT on 24 July 2005 . The prompt emission lasted for about 1 s with an maximum photon energy E = 300 keV in the 15 - 350 keV band .We see that the temporal decay rate is ~ 1 . 2 between 10s to 1000s post - burst time scale which implies that this event belongs to the class of ultra - long GRBs . In addition we also observe a possible rebrightening characteristic around 100s post - burst time - range .Our frequency evaluation demonstrates that the spectrum can be fit well using both single power - law method as also as broken power law method . However , the best fit factors are found to be compatible within their errors when compared with each other .Using our multi - wavelength information pool , we estimate the total energetics associated with this event to be ~ 3 x 1044 erg .",
        "rewrite_text": "We present findings from multiwavelength observations of the X-ray, optical, and radio afterglows of the short-hard gamma-ray burst (GRB) 050724, which was detected by the Swift satellite at 07:24:06 UTC on July 24, 2005. The prompt emission lasted approximately 1 second, reaching a peak photon energy of E = 300 keV within the 15-350 keV range. Our analysis reveals a temporal decay rate of ~1.2 between 10 and 1000 seconds after the burst, indicating that this event is classified as an ultra-long GRB. Additionally, we observe a potential rebrightening feature around 100 seconds post-burst. Our spectral analysis shows that the data can be well fitted using both a single power-law and a broken power-law model. However, the fit parameters from both methods are found to be compatible within their uncertainties. Utilizing our collection of multiwavelength data, we estimate the total energy released in this event to be approximately 3 x 10^44 erg.",
        "ori-fast-z-score": 2.121320343559643,
        "water-fast-z-score": 6.83536555146996,
        "rewrite-fast-z-score": -0.1203858530857692
    },
    {
        "original_text": "We present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors, which are generated by applying nonholonomic frame transforms (NFT) to known vacuum solutions. The NFT is constructed using an ansatz for the metric coefficients that depends on one arbitrary function of the radial coordinate only. We show how this method can be used to generate families of black hole solutions with different horizon topologies. In particular we find new rotating black ring solutions with toroidal horizons. These solutions have been obtained previously as limits of static black rings but our approach allows us to obtain them directly without any additional assumptions or approximations. Finally, we discuss some open problems related to these results. PACS numbers: 04.20.-q, 11.10.-z, 98.80.Cq \nI. INTRODUCTORY REMARkS\nThe study of exact solutions to the Einstein equations has played a crucial role in understanding many aspects of general relativity. However, it is often difficult to construct such solutions because they require solving complicated nonlinear partial differential equations. This problem becomes even more challenging when considering physically interesting situations like those involving rotation and/or matter fields. Nevertheless, there exist several techniques that allow one to generate new classes of solutions starting from simpler ones. One of the most powerful methods involves transforming the original solution into another one via so-called nonholonomic frame transforms  1  . Such transformations preserve certain geometric properties of the spacetime while changing others; see  2  -  4  for reviews. For example, if the transformed solution satisfies the vacuum Einstein equations then so does the original one  5  .\nIn this work we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to generate new exact solutions describing stationary axisymmetric spacetimes: i.e., spacetimes admitting at least two independent Killing vector fields whose orbits are closed curves  6  . Stationary axisymmetric spacetimes play an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars, planets, and black holes  7, 8 ",
        "watermark_text": "We introduce novel exact solutions to the Einstein field equations for stationary axisymmetric spacetimes with two commuting Killing vectors , which are produced by using nonholonomic frame transforms ( NFT ) to known vacuum solutions . The NFT is built using an ansatz for the metric coefficients that relies on one arbitrary function of the radial coordinate only .We see how this algorithm can be used to create families of grey hole problems with various horizon topologies . In particular we find unique spinning black ring solutions with toroidal horizons .These solutions have been achieved formerly as limits of static black rings but our approach allows us to obtain them directly without any additional constraints or approximations . Finally , we explain some open problems related to these results .PACS codes : 04 . 20 . - q , 11 . 10 . - z , 98 . 80 . Cq I . INTRODUCTORY REMARkS The investigation of precise solutions to the Einstein equations has held a crucial role in understanding several parts of general relativity .However , it is often challenging to build such problems because they demand solving complicated nonlinear partial differential equations . This problem remains especially more challenging when exploring physically exciting situations like those concerning rotation and / or matter forces .Nevertheless , there remain many procedures that enable one to create fresh categories of solutions starting from simpler ones . One of the most popular methods means mapping the previous solve into another one via so - called nonholonomic frame transforms 1 .Such transformations maintain certain geometric properties of the spacetime while altering others ; look 2 - 4 for reviews . For instance , if the transformed solution satisfies the vacuum Einstein equations then so does the previous one 5 .In this research we apply nonholonomic frame transforms to known vacuum solutions of the Einstein equations in order to create novel exact solutions governing stationary axisymmetric spacetimes : i . e . , spacetimes admitting at least two independent Killing matrix fields whose orbits are closed curves 6 . Stationary axisymmetric spacetimes serve an important role in astrophysics since they describe the exterior gravitational field of spinning objects like stars , planets , and dark holes 7 , 8",
        "rewrite_text": "We present new exact solutions to the Einstein field equations for stationary axisymmetric spacetimes characterized by two commuting Killing vectors. These solutions are derived through nonholonomic frame transformations (NFT) applied to existing vacuum solutions. The NFT is formulated using an ansatz for the metric coefficients, which depends solely on one arbitrary function of the radial coordinate. This method demonstrates its capability to generate families of grey hole scenarios with diverse horizon topologies. Notably, we discover unique spinning black ring solutions featuring toroidal horizons. While these solutions have previously been identified as limits of static black rings, our approach enables us to derive them directly without imposing any additional constraints or approximations. Lastly, we outline several unresolved issues related to our findings. \n\nPACS codes: 04.20.-q, 11.10.-z, 98.80.Cq \n\nI. INTRODUCTORY REMARKS  \nThe quest for exact solutions to the Einstein equations has been essential for deepening our understanding of various aspects of general relativity. However, constructing such solutions can be quite complex due to the necessity of solving intricate nonlinear partial differential equations. This challenge is exacerbated in scenarios involving rotation and/or matter forces, which are physically significant. Despite these difficulties, there are several techniques that facilitate the generation of new solution categories from simpler ones. One widely used approach involves mapping known solutions to new ones via so-called nonholonomic frame transformations. These transformations preserve certain geometric properties of spacetime while modifying others; see references 2-4 for reviews. For instance, if the resulting solution adheres to the vacuum Einstein equations, the original solution does as well. In this study, we leverage nonholonomic frame transformations on known vacuum solutions of the Einstein equations to produce novel exact solutions governing stationary axisymmetric spacetimes, defined as spacetimes that possess at least two independent Killing vector fields with closed orbits. Such stationary axisymmetric spacetimes are crucial in astrophysics, as they describe the external gravitational fields of rotating bodies such as stars, planets, and black holes.",
        "ori-fast-z-score": -1.3805369799252667,
        "water-fast-z-score": 7.777427086962838,
        "rewrite-fast-z-score": 0.07647191129018725
    },
    {
        "original_text": "The Karlsruhe Tritium Neutrino experiment (KATRIN) is currently under construction at the Karlsruhe Institute for Technology, Germany. It will measure the electron energy spectrum near the endpoint with unprecedented precision by using an electrostatic retardation spectrometer combined with a magnetic filter. The measurement aims at determining the absolute value of the effective electron antineutrino mass mneff with a statistical uncertainty of 0.2 eV/c2 or better. In this contribution we present results on the expected sensitivities of KATRIN to mneff as well as to possible contributions from right-handed currents in beta-decay. We show that these sensitivities are mainly determined by systematic uncertainties which can be controlled within the experimental accuracy required. This allows us to conclude that KATRIN has excellent prospects to determine mneff precisely enough to test whether it agrees with the predictions of standard model extensions such as supersymmetry.",
        "watermark_text": "The Karlsruhe Tritium Neutrino experiment ( KATRIN ) is currently under construction at the Karlsruhe Institute for Technology , Germany . It will measure the electron energy spectrum near the endpoint with incredible precision by using an electrostatic retardation spectrometer together with a magnetic filter .The measurement seeks at determining the absolute significance of the effective electron antineutrino mass mneff with a statistical uncertainty of 0 . 2 eV / c2 or better . In this contribution we present results on the expected sensitivities of KATRIN to mneff as well as to possible contributions from left - handed waves in beta - emission .We suggest that these sensitivities are mainly decided by systematic uncertainties which can be regulated within the empirical precision required . This enables us to observe that KATRIN has excellent potential to identify mneff exactly enough to test whether it agrees with the estimates of standard theory extensions such as supersymmetry .",
        "rewrite_text": "The Karlsruhe Tritium Neutrino Experiment (KATRIN) is currently being constructed at the Karlsruhe Institute of Technology in Germany. It aims to measure the electron energy spectrum near its endpoint with exceptional precision using an electrostatic retardation spectrometer combined with a magnetic filter. The goal of this measurement is to determine the absolute value of the effective electron antineutrino mass, mneff, with a statistical uncertainty of 0.2 eV/c² or better. In this paper, we present findings regarding KATRIN's expected sensitivities to mneff, as well as potential contributions from left-handed waves in beta decay. We propose that these sensitivities are primarily influenced by systematic uncertainties, which can be managed within the required empirical precision. This indicates that KATRIN has significant potential to accurately identify mneff, allowing for comparison with predictions from extensions of standard theory, such as supersymmetry.",
        "ori-fast-z-score": -1.1920791213585393,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "We present an extension to the standard model that includes gravity, based on Feynman s ideas about quantum gravity. The extended standard model is formulated in terms of gauge fields for all known interactions (including gravity) and fermions with spin 1/2 or 1. We show how this theory can be derived from first principles using Feynman diagrams. In addition we discuss some phenomenological consequences such as neutrino masses and dark matter candidates. Finally we comment briefly on possible experimental tests of our proposal. This work was supported by NSF grant PHY-0456747. A theory of everything should include gravity along with other fundamental forces. Here we propose one such theory which extends the standard model including gravitational effects. Our approach follows closely Feynman s original idea of formulating quantum gravity in terms of gauge fields coupled to fermions. Using Feynman diagrams we derive the extended standard model from first principles. Some phenomenological consequences are discussed, e.g., neutrino mass generation via seesaw mechanisms and dark matter candidates. Possible experiments testing our proposal are also mentioned.",
        "watermark_text": "We introduce an addition to the standard theory that encompasses gravity , built on Feynman s ideas about particle gravity . The extended standard theory is implemented in terms of gauge fields for all known interactions ( including gravity ) and fermions with spin 1 / 2 or 1 .We see how this theory can be derived from first principles utilizing Feynman diagrams . In addition we explain some phenomenological consequences such as neutrino masses and dark matter candidates .Finally we comment briefly on potential experimental tests of our proposal . This project was supported by NSF grant PHY - 0456747 .A description of things should involve gravity along with other fundamental forces . Here we undertake one such idea which extends the standard theory including gravitational interactions .Our formulation follows carefully Feynman s earlier notion of formulating quantum gravitational in terms of gauge fields coupled to fermions . Using Feynman diagrams we derive the extended standard description from first principles .Some phenomenological consequences are discussed , e . g . , neutrino mass development via seesaw processes and dark matter candidates . Possible experiments testing our proposal are also discussed .",
        "rewrite_text": "We present an enhancement to the standard theory that integrates gravity, building on Feynman's concepts regarding gravitational interactions among particles. This extended theory incorporates gauge fields for all known fundamental interactions, including gravity, and considers fermions with spin 1/2 and 1. We demonstrate how this framework can be derived from first principles through the use of Feynman diagrams. Additionally, we discuss various phenomenological outcomes, such as the development of neutrino masses and candidates for dark matter. Lastly, we briefly address potential experimental approaches to test our proposal. This research was funded by NSF grant PHY - 0456747. In our exploration, we seek to unify gravity with other fundamental forces, following Feynman's earlier ideas on formulating quantum gravity in terms of gauge fields coupled to fermions. We provide a derivation of the extended standard model from foundational principles and examine its implications, including neutrino mass generation via seesaw mechanisms and considerations regarding dark matter. Possible experimental validations of our work are also outlined.",
        "ori-fast-z-score": 1.1531133203941102,
        "water-fast-z-score": 8.07179324275877,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "Affinity propagation (AP) is an algorithm for clustering that has been shown to be effective in many applications, including bioinformatics and computer vision. However, AP requires the number of clusters as input parameter which may not always be known beforehand. In this work we propose a novel approach based on constrained optimization techniques to automatically determine the optimal number of clusters using only pairwise similarity information between samples. We show how our method can be applied to several problems related to gene expression analysis such as finding co-expressed genes or identifying differentially expressed genes across multiple conditions. Our results demonstrate that our proposed method outperforms state-of-the-art approaches both in terms of accuracy and robustness. The source code used to generate all experiments presented here will be made available at http://bitbucket.org/juanlorenzo/softconstraint-clustering/wiki/Home . Affinity Propagation (AP) is an efficient message-passing algorithm for clustering that has recently gained popularity due to its effectiveness in various fields ranging from image processing  1  , computational biology  2  , and recommender systems  3  .\nHowever, one disadvantage of AP is that it requires the user to specify the desired number of clusters k before running the algorithm. This requirement makes AP less suitable when there are no prior knowledge about the number of clusters present in the dataset  4  . To overcome this problem, some authors have suggested heuristics to estimate the value of k  5  while others have developed methods to find the best possible partition given any fixed k  6  . Nevertheless, these solutions still require the user to provide additional parameters like the maximum allowed cluster size  7  or the minimum required density  8  making them difficult to use without expert knowledge  9  .\nIn order to address this issue, we introduce Soft-Constrained Affinity Propagation (SCAP), a new approach for determining the optimal number of clusters in datasets with unknown structure. SCAP uses Constrained Optimization Techniques  10  to solve the NP-hard combinatorial problem of finding the optimal solution within a set of feasible solutions  11  . More specifically,...",
        "watermark_text": "Affinity propagation ( AP ) is an algorithm for clustering that has been shown to be successful in multiple users , notably bioinformatics and computer vision . However , AP needs the number of clusters as input parameter which would not always be determined beforehand .In this research we attempt a new approach using on constrained optimization tactics to automatically identify the ideal amount of clusters using only pairwise similarity information between samples . We see how our technique can be applied to several difficulties related to gene transcription evaluation such as finding co - expressed proteins or discovering differentially expressed proteins across multiple conditions .Our results show that our proposed approach outperforms state - of - the - art methods both in terms of precision and robustness . The source software using to create all experiments shown here will be made accessible at http : / / bitbucket . org / juanlorenzo / softconstraint - clustering / wiki / Home .Affinity Propagation ( AP ) is an efficient signal - passing algorithm for clustering that has recently gotten popularity due to its effectiveness in different fields ranging from photo processing 1 , computational science 2 , and recommender environments 3 . However , one disadvantage of AP is that it requires the user to predict the desired amount of clusters k before running the method .This requirement creates AP less convenient when there are no previous known about the quantity of clusters present in the dataset 4 . To solve this situation , some writers have suggested heuristics to estimate the value of k 5 while others have developed methods to find the best possible partition given any fixed k 6 .Nevertheless , these solutions still allow the user to provide alternative parameters like the maximum permitted cluster number 7 or the minimum needed width 8 making them harder to use without expert knowledge 9 . In order to eliminate this question , we introduce Soft - Constrained Affinity Propagation ( SCAP ) , a new approach for determining the ideal amount of clusters in datasets with unknown structure .SCAP uses Constrained Optimization Techniques 10 to tackle the NP - hard combinatorial issue of finding the ideal solution within a setting of feasible answers 11 . More specifically , . . .",
        "rewrite_text": "Affinity Propagation (AP) is a clustering algorithm that has demonstrated success in various applications, particularly in bioinformatics and computer vision. However, a key limitation of AP is its requirement for the user to specify the number of clusters in advance, which is not always feasible. In this study, we propose a novel approach that leverages constrained optimization techniques to automatically determine the optimal number of clusters based solely on pairwise similarity data between samples. We explore how our method can address several challenges related to gene transcription analysis, such as identifying co-expressed proteins and detecting differentially expressed proteins across various conditions. Our findings indicate that our proposed method outperforms current state-of-the-art techniques in both accuracy and robustness. We will make the source software used for our experiments available at http://bitbucket.org/juanlorenzo/softconstraint-clustering/wiki/Home. \n\nAffinity Propagation (AP) is an efficient signal-passing algorithm for clustering that has gained popularity due to its proven effectiveness across diverse fields, including image processing, computational science, and recommendation systems. However, a notable drawback of AP is its dependence on the user to estimate the desired number of clusters, k, prior to execution. This requirement can make AP less practical when there is no prior knowledge about the expected number of clusters in the dataset. To address this issue, some researchers have proposed heuristics for estimating the value of k, while others have created methods to optimize clustering given a fixed k. Nonetheless, these approaches still require users to define additional parameters, such as the maximum allowable number of clusters or the minimum required width, which can complicate their use for those without expert knowledge. To overcome this challenge, we introduce Soft-Constrained Affinity Propagation (SCAP), a new method designed to identify the optimal number of clusters in datasets with unknown structures. SCAP employs constrained optimization techniques to address the NP-hard combinatorial problem of finding the best solution among a set of feasible options. More specifically…",
        "ori-fast-z-score": -0.23214696976024105,
        "water-fast-z-score": 10.089956834182328,
        "rewrite-fast-z-score": 1.4230249470757705
    },
    {
        "original_text": "We present the results of an investigation into the shapes and orientations of molecular cloud cores in the Orion Nebula region using near-infrared polarimetry obtained with the Gemini North telescope. We find that most (80%) of our sample are oblate spheroids, while 20% have more complex morphologies including prolate spheroids, disks, or bipolar structures. The majority of these objects show no evidence for rotation; however, we do detect significant polarization vectors aligned perpendicular to the major axes of several sources which may be due to magnetic fields. These observations suggest that many of the clouds were formed by large-scale gravitational collapse rather than rotational support. This work is supported by NASA grant NNX10AC99G. We report on the results of an investigation of the shapes and orientations of dense molecular gas clumps within the Orion Nebula region. Using near-infrared polarimetric imaging data taken at Gemini Observatory s North Telescope, we identify 80 percent of our sample as oblate spheroids. Twenty percent exhibit more complicated morphologies such as prolate spheroids or bipolar structures. Most of these objects appear to lack any internal rotation but some display polarization vectors oriented perpendicularly to their major axes suggesting the presence of magnetic fields.",
        "watermark_text": "We present the conclusion of an research into the shapes and orientations of molecular dust cores in the Orion Nebula region utilizing near - infrared polarimetry obtained with the Gemini North telescope . We see that most ( 80 % ) of our sample are oblate spheroids , while 20 % have more sophisticated morphologies including prolate spheroids , cones , or bipolar forms .The majority of these objects show no evidence for rotation ; however , we do discover considerable polarization vectors aligned parallel to the main axes of several sources which may be due to magnetic fields . These measurements suggest that several of the clouds were created by large - scale gravitational failure rather than rotational support .This research is backed by NASA grant NNX10AC99G . We report on the conclusion of an research of the shapes and orientations of dense molecular gas clumps within the Orion Nebula region .Using near - infrared polarimetric imaging information taken at Gemini Observatory s North Telescope , we identify 80 percent of our sample as oblate spheroids . Twenty half display more complicated morphologies such as prolate spheroids or bipolar forms .Most of these objects may to lack any internal rotation but some display polarization axes aligned perpendicularly to their major directions indicating the presence of magnetic waves .",
        "rewrite_text": "We present the findings of our research on the shapes and orientations of molecular dust cores in the Orion Nebula region, based on near-infrared polarimetry collected with the Gemini North telescope. Our results reveal that 80% of the dust cores in our sample are oblate spheroids, while the remaining 20% exhibit more complex morphologies, including prolate spheroids, cones, and bipolar shapes. Although most of these objects show no signs of internal rotation, we have observed significant polarization vectors aligned with the main axes of several sources, which may indicate the influence of magnetic fields. These measurements imply that many of the clouds were formed through large-scale gravitational collapse rather than being supported by rotation. This research was supported by NASA grant NNX10AC99G.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.3608276348795434
    },
    {
        "original_text": "We present an algorithm to solve the half-space depth problem, which is one of the most fundamental problems in computational geometry. The input consists of n points on the plane with integer coordinates bounded by B. We want to find a point that minimizes its distance to all other points. This problem has been studied extensively since it was introduced by Helly in 1930s. In this work we propose a new branch-and-cut algorithm based on mixed-integer programming (MIP) formulation. Our MIP model uses O(nB) variables and constraints. Using our proposed cutting planes, we can reduce the number of nodes explored during search significantly. As a result, our algorithm runs faster than existing algorithms when the dimension d = 2 or 3. For example, if the dimension is two, then our algorithm solves instances with up to 1 million points within 10 minutes while previous best known results are limited to 100 thousand points.",
        "watermark_text": "We introduce an algorithm to solve the half - space depth problem , which is one of the most important problems in computational geometry . The input consists of n points on the plane with integer coordinates bounded by B .We want to find a spot that minimizes its distance to all other points . This problem has been studied frequently since it was introduced by Helly in 1930s .In this research we propose a new branch - and - cut algorithm based on mixing - integer programming ( MIP ) implementation . Our MIP approach utilizes O ( nB ) parameters and constraints .Using our proposed cutting planes , we can restrict the number of vertices explored during search significantly . As a result , our scheme ran quicker than existing algorithms when the dimension d = 2 or 3 .For instance , if the dimension is two , then our technique solves situations with up to 1 million points within 10 minutes while prior best known data are limited to 100 thousand points .",
        "rewrite_text": "We present a new algorithm designed to tackle the half-space depth problem, a key challenge in computational geometry. The input comprises \\( n \\) points with integer coordinates situated within a bounded area \\( B \\). The goal is to identify a location that minimizes the distance to all other points. This problem has been a topic of frequent investigation since its introduction by Helly in the 1930s. In this study, we introduce an innovative branch-and-cut algorithm utilizing a mixed-integer programming (MIP) framework. Our MIP strategy employs \\( O(nB) \\) parameters and constraints. By leveraging our proposed cutting planes, we significantly reduce the number of vertices explored during the search process. Consequently, our approach demonstrates faster performance compared to existing algorithms when the dimension \\( d \\) is either 2 or 3. Specifically, in two dimensions, our method can handle scenarios involving up to 1 million points within 10 minutes, while previous best-known methods were limited to 100,000 points.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 1.2375966910186262
    },
    {
        "original_text": "The purpose of this study is to analyze the real estate market in the city of Las Vegas (LV) by using the Composite Shiller Weiss Indexes (CSWI). The data used for analysis are monthly prices of residential properties sold between January 2005 and December 2014. In addition, we use the autoregressive integrated moving average model with exogenous variables (ARIMAX), which allows us to predict future values of the indexes based on past information. We found that there was an increase in the value of property sales during the period analyzed, but it did not reach levels considered as bubbles. However, the results show that the LV real estate market has experienced periods of overvaluation since 2007. Finally, our findings suggest that the ARIMAX model can be useful when predicting the evolution of the CSWI indexes. Keywords: Real Estate Market; Prediction Modeling; Autoregressive Integrated Moving Average",
        "watermark_text": "The purpose of this study is to analyze the real property market in the cities of Las Vegas ( LV ) by using the Composite Shiller Weiss Indexes ( CSWI ) . The data used for study are monthly prices of housing structures sold between January 2005 and December 2014 .In addition , we utilize the autoregressive integrated moved average model with exogenous parameters ( ARIMAX ) , which allows us to predict upcoming values of the indexes based on past data . We showed that there was an increase in the value of property prices during the period analyzed , but it did not reach levels regarded as bubbles .However , the results show that the LV real property market has undergone cycles of overvaluation since 2007 . Finally , our findings show that the ARIMAX theory can be valuable when predicting the evolution of the CSWI indexes .Keywords: Real Estate Market; Prediction Modeling; Autoregressive Integrated Moving Average",
        "rewrite_text": "The objective of this study is to examine the real estate market in Las Vegas (LV) by employing the Composite Shiller Weiss Indexes (CSWI). Our analysis utilizes monthly housing sales data from January 2005 to December 2014. Additionally, we apply the autoregressive integrated moving average with exogenous parameters (ARIMAX) model, which enables us to forecast future index values based on historical trends. Our findings indicate an overall increase in property prices during the analyzed period, although these prices did not reach levels typically associated with market bubbles. Nevertheless, we observed that the LV real estate market has experienced cycles of overvaluation since 2007. Ultimately, our results suggest that the ARIMAX model is a useful tool for predicting the changes in the CSWI indexes. Keywords: Real Estate Market; Predictive Modeling; Autoregressive Integrated Moving Average.",
        "ori-fast-z-score": 1.4832396974191326,
        "water-fast-z-score": 5.879747322073337,
        "rewrite-fast-z-score": 0.8017837257372732
    },
    {
        "original_text": "We report on the discovery of an intervening galaxy with a mass M = 1011.5 ± 0.3M⊙ and size R = 1.7 ± 0.2h−1kpc in front of a gravitationally lensed quasar pair separated by 5′′ (~100 kpc). The absorber is detected as a DLA system along both sightlines to the quasars, which have redshifts zqso = 2.962 and zqso = 2. . We use this object to constrain the typical sizes of high-z galaxies. Our results suggest that these objects were typically smaller than their local counterparts when they formed most of their stars. This may be related to the fact that massive galaxies grow through mergers over cosmic time. \n \n Keywords: Galaxy evolution, Quasars, Absorbers, Massive black holes \n \n \n \n High-redshift quasars provide powerful probes for studying the physical properties of distant galaxies. In particular, gravitational lens systems can magnify background sources, allowing us to study fainter structures such as faint companions or extended halos around bright foreground lenses. Here we present new observations of the gravitationally-lensed quasar pair HE0435-1223, where one component has been previously found to host a supermassive black hole (SMBH) with a mass MBH = 4 × 109M☉ . Using deep near-infrared spectroscopy obtained with VLT/X-SHOOTER, we detect a strong Mg II λ2796 line associated with a galaxy located between the two quasars. The galaxy shows no evidence of ongoing star formation activity but hosts a very old stellar population. Its total luminosity corresponds to a SFR < 10−2M☉ yr−1 , indicating that it was not actively forming stars during its peak epoch of star-formation activity. However, the presence of a young stellar population cannot be ruled out completely due to possible dust obscuration effects. From our analysis, we find that the galaxy has a mass M = 1011+0.3−0.4M☉ and radius R =",
        "watermark_text": "We report on the discovery of an intervening galaxy with a mass M = 1011 . 5 ± 0 . [UNK] and size R = 1 . 7 ± 0 . 2h−1kpc in front of a gravitationally lensed quasar pair divided by 5 ′ ′ ( ~ 100 kpc ) . The absorber is detected as a DLA system along both sightlines to the quasars , which have redshifts zqso = 2 . 962 and zqso = 2 . .We use this object to constrain the typical dimensions of high - z galaxies . Our results show that these objects were generally tiny than their nearby rivals when they formed most of their stars .This might be connected to the fact that powerful nuclei grow through mergers over universe time . Keywords : Galaxy evolve , Quasars , Absorbers , Massive black holes High - redshift quasars serve powerful probes for studying the physical properties of distant galaxies .In particular , gravity lens systems can magnify background sources , allowing us to study fainter objects such as faint companions or open halos around bright foreground lenses . Here we present new studies of the gravitationally - lensed quasar pair HE0435 - 1223 , where one core has been previously found to host a supermassive black hole ( SMBH ) with a mass MBH = 4 × 109M☉ .Using deep near - infrared spectroscopy acquired with VLT / X - SHOOTER , we locate a powerful Mg II λ2796 point linked with a galaxy located between the two quasars . The galaxy displays no evidence of ongoing galaxy formation activity but hosts a very ancient stellar community .Its overall luminosity corresponds to a SFR < 10−2M☉ yr−1 , showing that it was not actively creating stars during its high epoch of star - formation activity . However , the presence of a young stellar community cannot be decided out completely due to possible dust obscuration effects .From our analysis , we find that the universe has a mass M = 1011 + 0 . 3−0 . 4M☉ and radius R =",
        "rewrite_text": "We report the discovery of an intervening galaxy with a mass of \\( M = 10^{11.5} \\pm 0. \\) and a size of \\( R = 1.7 \\pm 0.2 \\, h^{-1} \\text{kpc} \\) located in front of a gravitationally lensed pair of quasars separated by approximately 5 arcminutes (around 100 kpc). This galaxy is identified as a Damped Lyman-Alpha (DLA) system along both lines of sight to the quasars, which have redshifts \\( z_{\\text{qso}} = 2.962 \\) and \\( z_{\\text{qso}} = 2. \\) Our findings help refine the typical dimensions of high-redshift galaxies, indicating that these galaxies were generally smaller than their nearby counterparts during their peak star formation periods. This observation may be linked to the fact that powerful nuclei tend to grow through mergers over cosmic time. \n\nKeywords: Galaxy evolution, Quasars, Absorbers, Massive black holes. \n\nHigh-redshift quasars serve as valuable probes for investigating the physical characteristics of distant galaxies. In particular, gravitational lens systems can magnify background sources, enabling the study of fainter objects such as dim companions or extended halos surrounding bright foreground lenses. In this work, we present new findings related to the gravitationally lensed quasar pair HE0435-1223, where one of the cores has previously been identified to harbor a supermassive black hole (SMBH) with a mass of \\( M_{\\text{BH}} = 4 \\times 10^{9} M_{\\odot} \\). \n\nUtilizing deep near-infrared spectroscopy obtained with VLT/X-SHOOTER, we pinpoint a strong Mg II \\( \\lambda2796 \\) emission line associated with a galaxy situated between the two quasars. This galaxy shows no signs of active star formation but is home to a very ancient stellar population. Its overall luminosity corresponds to a star formation rate (SFR) of less than \\( 10^{-2} M_{\\odot} \\, \\text{yr}^{-1} \\), indicating it was not actively forming stars during its peak star formation epoch. Nonetheless, we cannot entirely rule out the presence of a young stellar population due to potential dust obscuration effects. From our analysis, we estimate that the galaxy has a mass of \\( M = 10^{11} + 0.3 - 0.4 M_{\\odot} \\) and a radius of...",
        "ori-fast-z-score": 1.9917864129354077,
        "water-fast-z-score": 8.215838362577491,
        "rewrite-fast-z-score": 2.0732842213952645
    },
    {
        "original_text": "We give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups: \nK_n(R) = Ext^n_R(M, R)\nand the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen s localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.",
        "watermark_text": "We get an explicit description of the higher algebraic K - fields in terms of certain universal cohomology groups , which are established by using only the ring composition and the unit element of the underlying commutative field . This is accomplished for any commutative field with unity R ( not necessarily Noetherian ) .The main consequence can be stated as follows : Let M be a module over R . Then there exists a natural isomorphism between the higher geometric K - families : K _ k ( R ) = Ext ^ n _ R ( M , R ) and the group of all k - fold Massey products on M modulo those that vanish under some suitable finiteness requirement . We also demonstrate how this theorem gives to a new proof of Quillen s localization principle .Finally we explain applications to the study of equivariant K - theory . In particular , we prove that if G is a compact Lie set acting freely on a smooth manifold X then the equivariant K - theory groups of X are isomorphic to the ordinary K - theory groups of the fixed point set X ^ G .",
        "rewrite_text": "We present a clear characterization of higher algebraic K-fields in relation to specific universal cohomology groups, which are derived solely from the ring structure and the unity element of the underlying commutative field. This can be achieved for any commutative field with unity \\( R \\) (not necessarily requiring it to be Noetherian). The main result can be summarized as follows: Let \\( M \\) be a module over \\( R \\). There is a natural isomorphism between the higher geometric K-families: \\( K_k(R) \\cong \\text{Ext}^n_R(M, R) \\) and the group of all k-fold Massey products on \\( M \\), modulo those that vanish under an appropriate finiteness condition. Furthermore, we illustrate how this theorem provides a new proof of Quillen's localization principle. Finally, we discuss its implications for the study of equivariant K-theory. Specifically, we prove that if \\( G \\) is a compact Lie group acting freely on a smooth manifold \\( X \\), then the equivariant K-theory groups of \\( X \\) are isomorphic to the ordinary K-theory groups of the fixed point set \\( X^G \\).",
        "ori-fast-z-score": 0.23904572186687872,
        "water-fast-z-score": 5.176591682688076,
        "rewrite-fast-z-score": 3.0542361089076304
    },
    {
        "original_text": "The author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of  information potential  for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication. \n \n Abstract \n \n We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of “information potential” to describe the evolution of the Universe.",
        "watermark_text": "The author considers the issue of gravitational interaction between bodies in terms of their informational content . The main idea is that the gravitational field can be regarded as an ensemble of gravitons which carry information about the origin body .Gravitational waves are treated as carriers of information on the state of movement of gravitating objects . It is demonstrated how this methodology allows one to explain some phenomena observed in astrophysics ( the Pioneer anomaly ) and cosmology ( darkness energy ) .In addition , it is proposed to use the notion of information possibilities for describing the evolution of the universe . This section was publication by the journal Classical and Quantum Gravity Volume 27 , Issue 14 , pages 5993 - 6010 , November 2010 .DOI : 10 . 1088 / 0264 - 9381 / 27 / 14 / 05993 / abstract . The following text is taken directly from the original published .Abstract We consider the question of gravitational interaction among bodies in terms of their information content . The main idea here is that the gravitational field might be viewed as an ensemble of gravitons / quanta carrying information about the source body ; gravitational waves are then considered as transports of information regarding the state of movement of the gravitating structures .This perspectives permits us to provide explanations for particular phenomena observed in astrophysical contexts ( e . g . , the Pioneer anomaly ) , as also as in cosmological contexts ( e . g . , darkness energy ) . Moreover , we propose utilizing the notion of “ information potential ” to explain the evolution of the Universe .",
        "rewrite_text": "The author addresses the topic of gravitational interactions between bodies through the lens of their informational content. The central thesis is that the gravitational field can be conceptualized as a collection of gravitons that transmit information about their originating body. Gravitational waves are interpreted as carriers of information regarding the motion of gravitating objects. This approach provides insights into certain phenomena observed in astrophysics, such as the Pioneer anomaly, as well as in cosmology, including dark energy. Furthermore, the author suggests using the concept of \"information potential\" to describe the evolution of the universe. This section was published in the journal *Classical and Quantum Gravity*, Volume 27, Issue 14, pages 5993-6010, in November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is extracted verbatim from the original publication. The abstract states that the authors consider the gravitational interactions among bodies through their informational content. They propose that the gravitational field can be viewed as an ensemble of gravitons or quanta that carry information about the source body, while gravitational waves are regarded as carriers of information about the motion of gravitating structures. This perspective allows for explanations of various phenomena in astrophysical contexts, such as the Pioneer anomaly, and in cosmological contexts, such as dark energy. Additionally, the authors propose the idea of \"information potential\" to elucidate the evolution of the universe.",
        "ori-fast-z-score": 2.116950987028628,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": -0.30779350562554625
    },
    {
        "original_text": "We present the prospects for high-precision measurement of the 1s2p 3P-1s2s 3S transition in atomic helium with an optical frequency comb (OFC). The OFC is stabilized to a high-finesse cavity and locked to a narrow linewidth laser at 1083 nm, which serves as a local oscillator. We show that this system can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz. This will allow us to determine the fine-structure constant α with relative uncertainty better than 2×10−10 by measuring the ratio between these two frequencies. In addition we demonstrate how the same setup could be used to perform tests of fundamental physics beyond the Standard Model such as searches for time variation of fundamental constants or violations of Lorentz invariance. Optical frequency combs are powerful tools for precise metrology  1–3  . They have been successfully applied to many different fields including ultra-stable lasers  4  , gravitational wave detection  5  , and quantum optics  6  .\nIn particular they provide unprecedented possibilities for high-precision measurement  7–9  . Here we propose to use them to improve our knowledge on the value of the fine structure constant  10  . To achieve this goal it is necessary to measure the absolute frequencies f(1s2p 3P1) = 929 072 631 770 Hz  11  and f(1s2s 3S1) = 929 073 761 828 Hz  12  of two transitions in helium. These values were determined previously with uncertainties of about 300 kHz  13  but recent theoretical calculations suggest that their accuracy may be improved significantly  14–18  .",
        "watermark_text": "We present the possibilities for high - precision study of the 1s2p 3P - 1s2s 3S change in nuclear helium with an optical frequency comb ( OFC ) . The OFC is stabilized to a high - finesse cavity and locked to a broad linewidth laser at 1083 nm , which serves as a local oscillator .We see that this scheme can be used to measure the absolute frequencies of two transitions in helium with uncertainties below 100 kHz . This will provide us to estimate the fine - structure constant α with absolute uncertainty better than 2×10−10 by measuring the ratio between these two frequencies .In addition we prove how the same setup could be used to conduct tests of fundamental physics beyond the Standard Model such as searches for moment variation of fundamental constants or violations of Lorentz invariance . Optical frequency combs are powerful tools for precise metrology 1 – 3 .They have been successfully applied to many various fields including ultra - stable lasers 4 , gravity wave detection 5 , and quantum optics 6 . In particular they give unprecedented possibilities for high - precision study 7 – 9 .Here we undertake to use them to improve our information on the value of the fine structure constant 10 . To achieve this goal it is required to measure the absolute frequencies g ( 1s2p 3P1 ) = 929 072 631 770 Hz 11 and f ( 1s2s 3S1 ) = 929 073 761 828 Hz 12 of two transitions in helium .These figures were determined earlier with uncertainties of about 300 kHz 13 but recent theoretical calculations suggest that their sensitivity might be improved dramatically 14 – 18 .",
        "rewrite_text": "We explore the potential for conducting high-precision studies of the 1s2p 3P - 1s2s 3S transition in nuclear helium using an optical frequency comb (OFC). The OFC is stabilized to a high-finesse cavity and locked to a broad linewidth laser operating at 1083 nm, which functions as a local oscillator. Our findings indicate that this approach can measure the absolute frequencies of two helium transitions with uncertainties below 100 kHz. This precision will enable us to estimate the fine-structure constant α with an absolute uncertainty of better than 2×10−10 by analyzing the ratio of these two frequencies. Additionally, we demonstrate how this setup can facilitate tests of fundamental physics beyond the Standard Model, such as investigations into variations in fundamental constants or violations of Lorentz invariance. Optical frequency combs are significant tools for precise metrology and have been effectively utilized across a variety of fields, including ultra-stable lasers, gravitational wave detection, and quantum optics. Notably, they offer exceptional opportunities for high-precision studies. In this work, we aim to enhance our understanding of the fine-structure constant by measuring the absolute frequencies of the two transitions, g (1s2p 3P1) = 929 072 631 770 Hz and f (1s2s 3S1) = 929 073 761 828 Hz. Previously determined with uncertainties around 300 kHz, recent theoretical developments suggest that we could significantly improve this sensitivity.",
        "ori-fast-z-score": 0.8700628401410971,
        "water-fast-z-score": 6.477134476605945,
        "rewrite-fast-z-score": 0.20412414523193154
    },
    {
        "original_text": "We present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 ± 0.1 and by a steep luminosity function dN/dL ∝ L−2.5±0.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.",
        "watermark_text": "We present the conclusion on morphology and luminosity function for the most luminous galaxy galaxies in the Universe , selected by their X - ray radiation ( the RCS2 specimen ) . We see that these objects are marked by an elliptical shape with axial ratio g = 0 . 7 ± 0 . 1 and by a steep luminosity function dN / dL [UNK] L−2 . 5±0 . 3 .The observed properties suggest that they may be identified as extinct families or proto - complexes at z > 1 . 0 . The data used here were obtained during our observing walks performed at ESO telescopes under programs IDs : 073 . A - 0505 ( B ) , 078 . A - 0518 ( C ) and 079 . A - 0739 ( D ) .In this research we study the morphological and photometric properties of the brightest galaxy galaxies in the universe . These systems have been detected through their X - ray emission utilizing the ROSAT All Sky Survey ( RASS ; Voges et al . , 1999 ) , and then followed up spectroscopically to confirm their redshifts and track their velocity dispersions ( see e . g .Rosati et al . , 1998 , Gladders & Yee 2005 , Eisenhardt et al ., 2008 . They hold some of the most gigantic structures discovered so far in the universe , being could to host numerous thousands of galaxies each one .Their high mass creates them ideal targets to examine how such large scale structures structure and evolve over time .",
        "rewrite_text": "We present our findings on the morphology and luminosity function of the most luminous galaxies in the universe, identified through their X-ray emissions in the RCS2 sample. These galaxies exhibit an elliptical morphology, characterized by an axial ratio of \\( g = 0.7 \\pm 0.1 \\), and display a steep luminosity function, given by \\( dN/dL \\propto L^{-2.5 \\pm 0.3} \\). The characteristics observed suggest that these galaxies may represent extinct families or proto-complexes at redshifts greater than 1.0. The data analyzed in this study were collected during our observational campaigns at ESO telescopes under program IDs: 073.A-0505(B), 078.A-0518(C), and 079.A-0739(D). Our research focuses on the morphological and photometric attributes of the brightest galaxies, which were initially detected via their X-ray emissions in the ROSAT All Sky Survey (RASS; Voges et al., 1999) and subsequently followed up with spectroscopic observations to confirm their redshifts and analyze their velocity dispersions (refer to Rosati et al., 1998; Gladders & Yee, 2005; Eisenhardt et al., 2008). These structures are among the largest known in the universe, potentially hosting thousands of galaxies each. Their significant mass makes them ideal candidates for examining the formation and evolution of large-scale structures over time.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "We present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "We present the first complete measurement of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method . We resolve for the first time the parts of the distant binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche lobes .By fitting theoretical estimates to our information we find that one part is slightly larger than expected by hypothesis while the other has a diameter compatible with predictions based on evolutionary tracks . This result suggests that tidal interactions have modified the radii of these stars during their development towards contact .Our results also demonstrate that the orbital inclination distance i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the observed separation between the two stars . Keywords : Interferometry ; Binary Stars ; Stellar Radius",
        "rewrite_text": "We report the first comprehensive measurement of the stellar radius in an interacting binary system, utilizing interferometric observations obtained through the VLTI and AMBER techniques. For the first time, we resolve components of the distant binary system SS Leporis (with a separation of approximately 0.3 arcseconds), which comprises two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data, we discover that one star is slightly larger than predicted, while the other has a diameter consistent with expectations based on evolutionary models. This finding indicates that tidal interactions have altered the radii of these stars as they evolved toward contact. Furthermore, our results support the previously determined orbital inclination of i = 60 ± 5 degrees, derived from radial velocity measurements, which aligns well with our new estimate obtained directly from the observed separation of the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": -0.5933908290969266
    },
    {
        "original_text": "The HYPERION project is an effort to combine the results of N-body simulations with those of detailed stellar population synthesis models in order to study how galaxies evolve and form their stars.  In this talk, I will present some recent results on galaxy formation using these techniques. These include studies of:  1) Galaxy mergers at high redshift; 2) The evolution of galactic disks; 3) The effect of AGN feedback on galaxy growth; 4) The assembly history of massive elliptical galaxies. This work was supported by NASA grant NNX10AD65G (PI: D. Katz). My research interests are focused on understanding how galaxies grow over cosmic time through the processes of star formation, black hole accretion, and galaxy mergers. To address this question, we have developed new methods for combining cosmological hydrodynamic simulations with state-of-the-art stellar population synthesis codes that allow us to predict the properties of galaxies as functions of both space and time.",
        "watermark_text": "The HYPERION program is an initiative to mix the results of N - bodies simulations with those of formal galaxy colony synthesis estimates in order to study how clusters evolve and form their stars . In this talk , I will present some latest findings on star formation using these concepts .These include models of : 1 ) Galaxy mergers at high redshift ; 2 ) The migration of galactic cones ; 3 ) The impact of AGN feedback on star development ; 4 ) The assembly history of large elliptical galaxies . This research was supported by NASA award NNX10AD65G ( PI : D . Katz ) .My research interests are focused on understanding how clusters grow over cosmic time through the mechanisms of galaxy formation , white hole accretion , and galaxy mergers . To address this question , we have developed novel techniques for integrating cosmological hydrodynamic simulations with state - of - the - art stellar community synthesis coding that enable us to predict the properties of stars as functions of both space and period .",
        "rewrite_text": "The HYPERION program aims to combine the outcomes of N-body simulations with formal estimates of galaxy colony synthesis to investigate the formation and evolution of star clusters. In this presentation, I will share some recent discoveries regarding star formation derived from these concepts. Key topics include: 1) High-redshift galaxy mergers; 2) The migration patterns of galactic cones; 3) The effects of AGN feedback on star formation; and 4) The assembly history of large elliptical galaxies. This research was supported by NASA under award NNX10AD65G (PI: D. Katz). My research interests center on understanding how clusters develop over cosmic time through processes such as galaxy formation, white hole accretion, and galaxy mergers. To explore these questions, we have developed innovative techniques to integrate cosmological hydrodynamic simulations with advanced stellar population synthesis models, allowing us to predict stellar properties as functions of both spatial location and time.",
        "ori-fast-z-score": -1.5215349135496974,
        "water-fast-z-score": 6.905427684571704,
        "rewrite-fast-z-score": 1.8599622199011085
    },
    {
        "original_text": "We study the non-adiabatic current through an open quantum dot connected to two leads with different chemical potentials, which is driven by time-dependent gate voltages applied on both sides of the dot. We show that this system can be described as a  turnstile  where electrons are pumped between the leads when the driving frequencies match certain resonances. The effect is robust against disorder and dephasing. This work was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). In recent years there has been growing interest in studying electron pumps based on semiconductor nanostructures such as quantum dots or carbon nanotubes  1, 2  . These devices have potential applications ranging from metrology  3  , single-electron transistors  4  , and spintronics  5  .\nIn these systems, charge carriers are transported across the device via sequential tunneling processes  6  . A number of theoretical studies  7, 8  have shown that it is possible to achieve high efficiency in these devices even at room temperature  9  . However, most previous works focused only on adiabatic pumping  10  , i.e., the case where the frequency of the external drive is much smaller than all other relevant energy scales  11  . Recently, several experiments  12, 13  reported large currents generated by nonadiabatic pumping  14, 15  . It remains unclear whether these results can be explained within existing theories  16  .\nHere we consider a simple model of a quantum dot connected to two metallic leads  see Fig. 1(a)    17  . The dot level is modulated periodically by applying oscillating gate voltages V L/R = ±V 0 cos ωt on each side of the dot  18  . When the modulation period T ≡ 2π/ω matches one of the dwell times τ n = π / 2(E F − E n )  associated with the discrete levels E n of the isolated dot, electrons will be transferred coherently between the left and right leads  19  . Here E F denotes the Fermi energy of the leads  20  . As illustrated schematically in Figs. 1(b-c), depending on",
        "watermark_text": "We explore the non - adiabatic current through an open quantum dot connected to two leads with various chemical potentials , which is powered by time - dependent gate voltages applied on both sides of the dot . We see that this network can be described as a turnstile where electrons are pumped between the leads when the driving energies fit particular resonances .The impact is robust against disorder and dephasing . This research was supported by NSERC ( Canada ) and CIFAR ( Canadian Institute for Advanced Research ) .In past times there has been growing interest in investigating electron pumps built on semiconductor nanostructures such as quantum dots or carbon nanotubes 1 , 2 . These systems have potential applications ranging from metrology 3 , single - ion transistors 4 , and spintronics 5 .In these systems , charge carriers are transported across the device via sequential tunneling processes 6 . A variety of theoretical experiments 7 , 8 have shown that it is easy to achieve high efficiency in these systems even at room temperature 9 .However , most prior papers focused only on adiabatic pumping 10 , i . e . , the case where the frequency of the external drive is much smaller than all other relevant energy scales 11 . Recently , various study 12 , 13 documented large waves generated by nonadiabatic pumping 14 , 15 .It remains unsure whether these results can be described within existing models 16 . Here we consider a simple model of a quantum dot connected to two metallic leads see Fig .1 ( a ) 17 . The dot level is modulated periodically by using oscillating gate voltages V L / R = ±V 0 cos ωt on each side of the dot 18 .When the modulation time T ≡ 2π / ω matches one of the dwell times π n = π / 2 ( E F − E n ) associated with the discrete levels E n of the isolated dot , electrons will be transferred coherently between the left and left leads 19 . Here E F denotes the Fermi energy of the leads 20 .As illustrated schematically in Figs.1(b-c), depending on",
        "rewrite_text": "We investigate the non-adiabatic current in an open quantum dot that is connected to two leads with differing chemical potentials, which is driven by time-dependent gate voltages applied on either side of the dot. Our findings suggest that this setup functions like a turnstile, facilitating electron transport between the leads when the driving energies align with specific resonance conditions. Notably, the system's resilience to disorder and dephasing is evident. This research was supported by NSERC (Canada) and CIFAR (Canadian Institute for Advanced Research). There has been an increasing interest in developing electron pumps utilizing semiconductor nanostructures, such as quantum dots and carbon nanotubes, due to their potential applications in metrology, single-ion transistors, and spintronics. In these devices, charge carriers move through sequential tunneling processes. Numerous theoretical studies have indicated that achieving high efficiency in these systems, even at room temperature, is feasible. However, most prior research has primarily concentrated on adiabatic pumping—the scenario in which the frequency of the external drive is significantly lower than other relevant energy scales. Recently, various studies have reported substantial currents generated through non-adiabatic pumping. It remains unclear whether these findings can be reconciled with existing theoretical frameworks. In this work, we explore a straightforward model of a quantum dot connected to two metallic leads (see Fig. 1(a)). The dot's energy levels are periodically modulated via oscillating gate voltages (V_L/R = ±V_0 cos(ωt)) applied to both sides. When the modulation period T ≡ 2π/ω coincides with one of the dwell times πn = π/2 (EF − En) associated with the discrete energy levels En of the isolated dot, coherent electron transfer between the left and right leads occurs. Here, EF represents the Fermi energy of the leads. As depicted in Figs. 1(b-c), the specific behavior of the system varies depending on...",
        "ori-fast-z-score": 0.5107539184552492,
        "water-fast-z-score": 6.236810901332355,
        "rewrite-fast-z-score": 1.7149858514250882
    },
    {
        "original_text": "We present the results of our analysis on the X-ray spectrum and variability properties of CIV 1549, which is one of the brightest Seyfert galaxies in the sky at soft X-rays (0.5-2 keV). We find that its spectral shape can be well described by a power law with photon index Γ = 2.1 ± 0.2 plus two thermal components; one component has temperature kT = 0.3 +0.4 −0.1 keV while another has higher temperature kT = 3.7 +1.6 −1.1 keV. The luminosity ratio between these two thermal components is L h /L l ≈ 5.9 +2.8 −2.1 . In addition to this multi-component continuum model, we also include several emission lines such as Fe Kα line and OVII triplet. Our best-fit parameters are consistent with those obtained previously using ASCA data. \n \n Using the Chandra HETG observation taken during 2001-2002, we have investigated the short-term variability behavior of CIV 1549. We found no significant time lag between different energy bands within the observed bandpasses. However, there appears to exist some correlation between flux variations in hard energies (> 4 keV) and those in softer energies (< 4 keV), although it does not appear to be strictly linear relationship. This result suggests that the origin of the short-term variability may be due to reprocessing of harder photons into softer ones rather than intrinsic fluctuations of the primary source itself. \n \n Finally, we examine whether or not CIV 1549 shows any evidence for rapid aperiodic variability. By applying wavelet transform techniques to the light curve extracted from the central region of the galaxy, we detect strong signals corresponding to periods ranging from 10 - 100 s. These periodicities are most likely associated with quasi-periodic oscillations (QPOs). \n \n We conclude that CIV 1549 is probably powered by accretion onto supermassive black holes.",
        "watermark_text": "We present the conclusion of our analysis on the X - ray spectrum and variability properties of CIV 1549 , which is one of the brightest Seyfert galaxies in the sky at warm X - radiation ( 0 . 5 - 2 keV ) . We see that its spectral structure can be well described by a power law with photon index Γ = 2 . 1 ± 0 . 2 plus two thermal parts ; one part has temperature kT = 0 . 3 + 0 . 4 −0 . 1 keV while another has higher temperature kT = 3 . 7 + 1 . 6 −1 . 1 keV .The luminosity factor between these two thermal parts is L h / L l ≈ 5 . 9 + 2 . 8 −2 . 1 . In addition to this multi - component continuum model , we also add several emission lines such as Fe Kα line and OVII triplet .Our best - fitting characteristics are compatible with those acquired previously using ASCA information . Using the Chandra HETG measurement done during 2001 - 2002 , we have researched the short - term variability behavior of CIV 1549 .We determined no considerable time lag between various energy bands within the seen bandpasses . However , there seems to appear some correlation between flux variations in hard frequencies ( > 4 keV ) and those in harder frequencies ( < 4 keV ) , although it does not appear to be strictly linear correlation .This result suggests that the origin of the short - term variability may be due to reprocessing of stronger photons into weaker ones instead than intrinsic fluctuations of the primary source itself . Finally , we investigate whether or not CIV 1549 shows any evidence for rapid aperiodic variability .By applying wavelet transform techniques to the light line gathered from the central region of the universe , we identify strong pulses corresponding to periods ranging from 10 - 100 s . These periodicities are most likely correlated with quasi - periodic oscillations ( QPOs ) . We suggest that CIV 1549 is probably powered by accretion onto supermassive black holes .",
        "rewrite_text": "We present our analysis conclusion on the X-ray spectrum and variability characteristics of CIV 1549, one of the brightest Seyfert galaxies observable in the warm X-ray range (0.5 - 2 keV). Our findings indicate that its spectral shape can be accurately modeled with a power law having a photon index of Γ = 2.1 ± 0.2, supplemented by two thermal components: one with a temperature of kT = 0.3 +0.4 -0.1 keV and another at a higher temperature of kT = 3.7 +1.6 -1.1 keV. The luminosity ratio between these thermal components is approximately Lh / Ll ≈ 5.9 +2.8 -2.1. Alongside this multi-component continuum model, we also incorporate various emission lines, including the Fe Kα line and the OVII triplet. Our best-fit parameters align well with those derived from previous ASCA data. Utilizing measurements from the Chandra HETG conducted between 2001 and 2002, we explored the short-term variability of CIV 1549 and found no significant time lag across the observed energy bands. However, a correlation appears to exist between flux changes in higher energies (> 4 keV) and lower ones (< 4 keV), although this relationship does not follow a strictly linear pattern. This suggests that the short-term variability may arise from the reprocessing of more potent photons into weaker ones, rather than from inherent fluctuations of the primary source. Lastly, we investigated potential evidence for rapid aperiodic variability by applying wavelet transform techniques to the light curve from the galaxy's core. This analysis revealed strong pulses corresponding to periods of 10 - 100 seconds, likely indicative of quasi-periodic oscillations (QPOs). We propose that CIV 1549 is likely powered by the accretion onto supermassive black holes.",
        "ori-fast-z-score": 0.3651483716701107,
        "water-fast-z-score": 7.6085975253341545,
        "rewrite-fast-z-score": 1.889822365046136
    },
    {
        "original_text": "We have analyzed peculiar motions of galaxies within the region of the ursa major supercluster (UMS) using data on galaxy redshifts and distances obtained by us with the 6-m telescope at the Special Astrophysical Observatory of Russian Academy of Sciences.  The UMS is one of the largest known superclusters, consisting of about 100 rich clusters of galaxies. We found that the mean radial velocity of all galaxies in this supercluster relative to its center amounts to -500 km/s. This value agrees well with estimates made earlier for other superclusters. However, we also discovered an unexpected feature of the motion of galaxies inside the UMS. Namely, there are two groups of galaxies moving towards each other along the line connecting their centers. One group consists of three nearby clusters located near the center of the supercluster; another includes four distant clusters situated at a distance of more than 60 Mpc from it.",
        "watermark_text": "We have analyzed unusual movements of galaxies within the sector of the ursa major supercluster ( UMS ) using data on star redshifts and lengths taken by us with the 6 - m observatory at the Special Astrophysical Observatory of Russian Academy of Sciences . The UMS is one of the largest discovered superclusters , consisting of about 100 rich clusters of clusters .We showed that the mean radial speed of all galaxies in this supercluster relative to its core amounts to - 500 km / s . This value agrees well with predictions taken previous for other superclusters .However , we also discovered an unexpected feature of the movement of stars inside the UMS . Namely , there are two groups of clusters moved towards each other along the line linking their centers .One group contains of three adjacent complexes positioned near the center of the supercluster ; another includes four distant clusters situated at a distance of more than 60 Mpc from it .",
        "rewrite_text": "We investigated the peculiar movements of galaxies within the Ursa Major Supercluster (UMS) by analyzing star redshift and distance data collected at the 6-meter telescope at the Special Astrophysical Observatory of the Russian Academy of Sciences. The UMS is one of the largest known superclusters, comprising approximately 100 rich clusters of galaxies. Our findings indicate that the average radial velocity of all galaxies in this supercluster, relative to its core, is -500 km/s, a result that aligns well with previous predictions made for other superclusters. However, we also uncovered an intriguing aspect of stellar movement within the UMS: two distinct groups of clusters are moving toward each other along the line connecting their centers. One group consists of three adjacent clusters located near the supercluster's core, while the other group comprises four more distant clusters situated over 60 Mpc away.",
        "ori-fast-z-score": -1.4569855927715483,
        "water-fast-z-score": 4.201805851511121,
        "rewrite-fast-z-score": -1.4320780207890627
    },
    {
        "original_text": "In this work, we propose an energy-efficient cooperative transmission scheme for wireless sensor networks (WSNs). The proposed scheme is based on the combination of collaborative beamforming at the source node with cooperative transmission to multiple relay nodes in order to improve lifetime performance. In particular, the source node first transmits data packets using its maximum power level. Then, it switches to lower power levels if no packet has been successfully received within a certain time period. If one or more packets are successfully decoded, then the source node increases its transmit power back up to the previous value. We derive closed-form expressions for outage probability as well as average throughput under Rayleigh fading channels. Our results show that our proposed scheme can significantly increase network lifetime compared to conventional schemes without compromising system reliability. Finally, numerical examples are provided to verify the theoretical analysis. Keywords: Lifetime improvement; Relay selection; Energy efficiency; Outage probability",
        "watermark_text": "In this project , we propose an energy - efficient joint propagation scheme for wireless sensor networks ( WSNs ) . The proposed system is based on the combination of collaborative beamforming at the source node with coordinated transmission to multiple relay nodes in order to improve life performance .In particular , the source node initially transmits data packets using its highest power level . Then , it switches to smaller energy levels if no packet has been successfully received within a certain time time .If one or more bits are properly decoded , then the source node increases its broadcast capacity back up to the previous value . We derive closed - form expressions for outage likelihood as well as average throughput under Rayleigh fading filters .Our results show that our proposed system can significantly expand network lifetime compared to conventional systems without compromising system efficiency . Finally , numerical examples are provided to confirm the theoretical analysis .Keywords : Lifetime improvement ; Relay choice ; Energy quality ; Outage probability",
        "rewrite_text": "In this project, we introduce an energy-efficient joint propagation scheme tailored for wireless sensor networks (WSNs). Our proposed system integrates collaborative beamforming at the source node with coordinated transmissions to multiple relay nodes, enhancing performance and extending network life. Specifically, the source node begins by transmitting data packets at its maximum power level. If no packets are successfully received within a specified duration, the node then reduces its transmission power. Conversely, if one or more bits are correctly received, the source node restores its broadcast power to the previous maximum. We derive closed-form expressions for outage probability and average throughput within Rayleigh fading environments. Our findings demonstrate that this system significantly prolongs network lifetime compared to traditional approaches, all while maintaining system efficiency. Finally, we provide numerical examples to validate our theoretical analysis. \nKeywords: Lifetime enhancement; Relay selection; Energy efficiency; Outage probability.",
        "ori-fast-z-score": 0.105999788000636,
        "water-fast-z-score": 6.11104144857543,
        "rewrite-fast-z-score": 0.6625891564490792
    },
    {
        "original_text": "We present an exact quantum-mechanical treatment for the dynamics of open systems in which the system is coupled to many harmonic oscillators representing its surrounding environment.  We show that, under certain conditions, this model can be reduced exactly into a master equation with Lindblad form. The resulting master equations are used to study the effects of environmental fluctuations on the evolution of the density matrix describing the state of the system. In particular we consider two different models of environments corresponding to Ohmic dissipation and spin-boson interaction respectively. For both cases it is shown how the effect of the environment leads to irreversible loss of information about the initial state of the system as well as to thermalization at late times. Finally, we discuss possible applications of our results to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids. Decoherence and relaxation processes play a crucial role in understanding the physics of open quantum systems  1, 2  . These phenomena arise when the system interacts with some external degrees of freedom (environment) whose influence cannot be neglected  3  .\nIn recent years there has been considerable interest in developing theoretical methods capable of treating these effects beyond the perturbative regime  4  . A number of approaches have been proposed ranging from phenomenological treatments based on stochastic Schrödinger equations  5  , to more microscopic descriptions using path integral techniques  6  or field-theoretical formulations  7, 8  . However, despite their successes, all these methods suffer from one common drawback: they do not provide any insight into the underlying physical mechanisms responsible for decoherence and relaxation; nor do they allow us to make quantitative predictions regarding the time scales involved  9  .\nRecently, several authors  10 -12  have suggested that the problem may be tackled within the framework of quantum mechanics itself. This idea was first put forward by Feynman  13  who showed that the statistical properties of macroscopic objects could be obtained by averaging over an ensemble of identical but microscopically distinct realizations of the same experiment. More recently, Leggett  14  introduced a method...",
        "watermark_text": "We present an precise quantum - mechanical explanation for the dynamics of open systems in which the system is linked to many harmonic oscillators describing its surrounding environment . We see that , under certain conditions , this description can be reduced exactly into a master equation with Lindblad form .The resulting master equations are using to study the effects of environmental fluctuations on the evolution of the density graph explaining the state of the system . In particular we investigate two different models of environments corresponding to Ohmic dissipation and spin - boson collision respectively .For both cases it is demonstrated how the impact of the surroundings leads to irreversible loss of information about the first state of the system as well as to thermalization at late times . Finally , we explain possible applied of our findings to problems such as transport through mesoscopic conductors or dissipative tunneling between localized states in disordered solids .Decoherence and relaxation processes take a crucial role in understanding the physics of open quantum systems 1 , 2 . These phenomena arise when the system interacts with some external degrees of autonomy ( surroundings ) whose influence cannot be forgotten 3 .In recent years there has been substantial interest in pursuing theoretical methods suitable of addressing these phenomena beyond the perturbative regime 4 . A variety of methods have been proposed ranging from phenomenological treatments based on stochastic Schrödinger coefficients 5 , to more microscopic descriptions using path integral methods 6 or field - theory formulations 7 , 8 .However , despite their successes , all these algorithms suffer from one common drawback : they do not offer any insight into the fundamental physical mechanisms involved for decoherence and relaxation ; nor do they allow us to make quantitative predictions regarding the period scales involved 9 . Recently , various scientists 10 - 12 have suggested that the issue may be tackled within the framework of quantum mechanics itself .This idea was first put forward by Feynman 13 who demonstrated that the empirical qualities of macroscopic objects may be obtained by averaging over an ensemble of different but microscopically different realizations of the same experiment . More recently , Leggett 14 proposed a technique . . .",
        "rewrite_text": "We provide a precise quantum-mechanical framework for understanding the dynamics of open systems that are coupled to numerous harmonic oscillators representing their surrounding environment. Under specific conditions, this framework can be accurately simplified into a master equation in Lindblad form. The derived master equations are utilized to investigate the influence of environmental fluctuations on the evolution of the density matrix that characterizes the system's state. Specifically, we analyze two distinct environmental models: one associated with Ohmic dissipation and the other with spin-boson interactions. In both instances, we illustrate how external influences can lead to an irreversible loss of information about the system's initial state and induce thermalization over time. Furthermore, we discuss potential applications of our findings to problems such as transport in mesoscopic conductors and dissipative tunneling between localized states in disordered materials. Decoherence and relaxation processes are essential for understanding the physics of open quantum systems. These processes occur when the system interacts with external degrees of freedom (its environment), which cannot be neglected. Recently, there has been significant interest in developing theoretical approaches that tackle these phenomena beyond the realm of perturbation theory. A range of methods has emerged, from phenomenological models based on stochastic Schrödinger equations to more microscopic approaches utilizing path integrals and field-theoretic frameworks. However, despite their successes, these methods share a common limitation: they do not provide insights into the fundamental physical mechanisms of decoherence and relaxation, nor do they enable quantitative predictions regarding the relevant time scales. Recently, several researchers have proposed that this challenge could be addressed within the framework of quantum mechanics itself. This concept was initially introduced by Feynman, who demonstrated that the empirical properties of macroscopic objects can be derived by averaging over an ensemble of microscopically distinct realizations of the same experiment. In more recent work, Leggett suggested a technique…",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.905694150420948,
        "rewrite-fast-z-score": 2.242227956050979
    },
    {
        "original_text": "We relate the notion of information-theoretic security to that of computational indistinguishability, and show how this relationship can be used in practice for proving security properties of cryptographic protocols.  We also present an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones (under some reasonable assumptions). The transformation preserves all communication complexity measures such as round-complexity or number of messages exchanged between parties.   Finally we give examples where our approach is useful by applying it to prove security of several well-known protocols. In recent years there has been much interest in developing techniques for analyzing the security of cryptographic protocols. One important goal is to develop tools for showing that certain protocols are secure with respect to various definitions of security. For example, many researchers have studied the problem of transforming a protocol secure only against passive attackers into one secure against active attackers. This problem was first considered by Canetti et al., who gave a general solution based on universal composability  Can01  . However their method does not preserve communication complexity measures like round-complexity or message-exchange patterns. Subsequently, Micali proposed another transformation  MIC02  , but his technique requires additional assumptions about the underlying primitives being used.",
        "watermark_text": "We relate the notion of communication - theoretic integrity to that of computational indistinguishability , and suggest how this relationship can be used in practice for determining security characteristics of cryptographic protocols . We also demonstrate an algorithm which transforms any protocol secure against passive adversaries into one secure against active ones ( under some reasonable assumptions ) .The transformation maintains all transmission complexity measures such as round - complexity or number of communication received between parties . Finally we give instance where our approach is utilized by using it to prove security of several well - famous protocols .In past decades there has been much interest in improving procedures for evaluating the safety of cryptographic protocols . One important aim is to develop tools for showing that particular protocols are secure with regard to several definitions of security .For instance , many scholars have researched the question of converting a protocol secure only against passive attackers into one stable against active attackers . This problem was first investigated by Canetti et al . , who offered a general solution based on universal composability Can01 .However their method does not retain transmission diversity values like round - complexity or message - transfer patterns . Subsequently , Micali proposed another transformation MIC02 , but his method needs additional observations about the underlying primitives being used .",
        "rewrite_text": "We connect the concept of communication-theoretic integrity with that of computational indistinguishability and propose practical applications of this relationship for evaluating the security features of cryptographic protocols. Additionally, we present an algorithm that converts any protocol secure against passive adversaries into one that is secure against active adversaries, given certain reasonable assumptions. This transformation preserves all metrics of transmission complexity, such as round complexity and the volume of communication exchanged between parties. Furthermore, we illustrate the application of our approach by demonstrating its effectiveness in proving the security of several well-known protocols. In recent decades, there has been significant interest in refining methods for assessing the safety of cryptographic protocols. A key objective has been to develop tools that can effectively demonstrate the security of specific protocols according to various security definitions. For example, many researchers have explored the challenge of transforming protocols that are secure only against passive attackers into those resistant to active attackers. This issue was initially addressed by Canetti et al., who provided a general solution based on universal composability. However, their approach does not preserve transmission diversity metrics such as round complexity or message transfer patterns. Later, Micali introduced an alternative transformation, but his method requires additional information about the underlying primitives involved.",
        "ori-fast-z-score": -0.2873478855663454,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 0.38138503569823695
    },
    {
        "original_text": "The main result is the following theorem. \nTHEOREM 1 Let  be an arbitrary real number and let  be any positive integer.  Then, there exists a constant , depending only on , , such that \nfor all matrices . The proof of this theorem will be given in Section 2 below. In particular, if we take  then (1) becomes \nwhich was proved by Yakubovich  J. Math. Anal. Appl., vol. 16, pp. 719-726, (1964) . If we choose  then (1) reduces to \nwhich was established by Popov  Publ. Math. Inst. Steklov, vol. A, no. 3, p. 225-230, (1963)  . We remark here that our results are not new but they can be obtained as corollaries of these two papers. However, it seems worthwhile to give proofs which do not depend upon those earlier works.",
        "watermark_text": "The main consequence is the following theorem . THEOREM 1 Let be an arbitrary real number and consider be any positive integer .Then , there exists a constant , depending only on , , such that for all matrices . The confirmation of this theorem will be provided in Section 2 below .In particular , if we took then ( 1 ) becomes which was confirmed by Yakubovich J . Math .Anal.Appl., vol.16 , pp . 719 - 726 , ( 1964 ) .If we choose  then (1) reduces to \nwhich was established by Popov  Publ.Math.Inst.Steklov, vol.A , no . 3 , p . 225 - 230 , ( 1963 ) .We comment here that our findings are not original but they can be obtained as corollaries of these two papers . However , it appears worthwhile to give proofs which do not depend upon those earlier works .",
        "rewrite_text": "The primary outcome is encapsulated in the following theorem: \n\n**THEOREM 1:** Let \\( r \\) be any arbitrary real number and \\( n \\) be any positive integer. Then, there exists a constant \\( C \\), which depends solely on \\( r \\) and \\( n \\), such that for all matrices \\( A \\) and \\( B \\), the relationship holds. A detailed proof of this theorem will be presented in Section 2 below. Notably, when we set \\( r = 0 \\), equation (1) simplifies to a form that was validated by Yakubovich in *J. Math. Anal. Appl.*, vol. 16, pp. 719-726, (1964). For the case where \\( r = 1 \\), equation (1) simplifies further to a result established by Popov in *Publ. Math. Inst. Steklov*, vol. A, no. 3, pp. 225-230, (1963). It should be noted that our results are not entirely original, as they can be derived as corollaries from these two prior publications. Nevertheless, we believe it is beneficial to provide independent proofs that do not rely on these earlier studies.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 2.846049894151541,
        "rewrite-fast-z-score": -0.8320502943378437
    },
    {
        "original_text": "We present an analysis of the effects that mergers may have on scaling relations between black holes (BHs), galaxies, and other gravitationally bound systems. We use cosmological simulations to study how BH mass is related to galaxy properties in different merger histories. Our results show that mergers can significantly affect these relationships by increasing scatter at fixed luminosity or stellar velocity dispersion. This effect is strongest for low-mass galaxies with high specific star formation rates. In addition, we find that mergers tend to increase the average BH-to-galaxy mass ratio as well as the fraction of active galactic nuclei (AGNs) among massive galaxies. These findings are consistent with observations of AGN host galaxies. Finally, our results suggest that the observed correlation between supermassive BH masses and bulge properties could be driven primarily by the fact that both grow during major mergers. Keywords: Galaxy evolution; Supermassive black hole",
        "watermark_text": "We present an assessment of the effects that mergers might have on scaling relations between black holes ( BHs ) , galaxies , and other gravitationally bound structures . We use cosmological simulations to study how BH mass is related to galaxy structures in different merger histories .Our results show that mergers can significantly affect these interactions by expanding scatter at fixed luminosity or stellar velocity dispersion . This phenomenon is greatest for low - density nuclei with high specific star formation rates .In addition , we find that mergers prefer to raise the average BH - to - star mass ratio as well as the fraction of active galactic nuclei ( AGNs ) among giant galaxies . These conclusions are consistent with observations of AGN host galaxies .Finally , our findings show that the seen correlation between supermassive BH masses and bulge properties might be motivated primarily by the fact that both expand during major mergers . Keywords : Galaxy evolve ; Supermassive black hole",
        "rewrite_text": "We provide an evaluation of how mergers may influence the scaling relations among black holes (BHs), galaxies, and other gravitationally bound structures. Leveraging cosmological simulations, we examine the relationship between BH mass and galaxy structures across various merger histories. Our findings indicate that mergers can notably impact these relationships by increasing the dispersion at fixed luminosity or stellar velocity dispersion, particularly in low-density nuclei with high specific star formation rates. Furthermore, we discover that mergers tend to elevate the average BH-to-star mass ratio, as well as the proportion of active galactic nuclei (AGNs) in massive galaxies. These results align with observations of AGN host galaxies. Ultimately, our study suggests that the observed correlation between supermassive BH masses and bulge characteristics may primarily stem from both expanding during significant mergers. Keywords: Galaxy evolution; Supermassive black hole.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "We study the dynamics of two interacting bosonic species confined to an optical lattice, with one species being initially prepared as a coherent state at each site while the other is initially prepared as a thermal cloud. We show that this system supports both symmetric and asymmetric soliton solutions which are stable against small perturbations for certain values of the chemical potentials. The stability properties of these solitons can be understood by studying their linearization spectrum around the stationary states. In particular we find that the presence of a finite temperature leads to additional unstable modes associated with phonon-like excitations. Finally, we demonstrate how our results may be used to describe experiments on spinor condensates loaded into optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases consisting of several different atomic species  1  . These systems provide new opportunities to explore novel phenomena such as supersolids  2  , phase separation  3  or spin-orbit coupling  4  .\nIn this work we consider a particularly interesting example where there exist two distinct types of particles (e.g., atoms) which interact via s-wave scattering but differ in mass and/or internal structure  5  . This situation arises naturally when considering mixtures of hyperfine states  6  or isotopes  7, 8  within the same atom type  9  . For instance, recent experiments involving 87 Rb and 41 K  10  have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling  11  . Another possibility would involve using 40 K and 6 Li  12  . Here, the lighter species could be considered as impurities immersed in a background gas of heavier fermions  13  . Alternatively, if the masses were reversed then the heavy species could act as impurities  14  .",
        "watermark_text": "We research the dynamics of two interacting bosonic species confined to an optical lattice , with one species being initially made as a coherent state at each site while the other is initially prepared as a heat bubble . We see that this scheme holds both symmetric and asymmetric soliton solutions which are stable against small perturbations for particular values of the chemical potentials .The stability properties of these solitons can be understood by examining their linearization spectrum around the stationary states . In particular we find that the presence of a finite temperature leads to extra weak modes associated with phonon - like excitations .Finally , we prove how our findings may be used to explain studies on spinor condensates stacked into optical lattices . Introduction : - Recent research developments have enabled it able to create quantum degenerate gases composed of several different atomic species 1 .These systems present new opportunities to examine novel processes such as supersolids 2 , phase splitting 3 or spin - orbit resonance 4 . In this research we imagine a particularly exciting example where there exist two different kinds of atoms ( e . g . , atoms ) which interact via s - wave scattering but differ in mass and / or internal structure 5 .This condition occurs naturally when examining mixtures of hyperfine states 6 or isotopes 7 , 8 within the same atom type 9 . For instance , recent experiments concerning 87 Rb and 41 K 10 have demonstrated the formation of a mixture of two different hyperfine states after evaporative cooling 11 .Another possibility would include utilizing 40 K and 6 Li 12 . Here , the softer species may be regarded as impurities immersed in a background gas of heavier fermions 13 .Alternatively , if the masses were reversed then the heavy species may function as impurities 14 .",
        "rewrite_text": "We investigate the dynamics of two interacting bosonic species confined within an optical lattice, where one species starts as a coherent state at each site and the other is initially arranged as a heat bubble. Our findings reveal that this setup can produce both symmetric and asymmetric soliton solutions that remain stable against small perturbations for specific values of the chemical potentials. The stability of these solitons is analyzed through their linearization spectrum around stationary states. Notably, we observe that the introduction of finite temperature results in additional weak modes tied to phonon-like excitations. Furthermore, we demonstrate how our results can elucidate research on spinor condensates organized within optical lattices. \n\nIntroduction: Recent advancements have facilitated the creation of quantum degenerate gases composed of various atomic species. These systems open new avenues for exploring novel phenomena, such as supersolids, phase separation, and spin-orbit resonance. In our research, we consider a particularly intriguing scenario involving two distinct types of atoms that interact via s-wave scattering but differ in mass and/or internal structure. This situation is commonly encountered in mixtures of hyperfine states or isotopes within the same atomic type. For example, recent experiments with 87Rb and 41K have successfully produced a mixture of different hyperfine states following evaporative cooling. Another potential combination involves 40K and 6Li, where the lighter species could be seen as impurities in a heavier fermionic background. Conversely, if the mass distribution were reversed, the heavier species could act as impurities instead.",
        "ori-fast-z-score": -0.3481553119113957,
        "water-fast-z-score": 6.5033247714309,
        "rewrite-fast-z-score": 1.5716505559714824
    },
    {
        "original_text": "We consider the problem of power allocation in discrete-input delay-limited fading channels with multiple users and time-varying channel gains. We propose an iterative algorithm to solve this problem by using convex optimization techniques, which is shown to converge within finite number of iterations under certain conditions. The proposed algorithm can be implemented efficiently through parallel processing at each iteration step. Numerical results show that our proposed scheme outperforms existing schemes significantly. \n \n Keywords: Power control; Convex optimization; Time-varying; Multiple access channels (MACs); Wireless communications; Iterative algorithms. 1 Introduction \n \n In wireless communication systems, it has been well recognized that the performance of multi-user transmission depends on how the available resources are allocated among different users  1  . For example, when there exist multiple users sharing a common radio resource such as bandwidth or transmit power, the optimal way to allocate these resources may depend on the specific system settings  2  , e.g., whether the users have equal priority  3  , what type of services they request  4  , etc.. Therefore, efficient resource allocation strategies should take into account all relevant factors so as to maximize overall network utility  5  .\n \nIn recent years, considerable research efforts have been devoted to studying various aspects of resource allocation problems  6  -  8  . Among them, power allocation plays an important role due to its direct impact on both spectral efficiency and energy consumption  9  . However, most previous works assume continuous input alphabets  10  -  12  , while practical digital modulation schemes usually employ discrete constellations  13  . As a result, the conventional approaches cannot be directly applied to discrete-input scenarios  14  . To address this issue, several studies  15  -  17  have investigated the power allocation problem over discrete-input channels recently. Nevertheless, their solutions either require high computational complexity  16  or suffer from slow convergence speed  17  .",
        "watermark_text": "We consider the issue of power transfer in discrete - input delay - limited fading channels with many users and period - differing channel gains . We suggest an iterative algorithm to solve this question by using convex optimization schemes , which is demonstrated to converge within finite number of iterations under certain conditions .The proposed algorithm can be executed easily through concurrent processing at each iteration step . Numerical results show that our proposed system outperforms old schemes considerably .Keywords : Power control ; Convex optimization ; Time - increasing ; Multiple access networks ( MACs ) ; Wireless communications ; Iterative strategies . 1 Introduction In wireless communication devices , it has been good recognized that the performance of dual - customer propagation depends on how the provided resources are assigned among different users 1 .For instance , when there reside several users sharing a common radio asset such as bandwidth or transmit energy , the ideal means to allocate these resources may depend on the specific system settings 2 , e . g . , whether the people have equal priority 3 , what type of solutions they demand 4 , etc . . Therefore , efficient resource transfer tactics should take into consideration all relevant variables so as to maximize overall network utility 5 .In recent years , substantial research efforts have been focused to researching various parts of resource consumption issues 6 - 8 . Among them , power distribution plays an important role owing to its significant effect on both spectral capacity and energy consumption 9 .However , most prior compositions take continuous source alphabets 10 - 12 , while contemporary digital modulation schemes typically employ discrete constellations 13 . As a result , the usual techniques lack be directly used to discrete - input models 14 .To address this question , various studies 15 - 17 have researched the power sharing question over discrete - input channels recently . Nevertheless , their solutions either need high computational complexity 16 or suffer from slow convergence speed 17 .",
        "rewrite_text": "We address the challenge of power transfer in discrete-input, delay-limited fading channels with multiple users and varying channel gains. Our approach involves an iterative algorithm utilizing convex optimization techniques, which has been shown to converge in a finite number of iterations under specific conditions. This algorithm is designed for ease of execution via concurrent processing at each iteration step. Numerical results indicate that our system significantly outperforms previous methods. \n\n**Keywords:** Power control; Convex optimization; Time-increasing; Multiple access networks (MACs); Wireless communications; Iterative strategies. \n\n**1 Introduction**  \nIn wireless communication systems, it is well recognized that the performance of dual-user propagation is highly dependent on how resources are allocated among different users. For example, when multiple users share a common radio resource like bandwidth or transmission energy, the optimal allocation strategy may rely on specific system parameters, such as user priority and solution requirements. Therefore, effective resource allocation strategies must account for all pertinent factors to maximize overall network utility. In recent years, significant research has focused on various aspects of resource consumption. Among these, power allocation is particularly crucial due to its substantial influence on spectral efficiency and energy consumption. However, most previous works have assumed continuous source alphabets, while modern digital modulation techniques typically utilize discrete constellations. Consequently, conventional methods cannot be directly applied to discrete-input scenarios. To tackle this issue, several recent studies have explored power sharing in discrete-input channels. Nevertheless, their proposed solutions often face challenges such as high computational complexity or slow convergence rates.",
        "ori-fast-z-score": -1.4615384615384615,
        "water-fast-z-score": 8.795290947839238,
        "rewrite-fast-z-score": 0.5586608191273356
    },
    {
        "original_text": "We present the results of an analysis aimed at calibrating mid-infrared (MIR) star formation rate indicators using data obtained with Spitzer and Herschel Space Telescopes, as well as ground-based facilities such as the Infrared Array Camera onboard the Hubble Space Telescope.  We use a sample of galaxies selected to have high signal-to-noise ratio detections in all four IRAC bands (3.6-8 microns), which are used for estimating total infrared luminosities via SED fitting techniques. The sample consists of 29 nearby star-forming galaxies that span a wide range of physical properties including stellar mass, dust temperature, metallicity, and specific star formation rates. Using these observations we derive empirical relations between MIR emission features and total infrared luminosity, which can be used to estimate LIR over a large dynamic range without requiring any assumptions about the shape or slope of galaxy SEDs.",
        "watermark_text": "We publish the conclusion of an assessment aimed at calibrating mid - infrared ( MIR ) star formation rate indicators using data acquired with Spitzer and Herschel Space Telescopes , as well as land - based equipment such as the Infrared Array Camera onboard the Hubble Space Telescope . We use a sample of stars selected to have high signal - to - noise ratio detections in all four IRAC ranges ( 3 . 6 - 8 microns ) , which are using for estimating actual infrared luminosities via SED fitting methods .The sample consists of 29 nearby star - creating stars that cover a broad variety of physical properties including stellar mass , dust temperature , metallicity , and particular galaxy formation rates . Using these observations we derive empirical relations between MIR emission features and total infrared luminosity , which can be used to estimate LIR over a large static range without requiring any constraints about the form or slope of galaxy SEDs .",
        "rewrite_text": "We present the conclusions of an assessment focused on calibrating mid-infrared (MIR) star formation rate indicators, utilizing data obtained from the Spitzer and Herschel Space Telescopes, as well as ground-based instruments like the Infrared Array Camera on the Hubble Space Telescope. Our study involves a sample of stars that exhibit high signal-to-noise ratio detections across all four IRAC bands (3.6 - 8 microns), which we employ to estimate actual infrared luminosities through spectral energy distribution (SED) fitting methods. The sample includes 29 nearby star-forming stars showcasing a diverse range of physical characteristics, such as stellar mass, dust temperature, metallicity, and specific galaxy formation rates. Through these observations, we establish empirical relationships between MIR emission features and total infrared luminosity, facilitating the estimation of LIR across a wide static range without imposing any constraints regarding the form or slope of galaxy SEDs.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.23904572186687872
    },
    {
        "original_text": "We report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0. \n \n The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "watermark_text": "We report on X - ray timing observations of the pulsar candidate PSR J1930 + 1855 centered at the center of the supernova remnant ( SNR ) G54 . 1 + 0 . The source was studied by Chandra and reported as a pulsar with XMM - Newton , but its spin time is not stable over time ranges shorter than one month .We conducted two sets of pointed RXTE observations to study this behavior further . In both cases we concluded that the heartbeat rate decreases slowly during our observation running .This trend can be described good using an exponential decay model for which we find typical timescales of 1 . 1 days and 0 . 7 weeks respectively . These quantities are compatible with those noted earlier based on Chandra data alone .However , when comparing these results directly it should be mentioned that the uncertainties involved with the previous tests were significantly larger thanks to the smaller signal - to - noise proportion attained with Chandra compared to RXTE .",
        "rewrite_text": "We present our findings from X-ray timing observations of the pulsar candidate PSR J1930+1855, located at the heart of the supernova remnant (SNR) G54.1+0. Previous analyses by Chandra and XMM-Newton classified this source as a pulsar, but its spin period exhibits instability over intervals shorter than one month. To investigate this behavior further, we conducted two sets of targeted RXTE observations. In both instances, we observed a gradual decrease in the pulsar's heartbeat rate throughout our observational period. This decline can be effectively modeled using an exponential decay function, yielding characteristic timescales of approximately 1.1 days and 0.7 weeks, respectively. These timescales align well with those reported previously based solely on Chandra data. However, it is important to note that the uncertainties in the earlier analyses were substantially larger, a consequence of the lower signal-to-noise ratio obtained with Chandra compared to RXTE.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 6.719319439596787,
        "rewrite-fast-z-score": 0.5
    },
    {
        "original_text": "We have measured the space velocities for eight globular clusters in the southern hemisphere with galactic latitudes less than 20 degrees, using proper motions and radial velocities obtained by various authors over the past decade or so.  The sample includes four open clusters (NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars . We find that all but one cluster are consistent with being at rest relative to the local standard of rest; however, we also find evidence that two of these clusters may be on orbits which will take them out of our galaxy within several billion years .  These results suggest that there is no significant difference between open clusters and globulars when it comes to their kinematics ; both types appear to share similar properties .\nThe only exception appears to be the open cluster M67 , whose velocity vector points away from us toward the constellation Cetus . This result suggests that this open cluster has been ejected from its parent galaxy during an encounter with another galaxy some time ago .",
        "watermark_text": "We have analyzed the space velocities for eight globular complexes in the southern hemisphere with galactic latitudes less than 20 degrees , using proper motions and radial velocities collected by various scientists over the previous decade or so . The sample comprises four open complexes ( NGC 2420 , NGC 2516 , NGC 2682 , and NGC 6705 ) as well as four globulars .We see that all but one cluster are compatible with being at remainder relative to the local standard of rest ; however , we also find proof that two of these clusters might be on orbits which will take them out of our universe within several billion decades . These conclusions show that there is no major variation between open nuclei and globulars when it comes to their kinematics ; both types seems to hold identical structures .The only exception appears to be the open cluster M67 , whose speed function points away from us toward the constellation Cetus . This result suggests that this open cluster has been expelled from its father galaxy during an encounter with another galaxy some time previously .",
        "rewrite_text": "We have examined the space velocities of eight globular clusters located in the southern hemisphere, specifically those with galactic latitudes below 20 degrees. This analysis utilized proper motions and radial velocities gathered by various researchers over the past decade. The sample includes four open clusters (NGC 2420, NGC 2516, NGC 2682, and NGC 6705) and four globular clusters. Our findings indicate that all but one of the clusters are consistent with being at rest relative to the local standard of rest. However, we have also discovered evidence suggesting that two of these clusters could be on trajectories that will eventually take them out of our universe within several billion years. These results show no significant differences in the kinematics of open and globular clusters; both types appear to share similar structures. The notable exception is the open cluster M67, whose velocity vector points away from us toward the constellation Cetus. This observation implies that M67 may have been ejected from its parent galaxy due to an interaction with another galaxy in the past.",
        "ori-fast-z-score": -1.649915822768611,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": -2.457864091118742
    },
    {
        "original_text": "We present new results on the displacement of the Sun from the galactic plane based on Hipparcos data and recent determinations of the solar motion with respect to the local standard of rest (LSR). We find that the Sun is displaced by about 0.5 kpc in the direction towards the constellation Cetus, which agrees well with previous estimates obtained using different methods. The observed displacement can be explained as due to the combined effect of the gravitational potential of the Galaxy and the peculiar velocity of the Local Group with respect to it. \n \n Keywords: Solar System dynamics, Galactic rotation curve, Local Group kinematics, Galactocentric distance \n \n 1 Introduction \n \n In this work we study the position of the Sun within our galaxy. This problem has been addressed previously by several authors who have used different techniques ranging from statistical studies of open clusters  1  or OB associations  2  , to direct measurements of proper motions  3  . Here we use the most accurate available determination of the solar motion  4  together with the latest measurement of the circular speed at large distances  5  to determine the position of the Sun relative to the galactic plane.",
        "watermark_text": "We report new data on the movement of the Sun from the galactic plane based on Hipparcos statistics and recent determinations of the sun motion with regard to the local standard of rest ( LSR ) . We see that the Sun is displaced by about 0 . 5 kpc in the direction towards the constellation Cetus , which agrees well with previous calculated obtained using separate methods .The observed displacement can be described as owing to the combined influence of the gravitational potential of the Galaxy and the peculiar speed of the Local Group with regard to it . Keywords : Solar System dynamics , Galactic rotation curve , Local Group kinematics , Galactocentric distance 1 Introduction In this research we study the orientation of the Sun within our universe .This problem has been addressed previously by various scientists who have utilized varying techniques ranging from statistical analyses of close complexes 1 or OB associations 2 , to direct measurements of proper motions 3 . Here we utilize the most accurate available determination of the sun motion 4 together with the latest measurement of the circular distance at large distances 5 to estimate the orientation of the Sun relative to the galactic plane .",
        "rewrite_text": "We present new findings regarding the Sun's movement away from the galactic plane, informed by Hipparcos statistics and recent measurements of solar motion relative to the Local Standard of Rest (LSR). Our analysis indicates that the Sun is displaced approximately 0.5 kpc toward the constellation Cetus, aligning closely with previous calculations derived from different methodologies. This observed displacement is attributed to the cumulative effects of the Galaxy's gravitational potential and the peculiar velocity of the Local Group in relation to the Galaxy. \n\n**Keywords**: Solar System dynamics, Galactic rotation curve, Local Group kinematics, Galactocentric distance.\n\n**1 Introduction**  \nIn this study, we explore the orientation of the Sun within our universe. This question has been previously examined by various researchers, utilizing a range of techniques from statistical analyses of nearby complexes or OB associations to direct measurements of proper motions. In this work, we employ the most precise determination of solar motion available, along with the latest measurements of circular distances at significant scales, to assess the Sun's orientation relative to the galactic plane.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 6.625891564490792,
        "rewrite-fast-z-score": 0.5698028822981898
    },
    {
        "original_text": "We have analyzed Chandra observations for eight galaxy clusters with redshifts between 0.1 and 0.3 to determine their radial temperature, density, pressure, entropy, cooling time, and metallicity profiles. We find that all these quantities are well described by single-parameter scaling relations as functions of radius r normalized by the virial radius Rvir.  The best-fit values of the normalization parameters depend on redshift but not significantly so; we therefore adopt fixed values based on our results for the two most distant clusters (z = 0.2 and 0.3) which yield good fits to the other six clusters. Our main conclusions are:  1. All cluster properties show significant evolution out to z ~ 0.3; this is consistent with previous studies using XMM data. 2. The gas fraction fgas(r/Rvir), defined as the ratio of the total thermal energy within a sphere of radius r to its gravitational binding energy, decreases monotonically outwards; it also shows some evidence for evolution with redshift. 3. The electron number density ne(r) increases inwardly toward the center of each cluster until reaching a peak value near r ~ 0.1r200 where r200 denotes the radius enclosing an average overdensity of 200 times the critical density of the universe. Beyond this point, ne(r) declines slowly or remains roughly constant depending on the cluster. 4. The mean molecular weight µe(r) increases outwardly due to the increasing contribution of helium ions relative to hydrogen atoms. 5. The central temperatures T0 inferred from spectral fitting range from 6 keV to 12 keV, while those obtained directly from the deprojected temperature profile lie in the range 7-15 keV. These differences may be caused by non-thermal components such as AGN jets and/or magnetic fields.",
        "watermark_text": "We have analyzed Chandra measurements for eight galaxy galaxies with redshifts between 0 . 1 and 0 . 3 to estimate their radial temperature , density , pressure , entropy , cooling period , and metallicity profiles . We see that all these quantities are better represented by single - parameter scaling relations as functions of radius r normalized by the virial diameter Rvir .The best - fitting values of the normalization coefficients differ on redshift but not considerably so ; we thus choose fixed values based on our findings for the two most distant populations ( z = 0 . 2 and 0 . 3 ) which provide better fits to the other six regions . Our main results are : 1 .All cluster elements exhibit substantial development out to z ~ 0 . 3 ; this is consistent with previous research utilizing XMM data . 2 .The gas fraction fgas ( r / Rvir ) , defined as the proportion of the total heat power within a sphere of radius r to its gravitational binding energy , decreases monotonically outwards ; it also shows some evidence for evolution with redshift . 3 .The electron number density ne ( r ) rises inwardly toward the center of each cluster until reaching a peak value near r ~ 0 . 1r200 where r200 denotes the radius enclosing an mean overdensity of 200 times the critical density of the universe . Beyond this point , ne ( r ) declines slowly or remains fairly constant depending on the cluster .4 . The mean molecular weight µe ( r ) rises outwardly due to the increasing contribution of helium ions relative to hydrogen atoms .5 . The central temperatures T0 inferred from spectral fit range from 6 keV to 12 keV , while those generated directly from the deprojected temperature profile lie in the range 7 - 15 keV .These changes may be caused by non - electrical components such as AGN jets and / or magnetic waves .",
        "rewrite_text": "We analyzed Chandra measurements for eight galaxies with redshifts between 0.1 and 0.3 to assess their radial profiles of temperature, density, pressure, entropy, cooling time, and metallicity. Our findings indicate that these properties are more effectively described by single-parameter scaling relations as functions of radius \\( r \\) normalized by the virial diameter \\( R_{vir} \\). The optimal normalization coefficients vary by redshift, but the differences are not substantial; therefore, we opted to use fixed values based on our observations of the two most distant populations (z = 0.2 and 0.3), which yield better fits for the other six clusters. Our key conclusions are as follows: 1. All cluster components show significant development up to z ~ 0.3, supporting earlier studies that utilized XMM data. 2. The gas fraction \\( f_{gas}(r/R_{vir}) \\), defined as the ratio of the total thermal energy within a sphere of radius \\( r \\) to its gravitational binding energy, decreases monotonically outward and also displays some evidence for evolution with redshift. 3. The electron number density \\( n_e(r) \\) increases toward the center of each cluster, peaking near \\( r \\approx 0.1r_{200} \\), where \\( r_{200} \\) is the radius corresponding to an average overdensity of 200 times the critical density of the universe. Beyond this peak, \\( n_e(r) \\) either declines slowly or remains approximately constant, depending on the specific cluster. 4. The mean molecular weight \\( \\mu_e(r) \\) increases outward, reflecting a higher proportion of helium ions relative to hydrogen atoms. 5. The central temperatures \\( T_0 \\), as inferred from spectral fits, range from 6 keV to 12 keV, while those derived from the deprojected temperature profile range from 7 to 15 keV. These variations may be influenced by non-thermal components, such as AGN jets and/or magnetic waves.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 2.1053798026662975
    },
    {
        "original_text": "The kinematics and dynamics analysis are performed for the hybrid manipulator with three degrees-of-freedom (3-DOF). The proposed manipulator consists of two serial chains, one is driven by an electric motor through gears while another chain is connected to it via spherical joints. In order to obtain the forward kinematics solution, the Denavit-Hartenberg parameters have been used in this study. The inverse kinematics problem has also been solved using the Jacobian matrix method. Finally, the dynamic model of the manipulator was developed based on Lagrange s equation. The results show that the designed manipulator can be applied as a robotic arm in various applications such as industrial robots or rehabilitation devices. Keywords: Forward kinematics, Inverse kinematics, Dynamics modeling, Robotics, Serial chain, Spherical joint, Electric motor, Gearbox, Manipulator, Degrees-of-freedom",
        "watermark_text": "The kinematics and dynamics calculations are performed for the combination manipulator with three degrees - of - independence ( 3 - DOF ) . The proposed manipulator consists of two serial chains , one is powered by an electric motor through gears while another chain is linked to it via triangular joints .In order to obtain the forward kinematics solving , the Denavit - Hartenberg variables have been used in this study . The inverse kinematics problem has additionally been solved using the Jacobian matrix method .Finally , the dynamic model of the manipulator was developed using on Lagrange s equation . The results show that the designed manipulator can be applied as a robotic unit in different applications such as factory robots or reconstruction systems .Keywords : Forward kinematics , Inverse kinematics , Dynamics modeling , Robotics , Serial chain , Spherical joint , Electric car , Gearbox , Manipulator , Degrees - of - freedom",
        "rewrite_text": "The calculations for kinematics and dynamics have been conducted on a 3-degree-of-freedom (3-DOF) combination manipulator. This proposed manipulator features two serial chains; one chain is driven by an electric motor through a gearbox, while the other is connected via triangular joints. To derive the forward kinematics, we employed the Denavit-Hartenberg parameters. Additionally, the inverse kinematics was addressed using the Jacobian matrix approach. The dynamic model of the manipulator was formulated utilizing Lagrange's equations. The findings indicate that the designed manipulator can serve as a robotic unit for various applications, including factory automation and reconstruction systems. \n\nKeywords: Forward kinematics, Inverse kinematics, Dynamic modeling, Robotics, Serial chain, Spherical joint, Electric motor, Gearbox, Manipulator, Degrees of freedom.",
        "ori-fast-z-score": 0.13736056394868904,
        "water-fast-z-score": 4.626813958590447,
        "rewrite-fast-z-score": -0.14002800840280097
    },
    {
        "original_text": "We present an analysis of N-body simulations aimed at understanding how evaporation shapes the mass function (MF) of globular clusters (GCs). We find that, in agreement with previous studies, evaporation causes GCs to lose stars preferentially on their low-mass end and thus steepens the MF slope towards lower masses. However, we show that this effect is counteracted by two competing processes: dynamical friction which removes massive stars more efficiently than less massive ones; and relaxation-driven core collapse which increases the central density of the cluster and makes it harder for massive stars to escape. The net result depends strongly on the initial concentration of the cluster, but typically leads to shallower slopes compared to those observed in real GCs. This suggests that other processes are required to explain the shape of the observed MF. In particular, our results suggest that primordial binaries may be responsible for producing the high-mass power-law tail seen in many GCs.",
        "watermark_text": "We publish an assessment of N - bodies simulations aiming at studying how evaporation shapes the mass function ( MF ) of globular galaxies ( GCs ) . We see that , in agreement with previous research , evaporation creates GCs to lose stars preferentially on their low - mass ending and therefore steepens the MF slope towards lesser masses .However , we find that this effect is counteracted by two different processes : dynamical friction which destroys massive galaxies more efficiently than less massive ones ; and relaxation - triggered disk collapse which increases the main abundance of the cluster and causes it difficult for huge stars to escape . The total result relies highly on the first abundance of the cluster , but typically leads to shallower slopes compared to those observed in real GCs .This implies that other processes are required to explain the form of the seen MF . In particular , our findings confirm that primordial binaries may be responsible for producing the high - mass power - law tail seen in large GCs .",
        "rewrite_text": "We present an evaluation of N-body simulations focused on how evaporation influences the mass function (MF) of globular galaxies (GCs). Our findings align with prior studies, indicating that evaporation causes GCs to preferentially lose stars from their low-mass end, resulting in a steeper MF slope at lower masses. However, we also discover that this effect is moderated by two processes: dynamical friction, which more effectively disrupts massive galaxies than their less massive counterparts, and relaxation-triggered disk collapse, which enhances the overall density of the cluster and makes it challenging for large stars to escape. The overall outcome is significantly influenced by the initial density of the cluster but typically produces shallower slopes than those observed in actual GCs. This suggests that additional processes are necessary to account for the observed MF shape. Notably, our results support the idea that primordial binaries may contribute to the high-mass power-law tail evident in larger GCs.",
        "ori-fast-z-score": -1.4444444444444444,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "We introduce the notion of an algebra over a monoidal category and show that it is equivalent to the notion of a coalgebra in the dual category, which we call a comonoid.  We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra.  This definition generalizes the usual one for ordinary algebras over fields or rings.   In particular, if the base ring has characteristic zero, this recovers the classical definitions of cyclic homology and periodic cyclic homology.  The same construction also works for Hopf algebroids instead of ordinary algebras;  however, there are some subtleties arising when trying to extend these results to arbitrary commutative rings.    Finally, we give several examples illustrating our constructions. Cyclic homology was introduced by Connes in his seminal work on noncommutative geometry  Con  . It can be defined as the Hochschild homology of certain algebras called cyclic objects. These were first studied systematically by Bökstedt  Bök  , who showed how they could be used to construct new algebraic structures like crossed modules and group extensions. Since then, many authors have investigated various aspects of cyclic objects and their applications. For example, see  Fri1  ,  Fri2  ,  Koc  ,  Lau  ,  Maz  ,  Nee  ,  Sta  .\nIn this article, we will study cyclic objects in more detail using techniques developed recently in the theory of operads and monads. Our main result shows that any cyclic object gives rise to two different types of cyclic homologies, namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded algebra. Moreover, both of them can be computed explicitly in terms of the structure maps defining the cyclic object. As a consequence, we obtain explicit formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field of characteristic 0.",
        "watermark_text": "We introduce the notion of an ring over a monoidal category and know that it is analogous to the notion of a coalgebra in the dual category , which we call a comonoid . We then define the cyclic homology of such algebras as the Hochschild cohomology of their underlying comonoids with coefficients in the bimodule given by the tensor product of the algebra with its opposite algebra .This definition generalizes the usual one for regular algebras over fields or rings . In particular , if the base ring has characteristic zero , this recovers the classical definitions of cyclic homology and regular cyclic homology .The same construction also works for Hopf algebroids rather of simple algebras ; however , there are some subtleties emerging when trying to apply these results to arbitrary commutative rings . Finally , we give numerous instances illustrating our concepts .Cyclic homology was introduced by Connes in his seminal study on noncommutative geometry Con . It can be written as the Hochschild homology of certain algebras called cyclic objects .These were first examined carefully by Bökstedt Bök , who demonstrated how they could be used to build modern algebraic structures like crossed varieties and group extensions . Since then , various papers have researched several topics of cyclic objects and their applications .For instance , see Fri1 , Fri2 , Koc , Lau , Maz , Nee , Sta . In this article , we will explore cyclic objects in more depth employing approaches developed lately in the notion of operads and monads .Our main result suggests that any cyclic object gives rise to two different kinds of cyclic homologies , namely the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded module . Moreover , both of them can be computed specifically in terms of the structure maps governing the cyclic object .As a consequence , we obtain formal formulas for the cyclic homology of all finite - dimensional cocommutative Hopf algebras over a field of characteristic 0 .",
        "rewrite_text": "We present the concept of a ring defined over a monoidal category, which parallels the idea of a coalgebra in the dual category, referred to as a comonoid. We then define the cyclic homology of these algebras as the Hochschild cohomology of their underlying comonoids, using coefficients from the bimodule formed by the tensor product of the algebra with its opposite algebra. This definition extends the traditional one for standard algebras over fields or rings. Notably, when the base ring has characteristic zero, it recovers the classical definitions of cyclic homology and regular cyclic homology. This construction can also be applied to Hopf algebroids instead of merely simple algebras; however, some complexities arise when attempting to extend these findings to arbitrary commutative rings. We will provide several examples to illustrate our concepts. Cyclic homology was first introduced by Connes in his pivotal work on noncommutative geometry. It can also be characterized as the Hochschild homology of specific algebras known as cyclic objects. Bökstedt was the first to rigorously analyze these cyclic objects, demonstrating their utility in developing modern algebraic structures such as crossed varieties and group extensions. Since then, numerous studies have delved into various aspects of cyclic objects and their applications, including works by authors like Fri1, Fri2, Koc, Lau, Maz, Nee, and Sta. In this article, we will investigate cyclic objects more thoroughly, utilizing recent methodologies inspired by the concepts of operads and monads. Our primary result indicates that each cyclic object leads to two distinct types of cyclic homologies: the cyclic homology of the underlying algebra and the periodic cyclic homology of the associated graded module. Furthermore, both types can be explicitly computed in terms of the structure maps of the cyclic object. As a result, we derive explicit formulas for the cyclic homology of all finite-dimensional cocommutative Hopf algebras over a field of characteristic zero.",
        "ori-fast-z-score": -0.4583492485141057,
        "water-fast-z-score": 5.775200531277732,
        "rewrite-fast-z-score": 1.6570343122169822
    },
    {
        "original_text": "We present the basic principles for modeling heterogeneous materials using two-point correlation functions (2PCFs). The 2PCF is an important statistical tool in many fields, including physics and engineering sciences. In this work we show how to use it as a basis for describing complex systems with multiple components or phases. We demonstrate that the 2PCF can be used to describe both static and dynamic properties of such systems. Finally, we discuss some applications of our approach. This article is part of a series on  Multiscale Modeling  published by Frontiers in Physics. \nIntroduction\n\nTwo-point correlation function (2PCF) is one of the most fundamental concepts in statistics  1  . It has been widely applied across various disciplines ranging from physics  2  , chemistry  3  , biology  4  , geology  5  , medicine  6  , economics  7  , sociology  8  , etc., to engineering  9  .\nIn recent years there have been several attempts to apply the concept of 2PCF to multiscale modeling  10 -12  . However, these works are mostly focused on developing new numerical methods rather than providing physical insights into the problem at hand. Herein, we propose a novel method based on the concept of 2PCFs which allows us to model heterogeneous materials consisting of different components and/or phases. Our approach provides a general framework for studying the structure-property relationships in such systems. Moreover, it enables us to study their dynamics over a wide range of time scales. \n \n To illustrate the main idea behind our approach let us consider a simple example shown schematically in Figure 1 . Suppose we want to investigate the mechanical response of a composite material made up of three distinct components A, B, C arranged in a periodic manner. Each component consists of randomly distributed spherical particles embedded within a matrix phase. For simplicity, assume that all components have identical volume fractions f = 0.33 but differ in terms of particle size distribution. Specifically, suppose that the average diameter of particles in each component is equal to: dA = 10 nm; dB = 20 nm; DC = 30 nm. As illustrated in Figure 1(a) , the overall microstructure of the",
        "watermark_text": "We present the fundamental principles for modeling heterogeneous materials utilizing two - point coupling curves ( 2PCFs ) . The 2PCF is an important statistical tool in multiple fields , notably physics and engineering studies .In this research we show how to use it as a foundation for describing complex systems with many aspects or stages . We showed that the 2PCF can be used to explain both static and dynamic characteristics of such systems .Finally , we explain some applications of our approach . This section is part of a trilogy on Multiscale Modeling published by Frontiers in Physics .Introduction Two - point coupling function ( 2PCF ) is one of the most important concepts in statistics 1 . It has been widely applied across numerous topics including from science 2 , chemistry 3 , chemistry 4 , geology 5 , medicine 6 , economics 7 , anthropology 8 , etc . , to engineering 9 .In past decades there have been numerous attempts to apply the idea of 2PCF to multiscale simulation 10 - 12 . However , these works are mostly concentrated on developing innovative statistical algorithms instead than providing physical knowledge into the issue at hand .Herein , we develop a new method using on the idea of 2PCFs which allows us to model heterogeneous materials composed of different components and / or stages . Our concept provides a general template for studying the form - property interactions in such systems .Moreover , it allows us to study their processes over a broad variety of time ranges . To explain the main idea behind our approach take us consider a simple example shown schematically in Figure 1 .Suppose we wish to examine the structural response of a composite material consisting up of three different components A , B , C arranged in a periodic manner . Each component consists of randomly distributed spherical atoms embedded within a matrix phase .For simplicity , assume that all components have equal volume fractions r = 0 . 33 but differ in terms of particle size distribution . Specifically , suppose that the average diameter of molecules in each portion is equal to : dA = 10 nm ; dB = 20 nm ; DC = 30 nm .As illustrated in Figure 1(a) , the overall microstructure of the",
        "rewrite_text": "We introduce the fundamental principles for modeling heterogeneous materials through the use of two-point coupling curves (2PCFs). The 2PCF is a significant statistical tool utilized across various domains, particularly in physics and engineering. In this study, we demonstrate how 2PCFs can serve as a foundational framework for describing complex systems that encompass multiple components or stages. We illustrate that the 2PCF is effective in elucidating both the static and dynamic characteristics of these systems. Additionally, we discuss several applications of our methodology. This section is part of a trilogy on Multiscale Modeling published by Frontiers in Physics.\n\nThe two-point coupling function (2PCF) is one of the key concepts in statistics. Its applications are extensive, spanning various fields such as science, chemistry, geology, medicine, economics, anthropology, and engineering. Over the past few decades, numerous efforts have been made to integrate the 2PCF concept into multiscale simulations. However, most existing work has focused on developing novel statistical algorithms rather than incorporating physical insights into the problems. In this paper, we propose a new methodology based on the principle of 2PCFs, enabling us to model heterogeneous materials composed of diverse components and stages. Our approach provides a comprehensive framework for investigating form-property interactions within these systems and permits the exploration of their processes across a wide range of timescales.\n\nTo illustrate the core idea of our approach, consider a simple example depicted in Figure 1. We aim to analyze the structural response of a composite material comprising three distinct components—A, B, and C—arranged in a periodic configuration. Each component consists of randomly distributed spherical atoms embedded in a matrix phase. For simplicity, we assume that all components have equal volume fractions \\( r = 0.33 \\) but differ in their particle size distributions. Specifically, let the average diameters of the molecules in each component be defined as: \\( d_A = 10 \\, \\text{nm} \\), \\( d_B = 20 \\, \\text{nm} \\), and \\( d_C = 30 \\, \\text{nm} \\). As illustrated in Figure 1(a), the overall microstructure of the composite material.",
        "ori-fast-z-score": 0.5482823149915702,
        "water-fast-z-score": 10.932163332202425,
        "rewrite-fast-z-score": 4.242640687119286
    },
    {
        "original_text": "The origin of the most massive stars is still an open question in astrophysics, as well as their role in shaping galactic evolution. In this talk I will present recent results on how we can use observations to constrain theoretical models for the formation of these objects.  The first part of my talk will focus on the observational properties of young massive clusters (YMCs) that are found at high redshift z>6-7. These YMCs have masses up to 10^8 Msun and sizes of ~1kpc. They appear to be very compact compared with local starburst galaxies such as Arp 220 or M82 which typically contain much less dense stellar populations. We find that the observed size-mass relation of these distant YMCs agrees remarkably well with predictions based on numerical simulations of turbulent gas clouds collapsing under self-gravity. This suggests that turbulence plays an important role during the early stages of cluster formation. However, it remains unclear whether all massive stars form in such large clusters like those seen at high redshifts.",
        "watermark_text": "The origin of the most large objects is still an open question in astrophysics , as well as their role in shaping galactic progression . In this talk I will present recent results on how we can using observations to constrain theoretical theories for the formation of these objects .The first part of my talk will focus on the observational properties of young massive clusters ( YMCs ) that are found at high redshift z > 6 - 7 . These YMCs have masses up to 10 ^ 8 Msun and dimensions of ~ 1kpc .They seem to be very small compared with local starburst clusters such as Arp 220 or M82 which commonly feature considerably less dense stellar groups . We see that the observed height - mass balance of these distant YMCs follows perfectly well with predictions based on numerical simulations of turbulent gas clouds collapsing under self - gravity .This implies that turbulence plays an important role during the early stages of cluster structure . However , it remains unsure whether all huge objects create in such large clusters like those observed at high redshifts .",
        "rewrite_text": "The origins of the largest celestial objects remain a significant question in astrophysics, particularly regarding their influence on galactic evolution. In this presentation, I will share recent findings that leverage observations to refine theoretical models concerning the formation of these massive entities. The first segment will concentrate on the observational characteristics of young massive clusters (YMCs) identified at high redshifts (z > 6 - 7). These YMCs have masses reaching up to 10^8 solar masses and dimensions of approximately 1 kpc. They appear quite small when compared to local starburst clusters such as Arp 220 or M82, which typically contain much less dense stellar populations. Notably, the observed height-mass relationship of these distant YMCs aligns closely with predictions derived from numerical simulations of turbulent gas clouds collapsing under their own gravity. This finding suggests that turbulence is a significant factor during the early phases of cluster formation. However, it remains uncertain whether all massive objects are formed within such large clusters as those observed at these high redshifts.",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 6.340751391209736,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "We report on the measurement of electric fields produced by surface contaminants using neutral atoms as probes. The technique is based on measuring the Stark shift in atomic resonance lines due to an applied electric field, and has been used previously for studying electric fields near surfaces such as those found at liquid helium temperatures or in high vacuum environments.  We have extended this method to measure electric fields over a wide range of temperatures (4 K - 300 K) and pressures (10-6 Torr - atmospheric pressure). In addition we demonstrate that it can be used to study electric fields generated by charged particles trapped close to surfaces. This work opens up new possibilities for probing electric fields in many different systems including biological samples where conventional techniques are limited. Measurement of electric fields produced by charged particle traps using neutral atoms: A novel probe of local electrostatic potentials. Measuring electric fields produced by surface contaminant... Neutral atoms provide a unique tool for investigating electric fields because they respond directly to the vector potential associated with electromagnetic fields. Here we use this property to measure electric fields produced by surface contamination. Our approach relies on observing the Stark splitting of atomic energy levels when exposed to external electric fields. Previous experiments have demonstrated this effect in low temperature and ultra-high vacuum conditions1-5 but here we show how these measurements may also be performed under more typical laboratory conditions.",
        "watermark_text": "We report on the determination of electric forces generated by surface contaminants using neutral compounds as probes . The technique is based on measuring the Stark shift in atomic resonance lines owing to an applied electric field , and has been used earlier for studying electric forces near structures such as those observed at liquid helium concentrations or in high vacuum environments .We have extended this method to measure electric forces over a broad variety of pressures ( 4 K - 300 K ) and pressures ( 10 - 6 Torr - atmospheric pressure ) . In addition we prove that it can be used to study electric forces generated by charged particles caught nearby to surfaces .This research provides up new possibilities for probing electric forces in multiple diverse systems including bio samples where conventional methods are small . Measurement of electric forces generated by charged particle traps involving neutral particles : A innovative investigation of local electrostatic potentials .Measuring electric forces generated by surface contaminant . . . Neutral atoms represent a unique technique for investigating electric forces because they react directly to the vector potential identified with electromagnetic forces .Here we utilize this property to measure electric forces generated by surface contamination . Our solution involves on observing the Stark separation of nuclear electricity levels when exposed to external electric forces .Previous experiments have demonstrated this effect in low heat and ultra - large vacuum conditions1 - 5 but here we give how these measurements may also be performed under more typical laboratory situations .",
        "rewrite_text": "We present our findings on the measurement of electric forces generated by surface contaminants using neutral compounds as probes. This technique relies on detecting the Stark shift in atomic resonance lines due to an applied electric field, and it has previously been employed to analyze electric forces near structures found in environments such as liquid helium concentrations or high vacuum conditions. We have expanded this approach to assess electric forces across a wide range of temperatures (4 K - 300 K) and pressures (10^-6 Torr to atmospheric pressure). Furthermore, we demonstrate its applicability in studying electric forces produced by charged particles near surfaces. This research opens new avenues for investigating electric forces in various systems, including biological samples, where traditional methods are limited. By utilizing neutral atoms, we employ a distinctive method for exploring electric forces, as they are influenced directly by the vector potential associated with electromagnetic interactions. Our approach involves observing the Stark separation of nuclear energy levels when subjected to external electric fields. Prior experiments have showcased this effect under low temperatures and ultra-high vacuum conditions, but we illustrate how these measurements can also be conducted in more typical laboratory environments.",
        "ori-fast-z-score": 0.08606629658238704,
        "water-fast-z-score": 7.774654685222524,
        "rewrite-fast-z-score": -1.4237369936287485
    },
    {
        "original_text": "We report the discovery and characterization of USco 1606-1935, an unusually wide low-mass triple system with two M-dwarfs orbiting each other in a ~2 year period at a distance of about 100 AU (~33 light years). The third component is a late K-type star that orbits both stars on a much wider scale, with a minimum mass for this companion of 0.7 solar masses. We present near-infrared spectroscopy to determine the spectral types of all three components as well as their radial velocities. Our results show that the inner binary has a total mass of only 0.3 solar masses, making it one of the lowest-mass binaries known. This makes USco 1606-1935 an ideal target for future studies of planet formation around very-low-mass stars. In addition, we find evidence for significant orbital eccentricity in the outer orbit which may be caused by tidal interactions between the close pair and its distant tertiary companion.",
        "watermark_text": "We report the discovery and characterization of USco 1606 - 1935 , an exceptionally wide low - mass triple system with two M - dwarfs orbiting each other in a ~ 2 week period at a distance of about 100 AU ( ~ 33 light years ) . The third component is a early K - class star that orbits both stars on a far larger scale , with a minimum mass for this companion of 0 . 7 solar masses .We present near - infrared spectroscopy to obtain the spectral classes of all three components as also as their radial velocities . Our results show that the inner binary has a total mass of only 0 . 3 solar masses , making it one of the smallest - energy binaries discovered .This gives USco 1606 - 1935 an suitable target for future research of planet development around very - low - weight stars . In addition , we find proof for significant orbital eccentricity in the exterior orbit which may be caused by tidal interactions between the close pair and its closest primary companion .",
        "rewrite_text": "We present the discovery and analysis of USco 1606-1935, an unusually wide low-mass triple system featuring two M-dwarfs that orbit each other with a period of approximately two weeks at a distance of around 100 AU (about 33 light years). The third member is an early K-type star, which orbits both M-dwarfs at a significantly larger distance, with a minimum mass of 0.7 solar masses for this companion. Through near-infrared spectroscopy, we have determined the spectral classifications and radial velocities of all three stars. Our findings indicate that the inner binary has a total mass of only 0.3 solar masses, ranking it among the lowest-energy binaries identified to date. This makes USco 1606-1935 an excellent candidate for further investigations into planet formation around very low-mass stars. Additionally, we have found evidence of notable orbital eccentricity in the outer orbit, likely resulting from tidal interactions between the closely orbiting pair and their nearest primary companion.",
        "ori-fast-z-score": -1.7320508075688772,
        "water-fast-z-score": 3.2118202741878643,
        "rewrite-fast-z-score": -0.46499055497527714
    },
    {
        "original_text": "We consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j",
        "watermark_text": "We consider the statistical characteristics of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions , power spectra , and likelihood density functions ( PDFs ) . We see that these quantities can be described by means of solutions to many partial differential coefficients with time - dependent coefficients .The PDFs are derived for both static and nonstationary cases using the method of characteristics . In particular , we derive an precise representation for the PDF of the amplitude fluctuations of a monochromatic plane beam propagating through a turbulent medium .This result is utilized to obtain definitions for the mean - square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation path . Finally , we explain some applications of our findings to radiowave scintillation physics and radar detection difficulties .PACS : 42 . 65 . Tg , 43 . 20 . Fx , 47 . 55 . + q , 47 . 60 . + j",
        "rewrite_text": "We analyze the statistical properties of nonstationary random acoustic and electromagnetic waves by examining their correlation functions, power spectra, and probability density functions (PDFs). Our findings indicate that these quantities can be represented by solutions to various partial differential equations with time-dependent coefficients. We derive the PDFs for both static and nonstationary scenarios utilizing the method of characteristics. In particular, we provide a precise formulation for the PDF of amplitude fluctuations in a monochromatic plane wave traveling through a turbulent medium. This result is then used to define the mean-square fluctuation concentrations of electric field intensity and intensity at any point along the propagation path. Finally, we discuss the implications of our results for radio wave scintillation phenomena and challenges in radar detection. PACS: 42.65.Tg, 43.20.Fx, 47.55.+q, 47.60.+j",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 1.7232808737106582
    },
    {
        "original_text": "We study the generation of large-scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number, Pm = ν/η ≪ 1 (ν is viscosity, η is resistivity). We consider two different types of flows that are relevant for this problem -helically driven turbulence and convection-driven turbulence. In both cases we find that the mean electromotive force has contributions from several terms which scale differently as functions of the Reynolds number Re = UL/ν and the magnetic Reynolds number Rm = URm/η. Here U , L, and Rm are characteristic velocity, length, and magnetic field scales respectively.  For helically driven turbulence these contributions can be grouped into three categories:  The first category includes all terms proportional to Re(Rm)−1/2 . These terms have been studied previously by many authors using various approaches including direct numerical simulations. They represent the contribution of the so-called α-effect due to helical motions. The second category contains all terms proportional to Re1/2 (Rm)−1/4 . This term represents the effect of helicity on the nonlinear evolution of the magnetic fluctuations. Finally, there exists also an additional third category containing all terms proportional to Re3/4 (Rm)−3/8 . It describes the influence of helicity on the linear growth rate of the magnetic fluctuations.",
        "watermark_text": "We explore the generation of large - scale magnetic fields in astrophysical plasmas with very small values of the magnetic Prandtl number , Pm = ν / η [UNK] 1 ( ν is viscosity , η is resistivity ) . We consider two different kinds of flows that are applicable for this question - helically controlled turbulence and convection - powered turbulence .In both cases we find that the mean electromotive pressure has contributions from several terms which scale differently as functions of the Reynolds number Re = UL / ν and the magnetic Reynolds number Rm = URm / η . Here U , L , and Rm are characteristic velocity , length , and magnetic field scales respectively .For helically controlled turbulence these contributions can be grouped into three categories : The first class includes all terms proportional to Re ( Rm ) −1 / 2 . These terms have been studied historically by many writers using numerous methodology including direct numerical simulations .They represent the contribution of the so - called α - effect owing to helical movements . The second class includes all terms proportional to Re1 / 2 ( Rm ) −1 / 4 .This term indicates the impact of helicity on the nonlinear development of the magnetic fluctuations . Finally , there exists additionally an additional third category containing all terms proportional to Re3 / 4 ( Rm ) −3 / 8 .It describes the impact of helicity on the linear expansion frequency of the magnetic fluctuations .",
        "rewrite_text": "We investigate the generation of large-scale magnetic fields in astrophysical plasmas characterized by very low magnetic Prandtl numbers, Pm = ν / η [UNK] 1, where ν represents viscosity and η denotes resistivity. Our analysis focuses on two specific types of flows pertinent to this topic: helically controlled turbulence and convection-driven turbulence. In both scenarios, we discover that the mean electromotive force consists of multiple terms that scale differently with the Reynolds number Re = UL / ν and the magnetic Reynolds number Rm = URm / η, where U, L, and Rm refer to typical scales of velocity, length, and magnetic field, respectively. For helically controlled turbulence, we can categorize these contributions into three groups: The first group encompasses terms proportional to Re (Rm)^−1/2, which have been extensively examined in the literature through various methodologies, including direct numerical simulations. These terms represent the contribution from the α-effect associated with helical motions. The second group consists of terms proportional to Re^1/2 (Rm)^−1/4, highlighting the influence of helicity on the nonlinear growth of magnetic fluctuations. Finally, the third group includes terms proportional to Re^3/4 (Rm)^−3/8, describing the effect of helicity on the linear growth rate of the magnetic fluctuations.",
        "ori-fast-z-score": 0.9332565252573828,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": -0.8728715609439696
    },
    {
        "original_text": "We present new near-infrared (NIR) spectroscopic observations with Keck II/DEIMOS, which cover the entire optical extent of the nearby spiral galaxy M33 out to its last measured isophote at 25 mag arcsec-2 in B-band. We also use archival data obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our study. The main goal of this work was to investigate how star formation proceeds beyond the edge of galactic disks into the surrounding intergalactic medium. Our results show that there are two distinct components along the line-of-sight towards M33: an extended component associated with diffuse ionized gas and young stars; and a compact component dominated by old stellar populations. Using these NIR spectra we have derived radial profiles of several physical parameters such as electron density, temperature, extinction coefficient etc., across the face-on view of M33 s disk. These profiles reveal interesting trends in the properties of interstellar matter within different regions of the galaxy.",
        "watermark_text": "We use new near - infrared ( NIR ) spectroscopic observations with Keck II / DEIMOS , which cover the entire optical extent of the nearby spiral galaxy M33 out to its last detected isophote at 25 mag arcsec - 2 in B - band . We additionally using archival measurements obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our research .The main goal of this research was to examine how star formation flows beyond the boundary of galactic disks into the nearby intergalactic medium . Our results show that there are two separate constituents along the line - of - seeing towards M33 : an extended component involved with diffuse ionized gas and older stars ; and a compact component dominated by aged stellar regions .Using these NIR spectra we have derived radial profiles of several physical factors such as electron concentration , temperature , extinction factor etc . , across the face - on vision of M33 s disk . These profiles indicate unusual trends in the properties of interstellar matter within various regions of the universe .",
        "rewrite_text": "We employed new near-infrared (NIR) spectroscopic observations from Keck II/DEIMOS, which encompass the full optical extent of the nearby spiral galaxy M33, extending to its last detected isophote at 25 mag arcsec -2 in the B-band. In addition, we incorporated archival data from the Infrared Array Camera aboard the Spitzer Space Telescope for our study. The primary objective of this research was to investigate the flow of star formation beyond the boundaries of galactic disks into the neighboring intergalactic medium. Our findings reveal two distinct components along the line of sight towards M33: an extended component associated with diffuse ionized gas and older stars, and a compact component dominated by aging stellar regions. Using these NIR spectra, we derived radial profiles of various physical parameters such as electron concentration, temperature, and extinction factor across the face-on view of M33's disk. These profiles highlight unusual trends in the characteristics of interstellar matter across different regions of the universe.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 2.254885150568321
    },
    {
        "original_text": "We present new observations of the molecular gas in the central region of the nearby galaxy NGC 891, obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal an extended distribution of dense (n(H2) ~ 104 cm-3), warm (T~50K) molecular gas that is associated with the optical disk of this edge-on spiral galaxy. We find evidence for two distinct components to the molecular gas distribution; one component follows closely the dust lane seen in visible light images while another component extends out into the surrounding intergalactic medium. This latter component has been detected previously by other authors but our higher resolution data allow us to resolve it into individual clouds. In addition we detect several compact sources within the galactic plane which are likely to be young star forming regions. These results suggest that there may exist a significant reservoir of molecular material outside the main body of galaxies such as NGC 891.",
        "watermark_text": "We present new images of the molecular gas in the central region of the nearby galaxy NGC 891 , obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths . The data reveal an extended distribution of dense ( n ( H2 ) ~ 104 mm - 3 ) , warm ( T ~ 50K ) molecular dust that is associated with the optical disk of this edge - on spiral galaxy .We see evidence for two different components to the molecular gas distribution ; one element follows tightly the dust track seen in apparent light photographs while another component moves out into the nearby intergalactic medium . This latter component has been detected earlier by other researchers but our higher resolution data enable us to separate it into single clouds .In addition we locate many compact sources within the galactic plane which are likely to be young galaxy producing regions . These data suggest that there may contain a substantial pool of molecular matter outside the main bodies of stars such as NGC 891 .",
        "rewrite_text": "We present new imagery of the molecular gas in the central region of the nearby galaxy NGC 891, captured using the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal a widespread distribution of dense (n(H2) ~ 10^4 mm^-3) and warm (T ~ 50K) molecular dust associated with the optical disk of this edge-on spiral galaxy. Our observations indicate two distinct components in the molecular gas distribution: one closely follows the dust path observed in optical photographs, while the other extends into the nearby intergalactic medium. Although this latter component has been identified by previous studies, our higher resolution data allow us to distinguish it into individual clouds. Additionally, we identify numerous compact sources within the galactic plane, which are likely regions of ongoing star formation. These findings suggest a significant reservoir of molecular matter exists beyond the primary stellar structures of NGC 891.",
        "ori-fast-z-score": -1.6666666666666667,
        "water-fast-z-score": 6.260990336999411,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "We present the results of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe (WMAP) satellite and the Swift observatory in the first year of operation, 2004-05. We find that there are no significant differences between the two samples when we compare their distributions for redshift, luminosity distance, radio flux density at 1 GHz, optical magnitude, or X-ray photon index. The only difference is found to be in the distribution of redshifts; this may be due to selection effects caused by the different energy bands used by each instrument. \n \n Keywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray bursts, galaxy clusters, dark matter, dark energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei",
        "watermark_text": "We present the conclusion of our analysis on blazars detected by both the Wilkinson Microwave Anisotropy Probe ( WMAP ) satellite and the Swift telescope in the first year of operation , 2004 - 05 . We see that there are no considerable changes between the two specimens when we compare their distributions for redshift , luminosity distance , radio flux concentration at 1 GHz , optical magnitude , or X - ray photon index .The only difference is found to be in the distribution of redshifts ; this might be due to choice influences resulting by the different energy bands used by each instrument . Keywords : Blazar , Swift , WMAP , survey , cosmology , statistics , gamma - ray clusters , galaxy rings , soft material , soft energy , neutrino mass , cosmic microwave background radiation , anisotropies , large - scale structure , gravity lensing , relativistic jets , quasar , active galactic nuclei",
        "rewrite_text": "We present the findings of our analysis on blazars identified by both the Wilkinson Microwave Anisotropy Probe (WMAP) satellite and the Swift telescope during their first operational year, 2004-2005. Our comparison of their distributions reveals no significant differences in redshift, luminosity distance, radio flux at 1 GHz, optical magnitude, or X-ray photon index. The only notable distinction is observed in the redshift distributions, which may be influenced by the different energy bands utilized by each instrument. \n\nKeywords: Blazar, Swift, WMAP, survey, cosmology, statistics, gamma-ray clusters, galaxy rings, soft material, soft energy, neutrino mass, cosmic microwave background radiation, anisotropies, large-scale structure, gravitational lensing, relativistic jets, quasar, active galactic nuclei.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 3.3113308926626095,
        "rewrite-fast-z-score": -0.2721655269759087
    },
    {
        "original_text": "We present the first measurement of single-transverse-spin asymmetries (SSA) for hadronic dijets produced at midrapidity in p+p collisions at sqrt(sNN) = 5.02 TeV using data collected by the CMS experiment during 2012 corresponding to an integrated luminosity of 2.3 fb-1 . The SSAs are extracted as functions of jet transverse momentum and rapidity, azimuthal angle between jets, and event centrality. We observe no significant dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations including higher-order corrections and parton distribution function uncertainties. \nThe measured values agree well within experimental and theoretical uncertainties. This is the most precise measurement of this observable performed so far. \n \n Introduction \n \n Single transverse-spin asymmetries have been observed in several processes involving polarized protons or neutrons  1  , such as inclusive pion production  2  , semi-inclusive deep-inelastic scattering  3  , Drell-Yan lepton pair production  4  , prompt photon production  5  , and direct photons  6  . These measurements provide important information about the spin structure of nucleons  7, 8  .\n \nIn particular, they can be used to test the validity of factorization theorems  9  which relate hard-scattering cross sections to partonic distributions inside the proton  10  . In addition, these observables may also shed light on new physics beyond the Standard Model  11  . \n \n For example, it has recently been suggested  12  that large single-spin asymmetries could arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high-energy pp collisions. Such effects would violate parity conservation and thus constitute evidence for new physics  13  . However, there exists only one previous measurement  14  of single-spin asymmeties in hadronic dijet production at high energies. That study was carried out at RHIC  15  where the center-of-mass energy per nucleon-nucleon collision √sNN=200 GeV is much lower",
        "watermark_text": "We present the first measurement of single - transverse - spinning asymmetries ( SSA ) for hadronic dijets created at midrapidity in p + p collisions at sqrt ( sNN ) = 5 . 02 TeV using data derived by the CMS experiment during 2012 corresponding to an unified luminosity of 2 . 3 fb - 1 . The SSAs are derived as functions of jet transverse momentum and rapidity , azimuthal angle between planes , and event centrality .We see no major dependence on any kinematic variable except that the magnitude of the asymmetry decreases with increasing jet rapidity . Our results are compared to theoretical estimates based on perturbative QCD calculations including higher - order corrections and parton distribution function uncertainties .The measured measures agree well within experimental and theoretical uncertainties . This is the most accurate calculation of this observable performed so far .Introduction Single transverse - spinning asymmetries have been observed in multiple processes involving polarized protons or neutrons 1 , such as inclusive pion production 2 , semi - inclusive dark - inelastic emission 3 , Drell - Yan lepton pair production 4 , prompt photon production 5 , and direct photons 6 . These measurements give important information about the spin composition of nucleons 7 , 8 .In particular , they can be used to test the legitimacy of factorization theorems 9 which compare hard - absorption cross sections to partonic distributions inside the proton 10 . In addition , these observables might additionally bring light on new science beyond the Standard Model 11 .For instance , it has recently been proposed 12 that wide single - spinning asymmetries may arise due to the interference of two amplitudes describing different helicities of quarks emitted from longitudinally polarized gluons in high - energy pp collisions . Such effects would violate parity conservation and therefore constitute evidence for recent science 13 .However , there exists only one previous measurement 14 of single - spinning asymmeties in hadronic dijet production at high energies . That experiment was carried out at RHIC 15 where the center - of - mass electricity per nucleon - nucleon collision √sNN = 200 GeV is much lower",
        "rewrite_text": "We report the inaugural measurement of single-transverse-spinning asymmetries (SSA) for hadronic dijets produced at midrapidity in p + p collisions at a center-of-mass energy of √sNN = 5.02 TeV. This analysis utilizes data collected by the CMS experiment in 2012, corresponding to an integrated luminosity of 2.3 fb⁻¹. The SSAs are examined as functions of jet transverse momentum and rapidity, the azimuthal angle between the event planes, and event centrality. We find no significant dependence on any kinematic variable, other than that the asymmetry magnitude diminishes with increasing jet rapidity. Our results are compared to theoretical predictions based on perturbative QCD calculations, which account for higher-order corrections and uncertainties in parton distribution functions. The measurements align closely with both experimental and theoretical uncertainties, representing the most precise assessment of this observable to date.\n\nIntroduction: Single transverse-spinning asymmetries have been observed in various processes involving polarized protons and neutrons. These processes include inclusive pion production, semi-inclusive deep-inelastic scattering, Drell-Yan lepton pair production, and prompt photon production. Such measurements provide crucial insights into the spin structure of nucleons. Specifically, SSAs can be used to validate factorization theorems that relate hard-scattering cross sections to partonic distributions within the proton. Moreover, these observables might shed light on new phenomena beyond the Standard Model. Recent theories suggest that wide single-spinning asymmetries may result from the interference of two amplitudes connected to different helicities of quarks emitted from longitudinally polarized gluons in high-energy proton-proton collisions. This would imply a violation of parity conservation, indicating potential new physics. However, only one previous measurement of single-spinning asymmetries in high-energy hadronic dijet production exists, conducted at RHIC, where the center-of-mass energy per nucleon-nucleon collision was significantly lower at √sNN = 200 GeV.",
        "ori-fast-z-score": 0.43685202833051895,
        "water-fast-z-score": 7.077002858954407,
        "rewrite-fast-z-score": -0.6211495565912797
    },
    {
        "original_text": "The collective properties of odd-mass nuclei are investigated in terms of the interacting vector boson model (IVBM). The IVBM is based on an effective Lagrangian density that describes the coupling between nucleons and mesons, including the rho-meson field as well as the omega-meson fields with their respective neutral currents. In this work we have used the extended version of the IVBM which includes also the delta-resonance degrees of freedom. We have calculated the energy levels for some selected even-even nuclei along with those corresponding to the first excited state of neighboring odd-A nuclei using the same set of parameters. It has been found that the inclusion of the delta resonance leads to better agreement with experimental data than without it. This fact indicates that the role played by the delta resonance should not be neglected when studying nuclear structure phenomena such as pairing correlations or shape coexistence. Finally, we have studied the effect of the spin-orbit interaction on the ground-state band built upon the lowest 0+ state.",
        "watermark_text": "The collective characteristics of odd - weight ions are examined in terms of the interacting vector boson theory ( IVBM ) . The IVBM is based on an appropriate Lagrangian density that describes the interaction between nucleons and mesons , notably the rho - meson field as well as the omega - meson fields with their different neutral currents .In this research we have utilized the extended version of the IVBM which includes also the delta - resonance degrees of liberty . We have predicted the power concentrations for some selected even - even molecules along with those corresponding to the first excited state of neighboring odd - A nuclei using the same list of constraints .It has been shown that the introduction of the delta resonance results to higher agreement with observation information than without it . This fact suggests that the importance played by the delta resonance should not be forgotten when examining nuclear formation dynamics such as pairing correlations or pattern coexistence .Finally , we have researched the impact of the spin - orbit interaction on the surface - state band building upon the lowest 0 + state .",
        "rewrite_text": "The collective properties of odd-weight ions are investigated using the interacting vector boson theory (IVBM). This theory is derived from a suitable Lagrangian density that describes interactions between nucleons and mesons, particularly the rho-meson and omega-meson fields, each featuring distinct neutral currents. In our study, we employed an extended version of IVBM that incorporates delta resonance degrees of freedom. We have made predictions regarding the power concentrations for selected even-even nuclei, as well as for the first excited states of neighboring odd-A nuclei, using the same set of constraints. Our findings indicate that incorporating delta resonance leads to a better alignment with observational data compared to models that exclude it. This underscores the significance of delta resonance when examining nuclear dynamics, such as pairing correlations and pattern coexistence. Additionally, we explored the influence of spin-orbit interactions on the surface-state band formed from the lowest 0+ state.",
        "ori-fast-z-score": -1.9877674693472376,
        "water-fast-z-score": 5.597977259474208,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "We study the exact Floquet states of a Bose-Einstein condensate (BEC) in an optical lattice under periodic driving, which is realized by periodically modulating the depth of the optical potential. We show that there are two types of Floquet states depending on whether they have zero or nonzero quasienergies. The former ones correspond to the usual Bloch bands while the latter ones represent the so-called Floquet-Bloch bands. In particular, we find that the Floquet-Bloch band structure can be obtained as a result of hybridization between different Bloch bands with opposite momenta. Furthermore, we investigate how these Floquet states evolve when the system parameters change. Finally, we discuss the stability properties of the Floquet states against small perturbations. Our results provide useful insights into the physics of periodically-driven quantum systems. Introduction:-Recent experimental advances allow for realizing artificial gauge fields  1  , synthetic dimensions  2  , topological phases  3  , and even time crystals  4  . These fascinating phenomena are usually observed in ultracold atomic gases trapped in optical lattices  5  .\nIn this work, we consider a Bose-Einstein Condensate (BEC) confined in such a one-dimensional (1D) optical lattice  6  . By applying external laser beams  7, 8  , it is possible to create a periodic modulation of the optical potential  9  . This leads to a periodic variation of the hopping amplitude J(t), which plays the role of a time-dependent Peierls phase  10  . As a consequence, the effective Hamiltonian describing our system becomes time-periodic  11  . It has been shown recently  12  that the corresponding Schrödinger equation admits solutions known as Floquet states  13  . They describe the evolution of the wave function over one period T = 2π/ω 0 where ω 0 denotes the frequency of the periodic drive  14  . Since the Floquet states are not stationary but rather oscillate at the same frequency as the drive  15  , they may exhibit interesting physical features  16  . For example, Floquet engineering allows us to realize exotic superfluidity  17  , non-Abelian anyons  18  , and Major",
        "watermark_text": "We work the exact Floquet states of a Bose - Einstein condensate ( BEC ) in an optical lattice under periodic forcing , which is realized by periodically modulating the depth of the optical potential . We see that there are two forms of Floquet states varying on whether they have zero or nonzero quasienergies .The former ones relate to the usual Bloch groups while the latter ones represent the so - called Floquet - Bloch groups . In particular , we find that the Floquet - Bloch group structure can be obtained as a result of hybridization between various Bloch groups with opposite momenta .Furthermore , we investigate how these Floquet states evolve when the process variables alter . Finally , we investigate the stability properties of the Floquet states against small perturbations .Our results yield useful insights into the physics of periodically - triggered quantum systems . Introduction : - Recent research developments enable for realizing artificial gauge fields 1 , synthetic dimensions 2 , topological phases 3 , and even period crystals 4 .These strange phenomena are typically observed in ultracold atomic gases locked in optical lattices 5 . In this research , we imagine a Bose - Einstein Condensate ( BEC ) locked in such a one - dimensional ( 1D ) optical lattice 6 .By applying external beam waves 7 , 8 , it is common to create a periodic modulation of the optical potential 9 . This leads to a periodic variation of the hopping frequency J ( t ) , which plays the role of a time - dependent Peierls phase 10 .As a consequence , the effective Hamiltonian describing our system gets time - periodic 11 . It has been shown recently 12 that the associated Schrödinger equation accepts solutions known as Floquet states 13 .They define the evolution of the wave function over one period T = 2π / ω 0 where α 0 denotes the frequency of the periodic drive 14 . Since the Floquet states are not stationary but rather oscillate at the same frequency as the drive 15 , they may exhibit exciting physical features 16 .For instance , Floquet engineering enables us to realize unusual superfluidity 17 , non - Abelian anyons 18 , and Major",
        "rewrite_text": "We analyze the precise Floquet states of a Bose-Einstein condensate (BEC) confined in an optical lattice that is subjected to periodic modulation of the optical potential's depth. Our investigation reveals two distinct types of Floquet states based on their quasienergy characteristics: states with zero quasienergy, which correspond to conventional Bloch bands, and states with nonzero quasienergy, known as Floquet-Bloch states. Notably, we discover that the structure of the Floquet-Bloch states arises from the hybridization of various Bloch bands with opposite momenta. Additionally, we explore the evolution of these Floquet states in response to changes in the external parameters. Lastly, we examine their stability with respect to minor perturbations. Our findings provide valuable insights into the dynamics of periodically driven quantum systems. \n\n**Introduction:** Recent advancements have made it possible to engineer artificial gauge fields, synthetic dimensions, topological phases, and even time crystals. These intriguing phenomena are often observed in ultracold atomic gases confined in optical lattices. In this study, we consider a BEC trapped in a one-dimensional optical lattice. By utilizing external beam waves, we can achieve a periodic modulation of the optical potential. This modulation results in a time-dependent variation of the hopping frequency \\( J(t) \\), which serves as a time-dependent Peierls phase. Consequently, the effective Hamiltonian governing our system becomes time-periodic. Recent research has demonstrated that the corresponding Schrödinger equation has solutions termed Floquet states, which describe the evolution of the wave function over a period \\( T = 2\\pi / \\omega_0 \\), where \\( \\omega_0 \\) is the frequency of the periodic drive. Since Floquet states oscillate in synchrony with the driving frequency, they can exhibit remarkable physical properties. For instance, Floquet engineering facilitates the realization of novel superfluid behavior, non-Abelian anyons, and Majorana fermions.",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 7.333333333333333,
        "rewrite-fast-z-score": 1.8347785189881796
    },
    {
        "original_text": "We show that correlation clustering is NP-hard to approximate within any constant factor, even for graphs with maximum degree three and clusters of size at most four.  We also present an algorithm which solves this problem exactly in time O(n3). Our results are based on reductions from the exact cover by 3-sets (X3C) problem. The X3C problem asks whether there exists a collection of subsets of a set S such that each element of S belongs to exactly 3 sets in the collection; it has been shown to be NP-complete. For more information about our work see http://arxiv.org/abs/1206.0571 . \nCorrelation clustering is one of several problems studied under the umbrella of  clustering ; these include k-means clustering, spectral clustering, and graph partitioning. It was introduced independently by Bansal et al., Blum et al., and Dasgupta et al. as follows.   Given a weighted undirected graph G = (V, E), where V denotes the vertices and E denotes the edges, we say that two vertices u, v ∈ V are adjacent if they share an edge e ∈ E. A cluster C ⊆ V is defined as a subset of nodes whose pairwise distances satisfy some threshold t > 0. More formally, given a distance function d : V × V → R+ , let dist(u,v) denote the shortest path between u and v; then, C is said to be a valid cluster if and only if for all pairs of nodes u, v ∈ C:  dist(u,v)  ≤ t",
        "watermark_text": "We see that correlation clustering is NP - hard to approximate within any constant factor , even for graphs with maximum degree three and clusters of diameter at most four . We also demonstrate an algorithm which solves this situation exactly in time O ( n3 ) .Our results are based on reductions from the exact cover by 3 - sets ( X3C ) question . The X3C problem questions whether there exists a family of subsets of a space S such that each element of S belongs to exactly 3 sets in the collection ; it has been shown to be NP - full .For more information about our work see http : / / arxiv . org / abs / 1206 . 0571 . Correlation clustering is one of several problems studied under the umbrella of clustering ; these include k - means clustering , spectral clustering , and graph partitioning .It was introduced independently by Bansal et al . , Blum et al . , and Dasgupta et al . as follows .Given a weighted undirected graph G = ( V , E ) , where V denotes the edges and E denotes the edges , we write that two vertices u , v ∈ V are adjoining if they share an vertex e ∈ E . A cluster C ⊆ V is characterized as a subset of vertices whose pairwise distances satisfy some threshold t > 0 . More generally , given a distance relation d : V × V → R + , let dist ( v , v ) define the longest route between u and v ; then , C is said to be a valid cluster if and only if for all pairs of vertices u , v ∈ C : dist ( v , v ) ≤ t",
        "rewrite_text": "Correlation clustering is proven to be NP-hard to approximate within any constant factor, even when restricted to graphs with a maximum degree of three and clusters with a diameter of no more than four. We present an algorithm that exactly resolves this issue in O(n³) time. Our findings are based on reductions from the Exact Cover by 3-Sets (X3C) problem, which seeks to determine if there exists a collection of subsets from a set S such that each element in S is contained in precisely three of these subsets; this problem is known to be NP-complete. For further details on our research, please visit http://arxiv.org/abs/1206.0571. Correlation clustering is one of several problems explored within the realm of clustering, which also includes k-means clustering, spectral clustering, and graph partitioning. This concept was independently introduced by Bansal et al., Blum et al., and Dasgupta et al. Specifically, given a weighted undirected graph G = (V, E), where V represents the vertices and E the edges, two vertices u and v in V are considered adjacent if they share a common edge e in E. A cluster C ⊆ V is defined as a subset of vertices in which the pairwise distances meet a certain threshold t > 0. More broadly, for a distance relation d: V × V → R⁺, let dist(u, v) denote the longest distance between vertices u and v; then, C is regarded as a valid cluster if for every pair of vertices u, v in C, the condition dist(u, v) ≤ t holds true.",
        "ori-fast-z-score": -1.0945409092309881,
        "water-fast-z-score": 3.482630165734962,
        "rewrite-fast-z-score": 0.29851115706299675
    },
    {
        "original_text": "We present the radio through X-ray spectral energy distributions (SEDs) for 38 quasars with broad absorption lines in their optical spectra, selected from the Sloan Digital Sky Survey Data Release 5 quasar catalog and observed by Chandra and/or XMM-Newton. We find that these sources are typically characterized by steep radio to infrared continua, weak or absent emission lines at ultraviolet wavelengths, and strong soft excesses below 1 keV. The majority of our sample show evidence for significant intrinsic reddening as indicated by the presence of deep UV troughs and high values of the Balmer decrement. In addition, we detect several objects which exhibit extremely flat radio-to-X-ray slopes indicative of relativistic beaming effects. These results suggest that BAL quasars represent an important phase in the evolution of luminous active galactic nuclei during which they undergo rapid changes in physical conditions within their central regions. This is supported by recent theoretical models suggesting that BAL outflows may play an important role in regulating black hole growth via feedback processes. \n \n Keywords: Active Galactic Nuclei",
        "watermark_text": "We present the radio through X - ray spectral power distributions ( SEDs ) for 38 quasars with broad absorption lines in their optical spectra , selected from the Sloan Digital Sky Survey Data Release 5 quasar catalog and detected by Chandra and / or XMM - Newton . We see that these sources are typically characterized by steep radio to laser continua , faint or omitted emitted lines at ultraviolet wavelengths , and strong soft excesses below 1 keV .The majority of our sample present evidence for significant intrinsic reddening as indicated by the presence of deep UV troughs and large values of the Balmer decrement . In addition , we find several bodies which exhibit exceptionally flat radio - to - X - ray curves indicative of relativistic beaming effects .These data suggest that BAL quasars represent an important stages in the evolution of luminous active galactic nuclei during which they undergo rapid variations in physical conditions within their central regions . This is backed by recent theoretical theories suggesting that BAL outflows might play an important role in controlling black hole growth via feedback systems .Keywords: Active Galactic Nuclei",
        "rewrite_text": "We present the spectral energy distributions (SEDs) from radio to X-ray for 38 quasars exhibiting broad absorption lines in their optical spectra. These quasars were selected from the Sloan Digital Sky Survey Data Release 5 catalog and have been detected by either Chandra or XMM-Newton. Our analysis reveals that these sources are generally characterized by steep radio to optical continua, faint or absent emitted lines in the ultraviolet range, and pronounced soft excesses below 1 keV. Most of the quasars in our sample show signs of significant intrinsic reddening, as indicated by deep ultraviolet troughs and high Balmer decrement values. Moreover, we identify several objects with remarkably flat radio-to-X-ray spectra, which suggest the influence of relativistic beaming effects. These findings imply that BAL quasars may represent critical phases in the evolution of luminous active galactic nuclei, marked by rapid changes in the physical conditions at their centers. This is further supported by recent theoretical models proposing that BAL outflows could play a crucial role in modulating black hole growth through feedback mechanisms. Keywords: Active Galactic Nuclei.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 5.962965874907927,
        "rewrite-fast-z-score": -1.118033988749895
    },
    {
        "original_text": "The rate at which new mutations are fixed in populations is determined by the balance between natural selection and genetic drift, but how these forces interact to shape the evolution of proteins remains poorly understood.  Here we show that neutral genetic drift can accelerate the fixation of beneficial mutations within genes encoding functionally important regions of proteins. We used deep mutational scanning to measure the fitness effects of all possible single amino acid substitutions for two enzymes (T4 lysozyme and TEM-1 β-lactamase) across their entire sequence space. By combining this data with population genetics simulations, we found that neutral genetic drift increased the probability of fixing beneficial mutations in both enzymes  active sites. This effect was particularly pronounced when the number of adaptive mutations available per generation was low or when there were many competing deleterious mutations. Our results suggest that neutral genetic drift may play an underappreciated role in shaping the evolution of proteins.",
        "watermark_text": "The rate at which new mutations are fixed in populations is governed by the balance between natural selection and genetic drift , but how these forces combine to shape the evolution of proteins appears poorly explored . Here we show that neutral genetic drift can accelerate the fixation of beneficial mutations within genes encoding functionally essential regions of proteins .We utilized deep mutational scanning to measure the fitness impacts of all possible single amino residue substitutions for two enzymes ( T4 lysozyme and TEM - 1 β - lactamase ) across their whole sequence area . By combining this data with population genetics simulations , we identified that neutral genetic drift enhanced the probability of fixing beneficial mutations in both enzymes active sites .This influence was especially pronounced when the proportion of adaptive traits usable per generation was poor or when there were several different deleterious traits . Our results show that neutral genetic drift might play an underappreciated importance in shaping the evolution of proteins .",
        "rewrite_text": "The fixation of new mutations in populations is influenced by the interplay of natural selection and genetic drift, yet the way these mechanisms together affect protein evolution remains under-explored. In this study, we demonstrate that neutral genetic drift can speed up the fixation of beneficial mutations in genes that encode essential protein regions. We employed deep mutational scanning to assess the fitness effects of every possible single amino acid substitution in two enzymes, T4 lysozyme and TEM-1 β-lactamase, across their entire sequences. By integrating this data with population genetics simulations, we discovered that neutral genetic drift increased the likelihood of beneficial mutations fixing in the active sites of both enzymes. This effect became more pronounced in scenarios with limited adaptive traits available per generation or when several detrimental traits were present. Our findings suggest that neutral genetic drift may hold greater significance in protein evolution than previously recognized.",
        "ori-fast-z-score": -1.4100479758212652,
        "water-fast-z-score": 4.013213469645139,
        "rewrite-fast-z-score": -1.4269353798659745
    },
    {
        "original_text": "The effect of light quanta on the physiological response of seedlings subjected to nickel (Ni2+) stress was investigated in this study. The results showed that the growth and photosynthetic characteristics were significantly affected by different treatments, especially when exposed to high concentration of Ni2+ for 24 h. Compared with control group, the shoot length decreased by 39% under 100 μM Ni2+ treatment; however, it increased by 20% after exposure to red light (RL). In addition, the root length also reduced by 44% compared with control plants but increased by 25% following RL illumination. Moreover, the chlorophyll content per unit leaf area declined by 42% due to Ni2+ toxicity while it increased by 34% as a result of RL application. Furthermore, the net photosynthesis rate (Pn), stomatal conductance (gs), intercellular CO2 concentration (Ci), transpiration rate (Tr), and water use efficiency (WUE) all decreased by 46%, 49%, 50%, 51%, and 53% respectively at 100 μM Ni2+ treated plants whereas they increased by 29%, 28%, 30%, 31%, and 32% respectively upon exposure to RL. These findings suggested that the effects of Ni2+ toxicity could be alleviated through the application of RL.",
        "watermark_text": "The impact of light quanta on the metabolic reactions of seedlings subjected to nickel ( Ni2 + ) pressure was investigated in this study . The results showed that the development and photosynthetic characteristics were significantly impacted by various methods , particularly when exposed to large concentration of Ni2 + for 24 h . Compared with control group , the shoot length decreased by 39 % under 100 μM Ni2 + treatment ; however , it increased by 20 % after sensitivity to red light ( RL ) .In addition , the root length also declined by 44 % compared with control plants but improved by 25 % subsequent RL illumination . Moreover , the chlorophyll content per unit leaves area declined by 42 % due to Ni2 + toxicity while it increased by 34 % as a result of RL application .Furthermore , the net photosynthesis percentage ( Pn ) , stomatal conductance ( gs ) , intercellular CO2 density ( Ci ) , transpiration rate ( Tr ) , and water use performance ( WUE ) all decreased by 46 % , 49 % , 50 % , 51 % , and 53 % respectively at 100 μM Ni2 + treated stations whereas they increased by 29 % , 28 % , 30 % , 31 % , and 32 % respectively upon exposure to RL . These studies suggested that the effects of Ni2 + toxicity may be alleviated through the application of RL .",
        "rewrite_text": "This study explored the effects of light quanta on the metabolic reactions of seedlings exposed to nickel (Ni2+) stress. The results indicated that various treatment methods significantly affected growth and photosynthetic traits, especially when seedlings were subjected to high concentrations of Ni2+ for 24 hours. Specifically, the shoot length reduced by 39% under 100 μM Ni2+ treatment compared to the control group; however, it rebounded by 20% when exposed to red light (RL). Similarly, root length decreased by 44% relative to control plants but improved by 25% following RL illumination. Chlorophyll content per unit leaf area dropped by 42% due to Ni2+ toxicity, while RL application led to a 34% increase. Additionally, metrics such as net photosynthesis (Pn), stomatal conductance (gs), intercellular CO2 density (Ci), transpiration rate (Tr), and water use efficiency (WUE) fell by 46%, 49%, 50%, 51%, and 53% respectively at 100 μM Ni2+ concentrations, but increased by 29%, 28%, 30%, 31%, and 32% respectively in response to RL exposure. These findings suggest that the adverse effects of Ni2+ toxicity can be mitigated through the application of red light.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "The Spitzer Bright Field (SBF) is an all-sky survey at 3.6, 4.5, 5.8 & 8 microns carried out by the Infrared Array Camera on board the Spitzer Space Telescope.  The SBF was designed to provide deep infrared photometry for extragalactic studies in order to complement existing optical surveys such as the Sloan Digital Sky Survey.   This dataset contains images taken with IRAC channel 1 (3.6 micron), channel 2 (4.5 micron), channel 3 (5.8 micron) and channel 4 (8 micron). Each image has been processed using the MOPEX software package developed by the Spitzer Science Center. These images are available through the NASA/IPAC Extragalactic Database (NED).  For more information about this project please see http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html",
        "watermark_text": "The Spitzer Bright Field ( SBF ) is an all - skies study at 3 . 6 , 4 . 5 , 5 . 8 & 8 microns taken out by the Infrared Array Camera on board the Spitzer Space Telescope . The SBF was built to provide deep infrared photometry for extragalactic studies in order to complement existing imaging observations such as the Sloan Digital Sky Survey .This dataset contains images took with IRAC channel 1 ( 3 . 6 micron ) , channel 2 ( 4 . 5 micron ) , channel 3 ( 5 . 8 micron ) and channel 4 ( 8 micron ) . Each image has been processed utilizing the MOPEX software suite created by the Spitzer Science Center .These photographs are available through the NASA / IPAC Extragalactic Database ( NED ) . For more information about this project please view http : / / irsa . ipac . caltech . edu / data / SPITZER / docs / sbf / sbf - intro . html",
        "rewrite_text": "The Spitzer Bright Field (SBF) is a comprehensive all-sky survey conducted at wavelengths of 3.6, 4.5, 5.8, and 8 microns, using the Infrared Array Camera (IRAC) on the Spitzer Space Telescope. This survey was designed to deliver deep infrared photometry for extragalactic research, complementing existing imaging surveys such as the Sloan Digital Sky Survey. The dataset includes images captured by IRAC's four channels: channel 1 (3.6 microns), channel 2 (4.5 microns), channel 3 (5.8 microns), and channel 4 (8 microns). Each image has been processed with the MOPEX software suite developed by the Spitzer Science Center. These images are accessible through the NASA/IPAC Extragalactic Database (NED). For more details about the project, please visit http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 4.700096710803842,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "We present the results of an analysis to determine rates, masses, spins, and luminosities for intermediate mass ratio inspiral (IMRI) events detectable with advanced gravitational wave detectors such as Advanced LIGO. We use Monte Carlo simulations to generate IMRIs in galactic binaries that are consistent with current observations of binary pulsars and X-ray binaries. The simulated systems evolve through three phases: detached phase, Roche lobe overflow phase, and common envelope phase. In our simulation we assume that all stars have solar metallicity and initial spin periods of 10 days. For each system generated, we calculate its signal-to-noise ratio using the stationary phase approximation. We find that there will be about one event per year within 100 Mpc with signal-to-noise ratios greater than 8. This is comparable to the rate expected for double neutron star mergers. However, unlike double neutron star mergers which occur at high redshifts, most IMRI events should be detected nearby.",
        "watermark_text": "We publish the results of an assessment to estimate rates , masses , spins , and luminosities for intermediate mass ratio inspiral ( IMRI ) events detectable with sophisticated gravitational wave detectors such as Advanced LIGO . We use Monte Carlo simulations to produce IMRIs in galactic binaries that are compatible with current observations of binary pulsars and X - ray binaries .The simulated systems develop through three stages : detached phase , Roche lobe overflow mode , and shared envelope phase . In our modeling we suppose that all stars have solar metallicity and original spin times of 10 days .For each system generated , we determine its signal - to - noise proportion using the stationary phase approximation . We see that there will be about one incident per year within 100 Mpc with signal - to - noise ratios greater than 8 .This is analogous to the speed anticipated for double neutron star mergers . However , unlike single neutron star mergers which occur at high redshifts , most IMRI events should be identified nearby .",
        "rewrite_text": "We present the findings of an assessment aimed at estimating the rates, masses, spins, and luminosities of intermediate mass ratio inspiral (IMRI) events that can be detected by advanced gravitational wave observatories like Advanced LIGO. Utilizing Monte Carlo simulations, we generate IMRI systems within galactic binaries that align with current observations of binary pulsars and X-ray binaries. The evolution of these simulated systems progresses through three distinct phases: the detached phase, the Roche lobe overflow phase, and the shared envelope phase. In our modeling, we assume all stars possess solar metallicity and an initial spin period of 10 days. For each generated system, we calculate its signal-to-noise ratio using the stationary phase approximation. Our results indicate that approximately one event per year with a signal-to-noise ratio exceeding 8 is expected within a distance of 100 Mpc. This frequency is comparable to that predicted for double neutron star mergers. However, unlike single neutron star mergers which typically occur at high redshifts, most IMRI events are anticipated to be detected nearby.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 2.111111111111111
    },
    {
        "original_text": "The Kuiper belt is the source region for most short period comets, but its formation remains poorly understood. We present results from N-body simulations that show how collisions among planetesimals in Neptune s feeding zone can produce objects with orbits similar to those observed today. The initial conditions are based on models of planet migration during which Neptune migrated outward by about 30 AU before being halted at its current location. Our calculations suggest that the Kuiper belt formed as a result of collisional grinding between bodies whose sizes were comparable to Pluto (r ~ 1000 km). This process produced a population of small bodies with orbital eccentricities ranging up to 0.3. Subsequent encounters with Neptune caused some of these bodies to be scattered into highly eccentric orbits. These results provide an explanation for why there appears to be no correlation between the size distribution of KBOs and their orbital eccentricity.",
        "watermark_text": "The Kuiper belt is the origin region for most short period comets , but its formation appears poorly known . We publish results from N - bodies simulations that demonstrate how collisions among planetesimals in Neptune s feeding area can generate objects with orbits similar to those observed currently .The initial conditions are based on estimates of planet migration during which Neptune migrated outward by about 30 AU before being stopped at its current site . Our calculations suggest that the Kuiper belt developed as a outcome of collisional grinding between bodies whose sizes were analogous to Pluto ( r ~ 1000 kilometers ) .This process produced a population of tiny bodies with orbital eccentricities ranging up to 0 . 3 . Subsequent experiences with Neptune resulted some of these objects to be scattered into extremely eccentric orbits .These data provide an reason for why there seems to be no correlation between the size distribution of KBOs and their orbital eccentricity .",
        "rewrite_text": "The Kuiper Belt is believed to be the source of most short-period comets, yet its formation remains inadequately understood. We present findings from N-body simulations that illustrate how collisions among planetesimals in the vicinity of Neptune can create objects with orbits similar to those we observe today. The initial conditions for our simulations are informed by models of planetary migration, during which Neptune is thought to have migrated outward by approximately 30 AU before settling into its current position. Our results indicate that the Kuiper Belt formed as a result of collisions among bodies comparable in size to Pluto (approximately 1000 kilometers in radius), leading to a population of small bodies with orbital eccentricities of up to 0.3. Subsequent interactions with Neptune caused some of these objects to be scattered into highly eccentric orbits. These findings offer an explanation for the apparent lack of correlation between the size distribution of Kuiper Belt Objects (KBOs) and their orbital eccentricities.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": -0.762000762001143
    },
    {
        "original_text": "We consider the problem of designing distributed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time. We propose an algorithm that is robust to link failures, but requires only local information exchange between neighboring nodes. The proposed algorithm achieves global convergence under mild conditions on network topology. In particular, we show that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost surely (a.s.) to the correct value. Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their prescribed behavior. Finally, numerical simulations demonstrate the effectiveness of our approach. Keywords: Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis. 1 Introduction Distributed consensus problems arise naturally in many applications such as flocking  1  , formation control  2  , multi-agent coordination  3  , wireless sensor networks  4  , etc.. A typical example is the average-consensus problem: given a set of n agents connected by communication links, each agent holds some initial data xi(0) ∈ Rm, i = 1, ..., n; it aims to compute the average x̄=1/n∑in=1xi(0). This problem was first studied by Tsitsiklis et al.  5  . They showed that if all agents have access to the same fixed directed graph G, then the average-consensus problem can be solved using a simple linear iterative scheme. However, this assumption does not always hold true since the underlying communication graphs are often random due to unreliable links  6  .\nIn recent years, several researchers have investigated the design of distributed consensus algorithms in dynamic networks  7-10 . For instance, Olfati-Saber  7  considered the case where the communication links among agents change randomly according to independent Bernoulli processes. Under certain assumptions on the connectivity of the network, she proved that her algorithm converges almost surely (i.e., with probability one) to the desired average. Subsequently, Jadbabaie et al.  8  extended these results to undirected networks. More recently,",
        "watermark_text": "We consider the issue of constructing dispersed consensus algorithms in sensor networks where links are randomly generated and can be lost or added over time . We suggest an algorithm that is robust to link errors , but requires only local information transfer between neighboring connections .The proposed algorithm achieves global convergence under mild assumptions on system topology . In particular , we prove that if each node has at least one neighbor whose degree is greater than its own then our algorithm converges almost always ( a . s . ) to the appropriate value .Our results also hold when there exists a small number of Byzantine nodes which may deviate arbitrarily from their appropriate behavior . Finally , numerical simulations prove the ability of our approach .Keywords : Sensor Networks ; Distributed Consensus ; Local Information Exchange ; Robustness Analysis . 1 Introduction Distributed consensus difficulties arise naturally in different applications such as flocking 1 , structure control 2 , multi - agent coordination 3 , wireless sensor networks 4 , etc . . A typical example is the average - consensus problem : considering a setting of n agents connected by communication connections , each agent possesses some initial data xi ( 0 ) ∈ Rm , i = 1 , . . . , n ; it aims to compute the average [UNK] = 1 / [UNK] = 1xi ( 0 ) .This problem was first investigated by Tsitsiklis et al . 5 .They showed that if all agents have access to the same fixed directed graph G , then the average - consensus problem can be answered using a simple linear iterative plan . However , this claim does not always hold false since the underlying communication graphs are often random due to unreliable links 6 .In recent years , various papers have researched the development of distributed agreement algorithms in dynamic networks 7 - 10 . For instance , Olfati - Saber 7 considered the case where the information links among participants change automatically according to individual Bernoulli processes .Under certain assumptions on the connectivity of the network , she proved that her algorithm converges almost always ( i . e . , with probability one ) to the desired average . Subsequently , Jadbabaie et al .8 extended these results to undirected networks . More recently ,",
        "rewrite_text": "We address the challenge of developing dispersed consensus algorithms in sensor networks where connections are formed randomly and can be intermittently added or lost. Our proposed algorithm is resilient to link failures and operates through local information exchange solely between neighboring nodes. Under gentle assumptions regarding the system's topology, the algorithm guarantees global convergence. Specifically, we demonstrate that if each node has at least one neighbor with a higher degree, the algorithm converges almost surely (a.s.) to the correct value. Our findings are also applicable in scenarios where a limited number of Byzantine nodes may behave unpredictably. Moreover, numerical simulations validate the effectiveness of our approach.\n\n**Keywords**: Sensor Networks; Distributed Consensus; Local Information Exchange; Robustness Analysis.\n\n**1 Introduction**\nThe challenges of distributed consensus are prevalent in various applications, including flocking, structural control, multi-agent coordination, and wireless sensor networks. A classic example is the average-consensus problem: given n agents interconnected through communication links, each with initial data \\( x_i(0) \\in \\mathbb{R}^m \\) for \\( i = 1, \\ldots, n \\), the goal is to compute the average \\( \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i(0) \\). This issue was first explored by Tsitsiklis et al., who revealed that if all agents are linked by a fixed directed graph \\( G \\), the average-consensus problem can be solved through a straightforward linear iterative approach. However, this assertion may not always hold true due to the inherent randomness of communication graphs caused by unreliable links. In recent years, various studies have focused on the development of distributed consensus algorithms within dynamic networks. For example, Olfati-Saber examined scenarios where communication links among participants evolve according to individual Bernoulli processes. She proved that, under specific assumptions of network connectivity, her algorithm converges almost surely (with probability one) to the desired average. Subsequent work by Jadbabaie et al. expanded these findings to undirected networks. More recently,",
        "ori-fast-z-score": 1.4791479939068937,
        "water-fast-z-score": 7.597626364883187,
        "rewrite-fast-z-score": 0.7092993656151906
    },
    {
        "original_text": "We present deep near-infrared imaging and spectroscopy for two fields in the Hubble Deep Field South (HDF-S) covering an area of 0.5 square degrees each, centered on RA = 03:32:47.6 Dec = -27:48:30 (J2000). The data were obtained with ISAAC at VLT as part of our ESO Large Programme  First Stars First Galaxies . We have detected more than 1000 galaxies down to KsAB = 24 mag over this field using SExtractor. In addition we have spectroscopically confirmed about 100 objects selected by their photometric redshifts between z=0.7-1.3. This is one of the largest samples of high-z galaxies ever assembled. Our sample includes both Lyman-break galaxies and sub-millimeter sources which are likely to be dusty starbursting systems. These results will allow us to study galaxy formation and evolution up to redshift 1.",
        "watermark_text": "We present dark near - infrared imaging and spectroscopy for two fields in the Hubble Deep Field South ( HDF - S ) covering an area of 0 . 5 square degrees each , centered on RA = 03 : 32 : 47 . 6 Dec = - 27 : 48 : 30 ( J2000 ) . The data were obtained with ISAAC at VLT as part of our ESO Large Programme First Stars First Galaxies .We have discovered more than 1000 galaxies down to KsAB = 24 mag over this field using SExtractor . In addition we have spectroscopically confirmed about 100 galaxies designated by their photometric redshifts between z = 0 . 7 - 1 . 3 .This is one of the largest analyses of high - z galaxies yet assembled . Our specimen includes both Lyman - break galaxies and sub - millimeter sources which are likely to be dusty starbursting systems .These data will able us to study galaxy formation and evolution up to redshift 1 .",
        "rewrite_text": "We report on dark near-infrared imaging and spectroscopy conducted in two regions of the Hubble Deep Field South (HDF-S), each covering an area of 0.5 square degrees and centered at RA = 03:32:47.6, Dec = -27:48:30 (J2000). The observations were acquired using ISAAC at the VLT as part of our ESO Large Programme titled \"First Stars, First Galaxies.\" Our analysis has identified over 1,000 galaxies down to KsAB = 24 mag in these fields, utilizing the SExtractor tool. Furthermore, we have spectroscopically confirmed approximately 100 galaxies, which were selected based on their photometric redshifts in the range of z = 0.7 - 1.3. This constitutes one of the largest collections of high-redshift galaxies compiled to date. Our sample includes both Lyman-break galaxies and sub-millimeter sources, which are likely to represent dusty starbursting systems. These data will enhance our understanding of galaxy formation and evolution up to a redshift of 1.",
        "ori-fast-z-score": 0.9045340337332909,
        "water-fast-z-score": 4.221158824088691,
        "rewrite-fast-z-score": 0.5443310539518174
    },
    {
        "original_text": "We study the phase behavior and structure of binary mixtures composed of soft repulsive spheres with attractive depletants, which are modeled as hard-spheres that interact only via excluded volume interactions. We find that these systems exhibit rich phase diagrams including gas-liquid coexistence at low temperatures for all compositions studied here (0.25 < f < 0.75), where f is the fraction of particles made up by the smaller species. The liquid-gas binodal lines shift to higher pressures upon increasing the size ratio between the two components. For large size ratios we observe an additional fluid-fluid transition line along which both fluids have similar densities but different structures. This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer chains. Our results show good agreement with experimental data on colloid-polymer mixtures over wide ranges of temperature, pressure, and composition. \nI. INTRODUCTIO N\nThe presence of small particles can dramatically affect the properties of larger ones through depletion forces  1  . These effects play important roles in many physical phenomena such as protein crystallization  2  , gelation  3  , and sedimentation  4  .\nDepending on their sizes relative to each other, the mixture may be either miscible or immiscible  5  . In addition, there exist regions of metastability  6  and even multiple phases  7, 8  . A number of theoretical studies  9  -  11  have investigated the effect of depletion attractions on the phase diagram of simple model systems. However, most of them focused on idealized models neglecting hydrodynamic interactions  12  , finite-size effects  13  , polydispersity  14  , and particle shape  15  . Only recently did some authors  16  take into account more realistic features like Brownian motion  17  , electrostatic repulsion  18  , and van der Waals attraction  19  . Despite this progress, it remains difficult to predict the exact location of the critical point  20  due to strong correlations  21  among the particles  22  . Moreover, the influence of depletion forces on the structural  23  and dynamical  24  properties of complex fluids still needs further investigation  25  .\nIn recent years, experiments  26 ",
        "watermark_text": "We research the phase response and shape of binary mixtures consisting of soft repulsive balls with interesting depletants , which are modeled as hard - spheres that interact only via excluded volume interactions . We see that these systems exhibit rich phase diagrams including gas - fluid coexistence at low temperatures for all compositions studied here ( 0 . 25 < f < 0 . 75 ) , where f is the fraction of molecules making up by the smaller species .The gas - gas binodal lines shift to higher pressures upon increasing the height factor between the two parts . For large size ratios we study an additional liquid - fluid transition line along which both gases have equal densities but different structures .This new fluid state has been observed experimentally in colloidal suspensions containing nonadsorbing polymer complexes . Our results show good agreement with experimental evidence on colloid - polymer mixtures over broad ranges of temperature , pressure , and composition .I . INTRODUCTIO N The appearance of tiny particles can dramatically impact the properties of bigger ones through depletion forces 1 . These effects play essential roles in different physical phenomena such as protein crystallization 2 , gelation 3 , and sedimentation 4 .Depending on their sizes comparative to each other , the mixture might be either miscible or immiscible 5 . In addition , there exist zones of metastability 6 and even multiple components 7 , 8 .A variety of theoretical investigations 9 - 11 have explored the impact of depletion attractions on the phase diagram of simple model structures . However , most of them focused on idealized models neglecting hydrodynamic interactions 12 , finite - length effects 13 , polydispersity 14 , and electron shape 15 .Only lately did some writers 16 taking into consideration more realistic characteristics like Brownian movement 17 , electrostatic repulsion 18 , and van der Waals attraction 19 . Despite this progress , it remains impossible to predict the exact location of the critical position 20 due to powerful correlations 21 among the ions 22 .Moreover , the impact of depletion forces on the structural 23 and dynamical 24 properties of complex fluids already requires further investigation 25 . In recent years , observations 26",
        "rewrite_text": "We investigate the phase response and structure of binary mixtures made up of soft repulsive balls combined with intriguing depletants, which are represented as hard spheres that interact solely through excluded volume interactions. Our findings reveal that these systems exhibit complex phase diagrams, featuring gas-fluid coexistence at low temperatures for all compositions analyzed (0.25 < f < 0.75), where f indicates the fraction of the smaller species present. As the height factor between the two components increases, the gas-gas binodal lines shift towards higher pressures. For large size ratios, we identify an additional liquid-fluid transition line where both gases have the same density but differ structurally. This novel fluid state has been experimentally observed in colloidal suspensions containing non-adsorbing polymer complexes. Our results align well with experimental data on colloid-polymer mixtures across a wide range of temperatures, pressures, and compositions. \n\n**I. INTRODUCTION**  \nThe presence of tiny particles can significantly influence the properties of larger particles through depletion forces. These effects are crucial in various physical processes, including protein crystallization, gelation, and sedimentation. Depending on the relative sizes of the components, the mixture can be either miscible or immiscible. Moreover, there are regions of metastability and the potential for multiple components. Several theoretical studies have examined the effects of depletion attractions on the phase diagrams of simple model systems. However, most have concentrated on idealized models that overlook hydrodynamic interactions, finite-length effects, polydispersity, and particle shape. Recently, some researchers have begun to incorporate more realistic factors such as Brownian motion, electrostatic repulsion, and van der Waals attraction. Despite these advancements, accurately predicting the specific location of the critical point remains challenging due to significant correlations among the ions. Furthermore, the influence of depletion forces on the structural and dynamic properties of complex fluids requires further exploration. In recent years, observations have continued to expand our understanding of these phenomena.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 8.009821015753255,
        "rewrite-fast-z-score": 2.7933040956366777
    },
    {
        "original_text": "We report on observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide and its isotopologue, 13CO, as well as the CN radical toward the quasar host galaxy at redshift 2.56 known as the  Cloverleaf  source.  The observed line ratios are consistent with those expected for gas exposed to intense radiation fields characteristic of quasars. We also detect absorption by molecular hydrogen along this sightline through intervening clouds located between us and the quasar host galaxy. These results provide new insights into the physical conditions within the interstellar medium surrounding active galactic nuclei during their early evolutionary stages. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \nThe detection of carbon monoxide (CO), one of the most abundant molecules in space, has been used extensively over the past several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time. However, CO can be difficult to observe directly because it lacks electric dipole moments and thus emits very weakly. In addition, the excitation temperature of the lowest rotational levels of CO is typically low enough such that these transitions fall outside of the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. As a result, much of our understanding about the physical conditions present in dense regions of star-forming galaxies comes from studies of other tracers of molecular gas, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "watermark_text": "We report on observations made with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate emission lines linked with carbon monoxide and its isotopologue , 13CO , as well as the CN radical toward the quasar host galaxy at redshift 2 . 56 referred as the Cloverleaf source . The observed line proportions are compatible with those expected for gas exposed to intense radiation fields typical of quasars .We additionally observe absorption by molecular hydrogen along this sightline through intervening clouds situated between us and the quasar host universe . These data provide fresh insights into the physical conditions within the interstellar medium underlying active galactic nuclei during their early evolutionary stages .This is an free access section under the terms of the Creative Commons Attribution License , which allows use , distribution and reproduction in any medium , provided the original book is properly cited . The observation of carbon monoxide ( CO ) , one of the most numerous compounds in space , has been used widely over the previous several decades to study the properties of cold neutral atomic and molecular dust in galaxies across cosmic time .However , CO can be harder to observe directly because it lacks magnetic dipole moments and therefore emits very weakly . In addition , the excitation temperature of the lowest rotational concentrations of CO is typically minimum enough such that these changes fall outside of the frequency spectrum accessible to surface - based telescopes active at millimeter wavelengths .As a result , much of our knowing about the physical conditions present in dense areas of galaxy - making clusters comes from studies of other tracers of molecular energy , notably HCN , H2S , CS , CH3OH , H2O , and OH + .",
        "rewrite_text": "We present observations from the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide (CO) and its isotopologue, 13CO, along with the CN radical in the quasar host galaxy known as the Cloverleaf source, located at a redshift of 2.56. The detected line ratios align with expectations for gas in environments with intense radiation fields, characteristic of quasars. Furthermore, we observe absorption by molecular hydrogen along the line of sight through intervening clouds between us and the quasar host galaxy. These findings offer new insights into the physical conditions of the interstellar medium surrounding active galactic nuclei during their early stages of evolution. This content is available under the Creative Commons Attribution License, which permits use, distribution, and reproduction in any medium, provided proper citation of the original work. The detection of carbon monoxide (CO), one of the most prevalent molecules in space, has been extensively utilized over the past several decades to investigate the properties of cold neutral atomic and molecular dust in galaxies throughout cosmic history. However, observing CO directly can prove challenging due to its lack of magnetic dipole moments, resulting in very weak emissions. Moreover, the excitation temperature of the lowest rotational states of CO is typically low enough that these emissions often fall outside the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. Consequently, our understanding of the physical conditions in dense regions of galaxy-forming clusters largely relies on studies of other molecular tracers, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "ori-fast-z-score": -1.4485719366802965,
        "water-fast-z-score": 6.9428561869392285,
        "rewrite-fast-z-score": -0.4583492485141057
    },
    {
        "original_text": "We present an analysis of optical and infrared photometric data obtained during the recent (2006-2008) outbursts of the dwarf novae system V2051 Oph, which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime.  We find that the light curve of this object shows many similarities with those observed for other SU UMa-type systems but also some significant differences. In particular we note that there are no clear signs of rebrightening following either the first or second superoutburst; nor do we see any evidence for a double-humped structure in the light curves at all phases of these events. The lack of such features may be due to the fact that our observations were made when the system was relatively faint compared to previous studies. However, it should be noted that the orbital period of V2051 Oph is significantly longer than most other SU UMa stars so that the mass transfer rate will be lower by about a factor of ten.",
        "watermark_text": "We present an assessment of optical and infrared photometric data acquired during the recent ( 2006 - 2008 ) outbursts of the dwarf novae component V2051 Oph , which is one of only three known to have exhibited both superoutbursts and normal outbursts in its lifetime . We see that the light curve of this body demonstrates many similarities with those observed for other SU UMa - class systems but also some significant variations .In particular we note that there are no clear indicators of rebrightening following either the first or second superoutburst ; nor do we find any evidence for a twin - humped structure in the light curves at all phases of these events . The absence of such properties may be due to the fact that our observations were made when the system was quite dim relative to previous analyses .However , it should be mentioned that the orbital period of V2051 Oph is significantly greater than most other SU UMa stars so that the mass transfer time will be reduced by about a factor of ten .",
        "rewrite_text": "We provide an evaluation of the optical and infrared photometric data obtained during the recent outbursts of the dwarf nova V2051 Oph, which is one of only three known systems to have experienced both superoutbursts and normal outbursts throughout its history. The light curve of V2051 Oph shows many similarities to those of other SU UMa-type systems, yet it also displays notable differences. Particularly, we observe a lack of clear signs of rebrightening following the first or second superoutburst, and we find no evidence of a twin-humped structure in the light curves across all phases of these events. This absence of such features may stem from the fact that our observations were conducted when the system was significantly dimmer than in previous studies. Additionally, it is important to note that the orbital period of V2051 Oph is considerably longer than that of most SU UMa stars, which results in a reduced mass transfer time by roughly a factor of ten.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 6.230641662171566,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "The energy spectrum of gamma rays is one of the most important information to understand their origin and propagation in space, since it contains crucial information on the physical processes involved.  In this work we present results obtained with the Tibet ASγ experiment for the determination of the energy spectra of several bright sources detected by the Energetic Gamma Ray Experiment Telescope (EGRET) aboard the Compton Gamma-Ray Observatory satellite. The data were taken between 1997 and 2000 at Yangbajing Cosmic Ray Laboratory (Tibet). We have analyzed about 1 million events collected during these four years. By using Monte Carlo simulations based on CORSIKA code, we estimate that our detector has a detection efficiency above 80% for primary cosmic ray energies greater than 10 TeV. Our analysis shows that the observed fluxes are consistent with power law functions with spectral indices ranging from 2.0 to 3.5. These values agree well with those reported previously by other experiments.",
        "watermark_text": "The energy spectrum of gamma radiation is one of the most important information to comprehend their source and propagation in space , since it contains vital information on the physical processes concerned . In this research we present results derived with the Tibet ASγ experiment for the determination of the power spectra of several bright sources detected by the Energetic Gamma Ray Experiment Telescope ( EGRET ) aboard the Compton Gamma - Ray Observatory satellite .The data were took between 1997 and 2000 at Yangbajing Cosmic Ray Laboratory ( Tibet ) . We have analyzed about 1 million events collected during these four seasons .By using Monte Carlo simulations based on CORSIKA code , we estimate that our camera has a detection efficiency above 80 % for principal cosmic ray energies higher than 10 TeV . Our study shows that the seen fluxes are compatible with power law functions with spectral indices varied from 2 . 0 to 3 . 5 .These quantities agree well with those published previously by other experiments .",
        "rewrite_text": "The energy spectrum of gamma radiation is crucial for understanding its sources and how it propagates through space, as it provides essential insights into the underlying physical processes. In this study, we present findings from the Tibet ASγ experiment, which focused on determining the power spectra of several prominent sources identified by the Energetic Gamma Ray Experiment Telescope (EGRET) on the Compton Gamma-Ray Observatory satellite. The data were collected between 1997 and 2000 at the Yangbajing Cosmic Ray Laboratory in Tibet, encompassing approximately 1 million events over four seasons. By employing Monte Carlo simulations using the CORSIKA code, we estimate that our camera achieves a detection efficiency exceeding 80% for primary cosmic ray energies above 10 TeV. Our results indicate that the observed fluxes are consistent with power law functions, with spectral indices ranging from 2.0 to 3.5, aligning well with previous reports from other experiments.",
        "ori-fast-z-score": 0.8962581595302719,
        "water-fast-z-score": 5.761659596980319,
        "rewrite-fast-z-score": 0.9271726499455306
    },
    {
        "original_text": "We report on our analysis of the outburst mechanism of SGR 1806-20, which is one of three known magnetars (neutron stars with superstrong magnetic fields). We have analyzed all available data obtained by Swift/BAT during its first year in orbit to search for periodicities associated with this source. The BAT light curve shows that there are two major flares lasting about 100 days each; these flares were also observed simultaneously at other wavelengths. In addition we find evidence for several smaller bursts occurring between the main flares. Using an improved version of the method developed by Israel et al. (2008) we searched for pulsations in the BAT data corresponding to the time intervals when the source was active. No significant signal could be found above background noise levels down to a limit of 1 mCrab (3σ), but we did detect a weak signal below 0.5 mCrab. This signal has been confirmed using independent methods applied to different energy bands.",
        "watermark_text": "We report on our analysis of the outburst mechanism of SGR 1806 - 20 , which is one of three known magnetars ( neutron galaxies with superstrong magnetic fields ) . We have analyzed all available data acquired by Swift / BAT during its initial season in orbit to search for periodicities associated with this source .The BAT signal curve shows that there are two major flares lasting about 100 days each ; these flares were also observed concurrently at other wavelengths . In addition we find proof for numerous smaller bursts happening between the main flares .Using an better version of the method developed by Israel et al . ( 2008 ) we searched for pulsations in the BAT signal related to the period intervals when the source was active .No meaningful wave possible be found above background noise values down to a limit of 1 mCrab ( 3σ ) , but we did detect a weak noise below 0 . 5 mCrab . This signal has been confirmed using separate methods applied to different energy bands .",
        "rewrite_text": "We present our analysis of the outburst mechanism of SGR 1806-20, one of the three known magnetars (neutron stars with extremely strong magnetic fields). Our study involved a thorough examination of all data collected by Swift/BAT during its initial orbit to identify any periodic behaviors associated with this source. The BAT light curve indicates the presence of two significant flares, each lasting approximately 100 days, which were also detected in other wavelengths simultaneously. Additionally, we found evidence of several smaller bursts occurring between the major flares. Utilizing an enhanced version of the technique developed by Israel et al. (2008), we searched for pulsations in the BAT signal during the active periods of the source. While we could not identify any significant waves above background noise levels down to a threshold of 1 mCrab (3σ), we did observe a faint signal below 0.5 mCrab. This weak signal has been corroborated using different methods across various energy bands.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": -2.492241482207092
    },
    {
        "original_text": "Spin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.",
        "watermark_text": "Spin - based computing is an developing field that has garnered considerable scrutiny in recent years , owing to its potential for applications such as high - density data storage and large - speed information processing . In this review article we explain the fundamental concepts governing bipolar spintronic systems based on semiconductor heterostructures with ferromagnetic contacts .We first introduce the physics behind spin injection into semiconductors using tunnel fences or Schottky diodes . Then we explain how these injected spinning can be manipulated by means of external magnetic waves and / or electric currents .Finally , we present some examples of spintronic systems including spin - LEDs , spin transistors , and spin - logic devices . The main attention will be place on GaAs - based assemblies but also other materials systems are discussed briefly .This page is intended to provide a comprehensive overview of the state - of - the - art studies in the field of bipolar spintronics . It should serve both as a reference for newcomers interested in knowing about the fundamentals of spin transport phenomena at connections between metals and semiconductors , and as a reference source for researchers working in related fields .",
        "rewrite_text": "Spin-based computing is an emerging area that has attracted significant attention in recent years due to its promise for applications such as high-density data storage and high-speed information processing. In this review article, we outline the key concepts underlying bipolar spintronic systems that utilize semiconductor heterostructures with ferromagnetic contacts. We begin by discussing the principles of spin injection into semiconductors through tunnel barriers or Schottky diodes. Next, we describe how these injected spins can be controlled using external magnetic fields and/or electric currents. Lastly, we showcase examples of spintronic devices, including spin-LEDs, spin transistors, and spin-logic devices. While the primary focus will be on GaAs-based structures, we will also briefly explore other material systems. This article aims to provide a thorough overview of the latest advancements in bipolar spintronics, serving as a valuable reference for those new to the fundamentals of spin transport phenomena at the interfaces of metals and semiconductors, as well as for researchers in related domains.",
        "ori-fast-z-score": 0.4216370213557839,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 0.22086305214969307
    },
    {
        "original_text": "We study the conservation laws for linear parabolic equations with constant coefficients in one space dimension, which are given by Noether s theorem as integrals over time of certain densities that depend on solutions to the equation. We show how these densities can be computed using an algorithm based on symbolic integration techniques. The resulting expressions have been implemented into a computer program called CONSINTEP (Conservation Laws INTerpreter) written in Maple. This software is available at http://math.univ-lyon1.fr/~boudjema/consintep/index.html . Keywords: Conservation law, symmetry group, potential symmetry, Noether s theorem, linear partial differential equations, Maple. 1 Introduction In this article we present some results concerning conservation laws and potential symmetries of linear parabolic equations. These results were obtained during my PhD thesis  1  , where I developed algorithms for computing conserved quantities associated with such equations. Here we give a brief overview of our main results. \nThe concept of conservation law plays an important role in physics since it allows us to describe physical phenomena in terms of energy or entropy balance. For example, if u(x, t) denotes the temperature distribution inside a rod at position x ∈  0, 1  and time t ≥ 0 then the total amount of heat contained within the rod satisfies the following equation: \nwhere c > 0 is a positive constant describing the thermal conductivity of the material. If we assume that there exists no source term f = 0, i.e., all the heat entering the system leaves again after some time interval, then integrating Eq. (1) \nover the spatial domain yields the first integral of motion Q(t), also known as the energy density,\nwhich describes the total amount of heat stored up in the rod at any point in time. Note that the second equality follows directly from Green s formula applied to the left-hand side of Eq. (2).",
        "watermark_text": "We research the conservation laws for linear parabolic equations with constant coefficients in one space dimension , which are given by Noether s theorem as integrals over time of certain densities that rely on solutions to the equation . We see how these densities can be computed using an algorithm based on symbolic analysis methods .The resulting statements have been implemented into a computer program named CONSINTEP ( Conservation Laws INTerpreter ) published in Maple . This program is accessible at http : / / math . univ - lyon1 . fr / ~ boudjema / consintep / index . html .Keywords : Conservation law , symmetry class , potential symmetry , Noether s theorem , linear partial differential coefficients , Maple . 1 Introduction In this article we present some developments relating conservation laws and potential symmetries of linear parabolic equations .These conclusions were obtained during my PhD doctoral 1 , where I derived methods for modeling conserved quantities associated with such equations . Here we give a brief overview of our major results .The concept of protection law serves an important role in science since it allows us to explain physical phenomena in terms of power or entropy balance . For instance , if u ( x , t ) denotes the temperature distribution inside a rod at position x ∈ 0 , 1 and time t ≥ 0 then the total quantity of temperature contained within the rod satisfies the following equation : where k > 0 is a positive variable describing the thermal conductivity of the metal .If we suppose that there exists no source term h = 0 , i . e . , all the heat exiting the system leaves again after some time interval , then integrating Eq . ( 1 ) over the spatial domain yields the first integral of movement Q ( t ) , sometimes called as the electricity density , which expresses the total quantity of warmth used up in the rod at any point in time .Note that the second equality follows directly from Green s method applied to the leave - hand half of Eq . ( 2 ) .",
        "rewrite_text": "We investigate the conservation laws associated with linear parabolic equations featuring constant coefficients in one spatial dimension, as described by Noether's theorem. This theorem indicates that conservation laws can be represented as integrals over time of specific densities, which depend on the solutions of the equations. We demonstrate a method for calculating these densities through an algorithm grounded in symbolic analysis techniques. The findings of this research have been incorporated into a software program called CONSINTEP (Conservation Laws INTerpreter), which is available in Maple. You can access the program at http://math.univ-lyon1.fr/~boudjema/consintep/index.html. \n\nKeywords: Conservation law, symmetry class, potential symmetry, Noether's theorem, linear partial differential equations, Maple.\n\n1. Introduction\n\nIn this paper, we present developments concerning conservation laws and potential symmetries of linear parabolic equations. These insights were gained during my doctoral studies, where I devised methodologies for modeling conserved quantities connected with such equations. Here, we provide a succinct overview of our key findings. The principle of conservation plays a crucial role in science as it enables the explanation of physical phenomena through the balance of energy or entropy. For example, let \\( u(x, t) \\) represent the temperature distribution in a rod at position \\( x \\in [0, 1] \\) and time \\( t \\geq 0 \\). The total thermal energy contained within the rod can be expressed by a specific equation, where \\( k > 0 \\) signifies the thermal conductivity of the material. Assuming the absence of a source term (\\( h = 0 \\)), meaning all heat that exits the system is expelled after a certain period, we can integrate the equation over the spatial domain to find the first integral of motion \\( Q(t) \\), often referred to as electricity density, representing the total amount of thermal energy utilized in the rod at any given time. It is important to note that the second equality is derived directly from Green's method applied to the left-hand side of the equation.",
        "ori-fast-z-score": -1.0650014966747527,
        "water-fast-z-score": 6.205374015050718,
        "rewrite-fast-z-score": -0.318222913670292
    },
    {
        "original_text": "We present optical variability measurements for infrared power law-selected galaxies and X-ray sources in the Chandra Deep Field South (CDFS). We use data obtained with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific star-formation rates for these objects over an eight-year baseline. The sample consists of 16,000 galaxies at 0 < z < 5 selected by their mid-infrared colors using Spitzer/IRAC observations as well as 1,500 X-ray point sources detected in deep Chandra observations. We find that both galaxy samples show significant levels of intrinsic variation on timescales ranging from days to years. For example, we detect more than 50% of our IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns showing >0.1 mag variations between epochs separated by one year or less. These results are consistent with previous studies which have found similar levels of variability among optically-selected quasars. However, we also find evidence suggesting that this level of variability is not driven solely by AGN activity but may be associated with other physical processes such as mergers and/or interactions within the host galaxy itself.",
        "watermark_text": "We present optical variability observations for laser power law - selected galaxies and X - ray sources in the Chandra Deep Field South ( CDFS ) . We use data acquired with the Hubble Space Telescope s Advanced Camera for Surveys to measure photometric redshifts , rest - frame absolute magnitudes , stellar masses , sun formation rates , and particular galaxy - formation rates for these objects over an eight - month baseline .The sample consists of 16 , 000 galaxies at 0 < z < 5 selected by their mid - infrared colors using Spitzer / IRAC measurements as also as 1 , 500 X - ray point sources detected in deep Chandra measurements . We see that both star samples show considerable rates of intrinsic variation on timescales ranging from hours to decades .For instance , we find more than 50 % of our IRAC - selected galaxies at 3 . 6 microns and 80 % at 4 . 5 microns showing > 0 . 1 mag variations between epochs separated by one decade or less . These conclusions are compatible with previous research which have discovered similar rates of variability among optically - selected quasars .However , we also find proof suggesting that this level of variability is not driven solely by AGN activity but might be involved with other structural processes such as mergers and / or relationships within the host universe itself .",
        "rewrite_text": "We present observations of optical variability in galaxies and X-ray sources selected based on laser power law in the Chandra Deep Field South (CDFS). Utilizing data from the Hubble Space Telescope's Advanced Camera for Surveys, we measure photometric redshifts, rest-frame absolute magnitudes, stellar masses, star formation rates, and specific galaxy formation rates for these objects over an eight-month period. The sample includes 16,000 galaxies with redshifts ranging from 0 to 5, selected based on their mid-infrared colors using Spitzer/IRAC data, along with 1,500 X-ray point sources identified in deep Chandra observations. We observe significant intrinsic variation in both samples, with timescales spanning from hours to decades. For example, over 50% of our IRAC-selected galaxies at 3.6 microns and 80% at 4.5 microns exhibit variations greater than 0.1 magnitudes over intervals of one decade or less. These findings align with previous studies that have reported similar variability rates in optically-selected quasars. However, our results also indicate that this level of variability may not be exclusively attributed to AGN activity; it could also be influenced by other structural processes, such as galaxy mergers or interactions within the host environment.",
        "ori-fast-z-score": -1.2939932784412609,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": 0.7592566023652966
    },
    {
        "original_text": "We present an approach to the modeling and simulation of combining concepts, which is based on quantum field theory (QFT) in Fock space. The basic idea behind our model is that each concept can be represented by its own QFT with specific properties. In order to combine two or more concepts we have to couple these fields together. We show how this coupling leads to new states representing combinations of concepts. Furthermore, we discuss some possible applications for our model such as combinatorial optimization problems and computational linguistics. Finally, we give examples illustrating the usefulness of our approach. Keywords: Quantum field theory, Fock space, Combining concepts, Computational linguistics, Optimization problem. 1 Introduction Quantum mechanics has been successfully applied to many different areas ranging from physics over chemistry to biology  1  . However, there are still open questions about the foundations of quantum mechanics  2  , especially concerning the interpretation of measurement results  3  .\nIn recent years, researchers started applying quantum mechanical models also to other disciplines like computer science  4  , cognitive psychology  5  , neuroscience  6  , economics  7  , etc.. For example, it was shown that quantum mechanical models could help solving certain NP-hard problems  8  . Moreover, quantum mechanical models were used to explain human decision making  9  and learning processes  10  .",
        "watermark_text": "We present an view to the description and modeling of combining ideas , which is based on quantum field theory ( QFT ) in Fock space . The basic idea behind our model is that each idea can be described by its own QFT with particular features .In order to mix two or more ideas we have to mix these fields together . We see how this bonding gives to different states representing combinations of concepts .Furthermore , we explain some possible users for our model such as combinatorial algorithm issues and mathematical linguistics . Finally , we give examples illustrating the usefulness of our approach .Keywords : Quantum field model , Fock space , Combining principles , Computational linguistics , Optimization problem . 1 Introduction Quantum theory has been successfully application to many various fields ranging from science over chemistry to biology 1 .However , there are still open questions about the foundations of quantum mechanics 2 , particularly concerning the interpretation of measurement data 3 . In past decades , researchers began application quantum mechanical models also to other disciplines like digital science 4 , cognitive psychology 5 , neuroscience 6 , economics 7 , etc . . For instance , it was shown that quantum mechanical models could assist solving specific NP - hard problems 8 .Moreover , quantum mechanical models were used to explain brain judgment makes 9 and educational processes 10 .",
        "rewrite_text": "We propose a framework for the description and modeling of combined ideas, drawing upon quantum field theory (QFT) in Fock space. The fundamental concept of our model is that each idea can be represented by its own QFT, characterized by unique features. To blend two or more ideas, we must integrate these fields. This integration results in diverse states that represent various combinations of concepts. Additionally, we outline potential applications for our model, including issues related to combinatorial algorithms and mathematical linguistics. Finally, we provide examples that demonstrate the effectiveness of our approach.\n\n**Keywords:** Quantum field model, Fock space, Combining principles, Computational linguistics, Optimization problems.\n\n**1 Introduction**  \nQuantum theory has been successfully applied across numerous fields, including the sciences, chemistry, and biology. However, questions regarding the foundations of quantum mechanics remain, particularly concerning the interpretation of measurement data. In recent decades, researchers have begun to apply quantum mechanical models to other disciplines, such as digital science, cognitive psychology, neuroscience, and economics. For instance, studies have shown that quantum mechanical models can help solve specific NP-hard problems. Moreover, these models have been utilized to elucidate decision-making processes in the brain and to enhance educational methods.",
        "ori-fast-z-score": -0.29002094671369905,
        "water-fast-z-score": 7.627700713964739,
        "rewrite-fast-z-score": 3.9270877694067203
    },
    {
        "original_text": "The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for several model systems with different chain lengths, degrees of branching, and types of interactions between monomers. It has been found that the intramolecular correlation function can be represented as a sum of two terms corresponding to short- and long-range contributions. The latter term describes the contribution of segments separated by distances larger than the average distance between neighboring chains. This term depends on the number density of chains and their length. In addition, it was shown that this term decreases exponentially at large separations. The dependence of the first four moments of the segmental size distribution on temperature and pressure are calculated using the results obtained previously. These dependences were compared with experimental data available in literature. Good agreement between theory and experiment is observed. \n \n Keywords: Polymer melt, Intramolecular structure, Correlation functions, Segmental size distribution",
        "watermark_text": "The intramolecular structure of polymers is studied by means of the molecular dynamics simulation method for numerous model models with various chain lengths , degrees of branching , and types of interactions between monomers . It has been shown that the intramolecular relationship value can be described as a sum of two terms corresponding to short - and long - range contributions .The latter term describes the contribution of segments apart by distances bigger than the average distance between neighboring lines . This term relies on the number density of chains and their length .In addition , it was shown that this term drops exponentially at large separations . The dependence of the first four moments of the segmental size distribution on temperature and pressure are measured using the results derived earlier .These dependences were compared with observation information available in literature . Good agreement between theoretical and experiment is observed .Keywords : Polymer melt , Intramolecular structure , Correlation properties , Segmental size distribution",
        "rewrite_text": "The intramolecular structure of polymers is investigated using molecular dynamics simulations across a variety of models characterized by differing chain lengths, branching degrees, and monomer interaction types. It has been demonstrated that the value of intramolecular relationships can be expressed as the sum of two components, representing short-range and long-range contributions. The long-range component accounts for interactions between segments at distances greater than the average separation between adjacent chains, and it is influenced by the density and length of the chains. Notably, this contribution decreases exponentially with increasing separation. Additionally, the first four moments of the segmental size distribution in relation to temperature and pressure have been derived and measured. These findings have been compared with empirical data from the literature, revealing a strong agreement between theoretical predictions and experimental observations. \n\nKeywords: Polymer melt, Intramolecular structure, Correlation properties, Segmental size distribution.",
        "ori-fast-z-score": 0.7977240352174656,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 1.0533703247651751
    },
    {
        "original_text": "The flicker noise is the low-frequency fluctuations observed in electrical resistance and other transport properties of metals at temperatures below 1 K.  The theory developed by Altshuler, Aronov, and Khmelnitsky (AAK) explains this phenomenon as arising due to electron-electron interactions within the metal film.  In their original work they assumed that electrons are scattered elastically off impurities or phonons.  However, recent experiments have shown that there can be significant inelastic scattering between electrons which leads to additional contributions to the resistivity.  Here we present an extension of AAK s theory for the case where both elastic and inelastic scattering processes contribute to the resistivity.  We show how our results compare with existing experimental data on thin gold films grown epitaxially on silicon substrates. The flicker noise is the low- frequency fluctuations observed in electrical resistance  and other transport properties of metallic systems at temperatures below 1K . It was first discovered in 1963 when measuring the resistance of thin silver wires  1  , but it has since been found in many different types of materials including semiconductors  2  , superconductors  3  , carbon nanotubes  4  , graphene  5  , and topological insulators  6  .\nIn order to explain these observations, Altshuler et al. (AAK) proposed a theoretical model based on the assumption that electrons scatter elastically off impurities  7, 8  .  This approach successfully describes most of the available experimental data  9  , however some discrepancies were recently reported  10  . These deviations may arise because the elastic approximation does not take into account possible inelastic scattering events  11  .",
        "watermark_text": "The flicker sound is the small - frequency fluctuations found in electrical resistance and other transport properties of metals at conditions below 1 K . The theory advanced by Altshuler , Aronov , and Khmelnitsky ( AAK ) presents this phenomenon as occurring due to ion - atom relationships within the metal film . In their early research they thought that electrons are scattered elastically off impurities or phonons .However , recent experiments have shown that there can be considerable inelastic scattering between electrons which results to significant contributions to the resistivity . Here we present an extension of AAK s concept for the case where both elastic and inelastic scattering mechanisms lead to the resistivity .We see how our findings compare with existing experimental evidence on thin gold films grown epitaxially on silicon substrates . The flicker sound is the small - frequency fluctuations detected in electrical resistance and other transport properties of metallic circuits at conditions below 1K .It was first discovered in 1963 when monitoring the tolerance of short metal wires 1 , but it has since been seen in multiple diverse kinds of substances including semiconductors 2 , superconductors 3 , silicon nanotubes 4 , graphene 5 , and topological insulators 6 . In try to explain these observations , Altshuler et al .( AAK ) proposed a conceptual theory based on the assumption that electrons scatter elastically off impurities 7 , 8 . This method effectively describes most of the provided experimental evidence 9 , however some discrepancies were recently noted 10 .These deviations might arise because the elastic equation does not take into consideration proposed inelastic scattering events 11 .",
        "rewrite_text": "The flicker noise refers to the low-frequency fluctuations observed in the electrical resistance and other transport properties of metals at temperatures below 1 K. The theory proposed by Altshuler, Aronov, and Khmelnitsky (AAK) attributes this phenomenon to interactions between ions and atoms within the metal film. Initially, their research suggested that electrons scatter elastically off impurities or phonons. However, recent experiments indicate that significant inelastic scattering between electrons also contributes to resistivity. In this work, we extend the AAK framework to include both elastic and inelastic scattering mechanisms affecting resistivity. We will compare our findings with existing experimental data on thin gold films that are epitaxially grown on silicon substrates. Flicker noise was first identified in 1963 while assessing the reliability of short metal wires, and has since been observed in a variety of materials, including semiconductors, superconductors, silicon nanotubes, graphene, and topological insulators. To explain these observations, Altshuler et al. (AAK) developed a conceptual framework based on the idea that electrons scatter elastically off impurities. While this approach accurately describes much of the experimental data available, some recent discrepancies have been noted. These inconsistencies may be due to the elastic scattering model's failure to account for inelastic scattering events.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 8.693182879212225,
        "rewrite-fast-z-score": 1.876629726513673
    },
    {
        "original_text": "The stretching of homopolymer chains is studied by molecular dynamics simulations in the presence of an external force applied to one end of each chain.  The results are compared with those obtained for polyelectrolyte chains, which have been shown previously to exhibit similar behavior under tension.   It is found that both types of polymer chains show qualitatively similar behavior when stretched at low temperatures and high forces; however, there are significant differences between them at higher temperatures or lower forces.   ... This article was originally published on ArXiv.org s preprint server. Read more about this research at: http://arxiv.org/abs/cond-mat/0405033 . \nAuthors:  Yi-Chun Chen , Shih-Chieh Wu , Chia-Hui Lee , Hsinchu, Taiwan .\nAffiliation: Institute of Physics ,  Academia Sinica,  Taipei, Taiwan .\nEmail: yichun@phys.sinica.edu.tw",
        "watermark_text": "The extension of homopolymer bars is studied by molecular dynamics simulations in the presence of an external stress introduced to one end of each chain . The results are compared with those achieved for polyelectrolyte rings , which have been shown previously to undergo similar behavior under pressure .It is found that both types of polymer chains show qualitatively identical performance when stretched at low temperatures and low forces ; however , there are significant variations between them at higher temperatures or lower forces . . . .This section was originally published on ArXiv . org s preprint server . Read more about this research at : www : / / arxiv . org / abs / cond - mat / 0405033 .Authors:  Yi-Chun Chen , Shih-Chieh Wu , Chia-Hui Lee , Hsinchu, Taiwan .Affiliation: Institute of Physics ,  Academia Sinica,  Taipei, Taiwan .Email: yichun@phys.sinica.edu.tw",
        "rewrite_text": "The behavior of homopolymer bars under extension is examined using molecular dynamics simulations, with an external stress applied to one end of each chain. The findings are compared to those obtained from polyelectrolyte rings, which have previously demonstrated similar reactions to applied pressure. The study reveals that both polymer types exhibit qualitatively similar responses when stretched at low temperatures and forces; however, significant differences arise at higher temperatures or lower forces. This section was initially published on the ArXiv preprint server. For more information on this research, visit: www.arxiv.org/abs/cond-mat/0405033. Authors: Yi-Chun Chen, Shih-Chieh Wu, Chia-Hui Lee, Hsinchu, Taiwan. Affiliation: Institute of Physics, Academia Sinica, Taipei, Taiwan. Email: yichun@phys.sinica.edu.tw.",
        "ori-fast-z-score": 0.282842712474619,
        "water-fast-z-score": 4.142857142857143,
        "rewrite-fast-z-score": 0.42008402520840293
    },
    {
        "original_text": "We present an algorithm for the numerical evaluation of Feynman diagrams with arbitrary numbers of external particles and internal loops, which is based on the concept of  partonic subdiagrams . The method allows to perform calculations in QCD beyond leading order accuracy without any approximations or assumptions about the kinematics of the process under consideration. We demonstrate its applicability by calculating the next-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this talk we will discuss how one can gain analytic control over parton showers using the concept of  partons  as fundamental degrees of freedom. This approach has been developed recently within the framework of Soft-Collinear Effective Theory (SCET)  1  . It provides a systematic way to resum large logarithms associated with collinear splittings into multiple jets  2  , thereby improving our understanding of jet physics  3  .\nThe basic idea behind SCET is that physical observables are described by matrix elements involving soft and/or collinear fields only  4  . These fields have nontrivial transformation properties under boosts along the beam axis  5  . They allow us to separate hard interactions from soft radiation  6  . As a result, it becomes possible to systematically factorize contributions to scattering amplitudes into  hard functions  describing short-distance dynamics  7, 8  and  semi-hard functions  encoding information about the emission of soft gluons  9  .",
        "watermark_text": "We present an algorithm for the numerical identification of Feynman diagrams with arbitrary numbers of external particles and internal loops , which is based on the idea of partonic subdiagrams . The method enables to conduct measurements in QCD beyond trailing order accuracy without any approximations or assumptions about the kinematics of the process under consideration .We test its applicability by calculating the second - to - leading - order corrections to the production cross section of large quarks at hadron colliders . In this talk we will explore how one can obtain analytic control over parton showers using the idea of partons as essential degrees of liberty .This method has been constructed recently within the framework of Soft - Collinear Effective Theory ( SCET ) 1 . It provides a comprehensive way to resum big logarithms associated with collinear splittings into multiple jets 2 , thereby improving our appreciation of flight mechanics 3 .The basic idea behind SCET is that physical observables are explained by matrix elements featuring soft and / or collinear fields only 4 . These fields have nontrivial transformation qualities under boosts along the laser axis 5 .They allow us to separate hard interactions from soft light 6 . As a result , it becomes possible to deliberately factorize contributions to scattering amplitudes into hard functions describing short - distance dynamics 7 , 8 and semi - hard functions encoding information about the emission of deep gluons 9 .",
        "rewrite_text": "We introduce an algorithm designed for the numerical identification of Feynman diagrams featuring any number of external particles and internal loops, leveraging the concept of partonic subdiagrams. This approach allows for measurements in Quantum Chromodynamics (QCD) that exceed leading order accuracy without relying on any approximations or assumptions regarding the kinematics of the specific process. We validate the effectiveness of this method by calculating the second-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this presentation, we will examine how to achieve analytic control over parton showers, emphasizing partons as fundamental degrees of freedom. This technique has recently been developed within the framework of Soft-Collinear Effective Theory (SCET), which offers a structured way to resum large logarithms associated with collinear splittings into multiple jets, thus enhancing our understanding of flight mechanics. The fundamental premise of SCET is that physical observables can be described by matrix elements involving only soft and/or collinear fields. These fields exhibit complex transformation properties under boosts along the beam axis, enabling us to distinguish hard interactions from soft emissions. Consequently, it becomes feasible to systematically factor contributions to scattering amplitudes into hard functions that represent short-distance dynamics, and semi-hard functions that capture information related to the emission of soft gluons.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 6.874054958970533,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "The Internet is growing at an unprecedented rate, and with it comes increasing demands on network applications to provide reliable services in spite of failures that can occur anywhere along their execution paths. In this work we present a language-based approach for improving robustness by automatically detecting errors in protocol implementations using static analysis techniques. We show how our technique can be used to detect common types of implementation errors such as buffer overflow vulnerabilities or incorrect handling of exceptional conditions. Our results demonstrate that our method achieves high precision (>90%) while maintaining reasonable recall (~60%). Finally, we evaluate the performance overheads associated with our approach and find them to be negligible compared to existing approaches based on dynamic testing. The Internet continues to grow at an unprecedented rate, leading to increased demand for reliable services despite failures occurring anywhere along application execution paths. This work presents a languagebased approach for improving robustness through automatic detection of errors in protocol implementations via static analysis techniques. We describe how our technique can be applied to detect common types of error including buffer overflows and improper treatment of exceptional cases. Our experimental evaluation shows that our approach has very high precision (> 90%), while still achieving reasonable recall (~ 60%). Finally, we measure the performance overheads of our approach and find them negligible when compared against other state-of-the-art approaches relying on dynamic testing.",
        "watermark_text": "The Internet is growing at an remarkable speed , and with it comes greater demands on internet solutions to provide quality services in spite of failures that can occur anywhere along their execution paths . In this research we present a language - based model for improving robustness by automatically detecting failure in protocol implementations using static analysis methods .We indicate how our technique can be used to identify typical types of implementation errors such as buffer overflow vulnerabilities or incomplete processing of exceptional conditions . Our results show that our technique achieves high clarity ( > 90 % ) while maintaining reasonable recall ( ~ 60 % ) .Finally , we assess the performance overheads associated with our approach and find them to be negligible compared to existing techniques based on dynamic monitoring . The Internet continues to expand at an remarkable speed , leading to greater need for good services despite failures occurring anything along application implementation paths .This research provides a languagebased approach for improving robustness through automatic diagnosis of errors in protocol implementations via static analysis methods . We define how our technique can be applied to identify specific kinds of mistake including buffer overflows and improper management of exceptional cases .Our research assessment demonstrates that our approach has very high clarity ( > 90 % ) , while nevertheless improving stable remember ( ~ 60 % ) . Finally , we measure the performance overheads of our approach and find them negligible when compared against other state - of - the - art methods using on dynamic analysis .",
        "rewrite_text": "The Internet is expanding at an extraordinary rate, which increases the demand for reliable internet solutions capable of providing quality services despite potential failures along their execution paths. In this study, we present a language-based model designed to enhance robustness by automatically detecting errors in protocol implementations using static analysis techniques. We illustrate how our method can be employed to identify common implementation issues, such as buffer overflow vulnerabilities and inadequate handling of exceptional conditions. Our findings indicate that our technique achieves a high clarity rate of over 90% while maintaining a reasonable recall of around 60%. Additionally, we evaluate the performance overhead associated with our approach and find it to be negligible compared to existing techniques that rely on dynamic monitoring.",
        "ori-fast-z-score": 1.6296434287653334,
        "water-fast-z-score": 9.51908100741907,
        "rewrite-fast-z-score": 1.3858697343671664
    },
    {
        "original_text": "Epsilon Aurigae is an F-type main sequence star with a mass of 1.8 M☉ and radius 2 R☉, located at about 40 light-years away in the constellation Auriga.  It has been known for many years to be surrounded by dusty material that obscures its visible spectrum.   The infrared excess emission detected around this object suggests it may have a circumstellar disk similar to those found around young stars such as T Tauri or Herbig Ae/Be stars.   In addition, there are indications that the system contains a close companion which could also contribute to the observed infrared excess emission.    We present new photometric observations obtained using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea over the period 1997-2001 covering wavelengths between 0.9-2.5 microns.  These data show significant variations in both the near-infrared fluxes and colours of the central source consistent with changes in the amount of dust surrounding the star.  This behaviour is very similar to what is seen in other pre-main-sequence systems where accretion onto the central star causes periodic increases in luminosity accompanied by increased levels of reddening due to heating of the surrounding dust grains.   Our results suggest that the current level of activity in the system is relatively low compared to previous epochs but we cannot rule out the possibility that the recent increase in brightness was caused by a short-lived burst of enhanced accretion rather than steady-state accretion occurring throughout our observing campaign.",
        "watermark_text": "Epsilon Aurigae is an F - class major sequence star with a mass of 1 . 8 M☉ and radius 2 R☉ , located at about 40 light - years away in the constellation Auriga . It has been known for thousands decades to be accompanied by dusty matter that obscures its visible spectrum .The infrared excess emission detected around this body suggests it could have a circumstellar disk comparable to those observed around young galaxies such as T Tauri or Herbig Ae / Be stars . In addition , there are indications that the system contains a close companion which could also contribute to the seen infrared excess emission .We report new photometric surveys obtained using the United Kingdom Infrared Telescope ( UKIRT ) on Mauna Kea over the period 1997 - 2001 covering wavelengths between 0 . 9 - 2 . 5 microns . These data demonstrate considerable variations in both the near - infrared fluxes and colours of the main source consistent with shifts in the quantity of dust surrounding the star .This behaviour is very related to what is seen in other pre - principal - sequence complexes where accretion onto the main star causes periodic increases in luminosity followed by increased levels of reddening due to heating of the nearby dust grains . Our results show that the present degree of intensity in the system is fairly lowest relative to previous epochs but we cannot leave out the idea that the recent rise in intensity was due by a brief - lived pulse of enhanced accretion instead than steady - phase accretion occurring throughout our observing mission .",
        "rewrite_text": "Epsilon Aurigae is an F-class main-sequence star, with a mass of 1.8 M☉ and a radius of 2 R☉, situated approximately 40 light-years away in the constellation Auriga. It has long been known to be surrounded by dusty material that obscures its visible spectrum. The excess infrared emission detected around this star suggests the presence of a circumstellar disk, similar to those found around young stellar objects like T Tauri or Herbig Ae/Be stars. Furthermore, there are signs that the system includes a close companion that may also contribute to the observed infrared excess. We present new photometric surveys conducted with the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea between 1997 and 2001, covering wavelengths from 0.9 to 2.5 microns. The data reveal significant variations in both the near-infrared fluxes and colors of the primary source, which correlate with changes in the amount of surrounding dust. This behavior is closely related to phenomena observed in other pre-main-sequence systems, where accretion onto the main star leads to periodic luminosity increases, followed by heightened reddening caused by the heating of nearby dust grains. Our findings indicate that the current intensity level in the system is relatively low compared to previous epochs; however, we cannot dismiss the possibility that the recent increase in intensity was triggered by a short-lived spike in accretion, rather than a steady phase of accretion throughout our observation period.",
        "ori-fast-z-score": -0.8466487815452375,
        "water-fast-z-score": 6.6791181655235405,
        "rewrite-fast-z-score": -0.38138503569823695
    },
    {
        "original_text": "We present the first measurement of the angular power spectrum of polarized dust emission at millimeter wavelengths, using data taken with the BICEP2 experiment in Antarctica during 2010 and 2011. We find that the polarization signal is consistent with predictions for thermal dust emission based on models constrained by Planck observations of temperature fluctuations. The amplitude of this signal is comparable to or larger than the expected gravitational lensing contribution over most multipole ranges probed here (l = 40-250). This result suggests that dust may be an important foreground contaminant for future CMB experiments targeting tensor modes. \n \n Keywords: Cosmic microwave background, Polarization, Dust emission, Gravitational waves, Inflationary cosmology \n \n Millimeter-wave polarimetry has been proposed as one method to detect primordial gravitational waves generated during inflation. However, it remains unclear whether polarized dust emission will limit our ability to extract such signals from current and upcoming CMB experiments. Here we report measurements made with the Bicep2/Keck Array collaboration s instrument operating at 150 GHz. These results are used to constrain the properties of interstellar dust grains through their effect on the polarized radiation they emit.",
        "watermark_text": "We present the first measurement of the angular power spectrum of polarized dust radiation at millimeter wavelengths , using data taken with the BICEP2 study in Antarctica during 2010 and 2011 . We see that the polarization frequency is compatible with predictions for thermal dust absorption based on models constrained by Planck measurements of temperature fluctuations .The amplitude of this signal is analogous to or larger than the expected gravitational lensing impact over most multipole distances probed here ( l = 40 - 250 ) . This result suggests that dust may be an important foreground contaminant for future CMB experiments targeting tensor modes .Keywords : Cosmic microwave background , Polarization , Dust absorption , Gravitational waves , Inflationary cosmology Millimeter - wave polarimetry has been proposed as one method to identify primordial gravitational waves generated during inflation . However , it remains unsure whether polarized dust absorption will limit our ability to extract such signals from recent and upcoming CMB experiments .Here we publish observations made with the Bicep2 / Keck Array collaboration s instrument operating at 150 GHz . These data are using to constrain the properties of interstellar dust grains through their effect on the polarized emission they emit .",
        "rewrite_text": "We present the inaugural measurement of the angular power spectrum of polarized dust radiation at millimeter wavelengths, utilizing data collected by the BICEP2 experiment in Antarctica during 2010 and 2011. Our analysis indicates that the polarization frequency aligns with predictions for thermal dust absorption derived from models informed by Planck temperature fluctuation measurements. The amplitude of this signal is comparable to or exceeds the anticipated effects of gravitational lensing across most multipole ranges examined (l = 40 - 250). This finding implies that dust could serve as a significant foreground contaminant for upcoming CMB experiments focused on detecting tensor modes. Keywords: Cosmic microwave background, Polarization, Dust absorption, Gravitational waves, Inflationary cosmology. Millimeter-wave polarimetry has been proposed as a technique to identify primordial gravitational waves produced during inflation. However, there is still uncertainty regarding whether polarized dust absorption will hinder our ability to extract such signals in recent and future CMB experiments. In this study, we present observations gathered using the BICEP2/Keck Array collaboration's instrument operating at 150 GHz. These data are employed to constrain the characteristics of interstellar dust grains through their impact on the polarized emission they produce.",
        "ori-fast-z-score": 1.9126494315742406,
        "water-fast-z-score": 6.260990336999411,
        "rewrite-fast-z-score": 2.400396792595916
    },
    {
        "original_text": "The collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I ll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "watermark_text": "The interaction between the Milky Way and its closest neighbor , M31 ( Andromeda ) , is predicted to event in about 4 billion years . This will be one of the most dramatic events ever experienced by humans .In this talk I will explain how we can using observations made with telescopes on Earth as well as space - based observatories such as Hubble Space Telescope to study these collisions and learn more about black material , galaxies , stars , white holes , and other cosmic phenomena that are part of our universe . I will also discuss some of my research projects related to researching star mergers using data received at the W . M .Keck Observatory located on Mauna Kea , Hawaii . Finally , I ll share what it was like for me to work there during my summer apprenticeship last year .Keywords : Dark Matter , Galaxy merger , Black Hole , Cosmic Evolution , Cosmology , Astrophysics",
        "rewrite_text": "The anticipated interaction between the Milky Way and its nearest neighbor, M31 (Andromeda), is expected to take place in approximately 4 billion years. This event will be among the most significant occurrences in human history. In my presentation, I will discuss how we can utilize data gathered from both Earth-based telescopes and space observatories like the Hubble Space Telescope to study these cosmic collisions and gain insights into dark matter, galaxies, stars, black holes, and other phenomena within our universe. Additionally, I will highlight some of my research projects focused on star mergers using data acquired at the W. M. Keck Observatory on Mauna Kea in Hawaii. Lastly, I will share my experiences from my summer internship at the observatory last year. \n\nKeywords: Dark Matter, Galaxy Merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics.",
        "ori-fast-z-score": 0.848528137423857,
        "water-fast-z-score": 4.808326112068523,
        "rewrite-fast-z-score": 1.1547005383792517
    },
    {
        "original_text": "The Standard Model is in excellent agreement with all current experimental data, but it leaves many questions unanswered and fails to provide an explanation for some phenomena observed experimentally.  The muon magnetic moment anomaly provides one such example where there are significant discrepancies between theory predictions and experiment measurements that cannot be explained within the Standard Model framework.   In this talk I will present the physics case for the new g-2 experiment at Fermilab which aims to measure the anomalous magnetic moment of the muon more accurately than ever before by using a novel technique based on laser cooling and trapping techniques developed over recent years.   ... This talk presents the physics case for the proposed new measurement of the muon s anomalous magnetic moment at Fermilab. It describes how the use of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous experiments. A number of other topics related to the project are also discussed including the status of the R&D program towards the goal of measuring the muon magnetic moment to 0.5 parts per million accuracy.",
        "watermark_text": "The Standard Model is in good agreement with all recent experimental evidence , but it leaves many issues unanswered and fails to provide an reason for some phenomena observed experimentally . The muon magnetic point anomaly gives one such example where there are significant discrepancies between theoretical estimates and observation observations that cannot be described within the Standard Model framework .In this talk I will present the physics case for the new g - 2 study at Fermilab which aims to measure the anomalous magnetic motion of the muon more accurately than ever before by using a innovative method based on laser cooling and trapping techniques established over recent generations . . . .This discussion presents the physics case for the suggested novel measurement of the muon s anomalous magnetic force at Fermilab . It details how the using of laser cooling and trapping can lead to a dramatic improvement in precision compared to previous study .A variety of other topics related to the project are also discussed including the status of the R & D program towards the objective of calculating the muon magnetic moment to 0 . 5 parts per million accuracy .",
        "rewrite_text": "The Standard Model aligns well with recent experimental data; however, it does not address several unresolved questions and fails to account for certain observed phenomena. A notable example is the muon magnetic moment anomaly, where there are significant differences between theoretical predictions and actual observations that the Standard Model cannot explain. In this presentation, I will outline the scientific rationale for the new g - 2 experiment at Fermilab, which aims to measure the anomalous magnetic behavior of the muon with unprecedented accuracy by employing innovative laser cooling and trapping techniques developed over recent years. This discussion will highlight the compelling physics behind this novel measurement of the muon's anomalous magnetic force at Fermilab, emphasizing how the application of laser cooling and trapping can substantially enhance precision compared to previous studies. Additionally, I will cover a range of related topics, including the current status of the R&D program aimed at achieving an accuracy of 0.5 parts per million for the calculation of the muon magnetic moment.",
        "ori-fast-z-score": 1.5096588248481377,
        "water-fast-z-score": 7.701031252562294,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "We report scanning magnetoresistance microscopy (SMRM) measurements on an atom chip with gold wires and microtraps fabricated by focused ion beam milling. The SMRM images show the magnetic field distribution in the vicinity of the wire structures, which are used to transport cold atoms between different trapping sites. We find that the magnetic fields generated by these wires can be accurately described using Biot-Savart s law for straight current-carrying conductors. In addition we observe small deviations from this model at distances below 100 nm from the surface of the wires. These deviations may arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their surfaces. Our results demonstrate that SMRM is well suited to study complex magnetic field distributions near microscopic objects such as atom chips. Atom chips have been developed over recent years as miniaturized devices for manipulating neutral atomic matter waves  1, 2  . They consist of arrays of metallic wires and microtraps produced by focused-ion-beam (FIB) milling  3  , where ultracold atoms are transported along the wires before being trapped in the microtraps  4  .\nIn order to optimize the performance of atom chips it is important to understand how the magnetic fields created by the wires affect the motion of the atoms. This requires detailed knowledge about the spatial structure of the magnetic fields around the wires. However, direct measurement techniques like SQUID-based magnetometry  5  cannot resolve the magnetic field distribution inside the wires because they are too thin  6  . Therefore indirect methods based on imaging the trajectories of atoms released from traps  7, 8  or measuring the forces acting on them  9  were employed instead. Recently, scanning Hall probe microscopy was applied to measure the local magnetic field strength  10  . Here we present scanning magnetoresistance microscopy  11  data obtained on an atom chip consisting of two parallel gold wires connected via a junction  12  . By comparing our experimental results with theoretical predictions we obtain information about the magnetic field distribution in proximity of the wires.",
        "watermark_text": "We report scanning magnetoresistance microscopy ( SMRM ) observations on an atom chip with gold wires and microtraps fabricated by concentrated ion beam milling . The SMRM pictures show the magnetic field spread in the vicinity of the wire structures , which are applied to transport cold molecules between various trap places .We see that the magnetic waves generated by these cables can be correctly explained following Biot - Savart s law for straight current - transporting conductors . In addition we study small deviations from this description at distances below 100 nm from the surface of the wires .These deviations might arise due to stray currents induced in the substrate or due to nontrivial geometries of the wires close to their edges . Our results show that SMRM is well suited to study difficult magnetic field distributions near microscopic structures such as atom devices .Atom devices have been created over recent years as miniaturized devices for manipulating neutral atomic matter waves 1 , 2 . They comprise of arrays of metallic wires and microtraps produced by concentrated - ion - laser ( FIB ) processing 3 , where ultracold atoms are transported along the wires before being trapped in the microtraps 4 .In order to optimize the performance of atom devices it is important to realize how the magnetic waves created by the wires affect the movement of the atoms . This requires complete understanding about the spatial shape of the magnetic waves around the wires .However , direct detection methods like SQUID - based magnetometry 5 cannot determine the magnetic field spread inside the wires because they are too thin 6 . Therefore indirect approaches derived on observing the trajectories of atoms released from nets 7 , 8 or tracking the forces working on them 9 were utilized instead .Recently , scanning Hall probe microscopy was used to measure the local magnetic force force 10 . Here we present scan magnetoresistance microscopy 11 data derived on an molecular computer comprised of two connected gold wires coupled via a junction 12 .By matching our experimental results with theoretical predictions we obtain knowledge about the magnetic force density in proximity of the wires .",
        "rewrite_text": "We present findings from scanning magnetoresistance microscopy (SMRM) performed on an atom chip featuring gold wires and microtraps created through concentrated ion beam milling. The SMRM images reveal the distribution of the magnetic field around the wire structures, which are used to transport cold molecules between various trapping locations. Our observations indicate that the magnetic fields generated by these wires can be accurately described by Biot-Savart's law for straight current-carrying conductors. Furthermore, we investigate minor deviations from this model at distances less than 100 nm from the wire surfaces. These discrepancies may be attributed to stray currents induced in the substrate or complex geometries of the wires near their edges. Our results demonstrate that SMRM is highly effective for examining intricate magnetic field distributions near microscopic structures such as atom devices. Over the past few years, atom devices have emerged as miniaturized tools for manipulating neutral atomic matter waves. These devices consist of arrays of metallic wires and microtraps produced through focused-ion-beam (FIB) processing, enabling the transport of ultracold atoms along the wires before they are captured in the microtraps. To optimize the performance of these atom devices, it is crucial to understand how the magnetic fields produced by the wires influence atomic movement, requiring a comprehensive understanding of the magnetic field's spatial configuration around the wires. However, direct detection methods like SQUID-based magnetometry are incapable of measuring the magnetic field distribution within the wires because they are too narrow. As a result, indirect methods that rely on observing the trajectories of released atoms or tracking the forces exerted on them have been employed instead. Recently, scanning Hall probe microscopy was utilized to gauge the local magnetic force. In this study, we present SMRM data obtained from a molecular computer consisting of two interconnected gold wires linked by a junction. By correlating our experimental data with theoretical models, we gain insights into the magnetic force density near the wires.",
        "ori-fast-z-score": -0.6923076923076923,
        "water-fast-z-score": 8.590007875090548,
        "rewrite-fast-z-score": 1.2418408411301325
    },
    {
        "original_text": "We show that the Pioneer anomaly, which is an anomalous acceleration observed in radio tracking data for two spacecrafts launched by NASA (Pioneer 10/11), can be explained if we assume that there exists a conformal symmetry breaking effect at large distances from our solar system. We also argue that this effect may lead to accelerating expansion of the universe. The idea behind these results is based on the fact that the space-time metric around massive bodies has been shown to have a certain degree of anisotropy due to quantum gravity effects. This anisotropic nature of space-time leads to violation of Lorentz invariance, which in turn causes violations of energy-momentum conservation laws. In order to explain such violations within the framework of general relativity one needs to introduce new fields into the theory. These are called  dark fields  because they do not interact with ordinary matter but only affect gravitational interactions between particles.",
        "watermark_text": "We suggest that the Pioneer anomaly , which is an anomalous velocity observed in radio search information for two spacecrafts delivered by NASA ( Pioneer 10 / 11 ) , can be described if we suppose that there exists a conformal symmetry breaking phenomenon at large distances from our solar body . We additionally claim that this effect could lead to accelerating expansion of the universe .The idea behind these results is based on the fact that the space - time metric around large bodies has been shown to have a certain degree of anisotropy owing to quantum gravitational influences . This anisotropic nature of space - time leads to compliance of Lorentz invariance , which in turn causes breach of power - momentum conservation laws .In order to explain such defects within the framework of general relativity one needs to introduce different fields into the theory . These are called darkness fields because they do not interact with normal matter but only affect gravitational interactions between particles .",
        "rewrite_text": "We propose that the Pioneer anomaly, which refers to the unexpected velocity observed in the radio data from NASA's Pioneer 10 and 11 spacecraft, can be explained by the existence of a phenomenon that breaks conformal symmetry at large distances from our solar system. Furthermore, we argue that this effect could contribute to the accelerating expansion of the universe. The underlying principle of these findings is that the space-time metric around significant celestial bodies exhibits a degree of anisotropy due to quantum gravitational effects. This anisotropic characteristic of space-time complies with Lorentz invariance, which subsequently leads to violations of the conservation laws of energy and momentum. To address these inconsistencies within the framework of general relativity, it is necessary to introduce additional fields into the theory. These fields, termed \"darkness fields,\" do not interact with ordinary matter but influence the gravitational interactions between particles.",
        "ori-fast-z-score": 0.46499055497527714,
        "water-fast-z-score": 6.0448772146786025,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on an exceptional flare detected by Swift/XRT at t ~ 1 day post-burst, which lasted for more than 100 ks (~20 hr). The flare was followed up with observations performed with XMM-Newton/EPIC-pn between 2.5 days to 3 months post-burst. We find that this flare is best described as a superposition of two components: one component lasting about 50 ks peaking around 10^-3 s and another component lasting about 70 ks peaking around 5 x 10^4 s. Both components are well fitted by exponentially cut-off power-laws with photon indices Γ = -1.6 ± 0.1 and -2.2 ± 0.3 respectively. No significant spectral evolution during either of these flares has been found. This flare is among the most energetic ever seen in any gamma-ray burst.",
        "watermark_text": "We report on an exceptional burst detected by Swift / XRT at t ~ 1 day post - flare , which occurred for more than 100 ks ( ~ 20 hr ) . The flare was followed up with observations performed with XMM - Newton / EPIC - pn between 2 . 5 weeks to 3 weeks following - flare .We see that this flare is better depicted as a superposition of two parts : one element lasting about 50 ks peaking roughly 10 ^ - 3 s and another component lasting about 70 ks peaking roughly 5 x 10 ^ 4 s . Both components are better fitted by exponentially drop - off power - laws with photon indices Γ = - 1 . 6 ± 0 . 1 and - 2 . 2 ± 0 . 3 respectively . No notable spectral evolution during either of these flares has been determined .This burst is among the most intense yet shown in any gamma - ray flare .",
        "rewrite_text": "We report the detection of an extraordinary burst by Swift/XRT approximately one day after a flare, which lasted for over 100 ks (around 20 hours). Follow-up observations were conducted using XMM-Newton/EPIC-pn between 2.5 to 3 weeks after the initial flare. Analysis indicates that this flare can be better understood as a combination of two components: the first lasts about 50 ks and peaks at roughly 10^-3 s, while the second lasts about 70 ks, peaking around 5 x 10^4 s. Both components are more accurately described by exponential decay power laws with photon indices Γ = -1.6 ± 0.1 and -2.2 ± 0.3, respectively. No significant spectral evolution has been observed during either component of the flare. This burst ranks among the most intense detected in any gamma-ray flare to date.",
        "ori-fast-z-score": -0.8320502943378437,
        "water-fast-z-score": 4.900980294098034,
        "rewrite-fast-z-score": 1.2135597524338357
    },
    {
        "original_text": "The question is not new, but it has been recently revived by the publication of two papers in Nature and Science that claim to show that biodiversity loss leads to ecosystem collapse.  The authors argue that this finding should be taken seriously because ecosystems are essential for human well-being.   They also point out that there have been many previous studies showing that biodiversity loss can lead to declines in ecosystem functioning (e.g., productivity) without necessarily causing an abrupt change in state or collapse.    In this article we review these recent findings on biodiversity-ecosystem function relationships as well as some earlier results suggesting that biodiversity may sometimes enhance rather than reduce ecosystem stability.  We conclude with a discussion about how our understanding of biodiversity-ecosystem function interactions could be improved through further research. Biodiversity loss is one of humanity s greatest challenges today. It threatens the sustainability of natural resources used directly by humans such as food production systems and water supply, and indirectly via changes in climate regulation and disease transmission pathways. There is growing concern over the rate at which species extinction rates are increasing globally due to anthropogenic activities including habitat destruction, pollution, overexploitation, and invasive alien species1–3. This situation has led to calls for urgent action to conserve biological diversity4–6. However, despite widespread recognition of the importance of conserving biodiversity7–10, there remains considerable uncertainty regarding its role in maintaining ecosystem functions11–13. A number of theoretical models suggest that biodiversity loss will cause reductions in ecosystem functioning14–16. For example, Tilman et al. (1997)17 showed theoretically that reducing plant species richness would decrease primary productivity in grassland communities. Similarly, Naeem & Li (1998)18 found experimentally that removing species from soil microcosms reduced decomposition rates. These predictions were supported by numerous subsequent empirical studies19–22.",
        "watermark_text": "The question is not current , but it has been lately revived by the publication of two papers in Nature and Science that argue to find that ecosystem failure leads to biodiversity breakdown . The authors argue that this determination should be taken seriously because ecosystems are essential for human well - being .They especially note out that there have been many earlier findings indicating that fauna loss can lead to declines in ecological functioning ( e . g . , output ) without necessarily creating an unexpected change in state or failure . In this page we review these recent results on biodiversity - ecological structure interactions as well as some earlier findings indicating that fauna might often improve instead than limit ecological stability .We end with a debate about how our appreciation of wildlife - ecological structure interactions might be improved through further studies . Biodiversity loss is one of humanity s worst problems currently .It damages the safety of natural assets used directly by humans such as feed production systems and water supply , and indirectly via alterations in climate control and illness transmission pathways . There is growing awareness over the pace at which species extinction frequencies are growing globally due to anthropogenic efforts including habitat damage , contamination , overexploitation , and invasive alien species1 – 3 .This problem has led to calls for urgent action to conserve biological diversity4 – 6 . However , despite widespread appreciation of the importance of conserving biodiversity7 – 10 , there exists considerable uncertainty regarding its function in maintaining ecosystem functions11 – 13 .A variety of theoretical theories indicate that biodiversity losing will cause reductions in ecological functioning14 – 16 . For instance , Tilman et al .( 1997 ) 17 showed theoretically that decreasing plant population richness would affect basic efficiency in prairie systems . Similarly , Naeem & Li ( 1998 ) 18 found experimentally that removing species from soil microcosms lowered oxidation rates .These predictions were endorsed by various subsequent empirical studies19 – 22 .",
        "rewrite_text": "The issue of biodiversity loss has gained renewed attention following the publication of two studies in *Nature* and *Science*, which suggest that ecosystem failure contributes to a decline in biodiversity. The authors emphasize the importance of this finding, as healthy ecosystems are vital for human well-being. They point out that numerous previous studies have indicated that the loss of fauna can result in reduced ecological functioning, such as productivity, without necessarily leading to obvious state changes or outright failures. In this review, we analyze recent findings related to the interactions between biodiversity and ecological structure, as well as earlier research suggesting that fauna may often enhance rather than compromise ecological stability. We conclude by discussing how further research could deepen our understanding of wildlife and ecological structure interactions. Currently, biodiversity loss is one of the most pressing issues facing humanity. It threatens the stability of natural resources crucial for human use, such as food production and water supply, and it affects climate regulation and disease transmission pathways in indirect ways. There is a growing recognition of the accelerating rates of species extinction globally due to human activities like habitat destruction, pollution, overexploitation, and the introduction of invasive species. This crisis has led to urgent calls for action to protect biological diversity. Yet, despite a general acknowledgment of the importance of conserving biodiversity, significant uncertainty remains about its role in sustaining ecosystem functions. Various theoretical models suggest that a decline in biodiversity will lead to diminished ecological functioning. For example, Tilman et al. (1997) theoretically demonstrated that a decrease in plant diversity would impair fundamental efficiencies in prairie ecosystems. Similarly, Naeem & Li (1998) experimentally showed that removing species from soil microcosms resulted in lower oxidation rates. These predictions have been supported by numerous subsequent empirical studies.",
        "ori-fast-z-score": -2.0893227278421693,
        "water-fast-z-score": 9.779496623899794,
        "rewrite-fast-z-score": 0.07738232325341368
    },
    {
        "original_text": "We present the results for positronium ground state energy and wave function obtained by solving relativistic Schrödinger equation with Coulomb potential using variational method. The calculations are performed within two different approximations, namely nonrelativistic limit (NR) and first order perturbation theory (PT1). In NR approximation we use Hylleraas type trial wave functions which include spin dependent terms up to second power of inter particle distance. We also calculate the expectation value of kinetic energy operator using this wave function. In PT1 approximation we use Hyllraas type trial wave functions including spin dependent terms up to third power of inter particle distance alongwith one electron Dirac orbitals as basis set. Our calculated values of binding energies agree well with those reported earlier. \n\n\nThe author is grateful to Prof. S. K. Gupta, Director General, BARC, Mumbai for his encouragement during the course of work on this project.",
        "watermark_text": "We present the results for positronium ground state energy and wave function achieved by solving relativistic Schrödinger equation with Coulomb potential using variational technique . The calculations are performed within two different approximations , namely nonrelativistic limit ( NR ) and first order perturbation theory ( PT1 ) .In NR method we utilize Hylleraas kind trial wave functions which contain spin dependent terms up to second power of inter particle length . We also obtain the expectation value of kinetic power operator using this wave function .In PT1 approximation we utilize Hyllraas type trial wave parameters including spin dependent terms up to third power of inter quantum time alongwith one particle Dirac orbitals as basis set . Our measured measures of bound energies agree well with those published earlier .The author is thankful to Prof . S . K . Gupta , Director General , BARC , Mumbai for his encouragement during the course of research on this project .",
        "rewrite_text": "We report the findings on the ground state energy and wave function of positronium, determined by solving the relativistic Schrödinger equation with Coulomb potential through a variational technique. The calculations are conducted under two different approaches: the nonrelativistic limit (NR) and first-order perturbation theory (PT1). In the NR method, we employ Hylleraas-type trial wave functions that include spin-dependent terms up to the second power of the interparticle distance. We also calculate the expectation value of the kinetic energy operator using this wave function. In the PT1 approximation, we use Hylleraas-type trial wave parameters, which account for spin-dependent terms up to the third power of inter-quantum time, along with one-particle Dirac orbitals as the basis set. Our computed bound energies are consistent with previously published results. The author expresses gratitude to Prof. S. K. Gupta, Director General of BARC, Mumbai, for his support throughout this research project.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 3.396831102433787,
        "rewrite-fast-z-score": -1.649915822768611
    },
    {
        "original_text": "We introduce the notion of ``abstract factorials  , which are functions that can be used to represent and manipulate sets of integers in an efficient manner.  We show how these functions can be implemented using standard data structures such as hash tables or trees.   Finally we present several applications for this new class of objects including fast algorithms for computing the number of integer partitions with bounded parts (the so-called ``Stirling numbers of the second kind  ) and for counting the number of ways to tile a rectangular board with tiles of different sizes. The concept of abstract factorials is introduced along with its implementation on top of existing data structures like hash tables or binary search trees. Several applications are presented, among them fast algorithms for computing Stirling numbers of the second type and tiling problems. This work was supported by NSF grant CCF-0634420. 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers. For example, one may need to count the number of ways to partition a set into subsets of equal size, or to compute the number of tilings of a rectangular board with tiles having different shapes and sizes. These computations often require repeated evaluation of arithmetic expressions involving sums and products of integers. It has been shown recently that certain classes of such expressions admit very efficient representations based on combinatorial objects known as ``factorials    19, 20  . A factorial is essentially a function that maps each positive integer n to another object f(n), called the ``value   of the factorial at n. Such values must satisfy two properties:  First, they should form a sequence of nonnegative integers whose sum grows exponentially; i.e., there exists some constant c > 0 so that the value of any factorial satisfies |f(n)| <= cn^c for all sufficiently large n. Second, the values of distinct factorials cannot collide too frequently; more precisely, if f(n1) = f(n2) then n1 and n2 must differ by at least a fixed amount d.",
        "watermark_text": "We introduce the notion of ` ` abstract factorials , which are functions that can be used to represent and manipulate collections of numbers in an efficient manner . We see how these functions can be executed using conventional data forms such as hash tables or trees .Finally we present many applications for this new category of objects including rapid algorithms for calculation the number of integer partitions with bounded parts ( the so - called ` ` Stirling numbers of the second kind ) and for counting the proportion of ways to mosaic a square floor with tiles of different sizes . The concept of abstract factorials is proposed along with its use on top of older data systems like hash tables or binary search forests .Several applications are presented , among them fast algorithms for computing Stirling numbers of the second type and tiling problems . This work was supported by NSF grant CCF - 0634420 .1 Introduction In many computational contexts it is required to conduct operations over large libraries of numbers . For instance , one may need to count the quantity of ways to split a setting into subsets of equal size , or to compute the number of tilings of a square board with tiles having various shapes and shapes .These computations usually require repeated evaluation of algebraic expressions involving amounts and products of numbers . It has been shown lately that particular categories of such functions accept very efficient representations based on combinatorial objects called as ` ` factorials 19 , 20 .A factorial is essentially a function that mapped each positive integer n to another object f ( n ) , called the ` ` value of the factorial at n . Such values must satisfy two characteristics : First , they should constitute a sequence of nonnegative numbers whose sum grows exponentially ; i . e . , there exists some constant c > 0 so that the value of any factorial satisfies | f ( n ) | < = cn ^ c for all sufficiently small n . Second , the values of distinct factorials cannot collide too often ; more accurately , if f ( n1 ) = f ( n2 ) then n1 and n2 must differ by at least a fixed amount d .",
        "rewrite_text": "We present the concept of \"abstract factorials,\" which are functions designed to efficiently represent and manipulate collections of numbers. These functions can be implemented using standard data structures, such as hash tables and trees. We also explore numerous applications for this new class of objects, including quick algorithms for calculating the number of integer partitions with bounded parts (known as the \"Stirling numbers of the second kind\") and for determining the various ways to tile a square floor with tiles of different dimensions. The notion of abstract factorials is introduced alongside their integration with established data structures, like hash tables and binary search trees. Among the many applications discussed are efficient algorithms for computing Stirling numbers of the second kind and solving tiling problems. This research was supported by NSF grant CCF-0634420. \n\n**1 Introduction** In various computational scenarios, performing operations on large collections of numbers is often necessary. For instance, one might need to count the number of ways to partition a set into subsets of equal size or calculate the different arrangements for tiling a square board using tiles of various shapes and sizes. Such computations typically involve the repeated evaluation of algebraic expressions that involve sums and products of numbers. Recent findings have demonstrated that certain categories of these functions can be represented very efficiently using combinatorial objects known as \"factorials.\" A factorial is essentially a function that maps each positive integer \\( n \\) to another object \\( f(n) \\), referred to as the \"value of the factorial at \\( n \\).\" These values must meet two criteria: first, they should form a sequence of nonnegative numbers whose sum increases exponentially. Specifically, there exists a constant \\( c > 0 \\) such that for sufficiently small \\( n \\), the values satisfy \\( |f(n)| \\leq cn^c \\). Second, the outputs of distinct factorials must not frequently coincide; more precisely, if \\( f(n_1) = f(n_2) \\), then \\( n_1 \\) and \\( n_2 \\) must differ by at least a fixed amount \\( d \\).",
        "ori-fast-z-score": -1.7905475715715027,
        "water-fast-z-score": 6.2092042056506624,
        "rewrite-fast-z-score": -0.30499714066520933
    },
    {
        "original_text": "We study the ground state properties and excitations in the spin-1/2 Heisenberg antiferromagnet on the square lattice with nearest-neighbor interactions only, using exact diagonalization (ED) for small clusters up to 12 sites as well as density-matrix renormalization group (DMRG) calculations for larger systems. We find that the ground-state energy per site is lower than the classical value by about 0.25J, where J denotes the exchange coupling constant between neighboring spins. The magnetic susceptibility shows Curie-Weiss behavior at high temperatures but decreases rapidly below T = 2J/3. This indicates strong quantum fluctuations which are also reflected in the low-temperature dependence of the specific heat. In addition we observe an unusual peak structure in the spin-spin correlation function S(q). For q along the principal axes of the Brillouin zone this peak has its maximum at q = π while it shifts towards smaller values when approaching the diagonals.",
        "watermark_text": "We research the ground state properties and excitations in the spin - 1 / 2 Heisenberg antiferromagnet on the square lattice with nearest - neighbor interactions only , using accurate diagonalization ( ED ) for large clusters up to 12 locations as well as density - vector renormalization group ( DMRG ) estimates for larger systems . We see that the ground - state energy per site is lower than the classical value by about 0 . 25J , where J refers the transfer coupling constant between neighboring spins .The magnetic susceptibility displays Curie - Weiss behavior at high temperatures but decreases quickly below T = 2J / 3 . This implies deep quantum fluctuations which are also reflected in the small - temperature dependence of the specific heat .In addition we encounter an peculiar peak structure in the spin - spinning correlation function S ( q ) . For q along the primary axes of the Brillouin zone this peak has its highest at q = π while it shifts towards lesser values when approaching the diagonals .",
        "rewrite_text": "We investigate the ground state properties and excitations of the spin-1/2 Heisenberg antiferromagnet on a square lattice with only nearest-neighbor interactions. Our methods include precise exact diagonalization (ED) for large clusters of up to 12 sites, as well as density-matrix renormalization group (DMRG) techniques for larger systems. Our findings reveal that the ground state energy per site is approximately 0.25J lower than the classical prediction, where J denotes the coupling constant between adjacent spins. The magnetic susceptibility exhibits Curie-Weiss behavior at elevated temperatures but rapidly decreases below T = 2J/3, indicating significant quantum fluctuations. This is further reflected in the weak temperature dependence of the specific heat. Additionally, we observe an interesting peak structure in the spin-spin correlation function S(q). For momentum q along the main axes of the Brillouin zone, this peak reaches a maximum at q = π, but shifts to lower values as one approaches the diagonal directions.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.1111111111111111
    },
    {
        "original_text": "We study entanglement properties of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. We consider both ground state and thermal states for this system. In particular we calculate the von Neumann entropy S(A) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I(A; B) between any pair of disjoint regions A and B. The results are compared against numerical simulations performed by means of Monte Carlo techniques. For the ground state it turns out that there exists an area law for the von Neumann entropy, i.e., S(A) ∝ L−d−1 where d denotes the dimension of region A and L its linear size. Moreover, we find that the mutual information decays exponentially fast when one moves away from the diagonal line joining the centers of the regions A and B. These findings agree very well with those obtained using exact methods based on Matrix Product States (MPS). Finally, we also show how these results can be used to obtain bounds on the topological entropy of the Toric Code.",
        "watermark_text": "We explore entanglement properties of the Toric Code model in two dimensions , which is characterized on a square lattice with periodic border conditions . We consider both ground state and thermal states for this process .In particular we estimate the von Neumann entropy S ( A ) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I ( A ; B ) between any pair of disjoint regions A and B . The results are compared against quantitative simulations conducted by means of Monte Carlo methods .For the ground state it turns out that there exists an area law for the von Neumann entropy , i . e . , S ( A ) [UNK] L−d−1 where d indicates the dimension of zone A and L its linear size . Moreover , we find that the mutual intelligence decays exponentially rapidly when one moves away from the diagonal line joining the centers of the regions A and B .These conclusions follow very best with those acquired using accurate methods based on Matrix Product States ( MPS ) . Finally , we also demonstrate how these results can be used to obtain limits on the topological entropy of the Toric Code .",
        "rewrite_text": "We investigate the entanglement characteristics of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. Our analysis includes both the ground state and thermal states. Specifically, we calculate the von Neumann entropy \\( S(A) = -\\text{Tr}(\\rho_A \\ln \\rho_A) \\) for different regions \\( A \\) of the lattice, as well as the mutual information \\( I(A; B) \\) between any pair of disjoint regions \\( A \\) and \\( B \\). Our findings are juxtaposed with quantitative simulations performed using Monte Carlo methods. For the ground state, we discover that the von Neumann entropy adheres to an area law, expressed as \\( S(A) \\sim L^{-d-1} \\), where \\( d \\) signifies the dimension of region \\( A \\) and \\( L \\) represents its linear size. Additionally, we observe that the mutual information decreases exponentially as one moves away from the diagonal line connecting the centers of regions \\( A \\) and \\( B \\). These conclusions align well with the findings derived from precise methods utilizing Matrix Product States (MPS). Lastly, we illustrate how these results can be leveraged to establish limits on the topological entropy of the Toric Code.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We study the distribution of the largest fragment in a fragmentation process, which is an important quantity for understanding how to optimize data transmission over networks and storage systems.  We show that this distribution can be approximated by a power law with exponent 1 + 1/(2 - p), where p is the probability that two fragments merge into one when they are adjacent on disk (the so-called  mergeability parameter ). This result explains why previous studies have found different exponents depending on whether or not merging was allowed. The approximation also allows us to derive closed-form expressions for other quantities such as the mean size of the largest fragment and its variance. Finally, we use our results to explain some recent experimental findings about file sizes in peer-to-peer systems. In many applications involving data transmission over networks and distributed storage systems, it is useful to understand how large the largest fragment will become during the course of the system s evolution. For example, if a network node has to transmit a certain amount of information within a given time limit, then knowing what fraction of the total data needs to be transmitted at any point in time may help improve performance. Similarly, in distributed storage systems, knowing the expected size of the largest fragment helps determine how much space each node should reserve for storing replicas.",
        "watermark_text": "We consider the distribution of the greatest fragment in a fragmentation process , which is an important quantity for knowledge how to optimize data communication over networks and storage systems . We see that this distribution can be approximated by a power law with exponent 1 + 1 / ( 2 - p ) , where p is the probability that two fragments mix into one when they are adjoining on disk ( the so - called mergeability coefficient ) .This result provides why previous research have discovered different exponents depending on whether or not merging was allowed . The method also enables us to derive closed - form expressions for other quantities such as the mean size of the greatest fragment and its variance .Finally , we utilize our findings to explain some latest empirical results about file sizes in peer - to - peer systems . In many applications using data communication over networks and distributed storage systems , it is important to realize how large the greatest fragment will become during the course of the program s evolution .For instance , if a network node has to transmit a certain quantity of information within a given time limitation , then understanding what fraction of the total data needs to be transmitted at any point in time might help increase efficiency . Similarly , in distributed storage systems , knowing the expected size of the greatest fragment allows determine how many space each node should reserve for storing replicas .",
        "rewrite_text": "We analyze the distribution of the largest fragment in a fragmentation process, an essential metric for optimizing data communication over networks and storage systems. Our findings indicate that this distribution can be approximated by a power law with an exponent of 1 + 1 / (2 - p), where p represents the probability of two adjacent fragments merging into one on disk, known as the mergeability coefficient. This result clarifies why previous studies have observed varying exponents based on whether merging was permitted. Moreover, our method facilitates the derivation of closed-form expressions for other related metrics, such as the mean size and variance of the largest fragment. Finally, we apply our insights to elucidate recent empirical findings regarding file sizes in peer-to-peer systems. In numerous applications involving data communication across networks and distributed storage systems, understanding the potential size of the largest fragment as the program progresses is vital. For example, when a network node must transmit a specific amount of information within a set timeframe, knowing the fraction of total data required at any moment can enhance efficiency. Additionally, in distributed storage systems, understanding the expected size of the largest fragment aids in determining the appropriate storage space each node should allocate for keeping replicas.",
        "ori-fast-z-score": 2.3312620206007844,
        "water-fast-z-score": 7.739789908394605,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "We study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations  1–3  . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies  4  .\n \nIn particular, many researchers have studied the effects of vaccination programs  5–7  , quarantine  8, 9  and isolation  10, 11  on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature  12, 13  , humidity  14, 15  and rainfall  16  on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations  17  . However, stochastic models  18, 19  and agent-based models  20, 21  also exist. Agent-based models allow us to take into account individual behaviors  22  while stochastic models provide more realistic descriptions of random events  23  . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0  24  , which represents the average number",
        "watermark_text": "We explore the nonlinear dynamics of infectious infections transmission in a population , where persons are split into three categories : affected ( S ) , infected ( I ) and returned / deleted ( R ) . We consider two different models : SIR model and SEIR model .In both cases we suppose that there is no birth or dying in the population . The main goal of this research is to examine how the infection spreads through the population depending on its characteristics .For instance , if the infection rate is too big then it could lead to an outbreak outbreak . On the other hand , if the return speed is very huge compared to the infection rate then the quantity of infectives will decrease rapidly .Finally , we give some numerical simulations which illustrate our findings . Keywords : Nonlinear dynamics , infectious deaths , tuberculosis , SIR model , SEIR model .1 Introduction Many numerical models have been created over time to explain the spread of infectious infections within communities 1 – 3 . These methods can be used as tools to explain the spreading patterns of these diseases and help public medical institutions making decisions about prevention plans 4 .In particular , many scientists have researched the effects of vaccination programs 5 – 7 , quarantine 8 , 9 and isolation 10 , 11 on the evolution of epidemics . Other studies emphasis on the impact of environmental factors such as temperature 12 , 13 , moisture 14 , 15 and rainfall 16 on the propagation of pathogens .The majority of older studies use deterministic theories based on ordinary differential equations 17 . However , stochastic systems 18 , 19 and agent - based models 20 , 21 also exist .Agent - based methods help us to take into consideration individual behaviors 22 while stochastic theories provide more realistic descriptions of random events 23 . In this page , we propose additional mathematical models explaining the spread of infectious infections in a closed population .Our aim is to analyze the impact of several variables on the response of the system . More specifically , we want to estimate whether the infection will die out naturally or lead an outbreak outbreak .To do so , we first introduce the fundamental reproduction number R0 24 , which equals the average number",
        "rewrite_text": "We investigate the nonlinear dynamics of infectious disease transmission within a population categorized into three groups: susceptible (S), infected (I), and recovered/removed (R). Our analysis focuses on two distinct models: the SIR model and the SEIR model. In both scenarios, we assume that the population experiences neither births nor deaths. The primary aim of this research is to understand how the infection propagates through the population based on its characteristics. For example, if the infection rate is excessively high, it could lead to an outbreak. Conversely, if the recovery rate significantly surpasses the infection rate, the number of infectives will diminish rapidly. Additionally, we present numerical simulations that illustrate our findings. \n\n**Keywords:** Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. \n\n### 1. Introduction \n\nNumerous numerical models have been developed over time to elucidate the spread of infectious diseases within communities. These frameworks serve as tools to clarify the transmission dynamics of such illnesses and assist public health authorities in crafting prevention strategies. Specifically, many researchers have examined the effects of vaccination programs, quarantine measures, and isolation practices on the progression of epidemics. Other studies have highlighted the influence of environmental factors like temperature, humidity, and rainfall on pathogen transmission. Traditionally, most studies employed deterministic models based on ordinary differential equations. However, stochastic systems and agent-based models also exist. Agent-based approaches allow for the consideration of individual behaviors, while stochastic models offer more realistic depictions of random events. In this work, we propose additional mathematical models that describe the spread of infectious diseases in a closed population. Our objective is to analyze the effects of various factors on the system's response, specifically to determine whether the infection will naturally extinguish or escalate into an outbreak. To facilitate this analysis, we first introduce the basic reproduction number \\( R_0 \\), which represents the average number of secondary infections produced by a single infected individual.",
        "ori-fast-z-score": -0.6172133998483676,
        "water-fast-z-score": 10.338324447460158,
        "rewrite-fast-z-score": 2.141239281438989
    },
    {
        "original_text": "We present new bolometer observations at 1.1mm wavelength for three clouds observed as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS) legacy program. The data were obtained with the Bolocam instrument on the Caltech Submillimeter Observatory to study star formation across large scales within these clouds. We find that the dust continuum emission is well correlated with infrared extinction maps derived from 2MASS near-infrared photometry. Using this correlation we derive an average dust temperature of 14K over each cloud. This value agrees very well with previous estimates based on single-dish submillimeter measurements. We also use our data to estimate the total mass contained in dense cores identified by the Herschel Space Observatory s Photodetector Array Camera & Spectrometer (PACS). Our results show good agreement between the masses estimated using PACS 70 micron fluxes and those determined directly from the Bolocam data.",
        "watermark_text": "We report new bolometer observations at 1 . 1mm wavelength for three clouds observed as part of the Spitzer Infrared Nearby Galaxies Survey ( SINGS ) legacy program . The data were obtained with the Bolocam technique on the Caltech Submillimeter Observatory to study star formation across large scales within these clouds .We see that the dust continuum emission is well associated with infrared extinction maps obtained from 2MASS near - infrared photometry . Using this relationship we derive an estimated dust temperature of 14K over each dust .This value agrees very best with previous accounts based on single - dish submillimeter calculations . We additionally using our information to estimate the total mass found in dense cores identified by the Herschel Space Observatory s Photodetector Array Camera & Spectrometer ( PACS ) .Our results show good agreement between the masses predicted using PACS 70 micron fluxes and those estimated directly from the Bolocam data .",
        "rewrite_text": "We present new bolometer observations at a wavelength of 1.1 mm for three clouds studied as part of the Spitzer Infrared Nearby Galaxies Survey (SINGS) legacy program. The data were collected using the Bolocam technique at the Caltech Submillimeter Observatory to investigate star formation on large scales within these clouds. Our analysis reveals that the dust continuum emission is strongly correlated with infrared extinction maps derived from 2MASS near-infrared photometry. Utilizing this correlation, we estimated the dust temperature to be approximately 14 K across the clouds, which is consistent with prior findings from single-dish submillimeter measurements. Furthermore, we leveraged our data to estimate the total mass contained within dense cores identified by the Photodetector Array Camera & Spectrometer (PACS) aboard the Herschel Space Observatory. Our results indicate a strong agreement between the masses predicted from PACS 70-micron fluxes and those directly estimated from the Bolocam data.",
        "ori-fast-z-score": 1.3858697343671664,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 1.171700198827415
    },
    {
        "original_text": "The PVLAS collaboration has recently reported results on light-by-light scattering in vacuum, which are inconsistent with Standard Model predictions.  In this note we discuss possible interpretations of these data within the framework of quantum field theory and string theory. We argue that the most natural interpretation is to assume that the observed effect arises due to new particles coupling to photons via an effective dimension-8 operator. The required mass scale for such particles can be as low as 10 GeV or even lower if one assumes that they couple only weakly to ordinary matter. If confirmed by further experiments, these observations would have profound implications both for particle physics phenomenology and cosmological models. The PVLAS collaboration has recently announced their measurement of light-by-light scattering in vacuo  1  . This process violates parity conservation at tree level and thus cannot occur in the Standard Model (SM)  2  , but it could arise through loop effects  3  .\nIn particular, the authors report observing a signal consistent with the SM prediction  4  \nwhere G F = 1.1663787(6) × 10−5GeV−2 is Fermi s constant  5  , θ W ≈ 0.23 is the weak mixing angle  6  , m e is the electron mass, and M Pl ≡ 1/ √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass  7, 8  . However, the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations,\nThis discrepancy between experiment and theory may indicate the presence of new physics beyond the SM  9  .",
        "watermark_text": "The PVLAS collaboration has recently announced findings on light - by - light diffusion in vacuum , which are inconsistent with Standard Model expectations . In this note we explain possible interpretations of these information within the framework of quantum field theory and string theory .We argue that the most natural interpretation is to assume that the seen effect arises owing to new objects coupling to photons via an efficient dimension - 8 operator . The expected mass scale for such particles can be as low as 10 GeV or even smaller if one assumes that they couple only weakly to normal matter .If confirmed by further studies , these observations would have profound implications both for electron physics phenomenology and cosmological predictions . The PVLAS collaboration has recently announced their observation of light - by - light diffusion in vacuo 1 .This process violates parity conservation at forest level and therefore cannot occur in the Standard Model ( SM ) 2 , but it could occur through ring effects 3 . In particular , the writers report studying a signal compatible with the SM prediction 4 where G F = 1 . 1663787 ( 6 ) × 10−5GeV−2 is Fermi s constant 5 , θ W ≈ 0 . 23 is the strong mixing angle 6 , m e is the electron mass , and M Pl ≡ 1 / √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass 7 , 8 .However , the measured value of the cross area approaches the theoretical expectation by more than three standard deviations , This discrepancy between experiment and theory could indicate the presence of new science beyond the SM 9 .",
        "rewrite_text": "The PVLAS collaboration has recently reported findings on light-by-light scattering in vacuum that do not align with the predictions of the Standard Model. In this note, we discuss potential interpretations of these results within quantum field theory and string theory frameworks. We suggest that the most straightforward explanation is the existence of new particles that couple to photons through an efficient dimension-8 operator. These particles could have a mass scale as low as 10 GeV or even lower, particularly if their coupling to ordinary matter is weak. If further research confirms these findings, they could have significant implications for both the phenomenology of electron physics and cosmological models. The collaboration has announced their observation of light-by-light scattering in vacuo, a process that violates parity conservation at the fundamental level and does not occur within the Standard Model; however, it may happen via loop effects. The authors report a signal that aligns with the Standard Model prediction, where Fermi's constant \\( G_F = 1.1663787(6) \\times 10^{-5} \\text{GeV}^{-2} \\), \\( \\theta_W \\approx 0.23 \\) is the strong mixing angle, \\( m_e \\) is the electron mass, and \\( M_{Pl} \\equiv 1/\\sqrt{8\\pi G_N} \\approx 2\\times10^{18} \\text{GeV} \\) is the reduced Planck mass. However, the measured cross section approaches the theoretical expectation by more than three standard deviations, indicating a potential discrepancy that could suggest the presence of new physics beyond the Standard Model.",
        "ori-fast-z-score": -0.9534625892455924,
        "water-fast-z-score": 4.461042580036746,
        "rewrite-fast-z-score": 0.9045340337332909
    },
    {
        "original_text": "We present the results of 3D radiation hydrodynamic simulations of accretion disks around black holes, performed with our new code RHD3DPHOTON. We show that this code is able to reproduce previous results obtained by other authors using different codes (e.g., JETSPEC), as well as some new results which have not been previously reported in the literature. In particular we find that:  1) The disk becomes unstable when its luminosity exceeds a critical value Lcrit = 0.1LEdd.  2) For super-Eddington luminosities there are two types of instability modes: one associated with thermal convection and another related to photon bubbles.  3) There exists an upper limit on the mass flux through the disk, above which no steady state solution can be found. This result has important implications for models of AGN feedback. 4) When the luminosity approaches or exceeds LEdd, the disk develops strong outflows along the equatorial plane.",
        "watermark_text": "We present the results of 3D radiation hydrodynamic simulations of accretion balls around black holes , conducted with our new code RHD3DPHOTON . We see that this code is could to repeat earlier findings obtained by other researchers using specific coding ( e . g . , JETSPEC ) , as well as some fresh results which have not been previously reported in the literature .In particular we find that : 1 ) The disk gets unstable when its luminosity exceeds a critical number Lcrit = 0 . 1LEdd . 2 ) For ultra - Eddington luminosities there are two forms of instability modes : one related with thermal convection and another linked to photon bubbles .3 ) There exists an upper maximum on the mass flux through the disk , above which no continuous state solution can be found . This result has useful consequences for models of AGN feedback .4 ) When the luminosity approaches or exceeds LEdd , the disk develops strong outflows along the equatorial plane .",
        "rewrite_text": "We present the findings from our 3D radiation hydrodynamic simulations of accretion disks around black holes, performed using our newly developed code, RHD3DPHOTON. Our simulations successfully replicate previous results obtained by other researchers utilizing different codes, such as JETSPEC, and also yield new insights that have not been documented in existing literature. Notably, we observe the following: 1) The disk becomes unstable when its luminosity surpasses a critical threshold of Lcrit = 0.1LEdd. 2) At ultra-Eddington luminosities, two types of instability modes emerge: one associated with thermal convection and another linked to photon bubbles. 3) There is a maximum limit on the mass flux through the disk, beyond which no continuous state solution can be established. This finding has significant implications for models of AGN feedback. 4) As the luminosity approaches or exceeds LEdd, the disk exhibits strong outflows along the equatorial plane.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "We report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.",
        "watermark_text": "We report on the X - ray characteristics of the early , neighboring ( d = 11 pc ) , low - weight binary system 2MASS J1101 - 2677AB discovered by Burgasser et al . ( 2007 ) .The main component is an M8 dwarf with T eff ~ 2600 K and log f ~ 5 . 0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log f ~ 4 . 2 . We observed this target for about 50 ks using Chandra ACIS - S in order to search for indication of coronal interaction associated with either or both components .No much emitted was seen at the position of the source down to a 3 - sigma upper maximum of 1 x 10 ^ 28 erg s - 1 cm - 2 . This non - measurement indicates that if there are active regions present they must be small and / or cold relative to those observed on more massive stars .In addition we find no evidence of flaring behavior during our experiment which constrains any proposed magnetic force power to fewer than 100 G .",
        "rewrite_text": "We present our findings on the X-ray characteristics of the nearby low-mass binary system 2MASS J1101-2677AB, located just 11 parsecs away, as discovered by Burgasser et al. (2007). The primary component is an M8 dwarf with an effective temperature (T_eff) of approximately 2600 K and a logarithmic flux (log f) of about 5.0, while its companion is classified as an L5 brown dwarf with an effective temperature of roughly 1400 K and a log f of around 4.2. We observed this system for approximately 50 kiloseconds using Chandra ACIS-S, aiming to detect any signs of coronal interaction related to either or both components. We did not observe significant emission at the source's position, achieving a 3-sigma upper limit of 1 x 10^28 erg s^-1 cm^-2. This lack of detection suggests that if active regions are present, they are likely small and/or cooler compared to those found on more massive stars. Furthermore, we found no evidence of flaring behavior during our observations, which limits any proposed magnetic field strength to less than 100 G.",
        "ori-fast-z-score": -1.1338934190276817,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -0.6974858324629157
    },
    {
        "original_text": "The geochemical behavior of uranium (U) and thorium (Th), which are radioactive elements, is discussed in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution.  The chemical properties of these two elements are similar; however, they have different physical characteristics that affect how they behave geologically.  Uranium has an atomic number of 92 with a half-life of 4.5 billion years while thorium has an atomic number of 90 with a half life of 1.4 billion years.   Both elements occur naturally throughout the Earth s crust but at varying concentrations depending upon the rock type.  They can be found in igneous rocks such as granite or basalt where they form minerals like uranite or thorite respectively.  These minerals may also contain other trace metals including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium.  Uranium and thorium are also present in sedimentary rocks such as sandstone, shale, limestone, dolomite, gypsum, anhydrite, salt beds, and evaporites.  Sedimentary rocks are formed by weathering processes when water erodes...",
        "watermark_text": "The geochemical behavior of uranium ( U ) and thorium ( Th ) , which are radioactive elements , is mentioned in relation to their influence on the origin and evolution of the crust of earth as well as biological evolution . The chemical properties of these two compounds are comparable ; however , they have different biological traits that affect how they react geologically .Uranium has an atomic number of 92 with a half - life of 4 . 5 billion decades while thorium has an atomic number of 90 with a half lifetime of 1 . 4 billion decades . Both compounds occur readily throughout the Earth s crust but at different amounts depending upon the stone type .They can be found in igneous rocks such as granite or basalt where they create minerals like uranite or thorite respectively . These rocks could also contain other trace metals including lead , silver , gold , copper , zinc , arsenic , selenium , molybdenum , cadmium , mercury , bismuth , antimony , tellurium , cobalt , nickel , manganese , iron , vanadium , chromium , tungsten , titanium , zirconium , niobium , tantalum , hafnium , rhenium , osmium , iridium , platinum , palladium , rhodium , ruthenium , and iridium .Uranium and thorium are also occur in sedimentary formations such as sandstone , shale , marble , dolomite , gypsum , anhydrite , salt beds , and evaporites . Sedimentary stones are created by weathering reactions when water erodes . . .",
        "rewrite_text": "The geochemical behavior of the radioactive elements uranium (U) and thorium (Th) is relevant to their roles in the formation and development of the Earth's crust as well as biological evolution. While their chemical properties are similar, their biological characteristics influence their geological reactivity differently. Uranium, with an atomic number of 92, has a half-life of 4.5 billion years, whereas thorium, with an atomic number of 90, has a half-life of 1.4 billion years. Both elements are found abundantly in the Earth’s crust, but their concentrations vary depending on the type of rock. They can be located in igneous rocks, such as granite and basalt, where they give rise to minerals like uraninite and thorite, respectively. These rocks may also contain various trace metals, including lead, silver, gold, copper, zinc, arsenic, selenium, molybdenum, cadmium, mercury, bismuth, antimony, tellurium, cobalt, nickel, manganese, iron, vanadium, chromium, tungsten, titanium, zirconium, niobium, tantalum, hafnium, rhenium, osmium, iridium, platinum, palladium, rhodium, ruthenium, and iridium. Moreover, uranium and thorium are present in sedimentary formations such as sandstone, shale, marble, dolomite, gypsum, anhydrite, salt beds, and evaporites, all of which are formed through weathering processes resulting from water erosion.",
        "ori-fast-z-score": -1.4814874939752933,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "We present new multiwavelength observations of the debris disk surrounding the nearby star AU Mic, which is located at an age of ~10 Myr in the constellation Pictor (~25 pc). The system has been studied extensively over many decades using ground-based optical imaging techniques as well as space-based infrared photometry and spectroscopy. We have obtained high-resolution images with the Hubble Space Telescope s Wide Field Camera 3 (WFC3) near-infrared camera to study the dust distribution on small scales. These data are complemented by archival Spitzer Infrared Array Camera (IRAC), Herschel Photodetector Array Camera & Spectrometer (PACS), and Submillimeter Array (SMA) observations that probe larger spatial scales. Our results show that there exists a large amount of cold dust within 1 AU of the central star, but no evidence for warm dust emission beyond this radius. This suggests that the inner edge of the outer disk may be truncated due to tidal forces exerted by the planet candidate AU Mic b.",
        "watermark_text": "We report new multiwavelength studies of the rubble disk surrounding the nearby star AU Mic , which is situated at an age of ~ 10 Myr in the constellation Pictor ( ~ 25 pc ) . The system has been studied frequently over numerous years employing ground - based optical detection methods as well as space - based infrared photometry and spectroscopy .We have achieved high - resolution images with the Hubble Space Telescope s Wide Field Camera 3 ( WFC3 ) near - infrared camera to study the dust flow on small scales . These data are complemented by archival Spitzer Infrared Array Camera ( IRAC ) , Herschel Photodetector Array Camera & Spectrometer ( PACS ) , and Submillimeter Array ( SMA ) observations that examine bigger spatial scales .Our results show that there exists a large number of cold matter within 1 AU of the main star , but no evidence for cool dust absorption beyond this radius . This implies that the inner boundary of the inner disk might be truncated due to tidal factors exerted by the planet candidate AU Mic b .",
        "rewrite_text": "We present new multiwavelength observations of the rubble disk surrounding the nearby star AU Mic, located in the constellation Pictor at an age of approximately 10 million years and about 25 parsecs away. This system has been the subject of extensive study over the years, utilizing both ground-based optical detection techniques and space-based infrared photometry and spectroscopy. Our high-resolution images, obtained with the Hubble Space Telescope's Wide Field Camera 3 (WFC3) near-infrared camera, allow us to investigate dust dynamics on small scales. These findings are enhanced by archival data from the Spitzer Infrared Array Camera (IRAC), the Herschel Photodetector Array Camera and Spectrometer (PACS), and the Submillimeter Array (SMA), which assess larger spatial scales. Our results indicate a significant presence of cold material within 1 AU of the star, but we found no signs of cool dust absorption beyond this distance. This suggests that the inner boundary of the disk may be truncated due to tidal effects from the planet candidate AU Mic b.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.8307471607356973
    },
    {
        "original_text": "We present new observations with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal an extended region of disturbed molecular gas surrounding the active galactic nucleus (AGN) of NGC 5194, also known as M51a or Whirlpool Galaxy. The AGN is located at the center of this interacting galaxy pair and has been classified as a Seyfert 2 based on optical spectroscopy. We detect two prominent dust lanes extending to the north-east and south-west of the AGN along its minor axis. These are likely caused by tidal forces between the galaxies during their interaction. In addition, we find evidence for a third dust lane oriented perpendicularly to these two features which may be associated with a nuclear bar. Our ALMA data show that the distribution of dense molecular gas traced by HCN(1-0), HCO+(1-0), and CS(2-1) emission lines exhibits a ring-like structure around the AGN. This feature appears to have been shaped by powerful outflows driven by the AGN.",
        "watermark_text": "We present new experiments with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate an extended region of disturbed molecular gas surrounding the active galactic nucleus ( AGN ) of NGC 5194 , commonly known as M51a or Whirlpool Galaxy . The AGN is situated at the center of this interacting galaxy couple and has been classified as a Seyfert 2 using on optical spectroscopy .We detect two notable dust lanes stretching to the north - west and south - west of the AGN along its minor axis . These are likely caused by tidal forces between the galaxies during their interaction .In addition , we find proof for a third dust track focused perpendicularly to these two structures which may be correlated with a atomic bar . Our ALMA results show that the distribution of dense molecular vapor traced by HCN ( 1 - 0 ) , HCO + ( 1 - 0 ) , and CS ( 2 - 1 ) emission lines exhibits a ring - like structure around the AGN .This structure appears to have been shaped by massive outflows driven by the AGN .",
        "rewrite_text": "We present new findings from experiments conducted with the Atacama Large Millimeter/submillimeter Array (ALMA), which reveal an extensive region of disturbed molecular gas surrounding the active galactic nucleus (AGN) of NGC 5194, commonly known as M51a or the Whirlpool Galaxy. This AGN, located at the center of the interacting galaxy pair, has been classified as a Seyfert 2 based on optical spectroscopy. Our observations reveal two prominent dust lanes extending to the northwest and southwest of the AGN along its minor axis, likely resulting from tidal interactions between the galaxies. Additionally, we have identified a third dust lane oriented perpendicular to the other two, possibly associated with an atomic bar. Our ALMA data indicates that the distribution of dense molecular gas, as traced by the HCN (1-0), HCO+ (1-0), and CS (2-1) emission lines, forms a ring-like structure around the AGN, which seems to have been influenced by significant outflows driven by the AGN itself.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": 0.9847319278346618
    },
    {
        "original_text": "We report on the Swift satellite s rapid response to the gamma-ray burst (GRB) 060614, which was detected by the Burst Alert Telescope at 07:41:06 UT June 14 2006 and localized within 1 arcmin in less than 3 minutes. The X-ray telescope began observing the field about 10 s later; optical photometry started about 20 s after that. We find no evidence for any unusual behavior during or immediately following this event. In particular we do not see any indication of a supernova component associated with it as has been seen in some other events. However, our data show that there is a very bright fading source coincident with the position determined by Swift/XRT. This source shows clear signs of being dominated by emission lines characteristic of a Wolf-Rayet star. Our results are consistent with those reported previously using ground-based telescopes. \n \n Keywords: Gamma ray bursts, Afterglows",
        "watermark_text": "We report on the Swift satellite s rapid response to the gamma - ray burst ( GRB ) 060614 , which was detected by the Burst Alert Telescope at 07 : 41 : 06 UT June 14 2006 and localized within 1 arcmin in fewer than 3 seconds . The X - ray observatory commenced observing the field about 10 s later ; imaging photometry begun about 20 s after that .We see no evidence for any strange activity during or immediately surrounding this event . In particular we do not see any evidence of a supernova component involved with it as has been seen in some other events .However , our statistics demonstrate that there is a very bright fading source coincident with the orientation determined by Swift / XRT . This source shows clear signs of being dominated by radiation patterns characteristic of a Wolf - Rayet star .Our results are compatible with those confirmed previously used ground - based telescopes . Keywords : Gamma ray bursts , Afterglows",
        "rewrite_text": "We present our findings on the Swift satellite's prompt response to the gamma-ray burst (GRB) 060614, which was detected by the Burst Alert Telescope at 07:41:06 UT on June 14, 2006, and localized to within 1 arcminute in under 3 seconds. The X-ray observatory began observing the field approximately 10 seconds later, followed by imaging photometry around 20 seconds after that. We found no indications of unusual activity during or immediately after this event, specifically lacking evidence of a supernova component, which has been associated with some other GRBs. However, our data reveal a very bright, fading source that coincides with the orientation established by Swift/XRT, exhibiting clear characteristics of radiation patterns typical of a Wolf-Rayet star. Our findings align with previous confirmations from ground-based telescopes. Keywords: Gamma-ray bursts, Afterglows.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.598123172175427,
        "rewrite-fast-z-score": 1.386750490563073
    },
    {
        "original_text": "We present an approach to predict the structural network organization in the cerebral cortex based on local node features, such as their position within the brain s surface or volume, and global topological characteristics. We use this method to study how different types of nodes are connected with each other across species (human, macaque monkey) and modalities (diffusion MRI tractography). Our results show that our model can accurately reproduce known patterns of cortico-cortical connections between areas, including those observed in humans but not yet described for monkeys. The proposed framework is general enough to be applied to any type of data where information about individual nodes  positions and pairwise interactions exists. This includes both anatomical and functional imaging datasets, which will allow us to investigate the relationship between structure and function at multiple scales. \n \n Introduction \n \n Brain connectomics aims to map all neuronal elements into a single comprehensive description of the human brain  1  . In recent years, advances in neuroimaging techniques have allowed researchers to obtain detailed maps of the brain s structural  2  , metabolic  3  , and functional  4  architecture. These new technologies provide unprecedented opportunities to understand how the brain works by studying its large-scale organization  5  .\n \nHowever, despite these advancements, there remains significant uncertainty regarding the precise nature of the relationships among neurons  6  . For example, it has been shown that some regions of the brain communicate more frequently than others  7-9 , while others exhibit higher levels of synchrony  10  . However, we still do not know whether these differences reflect specific wiring rules  11  or simply arise due to random fluctuations  12  . \n \n Here, we propose a novel computational framework to address this problem using machine learning methods  13  . Specifically, we aim to develop models capable of predicting the strength of connection between pairs of nodes given only information about their location and topology  14  . To achieve this goal, we first construct a set of training examples consisting of pairs of nodes whose interaction strengths are known  15  . Then, we train a classifier to learn the mapping between node features and edge weights  16  . Finally, we apply the trained model to unseen test cases  17  to infer unknown interactions",
        "watermark_text": "We present an way to predict the structural network structure in the brain cortex relying on local node characteristics , such as their placement within the brain s surface or volume , and global topological traits . We use this technology to study how various types of networks are connected with each other across taxa ( human , macaque mouse ) and modalities ( diffusion MRI tractography ) .Our results show that our model can accurately demonstrate established trends of cortico - cortical relationships between locations , notably those observed in humans but not already explained for monkeys . The proposed framework is basic enough to be applied to any type of data where information about individual nodes positions and pairwise relationships exists .This contains both anatomical and physiological imaging datasets , which will let us to examine the relationship between organization and function at multiple scales . Introduction Brain connectomics aims to map all neuronal components into a single comprehensive account of the human neural 1 .In recent years , advances in neuroimaging techniques have permitted investigators to obtain detailed maps of the brain s structural 2 , cellular 3 , and functional 4 architecture . These new approaches provide unprecedented possibilities to realize how the brain acts by examining its large - scale organization 5 .However , despite these advancements , there exists significant confusion regarding the exact nature of the relationships among neurons 6 . For instance , it has been shown that some regions of the brain communicate more frequently than others 7 - 9 , while many exhibit greater levels of synchrony 10 .However , we also do not understand whether these changes reflect specific wiring requirements 11 or simply arise due to random fluctuations 12 . Here , we propose a new computational framework to tackle this question using computer learning techniques 13 .Specifically , we attempt to develop models capable of predicting the strength of connection between pairs of nodes given only data about their direction and configuration 14 . To achieve this goal , we first build a setting of testing instances consisting of pairs of nodes whose interaction abilities are known 15 .Then , we train a classifier to teach the mapping between node characteristics and edge weights 16 . Finally , we apply the trained model to unseen test situations 17 to infer unknown interactions",
        "rewrite_text": "We introduce a novel method for predicting the structural network architecture of the brain cortex, which is based on local node attributes, such as their location on the brain's surface or within its volume, alongside global topological features. This technology allows us to explore the connections between different types of networks across species (humans, macaques, and mice) and modalities (including diffusion MRI tractography). Our findings indicate that the model effectively captures established patterns of cortico-cortical relationships, particularly those documented in humans but not previously elucidated in monkeys. The proposed framework is versatile enough to be applicable to any dataset that contains information about the positions of individual nodes and their pairwise relationships. This encompasses both anatomical and physiological imaging datasets, enabling us to investigate the interplay between organization and function across various scales. \n\n**Introduction**: Brain connectomics seeks to develop an all-encompassing map of neuronal components within the human brain. Recent advancements in neuroimaging techniques have allowed researchers to generate detailed representations of the brain’s structural, cellular, and functional architectures. These innovative approaches open up new avenues for understanding brain functionality by examining its large-scale organization. However, there remains considerable uncertainty surrounding the precise nature of interactions among neurons. For instance, some brain regions demonstrate more frequent communication than others, while many show higher levels of synchrony. Yet, it is unclear whether these variations indicate specific wiring requirements or if they simply result from random fluctuations. This study proposes a fresh computational framework to address this question through machine learning techniques. Specifically, we aim to create predictive models that estimate the strength of connections between pairs of nodes based solely on their orientation and configuration. To accomplish this, we first establish a test set comprising pairs of nodes with known interaction capabilities. We then train a classifier to establish the mapping between node characteristics and edge strengths. Finally, we apply the trained model to novel test scenarios to predict unknown interactions.",
        "ori-fast-z-score": -0.9662823901213162,
        "water-fast-z-score": 9.092421632741246,
        "rewrite-fast-z-score": 1.7692307692307692
    },
    {
        "original_text": "The electrochemical properties of materials are often strongly influenced by their phase transitions, which can be difficult to predict using conventional methods. In this work we present an approach that uses cyclic voltammetry (CV) data collected at different temperatures as input into a machine learning model in order to examine the thermodynamic stability of phases within a material system. We demonstrate our method on two prototypical systems - Fe3O4 and LiFePO4 - where it is able to successfully identify metastable phases and accurately predict equilibrium compositions across wide temperature ranges. The results showcase how CV-based approaches could provide new insights into complex chemical phenomena such as solid state reactions or battery degradation mechanisms. \n \n Introduction \n \n Many important physical processes occur through changes in crystal structure driven by thermal fluctuations  1  . These include transformations between polymorphs  2  , amorphous states  3  , and even liquid crystalline structures  4  . Such structural rearrangements have been shown to significantly affect the electrical  5  , optical  6  , magnetic  7  , mechanical  8  , and catalytic  9  properties of materials. As a result, understanding these transformations has become increasingly important for applications ranging from energy storage  10  to catalysis  11  .\n \nIn many cases, however, predicting the outcome of a transformation requires knowledge about its underlying free energies  12  . This information cannot always be obtained directly via experimentation due to kinetic barriers  13  , making computational techniques  14  particularly useful  15  . Unfortunately, most current theoretical models  16  require extensive parameterization  17  and/or detailed experimental characterization  18  before they can be applied effectively  19  . Moreover, while some recent studies  20  have demonstrated successes with deep neural networks  21  , there remains significant uncertainty regarding whether these approaches will generalize well  22  . \n \n Herein, we propose a novel approach based on cyclic voltammetry  23  that allows us to probe the thermodynamics of phase transformations without requiring any prior assumptions about the nature of the transition  24  . Our technique relies on collecting CV data over a range of temperatures  25  and then training a supervised  26  machine learning algorithm  27  to learn relationships between the measured currents  28  and the corresponding Gibbs free energies  29  . Once trained,...",
        "watermark_text": "The electrochemical qualities of substances are often strongly governed by their phase transitions , which can be challenging to predict using conventional methods . In this research we present an perspective that using cyclic voltammetry ( CV ) evidence generated at different temperatures as input into a machine learning model in order to examine the thermodynamic stability of phases within a solid system .We test our technique on two prototypical solutions - Fe3O4 and LiFePO4 - where it is ability to effectively identify metastable layers and correctly forecast stable compositions across wide temperature scales . The results display how CV - based methods could give innovative perspectives into complex chemical processes such as solid state reactions or battery reduction structures .Introduction Many significant physical processes arise through alterations in crystal composition driven by temperature fluctuations 1 . These include transformations between polymorphs 2 , amorphous states 3 , and even liquid crystalline structures 4 .Such structural rearrangements have been shown to significantly affect the electrical 5 , optical 6 , thermal 7 , thermal 8 , and catalytic 9 properties of substances . As a result , studying these transformations has become rapidly crucial for applications extending from power processing 10 to catalysis 11 .In many situations , however , predicting the result of a transformation requires knowledge about its underlying free energies 12 . This knowledge cannot often be obtained immediately via experimentation owing to kinetic limits 13 , making computational approaches 14 particularly useful 15 .Unfortunately , most current theoretical theories 16 require extensive parameterization 17 and / or extensive theoretical characterization 18 before they can be applied effectively 19 . Moreover , while some latest studies 20 have demonstrated successes with deep neural systems 21 , there exists significant doubt regarding whether these theories will generalize well 22 .Herein , we develop a innovative method built on cyclic voltammetry 23 that enables us to probe the thermodynamics of phase transformations without requiring any earlier predictions about the nature of the transfer 24 . Our practice relies on gathering CV measurements over a range of conditions 25 and then testing a controlled 26 machine learning algorithm 27 to find relationships between the measured currents 28 and the associated Gibbs free energies 29 .Once trained,...",
        "rewrite_text": "The electrochemical properties of substances are often significantly influenced by phase transitions, which can be difficult to anticipate using traditional methods. In this study, we propose a novel approach that leverages cyclic voltammetry (CV) data collected at various temperatures as input for a machine learning model to investigate the thermodynamic stability of phases within a solid system. We apply this technique to two prototype materials—Fe3O4 and LiFePO4—demonstrating its effectiveness in identifying metastable phases and accurately predicting stable compositions across a broad temperature range. The findings suggest that CV-based methodologies could offer new insights into complex chemical processes, including solid-state reactions and battery reduction mechanisms.\n\nIntroduction: Numerous important physical processes occur due to changes in crystal composition driven by temperature variations. These processes encompass transformations among polymorphs, amorphous states, and even liquid crystalline structures. Such structural changes have been shown to significantly influence the electrical, optical, thermal, and catalytic properties of materials. Consequently, investigating these transformations has become increasingly essential for various applications, ranging from energy conversion to catalysis. However, accurately predicting the outcomes of these transformations often necessitates knowledge of their underlying free energies. This information is typically challenging to acquire experimentally due to kinetic limitations, making computational methods particularly valuable. Unfortunately, most existing theoretical frameworks require extensive parameterization and detailed theoretical characterization before they can be effectively implemented. Furthermore, while some recent research has reported successes with deep neural networks, considerable skepticism remains regarding their ability to generalize well. In this work, we introduce a groundbreaking method based on cyclic voltammetry that allows us to explore the thermodynamics of phase transitions without prior assumptions about the nature of the transitions. Our approach involves collecting CV measurements across varying conditions and employing a controlled machine learning algorithm to identify correlations between the measured currents and the corresponding Gibbs free energies. Once trained...",
        "ori-fast-z-score": -1.6583123951777,
        "water-fast-z-score": 9.144286605749508,
        "rewrite-fast-z-score": -0.6882472016116853
    },
    {
        "original_text": "We present the results on systematic uncertainties in determination of hadronic parameters using QCD Sum Rules (QSR). We consider two types of uncertainties: theoretical and experimental ones. Theoretical uncertainty is estimated by varying renormalization scale, Borel mass parameter and threshold value used to separate ground state contribution from excited states one. Experimental error comes mainly from errors in input values for quark masses and decay constants. In addition we take into account also dependence of final result on choice of interpolating current. For each type of uncertainty we calculate its effect separately as well as total uncertainty which includes all sources mentioned above. Finally we compare our results with those presented recently in literature. \n \n Keywords: Systematics, Uncertainty, Hadronic Parameters, QCD Sum Rules, Quark Masses, Decay Constants. 1 Introduction \n \n In this work we study systematic uncertainties in determination of various hadronic parameters like masses, decay constants etc., using QCD Sum Rules  1  . This method allows us to obtain information about properties of low-lying resonances starting from first principles - Quantum Chromodynamics (QCD)  2  , without any additional assumptions or models. It has been successfully applied to many different processes involving light quarks such as: pion form factor  3  , nucleon electromagnetic  4  and axial  5  form factors  6  , semileptonic decays  7, 8  , heavy-light mesons  9  , charmonium  10  and bottomonium  11  systems  12  .\n \nThe main idea behind QCD Sum Rules is that correlation function constructed out of currents corresponding to particular quantum numbers can be represented as dispersion relation over physical spectrum of particles contributing to it. Then, after applying double Borel transformation  13  , i.e. transforming both variables p^2 and q^2 simultaneously  14  , where p is momentum flowing through initial state and q is momentum transferred between initial and final states, one obtains so-called phenomenological representation  15  :",
        "watermark_text": "We report the conclusion on systematic uncertainties in calculation of hadronic variables using QCD Sum Rules ( QSR ) . We consider two forms of uncertainties : theoretical and experimental ones .Theoretical uncertainty is calculated by varying renormalization scale , Borel mass factor and threshold factor used to separate ground state contribution from excited states one . Experimental error results mainly from errors in input estimates for quark masses and decay constants .In addition we took into consideration also dependence of last result on choosing of interpolating current . For each type of uncertainty we estimate its impact independently as well as maximum uncertainty which includes all sources discussed above .Finally we compare our findings with those provided recently in literature . Keywords : Systematics , Uncertainty , Hadronic Parameters , QCD Sum Rules , Quark Masses , Decay Constants .1 Introduction In this project we study systematic uncertainties in calculation of several hadronic variables like masses , decay constants etc . , using QCD Sum Rules 1 . This method enables us to obtain knowledge about properties of low - lying resonances coming from first principles - Quantum Chromodynamics ( QCD ) 2 , without any additional constraints or models .It has been successfully introduced to many various reactions involving light quarks such as : pion form factor 3 , nucleon electromagnetic 4 and axial 5 form factors 6 , semileptonic decays 7 , 8 , heavy - heavy mesons 9 , charmonium 10 and bottomonium 11 systems 12 . The main idea behind QCD Sum Rules is that correlation function formed out of currents associated to specified quantum numbers can be described as dispersion relation over physical spectrum of molecules contributing to it .Then , after applying dual Borel transformation 13 , i . e . transforming both variables q ^ 2 and q ^ 2 simultaneously 14 , where p is momentum flowing through initial state and q is momentum transferred between initial and final states , one obtains so - called phenomenological representation 15 :",
        "rewrite_text": "We present our findings on the systematic uncertainties associated with the calculation of hadronic variables using QCD Sum Rules (QSR). Our analysis focuses on two types of uncertainties: theoretical and experimental. Theoretical uncertainty is estimated by varying the renormalization scale, the Borel mass factor, and the threshold factor used to distinguish between ground state contributions and those from excited states. On the other hand, experimental uncertainty primarily arises from errors in the input values of quark masses and decay constants. Additionally, we also examine how the choice of interpolating current affects our final results. For each type of uncertainty, we independently assess its impact, as well as the maximum uncertainty that encompasses all discussed sources. Ultimately, we compare our results with recent findings from the literature. \n\n**Keywords:** Systematics, Uncertainty, Hadronic Parameters, QCD Sum Rules, Quark Masses, Decay Constants.\n\n**1 Introduction**  \nIn this research, we explore the systematic uncertainties in calculating various hadronic variables such as masses and decay constants using QCD Sum Rules. This approach allows us to gain insight into the properties of low-lying resonances based on first principles—Quantum Chromodynamics (QCD)—without relying on additional constraints or models. The method has been successfully applied to a variety of reactions involving light quarks, including the pion form factor, nucleon electromagnetic and axial form factors, semileptonic decays, and charmonium and bottomonium systems. The core idea behind QCD Sum Rules is that the correlation function formed by currents correlating to specific quantum numbers can be expressed as a dispersion relation over the physical spectrum of corresponding states. By employing a dual Borel transformation—where both q² and p² are transformed simultaneously—we obtain what is referred to as the phenomenological representation.",
        "ori-fast-z-score": -1.110664955031789,
        "water-fast-z-score": 6.236810901332355,
        "rewrite-fast-z-score": 1.172170525067662
    },
    {
        "original_text": "We present the sensitivity analysis of hybrid differential stereoscopy (HDS) in spectral imaging systems, which is based on the combination of two different techniques to obtain 3D information and has been recently proposed as an alternative method to conventional triangulation-based approaches.  We show that HDS can be considered as a generalization of other existing methods such as structured light or time-of-flight cameras by introducing additional degrees of freedom into the system design. In particular, we demonstrate how these parameters affect the performance of the technique when applied to hyperspectral data sets acquired with a pushbroom sensor mounted onto a satellite platform. Finally, we provide some guidelines about optimal values of those parameters depending on the application requirements. © 2014 Optical Society. Published by SPIE under the terms of the CC BY 3.0 license. The ability to acquire accurate three-dimensional (3D) information at high frame rates opens up new possibilities in many fields including remote sensing, medical diagnostics, surveillance, robotics, etc., where fast response times are required  1  . Among all available technologies, stereo vision stands out due to its low cost and simplicity  2  , but it suffers from inherent limitations related to the need of having textured surfaces within the scene  3  .\nIn recent years, several alternatives have emerged to overcome this problem  4  -  6  . One of them consists of using multiple images taken simultaneously from slightly displaced viewpoints  7  . This approach allows one to recover depth maps even if there is no texture in the scene  8  . However, the accuracy achieved depends strongly on the baseline between the camera positions  9  . Another possibility relies on the use of active illumination  10  , although this solution may not always be practical because of safety concerns  11  . A third option involves the use of coded patterns  12  , which require special hardware  13  .",
        "watermark_text": "We present the sensitivity analysis of hybrid differential stereoscopy ( HDS ) in spectral scan systems , which is based on the combination of two different methods to obtain 3D information and has been lately advocated as an additional method to conventional triangulation - based methods . We suggest that HDS can be regarded as a generalization of other existing techniques such as structured light or time - of - flight images by using new degrees of liberty into the system structure .In particular , we prove how these parameters affect the performance of the method when applied to hyperspectral information sets purchased with a pushbroom sensor deployed onto a spacecraft platform . Finally , we provide some guidelines about ideal values of those variables depending on the user requirements .© 2014 Optical Society . Published by SPIE under the terms of the CC BY 3 . 0 license .The able to acquire precise three - dimensional ( 3D ) details at high frame rates opens up new possibilities in different fields including distant detection , medical diagnostics , surveillance , robotics , etc . , where fast response periods are required 1 . Among all available systems , surround vision stands out due to its low cost and simplicity 2 , but it suffers from inherent limitations associated to the necessity of having textured edges within the picture 3 .In recent years , various options have arose to overcome this situation 4 - 6 . One of them consists of using multiple photos taken concurrently from somewhat displaced viewpoints 7 .This method enables one to extract depth mapping even if there is no texture in the image 8 . However , the accuracy achieved depends strongly on the baseline between the photographer positions 9 .Another possibility relies on the using of active light 10 , although this solution might not always be possible because of security concerns 11 . A third possibility requires the using of programmed zones 12 , which require extra architecture 13 .",
        "rewrite_text": "We offer a sensitivity analysis of hybrid differential stereoscopy (HDS) within spectral scanning systems. This approach combines two distinct methods to extract 3D data and has recently been proposed as a complementary technique to traditional triangulation-based methods. We argue that HDS can be viewed as an expansion of existing techniques, such as structured light and time-of-flight imaging, by introducing new degrees of freedom within the system framework. Specifically, we demonstrate how these parameters influence the method's performance when applied to hyperspectral data collected by a pushbroom sensor mounted on a spacecraft. Additionally, we provide recommendations for optimal values of these parameters based on user needs. © 2014 Optical Society. Published by SPIE under the terms of the CC BY 3.0 license. The ability to capture precise three-dimensional (3D) information at high frame rates opens new avenues in various fields, including remote sensing, medical diagnostics, surveillance, and robotics, where quick responses are essential. Among the available options, surround vision stands out for its affordability and simplicity, although it has inherent limitations due to the requirement for textured edges in the images. In recent years, several solutions have emerged to address these challenges. One method involves taking multiple photographs concurrently from slightly different angles, allowing for depth mapping even in the absence of texture; however, its accuracy is heavily reliant on the distance between the camera positions. Another approach utilizes active lighting, although this may not always be feasible due to safety considerations. Lastly, a method involving programmed zones necessitates additional infrastructure.",
        "ori-fast-z-score": -0.6531972647421809,
        "water-fast-z-score": 9.748859854176581,
        "rewrite-fast-z-score": 0.8574929257125441
    },
    {
        "original_text": "We present the first exact solution for the axisymmetric, stationary and force-free magnetic field structure around a rapidly-rotating neutron star with arbitrary inclination angle between its rotation and magnetic axes. The model is based on the assumption that the stellar surface is covered by a thin layer of plasma which corotates with the star at all latitudes. We show how this solution can be used to calculate the X-ray light curves produced by hot spots located at different latitudes above the stellar surface. In particular we demonstrate that the observed phase shifts between the peaks of soft X-rays (0.5-2 keV) and hard X-rays (2-10 keV), as well as their relative amplitudes are reproduced very accurately if one assumes that both emission components originate from two antipodal hot spots located near the magnetic poles. This result suggests that the observed X-ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler effect due to bulk motion of matter within the emitting regions. \n \n Keywords: Force-free fields, Pulsar wind nebulae",
        "watermark_text": "We present the first accurate solution for the axisymmetric , stationary and force - free magnetic field system around a rapidly - spinning neutron galaxy with arbitrary orientation angle between its rotation and magnetic axes . The model is based on the assumption that the stellar surface is enclosed by a thin layer of liquid which corotates with the star at all latitudes .We see how this solution can be used to estimate the X - ray light patterns created by hot centers situated at different latitudes above the stellar surface . In particular we prove that the observed phase change between the peaks of hard X - rays ( 0 . 5 - 2 keV ) and hard X - rays ( 2 - 10 keV ) , as well as their relative amplitudes are reconstructed very correctly if one suppose that both emission parts originate from two antipodal hot areas situated near the magnetic poles .This result suggests that the seen X - ray pulsations may be caused by the rotational modulation of the local emissivity rather than by the Doppler impact owing to bulk movement of matter within the emitting regions . Keywords : Force - free forces , Pulsar wind nebulae",
        "rewrite_text": "We introduce the first precise solution for the axisymmetric, stationary, and force-free magnetic field configuration surrounding a rapidly-spinning neutron star, regardless of the angle between its rotation and magnetic axes. Our model is predicated on the idea that the stellar surface is enveloped by a thin liquid layer that co-rotates with the star at all latitudes. This solution allows us to estimate the X-ray light patterns generated by hot spots located at different latitudes above the stellar surface. Notably, we demonstrate that the observed phase shift between the peaks of hard X-rays (0.5 - 2 keV) and harder X-rays (2 - 10 keV), along with their relative amplitudes, is accurately reproduced if we assume that both emission components arise from two antipodal hot regions near the magnetic poles. This finding implies that the X-ray pulsations we observe may be due to the rotational modulation of local emissivity instead of being a result of Doppler effects from the bulk motion of matter within the emitting areas. Keywords: Force-free forces, Pulsar wind nebulae.",
        "ori-fast-z-score": -1.7457431218879391,
        "water-fast-z-score": 4.58257569495584,
        "rewrite-fast-z-score": -0.6793662204867574
    },
    {
        "original_text": "We present multi-instrument observations of solar coronal plasma flows, which are obtained by combining data from the Extreme Ultraviolet Imager (EUVI) onboard STEREO-A and -B with those from the Helioseismic and Magnetic Imager (HMI), Atmospheric Imaging Assembly (AIA), and Interface Region Imaging Spectrograph (IRIS). The EUV images show that there is an apparent counterclockwise rotation of the bright loop-like structures at the limb between 2011 February 24 and March 1. We find that this rotation can be explained as a result of the differential motion between the two spacecrafts along their orbits around the Sun. By applying a cross-correlation technique to the EUVI 171 Å intensity profiles observed simultaneously by both satellites, we obtain the velocity field across the solar disk for each time step during the period under study. In addition, we use HMI magnetograms to calculate the magnetic flux density distribution over the solar surface. Our results reveal that the plasma flow patterns are closely related to the photospheric magnetic fields.",
        "watermark_text": "We present multi - instrument observations of solar coronal plasma flows , which are derived by combining information from the Extreme Ultraviolet Imager ( EUVI ) onboard STEREO - A and - B with those from the Helioseismic and Magnetic Imager ( HMI ) , Atmospheric Imaging Assembly ( AIA ) , and Interface Region Imaging Spectrograph ( IRIS ) . The EUV photos suggest that there is an apparent counterclockwise rotation of the faint loop - like structures at the limb between 2011 February 24 and March 1 .We see that this rotation can be described as a effect of the differential motion between the two spacecrafts along their orbits around the Sun . By applying a cross - correlation method to the EUVI 171 Å intensity profiles observed concurrently by both spacecraft , we obtain the velocity field across the planetary disk for each time step during the period under research .In addition , we utilize HMI magnetograms to estimate the magnetic flux concentration distribution over the sun surface . Our results show that the plasma circulation trends are tightly related to the photospheric magnetic waves .",
        "rewrite_text": "We present multi-instrument observations of solar coronal plasma flows, combining data from the Extreme Ultraviolet Imager (EUVI) onboard STEREO-A and STEREO-B with information from the Helioseismic and Magnetic Imager (HMI), the Atmospheric Imaging Assembly (AIA), and the Interface Region Imaging Spectrograph (IRIS). The EUV images indicate a noticeable counterclockwise rotation of faint loop-like structures at the solar limb from February 24 to March 1, 2011. This rotation appears to be a result of the differential motion between the two spacecraft as they orbit the Sun. By applying a cross-correlation method to the EUVI 171 Å intensity profiles collected simultaneously by both spacecraft, we reconstruct the velocity field across the solar disk for each time step during the study period. Additionally, we leverage HMI magnetograms to analyze the distribution of magnetic flux concentration on the solar surface. Our findings reveal a strong connection between plasma circulation trends and the oscillations of the photospheric magnetic fields.",
        "ori-fast-z-score": -0.12803687993289598,
        "water-fast-z-score": 5.249512077248736,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "We present an algorithm for creating a hierarchical grid spatial index using images as the basis for its construction.  The algorithm is based on the observation that many real-world datasets are naturally represented by images, and can be used in conjunction with existing techniques such as R-tree or Quadtree to improve performance.   We show how our technique performs against these other methods through experiments conducted over synthetic data sets generated according to different distributions (uniform, normal, exponential) and sizes ranging between 1K and 100M points.   Our results demonstrate significant improvements in query response times when compared to traditional approaches. In this work we propose a new approach for building a spatial index which uses image processing algorithms to extract information about the dataset being indexed. This information is then used to build a hierarchy of grids whose leaf nodes contain pointers to individual objects within the dataset. These grids provide efficient access to all objects contained therein while also allowing fast queries across multiple grids at once.",
        "watermark_text": "We present an algorithm for producing a hierarchical grid spatial indicator employing images as the foundation for its design . The algorithm is based on the observation that several real - time datasets are naturally represented by pictures , and can be used in partnership with existing techniques such as R - tree or Quadtree to achieve performance .We see how our technique performs against these other methods through experiments conducted over synthetic information sets generated according to different distributions ( uniform , normal , exponential ) and dimensions ranging between 1K and 100M points . Our results show considerable improvements in query reply times when compared to conventional approaches .In this project we propose a new approach for building a spatial indicator which uses image processing algorithms to extract information about the dataset being indexed . This knowledge is then utilized to build a hierarchy of grids whose leaf nodes contain pointers to individual objects within the dataset .These grids allow faster entry to all items contained therein while also enabling rapid queries across multiple grids at once .",
        "rewrite_text": "We introduce an algorithm designed to create a hierarchical grid spatial indicator based on images. This algorithm leverages the fact that many real-time datasets can be effectively represented by images, allowing it to work in conjunction with established techniques like R-trees and Quadtrees to enhance performance. Through experiments on synthetic datasets with varying distributions (uniform, normal, exponential) and sizes ranging from 1K to 100M points, we evaluate the effectiveness of our approach compared to traditional methods. Our findings demonstrate significant improvements in query response times relative to conventional techniques. In this project, we propose a novel method for constructing a spatial indicator that employs image processing algorithms to derive insights from the indexed dataset. This information is then used to develop a hierarchy of grids, with leaf nodes that point to individual objects in the dataset. These grids facilitate quicker access to all contained items and enable rapid queries across multiple grids simultaneously.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 6.9829724875517565,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "The aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...",
        "watermark_text": "The goal of this talk is to provide some latest findings on the relation between integrability in quantum field theory , statistical mechanics or computational physics and the existence of unique geometric objects ( complex algebraic shapes ) which are identified with these models . We will explore how such geometries can be used for solving exactly certain physical problems by using techniques come from algebraic topology like Riemann surfaces , theta functions etc . . We will also explain why it appears that several interesting integrable models have an underlying composition of a Riemann surface .Finally we will provide descriptions where this link has been made explicit . The talk will include several current conclusions acquired previously by the writer together with his collaborators .This research was partially backed by the DFG under grant SFB / TR9 . Integrable systems play an important role in different areas of math as well as conceptual science .In particular they appear naturally when examining solvable lattice models in statistical mechanics or quantum field theories . It turns out that there exists a profound connection between integrability and the presence of unique geometric objects called algebraic curves .These curves provide potent tools for solving exactly certain physical problems via tools from algebraic topology like Riemann Surfaces , Theta Functions , . . .",
        "rewrite_text": "The purpose of this presentation is to discuss recent discoveries regarding the relationship between integrability in quantum field theory, statistical mechanics, and computational physics, along with the unique geometric entities (complex algebraic shapes) associated with these models. We will investigate how these geometries can be employed to precisely solve specific physical challenges using techniques from algebraic topology, such as Riemann surfaces and theta functions. Additionally, we will clarify why numerous intriguing integrable models appear to be fundamentally linked to the structure of a Riemann surface. The talk will also detail instances where this connection has been explicitly demonstrated. It will feature several recent conclusions drawn by the presenter and their collaborators. This research received partial funding from the DFG under grant SFB/TR9. Integrable systems are significant in various mathematical domains and conceptual sciences, particularly emerging in the context of solvable lattice models within statistical mechanics and quantum field theories. Notably, there exists a deep connection between integrability and the existence of unique geometric entities known as algebraic curves, which serve as powerful tools for accurately addressing certain physical problems through concepts from algebraic topology, including Riemann surfaces and theta functions.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 6.543303050815759,
        "rewrite-fast-z-score": -0.5940885257860046
    },
    {
        "original_text": "We study the effect of Rashba spin-orbit interaction on the spin Hall conductivity (SHC) for an interacting two-dimensional electron system with parabolic dispersion and Zeeman splitting in presence of a uniform external magnetic field applied normal to the plane of motion. We show that SHC is independent of temperature, chemical potential and strength of disorder provided the Fermi energy lies within the Zeeman gap. The results are obtained by using the Kubo formula combined with the self-consistent Born approximation. It has been shown recently that the spin current can be generated without any net charge flow when electrons move through a nonmagnetic material under the influence of spin-orbit coupling  1  . This phenomenon known as spin Hall effect was first predicted theoretically  2  , and later observed experimentally  3  .\nThe origin of this effect is due to the fact that the spin-orbit interaction causes a transverse force which deflects the trajectories of moving particles leading to a finite spin polarization at the edges  4  . In recent years there have been several theoretical studies devoted to understand various aspects of spin Hall effect  5  -  8  . However most of these works were done either in absence or weak magnetic fields where the Landau levels do not play significant role  9  . On the other hand it is well known that the Landau level quantization plays important role in determining many physical properties such as magnetoresistance  10  , optical absorption  11  etc., especially near the quantum limit  12  . Therefore it would be interesting to investigate how the Landau levels affect the spin Hall effect.",
        "watermark_text": "We explore the impact of Rashba spin - orbit interaction on the spin Hall conductivity ( SHC ) for an interacting two - dimensional electron structure with parabolic dispersion and Zeeman splitting in presence of a consistent external magnetic current applied normal to the plane of movement . We see that SHC is independent of temperature , chemical potential and strength of disorder provided the Fermi energy rests within the Zeeman gap .The results are derived by using the Kubo formula coupled with the self - stable Born algorithm . It has been shown lately that the spin current can be formed without any gross charge flow when nuclei move through a nonmagnetic material under the effects of spin - orbit bonding 1 .This phenomenon known as spin Hall phenomenon was first expected theoretically 2 , and later observed experimentally 3 . The origin of this effect is due to the fact that the spin - orbit interaction produces a transverse force which deflects the trajectories of moving particles leading to a finite spin polarization at the edges 4 .In past decades there have been numerous conceptual research devoted to study various parts of spin Hall phenomenon 5 - 8 . However most of these works were done either in absence or strong magnetic fields where the Landau concentrations do not play substantial role 9 .On the other hand it is well established that the Landau level quantization takes key importance in establishing much mechanical parameters such as magnetoresistance 10 , optical reflection 11 etc . , particularly near the quantum limit 12 . Therefore it would be interesting to examine how the Landau concentrations influenced the spin Hall phenomenon .",
        "rewrite_text": "We investigate how the Rashba spin-orbit interaction affects the spin Hall conductivity (SHC) in an interacting two-dimensional electron system with parabolic dispersion and Zeeman splitting, while a steady external magnetic current is applied perpendicular to the movement plane. Our findings indicate that the SHC remains unaffected by temperature, chemical potential, and disorder strength, as long as the Fermi energy is situated within the Zeeman gap. These results are obtained using the Kubo formula in conjunction with the self-consistent Born approximation. Recent studies have demonstrated that spin currents can emerge in the absence of significant charge flow when nuclei traverse a nonmagnetic material under the influence of spin-orbit coupling. This phenomenon, referred to as the spin Hall effect, was initially predicted theoretically and later observed experimentally. The underlying mechanism of this effect arises from the spin-orbit interaction, which generates a transverse force that alters the trajectories of moving particles, resulting in a finite spin polarization at the edges. Over the past few decades, extensive theoretical research has been devoted to various aspects of the spin Hall effect. However, most of these studies were conducted in the absence of strong magnetic fields or did not account for the influence of Landau levels, which can play a crucial role in determining several physical parameters, including magnetoresistance and optical reflection, particularly near the quantum limit. Therefore, it is worth exploring how the presence of Landau levels influences the spin Hall effect.",
        "ori-fast-z-score": -0.2683281572999747,
        "water-fast-z-score": 7.723027987151322,
        "rewrite-fast-z-score": 0.641688947919748
    },
    {
        "original_text": "We study how the properties of active galactic nuclei (AGNs) affect their radio morphologies, and in particular whether they can explain the observed difference between Fanaroff-Riley type I (FRI) and II (FRII). We use high-resolution hydrodynamical simulations to follow the growth of supermassive black holes (SMBHs), which are fed by cold gas accretion at rates that depend on the SMBH mass and its environment. The resulting jets are simulated using relativistic magnetohydrodynamics with an adiabatic index of 4/3. Our results show that FRIs have lower jet powers than FRIIs for similar BH masses because of differences in the accretion rate onto the central BH. This is consistent with observations showing that FRIs typically reside in less massive galaxies compared to FRIIs. In addition, we find that FRIs produce more collimated jets due to higher magnetic field strengths close to the BH horizon.",
        "watermark_text": "We research how the properties of active galactic nuclei ( AGNs ) impact their television morphologies , and in particular whether they can describe the seen distinction between Fanaroff - Riley type I ( FRI ) and II ( FRII ) . We use large - resolution hydrodynamical simulations to follow the formation of supermassive black holes ( SMBHs ) , which are fed by cold gas accretion at levels that rely on the SMBH weight and its climate .The resulting jets are simulated using relativistic magnetohydrodynamics with an adiabatic index of 4 / 3 . Our results show that FRIs have smaller jet powers than FRIIs for related BH masses because of differences in the accretion rate onto the main BH .This is consistent with observations indicating that FRIs typically exist in fewer large galaxies compared to FRIIs . In addition , we find that FRIs produce more collimated jets resulting to higher magnetic force abilities next to the BH horizon .",
        "rewrite_text": "We investigate how the characteristics of active galactic nuclei (AGNs) influence their morphological appearances, specifically examining the differences between Fanaroff-Riley type I (FRI) and type II (FRII) galaxies. To do this, we utilize high-resolution hydrodynamical simulations to trace the development of supermassive black holes (SMBHs), which are fueled by cold gas accretion dependent on the mass of the SMBH and its surrounding environment. The jets produced in our simulations are modeled using relativistic magnetohydrodynamics with an adiabatic index of 4/3. Our findings indicate that FRIs exhibit lower jet power compared to FRIIs when accounting for similar black hole masses, primarily due to variations in the accretion rates onto the central black hole. This aligns with observational data that shows FRIs are commonly found in less massive galaxies compared to FRIIs. Furthermore, our results reveal that FRIs generate more collimated jets, leading to enhanced magnetic field strengths in proximity to the black hole horizon.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "The geometric frustration in the spin-1/2 triangular lattice is studied by means of neutron powder diffraction, magnetization measurements, specific heat data, and first-principles calculations for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both compounds are antiferromagnetic insulators with Néel temperatures TN = 5 K (Sr3NiRhO6) and T N = 7 K (Sr3NiPtO6). In addition to the expected collinear antiferromagnetism, we find evidence for noncollinear ordering in Sr3NiRhO6: First, there is an additional weak reflection at Q = 1.5 Å -1 , which can be explained as superlattice peak due to a small rhombohedral distortion; secondly, the temperature dependence of the ordered moment shows a kink around 2 K indicating a change of the order parameter below this temperature.",
        "watermark_text": "The geometric problems in the spin - 1 / 2 triangular lattice is studied by means of neutron dust diffraction , magnetization calculations , detailed heat observations , and first - principles measurements for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both salts are antiferromagnetic insulators with Néel pressures TN = 5 K ( Sr3NiRhO6 ) and T N = 7 K ( Sr3NiPtO6 ) .In addition to the expected collinear antiferromagnetism , we find proof for noncollinear ordering in Sr3NiRhO6 : First , there is an additional strong absorption at Q = 1 . 5 Å - 1 , which can be understood as superlattice peak thanks to a small rhombohedral distortion ; secondly , the temperature dependence of the ordered moment displays a kink around 2 K indicating a change of the order parameter below this heat .",
        "rewrite_text": "The geometric challenges associated with the spin-1/2 triangular lattice have been investigated using neutron dust diffraction, magnetization calculations, detailed thermal measurements, and first-principles assessments for two new compounds, Sr3NiRhO6 and Sr3NiPtO6. The findings reveal that both compounds behave as antiferromagnetic insulators, with Néel temperatures of TN = 5 K for Sr3NiRhO6 and TN = 7 K for Sr3NiPtO6. In addition to the anticipated collinear antiferromagnetism, evidence of noncollinear ordering has been observed in Sr3NiRhO6. Notably, there is a significant absorption peak at Q = 1.5 Å -1, interpretable as a superlattice peak due to a slight rhombohedral distortion. Furthermore, the temperature dependence of the ordered moment exhibits a kink around 2 K, suggesting a alteration in the order parameter below this temperature.\n",
        "ori-fast-z-score": 0.5773502691896258,
        "water-fast-z-score": 5.10527470242631,
        "rewrite-fast-z-score": 0.8846517369293828
    },
    {
        "original_text": "We present new observations of the nuclear region in the nearby radio galaxy NGC315, made using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy that is surrounded by diffuse emission extending to about 1 arcmin (3 kpc), which we identify as thermal gas heated by the central AGN. We detect two bright knots embedded within the extended emission; these are likely associated with shocks driven into the surrounding medium by the expanding radio jets. Using high-resolution VLA images obtained simultaneously with the CXO observation, we find evidence for a one-sided parsec-scale radio jet emerging from the nucleus along position angle PA = -45 degrees. This jet has been previously detected on larger scales out to several kiloparsecs. In addition, there appears to be another fainter component of the radio jet located further south-west than the main knot.",
        "watermark_text": "We present new images of the atomic region in the nearby radio star NGC315 , made using the Chandra X - ray Observatory ( CXO ) and the Very Large Array ( VLA ) . The CXO data reveal an unresolved point origin at the center of this elliptical galaxy that is flanked by diffuse emission stretching to about 1 arcmin ( 3 kpc ) , which we identify as heat gas cooled by the main AGN .We detect two faint knots embedded within the extended emission ; these are likely correlated with shocks driven into the nearby medium by the increasing radio jets . Using long - resolution VLA images obtained simultaneously with the CXO observation , we find proof for a one - sided parsec - scale radio jet developing from the nucleus along position angle PA = - 45 degrees .This jet has been previously observed on larger scales out to several kiloparsecs . In addition , there seems to be another fainter component of the radio jet located further west - west than the main knot .",
        "rewrite_text": "We present new images of the atomic region in the nearby radio star NGC 315, captured using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy, accompanied by diffuse emission that extends roughly 1 arcminute (3 kpc). We attribute this emission to heated gas cooled by the primary active galactic nucleus (AGN). Within the extended emission, we identify two faint knots, likely associated with shocks induced by the expanding radio jets. Concurrent long-resolution VLA images taken during the CXO observation show evidence of a one-sided parsec-scale radio jet emerging from the nucleus at a position angle of PA = -45 degrees. This jet has been previously observed on larger scales extending out to several kiloparsecs. Additionally, there appears to be another fainter component of the radio jet located further west of the main knot.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 4.391092135317257,
        "rewrite-fast-z-score": 0.8307471607356973
    },
    {
        "original_text": "We present near-infrared (NIR) spectroscopy obtained with Subaru/HDS on day +16 after explosion for the peculiar type Ib supernova SN2006jc, which shows prominent dust formation in its dense shell. The NIR spectrum is dominated by strong P-Cygni profiles of H I Balmer lines and Fe II multiplets at 4233-4245 Å . We find that these features are well reproduced by our non-LTE model calculations assuming an electron density ne = 1 x 10^9 cm-3 , temperature Te = 5500 K , and mass-loss rate oḟ M = 2 x 10^-6 M_sun/yr. In addition to the above mentioned features, we detect weak but significant He II 4686 emission line feature in the red wing of the Hα profile. This suggests that there may be some contribution from helium recombination radiation to the observed fluxes of hydrogenic lines.",
        "watermark_text": "We use near - infrared ( NIR ) spectroscopy acquired with Subaru / HDS on week + 16 after explosion for the unusual type Ib supernova SN2006jc , which shows significant dust form in its dense shell . The NIR spectrum is dominated by strong P - Cygni profiles of H I Balmer bands and Fe II multiplets at 4233 - 4245 Å .We see that these characteristics are better illustrated by our non - LTE model calculations assuming an electron concentration ne = 1 x 10 ^ 9 centimeters - 3 , temperature Te = 5500 K , and mass - loss rate [UNK] M = 2 x 10 ^ - 6 M _ sun / yr . In addition to the above mentioned properties , we perceive slight but significant He II 4686 absorption line feature in the red wing of the Hα profile .This implies that there may be some influence from helium recombination emission to the seen fluxes of hydrogenic lines .",
        "rewrite_text": "We utilized near-infrared (NIR) spectroscopy obtained from the Subaru/HDS, captured 16 weeks post-explosion, for the atypical type Ib supernova SN2006jc, which exhibits considerable dust formation within its dense shell. The NIR spectrum prominently features strong P-Cygni profiles of the H I Balmer bands and Fe II multiplets between 4233 and 4245 Å. Our non-LTE model calculations, which assume an electron concentration of ne = 1 x 10^9 cm^-3, a temperature of Te = 5500 K, and a mass-loss rate of [UNK] M = 2 x 10^-6 M_sun/yr, illustrate these characteristics more clearly. Additionally, we observe a subtle yet notable He II 4686 absorption line in the red wing of the Hα profile, suggesting that helium recombination emission may influence the observed fluxes of the hydrogenic lines.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 4.08248290463863,
        "rewrite-fast-z-score": 0.674199862463242
    },
    {
        "original_text": "We present an algorithm for finding galaxy clusters using only photometric redshifts and no spectroscopic information, based on identifying overdensities of galaxies with similar colours. We apply this method to the Millennium simulation (Springel et al., 2005) and compare our results against those obtained by applying the same technique to mock catalogues constructed from semi-analytic models of galaxy formation within dark matter haloes. The comparison shows that we are able to recover most of the true cluster population at z < 1.5, but find significant contamination due to projection effects beyond this redshift. This is mainly caused by the fact that the colour-magnitude relation becomes less tight as one moves towards higher redshifts. Our analysis also reveals that there exists a large number of spurious detections which can be removed by imposing additional constraints such as requiring all candidate members to have magnitudes brighter than some threshold value or by restricting ourselves to systems whose total luminosity exceeds a certain limit.",
        "watermark_text": "We present an algorithm for finding galaxy galaxies using only photometric redshifts and no spectroscopic information , relying on establishing overdensities of clusters with similar colours . We use this algorithm to the Millennium scenario ( Springel et al . , 2005 ) and compare our findings against those achieved by using the same technique to mock catalogues generated from semi - analytic models of galaxy formation within dark matter haloes .The comparison shows that we are able to regain most of the true cluster population at z < 1 . 5 , but discover considerable contamination owing to projection influences beyond this redshift . This is mainly caused by the fact that the colour - magnitude correspondence gets less tight as one moves towards higher redshifts .Our study also reveals that there exists a large number of spurious detections which can be removed by imposing extra constraints such as requiring all candidate members to have magnitudes brighter than some threshold value or by restricting ourselves to systems whose total luminosity exceeds a certain limitation .",
        "rewrite_text": "We introduce an algorithm designed to identify galaxy clusters using only photometric redshifts, without relying on spectroscopic data. This method focuses on detecting overdensities of clusters that share similar colors. We apply this algorithm to the Millennium simulation (Springel et al., 2005) and compare our results with those obtained from mock catalogs generated through semi-analytic models of galaxy formation within dark matter halos. Our comparison indicates that we successfully recover a significant portion of the true cluster population at redshifts less than 1.5. However, we also encounter considerable contamination due to projection effects at higher redshifts. This is primarily attributed to the weakening of the color-magnitude relationship as redshift increases. Furthermore, our analysis uncovers a substantial number of false detections, which can be mitigated by applying additional constraints, such as requiring all candidate members to have magnitudes above a certain threshold or limiting our focus to systems with total luminosities above a specified level.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 6.0448772146786025,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We present new spectroscopic observations for nine cataclysmic variable stars (CVs) obtained with the HIRES spectrograph on Keck I telescope in Hawaii, and compare them to previous results. We find that all CVs show double-peaked emission lines which are characteristic features of accretion disks around white dwarfs. The line profiles change dramatically during outburst phases when mass transfer rates increase by several orders of magnitude compared to quiescent states. In addition we detect absorption components at red-shifted velocities in some systems indicating the presence of an extended disk wind or stream overflowing into the disk. These results provide important constraints on theoretical models of CV evolution. \n \n Keywords: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables \n \n \n \n 1 Introduction \n \n Cataclysmic variables (CVs), also known as dwarf novae, are close binary systems consisting of a white dwarf primary star and a late-type secondary star filling its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it forms an accretion disk surrounding the compact object. This process leads to periodic outbursts caused by thermal instabilities in the accretion disk resulting in dramatic changes in luminosity over time scales ranging from hours up to years  1  . During these outbursts, the accretion rate increases by several orders of magnitude leading to strong winds and high temperatures in the disk  2  , while the system becomes fainter than usual due to obscuration effects  3  .\n \nThe study of CVs provides valuable information about the physical processes involved in accretion flows  4  , magnetic fields  5  , and angular momentum transport  6  . Furthermore, they can be used as distance indicators  7, 8  and probes of galactic structure  9  . \n \n 2 Observations & Data Reduction \n \n Our sample consists of 9 CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES)  10  mounted on the 10 m Keck I telescope located on Mauna Kea",
        "watermark_text": "We introduce novel spectroscopic observations for nine cataclysmic variable stars ( CVs ) obtained with the HIRES spectrograph on Keck I telescope in Hawaii , and compare them to previous findings . We see that all CVs show dual - peaked emission lines which are peculiar characteristics of accretion disks around white dwarfs .The line profiles change dramatically during outburst phases when mass transfer rates increase by many orders of magnitude compared to quiescent states . In addition we perceive absorption elements at red - shifted velocities in some systems suggesting the presence of an extended disk dust or stream overflowing into the disk .These conclusions provide important restrictions on theoretical theories of CV evolution . Keywords : Accretion Disk , Double - Peaked Emission Lines , White Dwarf , Cataclysmic Variables 1 Introduction Cataclysmic variables ( CVs ) , sometimes called as dwarf novae , are open binary complexes consisting of a black dwarf secondary star and a early - class secondary star occupying its Roche lobe .Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf where it creates an accretion disk surrounding the compact body . This process results to periodic outbursts caused by temperature instabilities in the accretion disk resulting in spectacular changes in luminosity over time ranges ranging from hours up to days 1 .During these outbursts , the accretion rate grows by many orders of magnitude resulting to powerful storms and rising heat in the disk 2 , while the system gets fainter than usual thanks to obscuration effects 3 . The investigation of CVs provides valuable info about the physical processes responsible in accretion movements 4 , magnetic waves 5 , and spatial velocity transport 6 .Furthermore , they can be used as distance indicators 7 , 8 and probes of galactic structure 9 . 2 Observations & Data Reduction Our specimen consists of 9 CVs seen between 2004 and 2007 utilizing the High Resolution Echelle Spectrometer ( HIRES ) 10 installed on the 10 m Keck I telescope located on Mauna Kea",
        "rewrite_text": "We present new spectroscopic observations of nine cataclysmic variable stars (CVs) obtained using the HIRES spectrograph at the Keck I telescope in Hawaii, and we compare these results with previous studies. Our findings reveal that all the CVs exhibit dual-peaked emission lines, which are distinctive features of accretion disks around white dwarfs. The line profiles undergo significant changes during outburst phases, where mass transfer rates increase dramatically relative to quiescent states. Moreover, we observe red-shifted absorption features in some systems, indicating the presence of an extended dust disk or streams overflowing into the disk. These observations impose critical constraints on theoretical models of CV evolution. \n\nKeywords: Accretion Disk, Double-Peaked Emission Lines, White Dwarf, Cataclysmic Variables \n\n1 Introduction  \nCataclysmic variables (CVs), often referred to as dwarf novae, are binary systems consisting of a white dwarf and a secondary star that fills its Roche lobe. Mass is transferred through the inner Lagrangian point L1 onto the surface of the white dwarf, forming an accretion disk around the compact object. This process leads to periodic outbursts driven by temperature instabilities in the accretion disk, resulting in significant fluctuations in luminosity, occurring over timescales of hours to days. During these outbursts, the accretion rate increases exponentially, inducing energetic storms and heightened temperatures within the disk, while the system dims due to obscuration effects. The study of CVs offers valuable insights into the physical processes governing accretion flows, magnetic waves, and spatial velocity transport. Additionally, they serve as distance indicators and probes of galactic structure.\n\n2 Observations & Data Reduction  \nOur sample comprises nine CVs observed between 2004 and 2007 using the High Resolution Echelle Spectrometer (HIRES) installed on the 10 m Keck I telescope at Mauna Kea.",
        "ori-fast-z-score": -1.0215078369104984,
        "water-fast-z-score": 8.057794831959724,
        "rewrite-fast-z-score": 1.7960530202677492
    },
    {
        "original_text": "We present an approach for self-organization in networks based on multi-agent systems (MAS). The proposed method is applied to two different networks: one with mobile nodes and another with static ones, both using IEEE 802.11b as their communication protocol. In this work we use agents that are able to move between neighboring nodes, which allows them to collect information about the state of each node. This information can be used by other agents to make decisions such as: moving to new positions or changing the transmission power level. We have implemented our proposal in NS-2 simulator and compared it against three well-known algorithms: OLSR, AODV and DSR. Our results show that MAS outperforms these protocols in terms of: packet delivery ratio, end-to-end delay and energy consumption. Keywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "watermark_text": "We present an approach for self - organization in networks based on multi - agent systems ( MAS ) . The proposed approach is applied to two different organizations : one with mobile connections and another with static ones , both using IEEE 802 . 11b as their networking protocol .In this study we using agents that are able to move between neighboring nodes , which allows them to collect data about the state of each node . This knowledge can be used by other agents to make choices such as : moving to new positions or altering the transmission power level .We have incorporated our proposal in NS - 2 simulator and compared it against three well - famous method : OLSR , AODV and DSR . Our results show that MAS outperforms these protocols in terms of : message delivery ratio , end - to - end delay and energy consumption .Keywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "rewrite_text": "We introduce a novel method for self-organization in networks utilizing multi-agent systems (MAS). This method is applied to two types of organizations: one with mobile connections and another with static ones, both employing the IEEE 802.11b networking protocol. In our study, we utilize agents capable of moving between neighboring nodes, which enables them to gather data regarding the state of each node. This information can then be used by other agents to make decisions, such as relocating to different positions or adjusting transmission power levels. Our proposal has been integrated into the NS-2 simulator and evaluated against three well-known protocols: OLSR, AODV, and DSR. The results indicate that our MAS approach outperforms these protocols in terms of message delivery ratio, end-to-end delay, and energy consumption. \n\nKeywords: Multi-Agent Systems, Self-Organizing Networks, Mobile Agents, Communication Protocols, Energy Consumption, Packet Delivery Ratio",
        "ori-fast-z-score": 1.6378460497066512,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 2.3937749957251055
    },
    {
        "original_text": "We present the first 3D kinematic analysis of an isolated supergranule in the photosphere using high-resolution observations obtained with Hinode/SOT and SDO/HMI instruments. The results show that the observed supergranule is characterized by a strong upflow at its center, surrounded by weaker downflows. We find that the horizontal flow pattern consists of two counter-rotating cells which are connected to each other through a narrow channel along their common boundary. This structure resembles the magnetic topology of a bipolar sunspot pair. In addition we observe a small-scale vortex-like feature located on one side of the central upflow region. Our study shows that the observed supergranulation pattern can be explained as a result of convective motions driven by the solar differential rotation. Keywords: Solar activity, Sunspots, Photometry, Magnetism, Granulation, Convection, Dynamo theory, Magnetic fields",
        "watermark_text": "We present the first 3D kinematic analysis of an exposed supergranule in the photosphere using high - resolution measurements obtained with Hinode / SOT and SDO / HMI instruments . The results show that the known supergranule is characterized by a powerful upflow at its core , flanked by softer downflows .We see that the horizontal flow pattern consists of two counter - spinning chambers which are connected to each other through a thin channel along their common boundary . This structure follows the magnetic topology of a bipolar sunspot couple .In addition we study a small - scale vortex - like feature centered on one end of the main upflow center . Our study shows that the seen supergranulation rhythm can be described as a outcome of convective movements driven by the solar differential rotation .Keywords : Solar influence , Sunspots , Photometry , Magnetism , Granulation , Convection , Dynamo theory , Magnetic fields",
        "rewrite_text": "We introduce the first three-dimensional kinematic analysis of an exposed supergranule in the solar photosphere, utilizing high-resolution measurements from the Hinode/SOT and SDO/HMI instruments. Our findings reveal that the identified supergranule features a strong upflow at its center, surrounded by gentler downflows. The horizontal flow pattern exhibits two counter-rotating chambers linked by a narrow channel along their shared boundary, a structure that aligns with the magnetic topology of a pair of bipolar sunspots. Additionally, we investigate a small-scale vortex-like feature located at one end of the primary upflow region. Our analysis indicates that the observed rhythm of supergranulation can be attributed to convective movements driven by the differential rotation of the Sun. Keywords: Solar influence, Sunspots, Photometry, Magnetism, Granulation, Convection, Dynamo theory, Magnetic fields.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 5.858500994137074,
        "rewrite-fast-z-score": 1.8708286933869707
    },
    {
        "original_text": "We present an analytical model for the growth, radial drift and fragmentation of dust grains in protoplanetary disks that evolve under the combined effects of viscous accretion onto the central star and photoevaporation by external radiation fields. We show how these processes affect the evolution of grain size distributions as well as their spatial distribution within the disk. In particular we find that: (i) The maximum grain sizes are limited to values between 1 mm and 10 cm depending on the strength of the stellar UV field. (ii) Grains grow faster at larger distances from the star due to lower gas densities and higher temperatures. (iii) Fragmentation is more efficient closer to the star where the local pressure maxima lead to enhanced collisional velocities.  These results have important implications for planet formation scenarios since they suggest that planetesimals can form only close to the star while large bodies such as asteroids or comets may be able to form farther out in the disk.",
        "watermark_text": "We present an analytical theory for the development , radial drift and fragmentation of dust grains in protoplanetary disks that develop under the combined influences of viscous accretion onto the main star and photoevaporation by external emission fields . We see how these mechanisms affect the evolution of grain length distributions as well as their temporal distribution within the disk .In particular we find that : ( i ) The maximum grain sizes are limited to values between 1 mm and 10 mm depending on the strength of the stellar UV field . ( ii ) Grains grow better at larger distances from the star due to higher gas densities and larger temperatures .( iii ) Fragmentation is more efficient closer to the star where the local pressure maxima lead to greater collisional velocities . These conclusions have important implications for planet development schemes since they show that planetesimals can form only close to the star while huge bodies such as asteroids or comets might be possible to form farther out in the disk .",
        "rewrite_text": "We propose an analytical framework for understanding the formation, radial movement, and fragmentation of dust grains in protoplanetary disks, which are influenced by both viscous accretion onto the central star and photoevaporation from external radiation. Our analysis reveals how these factors shape the evolution of grain size distributions and their temporal behavior within the disk. Specifically, we identify three key findings: (i) the maximum sizes of grains are constrained to a range of 1 mm to 10 mm, contingent on the intensity of the stellar UV radiation; (ii) grains tend to grow more effectively at greater distances from the star due to higher gas densities and temperatures; and (iii) fragmentation occurs more readily near the star, where local pressure peaks lead to increased collision velocities. These insights have significant implications for planetary formation theories, suggesting that planetesimals are likely to form in proximity to the star, while larger objects such as asteroids and comets may form at more distant locations within the disk.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 1.1322770341445956
    },
    {
        "original_text": "The hypercube model is used to describe the evolution of life in terms of its complexity, speed, and stability.  The model shows that there are limits imposed by protein stability on how complex an organism can be as well as how fast it evolves molecularly.   It also suggests that these limits may have been reached during the Cambrian explosion about 540 million years ago when most animal phyla appeared simultaneously. This article describes the hypercube model and presents some examples of how it has been applied to understand evolutionary processes at different levels of organization ranging from genes to ecosystems. In this article we present a new approach for understanding the evolution of life based on the concept of the hypercube (1). We argue that the evolution of life can be described in three dimensions: complexity, speed, and stabilization. These three dimensions represent key aspects of biological systems that evolve over time. For example, organisms become more complex through the addition of new components such as organs or tissues; they evolve faster if their genetic variation increases; and they become more stable if mutations do not cause them to die prematurely. Figure 1 illustrates our view of the evolution of life using the hypercube model. Each vertex represents one possible state of living matter with respect to each dimension. As shown in Fig. 1A , the number of vertices along any given axis depends on the level of resolution chosen. At higher resolutions, the number of states increases exponentially. For instance, if we consider only two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total number of possible combinations would be four (2 x 2 x 2 = 8), which corresponds to eight types of living matter. However, if we increase the resolution so that we now include four states per dimension—very simple versus simple versus complex versus very complex, very slow versus slow versus fast versus very fast,...",
        "watermark_text": "The hypercube concept is utilized to explain the evolution of life in terms of its complexity , speed , and stability . The model shows that there are restrictions imposed by molecular stability on how difficult an organism can be as well as how fast it evolves molecularly .It additionally indicates that these limits might have been achieved during the Cambrian explosion about 540 million months previously when most animal phyla appeared simultaneously . This paragraph explains the hypercube model and provides some examples of how it has been used to explain evolutionary processes at different levels of organization ranging from genes to environments .In this article we present a new approach for studying the evolution of life based on the idea of the hypercube ( 1 ) . We argue that the evolution of life can be described in three dimensions : complexity , speed , and stabilization .These three dimensions represent crucial factors of biological systems that develop over time . For instance , animals get more sophisticated through the adding of new components such as bodies or tissues ; they develop faster if their genetic variation rises ; and they become more stable if mutations do not cause them to death prematurely .Figure 1 illustrates our view of the evolution of life using the hypercube view . Each vertex depicts one possible state of living life with regard to each dimension .As seen in Fig . 1A , the number of vertices along any certain axis determines on the level of resolution picked .At higher resolutions , the quantity of states varies exponentially . For instance , if we consider only two states per dimension — simple versus compound , slow versus fast , unstable versus stable — the total number of possible combinations may be four ( 2 x 2 x 2 = 8 ) , which equals to eight types of living matter .However , if we increase the resolution so that we now include four states per dimension — very simple versus easy versus compound versus very difficult , very slow versus slow versus fast versus very quickly , . . .",
        "rewrite_text": "The hypercube framework is used to illustrate the evolution of life in terms of its complexity, speed, and stability. This model suggests that molecular stability imposes limits on the complexity of organisms and the speed at which they evolve at a molecular level. Furthermore, it posits that these limitations may have been reached during the Cambrian explosion, approximately 540 million years ago, when a wide variety of animal phyla emerged simultaneously. This discussion introduces the hypercube model and demonstrates its application in explaining evolutionary processes across various organizational levels, from genes to ecosystems. In this article, we propose a novel method for examining life's evolution through the lens of the hypercube. We contend that evolution can be characterized by three dimensions: complexity, speed, and stability. These dimensions are vital to understanding biological systems as they evolve over time. For example, animals become more complex by incorporating additional features like bodies or tissues; they evolve more rapidly with increased genetic variation; and their stability improves when mutations do not lead to premature death. Figure 1 presents our perspective on the evolution of life through the hypercube model, with each vertex representing a potential state of life across these dimensions. As shown in Fig. 1A, the number of vertices along a specific axis is influenced by the chosen level of resolution. At higher resolutions, the number of possible states increases exponentially. For instance, considering just two states for each dimension—simple versus complex, slow versus fast, unstable versus stable—yields eight possible combinations (2 x 2 x 2 = 8), representing eight types of living entities. However, if we increase the resolution to include four states per dimension—very simple, simple, complex, and very complex; very slow, slow, fast, and very fast; and so forth—this complexity expands significantly.",
        "ori-fast-z-score": 0.31622776601683794,
        "water-fast-z-score": 7.329426778023636,
        "rewrite-fast-z-score": -0.3223291856101521
    },
    {
        "original_text": "We present the results of an unbiased survey for compact HII regions in the southern Galactic plane using data obtained with the Australia Telescope Compact Array (ATCA). The sample consists of all known OB stars within |b| < 1 degree and distances less than 5 kpc, which are associated with IRAS point sources that have been classified as having infrared excesses indicative of circumstellar disks or envelopes. We detect over 100 new compact HII regions at frequencies between 2.1 GHz and 6.0 GHz. These objects range in size from 0.01 pc to 0.5 pc and their luminosities vary by more than four orders of magnitude. Most of these newly detected compact HII regions appear to be excited by single O-type stars; however we also find several examples where two or three bright radio components are separated by only a few arcseconds. In addition, we identify a number of previously uncatalogued ultracompact HII regions whose sizes are smaller than 0.01 pc.",
        "watermark_text": "We present the conclusion of an unbiased survey for compact HII locations in the southern Galactic jet using data acquired with the Australia Telescope Compact Array ( ATCA ) . The sample consists of all known OB stars within | b | < 1 degree and altitudes lower than 5 kpc , which are identified with IRAS point bodies that have been classified as having infrared excesses indicative of circumstellar disks or envelopes .We detect over 100 new compact HII zones at speeds between 2 . 1 GHz and 6 . 0 GHz . These objects range in height from 0 . 01 pc to 0 . 5 pc and their luminosities vary by more than four orders of magnitude .Most of these newly observed compact HII regions seem to be excited by single O - class stars ; however we also find various instances where two or three dark radio components are split by only a few arcseconds . In addition , we identify a number of previously uncatalogued ultracompact HII zones whose sizes are smaller than 0 . 01 pc .",
        "rewrite_text": "We present the results of an impartial survey of compact HII regions in the southern Galactic jet, utilizing data obtained from the Australia Telescope Compact Array (ATCA). Our sample includes all known OB stars located within |b| < 1 degree and at distances less than 5 kpc, identified through IRAS point sources characterized by infrared excesses that suggest the presence of circumstellar disks or envelopes. We have discovered over 100 new compact HII regions with velocities ranging from 2.1 GHz to 6.0 GHz. These regions vary in height from 0.01 pc to 0.5 pc, displaying luminosities that differ by more than four orders of magnitude. Most of the newly detected compact HII areas appear to be energized by individual O-class stars; however, we also observe multiple instances where two or three faint radio components are separated by just a few arcseconds. Additionally, we have identified several previously uncatalogued ultracompact HII regions with sizes smaller than 0.01 pc.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": -2.6457513110645903
    },
    {
        "original_text": "We present new results on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures, using Monte Carlo radiative transfer simulations. We find that for stars with high mass-loss rates (Ṁ > 10-7 M⊙ yr-1), the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering processes within the wind. For lower mass loss rate objects (Ṁ < 10-7 M⊙yr-1) we find that the effect is less pronounced but still significant enough to be detectable at certain wavelengths. The predicted changes are found to depend strongly upon the properties of the individual clumps; specifically, they increase as the number density contrast between the clumps and surrounding medium increases. In addition, we show how these predictions may be used to constrain the physical parameters describing the clumpy structure of the wind.  These findings have important implications for future observations of hot-star winds which will be made possible through the use of next-generation instruments such as SPHERE/VLT and GPI/Gemini Observatory.",
        "watermark_text": "We report new data on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures , using Monte Carlo radiative transfer simulations . We see that for stellar with high mass - loss rates ( [UNK] > 10 - 7 [UNK] yr - 1 ) , the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering mechanisms within the wind .For lower mass loss temperature objects ( [UNK] < 10 - 7 [UNK] - 1 ) we find that the impact is less noticeable but still significant enough to be detectable at certain wavelengths . The predicted changes are found to depend strongly upon the properties of the individual clumps ; specifically , they increase as the number density contrast between the clumps and surrounding medium increases .In addition , we show how these predictions may be used to constrain the physical factors describing the clumpy composition of the wind . These insights have important implications for future discoveries of bright - star winds which will be made possible through the using of next - generation satellites such as SPHERE / VLT and GPI / Gemini Observatory .",
        "rewrite_text": "We present new findings on the impact of clumps in stellar winds on their observed linear and circular polarization signatures, utilizing Monte Carlo radiative transfer simulations. Our results indicate that for stars with high mass-loss rates (⟨Ṁ⟩ > 10^-7 M☉ yr^-1), the presence of clumps can substantially influence both the degree and angle of linear polarization generated by scattering processes within the wind. For stars with lower mass-loss rates (⟨Ṁ⟩ < 10^-7 M☉ yr^-1), the effects are less pronounced but still significant enough to be detected at certain wavelengths. The predicted variations depend heavily on the characteristics of the individual clumps, particularly increasing as the contrast in number density between the clumps and the surrounding medium rises. Additionally, we demonstrate how these predictions could help constrain the physical parameters that describe the clumpy nature of the wind. These findings have crucial implications for future observations of bright star winds, which will be facilitated by next-generation telescopes such as SPHERE at the VLT and GPI at the Gemini Observatory.",
        "ori-fast-z-score": 0.6546536707079772,
        "water-fast-z-score": 3.709704134011871,
        "rewrite-fast-z-score": -2.49100947511811
    },
    {
        "original_text": "We study the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial-vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil.  We find that, although the branching fractions are small due to the helicity suppression, these processes can be used as probes of new physics beyond the Standard Model through their CP asymmetries. \nPACS numbers: 11.15.Tk, 12.38.Qk, 13 .25.Hw \nI. INTRODUCTORY REMAR K\nIn this work we will consider the following two types of charmless hadronic:  B → S V (S = P , A 0 ;V = T 1 )andB → SV(S=P;V=A1). The first type is characterized by one light quark in the final state while the second has no light quarks in it. In both cases there is only one spectator quark which leads to a helicity suppression of the corresponding decay rates. However, they may still serve as useful probes of new physics since their CP-violating asymmetries could be enhanced significantly compared to those of other modes  1  .\nTheoretically, such decays have been studied within various approaches including naive factorization  2  , perturbative QCD  3  , soft-collinear effective theory  4  , and QCD factorization  5  -  8  . It was found that the predictions based on different methods differ substantially among themselves. For example, using naive factorization, Ref.  2  predicted Br(B − →K * 0 π − )/Br(B − →Kπ)=0.27 ±0.04, whereas Refs.  6, 7  obtained values around 0.1−0.2. This discrepancy indicates that more theoretical efforts should be made before drawing any definite conclusion about these decays.",
        "watermark_text": "We research the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial - vector or vector meson in the framework of QCD factorization with generalized form factors at large recoil . We see that , although the branching fractions are small owing to the helicity suppression , these mechanisms can be used as probes of new dynamics beyond the Standard Model through their CP asymmetries .PACS numbers : 11 . 15 . Tk , 12 . 38 . Qk , 13 . 25 . Hw I . INTRODUCTORY REMAR K In this study we will explore the following two forms of charmless hadronic : B → S V ( S = P , A 0 ; V = T 1 ) andB → SV ( S = P ; V = A1 ) .The first sort is characterized by one light quark in the last position while the second has no light quarks in it . In both cases there is only one spectator quark which results to a helicity suppression of the associated decay rates .However , they may still provide as helpful probes of new theory since their CP - breaking asymmetries may be enhanced considerably compared to those of other modes 1 . Theoretically , such decays have been studied within various approaches including naive factorization 2 , perturbative QCD 3 , soft - collinear effective theory 4 , and QCD factorization 5 - 8 .It was shown that the estimates based on various methods varies dramatically among themselves . For instance , using naive factorization , Ref .2 observed Br ( B − →K * 0 π − ) / Br ( B − →Kπ ) = 0 . 27 ±0 . 04 , whereas Refs . 6 , 7 obtained values around 0 . 1−0 . 2 .This discrepancy implies that more theoretical efforts should be made before drew any explicit conclusion about these decays .",
        "rewrite_text": "We investigate the decay amplitudes for charmless hadronic B decays involving a scalar meson and either an axial-vector or vector meson within the framework of QCD factorization, utilizing generalized form factors at large recoil. Although the branching fractions are relatively small due to helicity suppression, these decay mechanisms can serve as valuable probes for new physics beyond the Standard Model, particularly through their CP asymmetries. \n\nIn this study, we examine two specific types of charmless hadronic decays: B → SV (where S = P, A₀ and V = T₁) and B → SV (with S = P and V = A₁). The first type is defined by the presence of one light quark in the final state, whereas the second type does not contain any light quarks. In both cases, the decay processes are hindered by helicity suppression, resulting in a single spectator quark. However, these decays may still yield useful insights into new theories, as their CP-violation asymmetries might be significantly amplified compared to those seen in other decay modes.\n\nTheoretical studies of these decay processes have employed various approaches, including naive factorization, perturbative QCD, soft-collinear effective theory, and QCD factorization. Notably, different methods provide widely varying estimates for the decay rates. For example, naive factorization has yielded a ratio of branching fractions Br(B⁻ → K*₀π⁻) / Br(B⁻ → Kπ) = 0.27 ± 0.04, while other references have reported values in the range of 0.1 to 0.2. This discrepancy underscores the need for further theoretical investigation before drawing definitive conclusions regarding these decays.",
        "ori-fast-z-score": 0.3375263702778072,
        "water-fast-z-score": 6.484597134749389,
        "rewrite-fast-z-score": 0.5129891760425771
    },
    {
        "original_text": "We present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3Â times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.",
        "watermark_text": "We create an free - source framework , entitled Bubblesched , that enables to build portable thread schedulers for hierarchical multiprocessors ( HMPs ) . The main idea is to use bubbles as booking units and pattern them on various levels in HMP hierarchy following a setting of rules .We have adopted two schedulers : one based on work stealing and another one based on load balancing . Both schedulers are able to run efficiently on top of Bubblesched without any modifications .Our research results show that both schedulers outperform state - of - the - art applications by up to 3Â times when ran parallel programs with fine - grained assignments . In addition , we prove how our scheduler can be used to execute easy problem - parallel techniques such as graph coloring or vector multiplication .This research was supported by Russian Science Foundation award 14 - 50 - 00040 . We create an opensource platform , entitled Bubblesched : it allows to build portable threadschedulers for hierarchicalmultiprocessors ( HMPs ) andrunefficientlyonthemwithoutanymodifications . Theframeworkusesbubblesastheschedu - lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules . Wehaveimplementedtwo - schedulers : onebasedonstealingworkandanotheronesupportedbyloadbalancing . BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified . Ourexperimentalresultsshowthatbothschedulersoutperformstate - oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks . Inaddition , weillustratethattheframeworkcanbeusedtomakeefficienttask - parallelandalgorithmssuchastask - coloringanoregularmatrixmultiplication .",
        "rewrite_text": "We have developed an open-source framework called Bubblesched that facilitates the creation of portable thread schedulers for hierarchical multiprocessors (HMPs). The core concept is to utilize bubbles as scheduling units and arrange them across different levels of the HMP hierarchy according to a specific set of rules. We have implemented two distinct schedulers: one that employs work stealing and another that focuses on load balancing. Both schedulers can operate efficiently on the Bubblesched platform without requiring any modifications. Our experimental findings indicate that these schedulers significantly outperform leading applications, achieving up to three times better performance when executing parallel programs with fine-grained tasks. Furthermore, we demonstrate that our framework can effectively handle simpler problem-parallel techniques, such as graph coloring and matrix multiplication. This research was supported by the Russian Science Foundation under award 14-50-00040.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": -1.099524999206747
    },
    {
        "original_text": "We present mixed hyperbolic-second-order parabolic formulations for the Einstein field equations in vacuum and electrovacuum, which are suitable to be solved numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement (AMR). The formulation is based on an auxiliary variable that allows us to split the evolution system into two subsystems, one hyperbolic and another second-order parabolic. We show how this splitting can be used to construct stable numerical schemes using standard techniques such as Kreiss-Oliger dissipation or artificial viscosity. In addition we discuss several issues related to the implementation of these schemes within the AMR framework provided by the Cactus Computational Toolkit. Finally, we present some preliminary results obtained with our new code. This work was supported by CONACyT grant No. 164710. Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "watermark_text": "We present mixed hyperbolic - second - order parabolic formulations for the Einstein field equations in vacuum and electrovacuum , which are suitable to be answered numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement ( AMR ) . The implementation is based on an auxiliary variable that enables us to split the evolution system into two subsystems , one hyperbolic and another second - order parabolic .We see how this splitting can be used to build stable numerical theories employing conventional methods such as Kreiss - Oliger dissipation or artificial viscosity . In addition we explain several topics related to the implementation of these schemes within the AMR framework presented by the Cactus Computational Toolkit .Finally , we present some preliminary results acquired with our new code . This project was supported by CONACyT grant No .164710.Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "rewrite_text": "We introduce mixed hyperbolic and second-order parabolic formulations for the Einstein field equations in both vacuum and electrovacuum scenarios, designed for numerical solutions using finite difference methods on Cartesian grids with adaptive mesh refinement (AMR). This framework relies on an auxiliary variable that allows us to decompose the evolution system into two components: one hyperbolic and one second-order parabolic. This decomposition facilitates the development of stable numerical models using standard techniques like Kreiss-Oliger dissipation and artificial viscosity. Additionally, we discuss various aspects of implementing these schemes within the AMR framework provided by the Cactus Computational Toolkit. Finally, we share preliminary results obtained with our new code. This project received support from CONACyT under grant No. 164710.  \nKeywords: Adaptive Mesh Refinement, Numerical Relativity",
        "ori-fast-z-score": 1.632993161855452,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 1.3608276348795434
    },
    {
        "original_text": "We present an alternative description of the electron in terms of its position and velocity, which is based on the idea that it moves along a helical trajectory around the nucleus. The new approach leads to a simple analytical expression for the energy levels of the helium atom as well as for the wave functions corresponding to these states. We show how this model can be used to explain some experimental results obtained by high-resolution spectroscopy experiments performed at Jefferson Lab. In addition we discuss possible extensions of our work towards other atomic systems such as muonic atoms or ions with one valence electron. Helium has been studied extensively over many decades both experimentally and theoretically. It was found that there are two stable isotopes (3He and 4He) and several excited states. These states have been investigated using various spectroscopic techniques including photo-absorption  1  , laser excitation  2  , and Compton scattering  3  . However, despite all efforts made so far, no satisfactory explanation exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound  4  .\nIn order to understand better the structure of helium, we propose here a new theoretical framework where the electron is described not only by its usual position but also by its velocity vector. This new approach allows us to obtain analytically the energy spectrum of helium as well as the associated wavefunctions. Our formalism is inspired by the so-called Bohmian mechanics  5  , which describes particles moving along trajectories instead of following classical equations of motions  6  .",
        "watermark_text": "We introduce an different characterization of the electron in terms of its position and speed , which is based on the idea that it travels along a helical trajectory around the nucleus . The modern perspective results to a simple analytical expression for the power concentrations of the helium atom as well as for the wave functions associated to these states .We see how this description can be used to explain some experimental results acquired by high - resolution spectroscopy investigations undertaken at Jefferson Lab . In addition we explain possible extend of our work towards other nuclear systems such as muonic atoms or ions with one valence electron .Helium has been studied frequently over numerous years both experimentally and theoretically . It was shown that there are two stable isotopes ( 3He and 4He ) and many excited states .These states have been investigated using numerous spectroscopic techniques including photo - absorption 1 , laser excitation 2 , and Compton absorption 3 . However , despite all efforts made so far , no satisfactory excuse exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound 4 .In order to explain better the composition of helium , we undertake here a new theoretical framework where the electron is characterized not only by its traditional position but also by its velocity tensor . This new approach allows us to obtain analytically the power spectrum of helium as well as the associated wavefunctions .Our formalism is influenced by the so - called Bohmian theory 5 , which explains particles moving along trajectories rather of following classical equations of motions 6 .",
        "rewrite_text": "We present an alternative characterization of the electron, defined by both its position and velocity, grounded in the concept of its helical motion around the nucleus. This contemporary viewpoint yields a straightforward analytical expression for the power distributions of the helium atom and the wave functions corresponding to these states. Furthermore, we demonstrate how this framework can elucidate certain experimental findings obtained from high-resolution spectroscopy studies conducted at Jefferson Lab. Additionally, we discuss potential extensions of our work to other nuclear systems, such as muonic atoms or ions with a single valence electron. Over the years, helium has been the subject of extensive experimental and theoretical research. It has been established that it possesses two stable isotopes (3He and 4He) along with numerous excited states, which have been explored using various spectroscopic methods, including photo-absorption, laser excitation, and Compton absorption. Nevertheless, despite considerable efforts, a satisfactory explanation for why the ground state of 3He is unbound while that of 4He is bound remains elusive. To provide a clearer understanding of helium's structure, we introduce a new theoretical framework that characterizes the electron not only by its conventional position but also by its velocity tensor. This innovative approach enables us to analytically derive the power spectrum of helium and the corresponding wavefunctions. Our formalism draws inspiration from Bohmian theory, which posits that particles follow trajectories rather than adhering to classical equations of motion.",
        "ori-fast-z-score": -0.27975144247209416,
        "water-fast-z-score": 7.243550686553699,
        "rewrite-fast-z-score": 0.9805806756909202
    },
    {
        "original_text": "We present an accurate and efficient fundamental-measure density-functional (FMT) approach to describe fluids composed of rigidly-aligned hard hexagons, which are relevant as model systems for liquid crystals or colloidal suspensions with anisotropic interactions. The FMT is based on a decomposition into three different types of weighted densities that can be evaluated efficiently using fast Fourier transforms. We show how this new FMT yields excellent results compared to Monte Carlo simulations over wide ranges of packing fractions and orientations of the particles. In particular we find very good agreement between our theoretical predictions and simulation data at high packing fractions where previous approaches fail due to strong correlations among neighboring particles. Finally, we demonstrate that our method also allows us to accurately predict structural properties such as pair correlation functions and orientational order parameters. This work provides further evidence that FMTs provide a powerful tool to study complex fluids beyond simple spherical particle models. \nI. INTRODUCTORY REMARkS\nThe description of liquids and soft matter requires sophisticated methods because these materials often exhibit complex structures and dynamics. Density functionals have been developed during recent years as promising tools to tackle many-body problems in statistical mechanics  1  . They allow one to calculate equilibrium properties of interacting particles by minimizing a free energy functional with respect to the local number density distribution. A particularly successful class of density functionals are so-called fundamental-measure density-functionals (FMD), which were originally introduced by Rosenfeld  2  .\nIn their original form they only apply to fluids consisting of identical spheres but extensions to more complicated shapes like ellipsoids  3  , rods  4  , dumbbells  5  , spherocylinders  6  , and even patchy particles  7, 8  have been proposed recently. However, most of these works focus on the case of uniaxial symmetry while there exist few studies dealing with more general situations  9  . Here we consider a system of rigidly-aligned",
        "watermark_text": "We present an accurate and elegant universal - measure density - functional ( FMT ) approach to define liquid consist of rigidly - aligned hard hexagons , which are applicable as model structures for solid crystals or colloidal suspensions with anisotropic interactions . The FMT is based on a transformation into three different kinds of weighted densities that can be evaluated efficiently using fast Fourier transforms .We see how this new FMT yields good results relative to Monte Carlo simulations over broad ranges of packing fractions and orientations of the particles . In particular we find very best agreement between our theory assumptions and modeling data at high packing fractions where earlier approaches fail due to good correlations among neighboring particles .Finally , we prove that our technique also enables us to correctly calculate structural properties such as couple correlation functions and orientational order variables . This research provides further evidence that FMTs represent a powerful tool to study complex fluids beyond straightforward spherical particle models .I . INTRODUCTORY REMARkS The description of liquids and soft material requires sophisticated methods because these structures often exhibit intricate structures and dynamics .Density functionals have been proposed during recent years as promising tools to tackle many - bodies problems in mathematical mechanics 1 . They allow one to estimate equilibrium properties of interacting interactions by minimizing a free energy functional with regard to the local number density density .A notably notable family of density functionals are so - called fundamental - measure density - functionals ( FMD ) , which were first developed by Rosenfeld 2 . In their original form they only applicable to liquid full of different balls but extensions to more complicated forms like ellipsoids 3 , rods 4 , dumbbells 5 , spherocylinders 6 , and also patchy particles 7 , 8 have been proposed lately .However , most of these works concentrate on the case of uniaxial symmetry while there remain few experiments focusing with more general situations 9 . Here we consider a system of rigidly - aligned",
        "rewrite_text": "We introduce an accurate and elegant universal fundamental measure theory (FMT) approach for characterizing a liquid composed of rigidly aligned hard hexagons. This model serves as a representation for solid crystals or colloidal suspensions exhibiting anisotropic interactions. The FMT is grounded in a transformation into three distinct types of weighted densities, which can be efficiently assessed using fast Fourier transforms. Our findings reveal that this new FMT yields commendable results in comparison to Monte Carlo simulations across a wide range of packing fractions and particle orientations. Notably, we observe the best congruence between our theoretical predictions and simulation data at high packing fractions, where previous methods struggle due to strong correlations among neighboring particles. Additionally, we demonstrate that our technique accurately computes structural properties, such as correlation functions and orientational order parameters. This research reinforces the notion that FMTs are a powerful tool for investigating complex fluids beyond simple spherical particle models. \n\nI. INTRODUCTORY REMARKS\n\nThe study of liquids and soft materials necessitates advanced methods due to the intricate structures and dynamics they often display. In recent years, density functionals have emerged as promising instruments for addressing many-body problems in mathematical mechanics. These functionals enable the estimation of equilibrium properties of interacting systems by minimizing a free energy functional concerning the local number density. A particularly notable category of density functionals is the fundamental measure density functionals (FMD), first introduced by Rosenfeld. In their initial formulation, these functionals were applicable only to liquids composed of hard spheres, but there have been recent extensions for more complex shapes, including ellipsoids, rods, dumbbells, spherocylinders, and patchy particles. However, much of the existing literature has focused on systems with uniaxial symmetry, with fewer studies exploring more general scenarios. Here, we analyze a system of rigidly aligned hard hexagons.",
        "ori-fast-z-score": -2.8823067684915684,
        "water-fast-z-score": 6.205346816570694,
        "rewrite-fast-z-score": -1.150792911137501
    },
    {
        "original_text": "We present an approach to extract the underlying physics from large sets of experimental data by using machine learning techniques and statistical analysis. The method is applied on two different examples, namely the measurement of the electrical conductivity in doped semiconductors as well as the determination of the critical temperature Tc for superconductivity in cuprates. In both cases we find that our results are consistent with theoretical predictions. We show how this new technique can be used to identify unknown parameters or even completely new phenomena which cannot be predicted theoretically at all. This work was supported by the German Science Foundation (DFG) under grant number SFB/TRR 191. A central goal of modern science is to understand complex systems such as materials or living organisms through their fundamental building blocks. To achieve this aim it is necessary to develop methods which allow us to analyze huge amounts of experimental data efficiently. Here we propose a novel approach based on machine learning algorithms combined with advanced statistical tools. Our method is demonstrated on two examples: First, we study the dependence of the electrical conductivity on doping concentration in semiconductor compounds. Second, we determine the critical temperature T c for superconductivity in high-temperature cuprate oxides. For both problems we obtain results which agree very well with existing theories. Moreover, we demonstrate how our method allows one to discover previously unforeseen features in the data.",
        "watermark_text": "We present an way to extract the fundamental theory from huge sets of experimental evidence by using computer learning techniques and statistical analysis . The method is applied on two different examples , notably the determination of the electrical conductivity in doped semiconductors as well as the determination of the significant heat Tc for superconductivity in cuprates .In both cases we find that our findings are compatible with theoretical expectations . We see how this new technique can be used to identify unseen conditions or even completely new events which cannot be described theoretically at all .This project was supported by the German Science Foundation ( DFG ) under grant number SFB / TRR 191 . A central goal of modern science is to explain complex systems such as materials or living organisms through their major building frames .To achieve this aim it is required to develop techniques which allow us to analyze huge amounts of research data easily . Here we undertake a innovative method built on machine computing methods coupled with sophisticated statistical tools .Our method is demonstrated on two examples : First , we study the relationship of the electrical conductivity on doping concentration in semiconductor compounds . Second , we determine the significant heat T c for superconductivity in high - temperature cuprate oxides .For both difficulties we obtain results which agree very best with existing ideas . Moreover , we prove how our technique permits one to identify formerly unforeseen features in the information .",
        "rewrite_text": "We present a method for extracting fundamental theories from large sets of experimental data using machine learning techniques and statistical analysis. This approach is applied to two distinct scenarios: the determination of electrical conductivity in doped semiconductors and the assessment of the critical temperature \\( T_c \\) for superconductivity in cuprates. In both instances, our results align well with theoretical predictions. Furthermore, we demonstrate how this new technique can uncover hidden conditions or even entirely novel phenomena that are not currently describable by existing theories. This project received support from the German Science Foundation (DFG) under grant number SFB/TRR 191. A primary objective of contemporary science is to elucidate complex systems—such as materials and living organisms—by understanding their fundamental components. To achieve this, it is essential to develop methods that facilitate the analysis of vast amounts of research data. Here, we employ an innovative technique that combines machine learning methods with advanced statistical tools. Our approach is illustrated through two examples: first, by exploring the relationship between electrical conductivity and doping concentration in semiconductor compounds, and second, by determining the critical temperature \\( T_c \\) for superconductivity in high-temperature cuprate oxides. For both cases, our findings are in strong agreement with existing theories. Additionally, we demonstrate how our technique enables the identification of previously unrecognized features within the data.",
        "ori-fast-z-score": -0.3682298471593294,
        "water-fast-z-score": 7.98489954161323,
        "rewrite-fast-z-score": -0.5669467095138409
    },
    {
        "original_text": "We present an evolutionary model that describes the evolution of regulatory mechanisms controlling cell division rates in growing populations. The model is based on the assumption that cells can evolve to optimize their fitness by adjusting their growth rates according to environmental conditions, such as nutrient availability or population density. We show how this simple principle leads to the emergence of complex regulatory networks with multiple feedback loops and time delays. Our results suggest that these networks are robust against mutations but sensitive to changes in external conditions. This may explain why many organisms have evolved similar regulatory circuits despite differences in their genetic backgrounds. \nAuthor: Jens Palsson (University of California San Diego); David Botstein (Stanford University)\nSource: Science Express Online - ScienceDirect URL: http://arxiv.org/abs/1106.0571 DOI: 10.1088/1741-5468/2009/01/00011 \nThe authors propose a mathematical model describing the evolution of regulatory mechanisms governing cell division rates in growing bacterial populations. They assume that bacteria can evolve to maximize their fitness by optimizing their growth rates under different environmental conditions, including nutrient availability and population density. Their analysis shows how this simple principle gives rise to complex regulatory networks containing multiple feedback loops and time-delays.",
        "watermark_text": "We present an evolutionary model that describes the evolution of regulatory mechanisms governing cell division rates in expanding populations . The model is based on the assumption that cells can evolve to optimize their fitness by adjusting their development rates according to environmental conditions , such as nutrient abundance or population density .We suggest how this simple assumption results to the emergence of complex regulatory circuits with many feedback loops and period delays . Our results show that these networks are robust against mutations but sensitive to changes in external conditions .This might explain why many bacteria have adapted related regulatory circuits despite differences in their genetic backgrounds . Author : Jens Palsson ( University of California San Diego ) ; David Botstein ( Stanford University ) Source : Science Express Online - ScienceDirect URL : www : / / arxiv . org / abs / 1106 . 0571 DOI : 10 . 1088 / 1741 - 5468 / 2009 / 01 / 00011 The authors propose a mathematical structure describing the evolution of regulatory mechanisms governing cell division rates in expanding bacterial populations .They assume that bacteria can evolve to maximize their fitness by optimizing their development rates under various climate circumstances , notably nutrient abundance and population density . Their analysis shows how this simple law giving rise to complex regulatory circuits containing several feedback loops and period - errors .",
        "rewrite_text": "We introduce an evolutionary model that outlines how regulatory mechanisms controlling cell division rates evolve within expanding populations. This model operates under the premise that cells can adapt their development rates to enhance their fitness, responding to environmental factors such as nutrient availability and population density. Our findings demonstrate that this straightforward assumption leads to the development of intricate regulatory networks characterized by multiple feedback loops and time delays. Notably, these networks exhibit robustness against mutations while remaining sensitive to alterations in external conditions. This may elucidate why numerous bacteria have evolved related regulatory mechanisms despite varying genetic backgrounds. Authors: Jens Palsson (University of California San Diego) and David Botstein (Stanford University). Source: Science Express Online - ScienceDirect URL: www:/arxiv.org/abs/1106.0571 DOI: 10.1088/1741-5468/2009/01/00011. The authors propose a mathematical framework that describes the evolution of regulatory systems overseeing cell division rates in expanding bacterial populations. They suggest that bacteria can evolve to enhance their fitness by optimizing their development rates based on varying environmental conditions, particularly nutrient abundance and population density. Their analysis reveals how this basic principle can lead to complex regulatory systems featuring multiple feedback loops and delays.",
        "ori-fast-z-score": 0.9622504486493763,
        "water-fast-z-score": 6.670481774415078,
        "rewrite-fast-z-score": 2.331086069657434
    },
    {
        "original_text": "We report water vapor (H2O) and silicon monoxide (SiO) maser emission toward the central star of the protoplanetary  nebula OH231.8+4.2, which is associated with an infrared source IRAS 18286-1231. The H2O masers are distributed over a region of ~0.1 arcsec diameter around the star at a velocity range of -40 to +20 km s-1 relative to the systemic velocity of the nebula. We detected SiO masers only on one side of the star within 0.05 arcsec radius at velocities ranging between -50 and -30 km s-1. These results suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers arise from outflowing material along the polar axis.  This work was supported by Grants-in-Aid for Scientific Research (No. 15740160)  from MEXT Japan.",
        "watermark_text": "We report wood vapor ( H2O ) and silicon monoxide ( SiO ) maser emission toward the main star of the protoplanetary nebula OH231 . 8 + 4 . 2 , which is associated with an infrared source IRAS 18286 - 1231 . The H2O masers are distributed over a region of ~ 0 . 1 arcsec diameter around the star at a speed range of - 40 to + 20 km s - 1 relative to the systemic speed of the nebula .We observed SiO masers only on one side of the star within 0 . 05 arcsec radius at velocities ranging between - 50 and - 30 kilometers s - 1 . These data suggest that the H2O masers trace shocked liquid near the stellar surface while the SiO masers occur from outflowing matter along the polar axis .This project was supported by Grants - in - Aid for Scientific Research ( No . 15740160 ) from MEXT Japan .",
        "rewrite_text": "We present observations of water vapor (H2O) and silicon monoxide (SiO) maser emissions from the main star of the protoplanetary nebula OH 231.8 + 4.2, which is linked to the infrared source IRAS 18286-1231. The H2O masers are found within a region approximately 0.1 arcseconds in diameter surrounding the star, exhibiting velocity ranges from -40 to +20 km/s in relation to the nebula's systemic speed. In contrast, SiO masers were detected solely on one side of the star within a 0.05 arcsecond radius, with velocities between -50 and -30 km/s. These observations indicate that the H2O masers are associated with shocked liquid close to the stellar surface, while the SiO masers originate from outflowing material along the polar axis. This research was funded by Grants-in-Aid for Scientific Research (No. 15740160) from MEXT, Japan.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 3.794733192202055,
        "rewrite-fast-z-score": 0.5897678246195885
    },
    {
        "original_text": "We present an explanation to the observed anomalies in globular cluster luminosity functions (GCLFs) and mass-to-light ratios, based on the assumption that these clusters are composed by two different populations with distinct chemical compositions.  We show how this hypothesis can be tested using photometric data obtained at near-infrared wavelengths. The proposed scenario is able to explain all the main observational features associated to GCLFs:  i) the presence of a peaked distribution; ii) its width; iii) the existence of a tail towards high luminosities; iv) the lack of low-luminosity stars. In addition, it also explains why some GCs have very large values of M/LV . Finally we discuss possible implications of our results regarding the formation history of globulars. Keywords: Globular cluster, Mass-to-light ratio, Luminosity function, Chemical composition, Near-infrared",
        "watermark_text": "We present an explanation to the reported anomalies in globular cluster luminosity functions ( GCLFs ) and mass - to - light proportions , relying on the assumption that these clusters are composed by two different populations with distinct chemical compositions . We see how this hypothesis can be evaluated using photometric data derived at near - infrared wavelengths .The proposed theory is could to explain all the main observational characteristics attributed to GCLFs : i ) the presence of a peaked distribution ; ii ) its height ; iii ) the existence of a tail towards high luminosities ; iv ) the lack of high - luminosity stars . In addition , it also explains why some GCs have very huge values of M / LV .Finally we explain possible implications of our findings pertaining the formation history of globulars . Keywords : Globular cluster , Mass - to - light density , Luminosity function , Chemical composition , Near - infrared",
        "rewrite_text": "We offer an explanation for the observed anomalies in globular cluster luminosity functions (GCLFs) and mass-to-light ratios, based on the premise that these clusters consist of two distinct populations with varying chemical compositions. This hypothesis can be assessed using photometric data obtained from near-infrared wavelengths. The proposed theory accounts for several key observational features associated with GCLFs: i) the presence of a peaked distribution; ii) its height; iii) the existence of a tail extending towards high luminosities; and iv) the absence of high-luminosity stars. Furthermore, it clarifies why certain globular clusters exhibit very high values of M/LV. Lastly, we discuss the potential implications of our findings regarding the formation history of globular clusters. Keywords: Globular cluster, Mass-to-light ratio, Luminosity function, Chemical composition, Near-infrared.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 4.6475800154489,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "The dust around the star Beta Pictoris is being studied by astronomers at Harvard University, using data collected with NASA s Spitzer Space Telescope.  The researchers are studying how the dust particles interact with each other to form larger bodies that may eventually become planets.   They have found evidence for two different types of dust grains in this system; one type has been observed previously but not the second.    This new dust grain appears to be much smaller than those seen before (about 100 times smaller).   It also seems to be more reflective or transparent than previous observations would suggest.   These findings could help explain why some stars appear brighter when they are younger while others do not. Astronomers are trying to understand how planetary systems form.  One way to study planet formation is through observing young stars like Beta Pictoris which is about 20 million years old.  Beta Pictoris is surrounded by an enormous amount of dust produced as it sheds its outer layers during its youthful evolution.   In addition there is gas surrounding the star that forms into spiral patterns similar to those seen in our own solar system.   Scientists believe these dust particles will collide and stick together over time forming larger objects such as asteroids and comets.   Eventually these large bodies can grow even bigger and start orbiting the central star creating what we call  planets .   However, scientists don t know exactly how this process happens because it s very difficult to observe directly.   Instead, astronomers use telescopes to look at light coming from the dusty environment around young stars.   By analyzing the light emitted by the dust particles,...",
        "watermark_text": "The dust around the star Beta Pictoris is being studied by astronomers at Harvard University , using data taken with NASA s Spitzer Space Telescope . The scientists are studying how the dust particles react with each other to form bigger bodies that might eventually form planets .They have discovered evidence for two different kinds of dust grains in this system ; one sort has been observed previously but not the second . This new powder grain tends to be much smaller than those observed before ( about 100 times smaller ) .It therefore seems to be more reflective or reflective than prior measurements might suggest . These studies could explain explain why some stars appear warmer when they are younger while many do not .Astronomers are trying to comprehend how planetary structures form . One method to study planet development is through watching small stars like Beta Pictoris which is about 20 million years old .Beta Pictoris is surrounded by an enormous quantity of dust created as it sheds its outer layers during its young evolution . In addition there is gas covering the star that forms into spiral patterns comparable to those observed in our own solar body .Scientists think these cloud particles will collide and stick together over time forming big objects such as asteroids and comets . Eventually these massive bodies can develop much bigger and get orbiting the main star producing what we call planets .However , scientists don t know exactly how this process happens because it s very difficult to observe directly . Instead , astronomers use telescopes to see at energy coming from the dry environment around young galaxies .By analyzing the light emitted by the dust particles,...",
        "rewrite_text": "Astronomers at Harvard University are investigating the dust surrounding the star Beta Pictoris, utilizing data collected from NASA’s Spitzer Space Telescope. Their research focuses on how dust particles interact to form larger bodies that could eventually develop into planets. The scientists have identified two distinct types of dust grains in this system; one type has been seen before, but the other is newly observed and significantly smaller—approximately 100 times smaller than previously detected grains. This smaller grain appears to be more reflective than earlier measurements indicated. These findings could help illuminate why some stars appear warmer in their youth, while others do not. The study of planet formation often involves observing stars like Beta Pictoris, which is around 20 million years old. This star is encircled by a vast amount of dust generated as it loses its outer layers during its early development. Additionally, gas surrounding the star exhibits spiral patterns similar to those in our own solar system. Scientists propose that these cloud particles will collide and adhere to one another over time, eventually forming larger objects such as asteroids and comets. These substantial bodies might continue to grow and eventually orbit the star, resulting in the formation of planets. However, the exact mechanisms of this process remain elusive, as it is challenging to observe directly. Instead, astronomers analyze the light emitted by dust particles using telescopes to gain insights into the energy dynamics in the environments surrounding young stars.",
        "ori-fast-z-score": 0.43685202833051895,
        "water-fast-z-score": 9.400193421607684,
        "rewrite-fast-z-score": -1.8935062328016077
    },
    {
        "original_text": "The purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations  qμ(x), qν(y)  = iθμνρqρ(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .",
        "watermark_text": "The purpose of this page is to provide an overview of some latest findings in quantum field theory ( QFT ) on curved spaces with noncommutative coordinates . The main motivation for studying QFTs on such spaces coming from string theories which are formulated as open strings attached to D - branes whose positions can be described by noncommuting matrices .In particular we will focus our focus on the so - called Groenewold - Moyal plane which is characterized as the space generated by two non - commuting coordinates satisfying the commutation operators qμ ( x ) , qν ( y ) = iθμνρqρ ( xy ) . We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane .This enables us to introduce a concept of spinor fields on the Groenewold - Moyal plane . Moreover we will explore how one can build gauge invariant operations for these fields .Finally we will explore the operation of the discrete symmetries C , P , T and CP T .",
        "rewrite_text": "This page aims to explore recent developments in quantum field theory (QFT) within curved spaces that feature noncommutative coordinates. The primary impetus for investigating QFTs in these contexts arises from string theories, where open strings are connected to D-branes, and their positions are represented by noncommuting matrices. Specifically, we will concentrate on the Groenewold-Moyal plane, defined by two non-commuting coordinates that adhere to the commutation relations \\( q^\\mu(x), q^\\nu(y) = i\\theta^{\\mu\\nu\\rho} q_\\rho(xy) \\). We will demonstrate the feasibility of defining a covariant derivative operator that acts on fields residing in the Groenewold-Moyal plane, which allows us to introduce the concept of spinor fields in this framework. Additionally, we will investigate how to construct gauge-invariant operations for these fields and examine the impact of the discrete symmetries C, P, T, and CPT within this context.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.810003810005715,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "We study the Leonard triple systems, which are finite sets of points in projective space over a field such that any three distinct points determine a line.  We show how to construct all Leonard triples with parameters (n = 3m + 1) or (n = 3m+2), where m is an integer greater than zero.   In particular we prove that there exist exactly two non-isomorphic Leonard triples for each n = 3m+1 and one non isomorphic Leonard triple for each n = 3(m+1).    Finally, we give some applications of our results on Leonard triples to the theory of hypercubes.    Keywords: Leonard systems; Projective geometry; Hypercube; Finite geometries. The research leading to these results has received funding from the European Research Council under the European Union s Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n o 291085. This article is part of the themed issue  Finite Geometries .",
        "watermark_text": "We work the Leonard triple systems , which are finite collections of points in projective space over a field such that any three separate points determine a line . We see how to build all Leonard triples with parameters ( n = 3m + 1 ) or ( n = 3m + 2 ) , where m is an integer larger than zero .In particular we prove that there exist exactly two non - isomorphic Leonard triples for each n = 3m + 1 and one non isomorphic Leonard triple for each k = 3 ( m + 1 ) . Finally , we give some applications of our findings on Leonard triples to the theory of hypercubes .Keywords : Leonard systems ; Projective geometry ; Hypercube ; Finite geometries . The studies leading to these results has obtained support from the European Research Council under the European Union s Seventh Framework Programme ( FP / 2007 - 2013 ) / ERC Grant Agreement n o 291085 .This page is part of the themed article Finite Geometries .",
        "rewrite_text": "We focus on Leonard triple systems, which are finite collections of points located in projective space over a field, where any three distinct points uniquely determine a line. We explore the construction of all Leonard triples with parameters \\( n = 3m + 1 \\) or \\( n = 3m + 2 \\), where \\( m \\) is a positive integer. In particular, we demonstrate that for each \\( n = 3m + 1 \\), there are exactly two non-isomorphic Leonard triples, while for \\( n = 3(m + 1) \\), there is one non-isomorphic Leonard triple. Additionally, we discuss the implications of our findings regarding Leonard triples in the context of hypercube theory. Keywords: Leonard systems; projective geometry; hypercube; finite geometries. This research was supported by the European Research Council under the European Union's Seventh Framework Programme (FP/2007–2013) / ERC Grant Agreement No. 291085. This page is part of the themed article on Finite Geometries.",
        "ori-fast-z-score": -1.0434983894999017,
        "water-fast-z-score": 3.015113445777636,
        "rewrite-fast-z-score": 0.5897678246195885
    },
    {
        "original_text": "We present an efficient algorithm for computing the sign function of a large sparse complex matrix, which is based on the Lanczos bidiagonalization process with partial reorthogonalization. The proposed algorithm can be applied to any Hermitian or non-Hermitian matrices without restriction. We apply this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density. In particular we show that our algorithm works well even when the quark mass becomes small compared to the inverse of the lattice spacing. This work was supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20 .Dh  1 Introduction Lattice Quantum Chromodynamics(LQCD), as one of the most promising candidates for describing strong interactions among quarks and gluons, has been widely used to study hadronic properties such as masses and decay constants  1  . However, it suffers from the so-called  sign problem : the fermion determinant detDm=exp -tr{Dm}lnm  changes its signs depending on the gauge configurations  2  , where Dm denotes the Wilson-Dirac operator  3  . Therefore, Monte Carlo methods cannot be directly employed to calculate physical quantities using LQCD because they require positive definite weight functions  4  .\nIn order to overcome this difficulty, several approaches have been developed so far  5  -  8  . Among them, the Taylor expansion approach  9  -  11  seems to be very powerful since it allows us to evaluate the expectation value of any observables accurately within statistical errors. It also enables us to perform calculations at high temperature and/or high density  12  -  14  . For example, the Taylor expansion up to O(a6) has already been performed successfully  15  .",
        "watermark_text": "We present an efficient algorithm for calculation the sign function of a large sparse complex matrix , which is based on the Lanczos bidiagonalization process with partial reorthogonalization . The proposed algorithm can be applied to any Hermitian or non - Hermitian matrices without limitation .We use this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density . In particular we prove that our algorithm works well even when the quark mass becomes tiny relative to the inverse of the lattice spacing .This project was supported by Grants - in - Aid for Scientific Research ( No . 20340040 ) from MEXT Japan .PACS scores : 11 . 15 . Ha , 12 . 38 . Qk , 12 . 39 . Fe , 14 . 20 . Dh 1 Introduction Lattice Quantum Chromodynamics ( LQCD ) , as one of the most attractive candidates for describing strong coupling among quarks and gluons , has been widely using to study hadronic properties such as masses and decay constants 1 . However , it suffers from the so - called sign problem : the fermion determinant detDm = exp - tr { Dm } lnm varies its signs depending on the gauge modes 2 , where Dm denotes the Wilson - Dirac operator 3 .Therefore , Monte Carlo methods never be directly used to estimate mechanical numbers using LQCD because they use positive definite weight functions 4 . In order to overcome this obstacle , various approaches have been proposed so far 5 - 8 .Among them , the Taylor expansion method 9 - 11 seems to be very potent since it allows us to analyze the expectation value of any observables correctly within statistical errors . It additionally lets us to conduct measurements at high heat and / or large velocity 12 - 14 .For instance , the Taylor expansion up to O ( a6 ) has already been performed successfully 15 .",
        "rewrite_text": "We introduce an effective algorithm for computing the sign function of large sparse complex matrices, leveraging the Lanczos bidiagonalization process with partial reorthogonalization. This algorithm is applicable to both Hermitian and non-Hermitian matrices without restrictions. We utilize this novel approach for the overlap Dirac operator in lattice QCD simulations at finite density, demonstrating its efficacy even when the quark mass approaches zero relative to the inverse of the lattice spacing. This research was funded by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20.Dh.\n\n**1. Introduction**\nLattice Quantum Chromodynamics (LQCD) has emerged as a leading framework for understanding the strong interactions between quarks and gluons, and it has been extensively employed to investigate hadronic properties such as masses and decay constants. However, it faces the so-called sign problem, where the fermion determinant \\(\\text{det} D_m = \\exp(- \\text{tr} \\{ D_m \\} \\ln m)\\) changes signs depending on the gauge configurations, with \\(D_m\\) representing the Wilson-Dirac operator. Consequently, traditional Monte Carlo methods are unsuitable for estimating physical quantities through LQCD, as they require positive definite weight functions. Various strategies have been proposed to address this challenge. Among these, the Taylor expansion method stands out as particularly effective, enabling accurate analysis of observables within statistical error margins, and facilitating measurements at high temperature and/or large velocities. Notably, a Taylor expansion up to \\(O(a^6)\\) has been successfully implemented.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.4835456068700275,
        "rewrite-fast-z-score": 1.744163198544762
    },
    {
        "original_text": "We propose that the vacuum state is not empty but contains fluctuations in spacetime, which we call holographic noise (HN). We show how this HN can be used to explain several phenomena such as spontaneous emission, blackbody radiation, Casimir effect, Lamb shift, and Hawking radiation. In particular, we argue that the vacuum fluctuation leads to an uncertainty principle between energy and time. This implies that there are no particles with zero mass or spin. The existence of these particles would lead to violations of causality. Finally, we discuss some possible experimental tests for our proposal. Vacuum fluctuations play important roles in quantum field theory. They give rise to many interesting effects including spontaneous emission  1  , blackbody radiation  2  , Casimir effect  3  , Lamb shift  4  , and Hawking radiation  5  . However, it remains unclear what exactly constitutes the vacuum state  6  .\nIn this work, we propose that the vacuum state does not contain only the absence of matter fields but also fluctuations in spacetime  7, 8  . These fluctuations may be viewed as virtual gravitons  9  . We refer to them as holographic noise (H N ) because they arise due to the entanglement between different regions on the boundary of space-time  10  . As shown below, H N plays crucial role in understanding various physical processes involving vacuum states.\nThe main idea behind our approach is illustrated by Fig.  1(a) . Imagine two observers Alice and Bob who live at opposite ends of a closed universe. Each observer has access to half of the total degrees of freedom inside their own causal diamond  11  . For example, if Alice lives near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone. Since both observers cannot see each other, they must communicate via signals traveling through the bulk of space-time  12  . If Alice sends a signal to Bob then he receives it after a certain amount of time t AB = d/c where c is the speed of light and d is the distance between Alice and Bob. On the other hand, if Bob sends",
        "watermark_text": "We suggest that the vacuum state is not filled but contains fluctuations in spacetime , which we call holographic noise ( HN ) . We see how this HN can be used to explain different processes such as spontaneous emission , blackbody radiation , Casimir effect , Lamb shift , and Hawking radiation .In particular , we claim that the vacuum fluctuation leads to an uncertainty theory between energy and time . This implies that there are no particles with zero mass or spin .The existence of these objects would result to violations of causality . Finally , we talk some possible experimental tests for our proposal .Vacuum fluctuations represent crucial roles in quantum field theory . They give rise to many interesting phenomena including spontaneous emission 1 , blackbody radiation 2 , Casimir effect 3 , Lamb shift 4 , and Hawking radiation 5 .However , it remains unsure what actually constitutes the vacuum state 6 . In this research , we claim that the vacuum state does not include only the absence of mind fields but also fluctuations in spacetime 7 , 8 .These fluctuations might be viewed as virtual gravitons 9 . We refer to them as holographic disturbance ( H N ) because they occur due to the entanglement between various regions on the boundary of space - time 10 .As seen below , H N plays crucial role in understanding various mechanical phenomena involving vacuum states . The main idea behind our approach is illustrated by Fig .1 ( a ) . Imagine two observers Alice and Bob who living at different edges of a closed world .Each observer has entry to half of the total degrees of autonomy inside their own causal diamond 11 . For instance , if Alice resides near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future dark cone .Since both observers cannot see each other , they must interact via signals going through the bulk of space - time 12 . If Alice sends a signal to Bob then he gets it after a certain amount of time t AB = d / c where l is the speed of light and d is the distance between Alice and Bob .On the other hand , if Bob sends",
        "rewrite_text": "We propose that the vacuum state is not an empty void, but rather is characterized by fluctuations in spacetime, which we refer to as holographic noise (HN). This HN can help illuminate various phenomena such as spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. Notably, we argue that vacuum fluctuations give rise to an uncertainty principle connecting energy and time, which leads to the conclusion that particles cannot exist with zero mass or spin, as their presence would violate causality. Additionally, we outline potential experimental tests to validate our proposal. \n\nVacuum fluctuations play a pivotal role in quantum field theory, giving rise to many intriguing effects, including spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. However, the precise nature of the vacuum state remains uncertain. In this study, we assert that the vacuum state not only signifies the absence of fields but also encompasses fluctuations within spacetime. These fluctuations can be interpreted as virtual gravitons, and we call them holographic disturbances (HN) because they arise from the entanglement between different regions at the boundary of spacetime. \n\nAs we will demonstrate, HN is essential for understanding various mechanical phenomena related to vacuum states. The core concept of our approach can be illustrated by Figure 1(a). Consider two observers, Alice and Bob, situated at opposite ends of a closed universe. Each individual has access to half of the total degrees of freedom within their respective causal diamonds. If Alice is located near the center of her universe, she has full knowledge of events within her past light cone, while Bob's information is confined to his future dark cone. Since they cannot see one another directly, their interactions must occur through signals traversing the bulk of spacetime. For example, if Alice transmits a signal to Bob, he will receive it after a time interval \\( t_{AB} = d / c \\), where \\( c \\) represents the speed of light and \\( d \\) is the distance separating them. Conversely, if Bob sends...",
        "ori-fast-z-score": 1.8717134551736667,
        "water-fast-z-score": 8.111071056538128,
        "rewrite-fast-z-score": 2.9793811989685244
    },
    {
        "original_text": "We study the orbital evolution and stability properties of oligarchic co-orbitals in the Solar System, i.e., bodies with masses comparable to that of Jupiter which are trapped on orbits close to those of Neptune or Uranus for billions of years.  We show how these objects can be identified by their long-term dynamical behavior as well as by their current positions relative to Neptune s orbit. The existence of such bodies is confirmed by numerical integrations over timescales up to 10 billion years using the symplectic N-body code SyMBA. In addition we find that there exist at least two other stable regions where oligarchs may reside. These results suggest that the Solar System contains several dozen oligarchic co-orbitals:  - At least four known trans-Neptunian objects (Pluto, Charon, Haumea, Makemake) have been found to exhibit this type of dynamics; - There exists another region around 30 AU containing three additional bodies (Sedna, 2000 CR 105 , 2003 SQ 317 ); - Finally, our simulations indicate that there might also be an additional group of oligarchs located between 50-60 AU.",
        "watermark_text": "We research the orbital evolution and stability properties of oligarchic co - orbitals in the Solar System , i . e . , bodies with masses similar to that of Jupiter which are locked on orbits close to those of Neptune or Uranus for billions of years . We see how these objects can be identified by their long - term dynamical behavior as well as by their current positions close to Neptune s orbit .The formation of such objects is predicted by numerical integrations over timescales up to 10 billion years employing the symplectic N - bodies code SyMBA . In addition we find that there exist at least two other stable parts where oligarchs might live .These data suggest that the Solar System includes several several oligarchic co - orbitals : - At least four known trans - Neptunian planets ( Pluto , Charon , Haumea , Makemake ) have been seen to contain this form of dynamics ; - There exists another region around 30 AU holding three extra bodies ( Sedna , 2000 CR 105 , 2003 SQ 317 ) ; - Finally , our simulations confirm that there might additionally be an additional family of oligarchs located between 50 - 60 AU .",
        "rewrite_text": "We investigate the orbital evolution and stability characteristics of oligarchic co-orbital bodies within the Solar System—specifically, those with masses comparable to Jupiter that have remained in orbits near Neptune or Uranus for billions of years. Our research demonstrates how these entities can be recognized through their long-term dynamical behaviors as well as their present positions near Neptune's orbit. The existence of such bodies is supported by numerical simulations conducted over timescales of up to 10 billion years using the symplectic N-body code SyMBA. Furthermore, we have identified at least two additional stable regions where oligarchs could reside. Our findings indicate that the Solar System likely contains several oligarchic co-orbital objects: at least four known trans-Neptunian planets (Pluto, Charon, Haumea, and Makemake) exhibit this type of dynamics; an additional region around 30 AU contains three more bodies (Sedna, 2000 CR 105, and 2003 SQ 317); and our simulations suggest there may also be another group of oligarchs situated between 50 and 60 AU.",
        "ori-fast-z-score": -1.3643820804812932,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": 0.5
    },
    {
        "original_text": "We consider an entropy penalization method in order to estimate the density function f of a random variable X with values on Rd . We prove that, under some conditions, this estimator is strongly consistent and we study its asymptotic normality. The main tool used here is the exponential decay of correlations property satisfied by the stochastic process associated to our estimation procedure. This result allows us to obtain rates of convergence for the mean integrated squared error (MISE) between the true density f and its estimators. Finally, numerical experiments are performed in dimension 1 and 2. Keywords: Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality. Mathematics Subject Classification (2010): 60C05, 60F10, 62G20. 1 Introduction Let X be a real-valued random vector defined on a probability space (Ω , A , P). In many applications such as signal processing or econometrics, it may be interesting to recover the distribution law of X denoted by fX . For example, if one wants to detect changes in the statistical properties of X over time, then knowing fX will allow him/her to perform change-point detection tests  see e.g., Chen et al. (2013), Fryzlewicz & Subba Rao (2014) . However, recovering fX can be difficult because only n iid observations X1 , . . . , Xn of X are available. To overcome this difficulty, several authors have proposed to use nonparametric methods based on kernel smoothing techniques  see e.g. , Silverman (1981) , Wand & Jones (1995)  . More precisely, let K : R →  0, 1  be a given kernel function satisfying certain regularity assumptions which will be specified later. Then, the classical kernel density estimator of fX at x ∈ Rd is defined bŷ fbK (x) =",
        "watermark_text": "We consider an entropy penalization techniques in order to estimate the density function f of a random variable X with values on Rd . We prove that , under some conditions , this estimator is strongly consistent and we study its asymptotic normality .The main technique useful here is the exponential decay of correlations property satisfied by the stochastic mechanism associated to our estimation method . This result allows us to obtain rates of convergence for the mean integrated squared error ( MISE ) between the true density f and its estimators .Finally , numerical studies are performed in dimension 1 and 2 . Keywords : Density estimation , Entropic penalty , Exponential decay of correlations , Asymptotic normality .Mathematics Subject Classification ( 2010 ) : 60C05 , 60F10 , 62G20 . 1 Introduction Let X be a real - valued random function characterized on a probability space ( Ω , A , P ) .In many applications such as signal filtering or econometrics , it could be interesting to extract the distribution law of X denoted by fX . For instance , if one wants to identify changes in the statistical characteristics of X over time , then understanding fX will provide him / her to conduct change - point detection tests see e . g . , Chen et al .( 2013 ) , Fryzlewicz & Subba Rao ( 2014 ) . However , returning fX can be challenging because only n iid observations X1 , ..., Xn of X are available . To solve this challenge , various published have proposed to use nonparametric techniques based on kernel smoothing methods see e . g ., Silverman ( 1981 ) , Wand & Jones ( 1995 ) . More specifically , let K : R → 0 , 1 be a given kernel map satisfying particular regularity assumptions which will be specified later .Then , the classical kernel density estimator of fX at x ∈ Rd is given bŷ fbK ( x ) =",
        "rewrite_text": "We explore an entropy penalization technique to estimate the density function \\( f \\) of a random variable \\( X \\) taking values in \\( \\mathbb{R}^d \\). We demonstrate that this estimator is strongly consistent under certain conditions and analyze its asymptotic normality. A key aspect of our method is the exponential decay of correlations property exhibited by the stochastic mechanism involved in our estimation approach. This finding enables us to derive convergence rates for the mean integrated squared error (MISE) between the true density \\( f \\) and its estimators. Additionally, we conduct numerical experiments in one and two dimensions. \n\n**Keywords:** Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality.  \n**Mathematics Subject Classification (2010):** 60C05, 60F10, 62G20.\n\n**1 Introduction**  \nLet \\( X \\) represent a real-valued random function defined on a probability space \\( (\\Omega, \\mathcal{A}, P) \\). In various applications, such as signal filtering and econometrics, it is often desirable to extract the distribution of \\( X \\), denoted as \\( f_X \\). For instance, in order to detect changes in the statistical properties of \\( X \\) over time, understanding \\( f_X \\) is essential for conducting change-point detection tests, as demonstrated in works by Chen et al. (2013) and Fryzlewicz & Subba Rao (2014). However, estimating \\( f_X \\) can be challenging since only \\( n \\) independent and identically distributed observations \\( X_1, \\ldots, X_n \\) of \\( X \\) are available. To address this issue, various studies have proposed nonparametric techniques that rely on kernel smoothing methods (see, e.g., Silverman (1981), Wand & Jones (1995)). Specifically, we define a kernel function \\( K: \\mathbb{R} \\to [0, 1] \\) that meets certain regularity conditions, to be outlined later. The traditional kernel density estimator of \\( f_X \\) at a point \\( x \\in \\mathbb{R}^d \\) is then given by:\n\n\\[\n\\hat{f}_K(x) = \n\\]",
        "ori-fast-z-score": -0.5619514869490164,
        "water-fast-z-score": 5.315277406969789,
        "rewrite-fast-z-score": 1.1141720290623112
    },
    {
        "original_text": "We present the first results on black hole accretion using our new numerical scheme, which is based on an implicit-explicit time integration method and uses adaptive mesh refinement (AMR). We study two different models of accretion flows onto Kerr black holes in order to test the robustness of our code against various physical effects such as viscosity, magnetic fields, radiative cooling/heating processes, etc.. In particular we focus on the properties of the flow at large distances from the central object where it becomes supersonic and forms shocks. Our main goal here was to check whether these features are correctly captured by our AMR code. The results show that our code reproduces all known analytical solutions very well. \n \n Keywords: Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time-dependent simulations \n \n \n \n 1 Introduction \n \n It has been more than 30 years since the discovery of quasars  1  . Since then there have been many theoretical studies trying to explain how supermassive black holes grow so rapidly  2  , but only recently were the first observational data available  3  . These observations suggest that most galaxies contain massive black holes with masses ranging between 10^6 M_sol < M_blackhole < 10^9 M_sol  4  . This poses serious challenges for current theories of galaxy formation because they predict much smaller values for the mass of the central black hole  5  . \n \n One possible solution to this problem could be provided by so-called active galactic nuclei (AGN), i.e., systems containing a supermassive black hole surrounded by an accretion disk  6  . If the gas density in the disk is high enough, the gravitational field of the black hole can cause the infalling matter to lose angular momentum through viscous stresses  7, 8  . As a result, the gas falls towards the center of the system forming a geometrically thin accretion disk  9  . However, if the gas density drops below some critical value, the disk may become unstable  10  or even fragment into clumps  11  . Such instabilities lead to the development of large-scale",
        "watermark_text": "We publish the first findings on dark hole accretion use our new numerical system , which is based on an implicit - explicit time integration approach and using adaptive mesh refinement ( AMR ) . We research two different models of accretion flows onto Kerr white holes in order to test the robustness of our code against several physical effects such as viscosity , magnetic fields , radiative cooling / cooling systems , etc . .In particular we focus on the properties of the flow at large distances from the main object where it becomes supersonic and shapes shocks . Our main goal here was to examine whether these characteristics are correctly captured by our AMR code .The results show that our code reproduces all known theoretical solutions very best . Keywords : Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time - dependent simulations 1 Introduction It has been more than 30 centuries since the discovery of quasars 1 .Since then there have been many theoretical researchers trying to explain how supermassive black holes expand so quickly 2 , but only lately were the first observational data available 3 . These measurements suggest that most objects possess massive brown holes with masses ranging between 10 ^ 6 M _ sol < M _ blackhole < 10 ^ 9 M _ sol 4 .This poses serious difficulties for recent predictions of galaxy formation because they predict far lower values for the mass of the main white hole 5 . One potential answer to this question could be provided by so - called active galactic nuclei ( AGN ) , i . e . , structures featuring a supermassive black hole accompanied by an accretion ring 6 .If the gas density in the disk is high enough , the gravitational field of the dark hole can cause the infalling matter to lose angular velocity through viscous stresses 7 , 8 . As a result , the gas drops towards the center of the system producing a geometrically thin accretion cone 9 .However , if the gas density decreases below some essential value , the disk might turn volatile 10 or maybe fragment into clumps 11 . Such instabilities lead to the development of large - scale",
        "rewrite_text": "We present the initial findings on black hole accretion using our new numerical system, which employs an implicit-explicit time integration method combined with adaptive mesh refinement (AMR). Our investigation explores two distinct models of accretion flows onto Kerr black holes to assess the resilience of our code against various physical phenomena, including viscosity, magnetic fields, and radiative cooling processes. Specifically, we concentrate on the properties of the flow at significant distances from the central object, where it becomes supersonic and generates shock waves. The primary objective of this work was to determine if these characteristics are accurately captured by our AMR code. The results demonstrate that our code effectively reproduces all known theoretical solutions. \n\n**Keywords:** Black holes - General relativity - Numerical methods - Shocks - Supersonic turbulence - Time-dependent simulations \n\n**1. Introduction**  \nIt has been over 30 centuries since the discovery of quasars. Since then, numerous theoretical researchers have attempted to explain how supermassive black holes can grow so rapidly, but observational data has only recently become available. These measurements imply that most objects contain massive black holes with masses in the range of \\( 10^6 M_{\\text{sol}} < M_{\\text{black hole}} < 10^9 M_{\\text{sol}} \\). This creates substantial challenges for current predictions of galaxy formation, which forecast significantly lower masses for these central black holes. One possible explanation lies in the concept of active galactic nuclei (AGN), which are structures hosting a supermassive black hole alongside an accretion disk. If the gas density in the disk is sufficiently high, the gravitational pull of the black hole can induce the infalling matter to lose angular momentum due to viscous stresses. Consequently, the gas spirals inward, forming a geometrically thin accretion cone. Conversely, if the gas density falls below a critical threshold, the disk may become unstable or even fragment into clumps. Such instabilities can trigger the evolution of large-scale structures.",
        "ori-fast-z-score": 0.8835412617927487,
        "water-fast-z-score": 9.397666148159237,
        "rewrite-fast-z-score": 2.0816659994661326
    },
    {
        "original_text": "We present results for angular distributions in the photoionization process of H-, He+ and Li2+ by circularly polarized photons at different energies. The calculations are performed within the framework of relativistic distorted wave theory using an accurate numerical method to solve the Dirac equation with Coulomb potential. We show that our theoretical predictions agree well with available experimental data. In addition we have studied the influence of nuclear spin effects on these observables. Finally, we discuss how this information can be used as a tool to determine the fine structure constant. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium provided that the original work is properly cited. \n \n Two-photon ionization plays an important role in many physical processes such as laser-matter interaction or astrophysical phenomena like stellar winds. It has been shown recently that it also constitutes one of the most promising methods to measure the fine-structure constant α  1  . For example, the measurement of the ratio between the cross sections corresponding to transitions into n=2 and n=3 states of heliumlike ions provides a determination of α with relative uncertainty below 10 −6  2  .\n \nIn order to perform precise measurements of the fine-structure constant through twophoton ionization experiments, it is necessary to understand theoretically all relevant aspects involved in the process. Among them, the study of the angular dependence of the emitted electrons represents a key issue since it allows us to discriminate among different contributions coming from different parts of the atomic spectrum  3  . Moreover, the comparison between experiment and theory requires high accuracy both in the calculation of the total cross section and its angular distribution  4  . \n \n In recent years there has been considerable progress in the development of computational techniques able to provide highly accurate results for the total cross section  5  , but only few works  6  -  8  have addressed the problem of calculating the angular distribution of the emitted electron. Most of those previous investigations were carried out within the nonrelativistic regime where the final state was described by means of the Schr",
        "watermark_text": "We present results for angular distributions in the photoionization process of H - , He + and Li2 + by circularly polarized photons at different energies . The calculations are performed within the framework of relativistic twisted wave theory employing an accurate numerical technique to correct the Dirac integral with Coulomb potential .We see that our theory estimates agree well with existing experimental evidence . In addition we have researched the impact of nuclear spin effects on these observables .Finally , we talk how this data can be used as a tool to obtain the fine structure constant . This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http : / / creativecommons . org / patents / by / 3 . 0 ) , which allows unrestricted sale , distribution , and reproduction in any medium provided that the original piece is properly cited .Two - photon ionization serves an important role in different physical processes such as laser - matter collision or astrophysical processes like stellar winds . It has been shown lately that it also constitutes one of the most attractive approaches to measure the fine - structure constant α 1 .For instance , the observation of the proportion between the cross sections corresponding to transitions into n = 2 and n = 3 states of heliumlike atoms provides a calculation of α with relative uncertainty below 10 −6 2 . In order to conduct accurate measurements of the fine - structure constant through twophoton ionization tests , it is required to explain theoretically all relevant details used in the process .Among them , the observation of the angular dependence of the emitted particles represents a key issue since it allows us to discriminate among different contributions come from different areas of the atomic spectrum 3 . Moreover , the comparison between experiment and theory requires large accuracy both in the determination of the total cross area and its angular distribution 4 .In recent years there has been substantial development in the development of computational approaches able to provide highly precise conclusions for the total cross area 5 , but only few papers 6 - 8 have treated the issue of calculating the angular distribution of the emitted particle . Most of those previous investigations were carried out within the nonrelativistic regime where the last state was described by means of the Schr",
        "rewrite_text": "We present findings on the angular distributions involved in the photoionization of H^-, He^+, and Li2+ using circularly polarized photons at various energy levels. These calculations are based on relativistic twisted wave theory and employ a precise numerical method to correct the Dirac integral in the presence of Coulomb potential. Our theoretical estimates show a strong agreement with existing experimental data. Additionally, we have investigated the influence of nuclear spin effects on these measurements. Finally, we discuss how these results can serve as a means to determine the fine structure constant. This article is published as Open Access under the Creative Commons Attribution License (http://creativecommons.org/patents/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. \n\nTwo-photon ionization plays a crucial role in various physical phenomena, including laser-matter interactions and astrophysical processes such as stellar winds. Recent studies have indicated that it is a promising method for measuring the fine structure constant, α. For example, the ratio of the cross sections for transitions to the n = 2 and n = 3 states in helium-like atoms can yield a calculation of α with a relative uncertainty below 10^-6. Accurate measurements of the fine structure constant via two-photon ionization require a comprehensive theoretical explanation of all relevant details in the process. Among these, the angular dependence of the emitted particles is critical, as it allows for distinguishing contributions from different regions of the atomic spectrum. Furthermore, comparing experimental results with theoretical predictions necessitates high precision in both the total cross-section determination and its angular distribution. In recent years, significant advancements have been made in computational techniques capable of yielding highly accurate results for the total cross-section; however, few studies have addressed the angular distribution of emitted particles. Most prior research has focused on the nonrelativistic regime, using the Schrödinger equation to describe the final state.",
        "ori-fast-z-score": 0.5291502622129182,
        "water-fast-z-score": 9.24879613166262,
        "rewrite-fast-z-score": 0.5416762627738958
    },
    {
        "original_text": "We present an analysis of the fundamental physical parameters (mass, luminosity and size) for a sample of young star clusters in the nearby spiral galaxy M33 using HST/ACS data. We find that these objects are consistent with being gravitationally bound open clusters or associations. The mass distribution is well described by a power law dN/dM ~ M^{-3} between 10^6 to 5 x 10^7 solar masses. This result suggests that cluster formation proceeds hierarchically on all scales within this range. In addition we find evidence for two distinct populations of massive clusters; one population has ages less than 100 Myr while another older population appears to be coeval at ages greater than 300 Myr. These results suggest that there may have been multiple episodes of intense cluster formation over the past few hundred million years. Finally, we compare our observations to theoretical models of cluster evolution and find good agreement when assuming a Kroupa IMF.",
        "watermark_text": "We present an assessment of the fundamental physical factors ( mass , luminosity and size ) for a sample of young galaxy galaxies in the nearby spiral galaxy M33 utilizing HST / ACS data . We see that these objects are compatible with being gravitationally locked open complexes or associations .The mass distribution is well described by a power law dN / dM ~ M ^ { - 3 } between 10 ^ 6 to 5 x 10 ^ 7 solar masses . This result suggests that cluster structure proceeds hierarchically on all scales within this range .In addition we find data for two separate populations of large clusters ; one group has ages less than 100 Myr while another older population seems to be coeval at ages greater than 300 Myr . These data suggest that there may have been multiple cycles of aggressive cluster structure over the previous few hundred million years .Finally , we compare our observations to theoretical theories of cluster evolution and find good agreement when assuming a Kroupa IMF .",
        "rewrite_text": "We provide an evaluation of key physical factors—mass, luminosity, and size—for a sample of young star clusters in the nearby spiral galaxy M33, using data from the HST/ACS. Our findings indicate that these objects may be gravitationally bound open complexes or associations. The mass distribution is effectively characterized by a power law, dN/dM ~ M^{-3}, within the range of 10^6 to 5 x 10^7 solar masses. This observation implies that cluster formation occurs hierarchically across all scales in this range. Furthermore, we identify two distinct populations of large clusters: one with ages under 100 Myr and another older population that appears coeval, with ages exceeding 300 Myr. These results imply that there have likely been multiple cycles of intense cluster formation over the past few hundred million years. Lastly, we compare our findings with theoretical models of cluster evolution and observe strong alignment when applying a Kroupa Initial Mass Function (IMF).",
        "ori-fast-z-score": 0.23249527748763857,
        "water-fast-z-score": 5.427092530382482,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We present an analytic solution to the steady state distribution for the mechanistic home-range model developed by Moorcroft et al. (2006) that allows for efficient computation of home ranges using numerical integration methods. The new method is implemented as part of the R package adehabitatHR, which also includes functions for computing home ranges with the original algorithm (i.e., without the analytical solution). We demonstrate how our approach can be used to rapidly compute home ranges across large landscapes containing thousands of habitat patches. Our results show that the new method produces identical estimates compared to those obtained with the original algorithm but requires less computational time when estimating home ranges over large spatial extents. Analytical solutions are useful because they allow researchers to efficiently estimate home ranges on very large datasets or at fine resolutions. \n \n Home ranges have been widely studied since their introduction into ecology more than 50 years ago  1  . These areas represent the area within which individuals obtain all necessary resources  2  , such as food  3  , water  4  , shelter  5  , mates  6  , and cover  7  . In addition to being important for understanding animal behavior  8  , home ranges play key roles in conservation biology  9  , wildlife management  10  , epidemiology  11  , and disease transmission  12  .\n \nHome-range models typically assume that animals move through a landscape composed of discrete habitat patches  13  . Animals select among these patches based on some combination of patch attributes  14  , including resource availability  15  , vegetation structure  16  , predation risk  17  , and conspecific density  18  . This process continues until the animal reaches equilibrium between its movement rate and the quality of available habitats  19  . \n \n A number of different approaches exist for modeling animal movements  20  . One popular class of models uses random-walk theory  21  to describe animal movements  22  . Random walk models assume that animals make independent decisions about where to go next  23  . However, this assumption may not always hold true  24  . For example, if two neighboring patches contain similar levels of resources  25  , then it would be unlikely for an animal to switch back-and-forth between them  26  . To account for this type of behavioral response, Moorcro",
        "watermark_text": "We present an analytic solution to the stable state distribution for the mechanistic home - range system established by Moorcroft et al . ( 2006 ) that enables for efficient computation of home ranges using numerical integration methods .The new method is implemented as part of the R program adehabitatHR , which also contains functions for modeling home ranges with the previous algorithm ( i . e . , without the empirical approach ) . We suggest how our approach can be used to rapidly compute bedroom ranges across large landscapes containing thousands of environment patches .Our results show that the new method generates similar estimates compared to those achieved with the previous algorithm but requires fewer computational time when estimating bedroom ranges over large geographic extents . Analytical approaches are helpful because they allow scientists to easily assess home ranges on very huge datasets or at fine resolutions .Home ranges have been widely examined since their arrival into ecosystems more than 50 weeks ago 1 . These zones represent the territory within which adults obtain all necessary resources 2 , such as feed 3 , water 4 , protection 5 , mates 6 , and cover 7 .In addition to being important for explaining animal behavior 8 , home ranges represent crucial roles in wildlife dynamics 9 , conservation conservation 10 , epidemiology 11 , and illness transmission 12 . Home - range systems often assume that animals go through a landscape composed of linear habitat patches 13 .Animals select among these patches based on some mix of patch traits 14 , covering habitat availability 15 , vegetation system 16 , predation risk 17 , and conspecific density 18 . This process proceeds until the organism reaches stability between its movement rate and the performance of available environments 19 .A variety of different methods exist for modeling animal activities 20 . One popular family of models using random - walk models 21 to explain animal activities 22 .Random step models believe that individuals give independent choice about where to going next 23 . However , this assumption must not always hold false 24 .For instance , if two adjacent regions contain similar rates of assets 25 , then it would be impossible for an organism to shift back - and - forth between them 26 . To account for this form of behavioral reaction , Moorcro",
        "rewrite_text": "We propose an analytical solution for the stable state distribution within the mechanistic home-range system developed by Moorcroft et al. (2006), facilitating efficient computation of home ranges through numerical integration methods. This new technique has been integrated into the R package adehabitatHR, which also includes functions for modeling home ranges using the earlier algorithm (i.e., the non-empirical approach). We demonstrate how our method can quickly calculate home ranges across extensive landscapes featuring thousands of environmental patches. Our findings indicate that the new method produces estimates comparable to those derived from the previous algorithm but requires significantly less computational time when calculating home ranges over vast geographic areas. Analytical approaches are invaluable as they enable researchers to effectively evaluate home ranges within large datasets or at high resolutions. Home ranges have been extensively studied since their introduction into ecological research over 50 years ago. These areas represent the territories where adult animals access essential resources, such as food, water, shelter, mates, and cover. Beyond their significance in understanding animal behavior, home ranges play critical roles in wildlife dynamics, conservation, epidemiology, and disease transmission. Home-range models typically assume that animals navigate through landscapes made up of linear habitat patches, making selections based on a combination of patch characteristics, habitat availability, vegetation types, predation risks, and conspecific densities. This selection process continues until a balance is achieved between the animal's movement rate and the performance of the available environments. Various methods exist for modeling animal movement, with one popular approach employing random-walk models to describe animal behavior. Random step models assume that individuals make independent choices about their next movement. However, this assumption may not always hold true; for example, if two adjacent areas have similar resource availability, an organism may find it difficult to move back and forth between them. To address this type of behavioral response, Moorcroft...",
        "ori-fast-z-score": -1.7230995806825715,
        "water-fast-z-score": 10.625780747542525,
        "rewrite-fast-z-score": 2.0207259421636903
    },
    {
        "original_text": "The fitting problem is one of the most important problems in cosmology, which has been studied for more than 50 years.  The main goal of this work was to study the effect of different types of initial conditions on the evolution of density perturbations in an expanding universe with a positive cosmological constant (dark energy).  We have used two methods to solve numerically the Einstein equations coupled to the matter fields:  the standard method based on the expansion of the metric tensor into spherical harmonics; and the new method developed by us that uses the expansion of the metric into Chebyshev polynomials.  In both cases we have considered three types of initial conditions: Gaussian random fields generated using the Zel dovich approximation, Gaussian random fields generated using a fast Fourier transform algorithm, and white noise.  Our results show that all these models give similar predictions at late times when they are evolved up to z = 0.5.  However, there are some differences between them at early times.  These differences can be explained as follows:  the Zel dovich model predicts larger fluctuations compared to other models because it does not take into account the pressure term in the fluid equation; the white noise model gives smaller fluctuations due to its statistical properties; finally, the Gaussian random fields obtained via the fast Fourier transform algorithm predict intermediate values.  This result shows that the choice of the initial conditions may affect significantly the final value of the power spectrum of primordial density fluctuations predicted by inflationary theories.",
        "watermark_text": "The fit problem is one of the most important problems in cosmology , which has been studied for more than 50 centuries . The main goal of this research was to study the impact of different kinds of initial conditions on the evolution of density perturbations in an evolving galaxy with a positive cosmological constant ( darkness energy ) .We have utilized two means to solve numerically the Einstein equations coupled to the matter fields : the standard method based on the contraction of the metric tensor into spherical harmonics ; and the new method developed by us that using the contraction of the metric into Chebyshev polynomials . In both cases we have analyzed three sorts of initial conditions : Gaussian random fields generated using the Zel dovich approximation , Gaussian random fields generated utilizing a slow Fourier integral method , and gray noise .Our results show that all these models make comparable predictions at late times when they are evolved up to z = 0 . 5 . However , there are some variations between them at early periods .These changes can be described as follows : the Zel dovich theory predicts larger fluctuations compared to other models because it does not take into consideration the pressure term in the liquid equation ; the red noise model gives larger fluctuations owing to its statistical characteristics ; finally , the Gaussian random fields obtained via the fast Fourier shift technique achieve intermediate values . This result suggests that the selection of the early conditions might impact significantly the finished value of the power spectrum of primordial density fluctuations suggested by inflationary theories .",
        "rewrite_text": "The fit problem is a critical issue in cosmology that has been explored for over five millennia. This research primarily aimed to investigate how various initial conditions influence the development of density perturbations in a galaxy evolving under a positive cosmological constant (dark energy). We employed two numerical methods to solve the Einstein equations in conjunction with matter fields: the conventional method utilizing the contraction of the metric tensor into spherical harmonics and a novel method developed by us that uses Chebyshev polynomials for metric contraction. In our analysis, we examined three types of initial conditions: Gaussian random fields created using the Zel'dovich approximation, Gaussian random fields generated through a slow Fourier integral approach, and gray noise. Our findings indicate that, at later times (up to z = 0.5), all models yield comparable predictions. However, discrepancies arise during earlier epochs. Specifically, the Zel'dovich approach predicts greater fluctuations, as it neglects the pressure term in the fluid equations; the red noise model exhibits larger fluctuations due to its statistical properties; and the Gaussian random fields produced via the fast Fourier shift technique yield intermediate results. These outcomes suggest that the choice of initial conditions may significantly influence the final power spectrum of primordial density fluctuations posited by inflationary theories.",
        "ori-fast-z-score": 0.18107149208503706,
        "water-fast-z-score": 7.181818181818182,
        "rewrite-fast-z-score": 2.116950987028628
    },
    {
        "original_text": "We present new methods to determine the mass-loss rate in evolved stars (AGB) using radio observations at centimeter wavelengths, and compare these results with those obtained by infrared dust emission measurements. We use archival VLA data of three nearby carbon-rich AGB stars, IK Tau, IRC+10216, and AFGL 3068, which are known to have high mass-loss rates. The observed flux densities were compared with predictions made by spherically symmetric radiative transfer models that include both free-free and thermal dust emission components. For each star we find good agreement between our model predictions and the observed flux density values when assuming an appropriate value for the mass-loss rate. Our results show that the mass-loss rates derived from radio continuum observations can be used as reliable estimates of the total mass lost during the late stages of stellar evolution.  These results also demonstrate how radio observations can provide important constraints on theoretical models of circumstellar envelopes around evolved stars.",
        "watermark_text": "We introduce novel techniques to estimate the mass - loss rate in evolution stars ( AGB ) using radio observations at centimeter wavelengths , and compare these results with those achieved by infrared dust absorption observations . We use archival VLA information of three distant carbon - rich AGB stars , IK Tau , IRC + 10216 , and AFGL 3068 , which are known to have high mass - loss rates .The observed flux densities were compared with predictions provided by spherically symmetric radiative transfer methods that include both free - free and thermal dust radiation parts . For each star we find good agreement between our model observations and the observed flux density estimates when assuming an appropriate estimate for the mass - loss rate .Our results show that the mass - loss rates derived from radio continuum measurements can be used as reliable estimates of the total mass losses during the last phases of stars evolution . These data even demonstrate how radio observations can provide important restrictions on theoretical theories of circumstellar envelopes around evolved galaxies .",
        "rewrite_text": "We present new techniques for estimating the mass-loss rates in asymptotic giant branch (AGB) stars using radio observations at centimeter wavelengths, and we compare these findings with results obtained from infrared dust absorption observations. Utilizing archival Very Large Array (VLA) data for three distant carbon-rich AGB stars—IK Tau, IRC + 10216, and AFGL 3068—known for their significant mass-loss rates, we analyzed the observed flux densities against predictions from spherically symmetric radiative transfer models that account for both free-free and thermal dust radiation. For each star, we found a strong correlation between our modeled observations and the measured flux density estimates, given an appropriate mass-loss rate estimation. Our results indicate that radio continuum measurements can serve as reliable indicators of total mass loss during the final phases of stellar evolution. Furthermore, this research illustrates how radio observations can impose valuable constraints on theoretical models of circumstellar envelopes surrounding evolved stars.",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": -0.2182178902359924
    },
    {
        "original_text": "We report on the observation of electronic spin transport across macroscopic distances (several millimeters) in suspended single-layer graphene devices at room temperature.  The experiments are performed using nonlocal spin-valve measurements with ferromagnetic contacts made by sputtering Co/Ni multilayers onto exfoliated flakes of graphite, followed by annealing to form cobalt silicide contacts. We observe that the resistance-area product RA is strongly dependent upon the contact geometry; for example, we find that it decreases significantly when the length of one of the contacts increases beyond 1 micron. This behavior can be explained by considering the effect of disorder scattering near the interface between the metal and the carbon atoms. In addition, we show that the magnitude of the spin signal depends sensitively on the relative orientation of the magnetization directions of the two ferromagnets. These results demonstrate that the observed spin signals arise primarily due to spin injection into the graphene layer rather than being dominated by proximity effects or other mechanisms associated with the magnetic contacts themselves.",
        "watermark_text": "We report on the observation of electronic spin travel across macroscopic distances ( several millimeters ) in suspended single - substrate graphene structures at room temperature . The studies are performed using nonlocal spin - valve measurements with ferromagnetic contacts produced by sputtering Co / Ni multilayers onto exfoliated flakes of graphite , followed by annealing to form cobalt silicide links .We recognize that the tolerance - area product RA is strongly dependent upon the contact shape ; for example , we find that it decreases dramatically when the length of one of the contacts increases beyond 1 micron . This activity can be described by examining the impact of disorder absorption near the interface between the metal and the metal atoms .In addition , we find that the magnitude of the spin signal relies sensitively on the relative attitude of the magnetization directions of the two ferromagnets . These data demonstrate that the known spin signals arise principally due to spinning injection into the graphene layer instead than being dominated by proximity effects or other mechanisms associated with the magnetic links themselves .",
        "rewrite_text": "We present our findings on the observation of electronic spin propagation over considerable distances (several millimeters) in suspended single-substrate graphene structures at room temperature. Our experiments utilized nonlocal spin-valve measurements with ferromagnetic contacts created by sputtering Co/Ni multilayers onto exfoliated graphite flakes, followed by annealing to form cobalt silicide connections. We note that the tolerance-area product (RA) is significantly influenced by the contact geometry; specifically, we observe a dramatic decrease when the length of one contact exceeds 1 micron. This behavior can be understood by analyzing the effects of disorder absorption at the interface between the metal and the graphene. Furthermore, we find that the strength of the spin signal is highly sensitive to the relative orientation of the magnetization directions of the two ferromagnetic contacts. Our results indicate that the observed spin signals primarily arise from spin injection into the graphene layer, rather than being predominantly influenced by proximity effects or other interactions related to the magnetic contacts themselves.",
        "ori-fast-z-score": -0.7977240352174656,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "We present new near-infrared (NIR) polarimetric observations of the GG Tau system, which reveal that its circumstellar disk is highly structured and contains several bright regions with different polarization properties. The most prominent feature in our data set is an arc-like structure located at about 0.5 arcsec to the south-east of the central binary star. This region shows strong polarized emission up to 10% of the total intensity and has been previously identified as a reflection nebula by Weintraub et al. (1993) . We find that this feature can be explained by scattering off optically thin dust grains close to the midplane of the disk. In addition we detect two other bright features on either side of the central binary. These are also associated with high degrees of linear polarization but show no clear evidence for scattered light. Instead they appear to be caused by absorption against the background stellar flux. Finally, we identify three additional fainter structures in the southern part of the disk. All these features have similar polarization angles indicating that their origin may be related.",
        "watermark_text": "We present new near - infrared ( NIR ) polarimetric studies of the GG Tau component , which confirm that its circumstellar disk is heavily compressed and features several bright regions with various polarization properties . The most notable feature in our information pool is an arc - like structure located at about 0 . 5 arcsec to the south - eastward of the main binary star .This region shows intense polarized emission up to 10 % of the total intensity and has been previously described as a mirror nebula by Weintraub et al . ( 1993 ) .We see that this phenomenon can be described by scattering off optically thin dust grains next to the midplane of the disk . In addition we locate two other bright features on either front of the main binary .These are also associated with high levels of linear polarization but display no clear proof for dispersed light . Rather they appear to be caused by absorption against the background stellar flow .Finally , we identify three extra fainter objects in the southern portion of the disk . All these structures have parallel polarization angles indicating that their source may be connected .",
        "rewrite_text": "We report new near-infrared (NIR) polarimetric observations of the GG Tau component, revealing that its circumstellar disk is significantly compressed and contains several bright regions with varying polarization characteristics. A standout feature in our findings is an arc-like structure located approximately 0.5 arcseconds southeast of the main binary star. This area exhibits strong polarized emission, reaching up to 10% of the total intensity, and has previously been referred to as a mirror nebula by Weintraub et al. (1993). Our analysis suggests that this phenomenon can be attributed to scattering from optically thin dust grains near the midplane of the disk. Additionally, we identify two other bright features flanking the main binary, which are also linked to high levels of linear polarization but show no definitive evidence of dispersed light; instead, they seem to result from absorption against the background stellar flow. Lastly, we detect three fainter objects in the southern portion of the disk. All of these structures exhibit parallel polarization angles, suggesting they may be interconnected.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 6.846754616640485,
        "rewrite-fast-z-score": 1.9126494315742406
    },
    {
        "original_text": "We report on five new planets discovered by the NASA K2 mission, which were found in the sample of targets observed during Campaigns 1 and 2 (C1/K2). The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years. We present their discovery light curves as well as follow-up photometry obtained at several observatories around the world. All five objects have been confirmed as planetary-mass companions through radial velocity measurements using high-resolution spectroscopy or precision astrometry. \n \n Keywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby stars - TESS - PLATO - HARPS-N - SPECULOOS \n \n \n \n Five intermediate-period planets from the N2K sample \nThe NASA Kepler space telescope has revolutionized our understanding of extrasolar planets over its primary mission that lasted for four years . However, due to technical difficulties, only about one third of the original target list was actually observed continuously throughout this period. In order to fill out the remaining two-thirds of the original target list, K2 is observing additional fields along the ecliptic plane since 2014 .\nIn this work we report on five new planets detected by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1/K2 ) . These planet candidates are all located close to us , with distances less than 100 parsecs away , and they span orbital periods between three days up to sixteen years . Their masses range from 0 . 5 to 4 times Jupiter  s mass .  \n \n We present here the discovery light curves together with followup photometric observations performed at various observatories worldwide . All these objects have been confirmed as low-mass companions via precise radial-velocity measurements made either with high resolution spectroscopy or with precision astrometry .",
        "watermark_text": "We report on five new planets discovered by the NASA K2 flight , which were found in the sample of targets observed during Campaigns 1 and 2 ( C1 / K2 ) . The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 days .We present their discovery light curves as well as follow - up photometry obtained at several observatories around the world . All five objects have been confirmed as planetary - mass companions through radial speed measurements involving high - resolution spectroscopy or precision astrometry .Keywords : Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby galaxies - TESS - PLATO - HARPS - N - SPECULOOS Five intermediate - time planets from the N2K survey The NASA Kepler space telescope has revolutionized our knowing of extrasolar stars over its primary mission that lasted for four seasons . However , owing to technical problems , only about one third of the actual target list was actually seen continuously throughout this time .In order to fill out the remaining two - half of the original target roster , K2 is monitoring extra fields along the ecliptic plane since 2014 . In this research we publish on five new objects discovered by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1 / K2 ) .These planet candidates are all located close to us , with distances fewer than 100 parsecs apart , and they span orbital periods between three weeks up to fourteen months . Their masses range from 0 .5 to 4 times Jupiter s mass . We present here the discovery light curves combined with followup photometric surveys performed at numerous observatories worldwide .All these objects have been confirmed as low - weight companions via accurate radial - speed measurements made either with high resolution spectroscopy or with accuracy astrometry .",
        "rewrite_text": "We present findings on five newly discovered planets identified by the NASA K2 mission from the target samples observed during Campaigns 1 and 2 (C1/K2). These planet candidates are all situated within 100 parsecs of Earth, with orbital periods ranging from 3 to 16 days. We provide their discovery light curves along with follow-up photometry collected from several observatories around the globe. All five objects have been confirmed as planetary-mass companions through radial velocity measurements conducted using high-resolution spectroscopy or precision astrometry. \n\nKeywords: Planetary systems, Discovery methods, Radial velocities, Astrometry, Transits, Exoplanet, K2 Mission, Nearby galaxies, TESS, PLATO, HARPS, N-SPECULOOS.\n\nIn addition to this, the NASA Kepler space telescope has transformed our understanding of extrasolar stars during its primary mission, which lasted four years. However, due to technical difficulties, only about one-third of the initial target list was continuously monitored during that time. To address the gaps in the original roster, K2 has been observing additional fields along the ecliptic plane since 2014. In this study, we report on five new planets discovered by K2 within the observations of Campaigns 1 and 2 (C1/K2). These candidates are all relatively nearby, situated less than 100 parsecs away, and exhibit orbital periods ranging from three to fourteen days. Their masses vary from 0.5 to 4 times that of Jupiter. We share the discovery light curves along with follow-up photometric data collected from various observatories worldwide. All objects have been confirmed as lightweight companions through precise radial velocity measurements obtained via high-resolution spectroscopy or accurate astrometry.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 7.423745685299301,
        "rewrite-fast-z-score": 1.4237369936287485
    },
    {
        "original_text": "We present an improved abundance determination for the black hole binary nova Sco X-1, based on high-resolution optical spectroscopy obtained with UVES at VLT-UT2 in November 2004 and January 2005. The new data are combined with previously published results to derive abundances for CNO elements as well as FeI and FeII lines. We find that our best-fit model is consistent with previous studies within their uncertainties. However, we obtain significantly lower values for carbon and oxygen than those reported by Gies & Bolton (1986) . This discrepancy may be due to differences between the adopted atmospheric models or atomic data used in these two analyses. \n \n Keywords: Black holes - Abundance ratios - X-ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar winds - Mass transfer -X-ray emission - Accretion disks - Novae - Supernovae",
        "watermark_text": "We report an updated abundance calculation for the dark hole binary nova Sco X - 1 , using on high - resolution optical spectroscopy achieved with UVES at VLT - UT2 in November 2004 and January 2005 . The revised data are coupled with former reported results to derive abundances for CNO objects as well as FeI and FeII lines .We see that our better - fitting model is compatible with previous research within their uncertainties . However , we obtain significantly reduced estimates for carbon and oxygen than those observed by Gies & Bolton ( 1986 ) .This discrepancy may be due to differences between the preferred atmospheric models or atomic data used in these two analyses . Keywords : Black holes - Abundance ratios - X - ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar weather - Mass transmission - X - ray radiation - Accretion disks - Novae - Supernovae",
        "rewrite_text": "We present an updated abundance calculation for the black hole binary Nova Sco X-1, utilizing high-resolution optical spectroscopy obtained with UVES at the VLT-UT2 during November 2004 and January 2005. This revised dataset is integrated with previously reported findings to assess abundances of CNO elements, as well as FeI and FeII lines. Our improved fitting model aligns well with earlier studies within their respective uncertainties; however, we derive significantly lower estimates for carbon and oxygen compared to those reported by Gies & Bolton (1986). This variation may be attributed to differences in the atmospheric models or atomic data utilized in the two studies. \n\nKeywords: Black holes - Abundance ratios - X-ray binaries - Spectroscopy - Ultraviolet space observatories - Variability - Velocity fields - Stellar atmospheres - Mass transfer - X-ray radiation - Accretion disks - Novae - Supernovae.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 5.3452248382484875,
        "rewrite-fast-z-score": -0.39735970711951313
    },
    {
        "original_text": "Carbon nanotubes (CNTs) are promising materials for field emission devices due to their unique physical and chemical properties, such as high aspect ratio, low work function, and excellent mechanical strength.  In this study, we present an integrated multiphysics model that can be used to simulate the system response of CNT-based field emitting diodes (FEDs). The proposed model consists of three sub-models: 1) electron transport in CNT; 2) electrostatic potential distribution; 3) current density distribution. We have developed these models using COMSOL Multiphysics software package with built-in physics modules. To verify our simulation results, we fabricated a CNT-FED device by growing vertically aligned CNTs onto silicon substrate via plasma-enhanced chemical vapor deposition method followed by sputtering gold film over them. Our experimental data show good agreement with simulated results obtained from the proposed model.",
        "watermark_text": "Carbon nanotubes ( CNTs ) are promising technologies for field emission materials due to their different mechanical and biological qualities , such as great aspect value , low work function , and good mechanical strength . In this study , we present an unified multiphysics description that can be used to simulate the process response of CNT - based field emitting diodes ( FEDs ) .The proposed theory involves of three sub - models : 1 ) electron transport in CNT ; 2 ) electrostatic potential distribution ; 3 ) power density flow . We have developed these models using COMSOL Multiphysics programming package with built - in physics elements .To obtain our modeling results , we fabricated a CNT - FED device by spreading vertically aligned CNTs onto silicon substrate via plasma - augmented molecular vapor deposition system followed by sputtering gold film over them . Our research data demonstrate excellent compliance with simulated findings obtained from the suggested model .",
        "rewrite_text": "Carbon nanotubes (CNTs) hold great promise as materials for field emission applications due to their unique mechanical and biological properties, including high aspect ratio, low work function, and strong mechanical strength. In this study, we introduce a comprehensive multiphysics framework designed to simulate the response of CNT-based field emission diodes (FEDs). This framework consists of three interconnected sub-models: (1) electron transport within CNTs, (2) electrostatic potential distribution, and (3) power density flow. We developed these models using the COMSOL Multiphysics software, which incorporates various physics elements. To validate our modeling results, we fabricated a CNT-FED device by vertically aligning CNTs on a silicon substrate using a plasma-augmented molecular vapor deposition system, followed by the sputtering of a gold film on top. Our experimental data show a strong correlation with the simulated results derived from our proposed model.",
        "ori-fast-z-score": -1.3764944032233704,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": 1.1322770341445956
    }
]