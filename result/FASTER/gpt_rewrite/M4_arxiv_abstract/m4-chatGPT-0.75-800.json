[
    {
        "original_text": "Epsilon Aurigae is an F-type main sequence star with a mass of 1.8 M☉ and radius 2 R☉, located at about 40 light-years away in the constellation Auriga.  It has been known for many years to be surrounded by dusty material that obscures its visible spectrum.   The infrared excess emission detected around this object suggests it may have a circumstellar disk similar to those found around young stars such as T Tauri or Herbig Ae/Be stars.   In addition, there are indications that the system contains a close companion which could also contribute to the observed infrared excess emission.    We present new photometric observations obtained using the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea over the period 1997-2001 covering wavelengths between 0.9-2.5 microns.  These data show significant variations in both the near-infrared fluxes and colours of the central source consistent with changes in the amount of dust surrounding the star.  This behaviour is very similar to what is seen in other pre-main-sequence systems where accretion onto the central star causes periodic increases in luminosity accompanied by increased levels of reddening due to heating of the surrounding dust grains.   Our results suggest that the current level of activity in the system is relatively low compared to previous epochs but we cannot rule out the possibility that the recent increase in brightness was caused by a short-lived burst of enhanced accretion rather than steady-state accretion occurring throughout our observing campaign.",
        "watermark_text": "Epsilon Aurigae is an F - class major sequence star with a mass of 1 . 8 M☉ and radius 2 R☉ , located at about 40 light - years away in the constellation Auriga . It has been known for thousands decades to be accompanied by dusty matter that obscures its visible spectrum .The infrared excess emission detected around this body suggests it could have a circumstellar disk comparable to those observed around young galaxies such as T Tauri or Herbig Ae / Be stars . In addition , there are indications that the system contains a close companion which could also contribute to the seen infrared excess emission .We report new photometric surveys obtained using the United Kingdom Infrared Telescope ( UKIRT ) on Mauna Kea over the period 1997 - 2001 covering wavelengths between 0 . 9 - 2 . 5 microns . These data demonstrate considerable variations in both the near - infrared fluxes and colours of the main source consistent with shifts in the quantity of dust surrounding the star .This behaviour is very related to what is seen in other pre - principal - sequence complexes where accretion onto the main star causes periodic increases in luminosity followed by increased levels of reddening due to heating of the nearby dust grains . Our results show that the present degree of intensity in the system is fairly lowest relative to previous epochs but we cannot leave out the idea that the recent rise in intensity was due by a brief - lived pulse of enhanced accretion instead than steady - phase accretion occurring throughout our observing mission .",
        "rewrite_text": "Epsilon Aurigae is a main-sequence star of F-class, with a mass of 1.8 M☉ and a radius of 2 R☉, located approximately 40 light-years away in the constellation Auriga. For many decades, it has been known to be surrounded by dusty material that obscures its visible light. The excess infrared emission detected in this star suggests the presence of a circumstellar disk similar to those found around young stellar objects like T Tauri and Herbig Ae/Be stars. Additionally, there are hints that the system hosts a close companion that may also contribute to the observed infrared excess. We present new photometric data collected from the United Kingdom Infrared Telescope (UKIRT) on Mauna Kea between 1997 and 2001, covering wavelengths from 0.9 to 2.5 microns. These observations reveal significant variations in both the near-infrared flux and color of the main star, indicating changes in the amount of surrounding dust. This behavior closely resembles that observed in other pre-main-sequence systems, where accretion onto the primary star leads to periodic luminosity increases followed by heightened reddening due to the heating of nearby dust grains. Our findings suggest that the current intensity level in the system is relatively low compared to earlier periods; however, we cannot dismiss the possibility that the recent rise in intensity was due to a short-lived burst of enhanced accretion rather than steady accretion throughout our monitoring period.",
        "ori-fast-z-score": -0.8466487815452375,
        "water-fast-z-score": 6.6791181655235405,
        "rewrite-fast-z-score": -1.034792955221957
    },
    {
        "original_text": "We present new near-infrared (NIR) polarimetric observations of the GG Tau system, which reveal that its circumstellar disk is highly structured and contains several bright regions with different polarization properties. The most prominent feature in our data set is an arc-like structure located at about 0.5 arcsec to the south-east of the central binary star. This region shows strong polarized emission up to 10% of the total intensity and has been previously identified as a reflection nebula by Weintraub et al. (1993) . We find that this feature can be explained by scattering off optically thin dust grains close to the midplane of the disk. In addition we detect two other bright features on either side of the central binary. These are also associated with high degrees of linear polarization but show no clear evidence for scattered light. Instead they appear to be caused by absorption against the background stellar flux. Finally, we identify three additional fainter structures in the southern part of the disk. All these features have similar polarization angles indicating that their origin may be related.",
        "watermark_text": "We present new near - infrared ( NIR ) polarimetric studies of the GG Tau component , which confirm that its circumstellar disk is heavily compressed and features several bright regions with various polarization properties . The most notable feature in our information pool is an arc - like structure located at about 0 . 5 arcsec to the south - eastward of the main binary star .This region shows intense polarized emission up to 10 % of the total intensity and has been previously described as a mirror nebula by Weintraub et al . ( 1993 ) .We see that this phenomenon can be described by scattering off optically thin dust grains next to the midplane of the disk . In addition we locate two other bright features on either front of the main binary .These are also associated with high levels of linear polarization but display no clear proof for dispersed light . Rather they appear to be caused by absorption against the background stellar flow .Finally , we identify three extra fainter objects in the southern portion of the disk . All these structures have parallel polarization angles indicating that their source may be connected .",
        "rewrite_text": "We present new near-infrared (NIR) polarimetric studies of the GG Tau component, confirming that its circumstellar disk is significantly compressed and exhibits several bright regions with varying polarization characteristics. One of the most striking features in our data is an arc-like structure located approximately 0.5 arcseconds southeast of the primary binary star. This region displays strong polarized emission, reaching up to 10% of the total intensity, and was previously referred to as a mirror nebula by Weintraub et al. (1993). We interpret this phenomenon as resulting from scattering off optically thin dust grains located near the midplane of the disk. Additionally, we identify two other bright features flanking the main binary. These also exhibit high levels of linear polarization but show no clear evidence of dispersed light; instead, they appear to be a result of absorption against the background stellar flow. Lastly, we recognize three additional fainter objects in the southern part of the disk. All these structures exhibit parallel polarization angles, suggesting a possible connection among their sources.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 6.846754616640485,
        "rewrite-fast-z-score": 1.6464638998453551
    },
    {
        "original_text": "We report on the fluidization of a thin, horizontally vibrating granular bed by vertical oscillations at low frequency and amplitude. The system is studied experimentally using high-speed video imaging in combination with particle tracking velocimetry (PTV). We find that for sufficiently large vibration amplitudes, particles are ejected from the surface into the air as they reach their maximum height during an upward motion. This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container. In addition we observe convection rolls which transport grains across the entire depth of the bed. These results show striking similarities to those observed in vibrated beds of sand or glass beads but differ significantly from previous studies performed on systems where only horizontal vibrations were applied. Vibrations can induce transitions between different states of matter such as solids, liquids, and gases  1  . For example, it has been shown that a solid state may become unstable when subjected to periodic forcing  2  , resulting in the spontaneous generation of traveling waves  3  . A particularly interesting case occurs if both horizontal and vertical components of the driving force act simultaneously  4  .\nIn this work we study the response of a thin granular layer to simultaneous application of horizontal and vertical vibrations. Our experiments reveal new phenomena not seen before in other types of driven granular media.",
        "watermark_text": "We report on the fluidization of a thin , vertically vibrating granular floor by vertical oscillations at low frequency and amplitude . The system is studied experimentally utilizing large - speed video scanning in combination with particle tracking velocimetry ( PTV ) .We see that for enough large vibration amplitudes , particles are expelled from the surface into the air as they reach their maximum size during an upward moving . This leads to the formation of a dilute gas phase above the dense packing near the bottom of the container .In addition we study circulation rolls which carry grains across the entire depth of the bed . These data demonstrate striking parallels to those observed in vibrated beds of dust or glass bottles but differ significantly from previous analyses performed on structures where only horizontal vibrations were applied .Vibrations can induce changes between various states of matter such as solids , liquids , and gases 1 . For instance , it has been shown that a solid state would remain weak when exposed to periodic forcing 2 , resulting in the spontaneous production of traveling particles 3 .A notably important case occurs if both horizontal and horizontal elements of the driving stress act simultaneously 4 . In this research we study the response of a thin granular layer to continuous application of longitudinal and horizontal vibrations .Our experiments discover new phenomena not seen before in other types of driven granular material .",
        "rewrite_text": "We present our findings on the fluidization of a thin, vertically vibrating granular surface achieved through low-frequency, low-amplitude vertical oscillations. This study employs high-speed video scanning alongside particle tracking velocimetry (PTV) for in-depth experimental analysis. Our observations reveal that sufficiently large vibration amplitudes cause particles to be thrust into the air as they reach their maximum altitude during upward vibrations. This phenomenon results in the creation of a dilute gas phase above the dense packing at the bottom of the container. Furthermore, we investigate circulation rolls that transport grains throughout the entire depth of the granular bed. Our results exhibit remarkable similarities to those seen in vibrated dust beds or glass containers, but they markedly differ from earlier studies focused solely on systems subjected to horizontal vibrations. Vibrations can trigger transitions among different states of matter, including solids, liquids, and gases. For example, it has been established that a solid state remains weak under periodic forcing, leading to the spontaneous generation of moving particles. A particularly significant scenario arises when both vertical and horizontal components of the applied stress operate simultaneously. In this research, we explore how a thin granular layer responds to the continuous application of both longitudinal and horizontal vibrations, unveiling new phenomena that have not been previously observed in other types of driven granular materials.",
        "ori-fast-z-score": 1.3112201362143716,
        "water-fast-z-score": 8.241955141918908,
        "rewrite-fast-z-score": 1.2018504251546631
    },
    {
        "original_text": "We present new near-infrared photometric data for the Pleiades open cluster obtained with the Infrared Survey Facility (IRSF) on the South African Astronomical Observatory s 1-m telescope at Sutherland in November 2005. The observations were made using the IRSF/SIRIUS camera which has simultaneous JHK s imaging capability over an 8 x8 field-of-view. We have also used archival 2MASS data to supplement our sample. Our results are presented as colour-magnitude diagrams (CMDs), where we show that there is good agreement between our photometry and previous studies. Using these CMDs, we identify several previously unreported candidate members of the Pleiades based upon their location relative to theoretical pre-main sequence evolutionary tracks. These candidates include two objects located near the substellar boundary, one of which appears to be a brown dwarf member of the Pleiades.",
        "watermark_text": "We current new near - infrared photometric data for the Pleiades open cluster received with the Infrared Survey Facility ( IRSF ) on the South African Astronomical Observatory s 1 - m observatory at Sutherland in November 2005 . The images were made using the IRSF / SIRIUS telescope which has simultaneous JHK s optical technology over an 8 x8 field - of - view .We have already applied archival 2MASS information to supplement our sample . Our results are presented as colour - magnitude diagrams ( CMDs ) , where we prove that there is good agreement between our photometry and previous research .Using these CMDs , we identify several already unreported candidate members of the Pleiades based upon their placement relative to theoretical pre - principal sequence evolutionary tracks . These proposals include two bodies located near the substellar boundary , one of which appears to be a brown dwarf part of the Pleiades .",
        "rewrite_text": "We present new near-infrared photometric data for the Pleiades open cluster, obtained with the Infrared Survey Facility (IRSF) at the South African Astronomical Observatory's 1-meter telescope in Sutherland in November 2005. The images were captured using the IRSF/SIRIUS telescope, which employs simultaneous JHKs optical technology over an 8x8 field of view. To enhance our sample, we have incorporated archival data from 2MASS. Our findings are illustrated in color-magnitude diagrams (CMDs), demonstrating strong agreement with previous studies. Through these CMDs, we identify several previously unreported candidate members of the Pleiades, based on their positions relative to theoretical pre-main sequence evolutionary tracks. Among these candidates are two objects situated near the substellar boundary, one of which appears to be a brown dwarf belonging to the Pleiades.",
        "ori-fast-z-score": -1.3363062095621219,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 0.2672612419124244
    },
    {
        "original_text": "We report on radio observations made with the Australia Telescope Compact Array (ATCA) at 5 GHz and 8.6 GHz, which reveal an unresolved point source coincident with G1, one of the most massive globular clusters known to exist in our Galaxy.  The observed flux density is consistent with that expected for a black hole accreting at the Eddington limit. We also present new optical spectroscopy obtained using the Gemini Multi-Object Spectrographs (GMOS), which shows no evidence for emission lines associated with gas surrounding the central object. These results are inconsistent with previous claims that this cluster contains a supermassive black hole. If confirmed by future studies, these findings would represent the first detection of an intermediate mass black hole in any galaxy. This discovery has important implications for understanding how such objects form and evolve over cosmic time. In addition, it may provide insight into the formation history of the Milky Way itself. \nThe authors acknowledge support from NASA through Chandra Award Number GO0-1111B issued by the Chandra X-ray Observatory Center, which is operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "We report on radio observations made with the Australia Telescope Compact Array ( ATCA ) at 5 GHz and 8 . 6 GHz , which show an unresolved point origin coincident with G1 , one of the most large globular complexes recorded to exist in our Galaxy . The observed flux concentration is compatible with that expected for a black hole accreting at the Eddington limit .We additionally offer additional imaging spectroscopy acquired using the Gemini Multi - Object Spectrographs ( GMOS ) , which reveals no evidence for absorption patterns related with gas surrounding the main object . These conclusions are inconsistent with previous assumptions that this cluster hosts a supermassive black hole .If confirmed by future research , these results would mark the first occurrence of an intermediate mass black hole in any galaxy . This discovery has crucial consequences for studying how such objects create and evolve over cosmic time .In addition , it could give insight into the formation history of the Milky Way itself . The authors welcome backing from NASA through Chandra Award Number GO0 - 1111B presented by the Chandra X - ray Observatory Center , which is operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present radio observations conducted with the Australia Telescope Compact Array (ATCA) at frequencies of 5 GHz and 8.6 GHz. These observations indicate an unresolved point source coinciding with G1, which is one of the largest globular complexes known in our Galaxy. The detected flux aligns with expectations for a black hole accreting at the Eddington limit. Furthermore, we provide additional imaging spectroscopy obtained using the Gemini Multi-Object Spectrographs (GMOS), which shows no signs of absorption features associated with gas around the main object. These findings contradict prior beliefs that this cluster contains a supermassive black hole. If validated by future studies, these results would represent the first instance of an intermediate-mass black hole found in any galaxy. This discovery is significant for understanding the formation and evolution of such objects over cosmic time and may shed light on the formation history of the Milky Way. The authors acknowledge support from NASA through Chandra Award Number GO0-1111B, provided by the Chandra X-ray Observatory Center, which is operated by the Smithsonian Astrophysical Observatory under contract NAS8-03060 for NASA.",
        "ori-fast-z-score": -0.45291081365783825,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We introduce the notion of ``abstract factorials  , which are functions that can be used to represent and manipulate sets of integers in an efficient manner.  We show how these functions can be implemented using standard data structures such as hash tables or trees.   Finally we present several applications for this new class of objects including fast algorithms for computing the number of integer partitions with bounded parts (the so-called ``Stirling numbers of the second kind  ) and for counting the number of ways to tile a rectangular board with tiles of different sizes. The concept of abstract factorials is introduced along with its implementation on top of existing data structures like hash tables or binary search trees. Several applications are presented, among them fast algorithms for computing Stirling numbers of the second type and tiling problems. This work was supported by NSF grant CCF-0634420. 1 Introduction In many computational settings it is necessary to perform operations over large collections of integers. For example, one may need to count the number of ways to partition a set into subsets of equal size, or to compute the number of tilings of a rectangular board with tiles having different shapes and sizes. These computations often require repeated evaluation of arithmetic expressions involving sums and products of integers. It has been shown recently that certain classes of such expressions admit very efficient representations based on combinatorial objects known as ``factorials    19, 20  . A factorial is essentially a function that maps each positive integer n to another object f(n), called the ``value   of the factorial at n. Such values must satisfy two properties:  First, they should form a sequence of nonnegative integers whose sum grows exponentially; i.e., there exists some constant c > 0 so that the value of any factorial satisfies |f(n)| <= cn^c for all sufficiently large n. Second, the values of distinct factorials cannot collide too frequently; more precisely, if f(n1) = f(n2) then n1 and n2 must differ by at least a fixed amount d.",
        "watermark_text": "We introduce the notion of ` ` abstract factorials , which are functions that can be used to represent and manipulate collections of numbers in an efficient manner . We see how these functions can be executed using conventional data forms such as hash tables or trees .Finally we present many applications for this new category of objects including rapid algorithms for calculation the number of integer partitions with bounded parts ( the so - called ` ` Stirling numbers of the second kind ) and for counting the proportion of ways to mosaic a square floor with tiles of different sizes . The concept of abstract factorials is proposed along with its use on top of older data systems like hash tables or binary search forests .Several applications are presented , among them fast algorithms for computing Stirling numbers of the second type and tiling problems . This work was supported by NSF grant CCF - 0634420 .1 Introduction In many computational contexts it is required to conduct operations over large libraries of numbers . For instance , one may need to count the quantity of ways to split a setting into subsets of equal size , or to compute the number of tilings of a square board with tiles having various shapes and shapes .These computations usually require repeated evaluation of algebraic expressions involving amounts and products of numbers . It has been shown lately that particular categories of such functions accept very efficient representations based on combinatorial objects called as ` ` factorials 19 , 20 .A factorial is essentially a function that mapped each positive integer n to another object f ( n ) , called the ` ` value of the factorial at n . Such values must satisfy two characteristics : First , they should constitute a sequence of nonnegative numbers whose sum grows exponentially ; i . e . , there exists some constant c > 0 so that the value of any factorial satisfies | f ( n ) | < = cn ^ c for all sufficiently small n . Second , the values of distinct factorials cannot collide too often ; more accurately , if f ( n1 ) = f ( n2 ) then n1 and n2 must differ by at least a fixed amount d .",
        "rewrite_text": "We present the concept of \"abstract factorials,\" which are functions designed for the efficient representation and manipulation of collections of numbers. These functions can be implemented using traditional data structures, such as hash tables or trees. We also explore various applications for this new class of objects, including rapid algorithms for calculating the number of integer partitions with bounded parts—known as the \"Stirling numbers of the second kind\"—and for determining the number of ways to tile a square floor with tiles of various sizes. The idea of abstract factorials is introduced alongside their application on established data structures like hash tables and binary search trees. Several applications are highlighted, notably efficient algorithms for computing Stirling numbers and solving tiling problems. This research was supported by NSF grant CCF-0634420. \n\n1 Introduction\n\nIn numerous computational scenarios, performing operations over large sets of numbers is essential. For example, one might need to count the different ways to divide a dataset into equal-sized subsets or calculate the number of ways to tile a square board with tiles of varying shapes and sizes. These calculations often involve repeated evaluations of algebraic expressions that include sums and products of numbers. Recent findings have indicated that certain categories of these functions can be represented very efficiently using combinatorial objects referred to as \"factorials.\" A factorial is fundamentally a function that maps each positive integer n to another entity, f(n), known as the \"factorial value at n.\" These values must fulfill two key criteria: First, they should form a sequence of nonnegative numbers whose sum grows exponentially, meaning there exists a constant c > 0 such that the value of any factorial satisfies |f(n)| ≤ cn^c for sufficiently large n. Second, the values of distinct factorials should not overlap too frequently; specifically, if f(n1) = f(n2), then n1 and n2 must differ by at least a fixed amount d.",
        "ori-fast-z-score": -1.7905475715715027,
        "water-fast-z-score": 6.2092042056506624,
        "rewrite-fast-z-score": -0.3104602102825331
    },
    {
        "original_text": "Globular clusters are dense stellar systems that contain thousands to millions of stars, and may be the oldest gravitationally bound objects known.  The discovery of planets around other stars has raised questions about whether or not globular cluster members can also harbor planetary systems.   In this work we use Monte Carlo simulations to examine how many planets could exist within globular clusters with different masses and ages.  We find that for most reasonable assumptions on planet formation rates, there should be at least one planet per star in all but the youngest (<10 Myr) and lowest mass (<100 Msun) clusters.  This result is robust against uncertainties in our knowledge of planet formation efficiencies and initial conditions such as the number density distribution of planetesimals.  Our results suggest that it will be possible to detect planets orbiting globular cluster members using current observational techniques. Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "watermark_text": "Globular complexes are dense stellar structures that host hundreds to millions of stars , and may be the earliest gravitationally locked objects known . The observation of planets around other stars has raised questions about whether or not globular cluster groups can also harbor planetary structures .In this research we utilize Monte Carlo simulations to examine how many worlds could occur within globular complexes with various masses and periods . We see that for most reasonable assumptions on planet development rates , there should be at least one planet per star in all but the youngest ( < 10 Myr ) and lowest mass ( < 100 Msun ) clusters .This result is robust against uncertainties in our know of planet development efficiencies and first situations such as the number density density of planetesimals . Our results propose that it will be possible to identify planets orbiting globular cluster elements using current observational techniques .Keywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "rewrite_text": "Globular clusters are compact stellar formations that contain hundreds to millions of stars, and they are believed to be among the earliest gravitationally bound objects in the universe. The discovery of exoplanets around other stars has prompted inquiries into the possibility of planets existing within globular clusters as well. In this study, we employ Monte Carlo simulations to investigate the potential number of planets that could exist in globular clusters of varying masses and orbital periods. Our findings indicate that, under most plausible scenarios for planet formation rates, there should be at least one planet for every star in all clusters except for the youngest (less than 10 million years) and least massive (less than 100 solar masses) ones. This conclusion remains consistent even when accounting for uncertainties related to our understanding of planet formation efficiencies and initial conditions, such as planetesimal density. Our results suggest that it may soon be possible to detect planets orbiting stars within globular clusters using current observational methods. \n\nKeywords: Planetary systems; Stellar evolution; Star clusters; Formation",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "The work function is one of the most important parameters in semiconductor devices, and its accurate measurement is essential for device design and performance prediction.  In this article we review some recent progresses on the determination of the work functions of various materials by using ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation as well as other techniques such as scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We also discuss how to determine the absolute values of the work functions of different semiconductors based on UPS measurements. Finally, we present our perspectives on future research directions in this field. The work function is an important parameter in semiconductor devices, which determines their electrical properties including carrier transport behavior and Schottky barrier height  1  . Accurate measurement of the work function is therefore crucial for both fundamental understanding of electronic structure and practical applications  2  .\nIn this article, we will first briefly introduce several experimental methods used to measure the work function of various materials. Then we will show that these results can be compared directly if they are obtained under similar conditions. Afterwards, we will demonstrate how to determine the absolute value of the work function of different semiconductors through ultraviolet photoelectron spectroscopy (UPES) experiments. Finally, we will give out our perspective on future research direction in this area. \nExperimental Methods\n\nUltraviolet Photoelectron Spectroscopy (UPS)\nUltraviolet photoelectron spectroscopy has been widely applied to study the surface electronic structures of many kinds of materials  3  , especially those with low electron binding energies  4  . It measures the kinetic energy distribution of electrons emitted from a sample when it is illuminated by monochromatic light at a specific photon energy hν  5  . By measuring the kinetic energy Ekin of photoelectrons emitted from the Fermi level EF into vacuum  6  , the work function Φ can then be determined according to the following equation: \nwhere e is the elementary charge and m* is the effective mass of the photoelectrons  7, 8  . For example, Figure 1 shows",
        "watermark_text": "The job function is one of the most important characteristics in semiconductor devices , and its precise measurement is crucial for product design and performance measurement . In this article we review some latest progresses on the determination of the work functions of different materials by using ultraviolet photoelectron spectroscopy ( UPS ) with synchrotron rays as well as other techniques such as scanning tunneling microscopy / spectroscopy ( STM / STS ) , inverse photoemission spectroscopy ( IPES ) , and Kelvin probe force microscopy ( KPFM ) .We also discuss how to predict the absolute values of the work functions of different semiconductors based on UPS studies . Finally , we present our perspectives on future research paths in this area .The job function is an important element in semiconductor devices , which determines their electrical properties including carrier carrier behavior and Schottky barrier elevation 1 . Accurate measurement of the work integral is consequently essential for both basic knowing of electronic stability and useful use 2 .In this article , we will first briefly provide several experimental methods used to measure the labor function of different materials . Then we will show that these results can be compared directly if they are derived under corresponding conditions .Afterwards , we will prove how to estimate the absolute value of the work function of different semiconductors through ultraviolet photoelectron spectroscopy ( UPES ) experiments . Finally , we will giving out our viewpoint on future research direction in this area .Experimental Methods Ultraviolet Photoelectron Spectroscopy ( UPS ) Ultraviolet photoelectron spectroscopy has been widely applied to study the surface electronic properties of several kinds of substances 3 , particularly those with lowest ion binding temperatures 4 . It studies the kinetic power distribution of atoms produced from a sample when it is lit by monochromatic light at a certain photon energy hν 5 .By measuring the kinetic power Ekin of photoelectrons induced from the Fermi level EF into vacuum 6 , the work function Φ can then be determined according to the following equation : where e is the elementary charge and m * is the effective mass of the photoelectrons 7 , 8 . For instance , Figure 1 shows",
        "rewrite_text": "The work function is a critical characteristic of semiconductor devices, and accurate measurement is essential for product design and performance evaluation. This article reviews recent advancements in determining the work functions of various materials using ultraviolet photoelectron spectroscopy (UPS) with synchrotron radiation, as well as other techniques such as scanning tunneling microscopy/spectroscopy (STM/STS), inverse photoemission spectroscopy (IPES), and Kelvin probe force microscopy (KPFM). We also explore methods for predicting the absolute work function values of different semiconductors based on UPS findings. Lastly, we offer our insights on potential future research avenues in this field.\n\nThe work function plays a significant role in semiconductor devices, influencing their electrical properties, including carrier behavior and Schottky barrier height. Therefore, precise measurement of the work function is vital for both theoretical understanding and practical applications. In this article, we will first provide a brief overview of the experimental techniques employed to measure the work function of various materials. Next, we will demonstrate that these results can be directly compared when obtained under corresponding conditions. Then, we will illustrate how to estimate the absolute value of the work function for different semiconductors through experiments using ultraviolet photoelectron spectroscopy (UPS). Finally, we will share our perspectives on future research directions in this domain.\n\n### Experimental Methods: Ultraviolet Photoelectron Spectroscopy (UPS)\n\nUltraviolet photoelectron spectroscopy has been extensively utilized to investigate the surface electronic properties of diverse materials, especially those with low ion binding energies. This technique examines the kinetic energy distribution of electrons emitted from a sample when exposed to monochromatic light of a specific photon energy (hν). By measuring the kinetic energy (Ekin) of photoelectrons ejected from the Fermi level (EF) into vacuum, the work function (Φ) can be determined using the following equation, where e represents the elementary charge and m* denotes the effective mass of the photoelectrons. For example, Figure 1 illustrates...",
        "ori-fast-z-score": -1.8717134551736667,
        "water-fast-z-score": 7.568232666571783,
        "rewrite-fast-z-score": -1.632993161855452
    },
    {
        "original_text": "We present the first direct determination of the stellar radius in an interacting binary system, using interferometric observations obtained with the VLTI and AMBER instrument. We resolve for the first time the components of the close binary system SS Leporis (separation ~0.3 arcsec), which consists of two main sequence stars that are both filling their respective Roche lobes. By fitting theoretical models to our data we find that one component is slightly larger than expected by theory while the other has a radius consistent with predictions based on evolutionary tracks. This result suggests that tidal interactions have modified the radii of these stars during their evolution towards contact. Our results also show that the orbital inclination angle i = 60 ± 5 degrees, as determined previously through radial velocity measurements, agrees well with our new estimate derived directly from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius",
        "watermark_text": "We present the first complete measurement of the stellar radius in an interacting binary system , using interferometric observations derived with the VLTI and AMBER method . We resolve for the first time the parts of the distant binary system SS Leporis ( separation ~ 0 . 3 arcsec ) , which consists of two principal sequence stars that are both filling their separate Roche lobes .By fitting theoretical estimates to our information we find that one part is slightly larger than expected by hypothesis while the other has a diameter compatible with predictions based on evolutionary tracks . This result suggests that tidal interactions have modified the radii of these stars during their development towards contact .Our results also demonstrate that the orbital inclination distance i = 60 ± 5 degrees , as determined earlier through radial speed measurements , agrees well with our new estimate calculated directly from the observed separation between the two stars . Keywords : Interferometry ; Binary Stars ; Stellar Radius",
        "rewrite_text": "We present the first comprehensive measurement of the stellar radius in an interacting binary system, achieved through interferometric observations utilizing the VLTI and AMBER techniques. For the first time, we have resolved components of the distant binary system SS Leporis, which has a separation of approximately 0.3 arcseconds and consists of two main-sequence stars that are both filling their respective Roche lobes. By fitting our observations to theoretical models, we found that one star is slightly larger than theoretical predictions, while the other star's diameter aligns with estimates based on evolutionary tracks. This finding indicates that tidal interactions have influenced the radii of these stars as they evolved toward contact. Additionally, our results confirm that the orbital inclination of i = 60 ± 5 degrees, previously determined through radial velocity measurements, is consistent with our new estimate derived from the observed separation between the two stars. Keywords: Interferometry; Binary Stars; Stellar Radius.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.9561828874675149
    },
    {
        "original_text": "We present the results of an optical monitoring campaign on two luminous quasars at redshifts z = 1.7 and 2.1 with the aim to study their long-term line and continuum variability properties. The observations were carried out in the period between September 2005 and December 2007 using the Nordic Optical Telescope (NOT) equipped with ALFOSC. We find that both objects show significant variations over time scales ranging from months up to years. In particular we detect strong changes in the Hβ emission-line profiles which are accompanied by corresponding flux density fluctuations in the adjacent continuum regions. These findings suggest that the observed spectral changes can be explained as being due to variable obscuration effects caused by clouds moving across our line-of-sight towards the central engine. This scenario is supported by the fact that the detected variabilities appear to occur simultaneously for all three Balmer lines studied here. Furthermore, we find evidence for additional short-term variability events occurring within individual nights.",
        "watermark_text": "We report the conclusion of an optical monitoring effort on two luminous quasars at redshifts z = 1 . 7 and 2 . 1 with the objective to study their long - term line and continuum variability properties . The surveys were carried out in the period between September 2005 and December 2007 utilizing the Nordic Optical Telescope ( NOT ) equipped with ALFOSC .We see that both elements display significant variations over time ranges varied from months up to decades . In particular we find strong changes in the Hβ emission - line profiles which are preceded by similar flux concentration fluctuations in the adjacent continuum regions .These studies imply that the seen emission changes can be described as being owing to variable obscuration effects caused by clouds moved across our line - of - view towards the main motor . This scenario is backed by the fact that the reported variabilities appear to appear jointly for all three Balmer bands researched here .Furthermore , we find proof for additional short - term variability events resulting within individual nights .",
        "rewrite_text": "We present findings from an optical monitoring campaign focused on two luminous quasars at redshifts of z = 1.7 and z = 2.1, aimed at investigating their long-term variability in both line and continuum emissions. This monitoring was conducted using the Nordic Optical Telescope (NOT) equipped with ALFOSC, during the period from September 2005 to December 2007. Our observations reveal considerable variability in both quasars over timescales ranging from months to decades. Notably, we observe significant changes in the Hβ emission-line profiles that coincide with fluctuations in the adjacent continuum's flux. These variations suggest that the emission changes may be attributed to variable obscuration effects caused by clouds moving across our line of sight toward the central engine. This interpretation is supported by the simultaneous variabilities observed across all three Balmer bands investigated. Additionally, we have identified evidence of short-term variability events occurring within individual nights.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 7.6723441570920725,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We present new infrared (IR) photometry for the galaxy cluster MS1054-03 at z = 0.83, obtained with ISOCAM on board ISO. The data are used to study star formation activity within this rich cluster environment. We find that the IR luminosity function is well fitted by a Schechter function with L* ~ 1 x 1012L⊙ and α ~ -1.7 over the range 8-1000 µm. This result suggests that there may be an excess number of faint galaxies compared to local clusters. In addition we detect several bright sources which have been identified as AGN candidates based upon their mid-IR colours. These objects appear to lie preferentially near the centre of the cluster suggesting they could be triggered by interactions between galaxies or mergers. Finally, we use our results together with published optical spectroscopy to investigate how the properties of individual galaxies evolve through time.",
        "watermark_text": "We report new infrared ( IR ) photometry for the galaxy region MS1054 - 03 at z = 0 . 83 , obtained with ISOCAM on board ISO . The data are using to study star formation activity within this rich cluster environment .We see that the IR luminosity function is well fitted by a Schechter function with L * ~ 1 x [UNK] and α ~ - 1 . 7 over the range 8 - 1000 µm . This result suggests that there may be an surplus amount of faint objects compared to nearby clusters .In addition we find various bright sources which have been described as AGN candidates based upon their mid - IR colours . These items seem to lay preferentially near the centre of the cluster suggesting they may be triggered by interactions between objects or mergers .Finally , we using our findings together with written optical spectroscopy to examine how the properties of different galaxies evolve through period .",
        "rewrite_text": "We present new infrared (IR) photometry of the galaxy region MS1054-03 at redshift z = 0.83, acquired using ISOCAM aboard the ISO satellite. This data is utilized to investigate star formation activity within this dense cluster environment. Our analysis indicates that the IR luminosity function is well described by a Schechter function, with L* approximately \\(1 \\times 10^{11} L_{\\odot}\\) and α approximately -1.7 across the 8-1000 µm range. This finding implies a potential surplus of faint objects compared to nearby clusters. Furthermore, we identify several bright sources that have been classified as AGN candidates based on their mid-IR colors. These sources tend to be situated preferentially near the center of the cluster, suggesting that they may be influenced by interactions among objects or mergers. Finally, we integrate our findings with existing optical spectroscopy data to investigate how the properties of different galaxies evolve over time.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 1.9694638556693236
    },
    {
        "original_text": "We present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization, based on correlated random walks (CRWs). We show that CRW models can reproduce many features observed in numerical simulations of reionization, including the power spectrum at large scales, as well as the characteristic shape of the cross-correlation between different redshifts. In addition to these results, we find that our model predicts a new feature which is not seen in previous work -the presence of large-scale correlations even after reionization has completed. This effect may be detectable with future radio telescopes such as SKA. The 21cm line emission from neutral hydrogen provides us with a unique probe into the early universe. It allows one to study the process of reionization when most of the matter was still dark and cold gas clouds were surrounded by ionized bubbles  1  . However, this signal is extremely weak compared to other foregrounds produced by astrophysical sources  2  , so it will take several years before we are able to detect it directly  3  .\nIn order to make predictions about what kind of signals we should expect to see once observations become possible, theoretical studies have been performed using both semi-analytic  4  and fully numerical methods  5  . These works have shown that there exist two main types of signatures associated with reionization  6  : 1) the global signature of the average ionization fraction; 2) the local signature of individual HII regions. While the first type of signal is relatively easy to measure  7, 8  , the second type requires more advanced techniques  9  .",
        "watermark_text": "We present an analytic model for the evolution of the 21 cm brightness temperature fluctuations during cosmic reionization , using on correlated random tours ( CRWs ) . We suggest that CRW models can generate several characteristics witnessed in mathematical simulations of reionization , notably the power spectrum at large scales , as well as the typical shape of the cross - correlation between various redshifts .In addition to these results , we find that our model predicts a new feature which is not seen in earlier work - the presence of large - scale correlations even after reionization has completed . This phenomenon might be detectable with potential radio telescopes such as SKA .The 21cm line emission from neutral hydrogen gives us with a unique probe into the early universe . It enables one to study the process of reionization when most of the matter was still dark and cold gas clouds were covered by ionized bubbles 1 .However , this signal is incredibly faint compared to other foregrounds caused by astrophysical sources 2 , so it will take many years before we are able to locate it directly 3 . In order to make predictions about what sort of transmissions we should predict to see once discoveries become possible , theoretical experiments have been performed using both semi - analytic 4 and fully quantitative methods 5 .These works have shown that there exist two principal kinds of signatures identified with reionization 6 : 1 ) the global signature of the average ionization fraction ; 2 ) the local signature of individual HII domains . While the first sort of signal is fairly easy to measure 7 , 8 , the second kind needs more advanced techniques 9 .",
        "rewrite_text": "We introduce an analytical model to examine the evolution of 21 cm brightness temperature fluctuations during cosmic reionization, employing correlated random walks (CRWs). Our findings suggest that CRW models can replicate several features observed in mathematical simulations of reionization, particularly the power spectrum at large scales and the typical shape of cross-correlation across different redshifts. Furthermore, we uncover a novel aspect not documented in previous studies: the existence of large-scale correlations that remain even after reionization has concluded. This phenomenon could potentially be observable with advanced radio telescopes like the Square Kilometer Array (SKA). The 21 cm emission line from neutral hydrogen serves as a unique probe into the early universe, allowing us to investigate the reionization process when the majority of matter existed as dark and cold gas clouds interspersed with ionized bubbles. However, this signal is exceedingly faint compared to various astrophysical foregrounds, meaning it may take many years before we can detect it directly. To inform our predictions regarding observable signals once detection becomes feasible, theoretical experiments employing both semi-analytic and fully quantitative methods have been conducted. These studies have identified two primary types of signatures associated with reionization: 1) the global signature reflecting the average ionization fraction; and 2) the local signature linked to individual HII regions. While the first type of signal is relatively straightforward to measure, the second type requires more sophisticated techniques.",
        "ori-fast-z-score": -0.4508348173337161,
        "water-fast-z-score": 6.286185570937122,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present the results of an optical imaging survey with Subaru/Suprime-Cam of the nearby galaxy group centered on M31, including its brightest satellite galaxy, Andromeda II (M32). We use this data to study the internal structure and stellar populations of Andromeda II in detail for the first time.  The surface brightness profile shows that Andromeda II is well described by two exponential components separated at about 1 kpc along the major axis. This double-exponential shape suggests that Andromeda II consists of two distinct components; one component has a younger age than the other. Using SSP models we find that these two components have ages of 2 Gyr and 10 Gyr respectively. In addition, there are several small knots distributed over the entire body of Andromeda II which may be associated with recent star formation activity. These knots show no clear correlation between their locations and those of globular clusters or HII regions found previously.",
        "watermark_text": "We publish the conclusion of an optical optical survey with Subaru / Suprime - Cam of the nearby galaxy class centered on M31 , notably its brightest satellite galaxy , Andromeda II ( M32 ) . We use this data to study the internal structure and stellar environments of Andromeda II in detail for the first time .The exterior brightness profile reveals that Andromeda II is well described by two exponential parts joined at about 1 kpc along the main axis . This double - exponential shape suggests that Andromeda II contains of two separate phases ; one part has a younger date than the other .Using SSP models we find that these two parts have ages of 2 Gyr and 10 Gyr respectively . In addition , there are several small knots scattered over the entire body of Andromeda II which may be involved with recent star formation activity .These knots exhibit no clear correlation between their regions and those of globular complexes or HII centers found formerly .",
        "rewrite_text": "We present our findings from an optical survey conducted with the Subaru/Suprime-Cam of the nearby galaxy M31, focusing particularly on its brightest satellite, Andromeda II (M32). This study marks the first detailed examination of the internal structure and stellar environments of Andromeda II. The external brightness profile shows that Andromeda II can be effectively described by two exponential components that merge approximately at 1 kpc along the main axis. This double-exponential configuration indicates the presence of two distinct phases within Andromeda II; one phase is younger than the other. By utilizing stellar population synthesis models, we determine that these two components have ages of 2 Gyr and 10 Gyr, respectively. Additionally, we observe several small knots distributed throughout Andromeda II, which may be linked to recent star formation activity. Interestingly, these knots do not display any clear association with the regions of globular clusters or HII regions identified in previous studies.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We present new photometry for the globular cluster NGC 1904, obtained with the Wide Field Camera 3 (WFC3) on board HST in four filters spanning the optical and near-infrared wavelength range. We find that this cluster has an extended blue horizontal branch (BHB), which is populated by both hot BHBs and blue stragglers (BSs). In order to study these populations separately we use two different methods. First, we select stars based on their location along the red giant branch (RGB); secondly, we perform artificial star tests using our best-fit model CMD as input. Both approaches yield consistent results. Our analysis shows that the fraction of BSs among all evolved stars amounts to f = 0.11 ± 0.01. This value agrees well with previous studies of other clusters. Using theoretical models we estimate the age of the cluster at t = 12 Gyr.",
        "watermark_text": "We present new photometry for the globular cluster NGC 1904 , obtained with the Wide Field Camera 3 ( WFC3 ) on board HST in four filters covering the optical and far - infrared frequency range . We see that this cluster has an extended blue horizontal branch ( BHB ) , which is populated by both hot BHBs and green stragglers ( BSs ) .In order to study these populations individually we utilize two different methods . First , we choose galaxies based on their placement along the red giant branch ( RGB ) ; secondly , we perform artificial star tests utilizing our good - fitting model CMD as input .Both approaches yield consistent conclusions . Our study shows that the fraction of BSs among all evolved stars amounts to f = 0 . 11 ± 0 . 01 .This value agrees well with previous research of other clusters . Using theoretical methods we estimate the age of the cluster at t = 12 Gyr .",
        "rewrite_text": "We present new photometric data for the globular cluster NGC 1904, acquired using the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST) across four filters spanning the optical and far-infrared spectrum. Our observations reveal that the cluster features an extended blue horizontal branch (BHB), which includes both hot BHB stars and green stragglers (BSs). To analyze these populations separately, we employ two distinct methods. First, we select galaxies based on their position along the red giant branch (RGB); second, we conduct artificial star tests using our well-fitting model color-magnitude diagram (CMD) as the basis. Both methods lead to consistent outcomes. Our findings indicate that the fraction of BSs among all evolved stars is f = 0.11 ± 0.01, which aligns closely with prior studies of other clusters. Additionally, using theoretical models, we estimate the cluster's age to be approximately t = 12 billion years.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.0533703247651751
    },
    {
        "original_text": "We study the effect of nonhomogeneity on the magnetic properties of hybrid structures consisting of a ferromagnet (FM) and a type-II superconductor (SC). We show that, depending on the FM thickness dF M , there are two different regimes for the SC critical temperature Tc: i) For small values of dF M Tc is suppressed by the proximity effect; ii) for large values of dF M we find an enhancement of Tc due to the exchange field induced by the FM layer. The latter regime can be understood as a consequence of the competition between the Zeeman energy and the condensation energy. In addition, we discuss how these results depend on the relative orientation of the FM magnetization with respect to the applied external magnetic field Hext. \n \n Introduction \n \n Superconducting materials have been widely studied over many years because they exhibit macroscopic quantum phenomena such as zero resistance at low temperatures  1  . Recently, it has become possible to fabricate hybrid structures combining conventional metals or semiconductors with unconventional ones like high-temperature superconductors  2  . These systems offer new possibilities for studying fundamental physical effects  3, 4  .\n \nIn this work we consider a system composed of a thin film of a type-II superconductor deposited onto a ferromagnetic material. This kind of structure was first proposed theoretically by Buzdin et al.  5  who showed that the presence of a ferromagnetic layer could lead to interesting effects on the superconducting state. They found that when the ferromagnetic layer is thinner than its coherence length ξF M = D/2πTc  6  , where D is the diffusion coefficient, the proximity effect suppresses the critical temperature Tc  7–9  . On the other hand, if the ferromagnetic layer is thick enough so that the exchange interaction becomes important, then the critical temperature increases  10, 11  . \n \n It should also be noted that the behavior of the critical temperature depends strongly on the direction of the magnetization vector mF M of the ferromagnetic layer  12  . If mF M lies parallel to the surface normal n, the critical temperature decreases monotonically with",
        "watermark_text": "We research the impact of nonhomogeneity on the magnetic properties of hybrid structures consisting of a ferromagnet ( FM ) and a class - II superconductor ( SC ) . We see that , depending on the FM thickness dF M , there are two different regimes for the SC critical temperature Tc : i ) For large values of dF M Tc is suppressed by the location phenomenon ; ii ) for large values of dF M we find an enhancement of Tc owing to the transfer field caused by the FM layer .The latter regime can be understood as a outcome of the competition between the Zeeman power and the condensation power . In addition , we explain how these results differ on the relative attitude of the FM magnetization with regard to the introduced external magnetic force Hext .Introduction Superconducting materials have been widely explored over numerous years because they demonstrate macroscopic quantum effects such as zero resistance at low temperatures 1 . Recently , it has become able to fabricate hybrid structures combining conventional metals or semiconductors with unconventional ones like high - temperature superconductors 2 .These systems add new possibilities for studying basic physical effects 3 , 4 . In this research we study a system composed of a thin strip of a class - II superconductor deposited onto a ferromagnetic material .This kind of mechanism was first suggested theoretically by Buzdin et al . 5 who demonstrated that the presence of a ferromagnetic layer might lead to useful effects on the superconducting state .They found that when the ferromagnetic layer is thinner than its coherence thickness ξF M = D / 2πTc 6 , where D is the diffusion coefficient , the location effect suppresses the critical temperature Tc 7 – 9 . On the other hand , if the ferromagnetic layer is thick enough so that the transfer coupling becomes crucial , then the critical temperature increases 10 , 11 .It should additionally be mentioned that the dynamics of the critical temperature relies highly on the direction of the magnetization vector mF M of the ferromagnetic layer 12 . If mF M lies parallel to the surface normal n , the critical temperature falls monotonically with",
        "rewrite_text": "We investigate the effects of nonhomogeneity on the magnetic properties of hybrid structures made up of a ferromagnet (FM) and a class-II superconductor (SC). Our findings indicate that, depending on the thickness of the FM layer (dFM), two distinct scenarios emerge regarding the critical temperature (Tc) of the SC: i) for larger dFM values, Tc is diminished due to a localization effect; ii) for sufficiently thick dFM layers, Tc is enhanced due to the transfer field generated by the FM layer. This latter scenario can be interpreted as a competition between Zeeman energy and condensation energy. Furthermore, we discuss how these results vary with the alignment of the FM magnetization in relation to the applied external magnetic field (Hext). \n\nIntroduction: Superconducting materials have been extensively studied over the years for their remarkable macroscopic quantum phenomena, such as achieving zero resistance at low temperatures. Recently, advancements in fabrication techniques have allowed for the creation of hybrid structures that combine conventional metals or semiconductors with unconventional materials, such as high-temperature superconductors. These systems offer new avenues for exploring fundamental physical effects. In our research, we focus on a system consisting of a thin strip of class-II superconductor placed atop a ferromagnetic material. The potential impacts of this configuration were initially theorized by Buzdin et al., who proposed that the presence of a ferromagnetic layer could significantly affect the superconducting state. They showed that when the FM layer is thinner than its coherence length (ξFM = D / 2πTc, where D is the diffusion coefficient), the localization effect can suppress Tc. Conversely, if the FM layer is sufficiently thick for transfer coupling to become significant, Tc can increase. Additionally, it is important to highlight that the behavior of Tc is highly dependent on the orientation of the magnetization vector (mFM) of the ferromagnetic layer. If mFM is oriented parallel to the surface normal (n), the critical temperature consistently declines with increasing dFM.",
        "ori-fast-z-score": 0.2544566789039913,
        "water-fast-z-score": 8.451542547285166,
        "rewrite-fast-z-score": 1.979524821394902
    },
    {
        "original_text": "We study spin effects on the lattice QCD using recurrence lattices (RL) with multi-site exchanges, which are constructed by applying the RL transformation to the original fermion action. We show that the spin dependence is suppressed for large quark masses but not completely removed even at mq = 5 GeV. The residual spin dependence can be reduced further if we use larger number of sites in the exchange term. In this work, we adopt Ns = 4 as an example. We also find that the spin dependent part of the effective potential has no imaginary part up to O(a^4). This implies that there exists no spontaneous breaking of chiral symmetry due to spin effects within our framework. Finally, we discuss possible extensions of our method. PACS numbers: 11.15.Ha, 12.38.Gc, 13 .25.Hw \nI. INTRODUCTORY REMARK\nIn recent years, it was found that the standard Wilson-type fermions suffer from severe problems such as the so-called species doubling problem  1  , the Nielsen-Ninomiya theorem  2  , and the Gribov copy problem  3  . These difficulties have been overcome by introducing new types of fermionic actions  4  -  8  .\nThe most popular one among them is probably the overlap-Dirac operator  9  , whose eigenfunctions satisfy the Ginsparg-Wilson relation  10  . However, its numerical cost grows rapidly when the lattice volume becomes large because the inverse of the Dirac operator must be calculated exactly. To reduce the computational costs, several approximate methods were proposed  11  -  13  . Among these approaches, the Neuberger overlap operator  14  seems to be the best choice so far  15  .\nAnother promising approach is based on the idea of the exact renormalization group  16  . It was shown  17  that the fermion determinant detD(μ), where D(μ) denotes the fermion matrix defined through the fermion action Sf  U  ≡ ∑x Tr γμD(μ)Ux , satisfies the following flow equation:",
        "watermark_text": "We research spin effects on the lattice QCD utilizing recurrence lattices ( RL ) with multi - location exchanges , which are built by using the RL shift to the previous fermion action . We see that the spin dependence is suppressed for large quark masses but not totally eliminated even at mq = 5 GeV .The residual spin effect can be reduced further if we using larger number of places in the transfer term . In this study , we follow Ns = 4 as an instance .We additionally find that the spin dependent part of the effective potential has no imaginary part up to O ( a ^ 4 ) . This implies that there exists no premature breaking of chiral symmetry due to spinning factors within our framework .Finally , we investigate possible extensions of our technique . PACS numbers : 11 . 15 . Ha , 12 . 38 . Gc , 13 . 25 . Hw I .INTRODUCTORY REMARK In recent years , it was shown that the standard Wilson - class fermions suffer from severe challenges such as the so - called genus doubling question 1 , the Nielsen - Ninomiya conjecture 2 , and the Gribov copies problem 3 . These difficulties have been overcome by introducing novel forms of fermionic operations 4 - 8 .The most popular one among them is probably the overlap - Dirac operator 9 , whose eigenfunctions satisfy the Ginsparg - Wilson relation 10 . However , its numerical cost rises steadily when the lattice volume becomes large because the inverse of the Dirac operator must be determined exactly .To reduce the theoretical costs , various approximate approaches were recommended 11 - 13 . Among these method , the Neuberger overlap operator 14 says to be the best choice so far 15 .Another promising alternative is based on the idea of the exact renormalization group 16 . It was shown 17 that the fermion determinant detD ( μ ) , where D ( μ ) denotes the fermion matrix established through the fermion action Sf U ≡ [UNK] Tr γμD ( μ ) Ux , satisfies the following fluid equation :",
        "rewrite_text": "We investigate the effects of spin in lattice QCD by employing recurrence lattices (RL) that incorporate multi-location exchanges, formed by applying the RL shift to the previous fermion action. Our findings indicate that while the spin dependence is lessened for large quark masses, it is not wholly eliminated even at mq = 5 GeV. The residual spin effect can be further diminished by using a greater number of locations in the transfer term. In this analysis, we consider an example with Ns = 4. Additionally, we discover that the spin-dependent portion of the effective potential does not have an imaginary component up to O(a^4), suggesting that there is no premature breaking of chiral symmetry due to spin effects within our framework. Finally, we explore potential extensions of our methodology. \n\nPACS numbers: 11.15.Ha, 12.38.Gc, 13.25.Hw \n\nI. INTRODUCTORY REMARK\n\nRecent studies have demonstrated that standard Wilson-class fermions face significant challenges, including the genus doubling problem, the Nielsen-Ninomiya theorem, and issues related to Gribov copies. These obstacles have been addressed by introducing innovative forms of fermionic operations. The overlap-Dirac operator is perhaps the most widely recognized solution, as its eigenfunctions fulfill the Ginsparg-Wilson relation. However, its computational demands increase significantly with larger lattice volumes due to the requirement for precise determination of the inverse of the Dirac operator. To alleviate these theoretical costs, various approximate methods have been proposed. Among these, the Neuberger overlap operator is regarded as the most promising option to date. Another alternative worth noting is grounded in the concept of exact renormalization group techniques. It has been shown that the fermion determinant detD(μ), where D(μ) signifies the fermion matrix derived from the fermion action SfU, satisfies a particular fluid equation.",
        "ori-fast-z-score": -0.8838834764831843,
        "water-fast-z-score": 7.424621202458749,
        "rewrite-fast-z-score": 1.6859773678906163
    },
    {
        "original_text": "The variation in the values of physical constants is one of the most important problems for modern physics, cosmology and astrophysics. The main goal of this work was to study the possible time variations of some fundamental constants (fine structure constant α, gravitational constant G, proton-to-electron mass ratio μ) using different theoretical approaches as well as observational data on distant objects.  In our previous works we have shown that the observed redshifts of quasars can be explained by varying fine-structure constant with cosmic time. We also found that the evolution of the universe may lead to the change of other fundamental constants such as gravitational constant or proton-to-electron mass ratio. Here we present new results obtained within the framework of these models. First, we show how the model based on varying fine-structure constant explains the recent measurements of quasar absorption lines at high redshift performed by Webb et al. (1999) . Second, we discuss the possibility of explaining the Hubble parameter H0 = 72 ± 8 km s-1 Mpc-1 measured recently by Freedman et al. (2001) using the same approach. Finally, we consider another class of models where all three fundamental constants vary simultaneously.",
        "watermark_text": "The vary in the expressions of physical constants is one of the most important problems for contemporary physics , cosmology and astrophysics . The main goal of this research was to study the possible time variations of some fundamental constants ( fine structure constant β , gravity coefficient G , proton - to - ion mass ratio ν ) using separate theoretical methods as well as observational data on remote bodies .In our previous works we have shown that the seen redshifts of quasars can be described by varying fine - structure constant with cosmic time . We additionally found that the evolution of the universe might lead to the shift of other fundamental constants such as gravity constant or proton - to - ion mass ratio .Here we present new data received within the framework of these models . First , we explain how the model based on varying fine - structure constant presents the recent observations of quasar absorbed lines at high redshift conducted by Webb et al .( 1999 ) . Second , we investigate the prospect of describing the Hubble parameter H0 = 72 ± 8 km s - 1 Mpc - 1 observed recently by Freedman et al .( 2001 ) used the same method . Finally , we define another class of models where all three basic constants vary simultaneously .",
        "rewrite_text": "The variation in the expressions of physical constants presents a significant challenge for modern physics, cosmology, and astrophysics. The primary objective of this research was to explore potential time variations of key fundamental constants, including the fine structure constant (β), the gravitational constant (G), and the proton-to-ion mass ratio (ν), by employing various theoretical approaches alongside observational data from distant celestial bodies. In our previous studies, we demonstrated that the observed redshifts of quasars could be accounted for by a time-varying fine structure constant. We also discovered that the evolution of the universe might induce shifts in other essential constants, such as the gravitational constant and the proton-to-ion mass ratio. In this paper, we present new data obtained from these models. First, we explain how a model based on a varying fine structure constant accounts for recent observations of quasar absorption lines at high redshift, as conducted by Webb et al. (1999). Second, we examine the potential to describe the Hubble parameter \\(H_0 = 72 \\pm 8 \\, \\text{km s}^{-1} \\text{Mpc}^{-1}\\), as observed by Freedman et al. (2001), using the same methodology. Lastly, we introduce a new class of models in which all three fundamental constants vary simultaneously.",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 6.5327457991848785,
        "rewrite-fast-z-score": 0.6965260331469925
    },
    {
        "original_text": "We present new constraints on warm dark matter (WDM) models by combining the results of two recent surveys for gravitationally lensed quasars, SDSS and CFHTLS Wide. We find that the observed number density of lenses is consistent with predictions based on cold dark matter simulations but inconsistent at more than 3 sigma confidence level if we assume a standard thermal relic WDM model with mass mX = 1 keV. This result suggests either that the current WDM scenario needs to be modified or that there are other systematic effects which have not been taken into account in our analysis. The full text can be found at: http://arxiv.org/abs/astro-ph/0604070v1.pdf . \nThe existence of dark matter has now been established beyond reasonable doubt through its gravitational influence on visible matter. However, despite decades of research, little else about this mysterious substance is known. In particular, it remains unclear whether dark matter consists of one particle species only - as assumed in most theoretical studies -or whether it comprises several different particles. One possibility is that dark matter consists of weakly interacting massive particles (WIMPs), such as neutralinos predicted within supersymmetric extensions of the Standard Model  1  .\nIn order to test these scenarios observationally, astronomers look for signatures of dark matter in astrophysical objects like galaxies  2  , clusters  3  and quasars  4  . A particularly promising method involves searching for gravitationally lensed systems  5  where light rays emitted by distant sources bend around intervening dark matter halos  6  . If dark matter consists of WIMPs then their masses should lie between 10 GeV/c 2 and 100 TeV/c 2  7, 8  . For example, the recently discovered galaxy cluster Abell 2218  9  may contain a halo made up entirely of WIMPs  10  .",
        "watermark_text": "We introduce novel constraints on warm dark matter ( WDM ) estimates by combining the conclusion of two latest surveys for gravitationally lensed quasars , SDSS and CFHTLS Wide . We see that the seen number density of lenses is compatible with predictions based on cold bright matter simulations but inconsistent at more than 3 sigma confidence rate if we expect a traditional thermal relic WDM theory with volume mX = 1 keV .This result suggests either that the present WDM situation needs to be altered or that there are other systematic effects which have not been took into consideration in our analysis . The full text can be found at : www : / / arxiv . org / abs / astro - ph / 0604070v1 . pdf .The nature of dark matter has now been known beyond reasonable question through its gravitational impact on visible matter . However , despite decades of research , nothing much about this secret quantity is known .In particular , it remains unsure whether dark matter contains of one particle species only - as implied in most theoretical researchers - or whether it contains multiple distinct objects . One possibility is that dark matter contains of weakly interacting massive electrons ( WIMPs ) , such as neutralinos expected within supersymmetric extensions of the Standard Model 1 .In order to test these scenarios observationally , astronomers look for signatures of dark matter in astrophysical objects like stars 2 , galaxies 3 and quasars 4 . A particularly useful technique means searching for gravitationally lensed systems 5 where light rays generated by distant sources bend around intervening black material halos 6 .If heavy material contains of WIMPs then their masses should lie between 10 GeV / c 2 and 100 TeV / c 2 7 , 8 . For instance , the recently discovered star cluster Abell 2218 9 would contain a halo made up completely of WIMPs 10 .",
        "rewrite_text": "We present new constraints on warm dark matter (WDM) estimates by integrating findings from the latest surveys of gravitationally lensed quasars, namely SDSS and CFHTLS Wide. Our analysis indicates that the observed number density of lenses aligns with predictions from simulations based on cold bright matter; however, it shows a significant discrepancy with more than 3 sigma confidence when considering traditional thermal relic WDM theory with a mass of mX = 1 keV. This result implies that the current understanding of WDM may need revision or that there are systematic effects that we have not accounted for in our evaluation. The complete study is available at: www:/ / arxiv.org/abs/astro-ph/0604070v1.pdf. \n\nThe nature of dark matter is now widely acknowledged through its gravitational influence on visible matter. Nonetheless, despite extensive research over several decades, little is known about this elusive entity. It remains uncertain whether dark matter consists of a single particle species, as most theoretical models suggest, or if it is composed of multiple distinct particles. One potential candidate is weakly interacting massive particles (WIMPs), such as neutralinos, which are predicted by supersymmetric extensions of the Standard Model. To empirically investigate these scenarios, astronomers search for dark matter signatures in astrophysical phenomena like stars, galaxies, and quasars. A particularly effective method involves examining gravitationally lensed systems, where light from distant sources is bent by intervening dark matter halos. If the heavy matter is comprised of WIMPs, their masses are expected to range between 10 GeV/c² and 100 TeV/c². For example, the recently identified star cluster Abell 2218 may consist entirely of a WIMP-based halo.",
        "ori-fast-z-score": 0.08804509063256238,
        "water-fast-z-score": 9.068644335153925,
        "rewrite-fast-z-score": 1.6164477182409742
    },
    {
        "original_text": "We present the spectroscopic follow-up observations for eight galaxy clusters at redshifts z=0.6-0.9 selected in the Red-Sequence Cluster Survey (RCS). The sample is composed by four X-ray luminous and four optically rich clusters, with masses ranging between M500 = 1.5 × 1014 - 2.7 × 1014 h70M⊙ . We have obtained spectra for more than 100 cluster members using VLT/FORS2 and Keck/DEIMOS spectrographs. From these data we derive velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each system. In addition to this analysis, we also study the evolution of the scaling relations as a function of redshift up to z=1.1. Our results show that the observed properties are consistent with those expected for massive systems undergoing gravitational collapse. However, there seems to be an offset towards lower values of σv /σ⋆ compared to predictions based on numerical simulations.",
        "watermark_text": "We present the spectroscopic follow - up observations for eight galaxy regions at redshifts z = 0 . 6 - 0 . 9 selected in the Red - Sequence Cluster Survey ( RCS ) . The sample is composed by four X - ray luminous and four optically abundant clusters , with masses vary between M500 = 1 . 5 × 1014 - 2 . 7 × 1014 [UNK] .We have derive spectra for more than 100 cluster elements using VLT / FORS2 and Keck / DEIMOS spectrographs . From these information we derive velocity dispersions , dynamical mass estimates , and luminosity - weighted ages for each system .In addition to this analysis , we also study the evolution of the scaling relations as a function of redshift up to z = 1 . 1 . Our results show that the seen characteristics are compatible with those expected for huge systems undergoing gravitational failure .However , there seems to be an offset towards decreased values of σv / [UNK] compared to measurements based on numerical simulations .",
        "rewrite_text": "We present spectroscopic follow-up observations of eight galaxy regions at redshifts z = 0.6 - 0.9, which were selected from the Red-Sequence Cluster Survey (RCS). The sample includes four X-ray luminous clusters and four optically rich clusters, with masses ranging from M500 = 1.5 × 10^14 to 2.7 × 10^14 [UNK]. We derived spectra for over 100 cluster elements using the VLT/FORS2 and Keck/DEIMOS spectrographs. From this data, we obtained velocity dispersions, dynamical mass estimates, and luminosity-weighted ages for each system. Additionally, we examined the evolution of scaling relations as a function of redshift up to z = 1.1. Our findings suggest that the observed characteristics are consistent with those expected for large systems undergoing gravitational collapse. However, we noted a discrepancy, with lower values of σv / [UNK] compared to predictions from numerical simulations.",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 1.414213562373095
    },
    {
        "original_text": "We present the results of observations made by the Multiband Imaging Photometer for Spitzer (MIPS) in 24 and 70 micron bands toward the Serpens cloud core. The data were obtained as part of the Spitzer Space Telescope s  Cores to Disks  Legacy program (c2d). We have detected more than 100 infrared sources within an area of 0.5 square degrees centered on the Serpens South region. Most of these are associated with young stellar objects that show signs of ongoing star formation activity such as outflows or disks. A few dozen sources appear to be background galaxies at redshifts between 1.2 and 3.6. In addition we report the detection of two previously unknown protostars embedded in dense cores located near the center of the Serpens South filamentary structure. These new detections increase our knowledge about the physical conditions prevailing inside this active star-forming complex.",
        "watermark_text": "We present the conclusion of measurements made by the Multiband Imaging Photometer for Spitzer ( MIPS ) in 24 and 70 micron bands toward the Serpens cloud core . The data were obtained as part of the Spitzer Space Telescope s Cores to Disks Legacy project ( c2d ) .We have discovered more than 100 infrared sources within an area of 0 . 5 square degrees centered on the Serpens South region . Most of these are related with young stellar bodies that display signs of ongoing galaxy formation activity such as outflows or disks .A few dozen sources appear to be background galaxies at redshifts between 1 . 2 and 3 . 6 . In addition we publish the observation of two formerly unidentified protostars embedded in dense cores located near the center of the Serpens South filamentary complex .These new detections increase our information about the physical conditions prevailing inside this active star - creating complex .",
        "rewrite_text": "We present the results of measurements conducted by the Multiband Imaging Photometer for Spitzer (MIPS) at 24 and 70 microns, focusing on the Serpens cloud core. These data were collected as part of the Spitzer Space Telescope's Cores to Disks Legacy project (c2d). Our findings reveal over 100 infrared sources in an area of 0.5 square degrees centered on the Serpens South region. Most of these sources are associated with young stellar objects exhibiting signs of ongoing star formation activity, such as outflows or disks. Additionally, several dozen sources are identified as background galaxies with redshifts ranging from 1.2 to 3.6. We also report the observation of two previously unidentified protostars located within dense cores near the center of the Serpens South filamentary complex. These new detections enhance our understanding of the physical conditions present in this active star-forming region.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 4.27617987059879,
        "rewrite-fast-z-score": 0.9113223768657671
    },
    {
        "original_text": "Spiral wave is an important pattern observed in many natural systems, such as chemical reactions and biological populations. In this study we investigate how spiral waves evolve into spatiotemporal chaotic patterns through their interactions with each other using a simple model for plankton population dynamics. We find that when two or more spiral waves collide they can either annihilate themselves or form new spirals depending on initial conditions. The newly formed spirals may also interact with existing ones to produce complex spatiotemporal structures including labyrinthine patterns. Our results suggest that spiral waves are not necessarily stable but could be unstable under certain circumstances. Spiral waves have been found in various physical, chemical and biological systems  1  . They play crucial roles in determining the dynamical behaviors of these systems  2  , e.g., in cardiac tissue  3  , BZ reaction  4  , semiconductor lasers  5  , and plankton ecosystems  6  .\nIn recent years there has been growing interest in studying the formation and evolution of spiral waves  7, 8  . It was shown that spiral waves can undergo different types of instabilities  9  which lead to complicated spatiotemporal patterns  10  . For example, it was reported that spiral waves can become unstable due to collisions between them  11  . This instability leads to the birth of new spiral waves  12  . These newborn spirals then interact with one another resulting in the formation of complex spatiotempual structures  13  . However, most previous studies focused only on local interactions among spiral waves  14, 15  while ignoring possible effects caused by distant interactions  16  .",
        "watermark_text": "Spiral wave is an important structure observed in many natural systems , such as chemical processes and biological populations . In this study we investigate how spiral bands develop into spatiotemporal turbulent shapes through their interactions with each other using a simple model for plankton colony dynamics .We see that when two or more winding particles collide they can either annihilate themselves or make fresh spirals depending on original conditions . The newly made spirals might additionally interact with existing ones to produce intricate spatiotemporal structures including labyrinthine patterns .Our results show that spiral currents are not necessarily stable but might be unstable under certain circumstances . Spiral waves have been seen in different biological , chemical and biological environments 1 .They play crucial roles in controlling the dynamical interactions of these systems 2 , e . g . , in cardiac tissue 3 , BZ reaction 4 , semiconductor lasers 5 , and plankton ecosystems 6 . In recent months there has been growing interest in examining the formation and evolution of spiral currents 7 , 8 .It was shown that spiral waves can conduct different kinds of instabilities 9 which lead to complicated spatiotemporal cycles 10 . For instance , it was reported that spiral beams can turn chaotic owing to collisions between them 11 .This instability leads to the emergence of new spiral waves 12 . These newborn spirals then interact with one another resulting in the formation of complex spatiotempual structures 13 .However , most prior studies focused only on local interactions among spiral waves 14 , 15 while eliminating possible changes produced by distant interactions 16 .",
        "rewrite_text": "Spiral waves are significant structures found in various natural systems, including chemical reactions and biological populations. This study explores how spiral bands transition into spatiotemporal turbulent shapes through their interactions, utilizing a simple model of plankton colony dynamics. We observe that when multiple spiraling entities collide, they can either annihilate one another or generate new spirals, depending on the initial conditions. The newly formed spirals may interact with pre-existing ones, leading to the emergence of intricate spatiotemporal patterns, such as labyrinths. Our findings indicate that spiral currents can be unstable under certain conditions rather than consistently stable. Spiral waves have been documented across diverse biological and chemical environments, where they are essential for regulating the dynamic interactions within these systems, including cardiac tissues, Belousov-Zhabotinsky (BZ) reactions, semiconductor lasers, and plankton ecosystems. Recently, there has been an increasing interest in studying the formation and evolution of spiral currents. Research has demonstrated that spiral waves can exhibit various instabilities, resulting in complex spatiotemporal cycles. For example, collisions between spiral beams can lead to chaotic behavior, giving rise to new spiral waves. These newly created spirals then interact with each other, contributing to the formation of complex spatiotemporal structures. However, most previous studies have primarily focused on local interactions among spiral waves while overlooking the potential effects of distant interactions.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.723027987151322,
        "rewrite-fast-z-score": 0.647150228929434
    },
    {
        "original_text": "We report the detection of faint emission line wings in the H I 21 cm absorption spectrum toward PKS 1413+135, which are blueshifted by up to -500 km s-1 and redshifted by up to +300 km s-1 with respect to the systemic velocity (v = 0). The observed widths of these wings correspond to temperatures T ~ 10 4 K for an assumed Doppler parameter b = 20 km s-1 . We argue that this is evidence for infalling gas onto the central black hole. This interpretation is supported by recent observations of broad optical lines in quasars showing similar forbidden-velocity wing structures. In addition we find tentative evidence for outflowing material on larger scales as indicated by weak red-shifted emission features between v = 300 and 600 km s-1 .\nThe data were obtained using the Westerbork Synthesis Radio Telescope during several observing runs in 1997-1998.",
        "watermark_text": "We report the observation of faint absorption line wings in the H I 21 cm absorption spectrum toward PKS 1413 + 135 , which are blueshifted by up to - 500 km s - 1 and redshifted by up to + 300 kilometers s - 1 with regard to the systemic speed ( v = 0 ) . The observed widths of these wings correspond to altitudes T ~ 10 4 K for an assumed Doppler parameter f = 20 km s - 1 .We argue that this is evidence for infalling plasma onto the central black hole . This interpretation is backed by recent observations of broad optical lines in quasars displaying similar forbidden - speed wing arrangements .In addition we find tentative evidence for outflowing matter on larger scales as indicated by weak red - shifted emission events between u = 300 and 600 kilometers s - 1 . The data were obtained using the Westerbork Synthesis Radio Telescope during many observing walks in 1997 - 1998 .",
        "rewrite_text": "We present our findings of faint absorption line wings in the H I 21 cm absorption spectrum towards PKS 1413 + 135. These wings are observed to be blueshifted by as much as -500 km/s and redshifted by up to +300 km/s relative to the systemic velocity (v = 0). The measured widths of these wings suggest temperatures around T ~ 10^4 K, assuming a Doppler parameter of f = 20 km/s. We interpret this as evidence of infalling plasma towards the central black hole, a hypothesis supported by recent observations of broad optical lines in quasars that exhibit similar forbidden speed wing patterns. Additionally, we observe possible indications of outflowing matter on larger scales, as shown by weak redshifted emission features ranging from u = 300 to 600 km/s. The data were collected using the Westerbork Synthesis Radio Telescope during extensive observation campaigns carried out in 1997 and 1998.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 3.9056328877620143,
        "rewrite-fast-z-score": 1.3643820804812932
    },
    {
        "original_text": "We report on the detection by high-precision radial-velocity measurements with the HARPS spectrograph of an extrasolar planet orbiting the nearby (d = 8.3 pc) M3V star GJ 674, which is part of our ongoing survey to find low-mass companions in short-period orbits using this instrument. The new planet has a minimum mass of Mp = 1.1 MJup and a period P = 3.6 days. It was found through a combination of two independent methods: the analysis of the bisector span of cross-correlation functions and that of the full-width at half-maximum of these same functions. We also present evidence suggesting that there may be another companion in a wider orbit. This would make it one of only three known systems hosting more than one transiting exoplanet. Keywords: Extrasolar planet - Radial velocity - Nearby stars",
        "watermark_text": "We report on the discovery by high - precision radial - speed measurements with the HARPS spectrograph of an extrasolar planet orbiting the nearby ( d = 8 . 3 pc ) M3V star GJ 674 , which is part of our ongoing search to find lowest - mass companions in small - duration orbits using this instrument . The new planet has a minimum mass of Mp = 1 . 1 MJup and a period P = 3 . 6 days .It was obtained through a combination of two independent methods : the evaluation of the bisector span of inter - correlation functions and that of the full - length at half - maximum of these same functions . We additionally include evidence indicating that there may be another companion in a greater orbit .This might making it one of only three known systems hosting more than one transiting exoplanet . Keywords : Extrasolar planet - Radial velocity - Nearby stars",
        "rewrite_text": "We report the discovery of an extrasolar planet orbiting the nearby M3V star GJ 674 (8.3 pc away) through high-precision radial velocity measurements with the HARPS spectrograph. This finding is part of our ongoing efforts to identify the lowest-mass companions in short-duration orbits using this instrument. The newly detected planet has a minimum mass of Mp = 1.1 MJup and an orbital period of P = 3.6 days. Our results were achieved by combining two independent methods: analyzing the bisector span of the cross-correlation functions and examining the full width at half maximum of these functions. Furthermore, we provide evidence that suggests the presence of another companion in a larger orbit, which could make this system one of only three known to host multiple transiting exoplanets. Keywords: Extrasolar planet - Radial velocity - Nearby stars.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We present the mid-infrared (MIR) spectrum of the central region in the Virgo galaxy cluster, obtained with Spitzer/IRS at high spatial resolution. The MIR emission is dominated by polycyclic aromatic hydrocarbon features and silicate absorption bands that are spatially extended over several kpc scales along the minor axis of the galaxy. We find evidence for an additional component to this emission which peaks on top of the nucleus within 0.5 arcsec (0.1 pc). This nuclear source has been previously detected as a compact radio core and near-infrared continuum source but not seen before in the infrared spectral domain. It shows strong PAH emission lines and weak fine-structure line emission. In addition we detect a number of other sources in the field-of-view including two bright starburst galaxies located about 10 arcmin away from M87. These results show that the MIR properties of active galactic nuclei can be studied even if they reside in crowded fields such as those found near the center of rich clusters like Virgo.",
        "watermark_text": "We present the mid - infrared ( MIR ) spectrum of the central region in the Virgo star cluster , obtained with Spitzer / IRS at high spatial resolution . The MIR emission is dominated by polycyclic aromatic hydrocarbon characteristics and silicate emission bands that are spatially extended over several kpc scales along the minor axis of the constellation .We get confirmation for an additional element to this emission which peaks on top of the nucleus within 0 . 5 arcsec ( 0 . 1 pc ) . This nuclear source has been previously observed as a compact radio core and near - infrared continuum source but not seen before in the infrared spectral domain .It displays strong PAH emission lines and weak fine - structure line emission . In addition we locate a number of other sources in the field - of - view including two bright starburst objects located about 10 arcmin away from M87 .These data demonstrate that the MIR properties of active galactic nuclei can be examined even if they live in busy fields such as those contained near the center of rich clusters like Virgo .",
        "rewrite_text": "We present the mid-infrared (MIR) spectrum of the central region of the Virgo star cluster, acquired using Spitzer/IRS with high spatial resolution. The MIR emission prominently features characteristics of polycyclic aromatic hydrocarbons and silicate emission bands that extend over several kiloparsecs along the minor axis of the cluster. We confirm the presence of an additional emission component that peaks at the nucleus, within 0.5 arcseconds (0.1 pc). This nuclear source, previously detected as a compact radio core and near-infrared continuum source, had not been observed in the infrared spectral range until now. It shows strong PAH emission lines along with weak fine-structure line emission. Moreover, we identify several other sources within the field of view, including two bright starburst objects approximately 10 arcminutes from M87. These findings indicate that the MIR properties of active galactic nuclei can be studied even in densely populated regions like those found near the centers of rich clusters such as Virgo.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "We present new numerical results on the structure of accretion disks in binary systems with a black hole (BH) as well as their observational signatures, including eclipses by the BH shadow and reflection features due to strong gravitational lensing. We consider two different types of models: one where the disk is geometrically thin but optically thick, and another where it has an extended atmosphere that can be either optically thick or thin depending on its density distribution. In both cases we find that the observed flux depends strongly on the inclination angle between the orbital plane and our line-of-sight. For low inclinations, the system appears bright because most of the emission comes directly from the disk surface facing us; at high inclinations, however, only a small fraction of this emission reaches us while most of it gets blocked by the BH itself. The resulting eclipse profiles are very sensitive to the spin parameterâ€Š=âˆš/M2 of the BH, which determines how much of the disk is obscured during each orbit. This effect could potentially provide a way to measure the spin of supermassive BHs using observations of X-ray binaries.",
        "watermark_text": "We consider new numerical findings on the composition of accretion disks in binary systems with a black hole ( BH ) as well as their observational signatures , notably eclipses by the BH cloud and reflection features due to powerful gravity lensing . We consider two different kinds of models : one where the disk is geometrically thin but optically dense , and another where it has an extended atmosphere that can be either optically dense or dense depending on its volume distribution .In both cases we find that the seen flux relies highly on the inclination angle between the orbital plane and our line - of - view . For low inclinations , the system appears brilliant because most of the emission originates directly from the disk surface facing us ; at high inclinations , however , only a small fraction of this emission hits us while most of it becomes blocked by the BH itself .The resulting eclipse profiles are very sensitive to the spin parameterâ€Š = [UNK] / M2 of the BH , which determines how much of the disk is obscured during each orbit . This effect could potentially provide a way to measure the spin of supermassive BHs using observations of X - ray binaries .",
        "rewrite_text": "We present new numerical results on the composition of accretion disks in binary systems that contain a black hole (BH) and investigate their observational signatures, particularly eclipses caused by the BH’s surrounding cloud and reflection features resulting from strong gravitational lensing. We examine two distinct model types: one where the disk is geometrically thin yet optically dense, and another where the disk has a more extended atmosphere that can be either optically dense or less dense depending on its volume distribution. In both scenarios, we find that the observed flux is heavily influenced by the inclination angle between the orbital plane and our line of sight. At low inclinations, the system appears exceptionally bright, as most emission comes directly from the portion of the disk surface facing us. Conversely, at high inclinations, only a small portion of this emission reaches us, while the majority is obscured by the BH itself. The resulting eclipse profiles are highly sensitive to the spin parameter (a* = J/M^2) of the BH, which affects the amount of the disk that is concealed during each orbit. This phenomenon could offer a method for estimating the spin of supermassive black holes through observations of X-ray binaries.",
        "ori-fast-z-score": 0.2182178902359924,
        "water-fast-z-score": 4.364357804719848,
        "rewrite-fast-z-score": 0.41702882811414954
    },
    {
        "original_text": "We present the BLOX cluster sample based on optical data obtained with Megacam at CFHT (Canada-France-Hawaii Telescope) in combination with Xray data taken by Chandra or XMMNewton. We use photometric redshifts to select galaxy clusters over an area of 1 deg2 around the center of the field-of-view of the Advanced Camera for Surveys aboard Hubble Space Telescope (HST). Our selection is based on the detection significance of overdensities in color-color space as well as their luminosity function. In addition we require that all candidates are detected in X-rays. This yields a total number of 1253 candidate galaxy clusters between z=0.3 and 0.9. For each cluster we provide its position, redshift, richness estimate, mass estimates derived from weak lensing analysis, and temperature measurements inferred from X-ray observations. A detailed description of our method can be found in this article.",
        "watermark_text": "We present the BLOX cluster sample based on optical data acquired with Megacam at CFHT ( Canada - France - Hawaii Telescope ) in combination with Xray information taken by Chandra or XMMNewton . We use photometric redshifts to select galaxy regions over an area of 1 deg2 around the center of the field - of - view of the Advanced Camera for Surveys aboard Hubble Space Telescope ( HST ) .Our choosing is based on the detection significance of overdensities in color - color area as also as their luminosity function . In addition we require that all candidates are detected in X - radiation .This yields a total number of 1253 candidate star clusters between z = 0 . 3 and 0 . 9 . For each cluster we provide its position , redshift , richness average , mass estimates derived from soft lensing imaging , and heat measurements inferred from X - ray observations .A full description of our technique can be found in this page .",
        "rewrite_text": "We present the BLOX cluster sample, which is based on optical data collected using Megacam at the Canada-France-Hawaii Telescope (CFHT), along with X-ray data obtained from Chandra and XMM-Newton. To identify galaxy regions, we employ photometric redshifts over an area of 1 square degree surrounding the center of the Advanced Camera for Surveys field of view on the Hubble Space Telescope (HST). Our selection process relies on the detection significance of overdensities in color-color space and their associated luminosity functions. Additionally, we ensure that all candidates are detected in X-ray emissions. This approach results in a total of 1,253 candidate star clusters with redshifts ranging from 0.3 to 0.9. For each cluster, we provide details including its position, redshift, average richness, mass estimates derived from soft lensing imaging, and temperature measurements obtained from X-ray observations. A comprehensive description of our methodology can be found on this page.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.635863249727653,
        "rewrite-fast-z-score": 1.4770978917519928
    },
    {
        "original_text": "The theory of supercoupling is developed to explain the confinement of electromagnetic waves inside narrow channels or tight bends using epsilon-near-zero metamaterials (ENZMs). The ENZM can be realized by embedding metallic wires into dielectric materials with low permittivity values at microwave frequencies. A theoretical model based on the transmission line method is used for analyzing the field distribution along the channel. It shows that the electric fields are confined within the ENZM layer when it is placed between two conventional dielectrics. This phenomenon leads to an increase in the effective refractive index of the structure which results in wave guiding through the channel. In addition, the effect of squeezing the energy density is investigated numerically. Finally, the proposed design concept is verified experimentally by fabricating a prototype of a supercoupled channel made up of three layers of different materials: air as top cladding, polytetrafluoroethylene (PTFE) as middle core, and copper wire mesh as bottom ground plane.",
        "watermark_text": "The theory of supercoupling is developed to explain the confinement of electromagnetic currents inside narrow channels or tight bends utilizing epsilon - near - zero metamaterials ( ENZMs ) . The ENZM can be realized by embedding metallic wires into dielectric materials with poor permittivity values at microwave frequencies .A conceptual theory based on the transmission line concept is utilized for evaluating the field flow along the channel . It says that the electric forces are localized within the ENZM layer when it is put between two conventional dielectrics .This phenomenon contributes to an increase in the effective refractive index of the structure which results in wave guiding through the channel . In addition , the impact of pressing the energy density is investigated numerically .Finally , the suggested design concept is tested experimentally by fabricating a prototype of a supercoupled channel making up of three sheets of different materials : air as top cladding , polytetrafluoroethylene ( PTFE ) as middle core , and copper metal mesh as bottom ground plane .",
        "rewrite_text": "The theory of supercoupling has been formulated to elucidate the confinement of electromagnetic currents within narrow channels or sharp bends using epsilon-near-zero metamaterials (ENZMs). ENZMs can be created by incorporating metallic wires into dielectric materials that exhibit low permittivity at microwave frequencies. A conceptual framework based on transmission line theory is employed to assess the field distribution along the channel. This framework suggests that electric fields become concentrated within the ENZM layer when positioned between two conventional dielectric materials. This effect leads to an enhanced effective refractive index for the structure, facilitating wave guiding through the channel. Furthermore, the effect of compressing the energy density is examined through numerical simulations. Finally, the proposed design concept is validated experimentally by constructing a prototype of a supercoupled channel comprised of three distinct layers: air as the top cladding, polytetrafluoroethylene (PTFE) as the core, and a copper mesh serving as the bottom ground plane.",
        "ori-fast-z-score": 1.3949716649258315,
        "water-fast-z-score": 6.812733176437583,
        "rewrite-fast-z-score": 0.6974858324629157
    },
    {
        "original_text": "We study the nonlocal excitation spectra for doped Mott insulators by using the dynamical cluster approximation (DCA) combined with the exact diagonalization method and the Lanczos algorithm. We show that the doping dependence of the low-energy part of the spectrum is well reproduced even at half-filling, where the system has no charge carriers. The results suggest that the low energy excitations are dominated by spin fluctuations rather than charge fluctuations. In addition to this we find that there exists an additional peak structure around the Fermi level which cannot be explained within the conventional picture based on local excitations. These findings may provide important information about the nature of electronic states near the metal-insulator transition point. PACS numbers: 71.10.Pm, 72.20.-i, 73.40.Gk \nI. INTRODUCTORY REMARK\nThe two-dimensional (2D) doped Mott insulator is one of the most interesting subjects in condensed matter physics because it can exhibit various types of novel phenomena such as high-Tc superconductivity  1  , colossal magnetoresistance  2  , and quantum Hall effect  3  . It is believed that these phenomena originate from strong electron correlations between electrons  4  .\nIn order to understand the physical properties of strongly correlated systems theoretically, many numerical methods have been developed so far  5  -  8  . Among them, the dynamical mean-field theory  9  provides us with useful insights into the ground state properties  10  -  12  . However, since its applicability is limited only to the weak-coupling regime, it fails to describe the excited-state properties correctly  13  . On the other hand, the density matrix renormalization group  14  gives accurate results for both ground-and excited-states  15  but requires huge computational resources when applied to large clusters  16  . Therefore, it would be desirable if some efficient numerical techniques could be found to treat both ground-",
        "watermark_text": "We explore the nonlocal excitation spectra for doped Mott insulators by using the dynamical cluster algorithm ( DCA ) coupled with the exact diagonalization technique and the Lanczos algorithm . We see that the doping dependence of the small - energy part of the spectrum is well displayed especially at half - filling , where the system has no charge carriers .The results show that the small power excitations are dominated by spin fluctuations rather than charge fluctuations . In addition to this we find that there exists an additional peak structure around the Fermi level which cannot be described within the usual picture based on local excitations .These conclusions could give important information about the nature of electronic states near the metal - insulator transition point . PACS codes : 71 . 10 . Pm , 72 . 20 . - i , 73 . 40 . Gk I .INTRODUCTORY REMARK The two - dimensional ( 2D ) doped Mott insulator is one of the most exciting subjects in condensed matter science because it can exhibit several kinds of new events such as high - Tc superconductivity 1 , colossal magnetoresistance 2 , and quantum Hall impact 3 . It is suspected that these phenomena originate from strong electron correlations between electrons 4 .In order to comprehend the physical properties of highly correlated systems theoretically , various numerical methods have been constructed so far 5 - 8 . Among them , the dynamical mean - field model 9 offers us with useful insights into the ground state properties 10 - 12 .However , since its applicability is limited only to the weak - correlation regime , it fails to explain the excited - state properties correctly 13 . On the other hand , the density matrix renormalization group 14 provides reliable results for both ground - and excited - states 15 but requires massive computational resources when applied to large clusters 16 .Therefore , it would be desirable if some effective numerical techniques could be found to treat both ground -",
        "rewrite_text": "We investigate the nonlocal excitation spectra of doped Mott insulators by employing the dynamical cluster algorithm (DCA) in conjunction with exact diagonalization and the Lanczos algorithm. Our analysis reveals that the doping dependence of the low-energy portion of the spectrum, particularly at half-filling—where no charge carriers are present—is clearly illustrated. The findings indicate that low-energy excitations are primarily influenced by spin fluctuations rather than charge fluctuations. Additionally, we uncover an extra peak structure near the Fermi level that cannot be explained using the traditional framework grounded in local excitations. These results could provide crucial insights into the nature of electronic states close to the metal-insulator transition point. \n\n**PACS codes:** 71.10.Pm, 72.20.-i, 73.40.Gk\n\n**I. INTRODUCTORY REMARK** \n\nThe two-dimensional (2D) doped Mott insulator represents one of the most captivating areas in condensed matter physics due to its potential to exhibit various novel phenomena, including high-temperature superconductivity, colossal magnetoresistance, and quantum Hall effects. These behaviors are believed to arise from strong electron correlations. To theoretically understand the physical properties of these highly correlated systems, numerous numerical methods have been developed. Among these, the dynamical mean-field theory provides valuable insights into ground-state properties. However, its application is confined to regimes of weak correlation, rendering it inadequate for accurately describing excited-state properties. Conversely, the density matrix renormalization group yields reliable outcomes for both ground and excited states but is computationally intensive when applied to large clusters. Thus, developing effective numerical techniques that can address both ground and excited states remains a significant goal.",
        "ori-fast-z-score": 0.5262348115842176,
        "water-fast-z-score": 7.023590753145371,
        "rewrite-fast-z-score": 0.3481553119113957
    },
    {
        "original_text": "We present the first constraints on dark matter halos in the early universe using data from the Hubble Space Telescope (HST) Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS). We use photometric redshifts to select galaxies with stellar masses greater than 10$^{10}$ Msun/hour between 0 < z < 8.5. Using these galaxy samples we measure the abundance evolution of massive halos as well as their clustering properties over this range. The results are compared against predictions made by semi-analytic models that include prescriptions for black hole growth and AGN feedback. Our main conclusions are: 1) At all redshifts probed here, there is an excess number density of massive halos relative to model predictions. 2) This discrepancy increases towards higher redshifts. 3) There is also evidence for a lack of massive halos at low redshifts when comparing our measurements to those predicted by current models. 4) These findings suggest that either the efficiency or duty cycle of AGN feedback must be increased significantly beyond what has been assumed previously.",
        "watermark_text": "We introduce the first limitations on dark matter halos in the early universe using data from the Hubble Space Telescope ( HST ) Cosmic Assembly Near - Infrared Deep Extragalactic Legacy Survey ( CANDELS ) . We use photometric redshifts to select clusters with stellar masses greater than 10 $ ^ { 10 } $ Msun / hour between 0 < z < 8 . 5 .Using these galaxy samples we measure the density evolution of large halos as well as their clustering behavior over this range . The results are compared against models provided by semi - analytic models that include prescriptions for black hole growth and AGN feedback .Our main results are : 1 ) At all redshifts probed here , there is an increase number density of large halos relative to model observations . 2 ) This discrepancy increases towards higher redshifts .3 ) There is also evidence for a failure of large halos at low redshifts when comparing our measurements to those predicted by current estimates . 4 ) These conclusions propose that either the performance or duty cycle of AGN feedback could be improved substantially beyond what has been claimed ago .",
        "rewrite_text": "We present the first constraints on dark matter halos in the early universe, utilizing data from the Hubble Space Telescope's Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS). By employing photometric redshifts, we identify clusters with stellar masses exceeding 10^10 Msun/h in the redshift range of 0 < z < 8.5. With these galaxy samples, we investigate the density evolution and clustering behavior of large halos throughout this redshift interval. Our findings are compared to predictions from semi-analytic models that incorporate black hole growth and AGN feedback. Our key conclusions are: 1) At all examined redshifts, we observe an increased number density of large halos compared to model predictions. 2) This discrepancy becomes more pronounced at higher redshifts. 3) Additionally, we find evidence of a shortfall in the number of large halos at lower redshifts when juxtaposed with current predictions. 4) These results suggest that the effectiveness or duty cycle of AGN feedback may need significant enhancement compared to previous claims.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 4.541868715470696,
        "rewrite-fast-z-score": -0.1203858530857692
    },
    {
        "original_text": "We propose an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one-dimensional (1-D) measurements, i.e., link counts between pairs of nodes in the network. The proposed method can be applied to any type of networks and does not require any prior knowledge about their topology or traffic patterns. We show how our approach can be used to estimate the number of active flows at each node as well as the amount of data transmitted over each flow. Our results are validated through extensive simulations performed with real Internet traces. Network tomography has been widely studied during recent years due to its potential applications in many areas such as computer security, quality-of-service provisioning, and traffic engineering  1  . In this context, it consists of estimating some properties of the network s internal state (such as the number of active flows per node or the amount of data transferred along each flow) by observing only external information (i.e., link-level statistics). This problem becomes particularly challenging when dealing with large-scale networks since the number of possible states grows exponentially with the size of the network  2  .\nIn order to overcome these limitations, several approaches have been recently proposed which exploit specific characteristics of the underlying network  3  , e.g., sparsity  4  -  6  , symmetry  7  , or regularity  8  . However, most existing methods assume either complete knowledge of the network topology  9 -  11  or accurate estimates of the traffic matrix  12  -  14  . Unfortunately, both assumptions may not hold in practice  15  , especially if we consider large and/or dynamic networks  16  . For example, in IP-based networks, the exact location of routers cannot always be determined  17  while the traffic matrix is usually unknown  18  . Moreover, even if the network topology were known, collecting all necessary information would still be impractical because of scalability issues  19  . Finally, obtaining accurate estimates of the traffic...",
        "watermark_text": "We suggest an algorithm for network tomography that is able to reconstruct the internal structure of a network by using only one - dimensional ( 1 - D ) observations , i . e . , link counts between pairs of nodes in the network . The proposed approach can be applied to any type of networks and does not require any earlier knowledge about their topology or traffic behavior .We see how our approach can be used to estimate the total of active flows at each node as also as the quantity of content conveyed over each flow . Our results are validated through ongoing simulations conducted with real Internet traces .Network tomography has been widely explored during recent years owing to its potential applications in different areas such as data security , quality - of - service provisioning , and route management 1 . In this context , it consists of estimating some properties of the organization s internal state ( such as the number of active flows per node or the quantity of content transferred along each flow ) by observing only external information ( i . e . , link - level statistics ) .This problem remains particularly challenging when dealing with large - scale networks since the number of possible states expands exponentially with the length of the network 2 . In try to overcome these limitations , various approaches have been lately advocated which use particular features of the underlying network 3 , e . g . , sparsity 4 - 6 , symmetry 7 , or regularity 8 .However , most existing techniques assume either complete understanding of the traffic topology 9 - 11 or accurate calculations of the traffic matrix 12 - 14 . Unfortunately , both conclusions may not hold in practice 15 , particularly if we study huge and / or dynamic networks 16 .For instance , in IP - based networks , the exact location of routers never always be determined 17 while the traffic matrix is usually unknown 18 . Moreover , even if the network topology were known , compiling all necessary data would still be impractical because of scalability concerns 19 .Finally , obtaining adequate figures of the traffic . . .",
        "rewrite_text": "We propose a novel algorithm for network tomography that can reconstruct a network's internal structure using only one-dimensional (1-D) observations, specifically link counts between pairs of nodes. This method is versatile and applicable to various types of networks, requiring no prior knowledge of their topology or traffic patterns. Our approach enables the estimation of the total number of active flows at each node, as well as the volume of content transmitted through each flow. We validate our findings through simulations that utilize real Internet traffic data. \n\nIn recent years, network tomography has gained significant attention due to its potential applications in areas such as data security, quality-of-service provisioning, and route management. It involves estimating properties of a network's internal state, such as the number of active flows per node or the amount of data transferred across each flow, based solely on external observations, i.e., link-level statistics. This task becomes particularly challenging in large-scale networks, where the number of possible states grows exponentially with network length. \n\nTo address these challenges, various recent approaches have leveraged specific features of the underlying network, such as sparsity, symmetry, or regularity. However, many existing methods rely on a complete understanding of traffic topology or precise calculations of the traffic matrix. Unfortunately, these assumptions often do not hold true, especially in large or dynamic networks. For example, in IP-based networks, pinpointing the exact location of routers can be difficult, while the traffic matrix is typically unknown. Moreover, even if the network's topology were known, gathering all necessary data would still be impractical due to scalability issues. Ultimately, obtaining accurate traffic metrics remains a significant challenge.",
        "ori-fast-z-score": -0.8723567442899586,
        "water-fast-z-score": 6.899548795747854,
        "rewrite-fast-z-score": -0.16116459280507606
    },
    {
        "original_text": "We study the thermal properties of symmetric and asymmetric nuclear matter using an extended Thomas-Fermi model based on a momentum dependent effective nucleon-nucleon (NN) interaction, which is derived by solving the Bethe-Goldstone equation in ladder approximation. The results show that the density dependence of nuclear symmetry energy at normal nuclear matter density changes significantly when temperature increases up to 100 MeV. In addition, we find that the slope parameter L(ρ0), characterizing the density dependence of nuclear incompressibility K∞ = 9L(ρ0)(3π2ρ0/40MeV)2, decreases rapidly as temperature rises for both pure neutron matter and symmetric nuclear matter. This indicates that the stiffness of nuclear matter becomes softer at high temperatures.  We also calculate the pressure P , entropy S and specific heat Cv of nuclear matter as functions of baryonic number density nB and temperature T .",
        "watermark_text": "We research the thermal properties of symmetric and asymmetric atomic matter utilizing an extended Thomas - Fermi model based on a momentum dependent effective nucleon - nucleon ( NN ) collision , which is calculated by solving the Bethe - Goldstone equation in ladder approximation . The results show that the density relation of nuclear symmetry power at typical atomic matter density changes significantly when pressure changes up to 100 MeV .In addition , we find that the slope parameter L ( ρ0 ) , characterizing the density relation of nuclear incompressibility K∞ = 9L ( ρ0 ) ( 3π2ρ0 / 40MeV ) 2 , decreases quickly as temperature rises for both solid neutron particles and symmetric nuclear material . This implies that the stiffness of nuclear material gets softer at high temperatures .We additionally calculate the pressure P , entropy S and particular heat Cv of nuclear material as functions of baryonic number density nB and heat T .",
        "rewrite_text": "We investigate the thermal properties of both symmetric and asymmetric atomic matter using an extended Thomas-Fermi model, which incorporates a momentum-dependent effective nucleon-nucleon (NN) collision. This is achieved by solving the Bethe-Goldstone equation in its ladder approximation. Our findings indicate that the density relationship of nuclear symmetry energy at typical atomic matter densities undergoes significant changes when the pressure varies up to 100 MeV. Furthermore, we observe that the slope parameter L(ρ₀), which describes the density dependence of nuclear incompressibility Kₓₒ = 9L(ρ₀)(3π²ρ₀/40 MeV)², decreases rapidly with increasing temperature for both solid neutron matter and symmetric nuclear matter. This suggests that the stiffness of nuclear material decreases at elevated temperatures. Additionally, we compute pressure P, entropy S, and specific heat Cv of nuclear matter as functions of baryonic number density n₍ʙ₎ and temperature T.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 6.139678507374229,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We present an analysis of the accuracy with which different approximants to gravitational-wave (GW) signals emitted by coalescing binaries can be recovered using matched filtering techniques, in particular when applied to simulated detector noise. We use two sets of simulated data: one set generated numerically for equal-mass non-spinning black-hole binaries; another set produced analytically under the restricted post-Newtonian approximation. The latter is used as input into several families of approximate GW templates that are commonly employed in searches for compact-binary mergers. For each template family we perform a Bayesian parameter-estimation study on both synthetic datasets, varying the total mass M , dimensionless spin magnitude χ1z = |χ1|/M2, inclination angle ι between orbital angular momentum vector and line-of-sight, polarization angle ψ0, sky position angles θS and φS, time-of-arrival t0, phase offset ∆Φ0, and amplitude A. In addition, we also vary the distance D to the source. Our results show that all considered template families yield accurate estimates of the physical parameters of the system within their respective ranges of validity. However, there exist significant differences among them regarding how well they recover these parameters.",
        "watermark_text": "We present an assessment of the accuracy with which different approximants to gravitational - wave ( GW ) transmissions generated by coalescing binaries can be recovered using matched filtering tactics , in example when applied to modeled detector noise . We use two sets of simulated evidence : one set produced numerically for equivalent - mass non - spinning black - hole binaries ; another set produced analytically under the restricted pre - Newtonian approximation .The latter is utilized as input into numerous families of approximate GW templates that are often employed in searches for compact - binary mergers . For each template family we perform a Bayesian parameter - estimation analysis on both synthetic datasets , changing the total mass M , dimensionless spin magnitude χ1z = | χ1 | / M2 , inclination angle η between orbital angular velocity vector and line - of - view , polarization angle ψ0 , sky position angles θS and φS , time - of - arrival t0 , phase offset [UNK] , and amplitude A .In addition , we also varies the distance D to the origin . Our results show that all considered template groups yield reliable estimates of the physical components of the system within their different ranges of relevance .However , there remain considerable variations among them governing how best they recover these parameters .",
        "rewrite_text": "We evaluate the accuracy with which various approximants to gravitational wave (GW) emissions from coalescing binaries can be retrieved using matched filtering techniques, specifically in the context of synthetic detector noise. Our assessment is based on two sets of simulated data: one set generated numerically for equal-mass, non-spinning black hole binaries, and another set derived analytically under a simplified pre-Newtonian model. The latter serves as input for multiple families of approximate GW templates commonly used in searches for compact binary mergers. For each template family, we conduct a Bayesian parameter estimation analysis on both synthetic datasets, varying parameters such as the total mass \\( M \\), dimensionless spin magnitude \\( \\chi_{1z} = | \\chi_{1} | / M_{2} \\), the inclination angle \\( \\eta \\) between the orbital angular velocity vector and the line of sight, the polarization angle \\( \\psi_{0} \\), the sky position angles \\( \\theta_{S} \\) and \\( \\phi_{S} \\), time of arrival \\( t_{0} \\), phase offset, amplitude \\( A \\), and also the distance \\( D \\) to the source. Our findings indicate that all examined template families provide reliable estimates of the physical parameters of the system within their respective domains of applicability. However, there are significant differences among these families regarding their effectiveness in accurately recovering these parameters.",
        "ori-fast-z-score": -0.8955334711889903,
        "water-fast-z-score": 5.148767223478707,
        "rewrite-fast-z-score": -0.40406101782088427
    },
    {
        "original_text": "Spin-based electronics is an emerging field that has attracted considerable attention in recent years, due to its potential for applications such as high-density data storage and high-speed information processing.  In this review article we discuss the basic concepts underlying bipolar spintronic devices based on semiconductor heterostructures with ferromagnetic contacts. We first introduce the physics behind spin injection into semiconductors using tunnel barriers or Schottky diodes. Then we describe how these injected spins can be manipulated by means of external magnetic fields and/or electric currents. Finally, we present some examples of spintronic devices including spin-LEDs, spin transistors, and spin-logic circuits. The main focus will be put on GaAs-based structures but also other materials systems are discussed briefly. This article is intended to provide a comprehensive overview of the state-of-the-art research in the field of bipolar spintronics. It should serve both as a guide for newcomers interested in learning about the fundamentals of spin transport phenomena at interfaces between metals and semiconductors, and as a reference source for researchers working in related areas.",
        "watermark_text": "Spin - based computing is an developing field that has garnered considerable scrutiny in recent years , owing to its potential for applications such as high - density data storage and large - speed information processing . In this review article we explain the fundamental concepts governing bipolar spintronic systems based on semiconductor heterostructures with ferromagnetic contacts .We first introduce the physics behind spin injection into semiconductors using tunnel fences or Schottky diodes . Then we explain how these injected spinning can be manipulated by means of external magnetic waves and / or electric currents .Finally , we present some examples of spintronic systems including spin - LEDs , spin transistors , and spin - logic devices . The main attention will be place on GaAs - based assemblies but also other materials systems are discussed briefly .This page is intended to provide a comprehensive overview of the state - of - the - art studies in the field of bipolar spintronics . It should serve both as a reference for newcomers interested in knowing about the fundamentals of spin transport phenomena at connections between metals and semiconductors , and as a reference source for researchers working in related fields .",
        "rewrite_text": "Spin-based computing is an emerging field that has attracted significant attention in recent years due to its potential applications in high-density data storage and high-speed information processing. In this review article, we outline the fundamental principles that govern bipolar spintronic systems, particularly those based on semiconductor heterostructures with ferromagnetic contacts. We begin by discussing the physics behind spin injection into semiconductors using tunneling barriers or Schottky diodes. Next, we explain how the injected spins can be controlled using external magnetic fields and electric currents. Additionally, we showcase various examples of spintronic systems, including spin-LEDs, spin transistors, and spin-logic devices, with a primary focus on GaAs-based structures, while also briefly addressing other materials. This article aims to provide a comprehensive overview of the latest advancements in the field of bipolar spintronics, serving as a valuable resource for newcomers seeking to understand the fundamentals of spin transport at metal-semiconductor interfaces, as well as for researchers engaged in related disciplines.",
        "ori-fast-z-score": 0.4216370213557839,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 1.4100479758212652
    },
    {
        "original_text": "We propose that the vacuum state is not empty but contains fluctuations in spacetime, which we call holographic noise (HN). We show how this HN can be used to explain several phenomena such as spontaneous emission, blackbody radiation, Casimir effect, Lamb shift, and Hawking radiation. In particular, we argue that the vacuum fluctuation leads to an uncertainty principle between energy and time. This implies that there are no particles with zero mass or spin. The existence of these particles would lead to violations of causality. Finally, we discuss some possible experimental tests for our proposal. Vacuum fluctuations play important roles in quantum field theory. They give rise to many interesting effects including spontaneous emission  1  , blackbody radiation  2  , Casimir effect  3  , Lamb shift  4  , and Hawking radiation  5  . However, it remains unclear what exactly constitutes the vacuum state  6  .\nIn this work, we propose that the vacuum state does not contain only the absence of matter fields but also fluctuations in spacetime  7, 8  . These fluctuations may be viewed as virtual gravitons  9  . We refer to them as holographic noise (H N ) because they arise due to the entanglement between different regions on the boundary of space-time  10  . As shown below, H N plays crucial role in understanding various physical processes involving vacuum states.\nThe main idea behind our approach is illustrated by Fig.  1(a) . Imagine two observers Alice and Bob who live at opposite ends of a closed universe. Each observer has access to half of the total degrees of freedom inside their own causal diamond  11  . For example, if Alice lives near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future light cone. Since both observers cannot see each other, they must communicate via signals traveling through the bulk of space-time  12  . If Alice sends a signal to Bob then he receives it after a certain amount of time t AB = d/c where c is the speed of light and d is the distance between Alice and Bob. On the other hand, if Bob sends",
        "watermark_text": "We suggest that the vacuum state is not filled but contains fluctuations in spacetime , which we call holographic noise ( HN ) . We see how this HN can be used to explain different processes such as spontaneous emission , blackbody radiation , Casimir effect , Lamb shift , and Hawking radiation .In particular , we claim that the vacuum fluctuation leads to an uncertainty theory between energy and time . This implies that there are no particles with zero mass or spin .The existence of these objects would result to violations of causality . Finally , we talk some possible experimental tests for our proposal .Vacuum fluctuations represent crucial roles in quantum field theory . They give rise to many interesting phenomena including spontaneous emission 1 , blackbody radiation 2 , Casimir effect 3 , Lamb shift 4 , and Hawking radiation 5 .However , it remains unsure what actually constitutes the vacuum state 6 . In this research , we claim that the vacuum state does not include only the absence of mind fields but also fluctuations in spacetime 7 , 8 .These fluctuations might be viewed as virtual gravitons 9 . We refer to them as holographic disturbance ( H N ) because they occur due to the entanglement between various regions on the boundary of space - time 10 .As seen below , H N plays crucial role in understanding various mechanical phenomena involving vacuum states . The main idea behind our approach is illustrated by Fig .1 ( a ) . Imagine two observers Alice and Bob who living at different edges of a closed world .Each observer has entry to half of the total degrees of autonomy inside their own causal diamond 11 . For instance , if Alice resides near the center of her universe she will have access to all information about events within her past light cone while Bob s knowledge is limited to his future dark cone .Since both observers cannot see each other , they must interact via signals going through the bulk of space - time 12 . If Alice sends a signal to Bob then he gets it after a certain amount of time t AB = d / c where l is the speed of light and d is the distance between Alice and Bob .On the other hand , if Bob sends",
        "rewrite_text": "We propose that the vacuum state is not an empty void but instead features fluctuations in spacetime, which we refer to as holographic noise (HN). This HN may help explain various phenomena, including spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. Notably, we argue that these vacuum fluctuations give rise to an uncertainty relationship between energy and time, implying the non-existence of particles with zero mass or spin due to potential violations of causality. Additionally, we outline potential experimental tests to validate our hypothesis. Vacuum fluctuations play a vital role in quantum field theory and contribute to many intriguing effects, such as spontaneous emission, blackbody radiation, the Casimir effect, the Lamb shift, and Hawking radiation. Nevertheless, the exact nature of the vacuum state remains uncertain. In our study, we assert that the vacuum encompasses not only the absence of mind fields but also spacetime fluctuations. These fluctuations could be interpreted as virtual gravitons, hence we term them holographic disturbances (HN) due to their origin in the entanglement among different regions at the boundaries of spacetime. As we will demonstrate, HN are essential for understanding various phenomena associated with vacuum states. Our main concept is illustrated in Fig. 1(a). Picture two observers, Alice and Bob, residing at opposite ends of a closed universe. Each has access to half of the total degrees of freedom within their respective causal diamonds. For example, if Alice is located near the center of her universe, she can access all information pertaining to events within her past light cone, while Bob's knowledge is confined to his future dark cone. Since they cannot directly observe one another, they must communicate via signals traversing the bulk of spacetime. If Alice sends a signal to Bob, he will receive it after a time period t_AB = d/c, where c is the speed of light and d is the distance between them. Conversely, if Bob sends...",
        "ori-fast-z-score": 1.8717134551736667,
        "water-fast-z-score": 8.111071056538128,
        "rewrite-fast-z-score": 1.8461989882527161
    },
    {
        "original_text": "We study the relationship between distillable entanglement, entropy-area law (EAL), and von Neumann entropy for two classes of quantum systems -spin chains with nearest-neighbor interactions and harmonic oscillator lattices. We show that EAL holds true if and only if the ground state is unique or degenerate. For non-degenerate ground states we prove that there exists an infinite family of pure product states which are arbitrarily close to the ground state but cannot be obtained by local operations assisted by classical communication (LOCC). This implies that the amount of distillable entanglement can be strictly smaller than the von Neumann entropy of the ground state. Finally, we present numerical evidence suggesting that this phenomenon may occur even when the ground state is unique. The results presented here provide further insight into the nature of entanglement in many-body quantum systems. Entanglement plays a crucial role in various applications ranging from quantum information theory  1  , condensed matter physics  2  , and statistical mechanics  3  . In particular, it has been shown  4  that the ability to create maximally entangled pairs of qubits via LOCC is equivalent to the existence of Bell inequalities  5  .\nIn recent years much attention was devoted to understanding how entanglement behaves under different physical conditions  6  . It turns out  7, 8  that the behavior of entanglement depends on whether the underlying Hamiltonian satisfies certain properties such as uniqueness  9  or degeneracy  10  of its ground state. Moreover, it was found  11  that the presence of multiple ground states leads to a violation of the so-called entropyarea law  12  . However, despite these advances our knowledge about the structure of entanglement in manybody quantum systems remains incomplete  13  .\nThe main goal of this work is to investigate the relation between distillable entanglements  14  , entropy-area law  15  , and von Neumann entropy  16  for two classes of quantum sys-tems -spin chains  17  with nearest neighbor interactions  18  and harmonic oscillator lattices  19  . Our analysis reveals several interesting features of entanglement in many body quantum systems. First, we show that EAL  20  holds true if and only  21  if the ground state  22  is unique",
        "watermark_text": "We explore the relationship between distillable entanglement , entropy - area law ( EAL ) , and von Neumann entropy for two families of quantum systems - spinning chains with nearest - neighbor interactions and harmonic oscillator lattices . We see that EAL holds true if and only if the ground state is unique or degenerate .For non - degenerate ground states we prove that there exists an endless class of simple product states which are arbitrarily close to the ground state but cannot be obtained by local operations accompanied by classical communication ( LOCC ) . This implies that the extent of distillable entanglement can be strictly lower than the von Neumann entropy of the ground state .Finally , we present numerical findings indicating that this phenomenon might arise even when the ground state is unique . The results presented here provide further insight into the nature of entanglement in multiple - bodies quantum systems .Entanglement plays a crucial role in different applications diverse from quantum information physics 1 , condensed matter science 2 , and statistical mechanics 3 . In particular , it has been shown 4 that the ability to create maximally entangled combinations of qubits via LOCC is analogous to the existence of Bell inequalities 5 .In past decades considerable focus was devoted to discovering how entanglement behaves under various mechanical circumstances 6 . It turns out 7 , 8 that the activity of entanglement determines on whether the fundamental Hamiltonian satisfies certain characteristics such as uniqueness 9 or degeneracy 10 of its ground state .Moreover , it was shown 11 that the presence of multiple ground elements results to a violation of the so - called entropyarea law 12 . However , despite these developments our information about the dynamics of entanglement in manybody particle networks continues unfinished 13 .The main goal of this research is to examine the relation between distillable entanglements 14 , entropy - area law 15 , and von Neumann entropy 16 for two families of quantum sys - tems - spin rings 17 with nearest neighbor interactions 18 and harmonic oscillator lattices 19 . Our study reveals several interesting features of entanglement in different body quantum systems .First , we prove that EAL 20 stands true if and only 21 if the ground state 22 is unique",
        "rewrite_text": "We investigate the connections between distillable entanglement, the entropy-area law (EAL), and von Neumann entropy within two types of quantum systems: spin chains with nearest-neighbor interactions and harmonic oscillator lattices. Our findings indicate that EAL is valid if and only if the ground state is either unique or degenerate. For cases with non-degenerate ground states, we demonstrate the existence of an infinite class of simple product states that are arbitrarily close to the ground state but cannot be generated through local operations combined with classical communication (LOCC). This reveals that the amount of distillable entanglement can be strictly less than the von Neumann entropy of the ground state. Additionally, we present numerical results suggesting that this phenomenon may occur even when the ground state is unique. The insights gained from this study enhance our understanding of entanglement in many-body quantum systems. Entanglement is essential for a range of applications across fields such as quantum information science, condensed matter physics, and statistical mechanics. Notably, it has been established that the capacity to generate maximally entangled states of qubits through LOCC is related to the presence of Bell inequalities. Significant attention has been devoted to understanding the behavior of entanglement under various mechanical conditions over the past few decades. It has been found that the dynamics of entanglement are influenced by whether a fundamental Hamiltonian has certain properties, such as the uniqueness or degeneracy of its ground state. Furthermore, evidence suggests that the existence of multiple ground states leads to violations of the entropy-area law. Despite these advancements, our understanding of entanglement dynamics in many-body systems remains incomplete. The primary objective of this research is to analyze the relationship between distillable entanglement, the entropy-area law, and von Neumann entropy for two families of quantum systems: spin rings with nearest-neighbor interactions and harmonic oscillator lattices. Our study uncovers various intriguing aspects of entanglement across different quantum systems. Initially, we prove that the entropy-area law holds true if and only if the ground state is unique.",
        "ori-fast-z-score": 0.24576957615571215,
        "water-fast-z-score": 8.001666493091715,
        "rewrite-fast-z-score": 0.24576957615571215
    },
    {
        "original_text": "The authors present an overview of the role that knots play in proteins, with particular emphasis on their function and evolution.  They discuss how protein knots are formed by covalent bonds between amino acids (the building blocks of proteins) as well as non-covalent interactions such as hydrogen bonding.  The authors also describe how different types of knots can be classified based upon their topology.   Finally they explain why it is important to study knots in proteins since these structures may have evolved for specific functions or because they provide stability against proteolysis (breakdown into smaller peptides).    This article was originally published on BioMed Central. It has been re-posted here under Creative Commons License 3.0. Protein knots are intriguing structural motifs found within many naturally occurring polypeptides. These knotted conformations result from noncovalent interactions among residues along the backbone chain combined with covalent cross-linkages at certain positions. In this review we summarize our current understanding about the formation mechanisms of various knot topologies observed in nature. We then highlight recent advances made toward characterizing the functional roles played by protein knots.",
        "watermark_text": "The authors present an overview of the importance that knots play in proteins , with particular focuses on their function and evolution . They explore how protein knots are created by covalent interactions between amino acids ( the built stones of proteins ) as well as non - covalent interactions such as carbon bonding .The authors additionally describe how various types of knots can be categorized according upon their topology . Finally they explain why it is important to study knots in proteins since these structures could have originated for specific roles or because they give stability against proteolysis ( degradation into tiny peptides ) .This section was originally published on BioMed Central . It has been re - posted here under Creative Commons License 3 . 0 .Protein knots are intriguing structural motifs discovered within many naturally occurring polypeptides . These knotted conformations result from noncovalent interactions among proteins along the backbone ring coupled with covalent cross - linkages at different positions .In this review we summarize our latest understanding about the formation patterns of several knot topologies discovered in nature . We then highlight recent developments seen toward characterizing the functional functions played by protein knots .",
        "rewrite_text": "The authors provide a comprehensive overview of the role that knots play in proteins, emphasizing both their functions and evolutionary significance. They discuss the formation of protein knots, which arise from covalent interactions between amino acids—the fundamental building blocks of proteins—as well as non-covalent interactions like carbon bonding. Furthermore, the authors categorize different types of knots based on their topological properties. They also highlight the significance of studying protein knots, as these structures may have evolved for specific functions or may confer stability against proteolysis (the breakdown into smaller peptides). This section was originally published by BioMed Central and is re-posted here under Creative Commons License 3.0. Protein knots are fascinating structural motifs found in many naturally occurring polypeptides, resulting from non-covalent interactions along the protein backbone and covalent cross-linkages at various sites. In this review, we summarize the current understanding of the formation patterns of several knot topologies found in nature and discuss recent advancements in characterizing the functional roles of protein knots.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 6.88998622004134,
        "rewrite-fast-z-score": -0.10721125348377948
    },
    {
        "original_text": "We have observed the infrared colors (J-H, H-K) for 16 Mira variables with high resolution spectroscopy in order to investigate their relation to intensity ratios of SiO maser lines at 43 GHz. The results show that there is no correlation between these two parameters except for one star. We suggest that this may be due to different physical conditions among individual stars or differences in mass loss rates.  Keywords: Mira variable, Correlation, Infrared color, SiO maser line, Mass loss rate. 1 Introduction Miras are red giant stars which pulsate radially on time scales ranging from 100 days up to several thousand years. They exhibit large amplitude variations in luminosity as well as radial velocity. Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days  1  . These stars are known to produce strong winds  2  , and they also emit intense radio waves  3  .\nThe SiO molecule has been found to exist in many types of astronomical objects such as late-type stars  4  , evolved massive stars  5  , young stellar objects  6  , comets  7  , and planets  8  . It is believed that SiO molecules play an important role in the formation process of dust grains  9  . SiO masers were first detected toward AGB stars  10  . Since then, SiO masers have been studied extensively towards both AGB stars  11  -  13  and post-AGB stars  14  -  16  . Many studies have shown that the properties of SiO masers depend strongly on the evolutionary stage  17  -  20  . For example, it was reported that the peak flux density decreases rapidly during the transition phase from AGB to post-AGB  21  .",
        "watermark_text": "We have noted the infrared colors ( J - H , H - K ) for 16 Mira variables with high resolution spectroscopy in trying to examine their connection to intensity ratios of SiO maser lines at 43 GHz . The results show that there is no correspondence between these two parameters except for one star .We suggest that this might be due to different physical conditions among individual stars or variations in mass loss pressures . Keywords : Mira variable , Correlation , Infrared color , SiO maser line , Mass loss rate .1 Introduction Miras are red giant stars which pulsate radially on time ranges ranging from 100 hours up to several thousand years . They display large intensity variations in luminosity as well as radial speed .Their light curves can be described by a simple sinusoidal function with periods longer than about 300 days 1 . These galaxies are known to produce violent winds 2 , and they even emit intense radio pulses 3 .The SiO molecule has been shown to exist in multiple types of astronomical bodies such as early - class stars 4 , evolved large planets 5 , early stellar bodies 6 , comets 7 , and planets 8 . It is suspected that SiO compounds play an important role in the formation reaction of dust grains 9 .SiO masers were first detected toward AGB stars 10 . Since then , SiO masers have been studied frequently towards both AGB stars 11 - 13 and post - AGB stars 14 - 16 .Many experiments have shown that the properties of SiO masers depend greatly on the evolutionary stage 17 - 20 . For instance , it was reported that the maximum flux concentration drops rapidly during the shift stage from AGB to post - AGB 21 .",
        "rewrite_text": "We have observed the infrared colors (J - H, H - K) of 16 Mira variables using high-resolution spectroscopy to investigate their relationship with the intensity ratios of SiO maser lines at 43 GHz. Our findings indicate that, aside from one star, there is no correlation between these two parameters. We propose that this lack of correspondence may be attributed to varying physical conditions among the individual stars or differences in mass loss pressures. \n\n**Keywords:** Mira variable, correlation, infrared color, SiO maser line, mass loss rate. \n\n**1. Introduction** \nMira variables are red giant stars that undergo radial pulsations over timescales ranging from 100 hours to several thousand years. These stars exhibit significant fluctuations in luminosity and radial velocity, and their light curves can typically be modeled with a simple sinusoidal function, particularly for periods exceeding approximately 300 days. Known for generating strong stellar winds, Miras also emit powerful radio pulses. The SiO molecule is present in various astronomical objects, including early-class stars, evolved giant planets, early stellar bodies, comets, and planets. SiO molecules are thought to play a crucial role in the formation of dust grains. SiO masers were initially detected in AGB stars, and since then, they have been frequently studied in both AGB and post-AGB stars. Research has established that the characteristics of SiO masers are highly dependent on the evolutionary stage of the stars. For example, it has been noted that the peak flux of SiO masers decreases rapidly during the transition from the AGB to the post-AGB stage.",
        "ori-fast-z-score": -0.552344770738994,
        "water-fast-z-score": 7.058578427117228,
        "rewrite-fast-z-score": 0.4508348173337161
    },
    {
        "original_text": "We present the results of our numerical simulations of accretion disk annuli in which radiation pressure is comparable to gas pressure, but not dominant.  We find that for such disks there are two distinct regimes depending on whether or not the luminosity is dominated by advection (i.e., Ladv/Lvisc ≫ 1). In one regime we find that the temperature profile has a power-law form T ∝ r^{-(3/2)}, while in another it follows a more complicated dependence on radius. The latter case occurs when the luminosity is dominated either by viscous dissipation or by advection. For both cases, however, the radial velocity profiles have similar shapes. Finally, we show how these results can be used to explain observed properties of X-ray binaries. Subject headings: Black holes -accretion disks -X-ray binaries: general -Accretion, accretion disks",
        "watermark_text": "We present the results of our numerical simulations of accretion cone annuli in which radiation volume is analogous to liquid temperature , but not dominant . We see that for such disks there are two different regimes depending on whether or not the luminosity is dominated by advection ( i . e . , Ladv / Lvisc [UNK] 1 ) .In one regime we find that the temperature profile has a power - law form T [UNK] r ^ { - ( 3 / 2 ) } , while in another it takes a more complicated dependence on radius . The latter example happens when the luminosity is dominated either by viscous dissipation or by advection .For both cases , however , the radial speed profiles have parallel patterns . Finally , we show how these results can be used to explain observed properties of X - ray binaries .Subject headings : Black holes - accretion disks - X - ray binaries : general - Accretion , accretion disks",
        "rewrite_text": "We present the findings from our numerical simulations of accretion cone annuli, where the radiation volume is comparable to liquid temperature but not the predominant factor. Our analysis reveals the existence of two distinct regimes, contingent upon whether the luminosity is primarily driven by advection (i.e., L_adv / L_visc < 1). In one regime, the temperature profile follows a power-law distribution, expressed as T ∝ r^{-(3/2)}, while in the other, it exhibits a more complex relationship with radius. This complexity arises when the luminosity is governed either by viscous dissipation or advection. Nevertheless, in both scenarios, the radial speed profiles maintain similar patterns. Lastly, we demonstrate how these findings can elucidate the observed characteristics of X-ray binaries. Subject headings: Black holes - accretion disks - X-ray binaries: general - Accretion, accretion disks.",
        "ori-fast-z-score": -0.2773500981126146,
        "water-fast-z-score": 2.6605321596532185,
        "rewrite-fast-z-score": -0.13736056394868904
    },
    {
        "original_text": "We study the long-term orbital evolution of close-in giant planets that are in mean-motion resonances (MMRs) and have distant stellar companions, using numerical integrations for up to 10 Gyrs. We find that MMR can be broken by secular perturbations due to the companion star on timescales shorter than 1 Myr. The planet may then evolve into an eccentric orbit or even escape its host star s gravitational potential well if it is initially trapped in a high-order resonance such as 5:3 or 3:2. In addition, we show that the presence of a second planet can significantly affect the stability of the first planet through mutual interactions between them. Finally, we discuss how our results could explain some observed properties of hot Jupiters. Keywords: Planet migration; Mean motion resonance; Secular perturbation theory; Hot Jupiter; Double planet systems; Stability analysis; Eccentricity; Escape velocity",
        "watermark_text": "We research the long - term orbital evolution of close - in massive planets that are in mean - movement resonances ( MMRs ) and have nearby stellar companions , using numerical integrations for up to 10 Gyrs . We see that MMR can be broken by secular perturbations due to the companion galaxy on timescales shorter than 1 Myr .The planet may then evolve into an eccentric orbit or even escape its host star s gravitational potential poorly if it is initially trapped in a high - order resonance such as 5 : 3 or 3 : 2 . In addition , we find that the presence of a second planet can significantly affect the stability of the first planet through mutual interactions between them .Finally , we explain how our findings may describe some observed properties of hot Jupiters . Keywords : Planet drift ; Mean moving resonance ; Secular perturbation theory ; Hot Jupiter ; Double planet dynamics ; Stability analysis ; Eccentricity ; Escape velocity",
        "rewrite_text": "We investigate the long-term orbital evolution of close-in massive planets situated in mean-motion resonances (MMRs) with nearby stellar companions, utilizing numerical integrations that extend over 10 billion years. Our findings indicate that MMRs can be disrupted by secular perturbations from companion galaxies on timescales of less than 1 million years. This disruption can lead to the planet transitioning into an eccentric orbit or, in some cases, escaping the gravitational influence of its host star, particularly if it is initially caught in a high-order resonance like 5:3 or 3:2. Moreover, we discover that the presence of a second planet can significantly influence the stability of the first planet through their mutual interactions. Finally, we discuss how our results may offer insights into certain observed characteristics of hot Jupiters. \nKeywords: Planet drift; Mean-motion resonance; Secular perturbation theory; Hot Jupiter; Double planet dynamics; Stability analysis; Eccentricity; Escape velocity.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "We present an analysis of the dynamics of the four giant planets of our Solar System during their formation, using numerical simulations with N-body codes that include hydrodynamical effects (gas drag). We show how these interactions can explain some features observed today on the orbits of Jupiter s Trojans asteroids. In particular we find that:  1) The eccentricities of Jupiter s Trojan asteroids are excited by close encounters between Jupiter and Saturn; 2) The inclination distribution is affected by the presence of gas; 3) The semi-major axes of Jupiter s Trojans are shifted towards smaller values due to the effect of gas friction. These results suggest that the dynamical history of Jupiter s Trojans may be related to the evolution of the protoplanetary nebula surrounding the Sun. This work was supported by CONACyT grant No. 164713. We thank J. Laskar for providing us his code used to calculate the secular frequencies of the planetary systems. \n \n Keywords: Giant planet migration, Gas drag",
        "watermark_text": "We present an assessment of the dynamics of the four giant planets of our Solar System during their formed , using numerical simulations with N - bodies codes that include hydrodynamical factors ( gas drag ) . We see how these interactions can describe some features discovered today on the orbits of Jupiter s Trojans asteroids .In particular we find that : 1 ) The eccentricities of Jupiter s Trojan asteroids are excited by distant encounters between Jupiter and Saturn ; 2 ) The orientation distribution is affected by the presence of gas ; 3 ) The semi - major axes of Jupiter s Trojans are shifted towards smaller values due to the impact of gas compression . These conclusions propose that the dynamical history of Jupiter s Trojans might be connected to the evolution of the protoplanetary nebula surrounding the Sun .This project was supported by CONACyT grant No . 164713 .We praise J . Laskar for providing us his code used to estimate the secular frequencies of the planetary components . Keywords : Giant planet migration , Gas drag",
        "rewrite_text": "We evaluate the dynamic behaviors of the four giant planets in our Solar System during their formation, utilizing numerical simulations with N-body codes that incorporate hydrodynamic factors, such as gas drag. Our findings illustrate how these interactions help explain certain characteristics observed in the orbits of Jupiter’s Trojan asteroids. Specifically, we discover that: 1) Jupiter's Trojan asteroids experience increased eccentricities due to distant encounters with Saturn; 2) the distribution of orientations is influenced by the presence of gas; and 3) the semi-major axes of Jupiter’s Trojans shift towards smaller values as a result of gas compression. These results suggest that the dynamical evolution of Jupiter’s Trojans may be linked to the development of the protoplanetary nebula surrounding the Sun. This research was supported by CONACyT grant No. 164713, and we thank J. Laskar for providing his code used to calculate the secular frequencies of planetary components. Keywords: Giant planet migration, Gas drag.",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "We present the results of our study on mass-loss rates in luminous blue variables (LBVs) based on radio observations at 1.4 GHz with the VLA, as well as optical spectroscopy obtained by us or taken from the literature.  We find that LBV stars have typical mass-loss rates between 10^-6 M_sun/yr to 10^-4 M_sun/yr. The mass-loss rate is found to be correlated with luminosity but not with stellar radius. In addition we report quasi-periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic changes in their circumstellar environments. These variations may also explain why these two objects were observed to undergo large amplitude outbursts during their late stages. This research was supported by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution",
        "watermark_text": "We publish the conclusion of our research on mass - loss rates in luminous blue variables ( LBVs ) based on radio observations at 1 . 4 GHz with the VLA , as well as laser spectroscopy derived by us or taken from the literature . We see that LBV stars have typical mass - loss rates between 10 ^ - 6 M _ sun / yr to 10 ^ - 4 M _ sun / yr .The mass - loss rate is found to be correlated with luminosity but not with stars radius . In addition we study quasi - periodic modulations of radio supernovae associated with SN 1987A and SN 1993J which are likely due to periodic alterations in their circumstellar environments .These changes may also explain why these two bodies were found to undergo huge amplitude outbursts during their late stages . This research was supported by NASA grant NAG5 - 7262 .Keywords : Mass loss , Stellar evolution",
        "rewrite_text": "We present our findings on mass-loss rates in luminous blue variables (LBVs), derived from 1.4 GHz radio observations using the VLA, along with laser spectroscopy data obtained either through our own research or from existing literature. Our analysis shows that LBV stars typically exhibit mass-loss rates ranging from 10^-6 M_sun/yr to 10^-4 M_sun/yr. We have identified a correlation between mass-loss rates and luminosity, although no correlation was found with the stars' radius. Additionally, we investigate quasi-periodic variations in radio supernovae associated with SN 1987A and SN 1993J, likely resulting from periodic changes in their circumstellar environments. These fluctuations may also account for the significant outbursts observed in these two objects during their later phases. This research was funded by NASA grant NAG5-7262. Keywords: Mass loss, Stellar evolution.",
        "ori-fast-z-score": 0.5345224838248488,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "We present new high resolution (R = λ/Δλ ~ 20,000) far-ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer (FUSE), as well as archival Hubble Space Telescope (HST) data for the hot white dwarf central star in the planetary nebula Sh2-216. The FUSE spectrum shows numerous absorption lines due to highly ionized species such as C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We have modeled these features using synthetic line profiles generated by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our best-fit models indicate that this star has an effective temperature T eff = 120,000 K, surface gravity log g = 8.0, mass M = 0.6M☉ , radius R = 0.01R☉ , and is surrounded by a shell of material with density n(He II)/n(He I) = 1.5 x 10-3 .",
        "watermark_text": "We use new high resolution ( R = λ / Δλ ~ 20 , 000 ) far - ultraviolet spectra obtained with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) , as well as archival Hubble Space Telescope ( HST ) statistics for the cool white dwarf central star in the planetary nebula Sh2 - 216 . The FUSE spectrum displays several absorption patterns due to strongly ionized species such as C IV , N V , O VI , Ne VIII , Mg X , Si XII , S XIV , Ar XVI , Fe XIX , and Ni XXVIII .We have analyzed these characteristics utilizing artificial line profiles generated by the non - LTE model atmosphere code TLUSTY / SYNSPEC . Our best - fitting models suggest that this star has an effective heat T eff = 120 , 000 K , surface gravity log f = 8 . 0 , mass M = 0 . 6M☉ , diameter R = 0 . 01R☉ , and is enclosed by a shell of material with volume n ( He II ) / n ( He I ) = 1 . 5 x 10 - 3 .",
        "rewrite_text": "We analyze new high-resolution far-ultraviolet spectra (R = λ / Δλ ~ 20,000) obtained from the Far Ultraviolet Spectroscopic Explorer (FUSE), along with archival data from the Hubble Space Telescope (HST), focusing on the cool white dwarf central star of the planetary nebula Sh2-216. The FUSE spectrum reveals several absorption features from highly ionized species, including C IV, N V, O VI, Ne VIII, Mg X, Si XII, S XIV, Ar XVI, Fe XIX, and Ni XXVIII. We investigated these features using artificial line profiles produced by the non-LTE model atmosphere code TLUSTY/SYNSPEC. Our most accurate models indicate that the star has an effective temperature (T_eff) of 120,000 K, a surface gravity of log f = 8.0, a mass of M = 0.6 M☉, a diameter of R = 0.01 R☉, and is surrounded by a shell of material with a volume density ratio of n(He II) / n(He I) = 1.5 x 10^-3.",
        "ori-fast-z-score": -1.5109662034355793,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": -0.5443310539518174
    },
    {
        "original_text": "We present an analysis of the first two years (Feb 2005 -Jan 2007) of data taken by the Swift satellite, which has been designed to detect and study gamma ray bursts (GRBs). We find that GRB 050904 at z = 6.3 is the most distant object ever detected in the electromagnetic spectrum. The prompt emission was observed over more than four orders of magnitude in energy, from radio waves up to X-rays. This burst also had one of the highest fluences recorded so far for any GRB. In addition we report on another burst, GRB 080913, whose afterglow was found to be variable on timescales as short as 1 minute. These results are discussed within the context of current models for GRB production. Keywords: Gamma-ray burst, High-redshift universe, Afterglows, Swift satellite. Gamma-ray bursts (GRBs), intense flashes of high-energy radiation lasting only milliseconds, have now been discovered out to redshifts greater than six  1  . Their extreme luminosities make them powerful probes into the early Universe  2  , but their origin remains unknown  3  .\nSwift  4  , launched in November 2004, carries three instruments capable of detecting GRBs across the entire electromagnetic spectrum  5  : the Burst Alert Telescope  6  detects GRBs via their X-ray and/or optical emissions; the Ultraviolet/Optical Telescope  7  observes the afterglow through ultraviolet and visible light; and the X-ray telescope  8  monitors the afterglow s decaying flux. Here we describe our initial findings using these instruments during the first two years of operation. \nThe Burst Alert Telescope\n\nBurst Alert Telescope Observations of GRB 050904\nOn September 5 th , 2006, the Burst Alert Telescope triggered on a bright source located at RA=05h54m36.6s Dec=-69d21 59.6   9  . Follow-up observations revealed this event to be a new record holder among GRBs  10  . Its peak photon count rate reached 2 x 10 4 photons s -1 cm -2 in the 15-150 keV band  11  . It lasted about",
        "watermark_text": "We present an assessment of the first two years ( Feb 2005 - Jan 2007 ) of evidence gained by the Swift satellite , which has been designed to identify and track γ ray clusters ( GRBs ) . We see that GRB 050904 at z = 6 . 3 is the most distant object ever observed in the electromagnetic spectrum .The prompt emission was seen over more than four orders of magnitude in energy , from radio beams up to X - rays . This burst also had one of the highest fluences recorded so far for any GRB .In addition we paper on another burst , GRB 080913 , whose afterglow was shown to be varying on timescales as short as 1 minute . These conclusions are discussed within the context of recent versions for GRB development .Keywords : Gamma - ray flare , High - redshift universe , Afterglows , Swift satellite . Gamma - ray clusters ( GRBs ) , intense pulses of high - energy rays lasting only milliseconds , have now been detected out to redshifts greater than six 1 .Their intense luminosities give them potent probes into the early Universe 2 , but their source remains unidentified 3 . Swift 4 , launched in November 2004 , carries three devices capable of detecting GRBs across the entire electromagnetic spectrum 5 : the Burst Alert Telescope 6 detects GRBs via their X - ray and / or laser emissions ; the Ultraviolet / Optical Telescope 7 sees the afterglow through ultraviolet and visible radiation ; and the X - ray observatory 8 monitors the afterglow s decaying flux .Here we explain our first findings using these instruments during the first two years of operation . The Burst Alert Telescope Burst Alert Telescope Observations of GRB 050904 On September 5 th , 2006 , the Burst Alert Telescope triggered on a bright source located at RA = 05h54m36 . 6s Dec = - 69d21 59 . 6 9 .Follow - up observations showed this event to be a new record holder among GRBs 10 . Its peak photon number rate reached 2 x 10 4 photons s - 1 cm - 2 in the 15 - 150 keV band 11 .It lasted about",
        "rewrite_text": "We provide an evaluation of the first two years (February 2005 - January 2007) of data collected by the Swift satellite, engineered to detect and monitor gamma-ray bursts (GRBs). Notably, GRB 050904 at redshift z = 6.3 marks the furthest object ever observed in the electromagnetic spectrum. The prompt emission spanned over four orders of magnitude in energy, ranging from radio wavelengths to X-rays. This burst also recorded one of the highest fluences documented for any GRB to date. Furthermore, we discuss another burst, GRB 080913, whose afterglow exhibited variability on time scales as brief as 1 minute. These findings are analyzed in light of recent theories regarding the development of GRBs. \n\nKeywords: Gamma-ray burst, high-redshift universe, afterglows, Swift satellite. Gamma-ray bursts (GRBs) are intense bursts of high-energy radiation lasting just milliseconds and have been identified at redshifts exceeding six. Their significant luminosities make them valuable tools for probing the early Universe, although their origins remain a mystery. Swift, launched in November 2004, is equipped with three instruments capable of detecting GRBs across the full electromagnetic spectrum: the Burst Alert Telescope detects GRBs through their X-ray and optical emissions; the Ultraviolet/Optical Telescope captures the afterglow in ultraviolet and visible light; and the X-ray observatory tracks the declining flux of the afterglow. In this paper, we detail our initial findings from these instruments during their first two years of operation. \n\nOn September 5, 2006, the Burst Alert Telescope triggered on a bright source at RA = 05h54m36.6s Dec = -69d21m59.9s, marking a new record for GRBs. The peak photon number rate reached 2 x 10^4 photons s^-1 cm^-2 in the 15-150 keV band and lasted approximately six minutes.",
        "ori-fast-z-score": 0.4703604341917986,
        "water-fast-z-score": 7.243550686553699,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "The aim of this study is to develop an improved model for simulating diffusion processes within biological cells, especially those occurring at the nuclear membrane and its associated structures.  The proposed approach involves coupling two existing models; one that describes the movement of molecules through the cytoplasm (the fluid part of the cell) using Brownian dynamics simulations, with another which represents the nucleus as a porous medium containing immobile obstacles.  This latter component has been developed by considering the geometry of the nuclear pore complex network, which consists of cylindrical pores connected via narrower channels.   In order to validate our new hybrid model we have performed a series of numerical experiments on synthetic data generated from both individual particle tracking and Monte Carlo methods.  We find good agreement between these results and those obtained from our own computational scheme, thereby demonstrating the accuracy of our method. Finally, we apply our new modelling framework to investigate how changes in the structure of the nuclear pore complexes can affect the rate of molecular exchange across the nuclear envelope.",
        "watermark_text": "The goal of this study is to develop an better model for simulating diffusion processes within molecular cells , particularly those occurring at the atomic membrane and its associated structures . The proposed approach requires coupling two existing models ; one that describes the movement of molecules through the cytoplasm ( the liquid part of the cell ) using Brownian gravity simulations , with another which represents the nucleus as a porous medium containing immobile obstacles .This last component has been constructed by examining the topology of the atomic pore complex network , which consists of circular pores connected via narrower outlets . In order to validate our new hybrid system we have done a string of computational experiments on synthetic information generated from both individual beam monitoring and Monte Carlo methods .We get good agreement between these results and those acquired from our own computational scheme , thereby showing the accuracy of our technique . Finally , we apply our new modelling methodology to examine how differences in the composition of the atomic pore complexes can affect the frequency of molecular transfer across the atomic envelope .",
        "rewrite_text": "The aim of this study is to create an improved model for simulating diffusion processes within molecular cells, focusing particularly on those that occur at the atomic membrane and its related structures. Our proposed method involves integrating two existing models: one that utilizes Brownian gravity simulations to depict the movement of molecules in the cytoplasm (the cell's liquid component), and another that models the nucleus as a porous medium filled with immobile obstacles. This second model is developed by analyzing the topology of the atomic pore complex network, which features circular pores linked by narrower outlets. To validate our new hybrid system, we conducted a series of computational experiments using synthetic data generated from both individual beam monitoring and Monte Carlo methods. The results obtained show strong agreement with those from our computational framework, confirming the effectiveness of our approach. Finally, we apply this new modeling technique to investigate how variations in the composition of atomic pore complexes can influence the rate of molecular transfer across the atomic envelope.",
        "ori-fast-z-score": -1.1659976680069961,
        "water-fast-z-score": 6.677986644040068,
        "rewrite-fast-z-score": 1.2792042981336627
    },
    {
        "original_text": "We study the effects of selection biases in cluster samples, as well as covariance between observables, on scaling relations derived from X-ray data using mock galaxy clusters generated with the semi-analytic model GALFORM. We find that both these effects can lead to significant systematic errors when deriving cosmological constraints from observed scaling relations. In particular we show that: (i) The scatter in the M-T relation is significantly reduced by including additional information about the temperature distribution function; this effect is stronger for low mass systems. (ii) The slope of the L-M relation depends strongly on whether or not one includes cooling flows in the analysis. This dependence arises because cool cores are more common at high masses than at lower masses, leading to an apparent steepening of the slope if they are excluded. (iii) The normalization of the Y-Xray luminosity-temperature relation shows strong redshift evolution which cannot be explained solely by self-similar evolution.",
        "watermark_text": "We research the effects of selection biases in cluster specimens , as well as covariance between observables , on scaling relations derived from X - ray data utilizing simulated galaxy galaxies produced with the semi - analytic model GALFORM . We see that both these influences can lead to significant systematic errors when deriving cosmological limitations from observed scaling relations .In particular we prove that : ( i ) The scatter in the M - T relation is significantly reduced by including extra data about the temperature distribution function ; this effect is strengthened for low mass systems . ( ii ) The slope of the L - M relation depends strongly on whether or not one includes heating flows in the evaluation .This dependence occurs because cool cores are more common at high masses than at lower masses , leading to an apparent steepening of the slope if they are ignored . ( iii ) The normalization of the Y - Xray luminosity - temperature relation shows deep redshift development which cannot be described solely by self - similar development .",
        "rewrite_text": "We investigate the impact of selection biases in cluster samples and the covariance among observables on scaling relations derived from X-ray data, using simulated galaxies produced by the semi-analytical model GALFORM. Our findings indicate that these factors can introduce significant systematic errors when assessing cosmological constraints based on observed scaling relations. Specifically, we demonstrate that: (i) incorporating additional data about the temperature distribution function substantially reduces the scatter in the M-T relation, with this effect being more pronounced in low-mass systems; (ii) the slope of the L-M relation is highly sensitive to whether heating flows are considered in the analysis. This sensitivity arises because cool cores are more prevalent in high-mass clusters than in lower-mass ones, resulting in an apparent steepening of the slope when they are excluded; and (iii) the normalization of the Y-X-ray luminosity-temperature relation exhibits significant evolution with redshift that cannot be adequately explained by self-similar development alone.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We report the detection of cosmic far-infrared background (CFIRB) fluctuations using deep observations made by the Far Infrared Surveyor (FIS) onboard Akari satellite at 65 and 90 micron bands in the Lockman Hole field, which is one of the most sensitive fields for detecting extragalactic sources. The FIS has two photometric channels; N60 band covers 60 to 120 microns while WIDE-S channel covers 50 to 100 microns. We used data taken during the period between February 2005 and March 2007. After removing bright point-like objects detected by Spitzer/MIPS 24 micron survey, we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole. To estimate the contribution from Galactic cirrus emission, we subtracted the median value of each pixel after applying a 3 sigma clipping method. Then we calculated power spectrum density (PSD) of the residual map. By fitting the PSD with a single power law model, we obtained the best-fit slope as -2.1 ± 0.3 at 65 micron and -1.9 ± 0.4 at 90 micron. These slopes are consistent with those expected from clustering properties of infrared galaxies.",
        "watermark_text": "We report the observation of cosmic far - infrared background ( CFIRB ) fluctuations using deep surveys made by the Far Infrared Surveyor ( FIS ) onboard Akari spacecraft at 65 and 90 micron bands in the Lockman Hole field , which is one of the most important areas for detecting extragalactic sources . The FIS has two photometric channels ; N60 band encompasses 60 to 120 microns while WIDE - S channel measures 50 to 100 microns .We utilized information taken during the period between February 2005 and March 2007 . After removing bright point - like items detected by Spitzer / MIPS 24 micron measurement , we performed aperture photometry on all remaining pixels within an area of 1 deg2 centered around the Lockman hole .To estimate the impact from Galactic cirrus emission , we subtracted the median value of each pixel after applying a 3 sigma clipping method . Then we calculated power spectrum concentration ( PSD ) of the residual map .By fitting the PSD with a single power law description , we derived the best - fitting curve as - 2 . 1 ± 0 . 3 at 65 micron and - 1 . 9 ± 0 . 4 at 90 micron . These slopes are compatible with those expected from clustering qualities of infrared galaxies .",
        "rewrite_text": "We present our findings on cosmic far-infrared background (CFIRB) fluctuations, derived from deep surveys conducted by the Far Infrared Surveyor (FIS) aboard the Akari spacecraft. These observations were made in the 65 and 90 micron bands within the Lockman Hole, a key region for identifying extragalactic sources. The FIS features two photometric channels: the N60 band, which covers 60 to 120 microns, and the WIDE-S channel, which spans 50 to 100 microns. Our analysis utilized data collected between February 2005 and March 2007. After excluding bright point sources identified by Spitzer/MIPS 24 micron measurements, we carried out aperture photometry on all remaining pixels in a 1 square degree area centered on the Lockman Hole. To account for Galactic cirrus emission, we subtracted the median value of each pixel using a 3-sigma clipping method. We then computed the power spectral density (PSD) of the residual map, fitting it with a single power law to obtain the slopes of -2.1 ± 0.3 at 65 microns and -1.9 ± 0.4 at 90 microns. These slopes align well with the expected clustering characteristics of infrared galaxies.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.365641250653994,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "In this article, we continue the classification of finite dimensional complex filiform Leibniz algebras begun in Part 1.  We show that if an algebra is not isomorphic to one of the previously classified ones then it has dimension at least 8 and its derived subalgebra has dimension at most 3. In particular, any such algebra satisfies dim(DerA) = 3 or 4. If dim(DerA)=4, then A is isomorphic either to L4(C), L1(C2), L2(C2), L3(C2), or L5(C2). If dim(DerA )=3, then A is isomorphic to one of the following Lie algebras:  L6(C3), L7(C3), L8(C3), L9(C3), L10(C3), L11(C3), L12(C3), L13(C3), L14(C3), L15(C3), L16(C3), L17(C3), L18(C3), L19(C3), L20(C3), L21(C3), L22(C3), L23(C3), L24(C3), L25(C3), L26(C3), L27(C3), L28(C3), L29(C3), L30(C3), L31(C3), L32(C3), L33(C3), L34(C3), L35(C3), L36(C3), L37(C3), L38(C3), L39(C3), L40(C3), L41(C3), L42(C3), L43(C3), L44(C3), L45(C3), L46(C3), L47(C3), L48(C3), L49(C3), L50(C3), L51(C3), L52(C3), L53(C3), L54(C3), L55(C3), L56(C3), L57(C3), L58(C3), L59(C3), L60(C3), L61(C3), L62(C3), L63(C3",
        "watermark_text": "In this article , we continue the characterization of finite dimensional complex filiform Leibniz algebras completed in Part 1 . We see that if an algebra is not isomorphic to one of the previously classified ones then it has size at least 8 and its derived subalgebra has size at most 3 .In particular , any such module satisfies dim ( DerA ) = 3 or 4 . If dim ( DerA ) = 4 , then A is isomorphic either to L4 ( C ) , L1 ( C2 ) , L2 ( C2 ) , L3 ( C2 ) , or L5 ( C2 ) .If dim ( DerA ) = 3 , then A is isomorphic to one of the following Lie algebras : L6 ( C3 ) , L7 ( C3 ) , L8 ( C3 ) , L9 ( C3 ) , L10 ( C3 ) , L11 ( C3 ) , L12 ( C3 ) , L13 ( C3 ) , L14 ( C3 ) , L15 ( C3 ) , L16 ( C3 ) , L17 ( C3 ) , L18 ( C3 ) , L19 ( C3 ) , L20 ( C3 ) , L21 ( C3 ) , L22 ( C3 ) , L23 ( C3 ) , L24 ( C3 ) , L25 ( C3 ) , L26 ( C3 ) , L27 ( C3 ) , L28 ( C3 ) , L29 ( C3 ) , L30 ( C3 ) , L31 ( C3 ) , L32 ( C3 ) , L33 ( C3 ) , L34 ( C3 ) , L35 ( C3 ) , L36 ( C3 ) , L37 ( C3 ) , L38 ( C3 ) , L39 ( C3 ) , L40 ( C3 ) , L41 ( C3 ) , L42 ( C3 ) , L43 ( C3 ) , L44 ( C3 ) , L45 ( C3 ) , L46 ( C3 ) , L47 ( C3 ) , L48 ( C3 ) , L49 ( C3 ) , L50 ( C3 ) , L51 ( C3 ) , L52 ( C3 ) , L53 ( C3 ) , L54 ( C3 ) , L55 ( C3 ) , L56 ( C3 ) , L57 ( C3 ) , L58 ( C3 ) , L59 ( C3 ) , L60 ( C3 ) , L61 ( C3 ) , L62 ( C3 ) , L63 ( C3",
        "rewrite_text": "In this article, we further explore the characterization of finite-dimensional complex filiform Leibniz algebras initiated in Part 1. We establish that if an algebra is not isomorphic to any of the previously identified ones, then its dimension is at least 8, and its derived subalgebra has a dimension of at most 3. Specifically, any such algebra has either \\( \\text{dim}(Der A) = 3 \\) or \\( 4 \\). If \\( \\text{dim}(Der A) = 4 \\), then \\( A \\) is isomorphic to one of the following: \\( L4(\\mathbb{C}) \\), \\( L1(\\mathbb{C}^2) \\), \\( L2(\\mathbb{C}^2) \\), \\( L3(\\mathbb{C}^2) \\), or \\( L5(\\mathbb{C}^2) \\). Conversely, if \\( \\text{dim}(Der A) = 3 \\), then \\( A \\) is isomorphic to one of these Lie algebras: \\( L6(\\mathbb{C}^3) \\), \\( L7(\\mathbb{C}^3) \\), \\( L8(\\mathbb{C}^3) \\), \\( L9(\\mathbb{C}^3) \\), \\( L10(\\mathbb{C}^3) \\), \\( L11(\\mathbb{C}^3) \\), \\( L12(\\mathbb{C}^3) \\), \\( L13(\\mathbb{C}^3) \\), \\( L14(\\mathbb{C}^3) \\), \\( L15(\\mathbb{C}^3) \\), \\( L16(\\mathbb{C}^3) \\), \\( L17(\\mathbb{C}^3) \\), \\( L18(\\mathbb{C}^3) \\), \\( L19(\\mathbb{C}^3) \\), \\( L20(\\mathbb{C}^3) \\), \\( L21(\\mathbb{C}^3) \\), \\( L22(\\mathbb{C}^3) \\), \\( L23(\\mathbb{C}^3) \\), \\( L24(\\mathbb{C}^3) \\), \\( L25(\\mathbb{C}^3) \\), \\( L26(\\mathbb{C}^3) \\), \\( L27(\\mathbb{C}^3) \\), \\( L28(\\mathbb{C}^3) \\), \\( L29(\\mathbb{C}^3) \\), \\( L30(\\mathbb{C}^3) \\), \\( L31(\\mathbb{C}^3) \\), \\( L32(\\mathbb{C}^3) \\), \\( L33(\\mathbb{C}^3) \\), \\( L34(\\mathbb{C}^3) \\), \\( L35(\\mathbb{C}^3) \\), \\( L36(\\mathbb{C}^3) \\), \\( L37(\\mathbb{C}^3) \\), \\( L38(\\mathbb{C}^3) \\), \\( L39(\\mathbb{C}^3) \\), \\( L40(\\mathbb{C}^3) \\), \\( L41(\\mathbb{C}^3) \\), \\( L42(\\mathbb{C}^3) \\), \\( L43(\\mathbb{C}^3) \\), \\( L44(\\mathbb{C}^3) \\), \\( L45(\\mathbb{C}^3) \\), \\( L46(\\mathbb{C}^3) \\), \\( L47(\\mathbb{C}^3) \\), \\( L48(\\mathbb{C}^3) \\), \\( L49(\\mathbb{C}^3) \\), \\( L50(\\mathbb{C}^3) \\), \\( L51(\\mathbb{C}^3) \\), \\( L52(\\mathbb{C}^3) \\), \\( L53(\\mathbb{C}^3) \\), \\( L54(\\mathbb{C}^3) \\), \\( L55(\\mathbb{C}^3) \\), \\( L56(\\mathbb{C}^3) \\), \\( L57(\\mathbb{C}^3) \\), \\( L58(\\mathbb{C}^3) \\), \\( L59(\\mathbb{C}^3) \\), \\( L60(\\mathbb{C}^3) \\), \\( L61(\\mathbb{C}^3) \\), \\( L62(\\mathbb{C}^3) \\), \\( L63(\\mathbb{C}^3) \\).",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 1.4596008983995234,
        "rewrite-fast-z-score": 0.4082482904638631
    },
    {
        "original_text": "The deformation principle is the main tool in this article for constructing new geometric structures on physical spaces, which are not necessarily Riemannian manifolds but can be more general objects such as Finsler or pseudo-Finsler ones.  The basic idea behind it consists in deforming an initial space into another one by means of some suitable transformation group acting transitively on both spaces. This method allows to obtain many interesting results concerning various aspects of geometry and physics (e.g., geodesic flows,...). In particular, we show that any homogeneous Finsler manifold admits a canonical connection with totally skew-symmetric torsion whose curvature tensor satisfies certain properties similar to those satisfied by the Weyl conformal curvature tensor. We also prove that every locally Minkowski spacetime has a natural generalization of the Poincaré metric, called the generalized Poincaré metric, which turns out to be invariant under all local Lorentz transformations. Finally, we present several examples illustrating our approach.",
        "watermark_text": "The deformation concept is the main method in this page for constructing new geometric arrangements on physical spaces , which are not necessarily Riemannian manifolds but can be more formal structures such as Finsler or pseudo - Finsler ones . The basic idea behind it consists in deforming an initial space into another one by means of some suitable decomposition group acting transitively on both spaces .This method enables to obtain several interesting results concerning numerous topics of geometry and mechanics ( e . g . , geodesic flows , . . . ) . In particular , we prove that any homogeneous Finsler manifold admits a canonical relationship with totally skew - symmetric torsion whose curvature tensor satisfies certain characteristics similar to those fulfilled by the Weyl conformal curvature metric .We additionally prove that every locally Minkowski spacetime has a natural generalization of the Poincaré metric , known the generalized Poincaré metric , which turns out to be invariant under all local Lorentz transformations . Finally , we present many examples illustrating our approach .",
        "rewrite_text": "This page focuses on the concept of deformation as the primary method for creating new geometric configurations in physical spaces. These spaces need not be Riemannian manifolds; they can also include more formal structures like Finsler or pseudo-Finsler manifolds. The core idea is to transform an initial space into another through a suitable decomposition group that acts transitively on both spaces. This approach allows us to derive several intriguing results across various topics in geometry and mechanics, such as geodesic flows. Notably, we demonstrate that any homogeneous Finsler manifold has a canonical connection with totally skew-symmetric torsion, where the curvature tensor exhibits properties akin to those of the Weyl conformal curvature metric. Furthermore, we establish that every locally Minkowski spacetime has a natural extension of the Poincaré metric, referred to as the generalized Poincaré metric, which remains invariant under all local Lorentz transformations. Lastly, we provide numerous examples to illustrate our methodology.",
        "ori-fast-z-score": 1.7801724872907798,
        "water-fast-z-score": 6.3804502135457675,
        "rewrite-fast-z-score": 2.42535625036333
    },
    {
        "original_text": "We have performed in-orbit focal adjustment (IFA) for the infrared camera onboard AKARI satellite using its own data taken in orbit. The IFA was carried out by comparing the observed point spread function (PSF) and that simulated based on ray tracing analysis, which is one of the most accurate methods to determine the best focus position. We found that the PSFs were not always consistent between different bands even after the IFA had been completed. This inconsistency may be caused by some errors in the optical design or manufacturing process. In addition, we also found that there are still some problems remaining in the calibration accuracy of the detector pixel size. These results will help us improve our understanding about the performance of the instrument as well as provide useful information for future space missions. Keywords: Space mission, Focal adjustment, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera",
        "watermark_text": "We have done in - orbit lens adjustment ( IFA ) for the infrared camera onboard AKARI vehicle using its own signal taken in orbit . The IFA was carried out by comparing the seen point spread distribution ( PSF ) and that simulated based on ray tracing imaging , which is one of the most accurate ways to identify the best view point .We determined that the PSFs were not always compatible between various frequencies even after the IFA had been completed . This inconsistency may be caused by some failures in the optical design or manufacturing system .In addition , we also discovered that there are still some problems existing in the calibration reliability of the sensor pixel size . These data will assist us improve our knowing about the performance of the instrument as also as give valuable info for future orbital flights .Keywords : Space mission , Focal correction , Point spread value , Ray tracing investigation , Infrared astronomy , Infrared camera",
        "rewrite_text": "We conducted an in-orbit lens adjustment (IFA) for the infrared camera aboard the AKARI spacecraft using its onboard signal. The IFA involved comparing the actual point spread function (PSF) observed with that simulated through ray tracing, a highly accurate method for determining the optimal viewpoint. We found that the PSFs were not consistently compatible across different frequencies, even after completing the IFA. This inconsistency may stem from issues in the optical design or the manufacturing process. Additionally, we identified ongoing concerns regarding the calibration reliability of the sensor pixel size. The insights gained from this data will enhance our understanding of the instrument's performance and provide valuable information for future space missions. \n\nKeywords: Space mission, Focal correction, Point spread function, Ray tracing analysis, Infrared astronomy, Infrared camera.",
        "ori-fast-z-score": -0.8307471607356973,
        "water-fast-z-score": 6.052586456788652,
        "rewrite-fast-z-score": 0.6108472217815261
    },
    {
        "original_text": "We show that the maximum weighted edge biclique problem is NP-hard to approximate within any constant factor, even for graphs with bounded degree 3 or 4. We also present several applications of this result in computational biology. In particular we prove hardness results on finding conserved regions between two genomes under different evolutionary models as well as on reconstructing ancestral gene orders using parsimony principle. Finally, we give an efficient algorithm for computing all maximal cliques in chordal bipartite graphs. The main tool used in our proofs are reductions from the vertex cover problem which can be solved exactly by dynamic programming algorithms but not approximated efficiently unless P = N P . \nIntroduction\n\nThe maximum weighted edge bicluster (MWEB) problem asks us to find a subgraph H of G such that:  1. Each node v ∈ V belongs to at least one cluster C ∈ H; 2. For each pair of clusters C ,C ′ ∈ H there exists an edge e ∈ E(G) connecting some nodes u ∈ C and w ∈ C ′ ; 3. The weight of H is maximized over all possible choices of H. This problem has been studied extensively in recent years due to its wide range of applications including clustering analysis  19, 20  , data mining  7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18  , bioinformatics  1, 2  , etc.. It was shown recently  3  that MWEB cannot be approximated within any constant factor if P = N P . However, it remains open whether MWEB admits polynomial time approximation schemes when restricted to special classes of graphs. In fact, no non-trivial upper bound on the performance ratio of any polynomial-time approximation scheme for MWEB is known so far. \nIn this work we study the complexity of MWEB both theoretically and practically. First, we show that MWEB is NP-hard to approxi-",
        "watermark_text": "We see that the maximum weighted edge biclique problem is NP - hard to approximate within any constant factor , even for graphs with bounded degree 3 or 4 . We also demonstrate several uses of this fact in computational biology .In particular we prove hardness results on discovering conserved parts between two genomes under various evolutionary models as well as on reconstructing ancestral chromosome orders following parsimony principle . Finally , we give an efficient algorithm for solving all maximal cliques in chordal bipartite vertices .The main technique useful in our proofs are reductions from the vertex cover question which can be answered exactly by dynamic programming algorithms but not approximated accurately unless P = N P . Introduction The maximum weighted edge bicluster ( MWEB ) question seeks us to find a subgraph H of G such that : 1 .Each node v ∈ V belongs to at least one cluster C ∈ H ; 2 . For each couple of clusters C , C ′ ∈ H there exists an path e ∈ E ( G ) link some nodes u ∈ C and w ∈ C ′ ; 3 .The weight of H is maximized over all possible options of H . This problem has been studied thoroughly in recent months thanks to its large variety of applications notably clustering analysis 19 , 20 , computer mining 7 , 8 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , bioinformatics 1 , 2 , etc . . It was shown later 3 that MWEB never be approximated within any fixed factor if P = N P .However , it remains open whether MWEB admits polynomial time approximation schemes when confined to special classes of graphs . In indeed , no non - simple upper bound on the performance ratio of any polynomial - time approximation scheme for MWEB is known so far .In this research we study the complexity of MWEB both theoretically and literally . First , we prove that MWEB is NP - hard to approxi -",
        "rewrite_text": "The maximum weighted edge biclique problem is proven to be NP-hard to approximate within any constant factor, even for graphs with a bounded degree of 3 or 4. We illustrate several implications of this finding in the field of computational biology. Specifically, we establish hardness results for identifying conserved regions between two genomes across various evolutionary models and for reconstructing ancestral chromosome orders based on the parsimony principle. Additionally, we present an efficient algorithm for identifying all maximal cliques in chordal bipartite graphs. The primary technique employed in our proofs involves reductions from the vertex cover problem, which can be solved exactly using dynamic programming algorithms but cannot be accurately approximated unless P equals NP.\n\nThe maximum weighted edge bicluster (MWEB) challenge involves finding a subgraph H of G that satisfies three criteria: 1) each node v ∈ V is included in at least one cluster C ∈ H; 2) for every pair of clusters C and C′ ∈ H, there exists a path e ∈ E(G) that connects some nodes u ∈ C and w ∈ C′; and 3) the weight of H is maximized across all possible configurations of H. This problem has garnered significant attention in recent months due to its diverse applications, particularly in clustering analysis, data mining, and bioinformatics. It has been demonstrated that MWEB cannot be approximated within any fixed factor if P does not equal NP. However, it remains uncertain whether MWEB allows for polynomial-time approximation schemes when restricted to particular classes of graphs. To date, no general upper bound has been established for the performance ratio of any polynomial-time approximation scheme for MWEB. In this research, we examine the complexity of MWEB from both theoretical and practical perspectives. Initially, we prove that MWEB is NP-hard to approximate.",
        "ori-fast-z-score": -0.6260990336999411,
        "water-fast-z-score": 6.7082039324993685,
        "rewrite-fast-z-score": 1.227881227029841
    },
    {
        "original_text": "We consider the statistical properties of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions, power spectra, and probability density functions (PDFs). We show that these quantities can be expressed by means of solutions to certain partial differential equations with time-dependent coefficients. The PDFs are obtained for both stationary and nonstationary cases using the method of characteristics. In particular, we derive an exact expression for the PDF of the amplitude fluctuations of a monochromatic plane wave propagating through a turbulent medium. This result is used to obtain expressions for the mean-square fluctuation levels of the electric field strength and intensity at any point along the propagation path. Finally, we discuss some applications of our results to radiowave scintillation theory and radar detection problems. PACS: 42.65.Tg, 43.20 .Fx, 47.55.+q, 47.60.+j",
        "watermark_text": "We consider the statistical characteristics of nonstationary random acoustic and electromagnetical waves in terms of their correlation functions , power spectra , and likelihood density functions ( PDFs ) . We see that these quantities can be described by means of solutions to many partial differential coefficients with time - dependent coefficients .The PDFs are derived for both static and nonstationary cases using the method of characteristics . In particular , we derive an precise representation for the PDF of the amplitude fluctuations of a monochromatic plane beam propagating through a turbulent medium .This result is utilized to obtain definitions for the mean - square fluctuation concentrations of the electric field intensity and intensity at any point along the propagation path . Finally , we explain some applications of our findings to radiowave scintillation physics and radar detection difficulties .PACS : 42 . 65 . Tg , 43 . 20 . Fx , 47 . 55 . + q , 47 . 60 . + j",
        "rewrite_text": "We examine the statistical properties of nonstationary random acoustic and electromagnetic waves through their correlation functions, power spectra, and probability density functions (PDFs). Our analysis indicates that these quantities can be represented by solutions to various partial differential equations with time-dependent coefficients. Using the method of characteristics, we derive PDFs for both static and nonstationary scenarios. Specifically, we provide an accurate representation for the PDF of amplitude fluctuations in a monochromatic plane wave traveling through a turbulent medium. This result is then employed to define the mean-square fluctuation concentrations of electric field intensity and intensity at any point along the propagation path. Finally, we discuss the implications of our findings for radiowave scintillation physics and challenges in radar detection. PACS: 42.65.Tg, 43.20.Fx, 47.55.+q, 47.60.+j",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 2.286002286003429
    },
    {
        "original_text": "We propose an experimental scheme for generating graph states using the quantum Ising model in transverse magnetic field and its generalization to higher dimensions, which is realized by coupling spins via two-mode squeezed vacuum fields. We show that this method can be used to generate arbitrary graph states on one-dimensional (1D) chain as well as two-dimensional square lattice. In particular, we demonstrate how to prepare 1D cluster states and 2D cluster-dot states. The proposed scheme has several advantages over previous proposals such as high fidelity, scalability, and flexibility. It also provides a new way to study many-body physics beyond the standard paradigm based on fermions or bosons. Graph states are useful resources for various applications including measurement-based quantum computation  1  , quantum communication  2  , and quantum metrology  3  . They have been generated experimentally  4  -  8  .\nIn recent years there has been considerable interest in developing schemes for preparing graph states  9  -  17  . Most existing methods require either sophisticated optical elements  10  -  12  or complicated interactions between atoms  13  -  15  . Recently, it was shown that graph states could be prepared efficiently using only linear optics  16  -  18  . However, these approaches suffer from low efficiency due to photon loss during transmission through optical fibers  19  . Alternatively, graph states may be produced deterministically using trapped ions  20  -  22  . This approach requires precise control of ion-ion interaction strength and suffers from limited scalability  23  .",
        "watermark_text": "We suggest an experimental plan for generating graph states utilizing the molecular Ising model in transverse magnetic force and its generalization to higher dimensions , which is realized by bonding spins via two - mode squeezed vacuum fields . We see that this scheme can be used to create arbitrary graph states on one - dimensional ( 1D ) chain as well as two - dimensional square lattice .In particular , we prove how to produce 1D cluster states and 2D cluster - dot states . The proposed system has numerous benefits over past proposals such as wide fidelity , scalability , and flexibility .It additionally offers a new place to study many - bodies physics beyond the standard paradigm focusing on fermions or bosons . Graph laws are helpful resources for various uses including measurement - based quantum computation 1 , quantum communication 2 , and quantum metrology 3 .They have been constructed experimentally 4 - 8 . In recent years there has been substantial interest in implementing schemes for preparing diagram states 9 - 17 .Most existing techniques require either sophisticated optical groups 10 - 12 or complicated relationships between elements 13 - 15 . Recently , it was shown that graph states could be formed efficiently use only linear optics 16 - 18 .However , these solutions suffer from small performance due to photon losing during transmission through optical fibers 19 . Alternatively , graph states may be formed deterministically using trapped ions 20 - 22 .This method needs rigorous study of ion - ion interaction strength and suffers from reduced scalability 23 .",
        "rewrite_text": "We propose an experimental approach to generating graph states using the molecular Ising model within a transverse magnetic field, as well as its extension to higher dimensions. This method involves coupling spins through two-mode squeezed vacuum fields. Our analysis demonstrates that this strategy can be employed to produce arbitrary graph states on both one-dimensional (1D) chains and two-dimensional square lattices. Specifically, we illustrate how to create 1D cluster states and 2D cluster-dot states. Our proposed system offers several advantages compared to previous methods, including high fidelity, scalability, and flexibility. Moreover, it provides a novel framework for exploring many-body physics beyond the traditional focus on fermions and bosons. Graph states serve as valuable resources for various applications such as measurement-based quantum computation, quantum communication, and quantum metrology, and they have been realized in experimental settings. In recent years, there has been a growing interest in developing techniques to prepare graph states. Most existing methods rely on complex optical systems or intricate interactions among components. Recently, efficient formation of graph states using only linear optics has been demonstrated. However, these approaches are limited by reduced performance due to photon loss during transmission through optical fibers. Alternatively, graph states can be deterministically generated using trapped ions, but this method requires a detailed understanding of ion-ion interaction strengths and is hindered by scalability issues.",
        "ori-fast-z-score": -0.8838834764831843,
        "water-fast-z-score": 7.424621202458749,
        "rewrite-fast-z-score": -0.7863336509949341
    },
    {
        "original_text": "We present an analysis of the gravitational waveforms emitted by two neutron stars orbiting each other, and show that they can be used to detect violations of Lorentz invariance (LI). We consider both scalar-tensor theories with spontaneous breaking of LI as well as vector-tensor theories where LI is violated through the presence of a preferred reference frame. In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the observed gravitational waveform and those predicted within Einstein s theory. \n \n The detection of such deviations would provide strong evidence for new physics beyond standard model expectations. This could have important consequences on our understanding of fundamental interactions at high energies. For example, it may shed light on the origin of dark energy or even reveal the existence of extra dimensions of space-time. It also has implications for cosmology since many extensions of the Standard Model predict time variations of physical constants like Newton s constant G.",
        "watermark_text": "We present an assessment of the gravitational waveforms emitted by two neutron stars orbiting each other , and find that they can be used to identify violations of Lorentz invariance ( LI ) . We consider both scalar - vector models with spontaneous breaking of LI as well as vector - vector models where LI is enforced through the presence of a preferred source frame .In these models we find that there are characteristic deviations from general relativity which lead to measurable differences between the expected gravitational waveform and those predicted within Einstein s theory . The diagnosis of such deviations might give strong evidence for future physics beyond standard theory expectations .This might have important implications on our knowing of fundamental interactions at high energies . For instance , it could cast light on the origin of dark energy or actually expose the existence of extra dimensions of space - time .It additionally has implications for cosmology since several extensions of the Standard Model predict time variations of physical constants like Newton s constant G .",
        "rewrite_text": "We provide an evaluation of the gravitational waveforms produced by two neutron stars in orbit around each other, discovering that these waveforms can be utilized to detect potential violations of Lorentz invariance (LI). Our study examines both scalar-vector models that feature spontaneous breaking of LI, as well as vector-vector models where LI is maintained by a preferred source frame. Within these frameworks, we identify distinct deviations from general relativity, resulting in observable differences between the expected gravitational waveforms and those predicted by Einstein’s theory. Recognizing such deviations could offer compelling support for new physics beyond the standard theoretical framework, which may significantly enhance our understanding of fundamental interactions at high energies. For example, this research could provide insights into the origin of dark energy or reveal the existence of extra dimensions in spacetime. Additionally, it has cosmological implications, as various extensions of the Standard Model suggest that physical constants, such as Newton's constant G, may vary over time.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "We present here a detailed discussion on the concept of allovalency, which is defined as the simultaneous binding to multiple sites in one molecule by different ligands (or receptors). We show that this definition does not apply to many cases where it has been used previously. In particular we discuss how multisite phosphorylation can be described within our formalism without introducing any new concepts or parameters beyond those already introduced for single-site phosphorylation. Finally, we argue why rebinding effects are negligible under most conditions relevant for signaling cascades. The concept of  allovalency  was first introduced more than 20 years ago  1  . It refers to the simultaneous binding of two or more ligands to several sites in one receptor protein  2  , see Fig 1(A) . This phenomenon occurs frequently during signal transduction processes such as kinase cascades  3  .\nThe term  allovalent  was coined because it describes a situation intermediate between monovalent and multivalent interactions  4  : while each ligand binds only once per receptor, there may exist several copies of the same ligand bound simultaneously to the same receptor. Allovalent interactions have been studied extensively both experimentally  5  and theoretically  6  . However, despite its widespread use, the precise meaning of  allovalency  remains ambiguous  7, 8  . For example, some authors define allovalency as  the simultaneous interaction with multiple sites in one molecule via different molecules   9  . Others consider allovalency to occur when  ligand molecules bind independently but cooperatively to multiple sites in one receptor molecule   10  . Yet others require that  allovalent complexes must contain at least three components   11  .",
        "watermark_text": "We see here a detailed discussion on the notion of allovalency , which is characterized as the concurrent binding to multiple sites in one molecule by various ligands ( or receptors ) . We see that this definition does not apply to many situations where it has been used earlier .In particular we explain how multisite phosphorylation can be described within our formalism without mentioning any new concepts or parameters beyond those already adopted for single - location phosphorylation . Finally , we explain why rebinding impacts are negligible under most situations relevant for pathway cascades .The concept of allovalency was first applied more than 20 years early 1 . It refers to the concurrent binding of two or more ligands to several sites in one receptor protein 2 , see Fig 1 ( A ) .This phenomenon occurs commonly during signal transduction processes such as kinase cascades 3 . The term allovalent was developed because it describes a situation intermediate between monovalent and multivalent interactions 4 : while each ligand binds only once per receptor , there may contain many versions of the same ligand attached simultaneously to the same receptor .Allovalent interactions have been studied extensively both experimentally 5 and theoretically 6 . However , despite its widespread using , the exact significance of allovalency remains ambiguous 7 , 8 .For instance , some writers define allovalency as the concurrent interaction with many sites in one molecule via different molecules 9 . Others consider allovalency to occur when ligand molecules interact independently but cooperatively to multiple sites in one receptor molecule 10 .Yet others require that allovalent complexes must include at least three components 11 .",
        "rewrite_text": "This paragraph presents an in-depth exploration of the concept of allovalency, defined as the simultaneous binding of various ligands (or receptors) to multiple sites on a single molecule. It becomes evident that this definition does not encompass all instances in which it has previously been applied. Specifically, we illustrate how multisite phosphorylation can be effectively described within our framework without the introduction of new concepts or parameters beyond those already established for single-location phosphorylation. Additionally, we clarify why the effects of rebinding are typically minimal in most contexts relevant to pathway cascades. The term allovalency was first introduced over 20 years ago and refers to the simultaneous binding of two or more ligands to multiple sites on a receptor protein, as illustrated in Figure 1 (A). This phenomenon frequently occurs during signal transduction processes, such as kinase cascades. The term was coined to describe a situation that falls between monovalent and multivalent interactions. In allovalent interactions, each ligand binds once per receptor, even though multiple versions of the same ligand can simultaneously attach to the same receptor. Extensive studies—both experimental and theoretical—have been conducted on allovalent interactions. However, despite its widespread use, the precise meaning of allovalency remains unclear. For example, some authors define allovalency as the simultaneous interaction of various molecules with multiple sites on a single molecule, while others regard it as the independent yet cooperative interaction of ligand molecules with several sites on a receptor molecule. Furthermore, some definitions stipulate that allovalent complexes must consist of at least three components.",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.538530488755173,
        "rewrite-fast-z-score": -0.3713906763541037
    },
    {
        "original_text": "The Large Magellanic Cloud (LMC) is an ideal laboratory for studying Galactic structure, stellar populations, chemical evolution, and cosmology because it has many advantages over other nearby galaxies such as M31 or M33.  The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode. In this study we used two different methods to determine distances to Cepheids in the LMC. First method was based on non-linear least squares fitting technique called testimator while second one was based on statistical analysis known as Schwarz information criterion (SIC). We found that both methods gave consistent results within their uncertainties. Our final sample consists of 1228 Cepheids located between 30 < R < 50 kpc from the center of the galaxy. Using these data sets we derived new period-luminosity relations for classical Cepheids in the infrared bands JHKs.",
        "watermark_text": "The Large Magellanic Cloud ( LMC ) is an excellent laboratory for studying Galactic structure , stars populations , chemical evolution , and cosmology because it has numerous benefits over other nearby galaxies such as M31 or M33 . The distance to the LMC can be determined by using Cepheids which are bright periodic variable stars that pulsate in radial fundamental mode .In this study we using two different methods to estimate distances to Cepheids in the LMC . First method was based on non - linear least squares fit technique called testimator while second one was based on statistical analysis called as Schwarz information criterion ( SIC ) .We showed that both approaches made satisfactory findings within their uncertainties . Our last sample consists of 1228 Cepheids situated between 30 < R < 50 kpc from the hub of the galaxy .Using these information sets we derived additional period - luminosity relations for classical Cepheids in the infrared bands JHKs .",
        "rewrite_text": "The Large Magellanic Cloud (LMC) serves as a remarkable environment for investigating Galactic structure, stellar populations, chemical evolution, and cosmology due to its advantages over nearby galaxies such as M31 and M33. The distance to the LMC can be accurately measured using Cepheid variables, which are bright stars that exhibit periodic pulsations in their fundamental radial mode. In this study, we employed two distinct methods to estimate the distances to the Cepheids in the LMC. The first method utilized a non-linear least squares fitting technique called testimator, while the second method involved a statistical analysis known as the Schwarz Information Criterion (SIC). Our findings indicate that both methods yielded satisfactory results within their respective uncertainties. Our final sample comprised 1,228 Cepheids located between 30 and 50 kpc from the center of the galaxy. Using this dataset, we derived new period-luminosity relations for classical Cepheids in the infrared bands J, H, and Ks.",
        "ori-fast-z-score": 1.25,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "We propose to realize strong atom-field coupling between a single-mode microwave field and ultracold atoms trapped inside a microchip-based Fabry-Perot resonator with high finesse mirrors. The system is described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. We show that this scheme can be implemented using current technology. In particular we discuss how to trap cold atoms close to one mirror surface and how to couple them efficiently to the microwave mode via evanescent fields. Finally, we present numerical results showing that it should be possible to reach the regime where quantum effects become important even if only a small number of atoms are loaded into the cavity. This work was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP). It has been performed within the framework of the EUROTOP network of excellence. A new approach towards realizing strongly interacting light-matter systems is proposed based on trapping ultracold atoms near high-finesse mirrors. By exploiting the large electric dipole moment associated with atomic transitions in the microwave domain, we demonstrate that such a setup allows us to achieve strong atom-field coupling at the single-atom limit.",
        "watermark_text": "We suggest to realize strong element - field coupling between a single - mode microwave field and ultracold atoms trapped inside a microchip - based Fabry - Perot resonator with high finesse mirrors . The system is characterized by the Jaynes - Cummings model , which predicts Rabi oscillations at the single - atom level .We see that this scheme can be executed using current technology . In particular we explain how to capture cold molecules close to one reflection edge and how to link them efficiently to the microwave mode via evanescent fields .Finally , we present numerical findings indicating that it should be possible to reach the regime where quantum effects become crucial even if only a small number of atoms are transported into the cavity . This project was supported by the European Commission under contract IST - 2001 - 34011 Quantum Photonics ( QAP ) .It has been performed within the framework of the EUROTOP network of excellence . A modern alternative towards developing highly correlated light - matter structures is proposed based on trapping ultracold atoms near high - finesse mirrors .By exploiting the huge electric dipole moment associated with atomic interactions in the microwave domain , we prove that such a setup allows us to achieve powerful atom - field coupling at the single - atom limit .",
        "rewrite_text": "We propose establishing strong coupling between a single-mode microwave field and ultracold atoms confined within a microchip-based Fabry-Perot resonator featuring high finesse mirrors. This system can be effectively described by the Jaynes-Cummings model, which predicts Rabi oscillations at the single-atom level. Our analysis indicates that this scheme can be implemented using existing technology. Specifically, we outline the process of capturing cold molecules near one reflective edge and efficiently coupling them to the microwave mode through evanescent fields. Additionally, our numerical results suggest that it is feasible to attain a regime where quantum effects become significant, even with a limited number of atoms entering the cavity. This research was supported by the European Commission under contract IST-2001-34011 Quantum Photonics (QAP) and was conducted as part of the EUROTOP network of excellence. We present a contemporary approach to developing highly correlated light-matter structures by trapping ultracold atoms close to high-finesse mirrors. By leveraging the substantial electric dipole moments associated with atomic interactions in the microwave domain, we demonstrate that this configuration facilitates robust atom-field coupling at the single-atom level.",
        "ori-fast-z-score": 1.6915632233569815,
        "water-fast-z-score": 7.32709181802739,
        "rewrite-fast-z-score": 2.041241452319315
    },
    {
        "original_text": "We propose an algorithm that learns how to segment time series data into meaningful segments by using a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). The proposed method is based on the idea that each expert in our ensemble has its own internal representation of the input sequence which can be used as prior knowledge when estimating the uncertainty associated with the prediction made by this particular expert. We show that such approach leads to better results than state-of-the-art methods both in terms of accuracy and computational efficiency. \nThe main contributions of this work are summarized below: \n1. A novel framework for modeling uncertain predictions produced by multiple RNN Experts.\n2. An efficient online training procedure for updating parameters of all Experts simultaneously.\n3. Extensive experiments conducted on several real-world datasets demonstrate superior performance of the proposed method over existing approaches. Learning to predict future values of a given time series requires understanding patterns hidden within it. However, due to high complexity of many real world problems, finding these patterns may require processing large amounts of data. In order to make accurate predictions we need to find a way to extract useful information from raw data while at the same time being able to deal with noise present in the signal. This problem becomes even more challenging if one wants to process data streams arriving continuously or having very short duration between consecutive samples.",
        "watermark_text": "We suggest an algorithm that learns how to span moment series information into meaningful spans by using a combination of Recurrent Neural Network ( RNN ) Experts and Adaptive Variance Estimation ( AVE ) . The proposed approach is based on the idea that each expert in our ensemble has its own internal description of the input sequence which can be used as prior information when estimating the uncertainty associated with the prediction created by this given investigator .We suggest that such approach leads to good success than state - of - the - art methods both in terms of precision and mathematical efficiency . The main contributions of this research are presented below : 1 .A innovative paradigm for modeling uncertain estimates generated by many RNN Experts . 2 .An efficient electronic training method for updating values of all Experts simultaneously . 3 .Extensive experiments conducted on numerous real - global datasets prove excellent performance of the suggested method over existing techniques . Learning to predict upcoming estimates of a given time cycle requires studying behaviors hiding within it .However , owing to large complexity of several real life issues , finding these patterns could cost processing large quantities of evidence . In order to make accurate forecast we must to find a way to extract useful info from raw data while at the same time being able to deal with sound present in the signal .This problem becomes especially more challenging if one wants to analyze information streams arriving constantly or having very small periods between successive samples .",
        "rewrite_text": "We propose an algorithm that effectively captures moment series data by leveraging a combination of Recurrent Neural Network (RNN) Experts and Adaptive Variance Estimation (AVE). Our approach hinges on the concept that each expert in the ensemble possesses a unique internal representation of the input sequence, which serves as valuable prior information for assessing the uncertainty related to their predictions. We believe that this method outperforms state-of-the-art techniques in both accuracy and computational efficiency. The key contributions of this research include: 1. An innovative framework for modeling uncertain estimates produced by multiple RNN Experts. 2. An efficient electronic training method for simultaneously updating the parameters of all Experts. 3. Extensive experiments conducted on a variety of real-world global datasets demonstrating the superior performance of our method compared to existing techniques. Predicting future values in a time series involves uncovering underlying behaviors, but the complexity of real-world problems often requires processing vast amounts of data to identify these patterns. To achieve accurate forecasts, it is essential to extract relevant information from raw data while effectively managing noise in the signals. This challenge intensifies when dealing with continuous information streams or very short intervals between consecutive samples.",
        "ori-fast-z-score": -1.1445861782233109,
        "water-fast-z-score": 8.945991796931699,
        "rewrite-fast-z-score": 0.19611613513818404
    },
    {
        "original_text": "The author considers the problem of gravitational interaction between bodies in terms of their informational content. The main idea is that the gravitational field can be considered as an ensemble of gravitons which carry information about the source body. Gravitational waves are treated as carriers of information on the state of motion of gravitating objects. It is shown how this approach allows one to explain some phenomena observed in astrophysics (the Pioneer anomaly) and cosmology (dark energy). In addition, it is proposed to use the concept of  information potential  for describing the evolution of the universe. This article was published by the journal Classical and Quantum Gravity Volume 27, Issue 14, pages 5993-6010, November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is taken directly from the original publication. \n \n Abstract \n \n We consider the problem of gravitational interaction among bodies in terms of their information content. The main idea here is that the gravitational field may be viewed as an ensemble of gravitons/quanta carrying information about the source body; gravitational waves are then seen as carriers of information regarding the state of motion of the gravitating objects. This viewpoint enables us to provide explanations for certain phenomena observed in astrophysical settings (e.g., the Pioneer anomaly), as well as in cosmological contexts (e.g., dark energy). Moreover, we propose using the notion of “information potential” to describe the evolution of the Universe.",
        "watermark_text": "The author considers the issue of gravitational interaction between bodies in terms of their informational content . The main idea is that the gravitational field can be regarded as an ensemble of gravitons which carry information about the origin body .Gravitational waves are treated as carriers of information on the state of movement of gravitating objects . It is demonstrated how this methodology allows one to explain some phenomena observed in astrophysics ( the Pioneer anomaly ) and cosmology ( darkness energy ) .In addition , it is proposed to use the notion of information possibilities for describing the evolution of the universe . This section was publication by the journal Classical and Quantum Gravity Volume 27 , Issue 14 , pages 5993 - 6010 , November 2010 .DOI : 10 . 1088 / 0264 - 9381 / 27 / 14 / 05993 / abstract . The following text is taken directly from the original published .Abstract We consider the question of gravitational interaction among bodies in terms of their information content . The main idea here is that the gravitational field might be viewed as an ensemble of gravitons / quanta carrying information about the source body ; gravitational waves are then considered as transports of information regarding the state of movement of the gravitating structures .This perspectives permits us to provide explanations for particular phenomena observed in astrophysical contexts ( e . g . , the Pioneer anomaly ) , as also as in cosmological contexts ( e . g . , darkness energy ) . Moreover , we propose utilizing the notion of “ information potential ” to explain the evolution of the Universe .",
        "rewrite_text": "The author explores the concept of gravitational interactions between bodies through their informational content. The central idea is that the gravitational field can be perceived as a collection of gravitons that convey information about their originating body. Gravitational waves, in this framework, are viewed as carriers of information regarding the motion of gravitating objects. This approach demonstrates how certain phenomena in astrophysics, such as the Pioneer anomaly, and in cosmology, like dark energy, can be explained. Additionally, the author suggests employing the concept of \"information potential\" to describe the universe's evolution. This section was published in the journal *Classical and Quantum Gravity*, Volume 27, Issue 14, pages 5993-6010, in November 2010. DOI: 10.1088/0264-9381/27/14/05993/abstract. The following text is directly excerpted from the original publication. \n\nAbstract: We investigate the issue of gravitational interaction among bodies in terms of their information content. The core notion is that the gravitational field could be interpreted as a collection of gravitons/quanta that carry information about the source body; consequently, gravitational waves are considered as conveyors of information regarding the motion of gravitating structures. This perspective allows us to explain specific phenomena observed in astrophysics (e.g., the Pioneer anomaly) as well as in cosmology (e.g., dark energy). Furthermore, we propose the use of \"information potential\" to describe the evolution of the universe.",
        "ori-fast-z-score": 2.116950987028628,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": -0.6
    },
    {
        "original_text": "We present the first comprehensive study of visible spectroscopy for all known Jupiter trojans (JTs). We obtained high-resolution spectra with Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2 in order to determine their surface compositions. The sample includes 49 JTs including two new discoveries by our team. Our analysis shows that most JTs are S-complex asteroids or C-type asteroids. However, we found four objects which show unusual spectral features; one is an E-type asteroid, while three others have featureless red-sloped continuum. These objects may be primitive bodies such as carbonaceous chondrites. In addition, we find that some JTs belong to dynamically cold families. This suggests that these families were formed recently through catastrophic collisions among parent bodies. Finally, we discuss possible formation scenarios for JTs based on our findings. Keywords: Asteroids",
        "watermark_text": "We present the first comprehensive investigation of visible spectroscopy for all known Jupiter trojans ( JTs ) . We obtained high - resolution spectra with Subaru / HDS , Gemini / GNIRS , and Keck / NIRC - 2 in order to obtain their surface compositions .The sample comprises 49 JTs including two latest discoveries by our team . Our study shows that most JTs are S - complex asteroids or C - class asteroids .However , we identified four bodies which show peculiar spectral features ; one is an E - class asteroid , while three rest have featureless red - sloped continuum . These bodies may be ancient bodies such as carbonaceous chondrites .In addition , we find that some JTs belong to dynamically cold families . This implies that these families were created recently through catastrophic collisions among parent bodies .Finally , we explain possible formation scenarios for JTs based on our findings . Keywords : Asteroids",
        "rewrite_text": "We present the first thorough analysis of visible spectroscopy for all known Jupiter trojans (JTs). Utilizing high-resolution spectra obtained from Subaru/HDS, Gemini/GNIRS, and Keck/NIRC-2, we investigated their surface compositions. Our sample includes 49 JTs, featuring two recent discoveries made by our team. Our research reveals that the majority of JTs are S-complex or C-class asteroids. However, we identified four bodies with unusual spectral characteristics; one is classified as an E-class asteroid, while the other three exhibit a featureless red-slope continuum. These bodies may represent ancient objects akin to carbonaceous chondrites. Furthermore, we discovered that some JTs belong to dynamically cold families, suggesting these families formed relatively recently due to catastrophic collisions among their parent bodies. Lastly, we discuss potential formation scenarios for JTs based on our findings. Keywords: Asteroids.",
        "ori-fast-z-score": 0.13483997249264842,
        "water-fast-z-score": 4.900769721140662,
        "rewrite-fast-z-score": 0.2581988897471611
    },
    {
        "original_text": "We present an analysis of the properties of giant planet candidates detected by the Optical Gravitational Lensing Experiment (OGLE) survey, which has been monitoring stars towards the Galactic bulge for more than ten years. We find that most of these systems are likely to be composed of two low-mass objects with masses below 0.5 MJup orbiting each other on periods longer than 100 days. The majority of them have orbital inclinations close to 90 degrees. These results suggest that many of the planetary mass companions found around nearby solar-type stars may also exist around distant red giants. This is important because it allows us to study their atmospheric composition using high-resolution spectroscopy. \n \n Keywords: Planetary systems; Planets; Transiting exoplanet searches; Red-giant stars; Bulge; Massive star formation; Stellar evolution; Extrasolar planets; Astrophysics",
        "watermark_text": "We present an assessment of the properties of large planet candidates detected by the Optical Gravitational Lensing Experiment ( OGLE ) study , which has been monitoring stars towards the Galactic bulge for more than ten years . We see that most of these systems are likely to be composed of two low - weight objects with masses below 0 . 5 MJup orbiting each other on periods longer than 100 hours .The majority of them have orbital inclinations nearly to 90 degrees . These data suggest that several of the planetary mass companions seen around nearby planetary - class stars would also exist around distant red giants .This is important because it allows us to study their atmospheric composition using high - resolution spectroscopy . Keywords : Planetary systems ; Planets ; Transiting exoplanet searches ; Red - giant galaxies ; Bulge ; Massive star formation ; Stellar evolution ; Extrasolar stars ; Astrophysics",
        "rewrite_text": "We provide an evaluation of the characteristics of large planet candidates identified by the Optical Gravitational Lensing Experiment (OGLE), which has been observing stars in the Galactic bulge for over a decade. Our findings indicate that the majority of these systems likely consist of two low-mass objects, each with masses below 0.5 MJup, orbiting one another with orbital periods exceeding 100 hours. Most of these systems exhibit orbital inclinations close to 90 degrees. These observations imply that many of the planetary-mass companions discovered around nearby planetary-class stars may also be present around distant red giants. This information is significant as it enables the examination of their atmospheric compositions through high-resolution spectroscopy. \n\nKeywords: Planetary systems; Planets; Transiting exoplanet searches; Red giant galaxies; Bulge; Massive star formation; Stellar evolution; Extrasolar stars; Astrophysics.",
        "ori-fast-z-score": 0.6509445549041194,
        "water-fast-z-score": 5.598123172175427,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new HST/ACS observations of four nearby Fanaroff-Riley type-I (FR I) radio galaxies, 3C 31, 3C 264, 4C+55.17 and PKS 0745-191 . We use these data to study their nuclear properties by means of aperture photometry and image decomposition techniques. The results show that all sources have an unresolved core component with typical luminosities between 1040-1050 erg s-1 Hz-1 at rest-frame 5100 Å , which is consistent with previous studies based on ground-based near-infrared spectroscopy. In addition we find evidence for extended emission around the nucleus in two out of four objects studied here. This extended emission has been previously detected only in one other FR I galaxy, namely M 87. Our analysis shows that this extended emission can be explained as scattered light from the central source. Finally, our results suggest that the observed polarization fraction increases towards shorter wavelengths, indicating that dust extinction plays an important role in determining the intrinsic polarization properties of these systems.",
        "watermark_text": "We report new HST / ACS images of four nearby Fanaroff - Riley type - I ( FR I ) radio objects , 3C 31 , 3C 264 , 4C + 55 . 17 and PKS 0745 - 191 . We use these information to study their nuclear properties by means of aperture photometry and image decomposition techniques .The results show that all sources have an unresolved core component with typical luminosities between 1040 - 1050 erg s - 1 Hz - 1 at rest - frame 5100 Å , which is compatible with previous research based on ground - based near - infrared spectroscopy . In addition we find proof for extended emitted around the nucleus in two out of four observations studied here .This extended emitted has been previously observed only in one other FR I galaxy , namely M 87 . Our study shows that this extended emitted can be understood as distributed radiation from the main origin .Finally , our findings confirm that the seen polarization fraction increases towards shorter wavelengths , showing that dust extinction holds an important role in determining the intrinsic polarization properties of these systems .",
        "rewrite_text": "We present new HST/ACS images of four nearby Fanaroff-Riley type I (FR I) radio galaxies: 3C 31, 3C 264, 4C +55.17, and PKS 0745-191. We utilize these images to investigate their nuclear properties through aperture photometry and image decomposition methods. Our findings indicate that all sources possess an unresolved core component with typical luminosities ranging from 10^40 to 10^50 erg s^-1 Hz^-1 at a rest frame of 5100 Å, consistent with prior studies using ground-based near-infrared spectroscopy. Additionally, we observe evidence of extended emission around the nucleus in two out of the four galaxies examined. This extended emission has previously only been detected in one other FR I galaxy, M 87. Our study suggests that this extended emission can be interpreted as radiation dispersed from the main source. Finally, our results confirm that the polarization fraction observed increases with shorter wavelengths, highlighting the significant role of dust extinction in influencing the intrinsic polarization properties of these galaxies.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We study the quantum Hall effect (QHE) and its interplay with magnetism in monolayer graphene by using an effective low-energy model that takes into account both electron-electron interactions and disorder effects. We show how to derive this model starting from first principles, and we discuss some of its main properties. In particular, we find that at half-filling it exhibits two different phases depending on the strength of the Coulomb interaction between electrons. For weak coupling these are separated by a phase transition driven by spontaneous breaking of time-reversal symmetry; for strong coupling they correspond respectively to a conventional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be equivalent to a spin-1/2 Heisenberg antiferromagnet on the honeycomb lattice, which can thus be described within the framework of the so-called SU(4) bosonic representation.",
        "watermark_text": "We research the quantum Hall impact ( QHE ) and its interplay with magnetism in monolayer graphene by using an efficient low - energy theory that takes into consideration both electron - atom relationships and disturbance effects . We see how to derive this description starting from initial principles , and we explain some of its primary characteristics .In particular , we find that at half - filling it displays two different stages depending on the strength of the Coulomb interaction between electrons . For weak bonding these are distinct by a phase shift motivated by premature breaking of time - reversal symmetry ; for strong coupling they relate respectively to a traditional QHE state and to a novel fractionalized topological insulator characterized by chiral edge states .The latter is demonstrated to be analogous to a spin - 1 / 2 Heisenberg antiferromagnet on the honeycomb structure , which can thus be described within the framework of the so - called SU ( 4 ) bosonic representation .",
        "rewrite_text": "We investigate the quantum Hall effect (QHE) and its interaction with magnetism in monolayer graphene by employing an efficient low-energy theory that accounts for both electron-atom interactions and perturbative effects. We detail how to derive this framework from fundamental principles and highlight some of its key features. Notably, we discover that at half-filling, the behavior exhibits two distinct regimes based on the strength of the Coulomb interaction between electrons. In the weak coupling regime, these regimes are differentiated by a phase shift resulting from an early breaking of time-reversal symmetry. In contrast, in the strong coupling regime, they correspond to the conventional QHE state and a novel fractionalized topological insulator characterized by chiral edge states. The latter is shown to be analogous to a spin-1/2 Heisenberg antiferromagnet on a honeycomb lattice, which can be described using the SU(4) bosonic representation.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.5,
        "rewrite-fast-z-score": 0.6401843996644799
    },
    {
        "original_text": "We study the statistical properties of highly compressible turbulence by means of direct numerical simulations (DNS) at Mach numbers ranging between 10 and 100, using both isothermal and adiabatic equations of state for the gas.  We find that the scaling laws observed in incompressible flows are violated when the Mach number becomes large enough to produce shocks. In particular, we show that the energy spectrum exhibits an exponential decay with wavenumber k instead of the power-law behavior E(k) ~ k-5/3 predicted by Kolmogorov s theory. The intermittent nature of the flow is also investigated through the analysis of probability density functions (PDFs). It turns out that PDF tails become increasingly fat-tailed as M increases, which can be explained by invoking the presence of shock waves. Finally, we discuss how these results may affect our understanding of astrophysical phenomena such as supernova remnants or interstellar clouds.",
        "watermark_text": "We research the statistical characteristics of highly compressible turbulence by means of direct numerical simulations ( DNS ) at Mach numbers ranging between 10 and 100 , using both isothermal and adiabatic equations of state for the gas . We see that the scaling laws witnessed in incompressible flows are ignored when the Mach number becomes large enough to produce shocks .In particular , we prove that the power spectrum exhibits an exponential decay with wavenumber k instead of the power - law behavior E ( k ) ~ k - 5 / 3 predicted by Kolmogorov s principle . The intermittent nature of the flow is also examined through the examination of likelihood density functions ( PDFs ) .It turns out that PDF tails get increasingly fat - tailed as M rises , which can be described by invoking the presence of flash currents . Finally , we explain how these results may affect our perception of astrophysical processes such as supernova remnants or interstellar dust .",
        "rewrite_text": "We investigate the statistical properties of highly compressible turbulence through direct numerical simulations (DNS) at Mach numbers ranging from 10 to 100, employing both isothermal and adiabatic equations of state for the gas. Our findings indicate that the scaling laws observed in incompressible flows are disregarded when the Mach number is sufficiently high to generate shocks. Specifically, we demonstrate that the power spectrum exhibits an exponential decay with respect to wavenumber \\( k \\), diverging from the power-law behavior \\( E(k) \\sim k^{-5/3} \\) predicted by Kolmogorov's principle. Additionally, we explore the intermittent characteristics of the flow by analyzing probability density functions (PDFs), revealing that the tails of the PDFs become progressively fatter as the Mach number increases, which can be attributed to the influence of flash currents. Lastly, we discuss how these findings may alter our understanding of astrophysical phenomena such as supernova remnants and interstellar dust.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 5.581563056514381,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new results for light element abundances (Li, Be and B) in two metal-poor stars belonging to the Galactic Globular Cluster M4. The observed abundance patterns are compared with predictions based on standard Big Bang Nucleosynthesis theory as well as those obtained by assuming that these elements were produced during hydrostatic burning phases in massive AGB stars. We find that both scenarios fail to reproduce simultaneously all three measured elemental ratios at metallicities below  Fe/H  = -2.0 dex. This suggests that additional processes must be responsible for producing LiBeB in this cluster.  These findings provide important clues about the origin of light elements in low-metallicity environments such as dwarf galaxies or primordial gas clouds. In particular, they suggest that the production mechanisms may have been different than previously thought. Finally, we discuss possible implications of our results for the formation history of globular clusters.",
        "watermark_text": "We report new data for light element abundances ( Li , Be and B ) in two metal - poor stars belonging to the Galactic Globular Cluster M4 . The observed abundance patterns are compared with predictions based on normal Big Bang Nucleosynthesis explanation as well as those achieved by assuming that these elements were produced during hydrostatic heating phases in massive AGB stars .We see that both scenarios lack to predict simultaneously all three measured elemental levels at metallicities below Fe / H = - 2 . 0 dex . This implies that extra reactions must be responsible for producing LiBeB in this cluster .These studies provide important hints about the origin of light elements in low - metallicity habitats such as dwarf stars or primordial dust clusters . In particular , they propose that the production mechanisms may have been changed than previously thought .Finally , we explain possible possibilities of our findings for the formation history of globular complexes .",
        "rewrite_text": "We present new findings on the abundances of light elements (Li, Be, and B) in two metal-poor stars within the Galactic Globular Cluster M4. We compare the observed abundance patterns with predictions based on conventional Big Bang nucleosynthesis and those that consider the production of these elements during hydrostatic heating phases in massive AGB stars. Neither scenario adequately explains the simultaneous measurements of all three elements at metallicities below [Fe/H] = -2.0 dex, suggesting that additional reactions must account for the production of Li, Be, and B in this cluster. These investigations offer valuable insights into the origins of light elements in low-metallicity environments, such as dwarf stars or primordial dust clusters, indicating that production mechanisms may differ from previous assumptions. Ultimately, we discuss the implications of our findings for the formation history of globular clusters.",
        "ori-fast-z-score": 0.3611575592573076,
        "water-fast-z-score": 6.454234490405725,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We present an analysis of peculiar velocities in the local universe, based on data obtained by the Sloan Digital Sky Survey (SDSS). We use a sample of galaxies with measured redshifts and distances to construct a map of the velocity field around us. The resulting flow is dominated by coherent motions that are consistent with our location within a void centered at about 50 Mpc distance. This result confirms earlier findings using different methods. \n \n In addition we find evidence for a dipole component in this flow which points away from the center of the void towards Virgo. Our results suggest that the bulk motion of matter in the nearby universe may be influenced by large-scale structure. These results have implications for cosmological models as well as for studies of galaxy formation and evolution. They also provide new constraints on theories of dark energy. A full version of this article can be found at: http://arxiv.org/abs/astro-ph/0403320",
        "watermark_text": "We present an assessment of peculiar velocities in the local universe , using on evidence derived by the Sloan Digital Sky Survey ( SDSS ) . We use a sample of galaxies with calculated redshifts and distances to build a mapping of the velocity field around us .The produced stream is dominated by coherent motions that are compatible with our location within a void oriented at about 50 Mpc length . This result confirms earlier findings using different methods .In addition we find proof for a dipole component in this stream which points away from the center of the void towards Virgo . Our results propose that the bulk movement of matter in the nearby universe might be affect by large - scale structure .These conclusions have consequences for cosmological models as also as for research of galaxy formation and evolution . They also create novel constraints on explanations of dark energy .A full version of this page can be found at : www : / / arxiv . org / abs / astro - ph / 0403320",
        "rewrite_text": "We provide an evaluation of peculiar velocities in the local universe, based on data obtained from the Sloan Digital Sky Survey (SDSS). By analyzing a sample of galaxies with determined redshifts and distances, we construct a map of the surrounding velocity field. The resulting velocity stream is predominantly characterized by coherent motions consistent with our position within a void approximately 50 Mpc in length. This finding corroborates earlier results derived from different methodologies. Additionally, we identify evidence for a dipole component in this stream, directed away from the center of the void towards Virgo. Our findings suggest that the overall flow of matter in the nearby universe may be influenced by large-scale structures. These conclusions have implications for cosmological models, as well as for studies on galaxy formation and evolution. They also impose new constraints on theories related to dark energy. A complete version of this study can be accessed at: www:/ /arxiv.org/abs/astro-ph/0403320.",
        "ori-fast-z-score": -1.937329799813845,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": -2.038098661460272
    },
    {
        "original_text": "We present the discovery and characterization of two  hot Jupiter  planets orbiting stars that are members of wide binaries, HD 196885AB (a = 1.8 AU) and HD 208598AB (a = 3.6 AU). The planet around HD 196885A is an inflated gas giant with M sin i = 0.88 MJup and P = 4.3 days; it orbits its primary at a distance of only 0.04 AU. We find no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU. Both systems have orbital eccentricities consistent with zero. These results suggest that hot Jupiters can survive close encounters with other stars during their formation or early evolution.  - Introduction \n \n Hot Jupiters are massive gaseous planets on short-period orbits about solar-type stars. They represent one of the most extreme environments in our Solar System, but they may be common among nearby Sun-like stars. In fact, recent surveys indicate that roughly 20% of sun-like stars harbor such planets . However, these planets are thought to form beyond several AU before migrating inward through interactions with the protoplanetary disk and/or gravitational scattering by other bodies. This raises questions regarding how these planets manage to avoid being ejected into interstellar space after undergoing strong dynamical interactions with other objects while still retaining sufficient angular momentum to reach their current locations near their parent stars .\n\nIn this Letter we report the detection of two new  hot Jupiter  planets using high-precision radial velocity measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument (HARPS), which is installed on the European Southern Observatory s 3.6-m telescope located at La Silla Observatory in Chile. One of these planets has an extremely small semi-major axis of just 0.04 AU, making it one of the closest known exoplanets to its parent star.",
        "watermark_text": "We present the discovery and description of two hot Jupiter planets orbiting planets that are part of wide binaries , HD 196885AB ( a = 1 . 8 AU ) and HD 208598AB ( a = 3 . 6 AU ) . The planet around HD 196885A is an inflated gas giant with M sin i = 0 . 88 MJup and P = 4 . 3 days ; it orbits its primary at a distance of only 0 . 04 AU .We see no evidence for additional companions to either host star down to masses as low as 5 MJup within separations of 10 AU . Both components have orbital eccentricities consistent with zero .These data suggest that hot Jupiters can endure close contacts with other stars during their development or early evolved . - Introduction Hot Jupiters are enormous gaseous planets on short - duration orbits about solar - class stars .They represent one of the most intense environments in our Solar System , but they may be prevalent among neighboring Sun - like stars . In reality , recent studies confirm that approximately 20 % of sun - like stars harbor such planets .However , these planets are said to form beyond many AU before migrating inward through interactions with the protoplanetary disk and / or gravitational absorption by other bodies . This opens questions regarding how these planets cope to resist being ejected into interstellar space after undergoing stable dynamical interactions with other objects while nevertheless possessing adequate angular velocity to reach their current places near their sister planets .In this Letter we publish the observation of two new warm Jupiter planets using high - precision radial speed measurements obtained over more than eight years with the High Accuracy Radial Velocity Planet Searcher instrument ( HARPS ) , which is installed on the European Southern Observatory s 3 . 6 - m observatory situated at La Silla Observatory in Chile . One of these planets has an incredibly small semi - major axis of just 0 . 04 AU , making it one of the nearest known exoplanets to its parent star .",
        "rewrite_text": "We report the discovery and characterization of two hot Jupiter planets orbiting within wide binary systems, specifically HD 196885AB (with a semi-major axis of 1.8 AU) and HD 208598AB (with a semi-major axis of 3.6 AU). The planet orbiting HD 196885A is a significantly inflated gas giant with a minimum mass of 0.88 times that of Jupiter (M sin i = 0.88 MJup) and an orbital period of 4.3 days, located just 0.04 AU from its host star. We find no indication of any additional companions around either star, down to a mass limit of 5 MJup within a separation of 10 AU. The orbital eccentricities of both components appear to be close to zero. These findings imply that hot Jupiters may endure close encounters with other stars during their formation or early evolution. \n\n**Introduction**: Hot Jupiters are massive gaseous planets with short orbital periods around solar-like stars. They represent some of the most extreme environments found in our Solar System and are likely common around neighboring Sun-like stars. Recent studies indicate that roughly 20% of these stars contain such planets. Interestingly, these gas giants are believed to form at distances greater than several AU before migrating inward through interactions with the protoplanetary disk or gravitational influences from other bodies. This raises important questions about how these planets survive potential ejection into interstellar space while maintaining sufficient angular momentum to achieve their current orbits close to their parent stars. In this letter, we present the detection of two new warm Jupiter planets, utilizing high-precision radial velocity measurements collected over more than eight years with the High Accuracy Radial Velocity Planet Searcher (HARPS) instrument, located at the European Southern Observatory’s 3.6-meter telescope at La Silla Observatory in Chile. One of these planets has an exceptionally small semi-major axis of only 0.04 AU, making it one of the closest-known exoplanets to its host star.",
        "ori-fast-z-score": -1.7960530202677492,
        "water-fast-z-score": 6.286185570937122,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on an observation made with Chandra s High Energy Transmission Grating Spectrometer (HETGS) that shows dramatic variability of absorption lines in the spectrum of the black hole candidate Cygnus X1, which is known to have strong winds and outflows. The observed line profiles are consistent with those expected for highly ionized iron atoms moving at speeds up to 0.2c along our line-of-sight toward the central source. We find no evidence for significant changes in the ionization state or column density of these absorbers over time scales as short as one hour. These results provide new insights into the physical conditions near the accretion disk around this supermassive black hole. This work was supported by NASA under contract NAS8-03060. \n \n Keywords: Black holes; Winds; Outflows; Accretion disks \n \n Introduction \n \n In recent years there has been growing interest in studying the properties of winds and outflows associated with active galactic nuclei (AGN). Such flows may play important roles in regulating the growth of supermassive black holes through their effects on both the surrounding gas and radiation fields. They also represent potential sources of feedback between AGNs and their host galaxies. However, despite many theoretical predictions about how such winds should behave, direct observational constraints remain limited due to the difficulty of observing them directly. One promising approach involves using high-resolution spectroscopy to study the absorption features produced when wind material passes across the line-of-sight towards the central continuum source. Recent observations of several nearby Seyfert 1 galaxies show clear evidence for variable absorption lines arising from photoionized plasma flowing outward from the nucleus at velocities ranging from ~100-1000 km/sec (e.g., Kaspi et al. 2002; Crenshaw & Kraemer 2003; McKernan et al. 2007 ). Here we present another example of this phenomenon based on a deep Chandra/HETG observation of the brightest member of the class of Galactic black hole candidates (GBHCs), Cygnus X1. \n \n Cygnus X1 is located only 2 kpc away from Earth in the",
        "watermark_text": "We report on an observation made with Chandra s High Energy Transmission Grating Spectrometer ( HETGS ) that reveals significant variability of absorption patterns in the spectrum of the dark hole contender Cygnus X1 , which is known to have stable winds and outflows . The observed line profiles are compatible with those expected for highly ionized iron atoms moved at speeds up to 0 . 2c along our line - of - view toward the main source .We see no evidence for significant variations in the ionization state or column size of these absorbers over time ranges as short as one minute . These data provide fresh insights into the physical conditions near the accretion belt around this supermassive black hole .This project was supported by NASA under contract NAS8 - 03060 . Keywords : Black holes ; Winds ; Outflows ; Accretion disks Introduction In recent years there has been growing interest in investigating the properties of winds and outflows associated with active galactic nuclei ( AGN ) .Such streams may play important roles in controlling the development of supermassive black holes through their impacts on both the nearby gas and radiation fields . They also contribute potential sources of feedback between AGNs and their host galaxies .However , despite many theoretical estimates about how such winds should react , direct observational restrictions remain minimal owing to the difficulty of study them directly . One promising solution involves utilizing large - resolution spectroscopy to study the absorption elements caused when wind material passes across the line - of - view towards the main continuum source .Recent measurements of several neighbouring Seyfert 1 galaxies show good evidence for variable absorption paths derived from photoionized liquid flowing outward from the nucleus at velocities ranging from ~ 100 - 1000 kilometers / sec ( e . g . , Kaspi et al . 2002 ; Crenshaw & Kraemer 2003 ; McKernan et al .2007 ) . Here we present another example of this phenomenon based on a deep Chandra / HETG detection of the brightest member of the class of Galactic dark hole candidates ( GBHCs ) , Cygnus X1 .Cygnus X1 is situated only 2 kpc apart from Earth in the",
        "rewrite_text": "We present findings from an observation utilizing Chandra's High Energy Transmission Grating Spectrometer (HETGS) that uncovers notable variability in the absorption patterns within the spectrum of the dark hole candidate Cygnus X-1, which is recognized for its stable winds and outflows. The detected line profiles correspond to those anticipated for highly ionized iron atoms moving at speeds of up to 0.2c in our line of sight towards the primary source. Importantly, we found no indications of significant changes in the ionization state or column density of these absorbing elements over time spans as short as one minute. This data offers new perspectives on the physical conditions located near the accretion zone surrounding this supermassive black hole. This research was funded by NASA through contract NAS8-03060. \n\n**Keywords:** Black holes; Winds; Outflows; Accretion disks \n\n**Introduction:** Recent years have seen an increase in interest in studying the winds and outflows linked to active galactic nuclei (AGN). Such streams may significantly influence the evolution of supermassive black holes by affecting nearby gas and radiation fields. Additionally, they represent potential mechanisms of feedback between AGNs and their host galaxies. Nevertheless, despite numerous theoretical assessments on the expected behavior of these winds, direct observational constraints have been limited due to the challenges associated with studying them. A promising approach is to employ high-resolution spectroscopy to examine the absorption features produced when wind material crosses the line of sight to the main continuum source. Recent observations of several nearby Seyfert 1 galaxies have provided strong evidence for variable absorption pathways resulting from photoionized outflows dispersing from the nucleus at speeds ranging from approximately 100 to 1000 kilometers per second (e.g., Kaspi et al., 2002; Crenshaw & Kraemer, 2003; McKernan et al., 2007). In this report, we introduce another instance of this phenomenon, derived from a detailed Chandra/HETG observation of Cygnus X-1, the brightest member of the class of Galactic black hole candidates (GBHCs), located just 2 kpc from Earth.",
        "ori-fast-z-score": -1.0650014966747527,
        "water-fast-z-score": 8.382021125342941,
        "rewrite-fast-z-score": 2.137186834969645
    },
    {
        "original_text": "The aim of this study was to evaluate whether virtual ultrasound (US) puncture tracts can improve percutaneous renal access in patients with complex anatomy and difficult-to-visualize calices on fluoroscopy.  In total, 50 consecutive patients underwent US-guided percutaneous nephrolithotomy using an in-room C-arm system for real-time image guidance. The procedure was performed under general anesthesia or conscious sedation. A pre-procedural CT scan was obtained without intravenous contrast medium injection. Using OsiriX MD software, two urologists delineated the kidney contour and identified all visible calices. Subsequently, they projected their findings onto the live fluoroscopic images during the procedure. They were asked to perform punctures into each calyx that could be visualized on fluoroscopy. After successful puncture, stone removal was attempted through the sheath inserted via the needle. Successful puncture was defined as reaching at least one calix. Overall success rate was 88%. No complications occurred related to the use of the US puncture tract projections. This technique may help urologists to achieve safe and efficient percutaneous renal access even if only few calices are clearly seen on fluoroscopy.",
        "watermark_text": "The goal of this study was to examine whether digital ultrasound ( US ) puncture tracts can boost percutaneous renal entry in patients with difficult anatomy and difficult - to - visualize calices on fluoroscopy . In total , 50 consecutive patients received US - guided percutaneous nephrolithotomy employing an in - room C - arm network for real - time vision tracking .The technique was done under general anesthesia or conscious sedation . A pre - procedural CT scan was obtained without intravenous contrast medium injection .Using OsiriX MD software , two urologists delineated the kidney contour and identified all evident calices . Subsequently , they projected their findings onto the live fluoroscopic images during the surgery .They were asked to conduct punctures into each calyx that might be visualized on fluoroscopy . After successful puncture , stone extraction was attempted through the sheath inserted via the needle .Successful puncture was calculated as reaching at least one calix . Overall success percentage was 88 % .No complications caused associated to the using of the US puncture tract projections . This method may assist urologists to achieve safe and easy percutaneous renal entry even if only few calices are obvious saw on fluoroscopy .",
        "rewrite_text": "The objective of this study was to investigate whether digital ultrasound (US) puncture tracts could enhance percutaneous renal access in patients with challenging anatomical features and difficult-to-visualize calices during fluoroscopy. A total of 50 consecutive patients underwent US-guided percutaneous nephrolithotomy using an in-room C-arm system for real-time imaging. The procedure was performed under either general anesthesia or conscious sedation. A pre-operative CT scan was conducted without the use of intravenous contrast media. Two urologists utilized OsiriX MD software to outline the kidney's contours and identify all visible calices. They then projected their observations onto the live fluoroscopic images during the procedure and were directed to perform punctures into each calyx visible on fluoroscopy. After successful punctures, stone extraction was attempted through the sheath introduced by the needle. Success was defined as achieving access to at least one calyx, and an overall success rate of 88% was reported. No complications related to the use of the US puncture tract projections were encountered. This technique may provide urologists with a safer and more efficient method for achieving percutaneous renal access, even when only a few calices are clearly visible on fluoroscopy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.495752858199363,
        "rewrite-fast-z-score": 0.1111111111111111
    },
    {
        "original_text": "We propose an efficient numerical scheme to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary pumping profiles and cavity losses, which is based on the combination of two different truncation schemes. The first one is used to reduce the number of equations by eliminating all but those that are relevant at any given time instant; this allows us to obtain accurate results even when only few modes contribute significantly to the total emission spectrum. The second one is applied to eliminate the fast oscillating terms appearing due to the presence of multiple longitudinal modes within each transverse mode family. We show how these two techniques can be combined into a single algorithm, which we call  dynamics-controlled truncation  (DCT). Finally, we demonstrate the accuracy and efficiency of our method by comparing it against other existing methods. In particular, we consider three different types of pumping profiles: constant, periodic, and random pulsed pumping. \nI. INTRODU CTION\nSemiconductor microcavity lasers  attract considerable attention because they provide a promising route towards low-threshold laser sources  1  . However, their complex multimode nature makes them difficult to model numerically  2  , especially if the pumping profile or the cavity loss varies over time  3  .\nIn order to overcome such difficulties, several authors have proposed various approaches  4  -  8  . For example, in Ref.  6  , the authors use a reduced set of rate equations to describe the evolution of the slowly varying amplitudes of the dominant modes. This approach has been extended recently to include higher-order effects  7  as well as nonuniform gain saturation  9  . Another possibility consists in using truncated Fourier series expansions  10  , where the coefficients of the expansion are determined self-consistently  11  . Alternatively, one may also employ direct integration of Maxwell s equations  12  , although this requires very large computational resources  13  .",
        "watermark_text": "We suggest an efficient numerical system to solve the nonlinear dynamics of semiconductor microcavity lasers with arbitrary flow profiles and cavity costs , which is based on the combination of two different truncation schemes . The first first is utilized to reduce the number of equations by removing all but those that are applicable at any certain time instant ; this enables us to obtain precise conclusions even when only few modes contribute greatly to the total emission spectrum .The second one is applied to eliminate the fast oscillating terms appearing caused to the presence of multiple longitudinal frequencies within each transverse mode family . We see how these two procedures can be merged into a single method , which we call dynamics - controlled truncation ( DCT ) .Finally , we prove the accuracy and efficiency of our technique by comparing it against other existing techniques . In particular , we study three different kinds of flow profiles : constant , continuous , and random pulsed pumping .I . INTRODU CTION Semiconductor microcavity lasers draw great popularity because they give a viable path towards non - threshold beam sources 1 . However , their complex multimode nature making them harder to model numerically 2 , particularly if the pumping profile or the cavity gain varies over time 3 .In try to overcome such problems , various published have proposed several methods 4 - 8 . For instance , in Ref .6 , the papers use a reduced series of rate coefficients to explain the evolution of the slowly varying amplitudes of the dominant modes . This method has been extended recently to consider higher - order effects 7 as well as nonuniform gain saturation 9 .Another possibility consists in utilizing truncated Fourier series expansions 10 , where the coefficients of the integration are chosen self - consistently 11 . Alternatively , one may even employ direct combining of Maxwell s coefficients 12 , although this demands very huge computational resources 13 .",
        "rewrite_text": "We propose an efficient numerical approach to address the nonlinear dynamics of semiconductor microcavity lasers with arbitrary flow profiles and cavity characteristics. This method combines two distinct truncation strategies. The first strategy reduces the number of equations by retaining only those relevant at any given instant, allowing for accurate conclusions even when only a few modes significantly influence the total emission spectrum. The second strategy eliminates rapidly oscillating terms that arise due to the presence of multiple longitudinal frequencies within each transverse mode family. We demonstrate how these two strategies can be integrated into a single method, which we refer to as dynamics-controlled truncation (DCT). Finally, we validate the accuracy and efficiency of our technique by comparing it to other established methods. Specifically, we investigate three different types of flow profiles: constant, continuous, and randomly pulsed pumping.\n\nI. INTRODUCTION  \nSemiconductor microcavity lasers are gaining significant attention as they represent a promising option for non-threshold beam sources. However, their complex multimode behavior presents challenges for numerical modeling, particularly when the pumping profile or cavity gain varies over time. To address these challenges, several methods have been proposed in the literature. For example, in Reference 6, the authors employ a reduced series of rate coefficients to describe the evolution of the slowly varying amplitudes of the dominant modes. This approach has been recently extended to consider higher-order effects and nonuniform gain saturation. Additionally, truncated Fourier series expansions have been explored, where the integration coefficients are selected self-consistently. Another approach involves the direct combination of Maxwell’s coefficients, although this method can be computationally intensive.",
        "ori-fast-z-score": -1.632993161855452,
        "water-fast-z-score": 8.110396013138502,
        "rewrite-fast-z-score": 1.5322617553657476
    },
    {
        "original_text": "The hydrogen bond is the strongest noncovalent interaction in water, and it plays an important role in determining its physical properties.  The strength of this bond can be measured by infrared spectroscopy or nuclear magnetic resonance (NMR) techniques.   Infrared spectroscopy measures how much energy is absorbed when vibrating molecules are exposed to infrared light.   NMR uses radio waves instead of light to measure the amount of energy needed to change the spin state of atoms within a molecule.   ... Figure 1 . Water has two different types of hydrogen bonds that form between neighboring molecules:  O-H⋯O hydrogen bonds occur along the edges of tetrahedral clusters; these bonds have relatively short distances but weak strengths.  H-O⋯H hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals; these bonds have longer distances than those found on cluster edges but stronger strengths.  These images were created using VMD software.",
        "watermark_text": "The hydrogen bond is the strongest noncovalent interaction in water , and it serves an important role in determining its physical properties . The strength of this bond can be determined by infrared spectroscopy or atomic magnetic resonance ( NMR ) techniques .Infrared spectroscopy measures how many energy is absorption when vibrating compounds are exposed to infrared light . NMR employs radio beams instead of light to measure the amount of power needed to shift the spin state of atoms within a molecule .. . . Figure 1 .Water has two different types of hydrogen bonds that form between neighboring compounds : O - [UNK] hydrogen bonds arise along the edges of tetrahedral clusters ; these bonds have relatively short distances but weak strengths . H - [UNK] hydrogen bonds connect adjacent tetrahedra into larger structures called ice crystals ; these bonds have longer distances than those found on cluster edges but stronger strengths .These photographs were created using VMD software .",
        "rewrite_text": "The hydrogen bond is the strongest type of noncovalent interaction found in water, playing a crucial role in defining its physical properties. The strength of hydrogen bonds can be assessed through techniques such as infrared spectroscopy or nuclear magnetic resonance (NMR). Infrared spectroscopy measures the energy absorbed when vibrating molecules are exposed to infrared light, while NMR utilizes radio waves to determine the energy required to change the spin state of atoms within a molecule. As illustrated in Figure 1, water exhibits two distinct types of hydrogen bonds between neighboring molecules. Oxygen-[UNK] hydrogen bonds form along the edges of tetrahedral clusters; these bonds are characterized by relatively short distances but weaker strengths. In contrast, hydrogen-[UNK] bonds connect adjacent tetrahedra to form larger structures known as ice crystals, possessing longer distances but stronger strengths. The images shown were generated using VMD software.",
        "ori-fast-z-score": -0.5488212999484517,
        "water-fast-z-score": 2.208630521496931,
        "rewrite-fast-z-score": -0.5488212999484517
    },
    {
        "original_text": "We study the geometry of harmonic maps into spheres with values in vector bundles over Riemann surfaces. We prove that if such a map is not constant then it has no critical points outside its singular set (Theorem 1). This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without changing its homotopy class (Corollary 3).\nIn particular we show how this result leads to new proofs for some results about instantons on 4-dimensional manifolds due to Donaldson  D1  ,  D2  . In fact our proof gives more information than those given by Donaldson s arguments since it allows us to control the behavior of the harmonic section near its singularities. Finally we give examples showing that these results are sharp. The main theorem of this article states that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be deformed to another harmonic section which is smooth everywhere except at isolated points where it has only simple poles.",
        "watermark_text": "We research the topology of harmonic maps into spheres with values in vector bundles over Riemann spheres . We prove that if such a mapping is not zero then it has no important points outside its singular set ( Theorem 1 ) .This implies that any harmonic section of an oriented rank 2 bundle over a closed surface can be deformed to a smooth one without altering its homotopy class ( Corollary 3 ) . In particular we tell how this consequence leads to novel proofs for some results about instantons on 4 - dimensional manifolds due to Donaldson D1 , D2 .In reality our proof provides more information than those given by Donaldson s statements since it allows us to affect the response of the chord section near its singularities . Finally we give examples demonstrating that these results are sharp .The main theorem of this page states that every non - constant harmonic section of an oriented 2 - plane bundle over a closed surface S can be deformed to another harmonic section which is continuous everywhere except at isolated points where it has only simple poles .",
        "rewrite_text": "We investigate the topology of harmonic maps from spheres into vector bundles over Riemann spheres. Our primary finding is that if such a mapping is non-zero, it does not exhibit significant points outside of its singular set (Theorem 1). This result leads to the conclusion that any harmonic section of an oriented rank 2 bundle over a closed surface can be continuously transformed into a smooth section without changing its homotopy class (Corollary 3). Notably, we explain how this implication provides new proofs for certain results regarding instantons on 4-dimensional manifolds, originally established by Donaldson in D1 and D2. Our proof offers more insight than Donaldson's, as it enables us to analyze the behavior of the chord section near its singularities. Additionally, we present examples that illustrate the sharpness of these results. The main theorem articulated here asserts that every non-constant harmonic section of an oriented 2-plane bundle over a closed surface S can be deformed into another harmonic section that is continuous everywhere, except at isolated points where it has only simple poles.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 2.2941573387056176
    },
    {
        "original_text": "We have investigated charge injection capability (CIC) of Suzaku X-ray Imaging Spectrometer (XIS). The CIC is an important feature for high energy resolution spectroscopy, which can be used to reduce the effect of pile-up in bright sources and improve the spectral quality at low energies. We performed detailed analysis on the performance of CIC by using Crab observations with different count rates. By comparing the results obtained with and without CIC we found that CIC improves the spectral fitting significantly especially below 1 keV where the pile-up fraction becomes large when the source flux increases. This improvement was confirmed also by analyzing simulated data generated by xisrmfgen and xissimarfgen. Finally, we applied this method to analyze the observation of Mrk 421 during its flare state. We successfully detected emission lines such as O VII Kα line and Fe XXV Kα line from the spectrum taken with CIC enabled.  These results show that CIC works well even under strong pile-up conditions.",
        "watermark_text": "We have explored charge injection capability ( CIC ) of Suzaku X - ray Imaging Spectrometer ( XIS ) . The CIC is an important feature for high energy resolution spectroscopy , which can be used to reduce the impact of pile - up in bright sources and improve the absorption quality at low frequencies .We conducted thorough analysis on the performance of CIC by using Crab observations with various count rates . By comparing the results derived with and without CIC we concluded that CIC improves the spectral fit substantially especially below 1 keV where the pile - up proportion gets large when the source flux rises .This improvement was confirmed also by analyzing simulated evidence generated by xisrmfgen and xissimarfgen . Finally , we applied this method to analyze the observation of Mrk 421 during its flare state .We correctly discovered radiation groups such as O VII Kα line and Fe XXV Kα line from the spectrum collected with CIC enabled . These data demonstrate that CIC works well even under strong pile - up circumstances .",
        "rewrite_text": "We investigated the charge injection capability (CIC) of the Suzaku X-ray Imaging Spectrometer (XIS). CIC is a crucial feature for achieving high energy resolution in spectroscopy, as it helps mitigate the effects of pile-up in bright sources and enhances absorption quality at lower frequencies. Our analysis involved observations of the Crab Nebula at various count rates to evaluate CIC's performance. By comparing the results obtained with and without CIC, we found that CIC significantly improves spectral fitting, particularly below 1 keV, where pile-up becomes more pronounced with increasing source flux. This enhancement was further validated through analyses of simulated data generated by xisrmfgen and xissimarfgen. Additionally, we applied this technique to the observation of Mrk 421 during its flare state, successfully identifying spectral features such as the O VII Kα line and the Fe XXV Kα line in the data collected with CIC enabled. These findings illustrate that CIC functions effectively, even in scenarios with substantial pile-up.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 5.9648090806346055,
        "rewrite-fast-z-score": -0.1203858530857692
    },
    {
        "original_text": "The Long Baseline Neutrino Experiment (LBNE) is an international collaboration to build and operate a new accelerator-based neutrino beam facility in the United States, with a proposed location at Fermilab near Chicago, Illinois.  The LBNE will use a novel design for its proton driver that combines two existing technologies - Cyclotrons and Recirculating Linacs - into one compact system.  This report describes the status of this project as of September 2009.   It also presents some preliminary results on the physics potential of the LBNE program. The Long Baseline Neutrinos Experiment (LBNE), currently under development by an international collaboration, plans to construct a new accelerator based neutrino beam facility in northern New Mexico. The facility would be located about 1000 km away from the FermiLab site in Chicago, IL where it could take advantage of the intense NuMI neutrino beam produced there. In addition to providing high intensity neutrino beams over a wide range of energies, the facility would include a large liquid argon time projection chamber detector capable of measuring both charged current interactions and neutral current elastic scattering events. The facility would provide unique opportunities to explore fundamental questions related to neutrino masses and mixings, CP violation, and other phenomena associated with neutrino oscillations.",
        "watermark_text": "The Long Baseline Neutrino Experiment ( LBNE ) is an global project to build and install a new accelerator - based neutrino laser facility in the United States , with a planned installation at Fermilab near Chicago , Illinois . The LBNE will use a innovative design for its proton driver that combines two existing innovations - Cyclotrons and Recirculating Linacs - into one compact device .This report explains the status of this project as of September 2009 . It additionally offers some preliminary results on the physics potential of the LBNE program .The Long Baseline Neutrinos Experiment ( LBNE ) , currently under development by an global collaboration , hopes to build a new accelerator based neutrino laser facility in northern New Mexico . The plant would be located about 1000 kilometers away from the FermiLab campus in Chicago , IL where it could gain advantage of the strong NuMI neutrino light produced there .In addition to offering high intensity neutrino beams over a broad variety of energies , the facility would include a large liquid argon time projection chamber detector capable of monitoring both charged current interactions and neutral current elastic scattering events . The facility would offer innovative opportunities to examine fundamental questions related to neutrino masses and mixings , CP violation , and other processes associated with neutrino oscillations .",
        "rewrite_text": "The Long Baseline Neutrino Experiment (LBNE) is a global initiative aimed at developing and establishing a cutting-edge accelerator-based neutrino facility in the United States, specifically planned for installation at Fermilab, located near Chicago, Illinois. The LBNE will feature an innovative design for its proton driver that integrates two existing technologies—Cyclotrons and Recirculating Linacs—into a single, compact device. This report provides an update on the project's progress as of September 2009 and presents preliminary findings on the scientific potential of the LBNE program. Additionally, the LBNE, currently being developed by an international collaboration, envisions creating a new accelerator-based neutrino facility in northern New Mexico. This site will be approximately 1,000 kilometers from the Fermilab campus in Chicago, allowing it to utilize the powerful neutrino flux generated there at NuMI. The facility is designed to deliver high-intensity neutrino beams across a diverse range of energies and will incorporate a large liquid argon time projection chamber detector, capable of monitoring both charged-current interactions and neutral-current elastic scattering events. This innovative facility will provide unique opportunities to investigate fundamental questions related to neutrino masses and mixings, CP violation, and various processes linked to neutrino oscillations.",
        "ori-fast-z-score": -1.5554275420956378,
        "water-fast-z-score": 4.744537732790449,
        "rewrite-fast-z-score": -0.20851441405707477
    },
    {
        "original_text": "We present Spitzer Space Telescope observations at 24, 70, and 160 microns for 12 members of the nearby (140 pc) Taurus star-forming region with ages between 1 Myr to 10 Myr. We find that all sources show excess emission above photospheric levels indicative of circumstellar material surrounding each star. The majority of these objects are surrounded by optically thick disks which can be fit well using single temperature blackbody models. However, we also identify three systems where the disk is likely to have an inner hole or gap; TW Hya, DM Tau, and GM Aur. In addition, we detect two transitional disks around V4046 Sgr and Sz 91. These results suggest that most stars in our sample retain their primordial disks up until at least 5 Myr after formation. Finally, we use mid-infrared spectroscopy obtained with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks.",
        "watermark_text": "We present Spitzer Space Telescope observations at 24 , 70 , and 160 microns for 12 members of the nearby ( 140 pc ) Taurus star - creating area with periods between 1 Myr to 10 Myr . We see that all sources show extra emitted above photospheric concentrations indicative of circumstellar material surrounding each star .The majority of these objects are surrounded by optically dense disks which can be fit well using single temperature blackbody studies . However , we also identify three components where the disk is expected to have an inner cavity or gap ; TW Hya , DM Tau , and GM Aur .In addition , we find two transitional disks around V4046 Sgr and Sz 91 . These data suggest that most stars in our sample maintain their primordial disks up until at least 5 Myr after formed .Finally , we utilize mid - infrared spectroscopy acquired with the IRS instrument onboard Spitzer to study the composition of the dust grains in the disks .",
        "rewrite_text": "We present observations from the Spitzer Space Telescope at 24, 70, and 160 microns for 12 members of the nearby Taurus star-forming region, located about 140 parsecs away, with ages ranging from 1 to 10 million years. Our findings indicate that all sources exhibit excess emission above photospheric levels, suggesting the presence of circumstellar material around each star. Most of these stars are enveloped by optically thick disks, which can be effectively modeled using single-temperature blackbody fits. However, we also identify three stars—TW Hya, DM Tau, and GM Aur—that appear to have disks with inner cavities or gaps. Additionally, we discover two transitional disks around V4046 Sgr and Sz 91. These observations imply that most stars in our sample retain their primordial disks well into at least 5 million years after their formation. Lastly, we employ mid-infrared spectroscopy obtained with the IRS instrument aboard Spitzer to analyze the composition of the dust grains within these disks.",
        "ori-fast-z-score": 0.39056673294247163,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "Circinus X-1 is an X-ray binary system with a neutron star and its companion, which has been observed in many wavelengths ranging from radio to gamma-ray bands. The source shows periodic dipping activity at X-ray energies that are caused by obscuration of the central X-ray emitting region due to matter falling onto the accretion disk around the compact object. In this work we present results obtained using data collected during two different observational campaigns carried out with Suzaku satellite (from 2005 to 2007) and INTEGRAL/IBIS telescope (from 2003 to 2009). We have analyzed the spectral properties of the source for both observations separately as well as combined together. Our analysis reveals that the spectrum can be described by a combination of several components such as: blackbody emission from the neutron star surface; Comptonized component produced by hot plasma surrounding the neutron star; reflection component originating from reprocessing of hard radiation emitted by the central X-ray source into softer photons; iron line feature arising from fluorescence of cold material located close to the neutron star.",
        "watermark_text": "Circinus X - 1 is an X - ray binary system with a neutron star and its companion , which has been observed in multiple wavelengths ranging from radio to gamma - ray bands . The source shows irregular dipping activity at X - ray energies that are created by obscuration of the main X - ray emitting area owing to matter falling onto the accretion disk around the compact body .In this project we present results collected using data taken during two different observational campaigns carried out with Suzaku spacecraft ( from 2005 to 2007 ) and INTEGRAL / IBIS telescope ( from 2003 to 2009 ) . We have analyzed the spectral properties of the origin for both surveys independently as well as combined together .Our study reveals that the spectrum can be described by a combination of several parts such as : blackbody emission from the neutron star surface ; Comptonized component produced by hot plasma surrounding the neutron star ; reflection portion arising from reprocessing of hard radiation emitted by the main X - ray source into stronger photons ; iron line feature arising from fluorescence of cold matter located close to the neutron star .",
        "rewrite_text": "Circinus X-1 is an X-ray binary system consisting of a neutron star and its companion, which has been studied across a wide range of wavelengths, from radio to gamma rays. This source exhibits irregular dipping activity in X-ray energies due to the obscuration of the primary X-ray emission area, caused by matter falling onto the accretion disk surrounding the compact object. In this project, we present findings based on data collected during two separate observational campaigns: one conducted with the Suzaku spacecraft between 2005 and 2007, and another with the INTEGRAL/IBIS telescope from 2003 to 2009. We analyzed the spectral properties from both surveys independently and in combination. Our research indicates that the spectrum can be characterized by several components: blackbody emission from the surface of the neutron star, a Comptonized component produced by the hot plasma around the neutron star, a reflection component from the reprocessing of hard radiation emitted by the main X-ray source into higher-energy photons, and an iron line feature resulting from the fluorescence of cold matter located near the neutron star.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 6.041987916036252,
        "rewrite-fast-z-score": 2.27776980709589
    },
    {
        "original_text": "We have read with interest the recent preprint  1  . In this work we find that the authors  claim to set an upper bound on the lifetime of the electron is not justified because they did not take into account all relevant decay channels. \n \n We would like to comment briefly on their treatment of backgrounds as well as their choice of cuts used to select events. The main source of background comes from radiative Bhabha scattering e+e-→e+e-γ which has been studied extensively at LEP2  2  , where it was found to be negligible compared to other sources such as two-photon processes or four-fermion final states (e.g., W pair production). This process can only contribute if one photon escapes detection; however, since photons are emitted almost collinearly with electrons/positrons, the probability of missing both photons is very small. Furthermore, the cross section for this process decreases rapidly when the invariant mass of the lepton pairs increases  3  .\n \nThe authors also use a cut on the total energy of the event, Evis>10 GeV, which removes most of these events. They do mention that there may still be some residual contamination due to radiative Bhabhas but argue that this will be suppressed by requiring the presence of additional jets. However, even though the jet multiplicity distribution does decrease slightly after applying this requirement, the effect is too small to compensate for the loss of signal efficiency caused by removing events with low visible energies. \n \nIn addition, the authors state that the contribution from radiative Bhabhas should be included in the systematic uncertainty estimate. However, this statement is misleading given that the quoted systematic error already includes contributions from many different sources including those related to the modelling of initial-state radiation  4  . \n\n\nFinally, we note that the authors present results obtained using Monte Carlo simulations performed with PYTHIA 6  5  . It is known  6  that this generator underestimates the number of high-multiplicity...",
        "watermark_text": "We have read with importance the recent preprint 1 . In this research we find that the writers claim to setting an upper bound on the life of the electron is not justified because they did not take into consideration all relevant degradation channels .We would like to comment briefly on their handling of backgrounds as also as their choosing of cuts needed to select events . The main origin of background comes from radiative Bhabha scattering e + e - →e + e - γ which has been studied thoroughly at LEP2 2 , where it was shown to be negligible compared to other sources such as two - photon processes or four - fermion final states ( e . g . , W pair production ) .This process can only contribute if one photon escapes detection ; however , since photons are emitted virtually collinearly with electrons / positrons , the probability of missing both photons is very small . Furthermore , the cross area for this process reduces rapidly when the invariant mass of the lepton pairs increases 3 .The authors also use a cutting on the total energy of the event , Evis > 10 GeV , which eliminate most of these events . They do mention that there may still be some remnant contamination owing to radiative Bhabhas but suggest that this will be suppressed by requiring the presence of added jets .However , even though the jet multiplicity distribution does decrease slightly after applying this requirement , the effect is too small to compensate for the losing of signal efficiency created by removing events with lowered visible frequencies . In addition , the writers state that the impact from radiative Bhabhas should be included in the systematic uncertainty estimate .However , this statement is misleading given that the quoted systematic error also contains contributions from many various sources including those related to the modelling of initial - state radiation 4 . Finally , we note that the writers present results acquired using Monte Carlo simulations conducted with PYTHIA 6 5 .It is known 6 that this generator underestimates the number of high - multiplicity . . .",
        "rewrite_text": "We have placed significant importance on the recent preprint. In this study, we find that the authors' assertion of establishing an upper limit on the electron's lifespan is unfounded, as they failed to consider all pertinent degradation pathways. We would like to briefly address their treatment of background events, as well as their selection criteria for event cuts. The primary source of background arises from radiative Bhabha scattering \\( e^+ e^- \\rightarrow e^+ e^- \\gamma \\), which has been extensively investigated at LEP2, where it was demonstrated to be negligible in comparison to other sources, such as two-photon processes or four-fermion final states (e.g., W pair production). This background can only contribute if one photon goes undetected; however, due to the near-collinear emission of photons alongside electrons and positrons, the likelihood of both photons being missed is minimal. Moreover, the cross-section for this process decreases rapidly as the invariant mass of the lepton pairs increases. The authors also implement a cut on total event energy, \\( E_{\\text{vis}} > 10 \\) GeV, which effectively eliminates most of these background events. While they acknowledge that some residual contamination from radiative Bhabha scattering may persist, they propose that this will be mitigated by requiring the inclusion of additional jets. However, although the distribution of jet multiplicity does see a slight decrease post-requirement, this effect is insufficient to offset the loss of signal efficiency resulting from discarding events with lower visible energies. Furthermore, the authors claim that the effects of radiative Bhabha scattering should be considered in the systematic uncertainty assessment. This assertion is misleading, as the stated systematic error also encompasses contributions from various sources, including those related to initial-state radiation modeling. Lastly, we observe that the authors present results derived from Monte Carlo simulations performed using PYTHIA. It is well established that this generator underestimates events with high multiplicity.",
        "ori-fast-z-score": 0.9863939238321437,
        "water-fast-z-score": 7.127317708515653,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results obtained in digitizing and measuring astrometrically about 1000 photographic plates taken by the Carte du Ciel project at the Observatorio Astronómico Nacional (OAN), San Pedro Mártir Observatory (SPMO) between 1950 and 1960. The OAN/SPMO is located on Mount Wilson, California, USA. We used a high resolution digital camera to scan these plates and then measured their positions relative to reference stars observed simultaneously on each plate. This allowed us to obtain accurate absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 square degree centered around RA = 10 h 30 m , Dec = +60 deg . These data are available online through our web site http://obswww.unige.ch/~fuchs/CDCCatastro/Movies/movie1.html .\nThe accuracy achieved in this work is comparable or better than that reported previously for similar projects based on scanning film material. \n\n\nThis research was supported by NSF grant AST-0307481.",
        "watermark_text": "We present the results derived in digitizing and observing astrometrically about 1000 photo plates taken by the Carte du Ciel programme at the Observatorio Astronómico Nacional ( OAN ) , San Pedro Mártir Observatory ( SPMO ) between 1950 and 1960 . The OAN / SPMO is situated on Mount Wilson , California , USA .We utilized a high resolution digital camera to scan these plates and then calculated their movements relative to reference stars observed concurrently on each plate . This able us to obtain precise absolute proper motions for more than 100 000 stars down to V = 16 mag over an area of about 1 sq degree centered around RA = 10 h 30 m , Dec = + 60 deg .These data are available available through our online site www : / / obswww . unige . ch / ~ fuchs / CDCCatastro / Movies / movie1 . html . The accuracy achieved in this project is identical or better than that obtained previously for related works based on scan film material .This research was supported by NSF grant AST-0307481.",
        "rewrite_text": "We present the results of digitizing and astrometrically analyzing around 1,000 photographic plates from the Carte du Ciel programme at the Observatorio Astronómico Nacional (OAN) and the San Pedro Mártir Observatory (SPMO), conducted between 1950 and 1960. Located on Mount Wilson in California, USA, the OAN/SPMO utilized a high-resolution digital camera to scan these plates. We then calculated the movements of the stars in relation to reference stars observed on each plate. This process allowed us to obtain precise absolute proper motions for over 100,000 stars down to magnitude V = 16 across an area of approximately 1 square degree, centered at right ascension 10h 30m and declination +60°. Our data is accessible on our website: www.obswww.unige.ch/~fuchs/CDCCatastro/Movies/movie1.html. The accuracy achieved in this project matches or surpasses that of previous studies based on scanned film material. This research was supported by NSF grant AST-0307481.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 5.163977794943222,
        "rewrite-fast-z-score": 2.213211486674006
    },
    {
        "original_text": "We study the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two parallel metal plates separated by a dielectric layer. We show that, depending on the parameters of the system (the thickness of the dielectric layer, the density of electrons), different types of nonlinear waves can be excited. In particular, we find that for certain values of these parameters solitary wave solutions exist which are similar to those found earlier in 1D systems. \n \n The existence of such solitary waves is confirmed experimentally using time-resolved optical reflectivity measurements performed at room temperature on samples consisting of GaAs/AlGaAs quantum wells grown by molecular beam epitaxy. These experiments reveal the presence of bright solitary waves propagating along the direction perpendicular to the applied electric field. Their propagation velocities agree well with theoretical predictions based on numerical simulations of the underlying equations. \nThe results presented here may have important applications in semiconductor devices where it has been shown recently that the generation of solitary waves leads to improved performance characteristics.",
        "watermark_text": "We test the electrical excitation of nonlinear waves in a 2D channel with an applied voltage bias between two connected metal plates connected by a dielectric layer . We see that , depending on the variables of the device ( the length of the dielectric layer , the density of atoms ) , different kinds of nonlinear waves can be excited .In particular , we find that for particular values of these parameters solitary wave systems occur which are comparable to those shown previous in 1D solutions . The existence of such solitary waves is demonstrated experimentally using period - resolved laser reflectivity surveys performed at room temperature on samples consisting of GaAs / AlGaAs quantum wells produced by molecular beam epitaxy .These studies reveal the presence of bright solitary waves propagating along the direction perpendicular to the introduced electric field . Their propagation velocities agree well with theoretical estimates based on numerical simulations of the underlying equations .The results presented here possibly have important use in semiconductor devices where it has been shown lately that the generation of solitary waves gives to improved performance qualities .",
        "rewrite_text": "We investigate the electrical excitation of nonlinear waves in a two-dimensional channel, where a voltage bias is applied across two metal plates separated by a dielectric layer. Our experiments reveal that various types of nonlinear waves can be generated, influenced by device parameters such as the length of the dielectric layer and atomic density. Notably, certain values of these parameters lead to the formation of solitary wave systems that are analogous to those previously observed in one-dimensional solutions. The existence of these solitary waves is confirmed through periodic laser reflectivity measurements conducted at room temperature on GaAs/AlGaAs quantum well samples fabricated using molecular beam epitaxy. These investigations demonstrate the occurrence of bright solitary waves traveling perpendicular to the applied electric field, with their propagation velocities closely matching theoretical predictions derived from numerical simulations of the governing equations. The findings reported here may have significant implications for semiconductor devices, as recent studies indicate that the generation of solitary waves enhances performance characteristics.",
        "ori-fast-z-score": -0.10482848367219183,
        "water-fast-z-score": 5.692099788303083,
        "rewrite-fast-z-score": 0.953998092005724
    },
    {
        "original_text": "The following is an excerpt from René Decartes  Discourse on Method, Book II (1637): \nI have found by experiment that if I take any right line and divide it into equal parts in such manner as to make one part double another, then the square which shall be formed with these two parts will be equal to the rectangle whose sides are those lines; so that this figure may serve me instead of the square root. \n \n If we suppose that the diameter AB of a circle equals 2, and that the radius AC equals 1, then the area of the circle will be equal to 4 times the square of the radius, or 8 times the square of half its diameter, because the square of twice the diameter is equal to four squares of the same side. The area of the circle therefore being known, it remains only to find how many degrees there are contained within it; but since the circumference of a circle is always equal to three times the diameter, it follows that the number of degrees contained therein must also be 3 times the number of degrees contained between the diameter and the center of the circle.",
        "watermark_text": "The following is an excerpt from René Decartes Discourse on Method , Book II ( 1637 ) : I have discovered by experiment that if I taking any right line and divide it into equal parts in such manner as to make one part twice another , then the square which shall be formed with these two parts will be equal to the rectangle whose sides are those lines ; so that this figure might serve me instead of the square root . If we suppose that the radius AB of a ring equals 2 , and that the radius AC equals 1 , then the area of the circle will be equal to 4 times the square of the radius , or 8 times the square of half its radius , because the square of twice the radius is equal to four squares of the same side .The area of the circle therefore being known , it remains only to find how many degrees there are contained within it ; but since the circumference of a ring is usually equal to three times the radius , it appears that the total of degrees contained therein must additionally be 3 times the number of degrees contained between the radius and the center of the circle .",
        "rewrite_text": "Here is a rewritten version of the paragraph:\n\nThe following excerpt is taken from René Descartes' \"Discourse on Method,\" Book II (1637): Through experimentation, I have determined that if I take any straight line and divide it into equal segments such that one segment is twice the size of another, then the square created from these two segments will be equal to the rectangle formed by those lines. This figure can therefore act as a substitute for the square root. Assuming the radius AB of a ring is 2 and the radius AC is 1, the area of the circle will equal 4 times the square of the radius, or 8 times the square of half its radius, since the square of twice the radius corresponds to four squares of the same side. With the area of the circle established, the next step is to determine the total number of degrees contained within it. Given that the circumference of a ring typically equals three times the radius, it follows that the total degrees within must also be three times the number of degrees measured between the radius and the center of the circle.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": 1.1785113019775793
    },
    {
        "original_text": "We present an analysis of star-formation activity associated with low radio luminosity active galactic nuclei (AGNs) using data from the Sloan DigitalSky Survey (SDSS). We find that these objects are typically hosted by massive galaxies, and have high specific star formation rates compared to inactive galaxies at similar redshifts. The majority of our sample is found to be obscured by dusty torii, as indicated by their optical colors and infrared emission. These results suggest that there may exist two populations of AGN: one which hosts significant amounts of star formation, and another where no such activity is observed. This work was supported by NASA grant NNG05GJ40G. Active Galactic Nuclei (AGNs), powered by supermassive black holes accreting matter from surrounding gas clouds, are known to produce copious quantities of radiation across all wavelengths. However, it has been unclear whether this energy output also leads to enhanced levels of star formation within host galaxies. In order to investigate this question we use data from the SloanDigital Sky Survey (SDSS; York et al., 2000) , specifically targeting sources classified as narrow-line Seyfert 1 s (NLS1s) based on their optical spectra. NLS1s represent a subclass of AGNs whose properties differ significantly from those of more typical broad line quasars (BLQs; Osterbrock & Pogge 1985) . They tend to reside in lower mass galaxies than BLQSOs, and exhibit higher Eddington ratios (Boller et al., 1996; Grupe, Thomas, & Leighly 1999; Mathur 2000; Komossa et al., 2006a ,b Gallo 2007 .",
        "watermark_text": "We present an assessment of galaxy - formation activity related with poor radio luminosity active galactic nuclei ( AGNs ) using data from the Sloan DigitalSky Survey ( SDSS ) . We see that these objects are typically held by massive galaxies , and have high specific star formation rates relative to dormant clusters at comparable redshifts .The majority of our sample is found to be obscured by dusty torii , as indicated by their optical colors and infrared absorption . These data suggest that there may contain two communities of AGN : one which contains substantial deposits of galaxy formation , and another where no such activity is observed .This project was supported by NASA grant NNG05GJ40G . Active Galactic Nuclei ( AGNs ) , driven by supermassive black holes accreting matter from surrounding gas clouds , are known to produce copious quantities of radiation across all wavelengths .However , it has been uncertain whether this power output additionally results to intensified levels of galaxy formation within host galaxies . In order to examine this question we utilize results from the SloanDigital Sky Survey ( SDSS ; York et al . , 2000 ) , explicitly targeting components classified as shallow - range Seyfert 1 s ( NLS1s ) based on their light spectra .NLS1s represent a subclass of AGNs whose characteristics vary significantly from those of more typical broad line quasars ( BLQs ; Osterbrock & Pogge 1985 ) . They tend to live in smaller density clusters than BLQSOs , and feature greater Eddington ratios ( Boller et al . , 1996 ; Grupe , Thomas , & Leighly 1999 ; Mathur 2000 ; Komossa et al . , 2006a , b Gallo 2007 .",
        "rewrite_text": "We provide an evaluation of galaxy formation activity associated with low radio luminosity active galactic nuclei (AGNs) using data from the Sloan Digital Sky Survey (SDSS). Our findings indicate that these AGNs are generally hosted by massive galaxies and exhibit high specific star formation rates compared to dormant clusters at similar redshifts. The majority of the objects in our sample appear to be obscured by dusty tori, as suggested by their optical colors and infrared absorption characteristics. This evidence points to the existence of two distinct communities of AGN: one that shows significant galaxy formation activity and another where such activity is absent. This research received funding from NASA under grant NNG05GJ40G. Active Galactic Nuclei (AGNs), which are powered by supermassive black holes accreting matter from surrounding gas clouds, are known for emitting large amounts of radiation across all wavelengths. However, it remains uncertain whether this energy output leads to increased galaxy formation within the host galaxies. To investigate this issue, we analyze data from the Sloan Digital Sky Survey (SDSS; York et al., 2000), specifically focusing on components classified as narrow-line Seyfert 1s (NLS1s) based on their spectral characteristics. NLS1s represent a subclass of AGNs that differ significantly from more typical broad-line quasars (BLQs; Osterbrock & Pogge, 1985). They tend to exist in lower density clusters than BLQSOs and exhibit higher Eddington ratios (Boller et al., 1996; Grupe, Thomas, & Leighly, 1999; Mathur, 2000; Komossa et al., 2006a, b; Gallo, 2007).",
        "ori-fast-z-score": -0.5827715174143585,
        "water-fast-z-score": 7.057176370033344,
        "rewrite-fast-z-score": 0.47891314261057566
    },
    {
        "original_text": "The Galactic center (GC) is one of the most interesting regions in our Galaxy, but it has been difficult to study because of its proximity to the Sun. The GC contains many compact radio sources that are thought to be associated with young pulsars or magnetars. In this thesis we present results on two surveys at 1.4 GHz using the Australia Telescope Compact Array (ATCA). We have observed the central region of the Galaxy for about 100 hours over three epochs between 2005-2007. Our first survey covers an area of 2 degrees centered around Sgr A*. This survey was designed to search for new compact radio sources near the GC as well as to investigate the nature of the diffuse emission surrounding Sgr A*. Our second survey covered a larger area of 4 degrees centered around the GC. This survey was aimed at studying the distribution of molecular gas towards the GC by observing the 12 CO(J=1-0), 13 CO(J=1-0) and C 18 O(J=1-0) lines simultaneously.",
        "watermark_text": "The Galactic center ( GC ) is one of the most exciting regions in our Galaxy , but it has been difficult to study because of its proximity to the Sun . The GC features many compact radio sources that are said to be identified with young pulsars or magnetars .In this dissertation we present results on two studies at 1 . 4 GHz use the Australia Telescope Compact Array ( ATCA ) . We have discovered the central region of the Galaxy for about 100 hours over three epochs between 2005 - 2007 .Our first experiment encompasses an area of 2 degrees centered around Sgr A * . This study was built to search for new compact radio sources near the GC as well as to examine the nature of the diffuse emission surrounding Sgr A * .Our second survey included a greater area of 4 degrees centered around the GC . This study was aiming at studying the distribution of molecular gas towards the GC by observing the 12 CO ( J = 1 - 0 ) , 13 CO ( J = 1 - 0 ) and C 18 O ( J = 1 - 0 ) tracks jointly .",
        "rewrite_text": "The Galactic Center (GC) is one of the most intriguing areas in our Galaxy, yet it has posed challenges for study due to its closeness to the Sun. This region is home to numerous compact radio sources believed to be associated with young pulsars or magnetars. In this dissertation, we present findings from two studies conducted at 1.4 GHz using the Australia Telescope Compact Array (ATCA). Over the course of around 100 hours across three epochs from 2005 to 2007, we explored the central region of the Galaxy. Our first experiment focused on a 2-degree area centered on Sgr A*, aimed at discovering new compact radio sources near the GC and investigating the nature of the diffuse emissions surrounding Sgr A*. The second survey extended over a larger area of 4 degrees centered on the GC and sought to examine the distribution of molecular gas in this vicinity by jointly observing the 12CO (J=1-0), 13CO (J=1-0), and C18O (J=1-0) transitions.",
        "ori-fast-z-score": -0.13245323570650439,
        "water-fast-z-score": 5.165676192553671,
        "rewrite-fast-z-score": 0.39056673294247163
    },
    {
        "original_text": "The future evolution of the local large scale structure is investigated by using N-body simulations with different cosmological models, including dark energy (DE) and cold dark matter (CDM). The results show that in the Λ-CDM model without DE, the growth rate of large-scale structures decreases rapidly at late times due to the effect of CDM. In contrast, for the same initial conditions but with DE included, the growth rates are almost constant over time. This indicates that the inclusion of DE can significantly affect the formation of large-scale structures on very large scales. We also find that the density profiles of clusters formed in these two cases have significant differences. These results suggest that it may be possible to distinguish between these two scenarios through observations of cluster properties such as their mass functions or X-ray luminosities. Keywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology",
        "watermark_text": "The future development of the local big scale system is investigated by using N - bodies simulations with various cosmological models , notably dark energy ( DE ) and cold bright matter ( CDM ) . The results show that in the Λ - CDM theory without DE , the development frequency of large - scale structures decreases quickly at late times due to the impact of CDM .In contrast , for the same original conditions but with DE included , the development rates are almost steady over time . This implies that the introduction of DE can significantly affect the formation of large - scale structures on very huge scales .We additionally find that the density characteristics of clusters formed in these two situations have considerable variations . These data suggest that it could be possible to distinguish between these two scenarios through observations of cluster structures such as their mass distributions or X - ray luminosities .Keywords : Cold black material ; Dark energy ; Growth factor ; Clustering statistics ; Density profile ; Cosmology",
        "rewrite_text": "The future evolution of large-scale local structures is explored through N-body simulations utilizing various cosmological models, particularly those involving dark energy (DE) and cold dark matter (CDM). The results indicate that, under the Λ-CDM framework without DE, the growth rate of large-scale structures diminishes rapidly in the later epochs due to the influence of CDM. In contrast, when DE is included under the same initial conditions, the growth rates remain relatively constant over time. This suggests that the presence of DE plays a crucial role in the formation of large-scale structures on vast scales. Moreover, we observe significant differences in the density profiles of clusters formed under these two scenarios. These findings imply that it may be possible to differentiate between these two cosmological models through observations of cluster characteristics, such as their mass distributions or X-ray luminosities. \n\nKeywords: Cold dark matter; Dark energy; Growth factor; Clustering statistics; Density profile; Cosmology.",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 6.267831705280087,
        "rewrite-fast-z-score": -0.11396057645963795
    },
    {
        "original_text": "We consider the Standard Model (SM) in 5 dimensions, where one extra dimension is compactified to an orbifold S 1 /Z 2 . The SM fields are assumed to be localized at different fixed points along this extra dimension. We show that such models can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings. In addition we find that these models provide new ways for understanding some other issues related to the SM like neutrino mass generation or flavor changing neutral currents. Finally we discuss how our results could be tested experimentally. Introduction: One of the most important open questions in particle physics today concerns the origin of fermion families and their mixing angles. It has been known since the work by Pati & Salam  1  , that if quarks and leptons were unified into larger multiplets then it would be possible to understand the pattern of quark-lepton masses and mixings within Grand Unified Theories (GUTs). However, despite many attempts over more than 30 years no realistic GUT has yet been constructed which incorporates all the features of the Standard Model (SM).\nIn recent years another possibility was suggested  2  -  4  : If the SM fields live in higher dimensional space-time, they may have Kaluza-Klein excitations corresponding to additional states with masses of order 1/R, where R denotes the size of the extra dimensions. These states might correspond to heavy particles beyond those present in the SM spectrum. This idea leads to interesting phenomenological consequences  5  .\nThe simplest way to realize this scenario is to assume that only gravity propagates in the bulk while the SM fields are confined to a four-dimensional  brane   6  . Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by: \nwhere M P l = 1/ √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the number of extra spatial dimensions accessible to field i. For distances smaller than about 0.1 mm deviations from the inverse square law predicted by general relativity will become",
        "watermark_text": "We consider the Standard Model ( SM ) in 5 dimensions , where one extra dimension is compactified to an orbifold S 1 / Z 2 . The SM fields are expected to be localized at different fixed points along this extra dimension .We see that such theories can naturally explain why there should exist three generations of fermions and gauge bosons with their observed masses and mixings . In addition we find that these models bring fresh ways for explaining some other issues related to the SM like neutrino mass development or color shifting neutral currents .Finally we talk how our findings may be evaluated experimentally . Introduction : One of the most important open questions in particle science today issues the origin of fermion families and their mixing angles .It has been known since the paper by Pati & Salam 1 , that if quarks and leptons were organized into larger multiplets then it would be possible to comprehend the trend of quark - lepton masses and mixings within Grand Unified Theories ( GUTs ) . However , despite many efforts over more than 30 centuries no realistic GUT has already been constructed which includes all the details of the Standard Model ( SM ) .In recent work another possibility was suggested 2 - 4 : If the SM fields reside in larger dimensional space - time , they may have Kaluza - Klein excitations corresponding to extra states with masses of order 1 / R , where R denotes the height of the extra dimensions . These states could belong to heavy ions beyond those present in the SM spectrum .This idea results to useful phenomenological consequences 5 . The shortest way to realize this situation is to assume that only gravitational propagates in the bulk while the SM fields are localized to a four - dimensional brane 6 .Such theories lead to corrections to the Newtonian potential between two test masses m 1 and m 2 separated by distance r given by : where M P l = 1 / √ 8πG N ≈ 10 19 GeV is the reduced Planck scale and n i counts the quantity of added spatial dimensions accessible to field i . For distances smaller than about 0 . 1 mm deviations from the inverse square law predicted by particular relativity will become",
        "rewrite_text": "We explore a five-dimensional version of the Standard Model (SM), where one additional dimension is compactified into an orbifold S¹/Z₂. In this framework, the SM fields are localized at various fixed points along the extra dimension. This setup elegantly accounts for the existence of three generations of fermions and gauge bosons, as well as their observed masses and mixing patterns. Furthermore, our analysis reveals new approaches to address other SM-related challenges, such as the generation of neutrino masses and the phenomenon of color-shifting neutral currents. We also discuss how these theoretical insights could be tested through experimental means. \n\nIntroduction: A critical unresolved issue in particle physics is understanding the origin of fermion families and their mixing angles. Notably, Pati and Salam proposed that if quarks and leptons were organized into larger multiplets, it might be feasible to explain the patterns of quark-lepton masses and mixings within the framework of Grand Unified Theories (GUTs). However, despite extensive efforts spanning over three decades, a comprehensive GUT incorporating all aspects of the Standard Model has yet to be developed. Recently, another avenue of exploration emerged: if the SM fields exist in a higher-dimensional spacetime, they may exhibit Kaluza-Klein excitations, leading to additional states with masses on the order of 1/R, where R represents the scale of the extra dimension. These states could introduce heavier particles beyond those currently found in the SM spectrum, yielding intriguing phenomenological implications. The simplest implementation of this scenario postulates that only gravitational interactions propagate in the bulk, while the SM fields are confined to a four-dimensional brane. Such models predict modifications to the Newtonian potential between two test masses m₁ and m₂ separated by a distance r, described by the expression where Mₗ = 1/√(8πGₙ) ≈ 10¹⁹ GeV is the reduced Planck scale, and nₗ counts the number of additional spatial dimensions available to field i. For distances smaller than approximately 0.1 mm, deviations from the inverse square law predicted by special relativity will manifest.",
        "ori-fast-z-score": 2.685380346549405,
        "water-fast-z-score": 9.115037909077289,
        "rewrite-fast-z-score": 3.452378733412503
    },
    {
        "original_text": "The problem is to estimate the number of independent measurements in an experiment, given that some statistical properties are known for each measurement. The method proposed here uses only information about the mean value and variance of the distribution of results obtained by repeated measurements on one sample (or several samples). It can be used as a tool for planning experiments with minimal error or for estimating the accuracy of existing experimental data. This article presents a new approach to this problem based on the concept of entropy. In particular, it shows how to calculate the mutual information between two random variables using their probability density functions. A numerical example illustrates the application of these methods. \n \n Keywords: Redundancy estimation, Entropy, Mutual Information, Experiment Planning \n \n 1 Introduction \n \n When designing an experiment, we need to know what kind of precision our measuring instrument will give us. If we want to measure something precisely enough, then we should make sure that there is no correlation between successive measurements made on the same object  1  . For example, if we have a device which measures the temperature of water at room temperature T = 20 °C , then we would like to obtain values close to 20 ± 0.1°C when repeating the measurement many times  2  .\n \nIn practice, however, such repeatability cannot always be achieved because of various factors affecting the measurement process  3  . Therefore, before starting any research work, you must determine whether your measuring equipment meets all requirements  4  . \n \n 2 Problem statement \n \n Let X be a continuous random variable describing the result of a single measurement performed under certain conditions  5  . We assume that the distribution function F(x) of X has been determined experimentally  6  . Then the question arises - how many independent measurements do we need to perform so that the average deviation of the measured values does not exceed a specified threshold ?",
        "watermark_text": "The question is to estimate the proportion of separate measurements in an observation , provided that some statistical characteristics are known for each measurement . The method adopted here uses only data about the mean value and variance of the distribution of findings obtained by repeated measurements on one sample ( or several experiments ) .It can be used as a technique for planning studies with minimal loss or for estimating the accuracy of older experimental evidence . This page presents a new approach to this question based on the idea of entropy .In particular , it displays how to estimate the mutual information between two random factors using their likelihood density functions . A numerical example illustrates the implementation of these schemes .Keywords : Redundancy estimation , Entropy , Mutual Information , Experiment Planning 1 Introduction When designing an test , we must to realize what sort of precision our measuring instrument will provide us . If we wish to measure something precisely enough , then we should make sure that there is no coupling between successive measurements made on the same object 1 .For instance , if we have a device which studies the temperature of water at room temperature T = 20 °C , then we may wish to obtain readings nearly to 20 ± 0 . 1°C when repeating the observation many times 2 . In practice , however , such repeatability cannot often be obtained because of several variables affecting the observation technique 3 .Therefore , before beginning any study work , you must know whether your assessment equipment meets all requirements 4 . 2 Problem statement Let X be a consistent random variable describing the result of a single test conducted under certain conditions 5 .We assume that the distribution relation F ( x ) of X has been determined experimentally 6 . Then the question arises - how many independent surveys do we require to conduct so that the average deviation of the tested values does not reach a specified threshold ?",
        "rewrite_text": "The objective is to estimate the proportion of distinct measurements within an observation when certain statistical characteristics are known for each measurement. This approach relies solely on the mean value and variance of the findings derived from repeated measurements on a single sample or across several experiments. It can serve as a valuable technique for planning studies with minimal errors or for assessing the accuracy of previous experimental results. This document introduces a novel perspective on this issue, grounded in the concept of entropy. Specifically, it demonstrates how to estimate the mutual information between two random variables utilizing their likelihood density functions, complemented by a numerical example to illustrate the implementation of these methods. \n\n**Keywords:** Redundancy estimation, Entropy, Mutual Information, Experiment Planning\n\n**1 Introduction**  \nWhen designing an experiment, it is essential to understand the level of precision that our measuring instrument can provide. To achieve sufficiently precise measurements, it is crucial to ensure that there is no dependence between consecutive measurements taken from the same object. For example, if a device is used to measure the temperature of water at room temperature (T = 20 °C), the goal may be to obtain readings close to 20 ± 0.1 °C through repeated observations. However, in practice, achieving such consistency is often challenging due to various factors influencing the measurement technique. Therefore, before commencing any research, it is vital to verify that your measurement equipment meets the necessary criteria. \n\n**2 Problem Statement**  \nLet X represent a consistent random variable that describes the outcome of a single test performed under specific conditions. We assume that the distribution function F(x) of X has been experimentally established. This raises the question: how many independent surveys must be conducted to ensure that the average deviation of the test results does not exceed a predefined threshold?",
        "ori-fast-z-score": -0.6620847108818944,
        "water-fast-z-score": 9.434707130066995,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The rapid growth in the use and production of digital media has created an urgent need to develop new models that support long-term access, preservation, and reuse of personal archives.  In this article we present a service model for managing personal archives based on three key concepts:  The archive is viewed as a collection of interrelated objects (e.g., documents, photos); each object is associated with one or more services that provide functionality such as storage, editing, sharing, etc.; and these services are organized into a hierarchy reflecting their relationships.   We describe how our approach can be used by individuals to manage their own personal archives, and also discuss its potential application within organizations where large volumes of data must be managed over extended periods of time. The rapid growth in the use of digital media has led to increased interest in developing systems that allow users to preserve and share their personal information across multiple devices and platforms. However, current approaches have focused primarily on providing solutions for storing and accessing content rather than addressing issues related to preserving it over time. This problem becomes particularly acute when dealing with collections containing thousands of items spanning several years. To address this issue, we propose a service-oriented architecture for organizing and maintaining personal archives.",
        "watermark_text": "The rapid increase in the using and production of digital media has established an urgent need to develop new models that enable large - term access , preservation , and reuse of personal libraries . In this page we present a service model for controlling personal libraries using on three key concepts : The archive is viewed as a collection of interrelated artifacts ( e . g . , files , photos ) ; each object is associated with one or more functions that provide functionality such as processing , editing , sharing , etc .; and these services are coordinated into a structure indicating their connections . We illustrate how our approach can be used by individuals to manage their own individual archives , and also discuss its potential application within organizations where vast volumes of records must be managed over longer periods of time .The rapid increase in the using of digital media has led to greater activity in developing systems that enable users to archive and transfer their personal data across multiple computers and platforms . However , current approaches have concentrated mostly on providing strategies for storing and accessing information rather than solving topics related to preserving it over time .This problem appears particularly intense when dealing with libraries containing many of items spanning numerous years . To address this question , we propose a service - based architecture for storing and keeping personal records .",
        "rewrite_text": "The swift growth in the usage and production of digital media has created a pressing need for new models that facilitate long-term access, preservation, and reuse of personal libraries. In this document, we introduce a service model for managing personal libraries based on three essential concepts: viewing the archive as a collection of interconnected artifacts (such as files and photos); associating each item with various functions that allow for processing, editing, sharing, and more; and organizing these services into a framework that demonstrates their interrelationships. We demonstrate how individuals can utilize our approach to manage their personal archives effectively and explore its potential use in organizations that need to handle large volumes of records over extended periods. As digital media usage rises, there has been increased activity in creating systems that allow users to archive and transfer personal data across different computers and platforms. However, existing methods have primarily focused on strategies for storing and accessing information rather than addressing long-term preservation challenges. This issue is particularly pressing for libraries containing numerous items spanning many years. To tackle this challenge, we propose a service-oriented architecture designed for storing and preserving personal records.",
        "ori-fast-z-score": 0.7126966450997984,
        "water-fast-z-score": 9.698686309445845,
        "rewrite-fast-z-score": 0.8392543274162825
    },
    {
        "original_text": "We study the second-order correlation function for an atom interacting with two modes of light, one resonant and another off-resonant to atomic transition frequency. We show that higher order antibunching can be observed when the atom is initially prepared in an excited state or ground state superposition. The effect is more pronounced if the initial state has some population on the excited state. This phenomenon may have applications in quantum information processing. \n \n Introduction:-In recent years there has been considerable interest in studying nonclassical properties of radiation fields generated by atoms  1  . In particular, it was shown that the photon statistics of such systems are governed by the first-order coherence function g (1) (τ)  2  , which describes bunching behavior at short times and anti-bunching at longer times  3  . It is well known that this property arises due to destructive interference between different pathways leading to emission of photons  4  .\nRecently, several authors studied the effects of spontaneous emission on the second-order correlation functions  5  -  8  . They showed that the presence of spontaneous emission leads to sub-Poissonian statistics  6 - 8  . However, these studies were restricted only to the case where the atom interacts with a single mode of field. On the other hand, many experiments involving atoms interacting simultaneously with multiple modes of electromagnetic field have also been performed  9  -  11  . For example, in Ref.  10  , the authors investigated the influence of vacuum fluctuations on the fluorescence spectrum of a three-level system driven by two laser beams. In addition, they found that the intensity noise of the emitted light depends strongly on the relative phase difference between the driving lasers. Motivated by these experimental results we consider here the problem of calculating the second-order correlation function of an atom interacting simultaneously with two modes of light  12  .",
        "watermark_text": "We test the second - order correlation function for an element interacting with two modes of light , one resonant and another off - resonant to atomic transition frequency . We see that higher order antibunching can be experienced when the atom is initially prepared in an excited state or ground state superposition .The phenomenon is more pronounced if the first state has some population on the excited state . This phenomenon might have applications in quantum information processing .Introduction : - In recent years there has been substantial interest in investigating nonclassical characteristics of radiation fields generated by atoms 1 . In particular , it was shown that the photon statistics of such systems are governed by the first - order coherence function h ( 1 ) ( τ ) 2 , which explains bunching behavior at short periods and pro - bunching at shorter times 3 .It is well established that this property originates due to destructive interference between various pathways leading to emission of photons 4 . Recently , various papers studied the effects of induced emission on the second - order correlation functions 5 - 8 .They showed that the presence of spontaneous emission contributes to sub - Poissonian statistics 6 - 8 . However , these experiments were restricted only to the case where the atom interacts with a single mode of field .On the other hand , many tests utilizing atoms interacting simultaneously with various modes of electromagnetic field have already been performed 9 - 11 . For instance , in Ref .10 , the authors explored the impact of vacuum fluctuations on the fluorescence spectrum of a three - level network driven by two laser beams . In addition , they reported that the frequency sound of the emitted light depends strongly on the relative phase change between the driving lasers .Motivated by these observation findings we mention here the question of calculating the second - order correlation function of an element interacting simultaneously with two modes of light 12 .",
        "rewrite_text": "We examine the second-order correlation function for an atom interacting with two light modes: one resonant and the other off-resonant to the atomic transition frequency. Our findings indicate that higher-order antibunching occurs when the atom is initially in an excited state or a superposition of the ground state. This effect is particularly significant when there is some population in the excited state. Such phenomena may have potential applications in the realm of quantum information processing.\n\n**Introduction:** Recent years have seen a growing interest in exploring the nonclassical properties of radiation fields produced by atoms. Specifically, it has been demonstrated that the photon statistics in these systems are governed by the first-order coherence function \\( h^{(1)}(\\tau) \\), which accounts for bunching behavior at short time intervals and pro-bunching at even shorter times. This behavior arises from destructive interference among the various pathways that lead to photon emission. Recent studies have investigated the effects of induced emission on second-order correlation functions, revealing that spontaneous emission contributes to sub-Poissonian statistics. However, these studies have primarily focused on scenarios where the atom interacts with a single mode of the electromagnetic field. Conversely, there have been multiple experiments with atoms interacting with various modes of the electromagnetic field. For example, one study examined how vacuum fluctuations affect the fluorescence spectrum of a three-level system driven by two laser beams, noting that the frequency characteristics of the emitted light are highly dependent on the relative phase difference between the driving lasers. Inspired by these observations, we pose the question of how to calculate the second-order correlation function for an atom interacting simultaneously with two modes of light.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": 1.9100460366360192
    },
    {
        "original_text": "We study the existence, uniqueness and ergodicity of physical measures in some classes of infinite measure preserving transformations on probability spaces. We prove that if T is an infinite measure preserving transformation on (X, B(X), m) such that there exists a set A ∈ B(X) satisfying m(A) > 0 and m(T −1 A) = 0 then T has at least one physical measure which is unique up to scalar multiples. Moreover we show that any two physical measures are equivalent under suitable conditions. Finally we give examples showing how our results can be applied. The main motivation comes from the theory of infinite ergodic group actions where it was shown by Glasner and Weiss  GW  ,  GS1  that every infinite ergodic action of a countable amenable group G admits a unique invariant Radon measure µG . In this case the space X is taken as the dual group G equipped with its left Haar measure m.",
        "watermark_text": "We research the existence , uniqueness and ergodicity of physical measures in some categories of infinite measure retaining transformations on probability spaces . We prove that if T is an infinite measure preserving transformation on ( X , B ( X ) , m ) such that there exists a setting A ∈ B ( X ) satisfying m ( A ) > 0 and m ( T −1 A ) = 0 then T has at least one physical measure which is unique up to scalar multiples .Moreover we prove that any two physical measures are comparable under suitable conditions . Finally we give instance demonstrating how our findings can be applied .The main motivation arises from the principle of infinite ergodic group events where it was shown by Glasner and Weiss GW , GS1 that every infinite ergodic act of a countable amenable group G accepts a unique invariant Radon number µG . In this instance the space X is taken as the dual group G equipped with its leaving Haar measure m .",
        "rewrite_text": "We investigate the existence, uniqueness, and ergodicity of physical measures in certain categories of infinite measure-preserving transformations on probability spaces. We establish that if \\( T \\) is an infinite measure-preserving transformation on \\( (X, B(X), m) \\), and there exists a measurable set \\( A \\in B(X) \\) such that \\( m(A) > 0 \\) and \\( m(T^{-1} A) = 0 \\), then \\( T \\) possesses at least one physical measure that is unique up to scalar multiples. Furthermore, we demonstrate that under appropriate conditions, any two physical measures are comparable. We also provide examples illustrating the application of our results. The primary motivation for this research stems from the principle of infinite ergodic group actions, where Glasner and Weiss showed that every infinite ergodic action of a countable amenable group \\( G \\) has a unique invariant Radon measure \\( \\mu_G \\). In this context, the space \\( X \\) is taken to be the dual group \\( G \\) with its associated Haar measure \\( m \\).",
        "ori-fast-z-score": -0.9113223768657671,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": -0.39735970711951313
    },
    {
        "original_text": "The development and application of superconducting quantum interference devices (SQUIDs) in magnetic resonance imaging has been reviewed, with emphasis on the recent advances made by our group at UCLA. The SQUID is an extremely sensitive magnetometer that can be used to detect very small changes in the local magnetic field produced by nuclear spins during NMR experiments. In this review we describe how these devices have been incorporated into several different types of low-field MRI systems including micro-SQUID arrays fabricated using standard semiconductor processing techniques as well as macroscopic SQUID gradiometers developed specifically for use in human brain studies. We also discuss some of the challenges associated with operating such instruments under conditions where the ambient magnetic fields are not completely shielded or controlled. Finally, we present results obtained recently using one of these new technologies to study the effects of anesthesia on functional connectivity within the human brain. Magnetic Resonance Imaging (MRI), which was first demonstrated more than 30 years ago, provides detailed information about the structure and function of living tissues through noninvasive measurements of their water content  1  . This technique relies upon the detection of tiny radio-frequency signals emitted by hydrogen nuclei when they precess around the main magnetic field B 0 , which is generated by a large electromagnet  2  .\nIn conventional clinical scanners, the strength of B 0 ranges between 1 T and 3 T  3  . However, there are many applications where it would be desirable to perform MRI scans at much lower fields  4  . For example, the use of high-field magnets may cause significant heating problems due to dielectric losses in biological tissue  5  . Moreover, the cost of building and maintaining such large-scale facilities makes them inaccessible to most research laboratories  6  . Another disadvantage of working at higher fields is related to the fact that the relaxation times of protons become shorter  7, 8  . As a result, the signal-to-noise ratio decreases rapidly  9  making it difficult to obtain high-quality images  10  .",
        "watermark_text": "The development and application of superconducting quantum interference devices ( SQUIDs ) in magnetic resonance imaging has been examined , with emphasis on the recent developments shown by our team at UCLA . The SQUID is an incredibly sensitive magnetometer that can be used to identify very small changes in the local magnetic current generated by nuclear spins during NMR experiments .In this review we explain how these systems have been incorporated into numerous separate types of low - field MRI systems including micro - SQUID arrays fabricated use classic semiconductor processing procedures as well as macroscopic SQUID gradiometers developed specifically for use in human mind studies . We especially consider some of the challenges associated with operating such instruments under environments where the ambient magnetic areas are not totally shielded or monitored .Finally , we present results acquired previously using one of these new inventions to study the effects of anesthesia on functional functionality within the human mind . Magnetic Resonance Imaging ( MRI ) , which was first demonstrated more than 30 centuries earlier , offers general information about the composition and function of living tissues through noninvasive measurements of their water content 1 .This method relies upon the observation of tiny radio - frequency pulses emitted by hydrogen atoms when they precess around the main magnetic force B 0 , which is generated by a large electromagnet 2 . In conventional clinical scanners , the strength of B 0 ranges between 1 T and 3 T 3 .However , there are many applications where it would be beneficial to conduct MRI scans at much lower fields 4 . For instance , the using of high - field magnets might cause significant heating problems due to dielectric losses in biological tissue 5 .Moreover , the cost of building and keeping such large - scale institutions making them inaccessible to most research labs 6 . Another limitation of living at higher areas is related to the fact that the relaxation period of protons become shorter 7 , 8 .As a result , the signal - to - noise proportion decreases quickly 9 creating it difficult to obtain high - grade images 10 .",
        "rewrite_text": "The exploration and utilization of superconducting quantum interference devices (SQUIDs) in magnetic resonance imaging (MRI) have been analyzed, particularly highlighting the recent advancements made by our team at UCLA. SQUIDs are extremely sensitive magnetometers capable of detecting minute alterations in the local magnetic fields generated by nuclear spins during NMR experiments. In this review, we detail how these devices have been integrated into various low-field MRI systems, including micro-SQUID arrays created using traditional semiconductor manufacturing techniques, as well as macroscopic SQUID gradiometers specifically designed for studies involving the human brain. We also address some of the challenges faced when operating these instruments in environments where ambient magnetic fields are not fully shielded or monitored. Additionally, we share findings obtained from one of these innovative devices that investigated the effects of anesthesia on brain function. Magnetic Resonance Imaging (MRI), demonstrated over 30 years ago, provides valuable insights into the composition and function of living tissues through noninvasive measurements of their water content. This technique is based on the detection of weak radiofrequency signals emitted by hydrogen atoms as they precess around the primary magnetic field (B0) generated by a large electromagnet. In traditional clinical scanners, B0 typically ranges from 1 T to 3 T. However, there are numerous scenarios where conducting MRI scans at significantly lower fields would be advantageous. For example, high-field magnets can lead to considerable heating issues due to dielectric losses in biological tissues. Furthermore, the expense of constructing and maintaining such large-scale facilities renders them impractical for most research laboratories. Another challenge associated with high-field operation is the reduction in the relaxation time of protons, which results in a rapid decline in the signal-to-noise ratio, complicating the acquisition of high-quality images.",
        "ori-fast-z-score": 0.07881104062391006,
        "water-fast-z-score": 8.74573066576194,
        "rewrite-fast-z-score": -0.4181210050035454
    },
    {
        "original_text": "We study the phonon-assisted relaxation dynamics of an excited atom trapped by a surface-induced optical lattice (SIL). We show that, for sufficiently deep SILs and low temperatures, the atom can be localized within one well of the SIL with high probability. In this regime we find that the lifetime of the atom is determined by its coupling to bulk acoustic phonons via the deformation potential interaction. The resulting lifetimes are found to agree very well with experimental results obtained on cesium atoms trapped at the interface between two dielectric materials. \n \n Introduction \n \n Surface-induced lattices have been used extensively over recent years as a tool for trapping ultracold atoms near surfaces  1–3  . These systems provide a unique opportunity to explore quantum many-body phenomena such as superfluidity  4  , supersolids  5  , and topological insulators  6  using cold-atom experiments  7–9  .\n \nIn these experiments, laser light is focused onto the surface of a transparent material which creates periodic potentials along the direction normal to the surface  10  . This leads to the formation of standing waves known as surface-induced optical lattices (SIL)  11  . Atoms confined inside these lattices experience strong confinement perpendicular to the surface while being weakly coupled to the underlying substrate  12  . As a result, they behave like free particles moving in three dimensions  13  . \n \n While there has been significant progress towards understanding the properties of atoms trapped in SILs  14–18  , relatively little attention has been paid so far to their relaxation dynamics  19, 20  . Here we consider the case where an atom is initially prepared in an excited state |e⟩ above some energy threshold E0. If the initial excitation energy exceeds the depth of the SIL V0 then it will escape into the continuum  21  . However if the initial energy lies below E0 but still exceeds the recoil energy ER = 2 kL2 / 2mL  22  , where mL denotes the mass of the atom and kL is the wavevector associated with the lattice periodicity, then the atom may relax back down to the ground state |g⟩ through emission of a",
        "watermark_text": "We explore the phonon - aided vibration dynamics of an excited element trapped by a surface - mediated optical lattice ( SIL ) . We see that , for enough deep SILs and low temperatures , the atom can be localized within one well of the SIL with high probability .In this regime we find that the life of the atom is calculated by its interaction to bulk acoustic phonons via the deformation potential interaction . The resulting lifetimes are found to agree very well with theoretical results derived on cesium atoms trapped at the interface between two dielectric materials .Introduction Surface - mediated lattices have been used heavily over recent years as a technique for trapping ultracold atoms near layers 1 – 3 . These systems present a unique opportunity to examine quantum several - bodies phenomena such as superfluidity 4 , supersolids 5 , and topological insulators 6 using heat - atom experiments 7 – 9 .In these experiments , laser light is conducted onto the surface of a transparent material which forms periodic potentials along the direction regular to the surface 10 . This leads to the formation of standing currents termed as surface - mediated laser lattices ( SIL ) 11 .Atoms confined inside these lattices experience strong confinement parallel to the surface while being weakly linked to the underlying substrate 12 . As a result , they react like free particles moving in three dimensions 13 .While there has been significant progress towards studying the properties of atoms trapped in SILs 14 – 18 , fairly little attention has been paid so far to their relaxation behavior 19 , 20 . Here we study the case where an element is initially produced in an excited state | e ⟩ above some energy threshold E0 .If the first excitation energy reaches the height of the SIL V0 then it will flee into the continuum 21 . However if the first energy falls below E0 but still surpassed the recoil power ER = 2 kL2 / 2mL 22 , where mL denotes the mass of the atom and kL is the wavevector related with the crystal periodicity , then the atom might settle back down to the ground state | h ⟩ through emitted of a",
        "rewrite_text": "We investigate the phonon-assisted vibrational dynamics of an atom in an excited state that is confined by a surface-induced optical lattice (SIL). Our findings indicate that, under conditions of sufficiently deep SILs and low temperatures, the atom has a high probability of being localized within a single well of the lattice. In this scenario, the atom's lifetime is influenced by its interactions with bulk acoustic phonons through the deformation potential mechanism. The measured lifetimes align closely with theoretical predictions made for cesium atoms trapped at the boundary between two dielectric materials.\n\nIntroduction: Over recent years, surface-mediated lattices have been extensively employed as a means to trap ultracold atoms near surfaces. These systems offer a compelling platform to investigate complex quantum phenomena like superfluidity, supersolids, and topological insulators via heat-atom interactions. In these experiments, laser light is applied to the surface of a transparent medium, creating periodic potentials perpendicular to the surface, which give rise to standing wave patterns known as surface-induced laser lattices. Atoms confined within these lattices experience strong confinement parallel to the surface while maintaining a weak connection to the underlying substrate, behaving akin to free particles in three dimensions. \n\nAlthough substantial advancements have been made in studying the properties of atoms trapped in SILs, limited attention has been directed toward their relaxation dynamics. In this work, we examine a scenario in which an atom is initially excited to a state | e ⟩ above a specified energy threshold E0. If the energy of the first excitation reaches the height of the SIL, V0, the atom will escape into the continuum. Conversely, if it falls below E0 but remains above the recoil energy, ER = 2kL² / 2mL, where mL is the atom's mass and kL is the wavevector related to the crystal periodicity, the atom has the potential to relax back to the ground state | h ⟩ by emitting a phonon.",
        "ori-fast-z-score": -0.5659164584181103,
        "water-fast-z-score": 8.435513898799652,
        "rewrite-fast-z-score": 0.5853694070049635
    },
    {
        "original_text": "The quantum group of isometries (QGI) was introduced by Connes as the universal object for deformations of classical groups, which are obtained via spectral triples on commutative C*-algebras.  In this talk we will discuss how to define QGI s using noncommutative geometry techniques such as operator algebras and von Neumann algebras.   We will also explain how these objects can be used to study the classification problem of Riemannian manifolds with positive scalar curvature. The Quantum Group of Isometries (QGI), first defined by Alain Connes, plays an important role in both classical and noncommutative geometry. It is the universal object for deforming classical Lie groups into their corresponding quantum groups. This talk will give an introduction to QGI’s and show that they can be studied through operator algebra theory and von Neumann algebras. Finally it will present some results about the classification problem of Riemannain manifolds with positive scalar curvatures.",
        "watermark_text": "The quantum group of isometries ( QGI ) was introduced by Connes as the universal object for deformations of classical groups , which are derived via spectral triples on commutative C * - algebras . In this talk we will explore how to define QGI s using noncommutative geometry tools such as operator algebras and von Neumann algebras .We will also explain how these objects can be used to study the characterization question of Riemannian manifolds with positive scalar curvature . The Quantum Group of Isometries ( QGI ) , initially established by Alain Connes , holds an important role in both classical and noncommutative geometry .It is the fundamental object for deforming classical Lie fields into their resulting quantum groups . This discussion will provide an introduction to QGI ’ s and explain that they can be investigated through operator algebra analysis and von Neumann algebras .Finally it will present some results about the characterization question of Riemannain manifolds with positive scalar curvatures .",
        "rewrite_text": "The Quantum Group of Isometries (QGI), introduced by Alain Connes, serves as the universal framework for deforming classical groups through spectral triples associated with commutative C*-algebras. In this presentation, we will delve into the definition of QGIs utilizing tools from noncommutative geometry, including operator algebras and von Neumann algebras. Additionally, we will discuss how these structures can aid in addressing the characterization problem of Riemannian manifolds that possess positive scalar curvature. Given its pivotal role in both classical and noncommutative geometry, the QGI provides a crucial basis for transforming classical Lie groups into their quantum counterparts. This talk will offer a comprehensive introduction to QGIs and will showcase relevant findings concerning the characterization of Riemannian manifolds with positive scalar curvature.",
        "ori-fast-z-score": -0.2773500981126146,
        "water-fast-z-score": 4.437601569801833,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of our CCD photometric study of 42 open clusters in the southern hemisphere, carried out at the 1-meter telescope of the South African Astronomical Observatory (SAAO). The observations were made with an SBIG STL-1001E camera equipped with a Kodak KAF-0400 chip and Johnson V filter during three observing runs between September 1998 and February 1999. We have used DAOPHOT II to perform aperture photometry on all stars detected within each cluster field-of-view. A total number of about 15000 stars was measured for each cluster. In addition we obtained UBVRI photometry for some of these clusters using the same instrumentation as described above.  From this data set we derived the following parameters: reddening E(B-V), distance modulus DM, age t, metallicity  Fe/H  , mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute magnitude M.",
        "watermark_text": "We present the conclusion of our CCD photometric analysis of 42 open complexes in the southern hemisphere , conducted out at the 1 - meter telescope of the South African Astronomical Observatory ( SAAO ) . The images were made with an SBIG STL - 1001E camera equipped with a Kodak KAF - 0400 card and Johnson V filter during three observing runs between September 1998 and February 1999 .We have utilized DAOPHOT II to conduct aperture photometry on all stars observed within each cluster area - of - view . A total number of about 15000 stars was measured for each cluster .In addition we derived UBVRI photometry for some of these complexes use the same equipment as described above . From this data set we derived the following variables : reddening E ( B - V ) , distance modulus DM , age t , metallicity Fe / H , mass function slope x , core radius rc , central exterior brightness µ0 , concentration index c , and integrated absolute brightness M .",
        "rewrite_text": "We present the findings from our CCD photometric analysis of 42 open clusters in the southern hemisphere, conducted using the 1-meter telescope at the South African Astronomical Observatory (SAAO). The observations were captured with an SBIG STL-1001E camera featuring a Kodak KAF-0400 chip and a Johnson V filter over three observing sessions between September 1998 and February 1999. We employed DAOPHOT II for aperture photometry on all stars within the field of view for each cluster, resulting in the measurement of approximately 15,000 stars per cluster. Additionally, we obtained UBVRI photometry for some of these complexes using the previously mentioned equipment. From this dataset, we derived several key variables: reddening E(B-V), distance modulus DM, age t, metallicity [Fe/H], mass function slope x, core radius rc, central surface brightness µ0, concentration index c, and integrated absolute brightness M.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 2.9448482384566077,
        "rewrite-fast-z-score": 0.2581988897471611
    },
    {
        "original_text": "The authors have studied the growth rate and its fluctuations for different types of human cancer cells grown under controlled conditions, using an automated image analysis system to measure their size over time.  The results show that there is considerable variation between individual clones within each type of cell line as well as between different types of cell lines. This suggests that it may be possible to use this information to classify tumours into subtypes with distinct biological properties. In addition, they find evidence that the growth rates are correlated across generations of daughter cells, which could provide insight into how these correlations arise during tumorigenesis. Finally, by comparing the growth characteristics of normal and transformed cells, they conclude that transformation leads to increased heterogeneity among daughter cells. Cancer is characterized by uncontrolled proliferation of abnormal cells. Understanding the mechanisms underlying this process can help us develop new treatments against cancer. However, studying the dynamics of cancerous cell populations has been challenging because of difficulties associated with tracking large numbers of single cells simultaneously. Here we report our recent work on characterizing the growth behavior of thousands of individual cancer cells growing in culture dishes  1  . We used an automated imaging system to track the sizes of hundreds of thousands of cells belonging to several different types of human cancer cell lines ( Figure 1 ). Our results reveal significant differences in both average growth rates and growth fluctuations between different types of cell lines: some grow faster than others while also exhibiting larger fluctuations around their mean values  2  .\nWe found that the growth rates were highly variable even when measured at the level of individual clones derived from a common parent population  3  , suggesting that the observed phenotypic diversity might reflect genetic or epigenetic variations present in the original parental population  4  .  These findings suggest that it should be possible to use such measurements to classify tumors into subtypes based on their growth characteristics  5  .",
        "watermark_text": "The authors have researched the development time and its fluctuations for different kinds of human tumor tissues grown under regulated conditions , using an automated photo processing program to measure their size over time . The results show that there is substantial variation between individual clones within each type of cell line as well as between various types of cell lines .This shows that it could be possible to use this data to classify tumours into subtypes with distinct bio properties . In addition , they discover proof that the development rates are correlated across generations of daughter cells , which could give insight into how these correlations occur during tumorigenesis .Finally , by comparing the development patterns of normal and transformed tissue , they conclude that transformation results to greater heterogeneity among sister cells . Cancer is characterized by uncontrolled proliferation of irregular cells .Understanding the mechanisms governing this process can help us discover new therapy against disease . However , studying the dynamics of cancerous cell groups has been challenging because of delays associated with measuring large numbers of single cells simultaneously .Here we document our latest work on characterizing the development habits of tens of individual cancer cells growing in culture dishes 1 . We utilized an automated imaging technology to track the sizes of tens of hundred of cells belonging to several different kinds of human tumor tissue lines ( Figure 1 ) .Our results show considerable variations in both average growth rates and growth fluctuations between various types of cell lines : some develop longer than others while also displaying greater fluctuations around their average values 2 . We showed that the development rates were extremely varied even when measured at the level of individual clones originating from a common parent population 3 , showing that the reported phenotypic diversity could reflect genetic or epigenetic changes found in the original parental generation 4 .These studies propose that it should be possible to use such measurements to classify cancer into subtypes based on their growth parameters 5 .",
        "rewrite_text": "The authors investigated the development times and fluctuations of various human tumor tissues cultivated under controlled conditions, employing an automated image processing program to track their growth over time. The findings reveal significant variation not only among individual clones within the same cell line but also across different cell lines. This suggests the potential for classifying tumors into subtypes based on distinct biological properties. Furthermore, the study provides evidence of correlations in growth rates across generations of daughter cells, offering insights into the dynamics of tumorigenesis. By comparing the growth patterns of normal versus transformed tissues, the authors conclude that transformation leads to increased heterogeneity among sister cells. As cancer is characterized by the uncontrolled proliferation of abnormal cells, understanding the mechanisms behind this phenomenon is crucial for developing new therapies. However, analyzing the dynamics of cancerous cell populations has proven difficult due to challenges associated with concurrently measuring large numbers of individual cells. In this study, we detail our latest efforts to characterize the growth behaviors of tens of individual cancer cells cultured in dishes. We utilized automated imaging technology to monitor the sizes of hundreds of cells from several distinct human tumor tissue lines. Our results demonstrate significant variations in both average growth rates and growth fluctuations among different cell lines, with some exhibiting prolonged development and greater fluctuations around their average rates. Notably, we found that growth rates varied markedly even at the individual clone level within a shared parent population, indicating that the observed phenotypic diversity may stem from genetic or epigenetic variations present in the original parental generation. These findings suggest that growth measurements could be employed to classify cancers into subtypes based on their developmental parameters.",
        "ori-fast-z-score": -1.6678156958735875,
        "water-fast-z-score": 9.146740246823299,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "The MiniBooNE experiment has recently reported the observation of an excess in electron-neutrino-like events at low energies, which could be explained by sterile neutrinos with mass around 1 eV and mixing angle sin2(2θ) ~ 0.1. \n \n In this work we study how these results can be accommodated within the framework of three-flavor leptonic mixing using the latest global fits to experimental data on neutrino oscillation parameters as well as cosmological constraints on the sum of active neutrino masses. We find that the allowed parameter space is strongly constrained if one assumes that the observed excess corresponds to true neutrino oscillations into sterile states rather than being due to background systematics or statistical fluctuations. The best-fit values for the sterile neutrino mass splitting are found to be Δm32 = (0.5 - 2.3) meV and Δm2 = (0.4 - 3.6) meV, while the corresponding ranges for the mixing angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "watermark_text": "The MiniBooNE experiment has recently noted the observation of an increase in electron - neutrino - like phenomena at low energies , which could be described by sterile neutrinos with mass around 1 eV and mixing angle sin2 ( 2θ ) ~ 0 . 1 . In this research we study how these results can be accommodated within the framework of three - flavor leptonic mixing using the latest global fits to experimental evidence on neutrino oscillation components as well as cosmological limitations on the sum of active neutrino masses .We see that the allowed parameter space is strongly constrained if one assumes that the reported amount corresponds to genuine neutrino oscillations into sterile states instead than being owing to background systematics or statistical fluctuations . The best - fitting values for the sterile neutrino mass separation are found to be Δm32 = ( 0 . 5 - 2 . 3 ) meV and Δm2 = ( 0 . 4 - 3 . 6 ) meV , while the equivalent ranges for the blending angles are θ23 = 42° - 50° , θ13 < 5° and θ12 > 40° .",
        "rewrite_text": "The MiniBooNE experiment has recently reported an increase in low-energy electron-neutrino-like events, potentially explained by sterile neutrinos with a mass around 1 eV and a mixing angle of sin²(2θ) ~ 0.1. In this study, we explore how these findings fit within the three-flavor leptonic mixing framework, utilizing the latest global analyses of experimental neutrino oscillation data and cosmological constraints on the total mass of active neutrinos. We find that the allowed parameter space is tightly limited if one assumes that the observed phenomena genuinely correspond to neutrino oscillations into sterile states, rather than being the result of background systematic errors or statistical fluctuations. The best-fitting values for the sterile neutrino mass differences are determined to be Δm₃₂ = (0.5 - 2.3) meV and Δm² = (0.4 - 3.6) meV, while the corresponding ranges for the mixing angles are θ₂₃ = 42° - 50°, θ₁₃ < 5°, and θ₁₂ > 40°.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 4.106050306901635,
        "rewrite-fast-z-score": 0.9271726499455306
    },
    {
        "original_text": "We present analytic models for the potentials that can produce multiple images in strong gravitational lenses, and we use these to study how well different mass profiles are constrained by observations. We find that it is possible to obtain good constraints on both the total enclosed mass within an Einstein radius (the projected distance at which two images merge) and the slope of the density profile outside this radius using only three or four multiply-imaged systems with accurate photometric redshifts.  The results presented here should be useful for planning future surveys aimed at measuring dark matter properties through gravitational lensing. Gravitational lensing provides one of our most powerful tools for studying the distribution of dark matter in galaxies and clusters. In particular, if a galaxy cluster has been lensed into several distinct images, then the positions and fluxes of those images provide information about the shape of the cluster s gravitational potential as a function of position along its line-of-sight. This allows us to measure the total enclosed mass within some characteristic scale length, such as the Einstein radius, where two images merge together. However, because there are many degeneracies between the parameters describing the lens model, it is not always straightforward to determine what kind of mass profile best describes the data.",
        "watermark_text": "We create analytic models for the potentials that can generate several images in good gravitational lenses , and we utilize these to study how well distinct mass profiles are constrained by observed . We see that it is easy to obtain good limitations on both the total enclosed mass within an Einstein radius ( the projected height at which two images merge ) and the slope of the density profile outside this radius using only three or four multiply - imaged methods with correct photometric redshifts .The results presented here should be valuable for planning upcoming studies aimed at assessing gray object characteristics through gravity lensing . Gravitational lensing offers one of our most important skills for studying the distribution of darkened matter in galaxies and clusters .In particular , if a galaxy cluster has been lensed into numerous separate images , then the places and fluxes of those photographs give information about the form of the cluster s gravitational potential as a function of position along its line - of - view . This enables us to measure the total enclosed mass within some characteristic scale length , such as the Einstein radius , where two images merge together .However , because there are many degeneracies between the variables describing the lens model , it is not always straightforward to identify what sort of mass profile best explains the information .",
        "rewrite_text": "We develop analytical models for the potentials that can produce multiple images in strong gravitational lenses, which we use to examine how effectively different mass profiles are constrained by observations. Our findings indicate that it is relatively straightforward to derive robust estimates for both the total enclosed mass within an Einstein radius (the projected distance at which two images converge) and the slope of the density profile beyond this radius, using just three or four multiply-imaged methods with accurate photometric redshifts. The results presented here will be key for planning future studies focused on evaluating the characteristics of faint objects through gravitational lensing. Gravitational lensing is a crucial tool for investigating the distribution of dark matter in galaxies and clusters. Specifically, when a galaxy cluster produces multiple distinct images, the positions and brightnesses of these images provide insights into the shape of the cluster's gravitational potential along the line of sight. This allows us to measure the total enclosed mass within a characteristic scale, such as the Einstein radius, where two images coincide. However, due to various degeneracies among the parameters defining the lens model, identifying the optimal mass profile that best fits the data can be challenging.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.8657566124489255,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "Issai Schur (1886-1951) was one of the most influential mathematicians in twentieth century mathematics, and his work has had an enormous impact on many areas including number theory, functional analysis, operator theory, harmonic analysis, group representation theory, ergodic theory, probability theory, mathematical physics, computer science, statistics, numerical analysis, combinatorics, algebraic geometry, and dynamical systems.  In this talk we will survey some of these contributions with emphasis on their historical context.   We will also discuss how Schur s ideas have been developed by other mathematicians over the past seventy years or so. The lecture is intended for graduate students who are interested in learning about the history of modern mathematics as well as its current state-of-the-art. It should be accessible to undergraduates with a background in real variable calculus and linear algebra. This course fulfills the requirements of both MATH 3010 and MATH 3310 .",
        "watermark_text": "Issai Schur ( 1886 - 1951 ) was one of the most influential mathematicians in 20th period mathematics , and his work has had an enormous effect on numerous topics including number theory , functional analysis , operator analysis , harmonic analysis , group structure analysis , ergodic logic , probability theory , mathematical science , computer science , statistics , numerical investigation , combinatorics , algebraic topology , and dynamical systems . In this talk we will review some of these achievements with emphasis on their historical context .We will also discuss how Schur s ideas have been used by other mathematicians over the previous seventy years or so . The lecture is intended for graduate programs who are concerned in understanding about the history of modern mathematics as well as its current state - of - the - art .It should be available to undergraduates with a background in real variable calculus and linear algebra . This course fulfills the requirements of both MATH 3010 and MATH 3310 .",
        "rewrite_text": "Issai Schur (1886-1951) was a pivotal figure in 20th-century mathematics, whose contributions have profoundly influenced various fields, including number theory, functional analysis, operator theory, harmonic analysis, group structure, ergodic theory, probability, mathematical science, computer science, statistics, numerical methods, combinatorics, algebraic topology, and dynamical systems. In this lecture, we will explore some of Schur's significant achievements, placing particular emphasis on their historical context. Additionally, we will examine how Schur's ideas have been employed by other mathematicians over the past seventy years. This talk is designed for graduate students interested in the history of modern mathematics as well as its current advancements. It will also be accessible to undergraduates who have a foundational understanding of real variable calculus and linear algebra. This course meets the requirements for both MATH 3010 and MATH 3310.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.8325416653445783
    },
    {
        "original_text": "We present the results of our analysis of cosmic ray data collected by the PAMELA experiment in 2008 and 2009, which show an excess over background at energies between 1-10 GeV/nucleon that is consistent with being produced by particles accelerated in nearby supernova remnants (SNRs). We find that this signal can be explained if we assume that SNR RX J1713-3946 accelerates protons up to 10 TeV energy per nucleon. The observed fluxes are also compatible with those expected for other known sources such as pulsars or active galactic nuclei. However, these alternative scenarios cannot explain all features seen in the data set. In particular, they do not predict any significant anisotropy in arrival directions on angular scales below ~10 degrees. This prediction is confirmed by observations made using the Tibet ASγ air shower array. Finally, we discuss possible implications of our findings for models of particle acceleration in relativistic shocks.",
        "watermark_text": "We present the conclusion of our analysis of cosmic ray data taken by the PAMELA study in 2008 and 2009 , which show an amount over background at energies between 1 - 10 GeV / nucleon that is compatible with being produced by particles advanced in nearby supernova remnants ( SNRs ) . We see that this signal can be described if we suppose that SNR RX J1713 - 3946 accelerates protons up to 10 TeV power per nucleon .The observed fluxes are also consistent with those expected for other known sources such as pulsars or active galactic nuclei . However , these alternative situations cannot explain all characteristics found in the information pool .In particular , they do not predict any slight anisotropy in arrival angles on spatial scales below ~ 10 degrees . This prediction is confirmed by observations made using the Tibet ASγ air rain system .Finally , we explain possible possibilities of our findings for models of particle acceleration in relativistic shocks .",
        "rewrite_text": "We present the conclusion of our analysis of cosmic ray data collected by the PAMELA study in 2008 and 2009. Our findings indicate an excess over the background at energies between 1 and 10 GeV per nucleon, which is consistent with production by particles accelerated in nearby supernova remnants (SNRs). Specifically, we suggest that SNR RX J1713-3946 is capable of accelerating protons up to an energy of 10 TeV per nucleon. The observed flux levels also align with predictions for other known sources, like pulsars or active galactic nuclei. However, these alternative explanations fail to account for all the characteristics observed in the data. Notably, they do not predict a slight anisotropy in arrival angles on spatial scales smaller than approximately 10 degrees, a prediction that is supported by observations from the Tibet ASγ air shower array. Lastly, we discuss the implications of our findings for models of particle acceleration in relativistic shocks.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 0.13018891098082389
    },
    {
        "original_text": "In this thesis we study distributed algorithms for spectrum allocation, power control, routing, and congestion control problems in wireless networks. We first consider the problem of joint channel assignment and power control (JCAPC) with minimum total transmit power consumption subject to quality-of-service constraints on each link. The JCAPC problem is formulated as an integer program which can be solved by using standard linear programming techniques. However, such centralized solutions are not practical due to their high computational complexity. In order to overcome these difficulties, we propose two distributed algorithms based on dual decomposition methods. Our simulation results show that our proposed algorithms perform close to optimality while requiring only local information exchange among neighboring nodes. \n \n Next, we investigate the problem of joint routing and congestion control (JRCC). This problem arises when there exists multiple paths between source-destination pairs in a network. Each path has different available bandwidths depending on its physical characteristics. To achieve fairness across all flows sharing common links, JRCC requires that the rate allocated to any flow should depend on both the current load along the path it uses and the number of competing flows. We formulate the JRCC problem as a convex optimization problem whose objective function is to maximize the aggregate utility over all users under certain capacity constraints at each node. Then, we develop a distributed algorithm based on primal decomposition method to solve the JRCC problem. Finally, we present some numerical examples to demonstrate the performance of our proposed algorithm.",
        "watermark_text": "In this dissertation we study spread methods for spectrum allocation , power control , routing , and congestion control problems in telecommunications networks . We first consider the issue of joint network assignment and power control ( JCAPC ) with minimum total broadcast capacity output due to quality - of - service restrictions on each link .The JCAPC task is implemented as an integer code which can be answered by using conventional linear programming tools . However , such centralized methods are not practical owing to their high computational complexity .In order to overcome these problems , we propose two distributed methods using on double decomposition techniques . Our model results show that our proposed methods operate close to optimality while using only local information transfer among neighboring nodes .Next , we investigate the issue of joint routing and congestion control ( JRCC ) . This problem arises when there exists multiple paths between origin - destination pairs in a network .Each stream has various accessible bandwidths based on its physical qualities . To achieve fairness across all flows sharing shared links , JRCC requires that the frequency allocated to any flow should depend on both the current load along the path it utilizes and the quantity of competing paths .We formulate the JRCC problem as a convex optimization problem whose aim function is to maximize the aggregate utility over all users under certain capacity limitations at each node . Then , we develop a distributed algorithm based on primal decomposition technique to tackle the JRCC problem .Finally , we present some numerical examples to indicate the performance of our proposed algorithm .",
        "rewrite_text": "In this dissertation, we explore spread methods for spectrum allocation, power control, routing, and congestion control in telecommunications networks. We begin by addressing the joint network assignment and power control (JCAPC) problem, focusing on minimizing total broadcast capacity output while adhering to quality-of-service constraints on each link. The JCAPC challenge is framed as an integer coding problem, which can be solved using traditional linear programming techniques. However, these centralized approaches are often impractical due to their significant computational demands. To address these challenges, we introduce two distributed methods that leverage dual decomposition techniques. Our model results demonstrate that these methods achieve performance close to optimality by relying on localized information exchange among neighboring nodes. \n\nNext, we investigate the joint routing and congestion control (JRCC) problem, which emerges when multiple paths are available between origin-destination pairs in a network. Each stream offers varying bandwidths depending on its physical characteristics. To ensure fairness among flows sharing common links, JRCC mandates that the bandwidth allocated to any flow be contingent on both the current load of the utilized path and the number of competing paths. We formulate the JRCC challenge as a convex optimization problem aimed at maximizing the total utility for all users while respecting specific capacity constraints at each node. Subsequently, we develop a distributed algorithm utilizing a primal decomposition technique to address the JRCC problem. Finally, we provide numerical examples to illustrate the effectiveness of our proposed algorithm.",
        "ori-fast-z-score": -0.1690308509457033,
        "water-fast-z-score": 7.606388292556649,
        "rewrite-fast-z-score": 2.9371526469766245
    },
    {
        "original_text": "We present an analysis of the UKIRT Infrared Deep Sky Survey (UKIDSS) Galactic Cluster Survey data for the open cluster, Pleiades. We use this to derive the number ratio between binaries and single stars in the range 0.1 < M/M⊙ < 1.0 as well as the initial mass function (IMF). The results are compared against previous studies using different techniques. Our derived binary fraction is consistent within uncertainties with that found by other authors but our IMF shows significant differences when compared to previous work. These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been included in previous analyses. This study highlights the importance of accurate photometry over large areas combined with high resolution spectroscopy to fully understand the properties of young open clusters such as the Pleiades. Keywords: Open cluster; Binary star systems; Initial Mass Function; Substellar object",
        "watermark_text": "We present an assessment of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) Galactic Cluster Survey data for the open cluster , Pleiades . We use this to derive the number ratio between binaries and single galaxies in the range 0 . 1 < M / [UNK] < 1 . 0 as well as the initial mass function ( IMF ) .The results are compared against prior studies use different methods . Our derived binary fraction is compatible within uncertainties with that detected by other researchers but our IMF displays substantial differences when compared to previous research .These discrepancies may be due to contamination from background galaxies or unresolved multiple systems which have been involved in earlier analyses . This study highlights the importance of accurate photometry over large areas coupled with high resolution spectroscopy to fully realize the properties of young open nuclei such as the Pleiades .Keywords : Open cluster ; Binary galaxy structures ; Initial Mass Function ; Substellar object",
        "rewrite_text": "We provide an evaluation of the UKIRT Infrared Deep Sky Survey (UKIDSS) data from the Galactic Cluster Survey focusing on the open cluster Pleiades. Our analysis allows us to calculate the ratio of binary stars to single stars within the mass range of 0.1 < M / [UNK] < 1.0, in addition to determining the initial mass function (IMF). We compare our findings with previous studies that employed different methodologies. While our estimated binary fraction aligns within uncertainties with other researchers' findings, our IMF reveals significant differences when contrasted with earlier work. These variations may stem from contamination by background galaxies or unresolved multiple systems that have been considered in prior analyses. This research underscores the necessity of precise photometry across extensive areas, paired with high-resolution spectroscopy, to thoroughly understand the characteristics of young open clusters like the Pleiades. \n\nKeywords: Open cluster; Binary star systems; Initial Mass Function; Substellar objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -2.182820625326997
    },
    {
        "original_text": "We present new exact half-BPS solutions to the low-energy effective action for type-IIB string theory in ten dimensions, which are supported by fluxes on an arbitrary number of intersecting D3-branes. These solutions can be viewed as generalizations of the single Janus solution presented in  1  . We also discuss some properties of these solutions that were not discussed previously. \n \n The first part of this work was published as: \n \n  2  C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005. DOI: 10.1103/PhysRevLett.106.055005. URL: http://arxiv.org/abs/1111.5389v2. URL: http://www.sciencedirect.com/science/article/pii/S0167971011000203. URL: https://inspirehep.net/record/126898/files/CAJohnson_et_al_PRL_2011_055005.pdf",
        "watermark_text": "We present new exact half - BPS treatments to the small - energy effective action for type - IIB string theory in ten dimensions , which are favored by fluxes on an arbitrary number of intersecting D3 - branes . These solutions can be viewed as generalizations of the single Janus solution offered in 1 .We addition explore some properties of these solutions that were not discussed earlier . The first part of this study was publication as : 2 C . A . Johnson et al . , Phys .Rev.Lett.106 (2011) 055005.DOI: 10.1103/PhysRevLett.106.055005.URL : http : / / arxiv . org / abs / 1111 . 5389v2 . URL : http : / / www . sciencedirect . com / science / article / pii / S0167971011000203 .URL : https : / / inspirehep . net / record / 126898 / files / CAJohnson _ et _ al _ PRL _ 2011 _ 055005 . pdf",
        "rewrite_text": "We introduce new exact half-BPS solutions for the low-energy effective action of type IIB string theory in ten dimensions, which are enhanced by fluxes associated with an arbitrary number of intersecting D3-branes. These solutions can be seen as extensions of the single Janus solution presented previously. Additionally, we examine certain properties of these solutions that have not been addressed before. The initial part of this research was published in C. A. Johnson et al., Phys. Rev. Lett. 106 (2011) 055005, with the DOI: 10.1103/PhysRevLett.106.055005. For further details, you can access the following links: http://arxiv.org/abs/1111.5389v2, http://www.sciencedirect.com/science/article/pii/S0167971011000203, and https://inspirehep.net/record/126898/files/CAJohnson_et_al_PRL_2011_055005.pdf.",
        "ori-fast-z-score": 0.7453559924999299,
        "water-fast-z-score": 2.897472836319489,
        "rewrite-fast-z-score": 0.42008402520840293
    },
    {
        "original_text": "We present the theory for macroscopic quantum tunneling (MQT) in high-temperature superconductors with an emphasis on its application to c-axis junctions. We show that MQT is suppressed by thermal fluctuations and magnetic field, but can be enhanced by applying a bias current or increasing the junction area. The results are compared with experiments performed at 4 K and 77 K. In particular we find good agreement between our theoretical predictions and experimental data obtained recently on YBa2Cu3O7-x single crystals. \n \n Introduction \n \n Superconductivity was discovered more than half a century ago  1  . Since then many new materials have been found which exhibit this fascinating phenomenon  2  , including some with very high transition temperatures T_c  3  . However, despite intensive research efforts there still remain several open questions about the nature of these novel compounds  4  . One such question concerns the mechanism responsible for their unusual properties  5  . \n \n It has been suggested  6  that the pairing interaction may involve phonons  7 - 9  as well as spin excitations  10 - 12  . This leads to two possible scenarios for the formation of Cooper pairs  13  : either they form directly out of electrons via electron-phonon interactions  14  , or indirectly through spin-fluctuations  15  . These different mechanisms lead to distinct physical pictures  16  . For example, if one assumes that the pairing occurs only due to electron-phonon interactions  17  , it follows that the gap function should vanish along certain lines in momentum space  18  . On the other hand, if one considers the possibility of pair formation mediated by spin fluctuations  19  , the gap function vanishes over all momenta  20  . \nThe most important feature of both types of models is that they predict the existence of nodes  21  in the energy spectrum  22  . Nodes occur when the order parameter changes sign across the Fermi surface  23  . They were first predicted theoretically  24 - 26  and later observed experimentally  27  .",
        "watermark_text": "We present the model for macroscopic quantum tunneling ( MQT ) in high - temperature superconductors with an emphasis on its use to c - axis junctions . We see that MQT is suppressed by mechanical fluctuations and magnetic force , but can be enhanced by using a bias charge or increasing the junction region .The results are compared with experiments conducted at 4 K and 77 K . In particular we find good agreement between our theory estimates and observation information obtained recently on YBa2Cu3O7 - x single crystals . Introduction Superconductivity was known more than half a millennium later 1 .Since then many new materials have been found which exhibit this fascinating phenomenon 2 , notably some with very high transition conditions T _ c 3 . However , despite intensive study efforts there still continue several open questions about the nature of these novel substances 4 .One such issue concerns the process controlling for their extraordinary properties 5 . It has been proposed 6 that the pairing interaction may involve phonons 7 - 9 as well as spin excitations 10 - 12 .This leads to two possible possibilities for the formation of Cooper pairs 13 : either they create directly out of atoms via electron - phonon interactions 14 , or indirectly through spin - fluctuations 15 . These different processes lead to distinct physical pictures 16 .For instance , if one suppose that the pairing arises only due to atom - phonon coupling 17 , it appears that the gap expression should vanish along particular lines in momentum space 18 . On the other hand , if one considers the prospect of pair formation facilitated by spin fluctuations 19 , the gap expression vanishes over all momenta 20 .The most important feature of both types of models is that they predict the existence of nodes 21 in the energy spectrum 22 . Nodes happen when the order parameter moves sign across the Fermi surface 23 .They were first expected theoretically 24 - 26 and later observed experimentally 27 .",
        "rewrite_text": "We introduce a model for macroscopic quantum tunneling (MQT) in high-temperature superconductors, focusing specifically on c-axis junctions. Our findings indicate that MQT is inhibited by mechanical fluctuations and magnetic forces; however, it can be enhanced through the application of a bias charge or by expanding the junction area. We compare these results with experimental data collected at 4 K and 77 K, noting a strong concordance between our theoretical estimates and recent observations from YBa2Cu3O7-x single crystals. \n\nSuperconductivity has been recognized for over half a millennium. Since its discovery, numerous materials exhibiting this intriguing phenomenon have been identified, particularly those with very high critical temperatures (T_c). Despite extensive research, many questions remain regarding the nature of these unique substances. One key area of investigation revolves around the mechanisms underlying their remarkable properties. It has been suggested that pairing interactions may involve both phonons and spin excitations, leading to two potential scenarios for the formation of Cooper pairs: either through direct electron-phonon interactions from atoms or indirectly via spin fluctuations. These differing mechanisms yield distinct physical interpretations. For example, if one assumes that pairing occurs solely through electron-phonon coupling, the gap expression would vanish along certain lines in momentum space. Conversely, if pairing is influenced by spin fluctuations, the gap expression would diminish across all momenta. A critical aspect of both models is the prediction of nodes in the energy spectrum, which arise when the order parameter changes sign across the Fermi surface. These nodes were initially expected through theoretical work and have since been confirmed through experimental observations.",
        "ori-fast-z-score": -0.34050261230349943,
        "water-fast-z-score": 7.37443916112788,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an efficient distributed algorithm for implementing the most common query operations in relational databases, namely selection and projection. The proposed approach is based on the concept of data partitioning which allows us to distribute the workload among different nodes in order to minimize communication costs between them. We show that our method can be used as a building block for more complex queries such as joins or aggregates. Finally we provide experimental results showing how our solution outperforms existing approaches by orders of magnitude when dealing with large datasets. Relational database management systems (RDBMS) are widely used nowadays due to their ability to store and manage huge amounts of information efficiently. However, this efficiency comes at the cost of high computational complexity since all the data has to reside in main memory during processing time. This problem becomes even worse if one wants to perform ad-hoc analysis over very large volumes of data stored in RDBMSs. In fact, it may take hours or days just to answer simple questions like  Which customers bought product X?  or  What products were sold together last year? . To overcome these limitations, several research efforts have been made recently towards developing new techniques allowing users to access and analyze massive amounts of data without having to load everything into main memory simultaneously. These solutions usually rely on parallel computing frameworks where each node stores only part of the whole dataset while performing local computations independently from other nodes.",
        "watermark_text": "We suggest an efficient dispersed algorithm for applying the most common query operations in relational databases , namely choosing and projection . The proposed approach is based on the idea of data partitioning which allows us to organize the workload among different nodes in order to minimize transport fees between them .We see that our technique can be used as a building block for more sophisticated queries such as joins or aggregates . Finally we provide empirical results promising how our solution outperforms previous techniques by orders of magnitude when dealing with large datasets .Relational data management systems ( RDBMS ) are widely using today due to their capabilities to contain and manage huge amounts of content efficiently . However , this efficiency comes at the cost of high computational efficiency since all the information has to live in major memory during processing period .This problem arises even stronger if one wants to conduct ad - hoc evaluation over very huge volumes of statistics contained in RDBMSs . In reality , it could took days or months just to respond simple answers like Which customers bought brand X ?or What products were sell together last season ? .To solve these barriers , various study efforts have been made recently towards developing innovative techniques allowing consumers to access and retrieve massive sums of data without having to load everything into principal memory simultaneously . These solutions usually relies on parallel processing frameworks where each node stores only portion of the whole dataset while performing local computations separately from other connections .",
        "rewrite_text": "We propose an efficient distributed algorithm designed to facilitate the most common query operations in relational databases, specifically selection and projection. Our approach leverages data partitioning, which allows us to efficiently distribute the workload across multiple nodes, thereby reducing transportation costs between them. We believe that our method can serve as a foundational component for more complex queries, such as joins or aggregate functions. Additionally, we present empirical results demonstrating that our solution significantly outperforms previous techniques when handling large datasets. Relational database management systems (RDBMS) are extensively used today for their ability to manage and store vast amounts of data efficiently. However, this efficiency often leads to high computational demands, as all information must reside in primary memory during processing. This challenge becomes even more pronounced when conducting ad-hoc evaluations on large volumes of statistics within RDBMSs, with simple queries like \"Which customers bought brand X?\" or \"What products were sold together last season?\" potentially taking days or even months to resolve. To address these limitations, recent research has focused on developing innovative techniques that enable users to access and retrieve large volumes of data without loading everything into main memory at once. These solutions typically employ parallel processing frameworks, wherein each node stores only a portion of the overall dataset while conducting local computations independently of other nodes.",
        "ori-fast-z-score": -1.7277368511627202,
        "water-fast-z-score": 8.410966704383963,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The background radiation in space is dominated by cosmic rays and their secondary products, such as neutrons and gamma-rays. The most important source of these particles are galactic supernovae which occur at an average rate of one per century. In this work we present results on the background radiation expected to be measured with the pn-CCDs (p-type silicon charge-coupled devices) that will be used in the CERN Axion Solar telescope (CAST). We have simulated the response of CAST s detectors using GEANT4 Monte Carlo simulations. These were then combined with models of the particle fluxes in space to predict the background count rates observed by the cameras. Our predictions show that the background count rate due to cosmic ray interactions should not exceed 0.1 counts s-1 pixel-1 over the entire field-of-view of each camera. This corresponds to less than 1% of the signal expected from axions produced in the Sun s magnetic field.",
        "watermark_text": "The background radiation in space is dominated by gamma radiation and their secondary products , such as neutrons and alpha - rays . The most important source of these ions are galactic supernovae which occur at an estimated rate of one per century .In this research we present results on the background radiation anticipated to be recorded with the pn - CCDs ( p - class silicon charge - coupled devices ) that will be used in the CERN Axion Solar observatory ( CAST ) . We have modelled the response of CAST s detectors using GEANT4 Monte Carlo simulations .These were then combined with models of the particle fluxes in space to predict the background count levels observed by the cameras . Our predictions show that the background count rate due to cosmic ray molecules should not reach 0 . 1 counts s - 1 pixel - 1 over the entire field - of - view of each camera .This corresponds to little than 1 % of the signal expected from axions produced in the Sun s magnetic force .",
        "rewrite_text": "The background radiation in space primarily consists of gamma radiation along with secondary products like neutrons and alpha particles. The key contributor to these particles is galactic supernovae, which are estimated to occur roughly once every century. In this study, we present findings regarding the background radiation anticipated to be detected by the pn-CCDs (p-class silicon charge-coupled devices) that will be utilized in the CERN Axion Solar Telescope (CAST). We modeled the response of CAST's detectors using GEANT4 Monte Carlo simulations, which were then integrated with models of particle fluxes in space to forecast the background count levels that the cameras will observe. Our predictions indicate that the background count rate from cosmic ray particles should remain below 0.1 counts per second per pixel across the entire field of view of each camera. This value represents less than 1% of the signal expected from axions generated by the Sun’s magnetic fields.",
        "ori-fast-z-score": 1.4552137502179978,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 1.3242443839434612
    },
    {
        "original_text": "We present results on the volume operator in loop quantum gravity (LQG). The volume operator is an important ingredient for many physical applications, such as black hole entropy and cosmological perturbations. We show that it can be written as a sum over spin network states with coefficients which are determined by the geometry of the underlying graph. In particular we find that the spectrum of this operator agrees exactly with the one obtained using group field theory methods. This provides further evidence for the equivalence between LQG and group field theory at the level of operators. Finally, we discuss how to use these results to compute expectation values of the volume operator in semiclassical states. These results will appear elsewhere. DOI: 10.1088/1742-5468/2009/06/P06005. URL: http://arxiv.org/abs/0906.0571. PACS numbers: 04.20.-q, 11.25.Wx",
        "watermark_text": "We present results on the volume operator in loop quantum gravitational ( LQG ) . The volume operator is an important ingredient for numerous physical applications , such as black hole entropy and cosmological perturbations .We see that it can be written as a sum over spinning system states with coefficients which are decided by the topology of the underlying graph . In particular we find that the spectrum of this operator agrees exactly with the one achieved using group field theory techniques .This offers further evidence for the equivalence between LQG and group field theory at the level of operators . Finally , we talk how to use these results to compute expectation values of the volume operator in semiclassical states .These conclusions will appear elsewhere . DOI : 10 . 1088 / 1742 - 5468 / 2009 / 06 / P06005 .URL : http : / / arxiv . org / abs / 0906 . 0571 . PACS numbers : 04 . 20 . - q , 11 . 25 . Wx",
        "rewrite_text": "We present findings on the volume operator within loop quantum gravity (LQG). This operator plays a crucial role in various physical contexts, including black hole entropy and cosmological perturbations. Our analysis reveals that the volume operator can be expressed as a summation over states of spinning systems, with the coefficients determined by the topology of the associated graph. Notably, we discover that the spectrum of this operator precisely matches the one obtained through group field theory methods. This outcome provides additional support for the equivalence of LQG and group field theory at the operator level. Lastly, we discuss how these results can be used to calculate expectation values of the volume operator in semiclassical states. Further details on these conclusions will be published elsewhere. DOI: 10.1088/1742-5468/2009/06/P06005. URL: http://arxiv.org/abs/0906.0571. PACS numbers: 04.20.-q, 11.25.Wx.",
        "ori-fast-z-score": -0.1259881576697424,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "The hot intergalactic medium (IGM) is an important component in our understanding of galaxy formation, as it provides the fuel for star formation.  The IGM consists primarily of ionized hydrogen gas at temperatures between 10^6 K to 10^7 K with densities ranging from 1 atom/cm^3 to 100 atoms /cm^3.   In this talk we will discuss how observations of the Lyman-alpha forest can be used to probe the physical properties of the IGM on large scales.  We will also present results from recent work using data obtained by the Sloan Digital Sky Survey that show evidence for large-scale fluctuations in the temperature-density relation of the IGM. Finally, we will describe some current efforts aimed at measuring the thermal history of the universe through measurements of the kinetic SZ effect produced by clusters of galaxies. This research was supported by NASA grant NAG5-13286. The hot intergalactic medium is an important component in understanding galaxy formation because it provides the fuel for stars. It consists primarily of ionized Hydrogen gas at temperatures between 106K and 107K with densities ranging from 1atom/cm3 to 100atoms/cm3 .In this talk we will discuss observational probes of its physical properties on large-scales.We will present results showing evidence for large-scale fluctuations in teh temperature density relationship of the IGM based on data taken by the SDSS survey.Finally ,we will describe some current efforts to measure the thermal history of the Universe via measurement of the kinetic SZ effect generated by clusters of galaxies.This research has been funded by NASA grant NAG 5-13286",
        "watermark_text": "The warm intergalactic medium ( IGM ) is an important element in our science of galaxy formation , as it supplies the fuel for star formation . The IGM consists primarily of ionized hydrogen liquid at conditions between 10 ^ 6 K to 10 ^ 7 K with densities varied from 1 molecule / cm ^ 3 to 100 atoms / cm ^ 3 .In this talk we will explore how measurements of the Lyman - alpha forest can be used to probe the physical properties of the IGM on huge scales . We will also discuss results from recent work using data derived by the Sloan Digital Sky Survey that demonstrate proof for large - scale fluctuations in the temperature - density relation of the IGM .Finally , we will explain some latest attempts aimed at assessing the thermal evolution of the universe through measurements of the kinetic SZ phenomenon produced by clusters of galaxies . This research was supported by NASA loan NAG5 - 13286 .The warm intergalactic medium is an important element in understanding galaxy formation because it supplies the fuel for stars . It consists primarily of ionized Hydrogen plasma at levels between 106K and 107K with densities varied from 1atom / cm3 to 100atoms / cm3 . In this talk we will explore observational probes of its physical properties on huge - scales . We will present results promising confirmation for large - scale fluctuations in teh heat density correlation of the IGM based on evidence drawn by the SDSS sample . Finally , we will explain some latest attempts to measure the thermal evolution of the Universe via measurement of the kinetic SZ phenomenon generated by clusters of stars . This research has been sponsored by NASA grant NAG 5 - 13286",
        "rewrite_text": "The warm intergalactic medium (IGM) plays a crucial role in our understanding of galaxy formation, as it serves as the primary fuel for star formation. The IGM is mainly composed of ionized hydrogen plasma, existing at temperatures ranging from 10^6 K to 10^7 K, with densities varying between 1 atom/cm^3 and 100 atoms/cm^3. In this presentation, we will investigate how measurements of the Lyman-alpha forest can be utilized to examine the physical properties of the IGM on large scales. We will also present findings from recent analyses using data from the Sloan Digital Sky Survey, which provide evidence for significant large-scale fluctuations in the temperature-density relation of the IGM. Finally, we will discuss recent efforts to evaluate the thermal evolution of the universe through observations of the kinetic Sunyaev-Zeldovich (SZ) effect produced by galaxy clusters. This research was funded by NASA grant NAG5-13286.",
        "ori-fast-z-score": -0.7492686492653552,
        "water-fast-z-score": 7.553288946746542,
        "rewrite-fast-z-score": 2.0768805540571886
    },
    {
        "original_text": "We present the results of subjective evaluation experiments conducted on forms designed for use within immersive environments, such as virtual reality (VR) and augmented reality (AR). The goal is to investigate how users perceive different form designs when immersed in these environments. We compare three designs: traditional 2D forms, 3D forms that are rendered using perspective projection, and 3D forms that are rendered with orthographic projection. Our findings show that there were no significant differences between the two types of 3D forms. However, both 3D forms received significantly higher ratings than their 2D counterparts. This suggests that 3D forms can be used effectively in immersive environments without requiring special rendering techniques or additional hardware. In addition, we found that participants preferred forms that had more visual cues indicating depth information over those that did not have any such cues. Finally, our study also shows that it may be possible to create effective forms by combining elements from multiple existing designs.",
        "watermark_text": "We present the results of subjective assessment research conducted on forms designed for use within immersive environments , such as virtual reality ( VR ) and augmented reality ( AR ) . The goal is to examine how people interpret different form shapes when immersed in these habitats .We contrast three models : traditional 2D forms , 3D shapes that are rendered using view projection , and 3D shapes that are rendered with orthographic projection . Our findings show that there were no considerable variations between the two forms of 3D shapes .However , both 3D shapes took significantly greater grades than their 2D counterparts . This shows that 3D shapes can be used effectively in immersive environments without using special graphics techniques or additional hardware .In addition , we concluded that participants favored forms that had more graphical cues implying depth knowledge over those that did not have any such cues . Finally , our research also shows that it could be possible to create effective forms by combining components from multiple older patterns .",
        "rewrite_text": "We present findings from a subjective assessment study focused on forms designed for immersive environments like virtual reality (VR) and augmented reality (AR). The objective was to investigate how people perceive various form shapes when engaged in these settings. We compared three types of models: traditional 2D forms, 3D shapes rendered via view projection, and 3D shapes rendered using orthographic projection. Our results indicated that there were no significant differences between the two types of 3D shapes. However, both forms of 3D shapes received significantly higher ratings compared to their 2D versions, suggesting that 3D shapes are effective in immersive environments without the need for advanced graphics techniques or additional hardware. Furthermore, we found that participants preferred shapes with more graphical cues indicating depth over those lacking such cues. Lastly, our research suggests that effective forms can be created by combining elements from various traditional patterns.",
        "ori-fast-z-score": -1.1043152607484654,
        "water-fast-z-score": 7.067617668790178,
        "rewrite-fast-z-score": -0.22360679774997896
    },
    {
        "original_text": "We propose an image stabilizing method for video capturing in turbulent environments, which is based on the super-resolution technique. The proposed method can be used to improve the quality of captured videos with handheld cameras or other devices that are subject to motion blur caused by turbulence. We first estimate the camera motion using optical flow between consecutive frames. Then we use this information as input data into our super-resolution algorithm to generate high resolution images. Finally, these images are combined together to produce a stable output frame. Our experimental results show that the proposed method outperforms existing methods both visually and quantitatively. \n \n Keywords: Real time video processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization. 1 Introduction \n \n In recent years there has been growing interest in developing techniques for real time video processing applications such as video surveillance systems  1  , traffic monitoring  2  , remote sensing  3  . However, most of these applications require capturing clear images under challenging conditions like low-light illumination  4  , fast moving objects  5  , blurry scenes  6  , etc.. Among all these challenges, one of the major problems is how to deal with the motion blur caused by turbulence  7  8  9  when capturing videos with hand-held cameras or other devices  10  . \n \n Figure 1: An example of a video sequence taken at nighttime (a) and its corresponding ground truth (b).",
        "watermark_text": "We suggest an image stabilizing method for video capturing in volatile settings , which is based on the ultra - resolution technique . The proposed approach can be used to improve the performance of released movies with handheld cameras or other devices that are subject to moving fade caused by turbulence .We first estimate the image movement utilizing optical flow between successive frames . Then we utilize this data as input data into our super - resolution algorithm to create long resolution photos .Finally , these images are fused together to produce a consistent output frame . Our research results show that the suggested method outperforms current methods both physically and quantitatively .Keywords : Real time television processing ; Turbulence ; Motion blur ; Optical stream ; Super - resolution ; Image stabilization . 1 Introduction In recent years there has been growing interest in developing procedures for real time television processing applications such as video monitoring systems 1 , road monitoring 2 , remote sensing 3 .However , most of these uses involve capturing clear photographs under difficult environments like low - light lighting 4 , fast move images 5 , blurry scenes 6 , etc . . Among all these problems , one of the main problems is how to deal with the movement blur caused by turbulence 7 8 9 when capturing clips with hand - held photographers or other devices 10 .Figure 1 : An example of a video sequence taken at nighttime ( a ) and its equivalent ground truth ( b ) .",
        "rewrite_text": "We propose a method for stabilizing images in video captures within dynamic environments, utilizing ultra-resolution techniques. This approach aims to enhance the quality of films shot with handheld cameras or other devices affected by motion blur due to turbulence. Our method begins by estimating image displacement through optical flow analysis between consecutive frames. We then feed this information into our super-resolution algorithm to generate high-resolution images. Finally, these images are combined to create a coherent output frame. Our research findings indicate that the proposed method significantly surpasses existing techniques, both in physical performance and quantitative measures. \n\n**Keywords:** Real-time television processing; Turbulence; Motion blur; Optical flow; Super-resolution; Image stabilization.\n\n**1 Introduction**  \nIn recent years, there has been an increasing focus on developing techniques for real-time television processing applications, including video monitoring systems, road surveillance, and remote sensing. However, many of these applications require the capture of clear images in challenging conditions, such as low-light environments, fast-moving scenes, and blurry visuals. Among these challenges, managing motion blur resulting from turbulence during video recording with handheld devices is particularly critical. \n\n**Figure 1:** An example of a nighttime video sequence (a) alongside its corresponding ground truth (b).",
        "ori-fast-z-score": -0.8181818181818182,
        "water-fast-z-score": 7.483857967739688,
        "rewrite-fast-z-score": -1.958260097304659
    },
    {
        "original_text": "We present new measurements of the mean and scatter in the velocity dispersion-optical richness relation (VRR) for galaxy clusters selected by their red-sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use two different methods to measure cluster velocities, one based on spectroscopic observations of member galaxies and another that uses photometric redshift estimates of all galaxies within a fixed aperture centered on each cluster s brightest cluster galaxy (BGG).\nThe results are consistent with previous studies at low-redshift but show significant differences when compared to recent work done at higher redshifts. The discrepancies between our results and those presented in earlier works may be due to systematic effects associated with the measurement techniques used or could indicate evolution in the VRR over time. In either case, these results highlight the need for further investigation into this relationship as well as other scaling relations involving galaxy clusters. \n \n Keywords: galaxy cluster, optical richness",
        "watermark_text": "We create additional measurements of the mean and scatter in the velocity dispersion - optical richness relation ( VRR ) for galaxy galaxies designated by their red - sequence galaxies using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use two different methods to measure cluster velocities , one based on spectroscopic observations of member galaxies and another that using photometric redshift estimates of all galaxies within a fixed aperture concentrated on each cluster s brightest cluster galaxy ( BGG ) .The results are compatible with previous research at low - redshift but demonstrate considerable variations when compared to recent work done at higher redshifts . The discrepancies between our findings and those offered in earlier works might be due to systematic effects involved with the measurement methods used or could indicate evolution in the VRR over time .In either case , these results emphasize the necessity for further investigation into this relationship as well as other scaling connections involving galaxy regions . Keywords : galaxy region , optical richness",
        "rewrite_text": "We provide additional measurements of the mean and scatter in the velocity dispersion - optical richness relation (VRR) for red-sequence galaxies, utilizing data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our analysis employs two distinct methods to assess cluster velocities: one based on spectroscopic observations of member galaxies and another that relies on photometric redshift estimates for all galaxies located within a fixed aperture centered on each cluster's brightest galaxy (BGG). The findings align with previous studies at low redshifts but exhibit significant variations when compared to more recent research conducted at higher redshifts. The differences between our results and those from earlier studies may stem from systematic effects inherent in the measurement techniques or could suggest an evolution in the VRR over time. In any case, these results underscore the need for further research into this relationship and other scaling connections involving galaxy clusters. Keywords: galaxy clusters, optical richness.",
        "ori-fast-z-score": 1.835325870964494,
        "water-fast-z-score": 7.111887749987414,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present an overview of recent progress in the development and application of techniques to cool molecules by exploiting their interaction with optical cavities. We discuss how these methods can be used to prepare samples of cold, trapped molecules that are suitable for precision measurements or quantum information processing applications. In particular we focus on two different approaches which have been developed recently at our laboratory: (i) The use of electromagnetically induced transparency (EIT), combined with stimulated Raman adiabatic passage (STIRAP), to produce large numbers of optically trapped ground-state polar molecules. (ii) Cavity-enhanced photoassociation spectroscopy as a tool to study ultracold collisions between alkali-metal atoms. Finally, we briefly outline some possible future directions for this research area. Molecules offer many advantages over atomic systems when it comes to implementing novel quantum technologies such as high-precision metrology  1  , quantum simulation  2  , and quantum networks  3  . However, most molecular species cannot be directly cooled using conventional laser cooling schemes because they lack closed cycling transitions  4  .\nIn order to overcome this problem several alternative cooling strategies have been proposed  5  -  8  . One promising approach is based on the combination of electromagnetically-induced transparency (EIT)  9  and stimulated Raman adiabatic passages (STIRAP)  10  . This method has been successfully applied to create dense ensembles of ground state polar molecules  11  -  13  . Another possibility consists in trapping molecules via photoassociative processes  14  -  16  . Here one exploits the fact that the spontaneous emission rate into bound states increases exponentially with decreasing temperature  17  . By coupling the excited molecular levels to high-finesse optical cavities  18  -  20  , the resulting increase in radiative lifetime leads to efficient trapping  21  -  23  . These techniques allow us to trap up to 10 5 molecules per cm 3 inside a single-mode optical resonator  24  .",
        "watermark_text": "We illustrate an overview of recent progress in the development and application of techniques to cool substances by exploiting their connections with optical cavities . We discuss how these systems can be used to produce specimen of cold , trapped molecules that are suitable for precision observations or quantum information processing applications .In particular we focus on two different methods which have been pioneered recently at our laboratory : ( i ) The using of electromagnetically induced transparency ( EIT ) , combined with stimulated Raman adiabatic entry ( STIRAP ) , to produce substantial quantity of optically captured ground - state polar compounds . ( ii ) Cavity - augmented photoassociation spectroscopy as a technique to study ultracold collisions between alkali - glass atoms .Finally , we briefly outline some possible future paths for this research field . Molecules offer several advantages over nuclear systems when it comes to incorporating novel quantum technologies such as high - precision metrology 1 , quantum modeling 2 , and quantum networks 3 .However , most biological species cannot be directly warmed utilizing conventional optical cooling schemes because they lack locked cycling transitions 4 . In try to overcome this situation several alternative heating methods have been proposed 5 - 8 .One promising alternative is based on the combination of electromagnetically - triggered transparency ( EIT ) 9 and stimulated Raman adiabatic passages ( STIRAP ) 10 . This method has been successfully utilized to create tight groups of ground state polar interactions 11 - 13 .Another possibility consists in trap molecules via photoassociative processes 14 - 16 . Here one utilizes the fact that the spontaneous emission speed into bound products increases exponentially with decreasing temperature 17 .By coupling the excited molecular levels to large - finesse optical cavities 18 - 20 , the resulting increase in radiative duration results to efficient capturing 21 - 23 . These methods able us to trapping up to 10 5 molecules per cm 3 inside a single - mode optical resonator 24 .",
        "rewrite_text": "We provide an overview of recent advancements in the development and application of techniques that cool substances through their interactions with optical cavities. We explore how these systems can yield cold, trapped molecules suitable for precision measurements and quantum information processing applications. Our focus is particularly on two innovative methods recently pioneered in our laboratory: (i) Utilizing electromagnetically induced transparency (EIT) in conjunction with stimulated Raman adiabatic passage (STIRAP) to produce significant quantities of optically captured ground-state polar molecules, and (ii) Cavity-enhanced photoassociation spectroscopy as a means to investigate ultracold collisions between alkali-metal atoms. Additionally, we briefly discuss potential future directions for this research area. Molecules present several benefits over atomic systems for integrating novel quantum technologies, including high-precision metrology, quantum simulation, and quantum networking. However, many biological species cannot be directly cooled using conventional optical techniques due to the absence of closed cycling transitions. To address this limitation, several alternative cooling methods have been proposed. One promising approach combines electromagnetically induced transparency (EIT) with stimulated Raman adiabatic passage (STIRAP). This method has successfully created tightly bound groups of ground-state polar molecules. Another approach involves trapping molecules via photoassociative processes, leveraging the significant increase in spontaneous emission rates into bound states as temperature decreases. By coupling excited molecular states to high-finesse optical cavities, we achieve an increase in radiative lifetimes that facilitates efficient trapping. These techniques enable us to trap up to 10^5 molecules per cubic centimeter within a single-mode optical resonator.",
        "ori-fast-z-score": 0.48989794855663565,
        "water-fast-z-score": 8.707536508851407,
        "rewrite-fast-z-score": -0.2508726030021272
    },
    {
        "original_text": "We have carried out an extensive search for periodicities in the light curves of all known spectroscopic binary systems with components classified as A-type stars (AAB, AAO, AB) by using the phase dispersion minimization method.  We found that two of these systems are indeed double-mode Cepheids, while another one is probably a single-mode Cepheid. The remaining three systems show no significant periodicity at all. In addition to this we report on the discovery of a new multiperiodic Delta-Scuti variable which shows several frequencies between 0.5 and 1 d-1. This work was supported by the Hungarian OTKA grant K-81345. -The first column gives the name of the system; the second column lists the spectral type of each component according to SIMBAD database; the third column contains the orbital period taken from the literature; the fourth column indicates whether or not there exists any evidence for pulsation; the fifth column gives references where more information can be obtained about the individual objects.",
        "watermark_text": "We have carried out an extensive search for periodicities in the light curves of all known spectroscopic binary systems with elements categorized as A - class stars ( AAB , AAO , AB ) by using the phase dispersion minimization techniques . We determined that two of these systems are indeed double - mode Cepheids , while another one is probably a single - mode Cepheid .The remaining three systems show no considerable periodicity at all . In addition to this we publish on the discovery of a new multiperiodic Delta - Scuti variable which gives numerous frequencies between 0 . 5 and 1 d - 1 .This project was supported by the Hungarian OTKA funding K - 81345 . - The first column gives the naming of the scheme ; the second column shows the spectral type of each component according to SIMBAD website ; the third column contains the orbital period taken from the literature ; the third column denotes whether or not there exists any evidence for pulsation ; the fifth column gives references where more information can be obtained about the specific elements .",
        "rewrite_text": "We conducted a comprehensive investigation into the periodicities of the light curves for all known spectroscopic binary systems classified as A-class stars (AAB, AAO, AB), employing phase dispersion minimization techniques. Our analysis revealed that two of these systems are confirmed double-mode Cepheids, while one is likely a single-mode Cepheid. The other three systems did not exhibit any significant periodicity. Additionally, we announce the discovery of a new multiperiodic Delta Scuti variable, which displays multiple frequencies ranging from 0.5 to 1 d⁻¹. This project was supported by Hungarian OTKA funding K-81345. The first column lists the naming scheme, the second column indicates the spectral type of each component according to the SIMBAD database, the third column presents the orbital period sourced from literature, the fourth column assesses the evidence for pulsation, and the fifth column provides references for additional information on the specific properties.",
        "ori-fast-z-score": 1.0681034923744679,
        "water-fast-z-score": 5.5778737935111105,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present Hubble Space Telescope (HST) and Spitzer Space Telescope observations of the edge-on circumstellar disk surrounding 2MASS J04414489+2512172, a young brown dwarf with spectral type M8 located in Upper Scorpius at a distance of 145 pc. The HST data reveal that this object is surrounded by a bright ring-like structure extending to ~0.5′′ (~120 AU). We find evidence for two spiral arms emerging from the inner part of the ring toward its center. These features are also seen in near-infrared images obtained with the adaptive optics system NACO on VLT/UT4. In addition, we detect several knots along these spirals which may be caused by dust clumps or planetesimals embedded within them. Our results suggest that the observed structures could have been formed through gravitational instability triggered by rapid inward migration of solids due to gas drag forces.",
        "watermark_text": "We present Hubble Space Telescope ( HST ) and Spitzer Space Telescope observations of the edge - on circumstellar disk surrounding 2MASS J04414489 + 2512172 , a young brown giant with spectral category M8 situated in Upper Scorpius at a distance of 145 pc . The HST results show that this body is surrounded by a bright ring - like structure extending to ~ 0 . 5 ′ ′ ( ~ 120 AU ) .We get information for two spiral bodies emerging from the inner part of the circle toward its core . These features are also shown in near - infrared images obtained with the adaptive optics scheme NACO on VLT / UT4 .In addition , we find various threads along these spirals which may be caused by dust clumps or planetesimals contained within them . Our results propose that the known structures could have been formed through gravity instability caused by rapid inward movement of solids due to gas drag forces .",
        "rewrite_text": "We present observations from the Hubble Space Telescope (HST) and the Spitzer Space Telescope of the edge-on circumstellar disk surrounding 2MASS J04414489+2512172, a young brown dwarf classified as M8, located in Upper Scorpius and situated 145 parsecs away. The HST findings reveal that this object is encircled by a bright ring-like structure extending approximately 0.5″ (~120 AU). We observe two spiral features emerging from the inner region of the disk towards its center. Near-infrared images, captured using the adaptive optics system NACO on the VLT/UT4, also display these features. Furthermore, we identify various threads along the spirals, which may originate from dust clumps or planetesimals within the disk. Our results suggest that the observed structures may have formed as a result of gravitational instabilities induced by the rapid inward movement of solids influenced by gas drag forces.",
        "ori-fast-z-score": 0.629940788348712,
        "water-fast-z-score": 5.921443410477893,
        "rewrite-fast-z-score": 1.9205531989934397
    },
    {
        "original_text": "We present an extensive study on the mode stability properties of delta Scuti (δ Sct) pulsators, based on both theoretical and observational results obtained for open clusters with ages between 1 Myr and 2 Gyr. We have performed detailed non-radial stellar oscillation calculations using state-of-the-art evolutionary models that include overshooting at convective boundaries as well as microscopic diffusion processes. The main goal is to investigate how the observed frequency pattern changes during evolution due to the effects of rotation-induced mixing and chemical composition gradients. In particular we focus our attention on the so-called mixed modes which are trapped in the region where the hydrogen burning shell overlaps with the helium core. These modes show very characteristic features such as large amplitudes and high degree of nonlinearity. Our results indicate that these modes can be excited by turbulent pressure fluctuations associated with the convection zone located near the surface layers of the star. Moreover, they also suggest that the excitation mechanism may change significantly when the star evolves off the ZAMS towards higher luminosities.",
        "watermark_text": "We present an extensive research on the mode equilibrium properties of delta Scuti ( δ Sct ) pulsators , relying on both theoretical and observational results collected for open clusters with periods between 1 Myr and 2 Gyr . We have done extensive non - radial stellar oscillation calculations including state - of - the - art evolutionary predictions that include overshooting at convective boundaries as well as microscopic migration effects .The main goal is to examine how the seen frequency behavior changes during evolution owing to the effects of rotation - mediated mixing and chemical composition gradients . In particular we focus our focus on the so - called mixture modes which are trapped in the location where the hydrogen burning shell overlaps with the helium core .These modes display very typical characteristics such as huge amplitudes and large extent of nonlinearity . Our results show that these modes can be excited by turbulent volume fluctuations associated with the convection zone situated near the surface layers of the star .Moreover , they thus suggest that the excitation process may change considerably when the star evolves off the ZAMS towards higher luminosities .",
        "rewrite_text": "We present a comprehensive study of the mode equilibrium properties of delta Scuti (δ Sct) pulsators, utilizing both theoretical and observational data gathered from open clusters with ages ranging from 1 million to 2 billion years. Our research includes detailed non-radial stellar oscillation calculations, incorporating advanced evolutionary models that take into account overshooting at convective boundaries and microscopic migratory effects. The primary aim of this study is to investigate how the observed frequency behavior evolves, influenced by rotation-mediated mixing and gradients in chemical composition. Specifically, we concentrate on the so-called mixture modes, which are confined to the region where the hydrogen-burning shell overlaps with the helium core. These modes exhibit distinct features, including significant amplitudes and a high degree of nonlinearity. Our findings indicate that these modes can be excited by turbulent volume fluctuations linked to the convection zone located in the star's outer layers. Additionally, our results imply that the excitation process may undergo substantial changes as the star evolves away from the Zero Age Main Sequence (ZAMS) toward higher luminosity states.",
        "ori-fast-z-score": -1.3093073414159544,
        "water-fast-z-score": 4.74464202095129,
        "rewrite-fast-z-score": 1.7253243712550146
    },
    {
        "original_text": "We report the discovery of two low-luminosity quasars (LLQs) with redshifts ~4 in the Sloan Digital Sky Survey Data Release 7 quasar catalog. The LLQs are spectroscopically peculiar and have very weak emission lines compared to typical quasars at similar redshift. We find that their optical-to-infrared spectral energy distributions can be well-fitted by an accretion disk model with extremely high black hole masses (M BH >10 10 M sun ) and Eddington ratios (L/L edd >0.1). These results suggest that these objects may represent a new class of supermassive black holes which grow rapidly through gas-rich mergers or interactions during early cosmic epochs. Keywords: Black Hole Mass, Accretion Disk Modeling, Supermassive Black Holes, Quasars, Redshift 4, Optical-Infrared Spectral Energy Distributions",
        "watermark_text": "We report the discovery of two low - luminosity quasars ( LLQs ) with redshifts ~ 4 in the Sloan Digital Sky Survey Data Release 7 quasar catalog . The LLQs are spectroscopically strange and have very low emission lines relative to normal quasars at comparable redshift .We see that their infrared - to - infrared spectral power distributions can be well - suited by an accretion disk model with incredibly high black hole masses ( M BH > 10 10 M sun ) and Eddington ratios ( L / L edd > 0 . 1 ) . These data suggest that these objects may constitute a new category of supermassive black holes which grow dramatically through gas - rich mergers or relationships during ancient universe epochs .Keywords: Black Hole Mass, Accretion Disk Modeling, Supermassive Black Holes, Quasars, Redshift 4, Optical-Infrared Spectral Energy Distributions",
        "rewrite_text": "We present the discovery of two low-luminosity quasars (LLQs) with redshifts around 4, identified in the Sloan Digital Sky Survey Data Release 7 quasar catalog. These LLQs exhibit unusual spectral characteristics, including significantly weaker emission lines compared to typical quasars at similar redshifts. Our analysis reveals that their infrared-to-infrared spectral power distributions can be effectively described by an accretion disk model featuring extremely high black hole masses (MBH > 10^10 M☉) and elevated Eddington ratios (L/LEdd > 0.1). These findings imply that these objects may represent a novel category of supermassive black holes that undergo substantial growth through gas-rich mergers or interactions during the early epochs of the universe. \n\nKeywords: Black Hole Mass, Accretion Disk Modeling, Supermassive Black Holes, Quasars, Redshift 4, Optical-Infrared Spectral Energy Distributions.",
        "ori-fast-z-score": -1.507556722888818,
        "water-fast-z-score": 4.221158824088691,
        "rewrite-fast-z-score": -0.13736056394868904
    },
    {
        "original_text": "We study the dynamics of an overdamped Brownian particle in a periodic potential with time-delayed feedback control, where the delay is introduced by a finite response time of the controller. We show that this system can be described as a flashing ratchet and derive analytical expressions for its current-voltage characteristics. The results are compared to numerical simulations. \nI. INTRODUCTIO N\nThe motion of particles driven out of equilibrium has been studied extensively over recent years  1  . In particular, it was shown that under certain conditions such systems may exhibit directed transport even without any net bias  2  , which is known as stochastic resonance  3  or coherence resonance  4  .\nIn many cases, however, these effects occur only if the noise level exceeds some threshold value  5  . This problem can be overcome using delayed feedback control  6  , i.e., controlling the system not instantaneously but after a fixed amount of time t d . It turns out that this method allows one to obtain directed transport at arbitrarily small noise levels  7, 8  . Moreover, it also works when the external driving force vanishes  9  . However, all previous studies were restricted to unidirectional potentials (i.e., potentials with reflection symmetry). Here we consider a more general case of a bidirectional potential  10  , which exhibits two local minima separated by a barrier.",
        "watermark_text": "We explore the dynamics of an overdamped Brownian particle in a periodic potential with time - slowed feedback control , where the delay is created by a finite reaction period of the controller . We see that this device can be described as a blinking ratchet and derive analytical expressions for its current - voltage parameters .The results are compared to numerical simulations . I . INTRODUCTIO N The movement of molecules driven out of equilibrium has been studied thoroughly over recent seasons 1 .In particular , it was shown that under certain conditions such systems may exhibit directed transport even without any gross bias 2 , which is known as stochastic resonance 3 or coherence resonance 4 . In many situations , however , these influences occur only if the signal level exceeds some threshold factor 5 .This problem can be overcome using delayed feedback control 6 , i . e . , governing the process not instantaneously but after a specified quantity of time t d . It turns out that this method enables one to obtain directed transport at arbitrarily small noise heights 7 , 8 .Moreover , it also acts when the external driving field vanishes 9 . However , all previous research were restricted to unidirectional potentials ( i . e . , potentials with reflection symmetry ) .Here we define a more general example of a bidirectional potential 10 , which possesses two local minima separated by a barrier .",
        "rewrite_text": "We investigate the behavior of an overdamped Brownian particle within a periodic potential, utilizing time-delayed feedback control to account for the finite reaction time of the controller. This system can be conceptualized as a blinking ratchet, and we derive analytical formulas for its current-voltage characteristics, which we then compare against numerical simulations. \n\nI. INTRODUCTION  \nThe dynamics of molecules driven out of equilibrium have been extensively studied in recent years. Notably, it has been demonstrated that such systems can display directed transport even in the absence of a significant bias, a phenomenon referred to as stochastic resonance or coherence resonance. However, this effect typically only manifests when the signal level surpasses a certain threshold. Delayed feedback control offers a solution to this issue by allowing the governing process to react not instantly but after a specified delay time \\( t_d \\). This approach enables directed transport to emerge at arbitrarily low noise levels and continues to function even when the external driving field is absent. Previous studies, however, have primarily focused on unidirectional potentials, which display reflection symmetry. In this work, we present a broader example involving a bidirectional potential characterized by two local minima separated by a barrier.",
        "ori-fast-z-score": -0.19802950859533489,
        "water-fast-z-score": 5.81344274116933,
        "rewrite-fast-z-score": -0.30460384954008574
    },
    {
        "original_text": "We study fluctuations of finite-time Lyapunov exponent (FTLE) for chaotic orbits on the standard map with periodic boundary conditions. We show that FTLE fluctuates around its mean value, which is determined by the largest unstable periodic orbit embedded into the chaotic attractor. The amplitude of these fluctuations decreases exponentially as time increases. In addition to this exponential decay we observe an algebraic tail at large times. This algebraic tail can be explained by the presence of small islands inside the chaotic sea. These results are confirmed numerically using different methods. \nI. INTRODUCTORY REMARK\nThe concept of finite-time Lyapunovexponent(FTLE), introduced by Wolf et al  1  , has been widely used recently  2  -  4  . It describes how fast nearby trajectories diverge or converge during some fixed period of time T . For example, if one considers two neighboring points x 0 = x(t 0 ) and y 0 = x(t 1 ), where t 0 < t 1 , then their separation after time T will be given by: \nwhere λ max > 0 is the maximum Lyapunov exponent characterizing the rate of divergence between nearby trajectories  5  .\nIn order to calculate the FTLE it is necessary to solve the following variational equation:\nwhere J is the Jacobian matrix corresponding to the flow generated by Eq. (1). If the initial condition z 0 = x t0 + εy t0 is close enough to the reference trajectory x t0 , i.e., |ε| ≪ 1, then the solution of Eq.(2) can be written as:\nwhere P n (T ; t 0 , t 1 ) is the nth-order Taylor polynomial expansion of the evolution operator U (T ; t 0 ). Then the FTLE can be calculated from:",
        "watermark_text": "We test fluctuations of finite - time Lyapunov exponent ( FTLE ) for turbulent orbits on the standard mapping with periodic border conditions . We see that FTLE fluctuates around its average value , which is chosen by the greatest chaotic periodic orbit integrated into the chaotic attractor .The amplitude of these fluctuations decreases exponentially as time rises . In addition to this exponential decay we encounter an algebraic tail at large times .This algebraic tail can be described by the formation of tiny islands inside the chaotic water . These conclusions are confirmed numerically using varying methods .I . INTRODUCTORY REMARK The concept of finite - time Lyapunovexponent ( FTLE ) , invented by Wolf et al 1 , has been widely using recently 2 - 4 .It expresses how rapid nearby trajectories diverge or converge during some fixed period of time T . For instance , if one considers two adjacent points x 0 = x ( t 0 ) and y 0 = x ( t 1 ) , where t 0 < t 1 , then their separation after time T will be shown by : where λ max > 0 is the maximum Lyapunov exponent characterizing the frequency of divergence between neighboring trajectories 5 .In order to estimate the FTLE it is required to solve the following variational equation : where J is the Jacobian matrix corresponding to the flow generated by Eq . ( 1 ) .If the first condition x 0 = x t0 + εy t0 is nearest sufficient to the reference trajectory x t0 , i . e . , | ε | [UNK] 1 , then the solve of Eq . ( 2 ) can be written as : where P n ( T ; t 0 , t 1 ) is the nth - order Taylor polynomial expansion of the evolution function U ( T ; t 0 ) .Then the FTLE can be determined from :",
        "rewrite_text": "We investigate the fluctuations of the finite-time Lyapunov exponent (FTLE) for turbulent orbits in the standard mapping with periodic boundary conditions. Our findings indicate that the FTLE fluctuates around its average value, which is determined by the most chaotic periodic orbit embedded in the chaotic attractor. As time progresses, the amplitude of these fluctuations decreases exponentially. Furthermore, we observe an algebraic tail at larger times, which can be attributed to the formation of small islands within the chaotic sea. These results are corroborated by numerical simulations using various methods. \n\nI. INTRODUCTORY REMARKS \n\nThe finite-time Lyapunov exponent (FTLE), introduced by Wolf et al., has gained significant attention in recent studies. It quantifies how quickly nearby trajectories either diverge or converge over a fixed time interval \\( T \\). For example, if we consider two adjacent points \\( x_0 = x(t_0) \\) and \\( y_0 = x(t_1) \\), with \\( t_0 < t_1 \\), their separation after time \\( T \\) can be expressed, where \\( \\lambda_{\\text{max}} > 0 \\) is the maximum Lyapunov exponent that describes the rate of divergence between neighboring trajectories. To estimate the FTLE, it is essential to solve the variational equation where \\( J \\) is the Jacobian matrix related to the flow described by the equation. Under the condition that \\( x_0 = x_{t_0} + \\epsilon y_{t_0} \\) is sufficiently close to the reference trajectory \\( x_{t_0} \\) (i.e., \\( |\\epsilon| \\ll 1 \\)), the solution to this equation can be expressed in terms of the \\( n \\)-th order Taylor polynomial expansion of the evolution function \\( U(T; t_0) \\). Consequently, the FTLE can be calculated from this relationship.",
        "ori-fast-z-score": -2.154554539378824,
        "water-fast-z-score": 5.511351921262151,
        "rewrite-fast-z-score": 0.30460384954008574
    },
    {
        "original_text": "We present an overview of our recent work on vector meson production in heavy ion collisions at RHIC and LHC energies, based on holographic QCD models with chiral symmetry breaking (AdS/QCD). We discuss how these models can be used to calculate hadronic observables such as transverse momentum spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear reactions. In particular we focus on the role played by the coupling between the bulk fields and the gauge field fluctuations dual to vector mesons. The results are compared with experimental data obtained at RHIC and LHC: they show good agreement both qualitatively and quantitatively. \n \n Keywords: Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality \n \n 1 Introduction \n \n One of the most exciting discoveries made recently at RHIC is that strongly interacting matter behaves like a nearly perfect fluid  1  . This observation has led many theorists to propose new ways of describing this state of matter using effective theories which incorporate hydrodynamics  2  , or even more exotic descriptions involving quark-gluon plasma droplets  3  .\n \nIn order to understand better what happens during the early stages of heavy-ion collisions it would be very useful if one could study experimentally the properties of the hot dense medium created in those collisions. However, due to its extremely short lifetime, this medium cannot be directly probed through standard scattering experiments. Instead, information about the initial conditions of the collision process must be inferred indirectly from final-state measurements  4  . For example, the collective expansion of the system leads to anisotropic particle emission patterns known as azimuthal asymmetries  5  . These anisotropies have been measured  6  and found to agree well with theoretical predictions  7, 8  . \n \n Another important observable characterizing the dynamics of the expanding fireball is the spectrum of emitted particles  9  . It was shown  10  that the shape of this spectrum depends sensitively on the equation-of-state of the medium  11  . Moreover, the observed suppression  12  of high-pT hadrons",
        "watermark_text": "We present an overview of our latest work on vector meson production in heavy atom collisions at RHIC and LHC energies , using on holographic QCD models with chiral symmetry breaking ( AdS / QCD ) . We discuss how these models can be used to estimate hadronic observables such as transverse acceleration spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear compounds .In particular we focus on the part played by the interaction between the bulk fields and the gauge field fluctuations dual to vector mesons . The results are compared with experimental evidence derived at RHIC and LHC : they show good agreement both qualitatively and quantitatively .Keywords : Vector Mesons , Heavy Ion Collisions , Holography , Chiral Symmetry Breaking , Gauge / Gravity Duality 1 Introduction One of the most exciting advances making lately at RHIC is that strongly interacting material behaves like a nearly perfect fluid 1 . This prediction has led many theorists to propose innovative ways of describing this state of matter utilizing effective models which employ hydrodynamics 2 , or especially more exotic representations featuring quark - gluon liquid droplets 3 .In order to explain better what comes during the early stages of large - ion collisions it would be very useful if one might explore experimentally the properties of the hot dense medium produced in those collisions . However , owing to its incredibly small life , this medium unable be closely probed through conventional absorption studies .Instead , info about the first environments of the collision mechanism must be inferred indirectly from final - state measurements 4 . For instance , the collective expansion of the process results to anisotropic particle emission events known as azimuthal asymmetries 5 .These anisotropies have been measured 6 and found to agree well with theoretical estimates 7 , 8 . Another important observable characterizing the dynamics of the increasing fireball is the spectrum of emission particles 9 .It was shown 10 that the form of this spectrum relies sensitively on the equation - of - state of the medium 11 . Moreover , the seen suppression 12 of high - pT hadrons",
        "rewrite_text": "We provide an overview of our recent research on vector meson production in heavy ion collisions at RHIC and LHC energies, employing holographic QCD models that incorporate chiral symmetry breaking (AdS/QCD). In this work, we explore how these models can be utilized to estimate hadronic observables, including transverse acceleration spectra and elliptic flow coefficients for light quarks and gluons produced in nuclear matter. Our focus is particularly on the interactions between bulk fields and gauge field fluctuations that correspond to vector mesons. We compare our results to experimental data from RHIC and LHC, finding a strong qualitative and quantitative agreement. \n\n**Keywords:** Vector Mesons, Heavy Ion Collisions, Holography, Chiral Symmetry Breaking, Gauge/Gravity Duality\n\n**1 Introduction**\n\nOne of the most significant developments at RHIC recently is the finding that strongly interacting matter behaves like a nearly perfect fluid. This observation has prompted theorists to explore new methods for describing this state of matter through effective models that utilize hydrodynamics, as well as more complex representations involving quark-gluon liquid droplets. To better understand the conditions present in the early stages of large ion collisions, it would be invaluable to experimentally investigate the properties of the hot, dense medium produced in such events. However, due to its incredibly brief lifespan, probing this medium through conventional absorption techniques is not feasible. Consequently, insights into the initial conditions of the collision must be drawn from final-state measurements. For example, the collective expansion during the collision leads to anisotropic particle emission, resulting in azimuthal asymmetries. These anisotropies have been measured and show strong agreement with theoretical predictions. Additionally, the particle emission spectrum serves as another crucial observable characterizing the dynamics of the expanding fireball, and it has been demonstrated that the shape of this spectrum is highly sensitive to the medium's equation of state. Furthermore, the observed suppression of high-pT hadrons adds another layer of complexity to the understanding of the dynamics at play.",
        "ori-fast-z-score": 0.8391813582966892,
        "water-fast-z-score": 8.391813582966892,
        "rewrite-fast-z-score": 1.1666666666666667
    },
    {
        "original_text": "We present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during previous eruption in the recurrent nova RS Oph (T Sco). We find that the observed X-ray light curve can be reproduced by assuming an initial mass loss rate of ~10-6 Msun/yr for the red giant component, which is consistent with theoretical predictions.  The predicted temperature structure of the shocked region agrees well with the observationally inferred one. Our model also predicts that the optical depth to X-rays should increase as time goes on because of the increasing density of the ejecta. This prediction seems to be supported by recent Swift/XRT observations. In addition we show that the observed UV fluxes are not explained by the standard steady state photoionization model but require additional heating source such as shocks or magnetic reconnection. Finally we discuss possible scenarios for future evolution of this system based on our numerical results.",
        "watermark_text": "We present results of our numerical simulations of the interaction between the fast stellar winds and the slow dense shell ejected during earlier eruption in the recurrent nova RS Oph ( T Sco ) . We see that the emitted X - ray light curve can be reproduced by assuming an initial mass loss rate of ~ 10 - 6 Msun / yr for the red dwarf component , which is compatible with theoretical estimates .The predicted temperature structure of the shocked areas agrees well with the observationally inferred one . Our model also predicts that the optical height to X - radiation should expand as time go on because of the increasing volume of the ejecta .This prediction seems to be supported by recent Swift / XRT studies . In addition we prove that the seen UV fluxes are not explained by the standard steady state photoionization theory but need extra heating source such as shocks or mechanical reconnection .Finally we explain possible strategies for future evolution of this scheme depending on our numerical findings .",
        "rewrite_text": "We present the findings from our numerical simulations examining the interaction between fast stellar winds and the slow, dense shell ejected during prior eruptions of the recurrent nova RS Oph (T Sco). Our results indicate that the X-ray light curve can be accurately modeled by assuming an initial mass loss rate of approximately \\(10^{-6} \\, M_{\\odot} \\, \\text{yr}^{-1}\\) for the red dwarf component, which aligns with theoretical predictions. The temperature structure predicted for the shocked regions closely matches what has been inferred from observations. Furthermore, our model suggests that the ratio of optical height to X-ray radiation is expected to increase over time due to the expanding volume of the ejecta, a prediction that appears to be supported by recent Swift/XRT observations. Additionally, we demonstrate that the observed UV fluxes cannot be accounted for by the standard steady-state photoionization theory alone, necessitating an additional heating source, such as shocks or mechanical reconnection. Finally, we outline potential strategies for the future evolution of this model based on our numerical findings.",
        "ori-fast-z-score": 0.11547005383792514,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 2.0855209398041166
    },
    {
        "original_text": "The detection of exoplanets has been one of the most exciting recent developments in astronomy, but their characterization is still incomplete. The goal of this thesis was to develop new techniques for characterizing exoplanetary systems using dynamical methods. In particular, I studied how planets can be used as probes into the formation history of planetary systems by measuring the mass distribution of small bodies (planetesimals) that are trapped in mean motion resonances with them.  My first project investigated the effect of planetesimal size on the stability of planetary orbits. Using numerical simulations, we found that smaller planetesimals tend to destabilize planetary orbits more quickly than larger ones because they have higher orbital eccentricities. This result suggests that there may exist an upper limit to the size of planetesimals that form during planet migration. My second project examined the effects of mutual inclinations between planets on the stability of planetary systems. We showed that mutual inclination increases the likelihood of instability when two planets are locked in a 2:1 resonance. Finally, my third project explored the possibility of detecting terrestrial planets around white dwarfs through gravitational microlensing events.",
        "watermark_text": "The observation of exoplanets has been one of the most exciting latest advances in science , but their characterization is still incomplete . The goal of this dissertation was to develop new strategies for characterizing exoplanetary regions using dynamical techniques .In particular , I studied how planets can be used as probes into the formation history of planetary structures by monitoring the mass distribution of tiny bodies ( planetesimals ) that are locked in mean motion resonances with them . My main project researched the impact of planetesimal size on the stability of planetary orbits .Using numerical simulations , we concluded that smaller planetesimals prefer to destabilize planetary orbits more easily than larger ones because they have greater orbital eccentricities . This result suggests that there may exist an upper limitation to the size of planetesimals that form during planet migration .My second work examined the effects of mutual inclinations between orbits on the stability of planetary systems . We showed that mutual inclination increases the probability of collapse when two planets are locked in a 2 : 1 resonance .Finally , my third project researched the prospect of detecting terrestrial worlds around white dwarfs through gravity microlensing events .",
        "rewrite_text": "The study of exoplanets has emerged as one of the most thrilling recent developments in science, yet their detailed characterization remains incomplete. This dissertation aimed to create new approaches for characterizing exoplanetary regions through dynamical techniques. Specifically, I explored how planets can serve as indicators of the formation history of planetary structures by examining the mass distribution of small bodies, known as planetesimals, that are in mean motion resonances with them. My primary research focused on the influence of planetesimal size on the stability of planetary orbits. Through numerical simulations, we found that smaller planetesimals tend to destabilize planetary orbits more readily than larger ones due to their higher orbital eccentricities. This finding implies there may be an upper size limit for planetesimals that form during the migration of planets. In my second project, I investigated the effects of mutual orbital inclinations on the stability of planetary systems, revealing that mutual inclination increases the likelihood of instability when two planets are in a 2:1 resonance. Lastly, my third project examined the potential for detecting terrestrial planets around white dwarfs through gravitational microlensing events.",
        "ori-fast-z-score": -0.42640143271122083,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 0.5360562674188973
    },
    {
        "original_text": "We present an optimal design method to find the best occulting mask that can be used in direct imaging searches for extrasolar planets.  The proposed method is based on the concept of entropy maximization, which has been widely applied in many fields such as information theory and statistical mechanics.   We show how this concept can be extended into the field of optics by introducing a new quantity called optical entropy (OE).   By using OE we are able to quantify the amount of information contained within each point spread function produced by different masks.   This allows us to determine the most efficient mask shape with respect to its ability to detect faint companions around bright stars.   ...  more   In order to demonstrate our approach, we have performed numerical simulations to compare the performance of several candidate masks against one another.   Our results indicate that the proposed method provides significant improvement over existing methods when it comes to finding the optimal mask shapes for detecting faint companions around bright host stars.",
        "watermark_text": "We suggest an appropriate design algorithm to find the best occulting mask that can be used in direct imaging searches for extrasolar planets . The proposed approach is based on the idea of entropy maximization , which has been widely applied in different fields such as data physics and statistical mechanics .We see how this concept can be generalized into the field of optics by using a new quantity called optical entropy ( OE ) . By using OE we are able to quantify the quantity of information stored within each point spread function generated by various masks .This enables us to predict the most efficient mask shape with regard to its capacity to identify dim companions around bright stars . . . . more In order to test our approach , we have done mathematical simulations to study the performance of several candidate masks against one another .Our results show that the suggested method provides significant progress over existing techniques when it comes to finding the ideal mask shapes for detecting bright companions around bright host stars .",
        "rewrite_text": "We present a design algorithm aimed at identifying the optimal occulting mask for direct imaging searches of extrasolar planets. Our approach leverages the principle of entropy maximization, a concept that has found applications across various disciplines, including data physics and statistical mechanics. We extend this principle into optics through the introduction of a new metric known as optical entropy (OE). By utilizing OE, we can quantify the information contained in each point spread function produced by different masks, allowing us to predict which mask shape is most effective for detecting faint companions near bright stars. To validate our methodology, we conducted mathematical simulations comparing the performance of several candidate masks. Our findings indicate that our proposed approach significantly outperforms existing methods in identifying optimal mask shapes for the detection of dim companions around luminous host stars.",
        "ori-fast-z-score": -0.10846522890932808,
        "water-fast-z-score": 6.399448505650358,
        "rewrite-fast-z-score": -0.23570226039551587
    },
    {
        "original_text": "We show that the common origin of linear and nonlinear chiral multiplets is related to the existence of an extra dimension, which can be either time-like or space-like. We also discuss how this picture fits into the context of string theory. Theories with extended supersymmetry are known to have many interesting features such as duality symmetries  1  . In particular, theories with four supercharges (N = 4) possess remarkable properties  2  , including self-duality  3  .\nIn recent years there has been renewed interest in studying these theories due to their connection with string/M-theory  4  . For example, it was shown  5  that type-IIB strings on AdS 5 × S 5 background  6  correspond to maximally supersymmetric Yang-Mills theory in four dimensions  7, 8  . This correspondence allows one to study strongly coupled gauge theories using weakly-coupled gravitational description  9  . It turns out that the same idea works for other types of field theories  10  -  12  .",
        "watermark_text": "We see that the common origin of linear and nonlinear chiral multiplets is related to the existence of an additional element , which can be either time - like or space - like . We also discuss how this picture fits into the context of string theory .Theories with extended supersymmetry are known to have many interesting features such as duality symmetries 1 . In particular , theories with four supercharges ( N = 4 ) possess noteworthy characteristics 2 , notably self - duality 3 .In recent years there has been continued interest in investigating these theories owing to their connection with string / M - theory 4 . For instance , it was shown 5 that type - IIB sequences on AdS 5 × S 5 background 6 belong to maximally supersymmetric Yang - Mills theory in four dimensions 7 , 8 .This connection allows one to study heavily coupled gauge fields use weakly - coupled gravitational description 9 . It turns out that the same idea works for other types of field theories 10 - 12 .",
        "rewrite_text": "The shared foundation of linear and nonlinear chiral multiplets is linked to the presence of an additional element, which can be either time-like or space-like. We also explore how this concept aligns with the framework of string theory. Theories featuring extended supersymmetry are recognized for their intriguing properties, including duality symmetries. Specifically, theories with four supercharges (N = 4) exhibit remarkable traits, particularly self-duality. In recent years, there has been a sustained interest in the exploration of these theories due to their relationship with string/M-theory. For example, it has been demonstrated that type-IIB sequences on the AdS5 × S5 background correspond to maximally supersymmetric Yang-Mills theory in four dimensions. This relationship enables the examination of strongly coupled gauge fields through a weakly coupled gravitational framework. Interestingly, this approach also applies to various other types of field theories.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "We present radiative transfer models for the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565. The model parameters are constrained by fitting to near-infrared (NIR) observations at 2.2 microns with NICMOS on HST as well as optical data obtained with WFPC-2. We find that both galaxies have significant amounts of dust distributed along their disks out to large distances above the midplane. In addition we detect an extended halo component around each galaxy which is best described by a spherical shell-like structure. For NGC 891 this component has a scale height of 1 kpc and extends up to 5 kpc above the disk plane. It contains about 10% of all dust mass within 10 kpc distance from the center. For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights. This work was supported by NASA grant NAG5-7085.",
        "watermark_text": "We generate radiative transfer estimates for the dust distributions in two edge - on spiral galaxies , NGC 891 and NGC 4565 . The model variables are constrained by fitting to near - infrared ( NIR ) observations at 2 . 2 microns with NICMOS on HST as well as imaging information obtained with WFPC - 2 .We see that both galaxies have considerable amounts of dust spread along their disks out to large distances above the midplane . In addition we locate an extended halo element around each galaxy which is better represented by a spherical shell - like structure .For NGC 891 this component has a scale width of 1 kpc and extends up to 5 kpc above the disk plane . It contains about 10 % of all dust mass within 10 kpc length from the center .For NGC 4565 our results suggest a more complex geometry where the dust density decreases exponentially towards larger heights . This work was supported by NASA grant NAG5 - 7085 .",
        "rewrite_text": "We conducted estimates of radiative transfer for the dust distributions in two edge-on spiral galaxies, NGC 891 and NGC 4565. The model variables were refined by fitting them to near-infrared (NIR) observations at 2.2 microns collected with the NICMOS instrument on the Hubble Space Telescope, alongside imaging data obtained from WFPC-2. Our findings indicate that both galaxies possess significant amounts of dust distributed along their disks, extending to considerable distances above the midplane. Additionally, we identified an extended halo component surrounding each galaxy, best described as a spherical shell-like structure. For NGC 891, this component has a scale height of 1 kpc and reaches up to 5 kpc above the disk plane, containing approximately 10% of the total dust mass within a 10 kpc radius from the center. In contrast, our results for NGC 4565 suggest a more intricate geometry, with dust density decreasing exponentially at greater heights. This research was funded by NASA grant NAG5-7085.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 3.3565855667130946,
        "rewrite-fast-z-score": 2.287331208629615
    },
    {
        "original_text": "The mechanical properties of hair are determined by its molecular structure, which is mainly composed of proteins called keratins. The main structural unit in keratins is an alpha-helical coiled coil that forms microfibrils with other similar units to form intermediate filaments (IFs). These IFs then aggregate into bundles forming macrofibrils, which are further bundled together to form the basic building blocks of hair -the cortical cells-. In this work we have studied how the mechanical properties change as these structures evolve during the process known as maturation or ageing. We used confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres extracted from human scalp hairs at different stages of their growth cycle. Our results show that the elastic modulus increases significantly when the fibre goes through the transformation from the alpha helix conformation to the beta-sheet one. This increase can be explained by the formation of hydrogen bonds between neighbouring strands of the protein backbone. Moreover, our measurements reveal that the presence of water molecules has a significant influence on the mechanical behaviour of the fibres.",
        "watermark_text": "The mechanical effects of hair are controlled by its chemical structure , which is mainly composed of proteins called keratins . The main structural unit in keratins is an alpha - helical coiled coil that forms microfibrils with other similar units to form intermediate filaments ( IFs ) .These IFs then aggregate into bundles forming macrofibrils , which are further bundled together to form the fundamental foundation blocks of hair - the cortical cells - . In this research we have researched how the mechanical properties vary as these structures evolve during the process known as maturation or ageing .We utilized confocal Raman spectroscopy to measure the changes in the secondary structure of individual keratin fibres collected from human scalp hairs at different stages of their development process . Our results show that the elastic modulus increases substantially when the polymer goes through the transformation from the alpha helix conformation to the beta - sheet one .This increase can be described by the formation of hydrogen ties between neighbouring segments of the protein fiber . Moreover , our measurements reveal that the presence of water molecules has a substantial impact on the mechanical behaviour of the fibres .",
        "rewrite_text": "The mechanical properties of hair are determined by its chemical composition, primarily composed of proteins known as keratins. The fundamental structural component of keratins is the alpha-helical coiled coil, which combines with similar units to create microfibrils that assemble into intermediate filaments (IFs). These IFs further cluster into bundles, forming macrofibrils that constitute the essential building blocks of hair—the cortical cells. In this study, we investigated how the mechanical characteristics change as these structures develop during the maturation or aging process. We employed confocal Raman spectroscopy to assess the alterations in the secondary structure of individual keratin fibers sourced from human scalp hair at various developmental stages. Our findings indicate a significant increase in the elastic modulus as the polymer transitions from the alpha helix conformation to the beta-sheet structure. This enhancement can be attributed to the formation of hydrogen bonds between adjacent segments of the protein fiber. Additionally, our measurements demonstrate that water molecules play a crucial role in influencing the mechanical behavior of these fibers.",
        "ori-fast-z-score": 1.6464638998453551,
        "water-fast-z-score": 5.963302408041713,
        "rewrite-fast-z-score": 1.7669044171975445
    },
    {
        "original_text": "We present the results of N-body simulations aimed at studying tidal disruption and accretion in interacting galaxy pairs, with particular emphasis on the formation of tidally stripped dwarfs (TDGs). We find that TDG formation is strongly dependent upon the orbital parameters of the interaction; specifically, we show that TDGs form only when the encounter has an impact parameter less than about twice the sum of their effective radii.  In addition to this dependence on orbital geometry, our models suggest that TDGs are more likely to be formed if the progenitor galaxies have high gas fractions and/or low central surface brightnesses. Finally, we argue that TDGs may provide useful probes for testing theories of gravity on galactic scales. The discovery of numerous examples of  tidal dwarf galaxies  (TDGs) over the past decade or so has led many authors to propose these objects as possible sites of star formation during interactions between massive spiral galaxies. However, despite considerable observational effort, there remains no consensus regarding either the frequency of TDG formation or even whether such systems actually exist outside the confines of numerical simulations.",
        "watermark_text": "We present the conclusion of N - bodies simulations aiming at studying tidal disruption and accretion in correlated galaxy pairs , with particular focusing on the formation of tidally stripped dwarfs ( TDGs ) . We see that TDG formation is strongly dependent upon the orbital characteristics of the interaction ; specifically , we find that TDGs form only when the encounter has an impact parameter less than about half the sum of their effective radii .In addition to this dependence on orbital geometry , our calculations suggest that TDGs are more likely to be formed if the progenitor galaxies have greater gas fractions and / or low central exterior brightnesses . Finally , we claim that TDGs might represent helpful probes for studying concepts of gravitational on galactic scales .The observation of several examples of tidal dwarf stars ( TDGs ) over the previous decade or so has led many writers to propose these objects as possible areas of galaxy formation during interactions between massive spiral galaxies . However , despite considerable observational effort , there stands no discussion regarding either the frequency of TDG development or even whether such systems actually appear outside the confines of computational simulations .",
        "rewrite_text": "We present the findings of N-body simulations aimed at investigating tidal disruption and accretion in correlated galaxy pairs, with a specific focus on the formation of tidal dwarfs (TDGs). Our analysis reveals that the creation of TDGs is highly influenced by the orbital characteristics of the interaction; notably, TDGs tend to form only when the impact parameter of the encounter is less than approximately half the sum of their effective radii. Beyond this dependence on orbital geometry, our results indicate that TDG formation is more likely when the progenitor galaxies possess higher gas fractions and/or lower central exterior brightnesses. Additionally, we propose that TDGs could serve as valuable indicators for exploring gravitational phenomena on galactic scales. Over the past decade, the observation of several TDGs has prompted researchers to suggest that these objects may play a significant role in galaxy formation during encounters between massive spiral galaxies. However, despite substantial observational efforts, there remains a lack of discussion regarding the frequency of TDG formation and whether such systems indeed exist beyond the realm of computational simulations.",
        "ori-fast-z-score": -0.42640143271122083,
        "water-fast-z-score": 6.469966392206305,
        "rewrite-fast-z-score": -1.2792042981336627
    },
    {
        "original_text": "The purpose of this study is to examine the role that evaluation plays in communication between teachers and students during computer-based learning environments (CBLE). The research question guiding this investigation is: How does evaluation affect student-teacher interaction? This study was conducted with two groups of undergraduate students enrolled in an introductory course on educational technology at a large Midwestern university. Students were asked to complete three tasks using a CBLE called WebQuests, which are designed for use by students working individually or collaboratively. Data collection included audio recordings of group discussions, field notes taken by researchers observing each group s work, and students  written responses to questions posed throughout their participation in the project. Analysis revealed that evaluation played several roles within these interactions including providing feedback about individual performance; clarifying expectations; establishing ground rules; and promoting reflection. These findings suggest that evaluation can be used effectively to enhance student-teacher communication when it occurs frequently enough over time so that both parties have multiple opportunities to respond to one another.",
        "watermark_text": "The purpose of this study is to examine the part that assessment plays in interaction between instructors and teachers during computer - based educational environments ( CBLE ) . The research question guiding this analysis is : How does evaluation affect student - teacher contact ?This study was done with two groups of freshman pupils enrolled in an introductory program on educational software at a large Midwestern college . Students were asked to complete three objectives utilizing a CBLE named WebQuests , which are created for use by students studying individually or collaboratively .Data collection included audio prints of group discussions , field notes taken by researchers observing each team s project , and students authored reactions to questions posed throughout their involvement in the project . Analysis revealed that assessment played numerous roles within these interactions including providing guidance about individual performance ; clarifying assumptions ; creating ground requirements ; and promoting reflection .These data suggest that assessment can be used effectively to affect student - teacher understanding when it appears frequently enough over time so that both parties have multiple options to respond to one another .",
        "rewrite_text": "This study aims to investigate the role of assessment in the interactions between instructors and students in computer-based learning environments (CBLEs). The central research question driving this analysis is: How does evaluation influence student-teacher communication? The research involved two groups of freshman students enrolled in an introductory educational software program at a large college in the Midwest. Students were tasked with completing three objectives using a CBLE called WebQuests, designed for both individual and collaborative study. Data collection methods included audio recordings of group discussions, field notes taken by researchers observing each team's projects, and student responses to questions posed throughout the project. The analysis indicated that assessment served several functions in these interactions, including offering guidance on individual performance, clarifying assumptions, establishing common expectations, and fostering reflection. The findings suggest that consistent and frequent assessment can enhance student-teacher understanding by providing both parties with multiple opportunities to engage with one another.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 7.939798740547776,
        "rewrite-fast-z-score": 1.4439897447623107
    },
    {
        "original_text": "We report the observation of measurement-induced dephasing in a Bose-Einstein condensate (BEC) using Ramsey interferometry and atom counting.  The experiment is performed by splitting a single trapped BEC into two spatially separated clouds, which are allowed to evolve for different times before being recombined on a beam splitter. We observe that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the beam splitter. This effect can be explained by considering how repeated measurements affect the phase evolution of the system. Our results demonstrate that it is possible to use cold-atom experiments to study fundamental questions about quantum mechanics. Quantum mechanics predicts that any attempt to measure a physical quantity will disturb its value. In this work, we experimentally investigate such effects in a Bose-Einsteint Condensate (BEC). To do so, we perform Ramsey interferometry between two spatially separated regions of our sample. By varying the time spent evolving freely after splitting off part of the initial cloud, we control the relative phase accumulated during free evolution. After recombination, we count the number of atoms arriving at each output port of the beam-splitter and record their arrival-time distribution. As expected, we find that the visibility of the resulting interference pattern decreases when increasing the number of detected particles.",
        "watermark_text": "We report the observation of measurement - caused dephasing in a Bose - Einstein condensate ( BEC ) using Ramsey interferometry and electron searching . The observation is conducted by breaking a single trapped BEC into two spatially separated clouds , which are allowed to evolve for different times before being recombined on a laser splitter .We see that the visibility of interference fringes decreases as we increase the number of atoms counted at one output port of the light splitter . This phenomenon can be described by examining how repetitive measurements alter the phase evolution of the system .Our results show that it is easy to use cold - atom experiments to study essential problems about quantum mechanics . Quantum theory predicts that any attempt to measure a physical quantity will interrupt its value .In this research , we experimentally examine such interactions in a Bose - Einsteint Condensate ( BEC ) . To do so , we perform Ramsey interferometry between two spatially separated regions of our sample .By varying the period spending expanding freely after splitting off part of the first cloud , we control the relative phase accumulated during free evolution . After recombination , we list the quantity of atoms arriving at each output port of the laser - splitter and record their arrival - time distribution .As expected , we find that the visibility of the resulting interference pattern decreases when increasing the proportion of identified particles .",
        "rewrite_text": "We present our findings on measurement-induced dephasing in a Bose-Einstein condensate (BEC) using Ramsey interferometry and electron detection. In our experiment, we split a single trapped BEC into two spatially separated clouds, allowing them to evolve for varying durations before recombining them with a laser beam splitter. We observe a decrease in the visibility of the interference fringes as the number of atoms detected at one output port of the splitter increases. This observation can be explained by analyzing the impact of repeated measurements on the phase evolution of the system. Our results demonstrate that cold-atom experiments can effectively explore fundamental issues in quantum mechanics. According to quantum theory, any attempt to measure a physical quantity disrupts its value. In our research, we investigate these interactions within a BEC framework. We utilize Ramsey interferometry between two spatially separated regions of the condensate and control the relative phase accumulated during free evolution by adjusting the time allowed for expansion after splitting off part of the first cloud. Upon recombination, we quantify the number of atoms that reach each output port of the laser splitter and record their arrival-time distribution. As anticipated, we find that the visibility of the interference pattern diminishes with an increase in the proportion of detected particles.",
        "ori-fast-z-score": 0.6897304947150052,
        "water-fast-z-score": 6.404640308067906,
        "rewrite-fast-z-score": 2.7414346458607715
    },
    {
        "original_text": "We present the results of our analysis on the time-averaged spectrum and light curve of GRB 090902B, which is one of the brightest bursts ever detected by Fermi/GBM (Gamma-ray Burst Monitor). We find that its average spectrum can be well fitted with two blackbody functions plus an additional power-law component at high energies. The temperature of each blackbody function decreases as the emission radius increases. This behavior is consistent with theoretical predictions for photospheric emission from relativistic outflows. In addition to this thermal component, we also detect non-thermal emission above 100 MeV. Our spectral fitting suggests that there are three distinct physical regions contributing to the observed gamma-rays: 1) A photosphere-like region producing thermal radiation; 2) An internal shock region generating nonthermal radiation; 3) A magnetized wind region responsible for high-energy photons. These findings provide new insights into the physics of GRBs. \n \n Keywords: Black body, Photon index, Power law",
        "watermark_text": "We present the conclusion of our analysis on the period - averaged spectrum and light curve of GRB 090902B , which is one of the brightest bursts ever observed by Fermi / GBM ( Gamma - ray Burst Monitor ) . We see that its average spectrum can be well fitted with two blackbody functions plus an additional power - law component at high energies .The temperature of each blackbody function decreases as the emission diameter increases . This phenomenon is compatible with theoretical expectations for photospheric emission from relativistic outflows .In addition to this heat element , we also observe non - thermal emitted above 100 MeV . Our spectral fit suggests that there are three separate physical regions contributing to the seen γ - radiation : 1 ) A photosphere - like zone creating thermal rays ; 2 ) An inner blast area producing nonthermal emission ; 3 ) A magnetized wind zone responsible for high - energy photons .These studies provide fresh insights into the physics of GRBs . Keywords : Black substance , Photon index , Power law",
        "rewrite_text": "We present our findings on the period-averaged spectrum and light curve of GRB 090902B, one of the brightest bursts detected by the Fermi Gamma-ray Burst Monitor (GBM). Our analysis indicates that the average spectrum is well described by a combination of two blackbody functions and an additional power-law component at higher energies. Notably, the temperatures of the blackbody functions decrease with increasing emission diameter, a trend that aligns with theoretical predictions for photospheric emission from relativistic outflows. Alongside this thermal emission, we also detect non-thermal radiation above 100 MeV. Our spectral fitting reveals three distinct physical regions contributing to the observed γ-radiation: 1) a photosphere-like region generating thermal emissions; 2) an inner blast zone responsible for non-thermal emissions; and 3) a magnetized outflow region that produces high-energy photons. These observations offer new insights into the physics underlying gamma-ray bursts. Keywords: Blackbody, Photon index, Power law.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 6.289942788427422,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "We consider the problem of finding the statistics of nodal lines in random wave fields, where we assume that the field is generated by an ensemble of monochromatic waves with randomly distributed phases and amplitudes. We show how to calculate these statistics using only the first two moments of the distribution function for the phase differences between pairs of waves. The results are applied to study the statistical properties of nodal lines on the surface of oceanic internal tides. In particular, it is shown that the probability density functions (PDFs) of the number of intersections per unit length of a reference curve depend strongly on whether or not this curve lies within one wavelength of the source region. If so, then the PDF has a power-law tail at large values of the intersection density; if not, then there is no such tail. \n \n Keywords: Random wave fields, Oceanic internal tide, Nodal line, Phase difference",
        "watermark_text": "We consider the question of finding the statistics of nodal lines in random wave areas , where we suppose that the field is generated by an ensemble of monochromatic waves with randomly distributed phases and amplitudes . We see how to estimate these statistics utilizing only the first two moments of the distribution function for the phase differences between pairs of waves .The results are applied to study the statistical characteristics of nodal lines on the surface of oceanic internal tides . In particular , it is demonstrated that the probability density functions ( PDFs ) of the quantity of intersections per unit span of a reference curve rely highly on whether or not this curve lies within one frequency of the source region .If so , then the PDF has a power - law neck at large values of the intersection density ; if not , then there is no such tail . Keywords : Random wave forces , Oceanic internal wave , Nodal line , Phase difference",
        "rewrite_text": "We investigate the statistics of nodal lines in random wave environments, assuming that the field arises from a collection of monochromatic waves with randomly varying phases and amplitudes. Our approach estimates these statistics using only the first two moments of the distribution function for the phase differences between pairs of waves. The findings are applied to analyze the statistical properties of nodal lines on the surface of oceanic internal tides. Specifically, we demonstrate that the probability density functions (PDFs) for the number of intersections per unit length of a reference curve are significantly influenced by whether this curve falls within one frequency of the source region. If it does, the PDF exhibits a power-law tail at high values of intersection density; if it does not, such a tail is absent. \nKeywords: Random wave forces, Oceanic internal waves, Nodal line, Phase difference.",
        "ori-fast-z-score": 1.7232808737106582,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "We present new theoretical evolutionary tracks for the mass range 1.8 to 2.5 M⊙, which are based on an improved treatment of convection in stellar interiors. We use these tracks as input into our seismic modelling code CESAM2k to compute synthetic seismograms for two different sets of observed frequencies (from COROT) corresponding to two possible values of the inclination angle i = 90° or 60° . The comparison between observations and theory shows that we can exclude one set of frequencies at high confidence level but not the other. This is due to the fact that the frequency differences between ℓ=0 and ℓ=2 modes depend strongly on the inclination angle. In addition, we find that the best fit model has a radius R=1.95R⊙ , which agrees well with the value inferred by asteroseismology using only ℓ=0 modes. Finally, we show how this result could be used to determine the age of the star.  Keywords: Seismic modelling",
        "watermark_text": "We create additional theoretical evolutionary tracks for the mass range 1 . 8 to 2 . 5 [UNK] , which are based on an improved treatment of convection in stellar interiors . We use these tracks as input into our seismic modelling code CESAM2k to compute synthetic seismograms for two different sets of observed frequencies ( from COROT ) corresponding to two possible values of the inclination angle i = 90° or 60° .The comparison between observations and theory indicates that we can eliminate one group of frequencies at high confidence rate but not the other . This is due to the fact that the frequency differences between ℓ = 0 and ℓ = 2 modes differ highly on the inclination orientation .In addition , we find that the best fit model has a radius R = 1 . [UNK] , which agrees well with the value inferred by asteroseismology using only ℓ = 0 modes . Finally , we show how this result could be used to determine the age of the star .Keywords: Seismic modelling",
        "rewrite_text": "We have developed additional theoretical evolutionary tracks for stars with masses ranging from 1.8 to 2.5 [UNK], utilizing an enhanced approach to convection within stellar interiors. These tracks serve as inputs for our seismic modeling software, CESAM2k, allowing us to generate synthetic seismograms for two observed frequency sets from COROT, corresponding to inclination angles of i = 90° and 60°. Our comparisons between observations and theoretical predictions suggest that we can confidently rule out one group of frequencies, while the other remains plausible. This outcome is primarily due to the significant differences in frequency between the ℓ = 0 and ℓ = 2 modes, which are highly sensitive to the orientation of the inclination. Furthermore, we determine that the model which best fits the data has a radius of R = 1 . [UNK], aligning well with the radius estimates derived from asteroseismic analysis focusing solely on ℓ = 0 modes. Finally, we discuss how this finding can be leveraged to estimate the star's age. Keywords: Seismic modeling.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 2.80989722019502,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "We report on the detection of transient X-ray emission in normal galactic nuclei (NGNs) with Chandra and XMM-Newton observatories, which is likely to be associated with accretion onto supermassive black holes at their centers. The observed luminosities are consistent with those expected for steady nuclear activity powered by mass inflow through an optically thick disk around the central black hole. We find that the duration of this activity ranges between 10 3 -10 5 years, depending on the distance of NGN from Earth. This suggests that the majority of NGNs may have experienced such active phases during their lifetimes. Our results also imply that the current quiescent state of most NGNs could be due to either low-level accretion or obscuration effects. These findings provide new insights into the formation and evolution of massive galaxies as well as AGNs. Keywords: Black Hole Accretion Disk - Galaxy Evolution - Nuclear Activity",
        "watermark_text": "We report on the observation of transient X - ray radiation in regular galactic nuclei ( NGNs ) with Chandra and XMM - Newton observatories , which is expected to be involved with accretion onto supermassive black holes at their centers . The observed luminosities are compatible with those expected for steady nuclear activity driven by mass inflow through an optically dense disk around the main white hole .We see that the duration of this action ranges between 10 3 - 10 5 years , depending on the distance of NGN from Earth . This implies that the majority of NGNs might have experienced such active phases during their lifetimes .Our results also suggest that the present quiescent state of most NGNs might be due to either small - point accretion or obscuration effects . These studies provide fresh insights into the formation and evolution of large galaxies as well as AGNs .Keywords: Black Hole Accretion Disk - Galaxy Evolution - Nuclear Activity",
        "rewrite_text": "We report the detection of transient X-ray radiation in regular galactic nuclei (NGNs) using the Chandra and XMM-Newton observatories, which is believed to be linked to the accretion processes occurring at the centers of supermassive black holes. The luminosities observed align with those anticipated from stable nuclear activity driven by mass inflow through an optically dense disk surrounding the central white hole. The duration of these active phases varies between 1,000 and 100,000 years, depending on the NGN's distance from Earth. This indicates that many NGNs have likely experienced such active periods throughout their existence. Additionally, our findings suggest that the current quiescent states of most NGNs may result from either minimal point accretion or obscuration effects. These studies offer new perspectives on the formation and evolution of large galaxies and active galactic nuclei (AGNs).  \nKeywords: Black Hole Accretion Disk, Galaxy Evolution, Nuclear Activity.",
        "ori-fast-z-score": -1.0690449676496976,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "We study the Wightman functions and vacuum densities on a Z_2-symmetric, thick brane embedded in an anti-de Sitter (AdS) space-time with one extra dimension. We find that there are two types of solutions to the corresponding equations depending on whether or not the bulk mass is zero. In both cases we show how these quantities can be expressed as sums over modified Bessel functions. The results obtained here may have applications in quantum field theory at finite temperature and/or density. PACS: 11.10.Kk, 12.20.Ds, 98.80.Cq Keywords: Vacuum expectation value, Anti-de Sitter space time, Thick brane, Modified Bessel function. 1 Introduction An interesting feature of string theories is their ability to incorporate gravity into the fundamental description of nature. This has led to renewed interest in studying gravitational backgrounds which admit supersymmetry  1  . One such class of spacetimes is given by the so-called warped product spaces  2  , where the metric takes the form ds2 = e2A(y)(ημνdxμ dxν + dy 2 ),\n(1)\nwhere y denotes the coordinate along the extra dimension, A(y) is called the warp factor and ημν is the Minkowski metric. For example, if we consider the five-dimensional case then this corresponds to the Randall-Sundrum model  3  .\nIn recent years it was shown  4  -  8  that the presence of a nontrivial warp factor leads to new features in the physics associated with fields propagating in the bulk. These include modifications to the standard dispersion relations  9  , spontaneous symmetry breaking  10  , fermion localization  11  , etc.. It turns out  12  that the effects due to the warp factor depend crucially upon its behaviour near the boundary of the extra dimension. If the warp factor vanishes sufficiently rapidly at infinity then all physical observables will be identical to those computed using ordinary flat-space techniques. However, if the warp factor does not vanish fast enough then some novel phenomena occur.",
        "watermark_text": "We explore the Wightman functions and vacuum densities on a Z _ 2 - symmetric , thick brane embedded in an anti - de Sitter ( AdS ) space - time with one extra dimension . We see that there are two forms of solutions to the associated equations depending on whether or not the liquid weight is zero .In both cases we give how these quantities can be shown as sums over modified Bessel functions . The results derived here may have applications in quantum field theory at finite cooling and / or density .PACS : 11 . 10 . Kk , 12 . 20 . Ds , 98 . 80 . Cq Keywords : Vacuum expectation point , Anti - de Sitter space time , Thick brane , Modified Bessel relation . 1 Introduction An interesting feature of string theories is their power to insert gravitational into the fundamental description of nature .This has led to renewed emphasis in investigating gravitational backgrounds which admit supersymmetry 1 . One such family of spacetimes is given by the so - called warped product spaces 2 , where the metric takes the form ds2 = e2A ( y ) ( ημνdxμ dxν + dy 2 ) , ( 1 ) where y denotes the coordinate along the extra dimension , A ( y ) is dubbed the warp factor and ημν is the Minkowski metric .For instance , if we treat the five - dimensional case then this corresponds to the Randall - Sundrum model 3 . In recent years it was shown 4 - 8 that the presence of a nontrivial warp factor leads to novel features in the physics associated with fields propagating in the bulk .These include changes to the standard dispersion relations 9 , spontaneous symmetry breaking 10 , fermion localization 11 , etc . . It turns out 12 that the effects due to the warp factor rely crucially upon its behaviour near the boundary of the extra dimension .If the warp factor vanishes sufficiently quickly at infinity then all physical observables will be identical to those computed using ordinary flat - space methods . However , if the warp factor does not vanish fast enough then some interesting phenomena arise .",
        "rewrite_text": "We investigate the Wightman functions and vacuum densities within a Z_2-symmetric, thick brane situated in an anti-de Sitter (AdS) spacetime that includes an additional dimension. Our analysis reveals two types of solutions to the corresponding equations, which depend on whether the liquid weight is zero. In both scenarios, we demonstrate that these quantities can be expressed as sums involving modified Bessel functions. The findings presented here could have implications for quantum field theory under conditions of finite temperature and/or density. \n\nPACS: 11.10.Kk, 12.20.Ds, 98.80.Cq  \nKeywords: Vacuum expectation value, Anti-de Sitter spacetime, Thick brane, Modified Bessel functions.\n\n1. Introduction\n\nString theories possess the intriguing ability to weave gravity into the fundamental fabric of nature, which has spurred renewed interest in the study of gravitational backgrounds that allow for supersymmetry. One notable class of spacetimes is formed by what are known as warped product spaces, characterized by a metric of the form:\n\n\\[ ds^2 = e^{2A(y)} (\\eta_{\\mu\\nu} dx^\\mu dx^\\nu + dy^2) \\]\n\nwhere \\( y \\) represents the extra dimension coordinate, \\( A(y) \\) is the warp factor, and \\( \\eta_{\\mu\\nu} \\) is the Minkowski metric. For example, in the five-dimensional scenario, this aligns with the Randall-Sundrum model. Recent studies have shown that the presence of a nontrivial warp factor introduces novel aspects to the physics of fields propagating within the bulk, such as alterations to standard dispersion relations, spontaneous symmetry breaking, and fermion localization. Notably, the impact of the warp factor is highly sensitive to its behavior at the boundary of the extra dimension. If the warp factor diminishes rapidly at infinity, all physical observables will match those derived through conventional flat-space approaches. Conversely, if the warp factor does not diminish quickly enough, intriguing phenomena emerge.",
        "ori-fast-z-score": 0.8980265101338746,
        "water-fast-z-score": 6.2215204792052825,
        "rewrite-fast-z-score": 1.299867367239363
    },
    {
        "original_text": "We present an efficient method for computing the vacuum energy and force between two parallel plates using only standard numerical electromagnetism methods, without resorting to any approximations or special treatments such as analytic continuation into complex frequencies. The key idea is that we can use the Feynman-Kac formula to express the vacuum expectation value of the stress tensor at finite temperature T = 1/β (where β is the inverse temperature) in terms of the corresponding quantity at zero temperature but with one additional term involving the time evolution operator over a period of length β . We then show how this expression may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s equations on a periodic domain. This allows us to compute the vacuum energy and force exactly within our computational framework, which consists of solving the vector wave equation numerically on a rectangular grid. Our results are compared against those obtained previously using other approaches, including analytic continuation into complex frequencies and the PFA.",
        "watermark_text": "We present an efficient approach for calculation the vacuum energy and force between two connected sheets utilizing only conventional numerical electromagnetism methods , without resorting to any approximations or particular treatments such as analytic continuation into complex frequencies . The main idea is that we can using the Feynman - Kac formula to derive the vacuum expectation value of the strain vector at finite temperature T = 1 / beta ( where β is the inverse temperature ) in terms of the equivalent quantity at zero temperature but with one additional word regarding the period evolution operator over a period of length β .We then show how this formula may be evaluated efficiently by expressing it in terms of the Green s function of Maxwell s equations on a periodic domain . This enables us to compute the vacuum energy and force exactly within our computational framework , which consists of calculating the linear wave equation numerically on a rectangular grid .Our results are compared against those achieved previously used other methods , notably analytic continuation into complex frequencies and the PFA .",
        "rewrite_text": "We introduce an efficient method for calculating the vacuum energy and force between two interconnected sheets, relying solely on standard numerical electromagnetism techniques and avoiding any approximations or specialized treatments like analytic continuation into complex frequencies. The core concept involves utilizing the Feynman-Kac formula to express the vacuum expectation value of the strain vector at a finite temperature \\( T = \\frac{1}{\\beta} \\) (where \\( \\beta \\) is the inverse temperature) in terms of its zero-temperature counterpart, incorporating an additional term related to the periodic evolution operator over a duration of \\( \\beta \\). We demonstrate that this formula can be evaluated efficiently by reformulating it in terms of the Green's function of Maxwell's equations within a periodic domain. This approach allows for the precise computation of vacuum energy and force using our computational framework, which involves numerically solving the linear wave equation on a rectangular grid. We then compare our findings with results obtained from previously employed methods, particularly analytic continuation into complex frequencies and the proximity force approximation (PFA).",
        "ori-fast-z-score": 0.6625891564490792,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 1.0660035817780522
    },
    {
        "original_text": "We introduce the notion of dynamical objects associated to cohomologically expanding maps, and prove that they are equivalent to the usual ones in many cases.  We also show how these new objects can be used to study the dynamics of such maps on infinite-dimensional spaces. Let X be an infinite dimensional Banach space with norm . For each integer n ≥ 1 we define the open ball B(n) = {x ∈ X : x < n}. A map T : X → X is said to be cohomologically expanding if there exists some constant C > 0 so that for all integers m, n ≥ 1 one has  diam (T −m (B(n))) ≤ Cn. In this case it follows easily that T satisfies the following properties:\n(1)  T is continuous; \n(2)  T is surjective; \nThe main result of our work shows that under certain conditions, the existence of a dynamical object implies the existence of another one which behaves well when restricted to finite-dimensional subspaces. \nLet us now recall what a dynamical object is. Given any point x ∈ X , let O(x) denote the orbit of x; i.e., O(x) := {T k (x), k ∈ Z}. The set O(x) equipped with the metric dO defined by dO((x1, x2)) = sup{d(x1, x2), x1 ∈ O(x2), x2 ∈ O(x1)} becomes a compact metric space called the orbital space at x. If T is cohomologically expanding then every orbital space is homeomorphic to a Cantor set.",
        "watermark_text": "We introduce the notion of dynamical images attached to cohomologically extended spaces , and prove that they are analogous to the usual ones in many situations . We also demonstrate how these new objects can be used to study the dynamics of such maps on infinite - dimensional spaces .Let X be an infinite dimensional Banach space with norm . For each integer n ≥ 1 we define the open ball B ( n ) = { x ∈ X : x < n } .A mapping T : X → X is said to be cohomologically extended if there exists some variable C > 0 so that for all numbers m , n ≥ 1 one has diam ( T −m ( B ( n ) ) ) ≤ Cn . In this instance it appears easily that T satisfies the following properties : ( 1 ) T is continuous ; ( 2 ) T is surjective ; The main consequence of our work proves that under certain conditions , the existence of a dynamical object suggests the existence of another one which behaves well when confined to finite - dimensional subspaces .Let us now recall what a dynamical object is . Given any point x ∈ X , let O ( x ) define the orbit of x ; i . e . , O ( x ) : = { T k ( x ) , k ∈ Z } .The set O ( x ) equipped with the metric dO defined by dO ( ( x1 , x2 ) ) = sup { d ( x1 , x2 ) , x1 ∈ O ( x2 ) , x2 ∈ O ( x1 ) } becomes a compact metric space termed the orbital space at x . If T is cohomologically extended then every orbital space is homeomorphic to a Cantor set .",
        "rewrite_text": "We introduce the concept of dynamical images associated with cohomologically extended spaces and demonstrate that they are analogous to traditional dynamical images in various contexts. Furthermore, we illustrate how these newly defined objects can be used to analyze the dynamics of mappings in infinite-dimensional spaces. Let X represent an infinite-dimensional Banach space with a norm. For each integer \\( n \\geq 1 \\), we define the open ball \\( B(n) = \\{ x \\in X : \\| x \\| < n \\} \\). A mapping \\( T: X \\rightarrow X \\) is considered cohomologically extended if there exists a positive constant \\( C > 0 \\) such that for all integers \\( m, n \\geq 1 \\), it holds that \\( \\text{diam}(T^{-m}(B(n))) \\leq Cn \\). Under these conditions, it is clear that \\( T \\) satisfies the following properties: (1) \\( T \\) is continuous; (2) \\( T \\) is surjective. A key result of our work shows that, given certain conditions, the existence of a dynamical object implies the existence of another that behaves well when restricted to finite-dimensional subspaces. Now, let us define what a dynamical object is. For any point \\( x \\in X \\), let \\( O(x) \\) denote the orbit of \\( x \\), specifically, \\( O(x) = \\{ T^k(x) : k \\in \\mathbb{Z} \\} \\). The set \\( O(x) \\), equipped with the metric \\( d_O \\) defined by \\( d_O((x_1, x_2)) = \\sup \\{ d(x_1, x_2) : x_1 \\in O(x_2), x_2 \\in O(x_1) \\} \\), forms a compact metric space known as the orbital space at \\( x \\). If \\( T \\) is cohomologically extended, then every orbital space is homeomorphic to a Cantor set.",
        "ori-fast-z-score": 0.6123724356957946,
        "water-fast-z-score": 4.569057743101286,
        "rewrite-fast-z-score": 1.7085642859406605
    },
    {
        "original_text": "We present new observations and analysis of the infrared emission lines in the spectra of two yellow hypergiants, IRC+10420 and AFGL2136. We find that these objects have very high mass-loss rates (10^-6 to 10^-5 Msun/yr) with outflow velocities ranging between 100-200 km/sec. The observed line profiles are consistent with an expanding shell model for the wind. In addition we detect several forbidden transitions which indicate the presence of highly ionized species such as Fe + , Si ++ , S ++ . These ions may be formed by photoionization or collisional ionization processes within the stellar winds. \n \n Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass loss rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "watermark_text": "We report new experiments and investigation of the infrared absorption tracks in the spectra of two yellow hypergiants , IRC + 10420 and AFGL2136 . We see that these objects have very high mass - loss rates ( 10 ^ - 6 to 10 ^ - 5 Msun / yr ) with outflow velocities ranging between 100 - 200 kilometers / sec .The observed line profiles are compatible with an moving shell model for the wind . In addition we find various forbidden transitions which demonstrate the presence of highly ionized compounds such as Fe + , Si + + , S + + .These ions may be formed by photoionization or collisional ionization processes within the stellar winds . Keywords : Yellow Hypergiants ; Circumstellar Envelopes ; Mass loss rate ; Outflows ; Emission Lines ; IRAS 08544 - 4431 .Astronomy & Astrophysics manuscript no.aa20031118 May 31, 2003 \nThe chemical composition of the circumstellar-envelope",
        "rewrite_text": "We present new experiments and investigations into the infrared absorption features in the spectra of two yellow hypergiants, IRC + 10420 and AFGL 2136. Our findings indicate that these stars exhibit very high mass-loss rates, estimated between 10^-6 and 10^-5 solar masses per year, with outflow velocities ranging from 100 to 200 kilometers per second. The observed line profiles are consistent with a moving shell model for the stellar wind. Additionally, we identify several forbidden transitions that indicate the presence of highly ionized species, such as Fe+, Si++, and S++. These ions are likely formed through photoionization or collisional ionization processes within the stellar winds. Keywords: Yellow Hypergiants; Circumstellar Envelopes; Mass Loss Rate; Outflows; Emission Lines; IRAS 08544-4431. Astronomy & Astrophysics manuscript no. aa20031118, May 31, 2003. \nThe chemical composition of the circumstellar envelope.",
        "ori-fast-z-score": 0.7453559924999299,
        "water-fast-z-score": 4.323064756499593,
        "rewrite-fast-z-score": 2.4961508830135313
    },
    {
        "original_text": "We analyze the non-Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe (WMAP) first year sky maps using three different estimators, namely Minkowski Functionals (MF), genus curve and correlation functions. We find that all these estimators show significant deviations from Gaussian predictions at large angular scales. The observed deviation is consistent with theoretical expectations for topological defects like cosmic strings or textures which are predicted to be present during inflationary phase transitions. These results provide further support for the presence of primordial non-Gaussianities in CMB anisotropies. This work was supported by the Department of Atomic Energy under grant No. 06(B)/ST-IISc/04. The authors thank Sourav Chatterjee for useful discussions. PACS numbers: 98.80.Cq, 95.35.+d, 04.20.Jb  Keywords: Cosmic microwave background radiation",
        "watermark_text": "We evaluate the non - Gaussianity in the temperature fluctuations of the Wilkinson Microwave Anisotropy Probe ( WMAP ) first year sky mapping utilizing three different estimators , namely Minkowski Functionals ( MF ) , genus curve and correlation functions . We see that all these estimators exhibit substantial deviations from Gaussian predictions at large angular scales .The observed deviation is compatible with theoretical expectations for topological errors like cosmic strings or textures which are expected to be present during inflationary phase transitions . These data provide further evidence for the presence of primordial non - Gaussianities in CMB anisotropies .This project was supported by the Department of Atomic Energy under grant No . 06 ( B ) / ST - IISc / 04 .The authors praised Sourav Chatterjee for useful talks . PACS codes : 98 . 80 . Cq , 95 . 35 . + d , 04 . 20 . Jb Keywords : Cosmic microwave background radiation",
        "rewrite_text": "We analyze the non-Gaussianity in the temperature fluctuations of the first-year sky map from the Wilkinson Microwave Anisotropy Probe (WMAP) using three distinct estimators: Minkowski Functionals (MF), genus curve, and correlation functions. Our findings indicate that all these estimators reveal significant deviations from Gaussian predictions at large angular scales. This observed deviation aligns with theoretical expectations regarding topological defects, such as cosmic strings or textures, that could have formed during phase transitions in the inflationary epoch. This data serves as additional evidence for the presence of primordial non-Gaussianities in cosmic microwave background (CMB) anisotropies. The project received support from the Department of Atomic Energy under grant No. 06 (B)/ST-IISc/04. The authors express gratitude to Sourav Chatterjee for his valuable discussions. PACS codes: 98.80.Cq, 95.35.+d, 04.20.Jb. Keywords: Cosmic microwave background radiation.",
        "ori-fast-z-score": 0.5897678246195885,
        "water-fast-z-score": 5.013026509266503,
        "rewrite-fast-z-score": 2.4961508830135313
    },
    {
        "original_text": "The reed is the key component in single-reed musical instruments, such as clarinets or saxophones. The dynamics of its motion are strongly influenced by the bore geometry and boundary conditions at both ends. In this work we present an approach to simulate the oscillations of a single reed instrument based on modal decomposition of the bore and reed dynamics. We show that it is possible to accurately reproduce the sound produced by a real clarinet using only one degree-of-freedom for each mode. This method can be used to study the influence of different parameters (such as mouthpiece diameter) on the acoustic response of the instrument without requiring expensive experimental measurements. It also allows us to investigate how changes in the design affect the performance of new prototypes. \n \n Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "watermark_text": "The reed is the key component in single - reed musical instruments , such as clarinets or saxophones . The structures of its motion are strongly dictated by the bore pattern and boundary pressures at both ends .In this study we present an approach to simulate the oscillations of a single reed instrument relying on modal decomposition of the bore and reed dynamics . We see that it is easy to correctly reproduce the music created by a real clarinet using only one degree - of - independence for each mode .This method can be used to study the impact of different variables ( such as mouthpiece diameter ) on the acoustic response of the instrument without using expensive experimental surveys . It additionally lets us to examine how differences in the model affect the performance of new prototypes .Keywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method",
        "rewrite_text": "The reed serves as the essential element in single-reed musical instruments like clarinets and saxophones. Its motion is significantly influenced by the bore configuration and the boundary pressures at both ends. In this study, we introduce a method to simulate the oscillations of a single-reed instrument through modal decomposition of the bore and reed dynamics. Our findings demonstrate that we can accurately replicate the sound produced by a real clarinet using just one degree of freedom for each mode. This approach allows us to investigate the effects of various factors, such as mouthpiece diameter, on the instrument's acoustic response without the need for costly experimental surveys. Additionally, it enables us to analyze how variations in the model impact the performance of new prototypes. \n\nKeywords: Musical Instrument, Clarinet, Sound Generation, Modeling, Simulation, Acoustics, Vibration Analysis, Modal Analysis, Finite Element Method.",
        "ori-fast-z-score": -1.5756771943166705,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": -0.2672612419124244
    },
    {
        "original_text": "We study the nonlinear dynamics of infectious diseases transfer in a population, where individuals are divided into three classes: susceptible (S), infected (I) and recovered/removed (R). We consider two different models: SIR model and SEIR model. In both cases we assume that there is no birth or death in the population. The main goal of this work is to investigate how the disease spreads through the population depending on its parameters. For example, if the infection rate is too high then it may lead to an epidemic outbreak. On the other hand, if the recovery rate is very large compared to the infection rate then the number of infectives will decrease rapidly. Finally, we show some numerical simulations which illustrate our results. \n \n Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. 1 Introduction \n \n Many mathematical models have been developed over time to describe the spread of infectious diseases within populations  1–3  . These models can be used as tools to understand the transmission mechanisms of these diseases and help public health authorities make decisions about prevention strategies  4  .\n \nIn particular, many researchers have studied the effects of vaccination programs  5–7  , quarantine  8, 9  and isolation  10, 11  on the evolution of epidemics. Other studies focus on the impact of environmental factors such as temperature  12, 13  , humidity  14, 15  and rainfall  16  on the propagation of pathogens. \nThe majority of existing works use deterministic models based on ordinary differential equations  17  . However, stochastic models  18, 19  and agent-based models  20, 21  also exist. Agent-based models allow us to take into account individual behaviors  22  while stochastic models provide more realistic descriptions of random events  23  . \n \nIn this article, we propose new mathematical models describing the spread of infectious diseases in a closed population. Our aim is to analyze the influence of various parameters on the behavior of the system. More specifically, we want to determine whether the disease will die out naturally or cause an epidemic outbreak. To do so, we first introduce the basic reproduction number R0  24  , which represents the average number",
        "watermark_text": "We explore the nonlinear dynamics of infectious infections transmission in a population , where persons are split into three categories : affected ( S ) , infected ( I ) and returned / deleted ( R ) . We consider two different models : SIR model and SEIR model .In both cases we suppose that there is no birth or dying in the population . The main goal of this research is to examine how the infection spreads through the population depending on its characteristics .For instance , if the infection rate is too big then it could lead to an outbreak outbreak . On the other hand , if the return speed is very huge compared to the infection rate then the quantity of infectives will decrease rapidly .Finally , we give some numerical simulations which illustrate our findings . Keywords : Nonlinear dynamics , infectious deaths , tuberculosis , SIR model , SEIR model .1 Introduction Many numerical models have been created over time to explain the spread of infectious infections within communities 1 – 3 . These methods can be used as tools to explain the spreading patterns of these diseases and help public medical institutions making decisions about prevention plans 4 .In particular , many scientists have researched the effects of vaccination programs 5 – 7 , quarantine 8 , 9 and isolation 10 , 11 on the evolution of epidemics . Other studies emphasis on the impact of environmental factors such as temperature 12 , 13 , moisture 14 , 15 and rainfall 16 on the propagation of pathogens .The majority of older studies use deterministic theories based on ordinary differential equations 17 . However , stochastic systems 18 , 19 and agent - based models 20 , 21 also exist .Agent - based methods help us to take into consideration individual behaviors 22 while stochastic theories provide more realistic descriptions of random events 23 . In this page , we propose additional mathematical models explaining the spread of infectious infections in a closed population .Our aim is to analyze the impact of several variables on the response of the system . More specifically , we want to estimate whether the infection will die out naturally or lead an outbreak outbreak .To do so , we first introduce the fundamental reproduction number R0 24 , which equals the average number",
        "rewrite_text": "We investigate the nonlinear dynamics of infectious disease transmission within a population divided into three categories: susceptible (S), infected (I), and recovered (R). Our analysis involves two models: the SIR model and the SEIR model. In both models, we assume a closed population with no births or deaths. The primary objective of this research is to understand how the characteristics of the infection influence its spread within the population. For example, a high infection rate could result in an outbreak, whereas a rapid recovery rate, in comparison to the infection rate, may lead to a swift decline in the number of infected individuals. We provide numerical simulations to illustrate our results. Keywords: Nonlinear dynamics, infectious diseases, tuberculosis, SIR model, SEIR model. \n\n1. Introduction\n\nNumerous numerical models have been developed over time to analyze the spread of infectious diseases within communities. These models serve as valuable tools for understanding disease transmission patterns and assisting public health organizations in formulating prevention strategies. Notably, researchers have examined the effects of vaccination programs, quarantines, and isolation measures on epidemic dynamics. Other studies have highlighted the influence of environmental factors, such as temperature, humidity, and rainfall, on pathogen spread. Historically, much of this research has relied on deterministic models based on ordinary differential equations. However, there are also stochastic systems and agent-based models that contribute to our understanding. Agent-based approaches account for individual behaviors, while stochastic models offer more realistic representations of random events. In this paper, we propose new mathematical models to explain the transmission of infectious diseases in a closed population. Our goal is to analyze how various factors affect the system's response, specifically examining whether the infection will naturally die out or lead to an outbreak. To facilitate this, we first introduce the basic reproduction number, R0, which represents the average number of secondary infections generated by an infected individual.",
        "ori-fast-z-score": -0.6172133998483676,
        "water-fast-z-score": 10.338324447460158,
        "rewrite-fast-z-score": 2.0059435495071947
    },
    {
        "original_text": "We present an analytic solution to the steady state distribution for the mechanistic home-range model developed by Moorcroft et al. (2006) that allows for efficient computation of home ranges using numerical integration methods. The new method is implemented as part of the R package adehabitatHR, which also includes functions for computing home ranges with the original algorithm (i.e., without the analytical solution). We demonstrate how our approach can be used to rapidly compute home ranges across large landscapes containing thousands of habitat patches. Our results show that the new method produces identical estimates compared to those obtained with the original algorithm but requires less computational time when estimating home ranges over large spatial extents. Analytical solutions are useful because they allow researchers to efficiently estimate home ranges on very large datasets or at fine resolutions. \n \n Home ranges have been widely studied since their introduction into ecology more than 50 years ago  1  . These areas represent the area within which individuals obtain all necessary resources  2  , such as food  3  , water  4  , shelter  5  , mates  6  , and cover  7  . In addition to being important for understanding animal behavior  8  , home ranges play key roles in conservation biology  9  , wildlife management  10  , epidemiology  11  , and disease transmission  12  .\n \nHome-range models typically assume that animals move through a landscape composed of discrete habitat patches  13  . Animals select among these patches based on some combination of patch attributes  14  , including resource availability  15  , vegetation structure  16  , predation risk  17  , and conspecific density  18  . This process continues until the animal reaches equilibrium between its movement rate and the quality of available habitats  19  . \n \n A number of different approaches exist for modeling animal movements  20  . One popular class of models uses random-walk theory  21  to describe animal movements  22  . Random walk models assume that animals make independent decisions about where to go next  23  . However, this assumption may not always hold true  24  . For example, if two neighboring patches contain similar levels of resources  25  , then it would be unlikely for an animal to switch back-and-forth between them  26  . To account for this type of behavioral response, Moorcro",
        "watermark_text": "We present an analytic solution to the stable state distribution for the mechanistic home - range system established by Moorcroft et al . ( 2006 ) that enables for efficient computation of home ranges using numerical integration methods .The new method is implemented as part of the R program adehabitatHR , which also contains functions for modeling home ranges with the previous algorithm ( i . e . , without the empirical approach ) . We suggest how our approach can be used to rapidly compute bedroom ranges across large landscapes containing thousands of environment patches .Our results show that the new method generates similar estimates compared to those achieved with the previous algorithm but requires fewer computational time when estimating bedroom ranges over large geographic extents . Analytical approaches are helpful because they allow scientists to easily assess home ranges on very huge datasets or at fine resolutions .Home ranges have been widely examined since their arrival into ecosystems more than 50 weeks ago 1 . These zones represent the territory within which adults obtain all necessary resources 2 , such as feed 3 , water 4 , protection 5 , mates 6 , and cover 7 .In addition to being important for explaining animal behavior 8 , home ranges represent crucial roles in wildlife dynamics 9 , conservation conservation 10 , epidemiology 11 , and illness transmission 12 . Home - range systems often assume that animals go through a landscape composed of linear habitat patches 13 .Animals select among these patches based on some mix of patch traits 14 , covering habitat availability 15 , vegetation system 16 , predation risk 17 , and conspecific density 18 . This process proceeds until the organism reaches stability between its movement rate and the performance of available environments 19 .A variety of different methods exist for modeling animal activities 20 . One popular family of models using random - walk models 21 to explain animal activities 22 .Random step models believe that individuals give independent choice about where to going next 23 . However , this assumption must not always hold false 24 .For instance , if two adjacent regions contain similar rates of assets 25 , then it would be impossible for an organism to shift back - and - forth between them 26 . To account for this form of behavioral reaction , Moorcro",
        "rewrite_text": "We offer an analytical solution for the stable state distribution of the mechanistic home-range system developed by Moorcroft et al. (2006), facilitating efficient computation of home ranges through numerical integration methods. This new method is integrated into the R package adehabitatHR, which also includes functions for modeling home ranges based on the previous algorithm (i.e., without the empirical approach). We demonstrate how our method can quickly calculate home ranges across extensive landscapes featuring thousands of environmental patches. Our findings indicate that the new method produces estimates comparable to those generated by the previous algorithm, while requiring significantly less computational time for estimating home ranges across large geographic areas. Analytical approaches are advantageous as they enable researchers to efficiently evaluate home ranges on vast datasets or at fine resolutions. Home ranges have been extensively studied since their introduction into ecosystems over 50 years ago. These areas signify the territories in which adults acquire all essential resources, including food, water, shelter, mates, and cover. Besides their significance in understanding animal behavior, home ranges play vital roles in wildlife dynamics, conservation, epidemiology, and disease transmission. Home-range systems typically assume that animals traverse landscapes made up of linear habitat patches, selecting among these based on a combination of patch characteristics, habitat availability, vegetation structure, predation risk, and conspecific density. This selection process continues until the organism achieves a balance between its movement rate and the performance of the available environments. Numerous methods exist for modeling animal activities, with one common group employing random-walk models to elucidate these behaviors. Random step models posit that individuals make independent choices regarding their next movements. However, this assumption does not always hold true. For example, if two adjacent areas offer similar resource levels, it would be unrealistic for an organism to continuously switch back and forth between them. To address this type of behavioral response, Moorcroft...",
        "ori-fast-z-score": -1.7230995806825715,
        "water-fast-z-score": 10.625780747542525,
        "rewrite-fast-z-score": 3.0308072109137485
    },
    {
        "original_text": "The effect of the lubricant viscosity on friction in elastohydrodynamic (EHD) regime is investigated by using numerical simulations based on Reynolds equation coupled with elasticity equations for an axisymmetric slider bearing system. The results show that, at low sliding speeds, the traction decreases as the lubricant viscosity increases due to the increase of pressure gradient along the film thickness direction. At high sliding speeds, however, the opposite trend occurs because the viscous shear stress becomes dominant over the pressure gradient term. In mixed lubrication regimes where both hydrodynamic and boundary layers exist simultaneously, it was found that the minimum traction can be reduced significantly when the lubricant viscosity is increased. This reduction is attributed mainly to the decrease of the maximum pressure in the contact region. \n \n Keywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient Term",
        "watermark_text": "The impact of the lubricant viscosity on friction in elastohydrodynamic ( EHD ) regime is investigated by using numerical simulations based on Reynolds function combined with elasticity equations for an axisymmetric slider bearing system . The results show that , at low rolling rates , the traction decreases as the lubricant viscosity increases owing to the increase of tension gradient along the film thickness direction .At high sliding speeds , however , the opposite pattern occurs because the viscous shear force remains dominant over the pressure gradient term . In combined lubrication regimes where both hydrodynamic and boundary sheets arise simultaneously , it was shown that the average traction can be reduced greatly when the lubricant viscosity is enhanced .This decrease is attributed mostly to the decrease of the maximum pressure in the contact region . Keywords : Elastohydrodynamics ; Slider Bearing ; Minimum Film Thickness ; Viscosity ; Pressure Gradient Term",
        "rewrite_text": "The effect of lubricant viscosity on friction within the elastohydrodynamic (EHD) regime is explored through numerical simulations that integrate the Reynolds equation with elasticity equations for an axisymmetric slider bearing system. The findings indicate that at low rolling speeds, traction diminishes as lubricant viscosity rises, primarily due to an increased tension gradient along the thickness of the lubricant film. Conversely, at high sliding speeds, traction increases with viscosity because the viscous shear force takes precedence over the pressure gradient. In lubrication regimes where both hydrodynamic and boundary effects coexist, a significant reduction in average traction is observed with higher lubricant viscosity. This reduction is primarily linked to a decrease in the maximum pressure within the contact area. \nKeywords: Elastohydrodynamics; Slider Bearing; Minimum Film Thickness; Viscosity; Pressure Gradient.",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 5.938574464184706,
        "rewrite-fast-z-score": -0.14002800840280097
    },
    {
        "original_text": "We present new mid-IR photometry and spectroscopy of the HUDF-JD2 galaxy at redshift 2.081, which is one of the most luminous infrared galaxies known to date. The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum. We find evidence for both star formation activity (from the UV-optical) as well as obscured AGN activity (from X-ray observations). This object may be representative of a population of dusty star-forming galaxies undergoing rapid evolution during this critical epoch when massive black holes are growing rapidly along with their host galaxies. Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "watermark_text": "We present new mid - IR photometry and spectroscopy of the HUDF - JD2 galaxy at redshift 2 . 081 , which is one of the most luminous infrared galaxies known to date . The SED shows that it has an extremely red continuum with strong PAH emission features in its rest frame optical spectrum .We get data for both star formation activity ( from the UV - optical ) as well as obscured AGN activity ( from X - ray observations ) . This object may be representative of a population of dusty star - creating stars undergoing fast evolution during this critical epoch when massive brown holes are growing faster along with their host galaxies .Keywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies",
        "rewrite_text": "We present new mid-infrared photometry and spectroscopy of the HUDF-JD2 galaxy, located at a redshift of 2.081, which is currently among the most luminous infrared galaxies known. The spectral energy distribution (SED) reveals a notably red continuum accompanied by pronounced PAH emission features in its rest-frame optical spectrum. Our data encompasses both star formation activity, derived from UV-optical observations, and obscured AGN activity indicated by X-ray observations. This galaxy may serve as a representative example of a population of dusty star-forming galaxies that are experiencing rapid evolutionary changes during this crucial period, marked by the accelerated growth of massive black holes alongside their host galaxies. \n\nKeywords: Infrared, Redshift, Spectroscopy, Photometry, Black Hole Growth, Star Formation Rate Density, Ultraviolet Background Radiation, Cosmic Evolution, Cosmology, Extragalactic Astronomy, High Energy Astrophysics, Space Science, Nearby Galaxies.",
        "ori-fast-z-score": 0.2886751345948129,
        "water-fast-z-score": 2.3094010767585034,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "We present an analysis of the ages, metallicities and distances to nearby main-sequence F-type dwarf stars (d < 25 pc) using high-resolution spectroscopy obtained with the HARPS spectrograph on board of the European Space Agency s space observatory COROT. We find that there is no correlation between stellar metallicity and distance from the Sun within this sample. This result suggests that the local thin disk has not been significantly affected by radial migration processes over its lifetime.  The lack of any significant trend in  Fe/H  as a function of distance also implies that the mean age of the local thin disk does not vary strongly across the solar neighbourhood. Using our derived ages we show that the age distribution of the local thin disk can be well described by a single exponential decay law with characteristic timescale τ = 3 Gyr. Our results are consistent with previous studies based on Hipparcos parallaxes but provide more accurate ages due to the higher precision of the spectroscopic data used here.",
        "watermark_text": "We present an assessment of the periods , metallicities and distances to nearby major - sequence F - class dwarf stars ( d < 25 pc ) using high - resolution spectroscopy acquired with the HARPS spectrograph on board of the European Space Agency s space observatory COROT . We see that there is no correlation between stellar metallicity and proximity from the Sun within this specimen .This result suggests that the local thin disk has not been greatly impacted by radial migration cycles over its duration . The absence of any large trend in Fe / H as a function of distance also means that the mean age of the local thin disk does not varies strongly across the solar neighbourhood .Using our derived ages we prove that the age distribution of the local thin disk can be well described by a single exponential decay law with typical timescale τ = 3 Gyr . Our results are compatible with previous research based on Hipparcos parallaxes but give more accurate dating owing to the higher precision of the spectroscopic data used here .",
        "rewrite_text": "We provide an evaluation of the periods, metallicities, and distances of nearby major-sequence F-class dwarf stars (d < 25 pc) utilizing high-resolution spectroscopy obtained with the HARPS spectrograph aboard the European Space Agency's CoRoT space observatory. Our analysis reveals no correlation between stellar metallicity and distance from the Sun in this sample. This finding indicates that the local thin disk has not experienced significant effects from radial migration cycles throughout its history. Additionally, the lack of a substantial trend in the iron-to-hydrogen ratio (Fe/H) with distance suggests that the average age of the local thin disk remains relatively consistent across the solar neighborhood. Based on our calculated ages, we demonstrate that the age distribution of the local thin disk can be accurately described by a single exponential decay law, with a characteristic timescale of τ = 3 Gyr. Our findings are consistent with previous studies based on Hipparcos parallaxes, but they offer improved age estimates due to the enhanced precision of the spectroscopic data employed in this research.",
        "ori-fast-z-score": 0.11396057645963795,
        "water-fast-z-score": 5.661385170722978,
        "rewrite-fast-z-score": 0.9878783399072131
    },
    {
        "original_text": "The present work is devoted to the study of astrophysical plasma turbulence, with particular emphasis on its statistical properties at different scales. We consider both kinetic and fluid descriptions for the dynamics of collisionless plasmas, which are relevant for many space and laboratory applications. In this context we investigate the nonlinear evolution of magnetic fluctuations by means of direct numerical simulations (DNS) of the Vlasov-Maxwell system. The main results can be summarized as follows:  1. Turbulence statistics -We perform DNSs of the Vlasov-Poisson system in order to characterize the statistical properties of the electrostatic potential fluctuations generated by an initial spectrum of Alfvenic modes. Our analysis shows that the energy cascade proceeds towards smaller spatial scales until it reaches the ion Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping. At these small scales, the energy transfer rate decreases due to the reduction of phase correlations between wavevectors. This process leads to the formation of intermittency in the distribution function of particles.  2. Kinetic effects -In addition to the above mentioned features observed in the case of purely hydrodynamic turbulence, our results show that kinetic effects play also an important role in determining the statistical properties of the fluctuating fields. Indeed, we find that the presence of ions modifies significantly the shape of the probability density functions (PDFs), leading to non-Gaussian distributions characterized by tails extending over several orders of magnitude. Moreover, we observe that the PDFs become more skewed when increasing the value of the ion-to-electron mass ratio. Finally, we discuss how the inclusion of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating fields. \n3. Fluid description -By performing DNSs of the Euler-",
        "watermark_text": "The present work is committed to the observation of astrophysical plasma turbulence , with particular emphasis on its statistical characteristics at different scales . We consider both kinetic and fluid representations for the dynamics of collisionless plasmas , which are applicable for numerous space and lab applications .In this framework we investigate the nonlinear development of magnetic fluctuations by means of direct numerical simulations ( DNS ) of the Vlasov - Maxwell process . The main results can be summarized as follows : 1 .Turbulence statistics - We perform DNSs of the Vlasov - Poisson system in order to characterize the statistical characteristics of the electrostatic potential fluctuations formed by an initial spectrum of Alfvenic modes . Our study shows that the power cascade proceeds towards smaller spatial scales until it meets the electron Larmor radius scale where it is transferred into perpendicular wavenumbers through Landau damping .At these little scales , the power transfer frequency falls due to the reduction of phase correlations between wavevectors . This process results to the formation of intermittency in the distribution function of particles .2 . Kinetic effects - In addition to the above mentioned elements observed in the case of solely hydrodynamic turbulence , our findings show that kinetic influences play also an important role in determining the statistical characteristics of the fluctuating fields .Indeed , we find that the presence of atoms modifies substantially the morphology of the probability density functions ( PDFs ) , leading to non - Gaussian functions characterized by tails extending over numerous orders of magnitude . Moreover , we find that the PDFs get more skewed when increasing the value of the electron - to - atom weight ratio .Finally , we explain how the introduction of kinetic effects affects the scaling laws characterizing the power spectra of the fluctuating forces . 3 .Fluid description - By conducting DNSs of the Euler -",
        "rewrite_text": "This study focuses on observing astrophysical plasma turbulence, particularly its statistical features across different scales. We analyze both kinetic and fluid models to understand the dynamics of collisionless plasmas relevant to various space and laboratory scenarios. Within this context, we explore the nonlinear evolution of magnetic fluctuations using direct numerical simulations (DNS) of the Vlasov-Maxwell framework. The key findings can be summarized as follows: \n\n1. **Turbulence Statistics** - We conduct DNS of the Vlasov-Poisson system to investigate the statistical properties of electrostatic potential fluctuations arising from an initial spectrum of Alfvenic modes. Our analysis reveals that the power cascade progresses to smaller spatial scales until it reaches the electron Larmor radius scale, where it shifts to perpendicular wavenumbers due to Landau damping. At these small scales, the power transfer frequency decreases as phase correlations among wave vectors diminish, leading to intermittency in the particle distribution function.\n\n2. **Kinetic Effects** - In addition to the behaviors observed in purely hydrodynamic turbulence, our results indicate that kinetic effects significantly influence the statistical properties of the fluctuating fields. We find that the presence of particles alters the shape of the probability density functions (PDFs), resulting in non-Gaussian distributions with tails that extend across multiple orders of magnitude. Furthermore, as the electron-to-atom weight ratio increases, the PDFs become more skewed. We also discuss how the inclusion of kinetic effects modifies the scaling laws that describe the power spectra of the fluctuating forces.\n\n3. **Fluid Description** - By performing DNS of the Euler-... (the paragraph is incomplete, so please provide additional context for further expansion).",
        "ori-fast-z-score": -1.2374368670764582,
        "water-fast-z-score": 6.251201434911929,
        "rewrite-fast-z-score": -0.7302967433402214
    },
    {
        "original_text": "We propose an efficient reduced complexity sphere decoding (RSD) algorithm for square quadrature amplitude modulation (QAM). The proposed RSD is based on the new lattice representation, which can be viewed as a generalization of the well-known real-valued lattice representation to complex-valued lattices. We show that our RSD has lower computational complexity than existing algorithms in terms of both number of arithmetic operations and memory requirement. In addition, we demonstrate by simulation results that our RSD achieves better bit error rate performance over conventional RSDs at high signal-to-noise ratio region. Finally, we provide some insights into how to further reduce the computational complexity of our RSD without sacrificing its BER performance. Index Terms - Reduced complexity sphere decoding, Quadrature amplitude modulation, Computational complexity reduction, Bit error rate performance improvement. I. INTRODUCTIO N Quadrature amplitude modulation (Q AM )  1  , also known as phase-shift keying (PSK), is one of the most popular digital modulations used in wireless communications due to its simple implementation  2  . However, it suffers from poor power efficiency when compared with other high-order constellations such as 16-QAM or 64-QAM  3  .\nIn order to improve the power efficiency while maintaining good bit error rate (BER) performance, many research efforts have been made recently  4  -  8  . Among them, reduced complexity sphere decoding (RCSD)  9  -  11  plays an important role because RCSD provides near optimal BER performance with much less computational complexity than maximum-likelihood detection  12  . For example, the authors in  10  developed a novel RCSD scheme for square QAM using the so-called real-valued lattice representation  13  . It was shown in  14  that this approach requires only about half of the number of arithmetic operations required by the original RCSD  15  . Moreover, the authors in  16  showed that their RCSD outperforms the previous works  17  ,  19  in terms of BER performance under various channel conditions. Although these approaches are very promising, they still suffer from relatively large computational complexity especially at low-to-medium SNR",
        "watermark_text": "We suggest an efficient reduced complexity sphere decoding ( RSD ) algorithm for square quadrature amplitude modulation ( QAM ) . The proposed RSD is based on the new lattice representation , which can be viewed as a generalization of the better - famous real - valued lattice representation to complex - valued lattices .We suggest that our RSD has reduced computational complexity than existing algorithms in terms of both number of logic operations and memory need . In addition , we prove by simulation data that our RSD achieves good bit error rate quality over traditional RSDs at high noise - to - noise ratio region .Finally , we provide some understanding into how to further reduce the numerical capacity of our RSD without sacrificing its BER performance . Index Terms - Reduced complexity sphere decoding , Quadrature amplitude modulation , Computational complexity reduction , Bit error rate quality increase .I . INTRODUCTIO N Quadrature amplitude modulation ( Q AM ) 1 , sometimes called as phase - shift keying ( PSK ) , is one of the most popular digital modulations used in mobile communications thanks to its simple specification 2 . However , it suffers from poor energy efficiency when compared with other high - order constellations such as 16 - QAM or 64 - QAM 3 .In order to upgrade the power performance while maintaining good bit error rate ( BER ) performance , various study efforts have been made recently 4 - 8 . Among them , low complexity sphere decoding ( RCSD ) 9 - 11 holds an important role because RCSD presents near perfect BER results with far lower mathematical complexity than maximum - likelihood recognition 12 .For instance , the writers in 10 developed a new RCSD system for square QAM using the so - called real - valued lattice representation 13 . It was shown in 14 that this methodology demands only about half of the number of arithmetic functions needed by the original RCSD 15 .Moreover , the articles in 16 demonstrated that their RCSD outperforms the previous works 17 , 19 in terms of BER performance under various network conditions . Although these solutions are very promising , they still suffer from extremely huge computational difficulty including at low - to - medium SNR",
        "rewrite_text": "We propose an efficient reduced complexity sphere decoding (RSD) algorithm for square quadrature amplitude modulation (QAM). The RSD we developed utilizes a novel lattice representation that extends the well-known real-valued lattice framework to complex-valued lattices. Our RSD demonstrates reduced computational complexity compared to existing methods, both in terms of the number of logical operations and memory requirements. Additionally, simulations show that our RSD maintains superior bit error rate (BER) performance compared to traditional RSDs, especially in high signal-to-noise ratio (SNR) scenarios. We also offer insights on further minimizing the numerical capacity of our RSD without compromising its BER performance. \n\nIndex Terms: Reduced complexity sphere decoding, Quadrature amplitude modulation, Computational complexity reduction, Enhanced bit error rate performance.\n\nI. INTRODUCTION\nQuadrature amplitude modulation (QAM), sometimes referred to as phase-shift keying (PSK), is widely used in mobile communications due to its straightforward specification. However, it exhibits lower energy efficiency compared to higher-order constellations like 16-QAM or 64-QAM. To enhance power performance while ensuring a good bit error rate (BER), numerous research efforts have recently been undertaken. Among these, reduced complexity sphere decoding (RCSD) plays a significant role, as it provides near-optimal BER results with considerably less computational complexity than maximum-likelihood detection. For example, researchers have developed a new RCSD system for square QAM using the real-valued lattice representation, which has been shown to require only about half the arithmetic operations of the original RCSD algorithm. Furthermore, studies have indicated that this approach outperforms previous methods in terms of BER across various network conditions. Despite the promise of these solutions, they still face significant computational challenges, particularly at low to medium SNR levels.",
        "ori-fast-z-score": 1.5261167249147478,
        "water-fast-z-score": 9.337616548271178,
        "rewrite-fast-z-score": 1.720387033089985
    },
    {
        "original_text": "We present Chandra observations of supernova (SN) 2004et, which is one of only two type IIp SNe ever observed in X-rays. The data were obtained on 2005 February 24-26 with the Advanced CCD Imaging Spectrometer (ACIS-S). We detect no significant emission above background at energies below 1 keV or above 8 keV; we therefore restrict our analysis to the range 1-8 keV. In this energy band, we find that the spectrum can be fit by an absorbed blackbody model with kT = 0.7 ± 0.1 keV and N H = 2.5 +1.0 −0.8 × 10 22 cm −2 . These values are consistent with those found for other type IIp SNe. Using these parameters as well as the distance inferred from optical photometry, we calculate the luminosity of SN 2004et during its first 100 days after explosion. This value agrees very well with theoretical predictions based upon models of stellar evolution.",
        "watermark_text": "We report Chandra measurements of supernova ( SN ) 2004et , which is one of only two class IIp SNe actually seen in X - radiation . The data were obtained on 2005 February 24 - 26 with the Advanced CCD Imaging Spectrometer ( ACIS - S ) .We detect no considerable emission above background at energies below 1 keV or above 8 keV ; we thus restrict our analysis to the range 1 - 8 keV . In this power band , we find that the spectrum can be fit by an absorption blackbody simulation with kT = 0 . 7 ± 0 . 1 keV and N H = 2 . 5 + 1 . 0 −0 . 8 × 10 22 mm −2 .These quantities are compatible with those observed for other class IIp SNe . Using these parameters as well as the distance inferred from optical photometry , we estimate the luminosity of SN 2004et during its initial 100 days after explosion .This value agrees very best with theoretical estimates based upon theories of stars evolution .",
        "rewrite_text": "We present Chandra observations of supernova (SN) 2004et, which is one of only two class IIp SNe detected in X-rays to date. The data were collected from February 24 to 26, 2005, using the Advanced CCD Imaging Spectrometer (ACIS-S). We observe no significant emission above the background in the energy ranges below 1 keV or above 8 keV, leading us to focus our analysis on the 1 to 8 keV range. Within this energy band, we find that the spectrum can be modeled by an absorbed blackbody spectrum with a temperature of kT = 0.7 ± 0.1 keV and a column density of N_H = 2.5 +1.0 -0.8 × 10^22 cm^(-2). These parameters align with those seen in other class IIp SNe. Utilizing these measurements and the distance derived from optical photometry, we estimate the luminosity of SN 2004et during the first 100 days post-explosion. This luminosity aligns closely with theoretical predictions based on stellar evolution models.",
        "ori-fast-z-score": -0.42008402520840293,
        "water-fast-z-score": 5.285714285714286,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an algorithm for the decomposition of the local stellar kinematics into vector spherical harmonic functions (VSH). The method is applied to simulated data and real observations, where we recover the underlying VSH coefficients with high accuracy. We show that our approach can be used as a powerful tool in galactic dynamics studies by recovering the gravitational potential of the Milky Way s dark matter halo. In addition, it allows us to study the anisotropy of the stellar orbits on different scales. \n \n Keywords: Vector spherical harmonics, Galactic dynamics, Stellar kinematics, Gravitational potentials \n \n 1 Introduction \n \n Spherical Harmonic Analysis has been widely used over many decades to analyse astronomical datasets such as galaxy surveys or star counts. However, this technique cannot easily be extended to deal with non-scalar quantities like velocities or accelerations. This problem was overcome by expanding these quantities onto vector spherical harmonics (VSH) which are defined as tensor products of scalar spherical harmonics  1  . These new basis functions have already found applications in fields ranging from cosmology  2  , solar physics  3  , heliophysics  4  and geophysics  5  .\n \nIn recent years there has been growing interest in using VSHs to model the observed properties of galaxies  6  -  8  . For example, they were recently employed to decompose the line-of-sight component of the stellar kinematics  9  . Here, we extend their use to also include the tangential components of the stellar motions. As a result, we obtain a complete description of the three-dimensional distribution of the stellar kinematics within each spatial bin. Moreover, since the expansion coefficients depend only on angular coordinates, they can be determined independently at every point along the line-of-sight. Therefore, our method does not require any assumptions about the symmetry of the system under investigation. \n2 Vector spherical harmonics",
        "watermark_text": "We present an algorithm for the decomposition of the local stars kinematics into vector spherical harmonic functions ( VSH ) . The method is applied to modeled images and actual observations , where we recover the underlying VSH coefficients with high efficiency .We suggest that our approach can be used as a powerful tool in galactic dynamics experiments by searching the gravitational potential of the Milky Way s dark matter halo . In addition , it allows us to study the anisotropy of the stars orbits on various scales .Keywords : Vector spherical harmonics , Galactic mechanics , Stellar kinematics , Gravitational potentials 1 Introduction Spherical Harmonic Analysis has been widely using over numerous centuries to analyse astronomical datasets such as galaxy surveys or moon numbers . However , this methodology cannot often be generalized to deal with non - scalar parameters like velocities or accelerations .This problem was resolved by expanding these quantities onto vector spherical harmonics ( VSH ) which are specified as vector products of scalar spherical harmonics 1 . These new basis variables have already found uses in areas ranging from cosmology 2 , lunar science 3 , heliophysics 4 and geophysics 5 .In past times there has been growing interest in utilizing VSHs to model the known characteristics of galaxies 6 - 8 . For instance , they were recently employed to decompose the line - of - view component of the stellar kinematics 9 .Here , we stretch their application to additionally include the tangential parts of the stars movements . As a result , we obtain a complete model of the three - dimensional distribution of the stars kinematics within each spatial bin .Moreover , since the expansion equations depend only on angular coordinates , they can be determined independently at every position along the line - of - view . Therefore , our technique does not require any constraints about the symmetry of the process under investigation .2 Vector spherical harmonics",
        "rewrite_text": "We introduce an algorithm designed to decompose the kinematics of local stars into vector spherical harmonic functions (VSH). This method is applied to both simulated images and real observations, efficiently recovering the underlying VSH coefficients. We propose that our technique can serve as a robust tool for galactic dynamics research, particularly in exploring the gravitational potential of the Milky Way’s dark matter halo. Additionally, this approach facilitates the investigation of the anisotropy of stellar orbits across various scales.\n\n**Keywords**: Vector spherical harmonics, Galactic dynamics, Stellar kinematics, Gravitational potentials\n\n**1 Introduction**  \nSpherical harmonic analysis has been widely used for centuries to analyze astronomical datasets, including galaxy surveys and lunar observations. However, this methodology frequently struggles to effectively handle non-scalar parameters such as velocities or accelerations. This challenge is addressed by expanding these quantities into vector spherical harmonics (VSH), defined as vector products of scalar spherical harmonics. These new basis functions have found applications in diverse fields, including cosmology, lunar science, heliophysics, and geophysics. Recently, there has been an increasing interest in using VSH to model the properties of galaxies. For example, they have been employed to decompose the line-of-sight component of stellar kinematics. In this work, we extend their application to encompass the tangential components of stellar motion, resulting in a comprehensive model of the three-dimensional distribution of stellar kinematics within each spatial bin. Furthermore, since the expansion equations depend solely on angular coordinates, the coefficients can be determined independently at any position along the line of sight, allowing for analysis without assumptions regarding the symmetry of the process being studied. \n\n**2 Vector Spherical Harmonics**",
        "ori-fast-z-score": -1.4967665407535604,
        "water-fast-z-score": 7.60122529295103,
        "rewrite-fast-z-score": 0.6211495565912797
    },
    {
        "original_text": "We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at interfaces.  The interface between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), which are both parent compounds for high temperature superconductivity, is found to be highly conducting despite the large lattice mismatch between LSMO and YBCO. This suggests that charge transfer across the interface occurs due to strong electronic hybridization rather than strain relaxation alone. We also find that the hole concentration in the YBCO layer can be controlled by varying the thickness of the LSMO layer grown on top of it. These results demonstrate an alternative approach towards engineering the carrier density in cuprate superconductors using oxide heterostructures. High-temperature superconductivity has been observed only in materials containing copper-oxygen planes known as CuO2 layers  1  . In these systems, holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity  2  . However, the maximum critical temperature Tc = 92 K achieved so far in this class of materials is still well below the theoretical limit predicted by Bardeen-Cooper-Schrieffer theory  3  , raising questions about how to further enhance Tc  4  .\nIn recent years there have been significant efforts made to explore new routes toward enhancing Tc beyond its current record value  5  . One promising route involves introducing electrons into the CuO2 plane  6  . For example, replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system  7, 8  . Alternatively, one may introduce electrons directly into the CuO2 plane by growing thin films of transition metal oxides such as SrTiO3  9  or LaAlO3  10  onto the surface of cuprate superconductors. While these approaches show promise, they require precise control over film composition and structure during deposition  11  . An alternative strategy would involve controlling the carrier density in cuprates without changing their crystal structures  12  .",
        "watermark_text": "We report on the observation of electron doping in cuprate superconductors by interfacing them to manganite insulators through epitaxial growth and chemical bonding at connections . The interface between La0 . 7Sr0 . 3MnO3 ( LSMO ) and YBa2Cu3O6 + x ( YBCO ) , which are both parent molecules for high heat superconductivity , is found to be highly conducting despite the huge lattice mismatch between LSMO and YBCO .This implies that charge transfer across the interface comes driven to strong electronic hybridization instead than strain relaxation alone . We additionally find that the gap concentration in the YBCO layer can be determined by varying the height of the LSMO layer grown on top of it .These data demonstrate an additional method towards engineering the carrier density in cuprate superconductors using oxide heterostructures . High - temperature superconductivity has been observed only in structures carrying copper - oxygen planes known as CuO2 layers 1 .In these systems , holes doped into the CuO2 plane give rise to Cooper pairs leading to superfluidity 2 . However , the maximum essential temperature Tc = 92 K attained so far in this class of substances is already much below the theoretical maximum expected by Bardeen - Cooper - Schrieffer model 3 , placing questions about how to further enhance Tc 4 .In recent seasons there have been significant efforts made to pursue new routes toward promoting Tc beyond its current record value 5 . One promising route includes introducing electrons into the CuO2 plane 6 .For instance , replacing oxygen atoms in the CuO2 plane with fluorine leads to a reduction in the number of holes in the system 7 , 8 . Alternatively , one may introduce electrons directly into the CuO2 plane by expanding narrow bands of transition iron oxides such as SrTiO3 9 or LaAlO3 10 onto the surface of cuprate superconductors .While these models show success , they demand exact power over movie structure and shape during deposition 11 . An alternative approach would include governing the carrier density in cuprates without altering their crystal structures 12 .",
        "rewrite_text": "We present our findings on electron doping in cuprate superconductors achieved by interfacing them with manganite insulators through techniques such as epitaxial growth and chemical bonding. The junction between La0.7Sr0.3MnO3 (LSMO) and YBa2Cu3O6+x (YBCO), which are both foundational materials for high-temperature superconductivity, exhibits significant conductivity despite the considerable mismatch in their lattice structures. This observation suggests that charge transfer across the interface is primarily driven by strong electronic hybridization rather than strain relaxation. Furthermore, we have discovered that the gap concentration in the YBCO layer can be adjusted by changing the thickness of the LSMO layer deposited on it. These results highlight a new strategy for engineering carrier density in cuprate superconductors using oxide heterostructures. High-temperature superconductivity has only been achieved in materials featuring copper-oxygen planes, known as CuO2 layers. Within these systems, holes introduced into the CuO2 plane facilitate the formation of Cooper pairs, resulting in superfluidity. However, the highest critical temperature (Tc = 92 K) reached in this class of materials remains significantly below the theoretical upper limit predicted by the Bardeen-Cooper-Schrieffer model, raising questions about how to further elevate Tc. Recent research has explored various avenues to surpass the current maximum Tc, with one promising direction being the introduction of electrons into the CuO2 plane. For example, substituting oxygen atoms with fluorine reduces the number of holes, while directly injecting electrons into the CuO2 plane can be achieved by layering transition metal oxides such as SrTiO3 or LaAlO3 onto cuprate superconductors. Although these methods have shown promise, they require precise control over film structure and morphology during deposition. An alternative strategy could involve modulating the carrier density in cuprates without altering their crystal structures.",
        "ori-fast-z-score": 0.33567254331867563,
        "water-fast-z-score": 7.7754191435023525,
        "rewrite-fast-z-score": 1.7272727272727273
    },
    {
        "original_text": "We propose to use the concept of Maxwell s demon in order to explain how nanodevices can be used for information processing, storage or transmission. We show that this approach is useful because it allows us to understand why some devices are more efficient than others at performing these tasks. In particular we consider two types of nanodevices which have been proposed recently as candidates for quantum computers - spin chains and arrays of coupled cavities. The first type consists of an array of spins arranged on a line with nearest neighbour interactions between them while the second one has a similar structure but instead of spins it contains atoms trapped inside optical cavities. For both cases we calculate their efficiency using the Landauer principle. Finally we discuss possible experimental implementations of our ideas. Introduction:-The idea of Maxwell s demon was introduced by James Clerk Maxwell (1831-1879)  1  . It describes a hypothetical intelligent being who could control microscopic particles individually so that they would always move into separate containers depending on whether each particle had a higher energy level or lower energy level  2  .\nMaxwell s demon is usually described as a thought experiment whose purpose is to demonstrate that entropy cannot decrease spontaneously  3  , i.e., that heat does not flow spontaneously from hot bodies to cold ones  4  . However, there exists another interpretation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities  5  . This interpretation leads naturally to the question about what sort of physical system might behave like such a device  6  .",
        "watermark_text": "We suggest to use the idea of Maxwell s demon in order to explain how nanodevices can be used for information processing , processing or transmission . We suggest that this methodology is beneficial because it allows us to realize why some machines are more efficient than others at performing these tasks .In particular we investigate two forms of nanodevices which have been proposed lately as candidates for quantum computers - spinning chains and arrays of coupled cavities . The first sort consists of an array of spinning grouped on a line with nearest neighbour interactions between them while the second one has a similar composition but instead of spinning it contains atoms trapped inside optical cavities .For both cases we estimate their productivity using the Landauer law . Finally we talk proposed experimental implementations of our concepts .Introduction : - The idea of Maxwell s demon was introduced by James Clerk Maxwell ( 1831 - 1879 ) 1 . It describes a hypothetical intellectual being who might control microscopic particles individually so that they would everyone travel into independent tanks depending on whether each particle had a higher energy level or lesser energy level 2 .Maxwell s creature is usually characterized as a thought experiment whose purpose is to demonstrate that entropy cannot fall spontaneously 3 , i . e . , that heat does not flow spontaneously from hot bodies to hot ones 4 . However , there exists another explanation of Maxwell s demon according to which he imagined a device capable of sorting individual molecules based on their velocities 5 .This interpretation turns naturally to the question about what sort of natural system might perform like such a device 6 .",
        "rewrite_text": "We propose employing the concept of Maxwell's demon to elucidate the role of nanodevices in information processing, transmission, or computation. This approach is advantageous as it helps us understand why certain machines are more effective than others in executing these tasks. Specifically, we examine two types of nanodevices recently identified as potential candidates for quantum computing: spinning chains and arrays of coupled cavities. The first type features an array of spinning objects aligned in a line with interactions occurring between nearest neighbors, while the second type consists of atoms confined within optical cavities arranged in a similar structure. For both configurations, we analyze their efficiency using the Landauer principle. Lastly, we discuss proposed experimental implementations of our ideas.\n\nIntroduction: The concept of Maxwell's demon, introduced by James Clerk Maxwell (1831-1879), describes a hypothetical entity capable of manipulating microscopic particles on an individual basis, directing them into separate containers based on their energy levels—either higher or lower. Maxwell's demon is often regarded as a thought experiment designed to illustrate that entropy cannot spontaneously decrease, supporting the principle that heat does not flow from colder to hotter bodies without external work. Nevertheless, another interpretation exists, suggesting that Maxwell envisioned a device that could sort molecules based on their velocities. This leads naturally to the inquiry of what kind of natural systems could function similarly to such a device.",
        "ori-fast-z-score": 0.09016696346674323,
        "water-fast-z-score": 6.825001477017446,
        "rewrite-fast-z-score": -0.2822162605150792
    },
    {
        "original_text": "We present an explicit expression for the effective potential in terms of renormalization group functions, which is valid to all orders in perturbation theory and can be used as input into numerical calculations.  We show that this result agrees with previous results obtained by other methods at one-loop order. The new formula has several advantages over existing expressions:  It does not require any additional approximations beyond those inherent in the use of perturbation theory; it allows us to calculate the effective potential directly without having to solve complicated integral equations first; finally, we are able to obtain the full dependence on temperature and chemical potentials simultaneously. This work was supported by the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. In recent years there have been many attempts to determine the effective potential Veff(T) = ln Z(T), where T denotes the temperature, using various techniques such as the imaginary time formalism  1  , the real time formalism  2  or the functional renormalization group (FRG)  3  . These approaches provide useful information about the phase structure of quantum field theories but they usually involve some kind of approximation scheme. For example, in the FRG approach one often uses truncations of the exact flow equation  4  .\nIn this letter we will derive an explicit expression for Veff(T) in terms of renormalisation group functions  5  . Our method is based on the observation  6  that the effective action Γk(φ) (where k denotes the momentum scale) satisfies a differential equation known as the Wetterich equation  7, 8  \nHere Rk(Γk; φ) is called the regulator function and describes how the infrared modes are suppressed when integrating out high energy degrees of freedom. By solving Eq. (1) numerically  9  one obtains the running coupling constants gk(φ). Using these quantities together with the corresponding β-functions one can then compute Veff(T) according to",
        "watermark_text": "We present an explicit expression for the effective potential in terms of renormalization group functions , which is valid to all orders in perturbation theory and can be used as input into numerical analyses . We see that this consequence agrees with previous findings obtained by other methods at one - loop order .The revised formula has numerous benefits over existing expressions : It does not require any additional approximations beyond those inherent in the using of perturbation theory ; it allows us to estimate the effective potential directly without having to solve intricate integral equations first ; finally , we are able to obtain the full dependence on pressure and chemical potentials separately . This research was supported by the U . S . Department of Energy under Contract No .DE - AC02 - 76SF00515 . In recent history there have been many efforts to estimate the effective potential Veff ( T ) = ln Z ( T ) , where T denotes the temperature , using numerous algorithms such as the imaginary time formalism 1 , the real time formalism 2 or the functional renormalization group ( FRG ) 3 .These approaches provide useful details about the phase composition of quantum field theories but they generally include some kind of approximation scheme . For instance , in the FRG method one frequently uses truncations of the exact flow formula 4 .In this letter we will generate an explicit expression for Veff ( T ) in terms of renormalisation group functions 5 . Our formula is based on the observation 6 that the effective operation Γk ( φ ) ( where k denotes the velocity scale ) satisfies a differential equation known as the Wetterich equation 7 , 8 Here Rk ( Γk ; φ ) is dubbed the regulator integral and explains how the infrared modes are suppressed when integrating out large energy degrees of liberty .By solving Eq . ( 1 ) numerically 9 one obtains the running coupling constants gk ( φ ) .Using these quantities combined with the associated β - functions one can then compute Veff ( T ) according to",
        "rewrite_text": "We provide a clear formulation for the effective potential expressed in terms of renormalization group functions, applicable to all orders in perturbation theory, and suitable for input in numerical analyses. This result aligns with earlier findings from other methodologies at one-loop order. Our revised formula offers several advantages over existing ones: it requires no additional approximations other than those inherent in perturbation theory; it enables direct estimation of the effective potential without the need to first solve complex integral equations; and it allows us to fully account for the individual influences of pressure and chemical potentials. This research was funded by the U.S. Department of Energy under Contract No. DE-AC02-76SF00515. In recent years, there have been numerous attempts to estimate the effective potential V_eff(T) = ln Z(T), where T represents the temperature, using various algorithms such as the imaginary time formalism, the real time formalism, and the functional renormalization group (FRG). While these methods provide valuable insights into the phase structure of quantum field theories, they typically involve some form of approximation. For example, the FRG method often relies on truncating the exact flow equations. In this letter, we will derive an explicit expression for V_eff(T) in terms of renormalization group functions. Our approach is founded on the understanding that the effective operation Γ_k(φ)—where k signifies the velocity scale—satisfies the Wetterich equation. The regulator integral R_k(Γ_k; φ) describes how infrared modes are suppressed when integrating out higher energy degrees of freedom. By numerically solving this equation, we can derive the running coupling constants g_k(φ). These quantities, along with their corresponding β-functions, enable the computation of V_eff(T).",
        "ori-fast-z-score": 1.1188618555710317,
        "water-fast-z-score": 7.717436331412897,
        "rewrite-fast-z-score": 2.1677749238103
    },
    {
        "original_text": "We present an analysis of the kinematics, metallicity distribution function (MDF), and chemical abundances in the outer halo of our Galaxy using data obtained with the Subaru Telescope for two fields along the leading arm of the Magellanic stream. We find that the MDFs are well represented by three Gaussian components centered at  Fe/H  = -1.7, -0.9, and +0.2 dex, respectively. The metal-poor component is found to be associated with the Galactic thick disk/halo population, while both intermediate- and high-metallicity populations show significant differences between the two fields. In particular, we detect a large fraction of high-alpha stars in one field but not in another located farther away from the center of the LMC. These results suggest that the origin of these streams may have been triggered by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and/or the LMC.",
        "watermark_text": "We present an assessment of the kinematics , metallicity distribution relation ( MDF ) , and biological abundances in the exterior halo of our Galaxy using data acquired with the Subaru Telescope for two fields along the led limb of the Magellanic stream . We see that the MDFs are best represented by three Gaussian components centered at Fe / H = - 1 . 7 , - 0 . 9 , and + 0 . 2 dex , respectively .The metal - weak component is found to be correlated with the Galactic thick disk / halo population , while both intermediate - and large - metallicity populations display significant variations between the two fields . In particular , we find a large fraction of high - alpha objects in one field but not in another situated closer back from the center of the LMC .These data suggest that the origin of these streams may have been caused by tidal interactions between the Milky Way and its satellite galaxies such as the Sgr dwarf galaxy and / or the LMC .",
        "rewrite_text": "We provide an analysis of the kinematics, metallicity distribution function (MDF), and elemental abundances in the outer halo of our Galaxy using data obtained from the Subaru Telescope in two fields along the leading edge of the Magellanic Stream. Our findings indicate that the MDFs can be accurately characterized by three Gaussian components centered at [Fe/H] = -1.7, -0.9, and +0.2 dex, respectively. The metal-poor component shows a correlation with the Galactic thick disk/halo population, while the populations with intermediate and high metallicities exhibit considerable differences between the two fields. Notably, one field has a significant presence of high-alpha objects, whereas the other, located further from the center of the Large Magellanic Cloud (LMC), does not. These results suggest that the formation of these streams may have been influenced by tidal interactions between the Milky Way and its satellite galaxies, such as the Sagittarius dwarf galaxy and/or the LMC.",
        "ori-fast-z-score": -1.0504514628777804,
        "water-fast-z-score": 4.464418717230567,
        "rewrite-fast-z-score": -1.3608276348795434
    },
    {
        "original_text": "The collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is predicted to occur in about 4 billion years.  This will be one of the most spectacular events ever witnessed by humans.   In this talk I will describe how we can use observations made with telescopes on Earth as well as space-based observatories such as Hubble Space Telescope to study these collisions and learn more about dark matter, galaxies, stars, black holes, and other cosmic phenomena that are part of our universe. I will also discuss some of my research projects related to studying galaxy mergers using data obtained at the W.M. Keck Observatory located on Mauna Kea, Hawaii. Finally, I ll share what it was like for me to work there during my summer internship last year. Keywords: Dark Matter, Galaxy merger, Black Hole, Cosmic Evolution, Cosmology, Astrophysics",
        "watermark_text": "The interaction between the Milky Way and its closest neighbor , M31 ( Andromeda ) , is predicted to event in about 4 billion years . This will be one of the most dramatic events ever experienced by humans .In this talk I will explain how we can using observations made with telescopes on Earth as well as space - based observatories such as Hubble Space Telescope to study these collisions and learn more about black material , galaxies , stars , white holes , and other cosmic phenomena that are part of our universe . I will also discuss some of my research projects related to researching star mergers using data received at the W . M .Keck Observatory located on Mauna Kea , Hawaii . Finally , I ll share what it was like for me to work there during my summer apprenticeship last year .Keywords : Dark Matter , Galaxy merger , Black Hole , Cosmic Evolution , Cosmology , Astrophysics",
        "rewrite_text": "The impending collision between the Milky Way and its nearest neighbor, M31 (Andromeda), is expected to occur in approximately 4 billion years. This event is anticipated to be one of the most significant occurrences in human history. In this presentation, I will explain how we can utilize observations from Earth-based telescopes and space-based observatories like the Hubble Space Telescope to study these cosmic events and gain insights into dark matter, galaxies, stars, black holes, and various other phenomena in our universe. Additionally, I will discuss some of my research projects focused on star mergers, which utilize data collected at the W. M. Keck Observatory on Mauna Kea, Hawaii. Finally, I'll share my experiences from my summer internship there last year. Key topics include dark matter, galaxy mergers, black holes, cosmic evolution, cosmology, and astrophysics.",
        "ori-fast-z-score": 0.848528137423857,
        "water-fast-z-score": 4.808326112068523,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an algorithm to solve numerically the time-dependent equations describing the growth of ionized regions in the early universe, based on the Weighted Essentially Non-Oscillatory (WENO) scheme. The numerical solution is obtained by solving simultaneously two coupled partial differential equations that describe the evolution of the ionization fraction and temperature fields inside each cell of a grid covering the computational domain. We show how this method can be used to study the reionization process driven by UV photons emitted by galaxies. In particular we compare our results with those obtained using the Smooth Particle Hydrodynamics technique. Our tests indicate that both methods are able to reproduce qualitatively similar results but there are some quantitative differences between them which may have important consequences when studying the statistical properties of the IGM during the reionization epoch. \n \n Keywords: Numerical Methods, Computational Science, Cosmology, Reionization",
        "watermark_text": "We present an algorithm to solve numerically the time - dependent equations explaining the development of ionized areas in the early universe , using on the Weighted Essentially Non - Oscillatory ( WENO ) scheme . The mathematical solution is found by solving simultaneously two coupled partial differential equations that explain the evolution of the ionization fraction and heat fields inside each cell of a grid covering the theoretical domain .We see how this technology can be used to study the reionization process driven by UV photons generated by galaxies . In particular we compare our findings with those achieved using the Smooth Particle Hydrodynamics technique .Our tests show that both approaches are able to predict qualitatively identical outcome but there are some quantitative variations between them which would have important implications when examining the empirical features of the IGM during the reionization epoch . Keywords : Numerical Methods , Computational Science , Cosmology , Reionization",
        "rewrite_text": "We introduce an algorithm designed to numerically solve the time-dependent equations that describe the formation of ionized regions in the early universe, utilizing the Weighted Essentially Non-Oscillatory (WENO) scheme. The mathematical solution is derived by simultaneously resolving two coupled partial differential equations that describe the evolution of ionization fraction and heat distribution within each cell of a grid that spans the theoretical domain. This methodology allows for an investigation into the reionization process induced by UV photons emitted from galaxies. Specifically, we compare our results with those obtained through the Smooth Particle Hydrodynamics technique. Our tests demonstrate that both methods yield qualitatively similar results, although some quantitative discrepancies exist, which could significantly affect the examination of the empirical characteristics of the intergalactic medium (IGM) during the reionization epoch. Keywords: Numerical Methods, Computational Science, Cosmology, Reionization.",
        "ori-fast-z-score": -2.1766269588592317,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.6509445549041194
    },
    {
        "original_text": "We present X-ray luminosity functions (XLFs) for normal galaxies at z=0.5-2, based on deep Chandra observations of the Great Observatories Origins Deep Survey-North field. We use photometric redshifts to select galaxy samples with different stellar masses and star formation rates. The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx=1041 erg s-1. Our results show that the faint-end slope is steeper than -1.0 for all mass bins considered here. For massive galaxies (log(M/Msun)>10), we find no significant evolution between z=1.6 and 0.7. However, there appears to be some evidence for evolution towards lower characteristic luminosities as well as flatter slopes at higher redshift. These results suggest that the most massive galaxies may have evolved into less luminous systems over time.  We also compare our results with previous studies which used shallower data or smaller sample sizes.",
        "watermark_text": "We create X - ray luminosity functions ( XLFs ) for normal galaxies at z = 0 . 5 - 2 , relying on deep Chandra measurements of the Great Observatories Origins Deep Survey - North area . We use photometric redshifts to select galaxy samples with various stellar masses and sun formation rates .The XLFs are derived by fitting the observed number counts using a Schechter function plus an exponential cutoff below Lx = 1041 erg s - 1 . Our results show that the faint - end slope is steeper than - 1 . 0 for all mass bins included here .For huge objects ( log ( M / Msun ) > 10 ) , we find no considerable evolution between z = 1 . 6 and 0 . 7 . However , there seems to be some evidence for expansion towards lesser characteristic luminosities as well as flatter slopes at higher redshift .These conclusions show that the most large galaxies must have transformed into fewer luminous systems over time . We additionally compare our findings with previous research which using shallower evidence or smaller data numbers .",
        "rewrite_text": "We have constructed X-ray luminosity functions (XLFs) for normal galaxies at redshifts of z = 0.5 to 2, utilizing deep Chandra observations from the Great Observatories Origins Deep Survey - North region. By employing photometric redshifts, we selected galaxy samples with a range of stellar masses and star formation rates. The XLFs are obtained by fitting the observed number counts with a Schechter function, incorporating an exponential cutoff below Lx = 10^41 erg s^-1. Our findings indicate that the faint-end slope is steeper than -1.0 across all mass bins analyzed. For massive galaxies (log(M/Msun) > 10), we observe no significant evolution between z = 1.6 and 0.7. However, there is some indication of a shift towards lower characteristic luminosities and flatter slopes at higher redshifts. These results suggest that the largest galaxies may have evolved into fewer, less luminous systems over time. Additionally, we compare our results with earlier studies that utilized shallower data or smaller sample sizes.",
        "ori-fast-z-score": -1.0681034923744679,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": -1.0834726777719228
    },
    {
        "original_text": "The renormalization group is used to study the effective action in quantum gravity, which describes how matter fields interact with gravitons.  The gravitational field equations are derived by varying this action with respect to the metric tensor. This approach leads to an understanding of why general relativity has no ultraviolet divergences at one-loop order (i.e., when only one graviton loop is included). It also provides insight into the physical meaning of the cosmological constant term that appears in Einstein s field equations. In addition, it allows us to calculate corrections to the classical theory due to higher-order terms in the expansion of the effective action about flat space-time. Finally, we discuss some open problems related to the application of these techniques to more complicated theories such as superstrings or supergravity. The renormalization group is applied to the problem of studying the effective action in quantum-gravitational systems. We derive the gravitational field equations by varying this action with regard to the metric tensor. These results provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from higher-order terms in the effective-action expansion around flat spacetime.",
        "watermark_text": "The renormalization group is utilized to study the effective act in quantum gravitational , which explains how matter forces behave with gravitons . The gravitational field equations are derived by varying this action with regard to the metric tensor .This method results to an knowledge of why general relativity has no ultraviolet divergences at one - loop order ( i . e . , when only one graviton loop is included ) . It additionally offers knowledge into the physical context of the cosmological continuous term that arises in Einstein s field equations .In addition , it allows us to estimate corrections to the classical theory owing to higher - order terms in the evolution of the effective action about flat space - time . Finally , we explain some open problems related to the application of these concepts to more complicated concepts such as superstrings or supergravity .The renormalization group is applied to the question of determining the effective act in particle - gravitational materials . We derive the gravitational field equations by varying this action with regard to the metric tensor .These data provide insights into the physical meaning of both the cosmological constant appearing in Einstein s field equations and the corrections to the classical theory arising from greater - order terms in the effective - action expansion around flat spacetime .",
        "rewrite_text": "The renormalization group is employed to investigate the effective action in quantum gravity, which sheds light on how matter interacts with gravitons. By varying this action with respect to the metric tensor, we can derive the gravitational field equations. This approach clarifies why general relativity exhibits no ultraviolet divergences at one-loop order (that is, when only one graviton loop is considered). It also provides insight into the physical significance of the cosmological constant that appears in Einstein's field equations. Furthermore, this method allows us to estimate corrections to the classical theory stemming from higher-order terms in the effective action expansion around flat spacetime. Finally, we address several open questions related to applying these concepts to more complex frameworks such as superstrings or supergravity. The renormalization group thus plays a crucial role in enhancing our understanding of the effective action in particle-gravity interactions, as it facilitates the derivation of gravitational field equations through variations in the metric tensor. These insights contribute to our understanding of the cosmological constant and the corrections to classical theory resulting from higher-order terms in the effective action expansion.",
        "ori-fast-z-score": 0.09950371902099892,
        "water-fast-z-score": 5.6,
        "rewrite-fast-z-score": 0.10721125348377948
    },
    {
        "original_text": "The Riemann hypothesis is one of the most famous open problems in mathematics and has been studied for more than 150 years.  In this article we present an outline of the proof that was announced by Andrew Wiles on 16 May 1993 at the International Congress of Mathematicians in Zurich.   The full text can be found here. The Riemann hypothesis is one of  the most famous open problems  in mathematics and has been investigated for over 150 years. It states that all non-trivial zeros of the Riemann zeta function have real part equal to 1/2. This theorem will be proved using the theory of modular forms. A modular form is a complex-valued function defined on the group SL(2,Z) which satisfies certain conditions. We begin with some basic definitions and properties of modular forms before moving onto the main topic of the article - the proof of the Riemann hypothesis given by Andrew Wiles. For further reading see  1  .",
        "watermark_text": "The Riemann conjecture is one of the most famous open problems in math and has been studied for more than 150 years . In this page we present an outline of the proof that was announced by Andrew Wiles on 16 May 1993 at the International Congress of Mathematicians in Zurich .The full text can be found here . The Riemann conjecture is one of the most famous open problems in math and has been investigated for over 150 years .It says that all non - trivial zeros of the Riemann zeta system have real part equal to 1 / 2 . This formula will be proved using the principle of modular forms .A modular structure is a complex - valued function characterized on the group SL ( 2 , Z ) which satisfies certain conditions . We begin with some fundamental definitions and features of modular forms before moving onto the main theme of the article - the proof of the Riemann conjecture given by Andrew Wiles .For further reading see 1 .",
        "rewrite_text": "The Riemann conjecture is one of the most renowned unsolved problems in mathematics, having been explored for over 150 years. In this article, we provide an outline of the proof announced by Andrew Wiles on May 16, 1993, at the International Congress of Mathematicians in Zurich. The complete text of the proof can be accessed here. The conjecture asserts that all non-trivial zeros of the Riemann zeta function have a real part of 1/2. This assertion will be demonstrated through the use of modular forms. A modular form is a complex-valued function defined on the group SL(2, Z) that meets specific criteria. We will start with some essential definitions and properties of modular forms before delving into the main topic of the article—Andrew Wiles' proof of the Riemann conjecture. For additional information, please refer to source 1.",
        "ori-fast-z-score": 1.0504514628777804,
        "water-fast-z-score": 4.989644448669457,
        "rewrite-fast-z-score": 1.1094003924504583
    },
    {
        "original_text": "We report the discovery of a new isolated neutron star (INS) candidate, PSR J1852+0040, in the southern hemisphere using data collected by the Parkes radio telescope and the Australia Telescope Compact Array (ATCA). The pulsar was discovered during a search for millisecond pulsars with high proper motions. It has a spin period P = 1.4 ms and is located at a distance D = 3 kpc. Its dispersion measure DM = 0.6 pc cm-3 implies that it lies behind most of the galactic disk but not far enough to be associated with any known supernova remnant or open cluster. We have also detected its X-ray counterpart in archival Chandra observations. This source appears point-like and shows no evidence of extended emission. Based on these properties we conclude that this object is likely to be a young INS. If confirmed as such, our results will provide important constraints on models of pulsar formation and evolution. \n \n Keywords: Neutron stars",
        "watermark_text": "We report the discovery of a new isolated neutron star ( INS ) candidate , PSR J1852 + 0040 , in the southern hemisphere using data provided by the Parkes radio telescope and the Australia Telescope Compact Array ( ATCA ) . The pulsar was discovered during a scan for millisecond pulsars with high proper motions .It has a spinning period P = 1 . 4 ms and is situated at a distance D = 3 kpc . Its dispersion measure DM = 0 . 6 pc mm - 3 implies that it lies behind most of the galactic disk but not long enough to be identified with any observed supernova remnant or open cluster .We have already detected its X - ray counterpart in archival Chandra measurements . This source looks point - like and shows no evidence of extended decay .Based on these characteristics we determine that this body is probably to be a young INS . If confirmed as such , our findings will provide important restrictions on estimates of pulsar structure and evolution .Keywords: Neutron stars",
        "rewrite_text": "We present the discovery of a new candidate for an isolated neutron star (INS), designated PSR J1852 + 0040, located in the southern hemisphere, utilizing data from the Parkes radio telescope and the Australia Telescope Compact Array (ATCA). This pulsar was identified while searching for millisecond pulsars exhibiting high proper motions. It has a spin period of P = 1.4 ms and is estimated to be at a distance of D = 3 kpc. The dispersion measure of DM = 0.6 pc cm⁻³ suggests that it is located behind a majority of the galactic disk, but not far enough to associate it with any known supernova remnants or open clusters. Additionally, we have detected its X-ray counterpart through archival observations from Chandra, which appears point-like and shows no signs of extended emission. Given these traits, we propose that this object is likely a young INS. If this is confirmed, our findings could significantly inform models of pulsar structure and evolution. Keywords: Neutron stars.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 5.505585837114527,
        "rewrite-fast-z-score": -0.762000762001143
    },
    {
        "original_text": "We present an analysis of Lyman alpha emitters (LAEs) selected by narrowband imaging with Subaru/Suprime-Cam and spectroscopic follow-up observations using VLT/VIMOS at z ~ 3.1, which is one of the most active epochs for galaxy formation. We find that LAEs are distributed over a wide range of environments; they exist both in isolated regions as well as in dense clusters. The clustering properties of LAEs depend on their luminosities. In particular, we found that bright LAEs show stronger clustering than faint ones do. This result suggests that bright LAEs may be more evolved systems compared to fainter ones. Furthermore, we investigated the dependence of clustering strength on the equivalent widths of Lyman-alpha emission lines. Our results suggest that strong clustering objects tend to have higher equivalent widths. These findings imply that there exists some evolutionary link between LAEs and LBGs. \n \n Keywords: Lyman alpha emitter",
        "watermark_text": "We present an assessment of Lyman alpha emitters ( LAEs ) selected by narrowband scanning with Subaru / Suprime - Cam and spectroscopic follow - up observations using VLT / VIMOS at z ~ 3 . 1 , which is one of the most stable epochs for galaxy formation . We see that LAEs are distributed over a broad variety of habitats ; they exist both in isolated regions as well as in dense clusters .The clustering qualities of LAEs depend on their luminosities . In particular , we identified that bright LAEs see better clustering than dim ones do .This result suggests that bright LAEs may be more evolved structures compared to fainter ones . Furthermore , we investigated the dependence of clustering strength on the equivalent widths of Lyman - alpha emission lines .Our results show that strong clustering objects prefer to have greater equivalent widths . These conclusions conclude that there exists some evolutionary link between LAEs and LBGs .Keywords: Lyman alpha emitter",
        "rewrite_text": "We present an evaluation of Lyman alpha emitters (LAEs) identified through narrowband imaging with Subaru/Suprime-Cam, followed by spectroscopic observations using VLT/VIMOS at a redshift of approximately 3.1, a significant period for galaxy formation. Our findings indicate that LAEs occupy a diverse range of environments, being found both in isolated areas and within dense clusters. The clustering characteristics of LAEs appear to be influenced by their luminosities, with notably brighter LAEs exhibiting stronger clustering compared to their dimmer counterparts. This observation suggests that brighter LAEs may represent more advanced structures than fainter ones. Additionally, we explored the relationship between clustering strength and the equivalent widths of Lyman-alpha emission lines, revealing that objects with stronger clustering tend to have larger equivalent widths. These results imply a possible evolutionary connection between LAEs and Lyman Break Galaxies (LBGs). Keywords: Lyman alpha emitter.",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": -0.6401843996644799
    },
    {
        "original_text": "We study the mechanical properties of disordered networks of fibers with different bending rigidities by means of numerical simulations based on molecular dynamics (MD) and Monte Carlo (MC). We find that the elastic moduli are strongly dependent on the bending rigidity, which is related to the persistence length of the fibers. The results show that the shear modulus increases as the bending rigidity decreases while the bulk modulus remains almost unchanged for all values of the bending rigidity studied here. In addition we observe that the distribution functions of forces acting between particles depend significantly on the bending rigidity. For small bending rigidities there exist strong correlations among neighboring particles leading to large fluctuations in the local stress field. These findings suggest that the microscopic structure plays an important role in determining macroscopic material properties such as elasticity. Disordered networks of fibers have been widely used to model various materials ranging from biological tissues to granular media  1-3 . However, it has not yet been fully understood how the microstructure affects their macroscopic behavior. Here we investigate this problem using computer simulation techniques.",
        "watermark_text": "We research the mechanical behavior of disordered systems of fibers with various twisting rigidities by means of computational simulations based on molecular mechanics ( MD ) and Monte Carlo ( MC ) . We see that the elastic moduli are strongly dependent on the bending rigidity , which is related to the persistence length of the materials .The results show that the shear modulus increases as the twisting rigidity decreases while the bulk modulus remains virtually unchanged for all values of the twisting rigidity examined here . In addition we determine that the distribution functions of pressures acting between particles depend greatly on the twisting rigidity .For small bending rigidities there exist strong correlations among neighboring objects led to large fluctuations in the local tension area . These studies suggest that the microscopic shape plays an important role in determining macroscopic material structures such as elasticity .Disordered networks of fibers have been widely using to model numerous structures ranging from biological tissues to granular material 1 - 3 . However , it has not already been clearly explained how the microstructure affects their macroscopic behavior .Here we investigate this situation using computer simulation tactics .",
        "rewrite_text": "We explore the mechanical behavior of disordered fiber systems with varying twisting rigidities through computational simulations grounded in molecular dynamics (MD) and Monte Carlo (MC) methods. Our findings reveal that the elastic moduli exhibit a strong dependence on bending rigidity, which is associated with the materials' persistence length. Notably, we observe that as the twisting rigidity decreases, the shear modulus increases, while the bulk modulus remains largely unchanged across all twisting rigidity values considered. Furthermore, we find that the distribution functions of pressures between particles are significantly influenced by twisting rigidity. For fibers with lower bending rigidities, there are marked correlations among neighboring fibers, resulting in substantial fluctuations in local tension. This research indicates that microscopic shapes significantly affect the macroscopic structural properties, such as elasticity. Disordered fiber networks have been extensively used to model a variety of structures, from biological tissues to granular materials. However, the influence of microstructure on macroscopic behavior has yet to be fully elucidated. In this study, we address this gap using computational simulation techniques.",
        "ori-fast-z-score": -1.078327732034384,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 0.22086305214969307
    },
    {
        "original_text": "We study the effect of competing short-range attractive and repulsive interactions in a model for collapsed polymers, which are described by an effective one-dimensional potential with two minima separated by a barrier. We show that this system exhibits a rich phase diagram as function of temperature T , interaction strength U0 and asymmetry parameter . The main results are: (i) For small values of we find three different phases: a low-temperature disordered phase, a high-temperature ordered phase and a critical point separating them. (ii) In the limit of large barriers between the wells, i.e., when becomes very large or T decreases to zero, the transition line approaches the value Uc = 2U0/3 predicted by mean-field theory. \n \n Introduction \n \n Collapsed polymers have been studied extensively over many years  1 - 6 . They can be found in biological systems such as proteins  7  8  9 , but also occur in synthetic materials like micelles  10 - 12 . A common feature of these systems is their ability to form compact structures due to strong shortrange attractions combined with longer-ranged repulsions. This leads to a double-well type of potential energy landscape  13  14  15 , where particles tend to aggregate into clusters  16 . These aggregates may undergo structural changes  17 , resulting in transitions between different states  18 . Such phenomena are often observed experimentally  19  20  21   22 . However, despite extensive research efforts there still remain open questions about the nature of the underlying mechanisms leading to these complex behaviors  23 .",
        "watermark_text": "We research the impact of competing short - range attractive and repulsive molecules in a theory for crumpled polymers , which are explained by an efficient one - dimensional potential with two minima separated by a barrier . We see that this scheme exhibits a rich phase diagram as function of temperature T , coupling strength U0 and asymmetry parameter .The main results are : ( i ) For small values of we find three different stages : a high - temperature disordered phase , a high - temperature ordered phase and a critical point connecting them . ( ii ) In the limit of large obstacles between the wells , i . e . , when becomes very small or T decreases to zero , the transfer point approaches the value Uc = 2U0 / 3 predicted by mean - field model .Introduction Collapsed polymers have been studied frequently over numerous years 1 - 6 . They can be found in biological systems such as proteins 7 8 9 , but also occur in artificial devices like micelles 10 - 12 .A popular characteristic of these systems is their tendency to form dense structures owing to powerful shortrange attractions coupled with longer - ranged repulsions . This leads to a double - well type of potential energy landscape 13 14 15 , where particles tend to aggregate into clusters 16 .These aggregates might undergo physical changes 17 , resulting in transitions between various states 18 . Such effects are often observed experimentally 19 20 21 22 .However , despite extensive research efforts there still continue open questions about the nature of the underlying mechanisms leading to these complex behaviors 23 .",
        "rewrite_text": "We investigate how competing short-range attractive and repulsive molecules affect crumpled polymers through a theoretical framework that utilizes an efficient one-dimensional potential with two minima separated by a barrier. Our findings reveal a complex phase diagram depending on temperature (T), coupling strength (U0), and an asymmetry parameter. The key outcomes are: (i) At low values, we identify three distinct phases: a disordered phase at high temperatures, an ordered phase at high temperatures, and a critical point linking the two. (ii) When the barriers between the wells are significant—specifically, when the coupling becomes very small or as T approaches zero—the transfer point converges on the value Uc = 2U0 / 3, as anticipated by mean-field theory. \n\nIntroduction: Collapsed polymers have been the focus of extensive research over the years. They are present in biological systems, such as proteins, as well as in synthetic materials like micelles. A prominent feature of these systems is their ability to form dense structures due to strong short-range attractions combined with longer-ranged repulsions. This interaction results in a double-well potential energy landscape, promoting particle aggregation into clusters. These aggregates can experience physical transformations, causing transitions between different states. Such phenomena are frequently observed in experimental studies. However, despite the significant research conducted, questions remain regarding the fundamental mechanisms that drive these complex behaviors.",
        "ori-fast-z-score": 1.2675004445952593,
        "water-fast-z-score": 7.303524040806201,
        "rewrite-fast-z-score": 1.9250668437592438
    },
    {
        "original_text": "The question whether time is an illusion or not has been debated for centuries, and the answer remains elusive.  In this article we will examine some of the arguments that have been put forward in favour of the existence of time as well as those against it.   We will then consider how these arguments relate to our understanding of space-time and its relationship with matter.   Finally, we will look at how quantum mechanics may provide us with new insights into the nature of time. The question  Does time exist?  has been asked many times throughout history by philosophers, scientists and ordinary people alike. It seems like such a simple question but there are actually several different ways one can interpret what  time  means when asking about its existence. For example, if you ask yourself  Is time real?  your answer might be yes because you think that time is something that exists independently of any observer (i.e., outside of ourselves). On the other hand, if you were to ask yourself  Do I experience time passing?  your answer would probably be no since you believe that only humans experience time.",
        "watermark_text": "The question whether history is an illusion or not has been discussed for centuries , and the response remains elusive . In this article we will investigate some of the ideas that have been put forward in favour of the existence of time as well as those against it .We will then discuss how these ideas correspond to our grasp of space - time and its connection with matter . Finally , we will look at how quantum mechanics may provide us with novel knowledge into the nature of time .The question Does time exist ? has been wondered much times throughout history by philosophers , scientists and normal people alike .It sounds like such a simple question but there are apparently several different methods one can comprehend what life says when asking about its existence . For instance , if you tell yourself Is time real ?your answer might be yes because you believe that life is something that arises independently of any observer ( i . e . , outside of ourselves ) . On the other hand , if you were to ask yourself Do I encounter time passing ?your answer would probably be no since you believe that only people receive time .",
        "rewrite_text": "The debate over whether history is an illusion has persisted for centuries, with a conclusive answer still out of reach. In this article, we will explore various arguments supporting the existence of time, as well as those that challenge it. We will examine how these perspectives relate to our understanding of space-time and its relationship with matter. Additionally, we will consider how quantum mechanics might offer new insights into the nature of time. The question \"Does time exist?\" has intrigued philosophers, scientists, and ordinary individuals throughout history. Though it seems straightforward, there are multiple ways to interpret what life suggests about its existence. For example, if you ask yourself, \"Is time real?\" you might respond affirmatively, believing that time exists independently of any observer (i.e., outside of ourselves). Conversely, if you pose the question, \"Do I perceive time passing?\" your answer might lean toward no, as you may think that only individuals experience the passage of time.",
        "ori-fast-z-score": 1.2939932784412609,
        "water-fast-z-score": 7.201190377787749,
        "rewrite-fast-z-score": 2.5555555555555554
    },
    {
        "original_text": "We report on INTEGRAL observations of an X-ray transient discovered by Swift/BAT in December 2006 and detected at hard X-rays with IBIS/ISGRI up to 100 keV during its outbursts between January 2007 and March 2008. The source was also observed simultaneously by RXTE, Suzaku, Chandra and XMM-Newton telescopes which allowed us to determine its position as RA = 11 h 32 m 01 s .6 Dec = -53°10′19′′ (J2000) with an uncertainty radius of 1 arcsec. We show that this new source is likely associated with the optical counterpart VLT/VLBA J1131-5321 previously identified by Chatfield et al. (2007). Its spectrum can be described by a power law model with photon index Γ ~ 2.5-3.0 absorbed by NH ~ 5 × 1021 cm-2. No pulsations were found down to 3 ms resolution using data obtained with RXTE PCA and HEXTE instruments.",
        "watermark_text": "We report on INTEGRAL observations of an X - ray transient discovered by Swift / BAT in December 2006 and detected at hard X - radiation with IBIS / ISGRI up to 100 keV during its outbursts between January 2007 and March 2008 . The source was also observed concurrently by RXTE , Suzaku , Chandra and XMM - Newton telescopes which allowed us to estimate its position as RA = 11 h 32 m 01 s . 6 Dec = - 53°10 ′ 19 ′ ′ ( J2000 ) with an uncertainty diameter of 1 arcsec .We indicate that this new source is probably associated with the optical counterpart VLT / VLBA J1131 - 5321 previously described by Chatfield et al . ( 2007 ) .Its spectrum can be described by a power law description with photon index Γ ~ 2 . 5 - 3 . 0 absorbed by NH ~ 5 × 1021 centimeters - 2 . No pulsations were found down to 3 ms resolution utilizing information obtained with RXTE PCA and HEXTE observations .",
        "rewrite_text": "We present findings from INTEGRAL observations of an X-ray transient identified by Swift/BAT in December 2006, which was detected emitting hard X-rays with IBIS/ISGRI up to 100 keV during outbursts between January 2007 and March 2008. Concurrent observations by RXTE, Suzaku, Chandra, and XMM-Newton telescopes enabled us to pinpoint its position at RA = 11h 32m 01s.6 Dec = -53°10′19′′ (J2000) with an uncertainty of 1 arcsecond. We suggest that this newly identified source is likely connected to the optical counterpart VLT/VLBA J1131-5321, as previously noted by Chatfield et al. (2007). Its spectrum is characterized by a power law with a photon index Γ ~ 2.5 - 3.0, subjected to absorption by NH ~ 5 × 10^21 cm^-2. No pulsations were detected down to a resolution of 3 ms using data from RXTE PCA and HEXTE observations.",
        "ori-fast-z-score": -1.0674899923282326,
        "water-fast-z-score": 2.897472836319489,
        "rewrite-fast-z-score": 0.6488856845230502
    },
    {
        "original_text": "We study the glass transition of an ensemble of adhesive hard spheres with repulsive interactions decaying as 1/r6, where r is distance between particles. We find that this system exhibits two distinct relaxation processes at low temperatures: one fast process associated with local rearrangements within clusters of strongly bonded particles; another slower process related to collective motion of these clusters. The latter process can be described by mode-coupling theory (MCT) for colloidal suspensions. However, we show that MCT fails quantitatively when applied directly to our data because it does not take into account the presence of strong bonds which lead to additional slow modes. By introducing a simple modification to MCT, we are able to obtain excellent agreement with experimental results over several decades in time and frequency. This modified version of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg. Our work shows how quantitative tests of theoretical predictions may help improve their accuracy and applicability range.",
        "watermark_text": "We test the glass transition of an ensemble of adhesive solid surfaces with repulsive interactions decaying as 1 / r6 , where r is distance between particles . We see that this scheme exhibits two different relaxation processes at low temperatures : one fast process associated with local rearrangements within clusters of highly bonded particles ; another slower process related to collective motion of these complexes .The latter system can be described by mode - coupling theory ( MCT ) for colloidal suspensions . However , we prove that MCT fails quantitatively when applied directly to our information because it does not take into consideration the presence of stable bonds which lead to extra slow modes .By introducing a simple change to MCT , we are able to obtain excellent compliance with experimental results over numerous years in time and frequency . This amended variant of MCT also predicts correctly the temperature dependence of the structural relaxation time near Tg .Our research shows how standardized tests of theoretical estimates may improve improve their accuracy and applicability range .",
        "rewrite_text": "We investigate the glass transition of a set of adhesive solid surfaces characterized by repulsive interactions that diminish with a 1/r^6 dependence, where r represents the distance between particles. Our findings reveal two distinct relaxation processes at lower temperatures: a rapid process linked to local rearrangements within groups of highly bonded particles, and a slower process associated with the collective motion of these complexes. The latter can be understood within the framework of mode-coupling theory (MCT) for colloidal suspensions. However, our analysis demonstrates that MCT falls short in accurately describing our data because it overlooks the impact of stable bonds, which introduce additional slow modes. By making a straightforward adjustment to MCT, we achieve remarkable agreement with experimental results across various time and frequency scales. This modified version of MCT also accurately predicts the temperature dependence of the structural relaxation time near the glass transition temperature (Tg). Our findings illustrate how systematic evaluations of theoretical models can enhance their precision and broaden their range of applicability.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.2792042981336627
    },
    {
        "original_text": "We study the ground state properties of an interacting electron gas confined to a onedimensional (1D) quantum dot with parabolic confinement potential and repulsive Coulomb interaction between electrons. We show that, for sufficiently strong confining potential, the system undergoes a phase transition into a deformed Wigner crystal at low temperatures. The results are obtained by using density functional theory within the local spin-density approximation combined with exact diagonalization method. In this regime, we find that the charge distribution is characterized by alternating peaks separated by valleys which become more pronounced as temperature decreases. This behavior can be understood in terms of formation of a periodic structure due to inter-particle correlations. Our results suggest that such structures may exist experimentally in semiconductor nanowires or carbon nanotubes. Introduction:-In recent years there has been considerable interest in studying the electronic properties of nanostructures  1  . One dimensional systems have attracted particular attention because they provide a unique opportunity to investigate fundamental physical phenomena like Luttinger liquid  2  , fractional statistics  3  , and Wigner crystallization  4  .\nTheoretical studies of 1D quantum dots (QDs), i.e., QDs with only one dimension much smaller than other two dimensions, were first carried out by Lieb et al  5  who showed that these systems exhibit interesting features including shell filling effects  6  . Subsequently, several authors studied various aspects of QD physics  7, 8  . For example, it was shown that the energy spectrum of a QD depends strongly on its shape  9  . It also turns out that the single particle wave functions of a QD depend sensitively on the boundary conditions  10  . Recently, some experimental progress has been made towards realizing 1D QDs  11  -  13  . However, most experiments so far have focused mainly on transport measurements  14  -  16  rather than direct imaging  17  . Therefore, theoretical investigations play an important role in understanding the underlying physics of these systems  18  -  20  .\nIn this work, we consider a model consisting of N non-interacting fermions confined to a 1D QD with parabolic confinement potential V(x). The total energy E tot = ∑ i=1...N",
        "watermark_text": "We explore the ground state properties of an interacting electron gas restricted to a onedimensional ( 1D ) quantum dot with parabolic confinement potential and repulsive Coulomb interaction between electrons . We see that , for enough strong confining potential , the system undergoes a phase shift into a deformed Wigner crystal at low temperatures .The results are derived by using density functional theory within the local spin - density algorithm coupled with exact diagonalization technique . In this regime , we find that the charge distribution is characterized by alternating ridges separated by regions which turn more pronounced as temperature falls .This phenomenon can be understood in terms of formation of a periodic pattern leading to inter - particle correlations . Our results propose that such structures could occur experimentally in semiconductor nanowires or carbon nanotubes .Introduction : - In recent years there has been substantial interest in examining the electronic properties of nanostructures 1 . One dimensional complexes have garnered especially attention because they give a unique opportunity to examine fundamental physical phenomena like Luttinger vacuum 2 , fractional statistics 3 , and Wigner crystallization 4 .Theoretical experiments of 1D quantum dots ( QDs ) , i . e . , QDs with only one dimension much smaller than other two dimensions , were first conducted out by Lieb et al 5 who demonstrated that these systems exhibit unusual characteristics notably barrel filling effects 6 . Subsequently , various scientists researched several elements of QD physics 7 , 8 .For instance , it was shown that the energy spectrum of a QD depends strongly on its shape 9 . It also turns out that the single molecule wave systems of a QD depend sensitively on the boundary parameters 10 .Recently , some experimental development has been achieved towards realizing 1D QDs 11 - 13 . However , most studies so far have concentrated mostly on transport measurements 14 - 16 rather than direct scanning 17 .Therefore , theoretical investigations play an important role in understanding the fundamental theory of these systems 18 - 20 . In this research , we study a theory consisting of N non - interacting fermions localized to a 1D QD with parabolic confinement potential V ( x ) .The total energy E tot = ∑ i=1...N",
        "rewrite_text": "We investigate the ground state characteristics of an interacting electron gas confined to a one-dimensional (1D) quantum dot, which features a parabolic confinement potential and repulsive Coulomb interactions among electrons. Our findings indicate that, when the confining potential is sufficiently strong, the system transitions into a deformed Wigner crystal phase at low temperatures. These results are obtained using density functional theory based on the local spin-density approximation, combined with exact diagonalization techniques. In this low-temperature regime, we observe a charge distribution marked by alternating ridges, which become more pronounced as the temperature decreases. This pattern can be interpreted as the emergence of periodic structures that enhance inter-particle correlations. Our findings suggest that such phenomena could potentially be observed in experiments involving semiconductor nanowires or carbon nanotubes.\n\n**Introduction**: In recent years, there has been a significant interest in studying the electronic properties of nanostructures. One-dimensional complexes, in particular, have attracted considerable attention due to their ability to provide unique insights into fundamental physical concepts such as the Luttinger vacuum, fractional statistics, and Wigner crystallization. The pioneering theoretical work on 1D quantum dots (QDs), which are defined by having one dimension much smaller than the other two, was conducted by Lieb et al. They revealed that these systems exhibit distinctive features, notably barrel filling effects. Since then, numerous researchers have explored various aspects of QD physics, demonstrating, for instance, that the energy spectrum of a QD is heavily influenced by its geometric shape. Additionally, the wavefunctions of single molecules within a QD are sensitive to boundary conditions. Recent advancements have been made towards the experimental realization of 1D QDs. However, most prior research has primarily focused on transport measurements rather than direct scanning techniques. Thus, theoretical investigations are crucial for deepening our understanding of the foundational concepts underlying these systems. In this study, we analyze a model comprising N non-interacting fermions confined to a 1D QD with a parabolic confinement potential V(x). The total energy is given by E_tot = ∑ i=1...N.",
        "ori-fast-z-score": -0.9370425713316364,
        "water-fast-z-score": 6.9853547313569955,
        "rewrite-fast-z-score": 0.3892494720807615
    },
    {
        "original_text": "We study evolutionary games played by agents that are randomly distributed over the nodes of an unstructured network, and interact only with their immediate neighbors in this network. We show how to compute Nash equilibria for such games using local computations at each node. Our approach is based on computing best responses locally at every agent s neighborhood, which we then use as input to a global algorithm that computes all Nash equilibria. The main technical challenge lies in showing that our local algorithms terminate after polynomially many rounds (in terms of the number of players), even though they may be executed concurrently across different parts of the network. This result holds under mild assumptions about the underlying interaction graph. In particular, it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices. As a consequence, we obtain polynomial-time algorithms for finding Nash equilibria in several classes of games including congestion games, matching pennies games, and coordination games.",
        "watermark_text": "We research evolutionary matches played by agents that are randomly distributed over the nodes of an unstructured system , and interact only with their immediate relatives in this network . We see how to compute Nash equilibria for such players using local computations at each node .Our strategy is based on solving best responses locally at every agent s neighborhood , which we then use as input to a global algorithm that computes all Nash equilibria . The main engineering obstacle lies in showing that our regional algorithms end after polynomially many tournaments ( in terms of the number of teams ) , even though they may be executed concurrently across different components of the organization .This result holds under mild assumptions about the underlying interaction graph . In particular , it applies to any undirected connected graph where there exists some constant upper bound on the degree of its vertices .As a consequence , we obtain polynomial - time computers for finding Nash equilibria in multiple groups of games including congestion games , matching pennies games , and coordination games .",
        "rewrite_text": "We investigate evolutionary matches involving agents that are randomly distributed across the nodes of an unstructured system, interacting solely with their immediate neighbors within this network. Our approach focuses on calculating Nash equilibria for these players through local computations at each node. Specifically, we solve for the best responses within each agent's neighborhood, which are subsequently used as inputs for a global algorithm that determines all Nash equilibria. A key engineering challenge is demonstrating that our local algorithms conclude after a polynomial number of tournaments (relative to the number of teams), despite being executed concurrently across various components of the system. This finding is valid under mild conditions concerning the underlying interaction graph. Notably, it is applicable to any undirected connected graph where the degree of vertices has a constant upper limit. Consequently, we develop polynomial-time algorithms for computing Nash equilibria in various categories of games, including congestion games, matching pennies games, and coordination games.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 4.939391699536065,
        "rewrite-fast-z-score": -0.22360679774997896
    },
    {
        "original_text": "We present an explicit formula for solutions to some classes of nonlinear fractional diffusion equations with variable coefficients and nonlocal initial conditions, which are expressed by means of the generalized Mittag-Leffler function Eα(−z) and the H-function. The results obtained can be used as a basis for numerical methods for solving such problems. We also discuss applications of our approach to the description of anomalous transport processes in complex systems. Introduction.\nThe theory of fractional calculus has been developed rapidly during recent years due to its numerous applications in various fields of science and engineering (see e.g.,  1  -  4  ). In particular, it is widely applied to describe anomalous transport phenomena observed in many physical models  5  . For example, this approach was successfully employed to model subdiffusion  6  , superdiffusion  7  , and Lévy flights  8  .\nIn general, fractional differential equations have no exact analytical solution  9  . Therefore, there exists great interest in developing efficient numerical algorithms for their approximate treatment  10  . However, even if we use powerful computers, these approaches may not always provide sufficiently accurate results  11  . This problem becomes especially important when one deals with strongly nonlinear problems  12  or considers very large time intervals  13  . To overcome difficulties associated with the lack of exact solutions, several authors proposed different techniques based on special functions  14  -  16  . Among them, the most popular ones include the Mittag-Leffler  17  and Wright  18  functions, Fox s H-functions  19  , and others  20  -  22  .",
        "watermark_text": "We introduce an explicit formula for solutions to some categories of nonlinear fractional absorption equations with variable coefficients and nonlocal initial conditions , which are written by means of the generalized Mittag - Leffler function Eα ( −z ) and the H - function . The results derived can be used as a framework for numerical models for solving such problems .We also discuss uses of our approach to the description of anomalous transport systems in complex systems . Introduction .The theory of fractional calculus has been constructed fast during recent years owing to its numerous uses in different fields of science and engineering ( saw e . g . , 1 - 4 ) . In particular , it is widely applied to explain anomalous transport phenomena observed in many physical models 5 .For instance , this methodology was successfully utilized to model subdiffusion 6 , superdiffusion 7 , and Lévy flying 8 . In general , fractional differential operators have no complete analytical solution 9 .Therefore , there exists considerable interest in building powerful mathematical algorithms for their approximate therapy 10 . However , even if we using powerful computers , these solutions may not always supply sufficiently accurate outcomes 11 .This problem appears particularly important when one deals with strongly nonlinear solutions 12 or considers very huge time periods 13 . To address difficulties related with the lack of precise solutions , various scientists proposed different methods using on particular functions 14 - 16 .Among them , the most popular ones contain the Mittag - Leffler 17 and Wright 18 functions , Fox s H - systems 19 , and others 20 - 22 .",
        "rewrite_text": "We present a clear formula for the solutions of certain types of nonlinear fractional absorption equations characterized by variable coefficients and nonlocal initial conditions. These solutions are expressed using the generalized Mittag-Leffler function \\( E_{\\alpha}(-z) \\) and the H-function. The findings can serve as a foundation for developing numerical models to tackle such problems. Additionally, we explore the application of our approach in describing anomalous transport systems within complex environments.\n\nIntroduction. The field of fractional calculus has rapidly evolved in recent years due to its extensive applications across various scientific and engineering disciplines (see, for example, references 1-4). In particular, it is commonly applied to model anomalous transport phenomena observed in numerous physical contexts (reference 5). This framework has been effectively employed to represent subdiffusion (reference 6), superdiffusion (reference 7), and Lévy flights (reference 8). Generally, fractional differential operators do not have complete analytical solutions (reference 9), leading to significant interest in developing robust mathematical algorithms for their approximate solutions (reference 10). Nevertheless, even with advanced computational resources, these solutions may not always yield sufficiently accurate results (reference 11). This challenge becomes increasingly critical when addressing strongly nonlinear solutions (reference 12) or considering very long time periods (reference 13). To overcome issues stemming from the absence of precise solutions, various researchers have proposed alternative methods relying on specific functions (references 14-16). Among the most widely used approaches are the Mittag-Leffler (reference 17) and Wright (reference 18) functions, as well as Fox’s H-systems (reference 19), among others (references 20-22).",
        "ori-fast-z-score": -0.811502671200689,
        "water-fast-z-score": 7.723027987151322,
        "rewrite-fast-z-score": 0.5937322507759797
    },
    {
        "original_text": "The universal spectrum is the most important concept in physics and mathematics, which has been widely used to study various physical phenomena such as energy transfer processes in biological systems.  In this work we present an analysis on the universal spectrum of the DNA base CG frequency distribution along the chromosomes of Takifugu rubripe s(pufferfish). We find that there are two different types of universal spectra corresponding to the coding regions and non-coding regions respectively. The universal spectrum of the coding region shows a power law behavior with exponent 1.5 while that of the noncoding region exhibits a fractal structure. Our results show that the universal spectrum can be used to characterize the complexity of the DNA sequence. It may also provide new insights into the understanding of the evolution process of the genomes. Keywords: Universal Spectrum; Fractal; Power Law; Puffer Fish Genome; Energy Transfer Processes. Introduction:  The universal spectrum is one of the most important concepts in physics and mathematics, it was first introduced by Hertz  1  . Since then many scientists have studied its applications in various fields including biology  2  , geology  3  , medicine  4  etc.. Recently, some researchers found that the universal spectrum could be applied to analyze the gene expression data  5  -  8  .\nIn recent years, more and more attention has been paid to the relationship between the universal spectrum and the energy transfer processes in biological system  9  -  11  . For example, Li et al.  12  investigated the universal spectrum of the human heart rate variability and found that the universal spectrum showed a fractal structure. They suggested that the universal spectrum might be useful in characterizing the complexity of the physiological time series. Wang et al.  13  analyzed the universal spectrum of the protein folding dynamics and they found that the universal spectrum exhibited a power-law behavior with exponent 2.0. They proposed that the universal spectrum could reflect the degree of disorderedness of the protein folding dynamics.",
        "watermark_text": "The universal spectrum is the most important notion in science and mathematics , which has been widely applied to study various physical phenomena such as energy flow processes in biological systems . In this research we present an assessment on the universal spectrum of the DNA base CG frequency distribution along the chromosomes of Takifugu rubripe s ( pufferfish ) .We see that there are two different kinds of universal spectra corresponding to the code regions and non - codes provinces respectively . The universal spectrum of the code region shows a power law behavior with exponent 1 . 5 while that of the noncoding region displays a fractal structure .Our results show that the fundamental spectrum can be used to characterize the complexity of the DNA sequence . It might additionally offer new understanding into the knowledge of the evolution mechanism of the genomes .Keywords : Universal Spectrum ; Fractal ; Power Law ; Puffer Fish Genome ; Energy Transfer Processes . Introduction : The universal spectrum is one of the most important concepts in science and mathematics , it was first described by Hertz 1 .Since then many scientists have researched its applications in different fields including biology 2 , geology 3 , medicine 4 etc . . Recently , some researchers found that the universal spectrum could be applied to analyze the gene function data 5 - 8 .In recent years , more and more attention has been paid to the relationship between the fundamental spectrum and the energy flow processes in biological cycle 9 - 11 . For instance , Li et al .12 examined the universal spectrum of the human chest rate variability and found that the universal spectrum demonstrated a fractal structure . They suggested that the universal spectrum might be used in characterizing the complexity of the physiological time cycle .Wang et al . 13 examined the universal spectrum of the protein folding dynamics and they concluded that the universal spectrum displayed a power - law behavior with exponent 2 . 0 .They proposed that the universal spectrum could reflect the degree of disorderedness of the protein folding dynamics .",
        "rewrite_text": "The universal spectrum is a key concept in science and mathematics, extensively utilized to explore various physical phenomena, including energy transfer processes within biological systems. In this study, we evaluate the universal spectrum of the CG frequency distribution of DNA bases along the chromosomes of Takifugu rubripes (pufferfish). Our findings reveal two distinct types of universal spectra: one for coding regions and another for non-coding regions. The universal spectrum of coding regions exhibits a power law behavior with an exponent of 1.5, whereas the non-coding region displays a fractal structure. These results indicate that the fundamental spectrum can effectively characterize the complexity of DNA sequences and may provide new insights into the evolutionary mechanisms of genomes.\n\nKeywords: Universal Spectrum; Fractal; Power Law; Pufferfish Genome; Energy Transfer Processes.\n\nIntroduction: The universal spectrum is a fundamental concept in both science and mathematics, first articulated by Hertz. Since its inception, numerous researchers have explored its applications across various fields, including biology, geology, and medicine. Recently, scholars have discovered that the universal spectrum can be instrumental in analyzing gene function data. Increasingly, attention has turned to the relationship between the fundamental spectrum and energy flow processes in biological cycles. For example, Li et al. investigated the universal spectrum of human heart rate variability and identified a fractal structure, suggesting its potential for characterizing the complexity of physiological cycles. Similarly, Wang et al. studied the universal spectrum of protein folding dynamics, concluding that it exhibits a power law behavior with an exponent of 2.0, indicating its ability to reflect the degree of disorder in the dynamics of protein folding.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.531972647421808,
        "rewrite-fast-z-score": 0.17277368511627203
    },
    {
        "original_text": "We present an analysis of galaxy clusters selected by their red-sequence galaxies (Gladders & Yee 2005) from the Sloan Digital Sky Survey Data Release 4 (DR4). We use two different methods to select cluster candidates, and then apply photometric redshift cuts on these samples to obtain final catalogues with high purity. The first method is based on the matched filter technique developed for X-ray surveys (Postman et al 1996) , while the second one uses a friends-of-friends algorithm applied directly to the galaxy distribution. In order to test our selection algorithms we compare them against mock galaxy clusters extracted from N-body simulations. Our main results are as follows:  1. Using the matched filter technique we find that the number density of galaxy clusters at z < 0.5 is n(z<0.5) = 3.6 +/- 0.2 x 10-3 h3 Mpc-3 . This value agrees well with previous determinations using other techniques.  2. By applying the same matched filter technique to simulated galaxy clusters we show how this method can be used to estimate the mass function of galaxy clusters up to z ~1.0.",
        "watermark_text": "We present an assessment of galaxy regions selected by their red - sequence galaxies ( Gladders & Yee 2005 ) from the Sloan Digital Sky Survey Data Release 4 ( DR4 ) . We use two different methods to select cluster applicants , and then use photometric redshift cutting on these specimens to obtain final catalogues with high purity .The first method is based on the matched filter technique developed for X - ray observations ( Postman et al 1996 ) , while the second one uses a enemies - of - friends method applied directly to the galaxy distribution . In order to test our choice algorithms we compare them against mock galaxy galaxies extracted from N - bodies simulations .Our main results are as follows : 1 . Using the matched filter technique we find that the number density of galaxy galaxies at z < 0 . 5 is n ( z < 0 . 5 ) = 3 . 6 + / - 0 . 2 x 10 - 3 h3 Mpc - 3 .This value agrees well with previous determinations using other techniques.2.By applying the same matched filter technique to modeled galaxy galaxies we find how this algorithm can be used to estimate the mass value of galaxy galaxies up to z ~ 1 . 0 .",
        "rewrite_text": "We provide an evaluation of galaxy regions identified by red-sequence galaxies, as per Gladders & Yee (2005), utilizing data from the Sloan Digital Sky Survey Data Release 4 (DR4). Our approach involves two distinct techniques for selecting cluster candidates, followed by applying photometric redshift cuts to achieve final catalogs with high purity. The first technique employs the matched filter method initially designed for X-ray observations (Postman et al. 1996), while the second utilizes a friends-of-friends algorithm applied directly to the distribution of galaxies. To validate our selection methods, we compare our results with mock galaxy samples derived from N-body simulations. Our key findings are as follows: 1. Using the matched filter technique, we determine that the number density of galaxies at z < 0.5 is n(z < 0.5) = 3.6 ± 0.2 × 10^-3 h^3 Mpc^-3, which aligns well with prior estimates obtained through different methods. 2. Furthermore, by applying the matched filter technique to modeled galaxy samples, we demonstrate that this algorithm can effectively estimate the mass of galaxy clusters up to z ~ 1.0.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 0.3144854510165755
    },
    {
        "original_text": "We present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) onboard the Coriolis satellite, which show dynamic fibrils in both H-alpha and CIV lines. The SUVI data are compared to simultaneous ground-based observations made at Big Bear Observatory using the 1-meter telescope equipped with an H-Alpha filter and a Fabry-Perot interferometer tuned to the CIV line. We find that the observed structures have similar properties as those seen previously by other authors but we also see some differences between them. In particular, our results suggest that the fibril structure is more complex than it was thought before. This complexity may be related to the fact that these structures are formed under different physical conditions. Our analysis shows that the observed features can be explained by assuming that they represent plasma flows along magnetic field lines. These flows could play important role in heating up the upper layers of the solar atmosphere.",
        "watermark_text": "We present new surveys of the solar atmosphere acquired with the Solar Ultraviolet Imager ( SUVI ) onboard the Coriolis spacecraft , which show dynamic fibrils in both H - alpha and CIV lines . The SUVI images are compared to simultaneous ground - based observations made at Big Bear Observatory using the 1 - meter telescope fitted with an H - Alpha filter and a Fabry - Perot interferometer tuned to the CIV line .We see that the seen features have related properties as those saw previously by other researchers but we also saw some variations between them . In particular , our findings confirm that the fibril structure is more sophisticated than it was considered before .This difficulty must be connected to the fact that these structures are created under distinct physical conditions . Our study shows that the seen features can be described by assuming that they represent plasma flows along magnetic field lines .These flows could play major part in heating up the higher layers of the solar atmosphere .",
        "rewrite_text": "We present new observations of the solar atmosphere obtained with the Solar Ultraviolet Imager (SUVI) aboard the Coriolis spacecraft, revealing dynamic fibrils in both H-alpha and CIV lines. These SUVI images are compared to concurrent ground-based data from Big Bear Observatory, which employed a 1-meter telescope equipped with an H-alpha filter and a Fabry-Perot interferometer tuned to the CIV line. Our analysis indicates that the characteristics of the observed features are consistent with previous research; however, we noted some variations as well. Specifically, our results confirm that the fibril structure is more complex than previously understood. This complexity is likely linked to the formation of these structures under different physical conditions. Our study suggests that these features can be interpreted as plasma flows along magnetic field lines, which may play a significant role in heating the upper layers of the solar atmosphere.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.277372492166241,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "We present sub-arcsecond resolution interferometric observations in the millimeter continuum, 12CO(J=1-0), 13CO(J=1-0) and C18O(J=1-0). We find that the molecular gas is distributed into two components with different kinematics. The main component has an extent of 1 kpc (0.5 arcsec at z = 2.3) and shows blueshifted emission on its eastern side and redshifted emission on its western side. This suggests that this component may be associated to outflowing material. A second component is detected only in 12CO(J=1-1) and it is located 0.2 arcsec eastward of the first one. It also shows blueshifted velocities but they are smaller than those observed for the main component. Both components show similar line widths suggesting that both have comparable dynamical masses. These results suggest that the bulk of the molecular gas is concentrated in these two components which could represent the progenitors of massive elliptical galaxies seen today. \n \n Keywords: Ultraluminous Infrared Galaxies",
        "watermark_text": "We create sub - arcsecond resolution interferometric observations in the millimeter continuum , 12CO ( J = 1 - 0 ) , 13CO ( J = 1 - 0 ) and C18O ( J = 1 - 0 ) . We see that the molecular vapor is spread into two parts with varying kinematics .The main component has an extent of 1 kpc ( 0 . 5 arcsec at z = 2 . 3 ) and shows blueshifted emission on its eastern side and redshifted emission on its southeastern side . This implies that this component may be correlated to outflowing matter .A second component is detected only in 12CO ( J = 1 - 1 ) and it is situated 0 . 2 arcsec east of the first one . It still shows blueshifted velocities but they are smaller than those observed for the main component .Both components exhibit similar line widths suggesting that both have equal dynamical volumes . These data suggest that the majority of the molecular energy is concentrated in these two parts which could constitute the progenitors of large elliptical galaxies found today .Keywords: Ultraluminous Infrared Galaxies",
        "rewrite_text": "We conducted high-resolution interferometric observations in the millimeter continuum, focusing on 12CO (J = 1–0), 13CO (J = 1–0), and C18O (J = 1–0). Our findings reveal that the molecular gas is divided into two distinct regions with differing kinematic properties. The primary component spans approximately 1 kpc (0.5 arcseconds at z = 2.3) and displays blueshifted emission on its eastern side and redshifted emission on its southeastern side, suggesting a possible connection to outflowing matter. A secondary component, detectable only in 12CO (J = 1–1), is located 0.2 arcseconds east of the primary component. While this second component also shows blueshifted velocities, they are less pronounced than those of the main component. Both regions exhibit similar line widths, indicating that they possess comparable dynamical volumes. These observations imply that the majority of the molecular energy is concentrated in these two regions, which may represent the progenitors of the large elliptical galaxies observed today. Keywords: Ultraluminous Infrared Galaxies.",
        "ori-fast-z-score": -1.3130643285972254,
        "water-fast-z-score": 4.989644448669457,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We present new photometric data for the remote Galactic globulars M92 and NGC 2419, obtained with the 1-m telescope at Mt. Wilson Observatory in California during two observing runs (in February-March 2005 and September-October 2006). The main goal was to obtain accurate color indices for these clusters which are needed as input parameters into theoretical models of stellar evolution.  We have determined the following basic parameters of both clusters:  distance modulus DM = 13.20 ± 0.10 mag; reddening E(B-V) = 0.04 ± 0.01 mag; metallicity  Fe/H  = -1.30 ± 0.05 dex for M92 and DM = 14.00 ± 0.15 mag; E(B-V) < 0.02 mag;  Fe/H  = -2.40 ± 0.10 dex for NGC 2419. These values agree well with previous determinations based on other methods.",
        "watermark_text": "We present new photometric data for the distant Galactic globulars M92 and NGC 2419 , obtained with the 1 - m observatory at Mt . Wilson Observatory in California during two observing walks ( in February - March 2005 and September - October 2006 ) .The main goal was to obtain precise color indices for these clusters which are needed as input parameters into fundamental theories of stars evolution . We have determined the following fundamental variables of both clusters : distance modulus DM = 13 . 20 ± 0 . 10 mag ; reddening E ( B - V ) = 0 . 04 ± 0 . 01 mag ; metallicity Fe / H = - 1 . 30 ± 0 . 05 dex for M92 and DM = 14 . 00 ± 0 . 15 mag ; E ( B - V ) < 0 . 02 mag ; Fe / H = - 2 . 40 ± 0 . 10 dex for NGC 2419 .These measurements agree well with previous determinations based on other methods .",
        "rewrite_text": "We present new photometric data for the distant Galactic globular clusters M92 and NGC 2419, collected using the 1-meter telescope at Mt. Wilson Observatory in California during two observational campaigns (February-March 2005 and September-October 2006). The primary objective was to obtain accurate color indices for these clusters, which are essential input parameters for fundamental theories of stellar evolution. We established the following key parameters for both clusters: for M92, we found a distance modulus of DM = 13.20 ± 0.10 mag, reddening of E(B-V) = 0.04 ± 0.01 mag, and metallicity of [Fe/H] = -1.30 ± 0.05 dex; for NGC 2419, we determined a distance modulus of DM = 14.00 ± 0.15 mag, reddening of E(B-V) < 0.02 mag, and metallicity of [Fe/H] = -2.40 ± 0.10 dex. These findings are consistent with previous measurements obtained through different methods.",
        "ori-fast-z-score": 0.9486832980505138,
        "water-fast-z-score": 3.794733192202055,
        "rewrite-fast-z-score": 0.8846517369293828
    },
    {
        "original_text": "The present work is devoted to the study of some new effects in the process of combustion of liquid fuels, which are observed under the action of laser radiation with different wavelengths and power densities on the fuel surface. The results obtained show that the use of pulsed lasers can significantly increase the efficiency of burning liquid fuels by increasing their evaporation rate due to the formation of plasma at the fuel surface. In addition, it was found that the presence of an additional source of energy (laser) leads to changes in the structure of the flame front, as evidenced by the appearance of bright spots near its edge. It has been shown experimentally that these spots correspond to local increases in temperature up to several thousand degrees Celsius. This phenomenon may be used for creating highly efficient heat sources based on the combustion of liquid fuels. \n \n Keywords: Flame, Laser, Combustion, Evaporation, Plasma",
        "watermark_text": "The present work is devoted to the observation of some new consequences in the process of burning of liquid materials , which are observed under the action of laser sunlight with various wavelengths and power densities on the fuel surface . The results collected show that the using of pulsed lasers can significantly boost the performance of burning liquid materials by increasing their evaporation rate due to the formation of plasma at the fuel surface .In addition , it was shown that the presence of an additional source of electricity ( laser ) results to changes in the composition of the burning front , as demonstrated by the appearance of bright places near its tip . It has been shown experimentally that these streaks correspond to local changes in heat up to several thousand degrees Celsius .This phenomenon might be used for producing highly efficient energy sources dependent on the burning of liquid materials . Keywords : Flame , Laser , Combustion , Evaporation , Plasma",
        "rewrite_text": "This study focuses on exploring new effects observed during the combustion of liquid materials when exposed to laser light of varying wavelengths and power densities on the fuel surface. The findings indicate that the use of pulsed lasers can significantly enhance the combustion efficiency of liquid fuels by accelerating their evaporation rates due to the formation of plasma at the surface. Furthermore, the introduction of an additional energy source, such as a laser, alters the composition of the combustion front, evidenced by the emergence of bright spots near its edge. Experimental results demonstrate that these spots are associated with localized temperature increases of several thousand degrees Celsius. This phenomenon has potential applications for the development of highly efficient energy sources based on the combustion of liquid materials. Keywords: Flame, Laser, Combustion, Evaporation, Plasma.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "We present the results of an analysis aimed at improving the stellar parameters for the host star of planet TrES-2, as well as its planetary system properties. We use high-precision photometry obtained with the MOST satellite to derive new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD) and radius ratio (Rp/Rs = 0.11). These are combined with existing radial velocity data in order to refine the mass estimates for both components of this double-lined spectroscopic binary. Our best-fit model yields masses of 1.06 ± 0.04M⊙ and 0.84 ± 0.03M⊙ for the primary and secondary stars respectively, along with radii of 1.16 ± 0.02R⊙ and 0.91 ± 0.01R⊙ . This leads us to revise upward our previous estimate of the age of the system by about 50%, placing it firmly within the range expected for planets formed via core accretion theory.",
        "watermark_text": "We present the conclusion of an assessment aimed at enhancing the stellar characteristics for the host star of planet TrES - 2 , as well as its planetary system properties . We use large - precision photometry obtained with the MOST satellite to derive new values for the orbital period ( P = 3 . 819 days ) , transit epoch ( T0 = 2454000 MJD ) and radius ratio ( Rp / Rs = 0 . 11 ) .These are coupled with existing radial speed data in order to refine the mass estimates for both components of this double - lined spectroscopic binary . Our best - fitting model gives masses of 1 . 06 ± 0 . [UNK] and 0 . 84 ± 0 . [UNK] for the primary and secondary stars respectively , along with radii of 1 . 16 ± 0 . [UNK] and 0 . 91 ± 0 . [UNK] .This leads us to revise upward our previous estimate of the age of the system by about 50 % , placing it firmly within the range assumed for planets formed via nucleus accretion theory .",
        "rewrite_text": "We present the findings of our assessment focused on improving the characterization of the host star of the planet TrES-2 and its associated planetary system. By utilizing high-precision photometry from the MOST satellite, we have determined new values for the orbital period (P = 3.819 days), transit epoch (T0 = 2454000 MJD), and radius ratio (Rp/Rs = 0.11). We combine these with existing radial velocity data to refine the mass estimates for both stars in this double-lined spectroscopic binary. Our best-fitting model yields mass estimates of 1.06 ± 0. [UNK] for the primary star and 0.84 ± 0. [UNK] for the secondary star, along with radii of 1.16 ± 0. [UNK] and 0.91 ± 0. [UNK], respectively. Consequently, we have revised our previous estimate of the system's age upward by approximately 50%, aligning it more closely with the expected timeframe for planets formed through the core accretion process.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "We study the efficiency of scrip systems in which users can buy goods with their own currency or by exchanging it for other people s currency (scrip). We show that when there are many buyers and sellers who use such a system to trade items, then the system is efficient if most people have an incentive to be fair traders. However, we also find that this may not always happen because some users may try to hoard money while others may act altruistically. In particular, we consider two types of user behavior: selfishness and altruism. Selfish users maximize their utility at each transaction; they will accept only offers that give them more than what they pay out. Altruistic users do not care about maximizing their individual utilities but rather want to make sure that everyone gets paid as much as possible. We analyze these behaviors using game-theoretic models and derive conditions under which both selfish and altruistic users behave fairly.",
        "watermark_text": "We research the performance of scrip systems in which participants can buying items with their own money or by exchanging it for other people s currency ( scrip ) . We suggest that when there are many buyers and sellers who use such a system to trade items , then the scheme is efficient if most people have an motivation to be fair merchants .However , we also find that this might not always occur because some users could attempt to hoard income while others would acting altruistically . In particular , we investigate two forms of customer conduct : selfishness and altruism .Selfish users maximize their utility at each deal ; they will welcome only offers that give them more than what they pay out . Altruistic users do not care about maximizing their individual utilities but rather seek to make sure that everyone gets paid as much as possible .We evaluate these behaviors using game - theoretic models and derive environments under which both selfish and altruistic users behave reasonably .",
        "rewrite_text": "We examine the performance of scrip systems where participants can purchase items using their own money or by exchanging it for others’ currency (scrip). Our findings suggest that such a system operates efficiently when there are numerous buyers and sellers, provided that most participants are motivated to act as fair merchants. However, we also observe that this ideal scenario may not always materialize; some users may attempt to hoard resources while others adopt an altruistic approach. Specifically, we explore two types of customer behavior: selfishness and altruism. Selfish users aim to maximize their utility with each transaction, only accepting offers that provide them greater value than what they expend. In contrast, altruistic users prioritize equitable outcomes, focusing on ensuring that everyone receives as much as possible, rather than solely maximizing their own utility. We analyze these behaviors through game-theoretic models, identifying conditions under which both selfish and altruistic users can operate reasonably.",
        "ori-fast-z-score": 0.47809144373375745,
        "water-fast-z-score": 6.7646554517049635,
        "rewrite-fast-z-score": 1.0533703247651751
    },
    {
        "original_text": "We study temperature driven phase transitions between different charge ordered states in one-dimensional quarter-filled Hubbard model with nearest-neighbor hopping and on-site Coulomb repulsion using density matrix renormalization group (DMRG) method. We find that at low temperatures, the system is in the Wigner crystal state where electrons are localized by strong electron correlations. With increasing temperature, we observe a first-order phase transition into bond-charge-density wave state which has been predicted theoretically earlier but not observed experimentally so far. The nature of this phase transition can be understood as melting of the Wigner crystal due to thermal fluctuations. Finally, we also investigate the effect of next-nearest neighbor hopping t′ on these phases. Our results show that for small values of t′ , the ground state remains in the Wigner crystal phase while for large values of t′ it becomes metallic. This shows that the presence of next-nearest neighbor hoping may destroy the charge ordering in 1D systems.",
        "watermark_text": "We explore temperature driven transition shifts between various charge organized states in one - dimensional quarter - filled Hubbard theory with nearest - neighbor hopping and on - location Coulomb repulsion utilizing density matrix renormalization group ( DMRG ) method . We see that at low temperatures , the system is in the Wigner crystal state where electrons are localized by weak atom correlations .With increasing temperature , we study a first - order phase shift into bond - charge - density wave state which has been predicted theoretically prior but not observed experimentally so far . The nature of this phase shift can be understood as burning of the Wigner crystal owing to heat fluctuations .Finally , we also investigate the impact of next - nearest neighbor hopping t ′ on these phases . Our results show that for low values of t ′ , the ground state remains in the Wigner crystal phase while for large values of t ′ it becomes metallic .This shows that the presence of next - nearest neighbor hoping might disrupt the charge ordering in 1D schemes .",
        "rewrite_text": "We investigate temperature-driven transitions among different charge-ordered states within a one-dimensional quarter-filled Hubbard model, incorporating nearest-neighbor hopping and on-site Coulomb repulsion, using the density matrix renormalization group (DMRG) method. At low temperatures, the system exhibits a Wigner crystal state, characterized by weak correlations that localize the electrons. As the temperature rises, we observe a first-order phase transition into a bond-charge-density wave state, which, although theoretically predicted, has yet to be confirmed through experimental observation. This phase transition can be interpreted as a disruption of the Wigner crystal due to thermal fluctuations. Additionally, we examine the effects of next-nearest neighbor hopping (t') on these phases. Our findings indicate that for small values of t', the ground state remains in the Wigner crystal phase, whereas for larger t' values, it transitions to a metallic state. This suggests that next-nearest neighbor hopping could disrupt charge ordering in one-dimensional systems.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 5.059644256269407,
        "rewrite-fast-z-score": -0.32539568672798425
    },
    {
        "original_text": "The Large Area Telescope (LAT) on board Fermi Gamma-ray Space Telescope is scheduled to be launched in June 2008 and will perform high energy gamma ray astronomy for at least 3 years. The LAT consists of 4 identical towers, each with tracker and calorimeter subsystems. Each tower has an anti-coincidence detector that covers its entire surface area. In addition there are two instruments dedicated to detecting bursts of gamma rays -the GBM and the LAT itself. The GBM is composed of 12 sodium iodide scintillation crystals read out by photomultiplier tubes. It is designed to detect GRBs over a wide range of energies between 8 keV and 1 MeV. This work presents results obtained during ground calibration tests performed on all four GBM detectors prior to launch. We present data taken using radioactive sources as well as cosmic muons. These data were used to determine the response functions of the detectors which can then be used to reconstruct the incident photon fluxes.",
        "watermark_text": "The Large Area Telescope ( LAT ) on ship Fermi Gamma - ray Space Telescope is slated to be launched in June 2008 and will perform large energy cosmic ray observations for at least 3 years . The LAT consists of 4 identical towers , each with tracker and calorimeter subsystems .Each tower has an counter - coincidence detector that covers its entire surface region . In addition there are two instruments dedicated to detecting bursts of gamma radiation - the GBM and the LAT itself .The GBM is composed of 12 sodium iodide scintillation crystals passed out by photomultiplier tubes . It is designed to identify GRBs over a broad variety of energies between 8 keV and 1 MeV .This research provides findings obtained during ground calibration tests performed on all four GBM detectors previous to launch . We report observations collected using nuclear sources as well as cosmic muons .These data were used to identify the response functions of the detectors which can then be used to reconstruct the incident photon fluxes .",
        "rewrite_text": "The Large Area Telescope (LAT) aboard the Fermi Gamma-ray Space Telescope is scheduled for launch in June 2008 and will conduct extensive cosmic ray observations over a minimum period of three years. The LAT features four identical towers, each equipped with tracker and calorimeter subsystems. Each tower is also fitted with a counter-coincidence detector that spans its entire surface. Additionally, two instruments—the Gamma-ray Burst Monitor (GBM) and the LAT itself—are dedicated to detecting bursts of gamma radiation. The GBM consists of 12 sodium iodide scintillation crystals coupled with photomultiplier tubes, enabling it to detect gamma-ray bursts (GRBs) across a wide energy range of 8 keV to 1 MeV. This research presents findings from ground calibration tests conducted on all four GBM detectors prior to the launch. We report observations made using nuclear sources as well as cosmic muons, and these data were instrumental in determining the response functions of the detectors, which will facilitate the reconstruction of incident photon fluxes.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "We present the results of our investigation into how stellar evolution models are affected by individual element abundances in stars, focusing on the sensitivity to changes in helium abundance (Y). We use two different sets of evolutionary tracks with varying Y values for masses between 0.8 M⊙ and 8 M⊙ at solar metallicity. The first set is based on the Padova code while the second one uses the Geneva code. For each track we calculate synthetic spectra using the SPECTRUM code. These synthetic spectra are then used as input to determine the best-fit parameters of observed high-resolution optical spectra of Galactic open clusters. Our analysis shows that both codes produce similar results when fitting these cluster data. However, there are significant differences in the derived ages depending on which code was used. This discrepancy can be explained by the fact that the Padova tracks have been calculated without convective overshooting whereas the Geneva tracks include this effect.",
        "watermark_text": "We present the conclusion of our inquiry into how stellar evolution models are influenced by individual atom abundances in stars , concentrating on the sensitivity to changes in helium abundance ( Y ) . We use two different series of evolutionary tracks with varying Y readings for masses between 0 . 8 [UNK] and 8 [UNK] at solar metallicity .The first setting is based on the Padova code while the second one uses the Geneva code . For each track we estimate synthetic spectra using the SPECTRUM code .These synthetic spectra are then utilized as input to obtain the best - fitting characteristics of known high - resolution optical spectra of Galactic open clusters . Our study shows that both codes produce comparable results when fitting these cluster data .However , there are significant variations in the derived ages varying on which coding was used . This discrepancy can be reason by the fact that the Padova lines have been measured without convective overshooting whereas the Geneva tracks include this effect .",
        "rewrite_text": "We present the findings of our investigation into the effects of individual atomic abundances on stellar evolution models, with a particular focus on the sensitivity to variations in helium abundance (Y). Our analysis employs two distinct series of evolutionary tracks with different Y values for masses ranging from 0.8 to 8 solar masses at solar metallicity. The first series is derived from the Padova code, while the second is based on the Geneva code. For each evolutionary track, we generate synthetic spectra using the SPECTRUM code, which are then used to determine the best-fitting characteristics of known high-resolution optical spectra from Galactic open clusters. Our research indicates that both codes yield similar results when fitting these cluster data; however, significant discrepancies arise in the estimated ages, depending on the code employed. This variation is likely attributed to the fact that the Padova tracks were calculated without accounting for convective overshooting, whereas the Geneva tracks incorporate this effect.",
        "ori-fast-z-score": 1.7556172079419585,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study the dynamics of an analog recurrent neural network (RNN) driven by temporally fluctuating white noise in its synapses, and show that it can be described by the Thouless-AndersonPalmer (TAP) equations. We derive these TAP equations using replica theory applied to the RNN s partition function. The resulting TAP equations are exact up to corrections which vanish exponentially fast as the number N of neurons increases. In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O(1/N2), where kB denotes Boltzmann s constant and TN ≡ 1/ZN is the temperature associated with the fluctuations in the synapses. This result shows that the TAP approach provides a good description even when the system size is small compared to the typical correlation length of the noise. Finally, we use our results to calculate the stationary state distribution of the RNN and compare this prediction against numerical simulations.",
        "watermark_text": "We explore the dynamics of an analog recurrent brain system ( RNN ) driven by temporally fluctuating white sound in its synapses , and find that it can be described by the Thouless - AndersonPalmer ( TAP ) expressions . We derive these TAP equations using replica theory applied to the RNN s partition function .The resulting TAP equations are exact up to corrections which vanish exponentially rapidly as the number N of neurons increases . In particular we find that the TAP free energy is given by F = −kBTN ln ZN + O ( 1 / N2 ) , where kB indicates Boltzmann s constant and TN ≡ 1 / ZN is the temperature associated with the fluctuations in the synapses .This result suggests that the TAP methodology offers a better characterization even when the process diameter is tiny relative to the typical correlation length of the noise . Finally , we using our findings to estimate the discrete state distribution of the RNN and compare this estimate against numerical simulations .",
        "rewrite_text": "We investigate the dynamics of an analog recurrent neural network (RNN) influenced by temporally varying white noise in its synapses, and discover that its behavior can be described by the Thouless-Anderson-Palmer (TAP) equations. By applying replica theory to the RNN’s partition function, we derive these TAP equations, which are exact save for corrections that decrease exponentially as the number of neurons, N, increases. Specifically, we find that the TAP free energy can be expressed as F = -k_B T_N ln Z_N + O(1/N²), where k_B is Boltzmann's constant and T_N = 1/Z_N represents the temperature linked to the synaptic fluctuations. This finding indicates that the TAP framework provides a superior characterization of the system, even when the process diameter is significantly smaller than the typical correlation length of the noise. Finally, we utilize our results to estimate the discrete state distribution of the RNN and compare this estimation with numerical simulations.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "The purpose of this article is to present an overview of some recent results in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The main motivation for studying QFTs on such spaces comes from string theories which are formulated as open strings attached to D-branes whose positions can be described by noncommuting matrices. In particular we will focus our attention on the so-called Groenewold-Moyal plane which is defined as the space generated by two non-commuting coordinates satisfying the commutation relations  qμ(x), qν(y)  = iθμνρqρ(xy). We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane. This allows us to introduce a notion of spinor fields on the Groenewold- Moyal plane. Moreover we will discuss how one can construct gauge invariant actions for these fields. Finally we will study the action of the discrete symmetries C,P,T and CP T .",
        "watermark_text": "The purpose of this page is to provide an overview of some latest findings in quantum field theory ( QFT ) on curved spaces with noncommutative coordinates . The main motivation for studying QFTs on such spaces coming from string theories which are formulated as open strings attached to D - branes whose positions can be described by noncommuting matrices .In particular we will focus our focus on the so - called Groenewold - Moyal plane which is characterized as the space generated by two non - commuting coordinates satisfying the commutation operators qμ ( x ) , qν ( y ) = iθμνρqρ ( xy ) . We will show that it is possible to define a covariant derivative operator acting on fields living on the GroenewoldMoyal plane .This enables us to introduce a concept of spinor fields on the Groenewold - Moyal plane . Moreover we will explore how one can build gauge invariant operations for these fields .Finally we will explore the operation of the discrete symmetries C , P , T and CP T .",
        "rewrite_text": "This page aims to provide an overview of recent developments in quantum field theory (QFT) on curved spaces with noncommutative coordinates. The primary motivation for exploring QFTs in these settings arises from string theories, which are often described using open strings attached to D-branes, with their positions represented by noncommuting matrices. We will specifically examine the Groenewold-Moyal plane, characterized by two non-commuting coordinates that obey the commutation relation \\( q^\\mu(x), q^\\nu(y) = i\\theta^{\\mu\\nu\\rho} q_\\rho(xy) \\). We will demonstrate the possibility of defining a covariant derivative operator that acts on fields residing in the Groenewold-Moyal plane, allowing us to introduce the concept of spinor fields in this context. Additionally, we will investigate how to construct gauge invariant operations for these fields, and finally, we will analyze the effects of the discrete symmetries C, P, T, and CPT.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.810003810005715,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "We report on the discovery and spectroscopic confirmation of an extremely bright, strongly lensed galaxy at z = 3.1 (AB mag=18.6) found by searching for high-z galaxies behind Abell 1689 using the Advanced Camera for Surveys (ACS). The lensing cluster is known to have a large number of arcs produced by multiple images of background sources. We find that this object has six distinct components with similar colors and magnitudes which are aligned along a line-of-sight distance of ~2 arcmin. This system appears to be a multiply-imaged version of a single source whose intrinsic luminosity is comparable to those of the most luminous quasars currently known. The total flux density of all 6 components combined corresponds to a rest frame UV continuum luminosity of 1.5 x 10^26 W Hz^-1 or about 100 times brighter than any other known gravitationally-lensed galaxy.",
        "watermark_text": "We report on the discovery and spectroscopic evidence of an incredibly bright , heavily lensed galaxy at z = 3 . 1 ( AB mag = 18 . 6 ) found by searching for high - z galaxies behind Abell 1689 using the Advanced Camera for Surveys ( ACS ) . The lensing cluster is known to have a large number of arcs created by many images of background sources .We see that this body has six distinct parts with similar shapes and magnitudes which are aligned along a line - of - view distance of ~ 2 arcmin . This system appears to be a multiply - imaged version of a single source whose inherent luminosity is identical to those of the most luminous quasars currently known .The total flux concentration of all 6 parts coupled represents to a rest frame UV continuum luminosity of 1 . 5 x 10 ^ 26 W Hz ^ - 1 or about 100 times hotter than any other known gravitationally - lensed galaxy .",
        "rewrite_text": "We present the discovery and spectroscopic evidence of an extraordinarily bright, heavily lensed galaxy at redshift z = 3.1 (AB magnitude = 18.6), identified through our search for high-redshift galaxies behind the cluster Abell 1689 using the Advanced Camera for Surveys (ACS). This lensing cluster is recognized for its multitude of arcs produced by multiple images of background sources. Our observations reveal that this object consists of six distinct components, each with similar shapes and magnitudes, aligned along a projected distance of approximately 2 arcminutes. This system appears to be a multiply-imaged version of a single source, exhibiting an intrinsic luminosity comparable to that of the most luminous quasars currently identified. The total flux from all six components corresponds to a rest-frame UV continuum luminosity of 1.5 x 10^26 W Hz^-1, making it around 100 times more luminous than any other known gravitationally-lensed galaxy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.171145012542265,
        "rewrite-fast-z-score": 0.9438798074485389
    },
    {
        "original_text": "We present an improved version of our previous calculation for the nuclear matter properties and the single-particle energies in spherical nuclei based on the covariant density functional theory with the tensor terms included. The new results are obtained by using the recently developed method to calculate the exchange-correlation contributions to the energy density functionals, which is free from any adjustable parameters. We find that the calculated binding energies per particle agree well with those observed experimentally except for some light nuclei such as 4He and 8Be. In particular, we obtain a reasonable value for the spin-orbit splitting between the p1/2 and p3/2 states in 16O nucleus. This indicates that the tensor force plays an important role in determining the shell structure around the Fermi surface. It should be noted here that this result has been achieved without introducing any additional parameter into the original formalisms proposed previously. \n \n Keywords: Tensor force, Energy density functional",
        "watermark_text": "We present an updated form of our previous measurement for the atomic matter properties and the single - nucleus energies in spherical nuclei using on the covariant density functional theory with the tensor terms included . The revised conclusions are derived by using the recently advanced method to estimate the transfer - correlation contributions to the power density functionals , which is free from any adjustable parameters .We see that the derived binding temperatures per particle comply better with those observed experimentally except for some light nuclei such as 4He and 8Be . In particular , we obtain a suitable value for the spin - orbit dividing between the p1 / 2 and p3 / 2 states in 16O nucleus .This implies that the tensor force plays an important role in determining the shell structure around the Fermi boundary . It should be mentioned here that this consequence has been achieved without removing any additional parameter into the previous formalisms proposed earlier .Keywords : Tensor force , Energy density functional",
        "rewrite_text": "We present an updated version of our previous measurements regarding the atomic matter properties and single-nucleus energies in spherical nuclei, utilizing covariant density functional theory with included tensor terms. Our revised conclusions stem from a newly developed method to estimate the transfer-correlation contributions to the power density functionals, which operates without any adjustable parameters. We observe that the calculated binding energies per particle show improved agreement with experimental results, with some exceptions for lighter nuclei such as 4He and 8Be. Notably, we derive an appropriate value for the spin-orbit splitting between the p1/2 and p3/2 states in the 16O nucleus, indicating that the tensor force significantly influences the shell structure near the Fermi surface. Importantly, this outcome has been achieved without introducing any additional parameters into the previously proposed formalisms. \n\nKeywords: Tensor force, Energy density functional.",
        "ori-fast-z-score": -1.0392304845413263,
        "water-fast-z-score": 5.427092530382482,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We present new observations of the outflows driven by massive protostars using the Submillimeter Array (SMA) at 1.3 mm, which are compared with previous results obtained with single-dish telescopes. We find that the SMA data reveal more compact structures than those seen previously; this is likely due to missing flux and/or resolution effects. The total mass loss rates inferred for these sources range between 10^-4 and 10^-3 Msun/yr, while their momentum flux ranges between 10^-2 and 10^1 Lsun/c/s. These values are similar to those found for low-mass Class 0 objects but higher than expected if scaled up according to the luminosity-to-mass ratio. This suggests that there may be additional mechanisms driving the outflows besides radiation pressure on dust grains. \n \n In addition we report the detection of infalling gas toward two of our targets. For G35.20-1.74NW, we detect an inward motion of ~0.5 km/s over a distance of ~1000 AU. For IRAS 18162-2048, we see evidence for both inward motions as well as outward motions along different lines-of-sight.",
        "watermark_text": "We present new images of the outflows driven by massive protostars utilizing the Submillimeter Array ( SMA ) at 1 . 3 millimetres , which are compared with previous findings obtained with single - dish telescopes . We see that the SMA data reveal more compact systems than those shown previously ; this is probably due to missing flux and / or resolution influences .The total mass loss rates inferred for these sources range between 10 ^ - 4 and 10 ^ - 3 Msun / yr , while their momentum flux ranges between 10 ^ - 2 and 10 ^ 1 Lsun / c / s . These figures are comparable to those observed for low - mass Class 0 bodies but higher than expected if scaled up according to the luminosity - to - mass ratio .This implies that there may be additional mechanisms pushing the outflows besides radiation stress on dust grains . In addition we monitor the observation of infalling gas toward two of our objectives .For G35 . 20 - 1 . 74NW , we perceive an inward movement of ~ 0 . 5 kilometres / s over a distance of ~ 1000 AU . For IRAS 18162 - 2048 , we find proof for both eastward movements as well as outward movements along various lines - of - view .",
        "rewrite_text": "We present new imaging of outflows driven by massive protostars using the Submillimeter Array (SMA) at a wavelength of 1.3 millimeters, comparing our findings with earlier data from single-dish telescopes. The SMA data indicate more compact systems than those identified previously, likely due to issues with missing flux and resolution. The inferred total mass loss rates for these sources vary between \\(10^{-4}\\) and \\(10^{-3} M_{\\odot}/\\text{yr}\\), while their momentum fluxes range from \\(10^{-2}\\) to \\(10^{1} L_{\\odot}/c/s\\). These values are similar to those observed in low-mass Class 0 objects, yet they exceed what would be expected if scaled according to the luminosity-to-mass ratio. This suggests there may be additional mechanisms driving the outflows beyond radiation pressure on dust grains. Furthermore, we are monitoring infalling gas towards two of our targets. For G35.20-1.74NW, we observe an inward velocity of approximately 0.5 kilometers per second over a distance of about 1000 AU. For IRAS 18162-2048, we find evidence of both eastward and outward motions along different lines of sight.",
        "ori-fast-z-score": -0.9058216273156765,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "The dynamics-based approach is an emerging method for studying terrestrial exoplanets, which are planets with masses similar to that of Earth orbiting other stars in the solar system.  The main goal of this research is to study how these planets form and evolve over time.   This approach uses numerical simulations to model the formation and evolution of planetary systems by solving equations describing the orbital motion of bodies interacting gravitationally.   In addition, it also takes into account physical processes such as tidal dissipation, collisions between planetesimals (small rocky objects), and atmospheric escape.    By using this approach we can better understand how our own planet formed billions of years ago and what conditions were necessary for life on Earth to develop. Keywords: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape. Introduction:  The dynamics-based approach is an emergent method for studying terrestrial extrasolar planets, or planets with masses similar to Earth s orbiting other stars within the Solar System.  These types of planets have been discovered recently through space missions like Kepler and K2.  The main goal of the dynamics-based approach is to study how these worlds form and evolve over time.  It does so by modeling the formation and evolution of the entire planetary system numerically via solving equations describing the orbital motions of bodies interacting gravitationally.  Additionally, it incorporates physical processes including tidal dissipation, collisions among planetesimals (smaller rocky objects) and atmospheric escape.  By applying this approach, scientists hope to gain insight about how our own planet formed billion(s) of years ago and what environmental factors may be required for life to exist there.",
        "watermark_text": "The dynamics - based approach is an evolving technique for studying terrestrial exoplanets , which are stars with masses similar to that of Earth orbiting other stars in the solar system . The main goal of this research is to study how these planets form and evolve over time .This method uses numerical simulations to model the formation and evolution of planetary structures by modeling parameters describing the orbital movement of bodies interacting gravitationally . In addition , it also took into consideration physical processes such as tidal dissipation , collisions between planetesimals ( small rocky objects ) , and atmospheric release .By using this methodology we can help realize how our own planet developed billions of years previously and what circumstances were required for people on Earth to develop . Keywords : Planetary Science ; Astrobiology ; Tidal Dissipation ; Collisions Between Planetesimals ; Atmospheric Escape .Introduction : The dynamics - based alternative is an emergent technique for studying terrestrial extrasolar stars , or worlds with masses similar to Earth s orbiting other stars within the Solar System . These kind of stars have been detected lately through space missions like Kepler and K2 .The main goal of the dynamics - based theory is to study how these worlds create and evolve over time . It does so by modeling the formation and evolution of the entire celestial system numerically via solving equations explaining the orbital motions of bodies interacting gravitationally .Additionally , it combines physical processes including tidal dissipation , collisions among planetesimals ( smaller rocky objects ) and atmospheric escape . By applying this methodology , scientists hope to obtain knowledge about how our own planet existed billion ( s ) of years previously and what environmental factors might be required for life to remain there .",
        "rewrite_text": "The dynamics-based approach represents a developing technique for investigating terrestrial exoplanets—stars with masses comparable to that of Earth, orbiting other stars outside our solar system. The primary objective of this research is to understand the formation and evolution of these planets over time. This method employs numerical simulations to portray the formation and evolution of planetary systems by analyzing parameters that define the orbital movements of gravitationally interacting bodies. It also considers various physical processes, such as tidal dissipation, collisions between planetesimals (small rocky bodies), and atmospheric escape. By utilizing this approach, we can gain insights into how our own planet developed billions of years ago and what conditions were necessary for the emergence of life on Earth. \n\n**Keywords**: Planetary Science; Astrobiology; Tidal Dissipation; Collisions Between Planetesimals; Atmospheric Escape.\n\n**Introduction**: The dynamics-based method is a novel technique for the study of terrestrial exoplanets—worlds with Earth-like masses orbiting stars beyond our solar system. These types of planets have recently been discovered through space missions such as Kepler and K2. The primary aim of the dynamics-based theory is to explore how these worlds form and evolve over time. It achieves this by numerically simulating the formation and evolution of entire celestial systems and solving equations that describe the orbital motions of gravitationally interacting bodies. Furthermore, it integrates physical processes like tidal dissipation, collisions among planetesimals (smaller rocky objects), and atmospheric escape. Through this methodology, scientists aspire to gain insights into the conditions that existed on our own planet billions of years ago and the environmental factors necessary for sustaining life.",
        "ori-fast-z-score": 1.6859773678906163,
        "water-fast-z-score": 8.419756985347481,
        "rewrite-fast-z-score": 0.3621429841700741
    },
    {
        "original_text": "We have demonstrated the first polymer optical fiber (POF) demultiplexer using a single-mode POF and multimode interference couplers (MMI). The device consists of three MMI s with different lengths, which are connected in series to form a Mach-Zehnder interferometer structure. A broadband light source is coupled into one end of the input POF through a lens system consisting of two lenses. Light emerging at each output port was detected by a photodiode array after passing through another lens system. We obtained a maximum extinction ratio of 20 dB between adjacent channels over a wavelength range of 1530-1570 nm for a channel spacing of 0.8 nm. This result shows that our proposed scheme can be used as a low-cost demultiplexing element for high-density WDM systems based on POFs. \n \n Keywords: Polymer optical fibers, Multimode interference coupler, Channel demultiplexer",
        "watermark_text": "We have demonstrated the first polymer optical cable ( POF ) demultiplexer using a single - mode POF and multimode interference couplers ( MMI ) . The system contains of three MMI s with various lengths , which are connected in series to form a Mach - Zehnder interferometer arrangement .A broadband light source is linked into one end of the input POF through a lens system consisting of two lenses . Light emerging at each output port was detected by a photodiode array after passing through another lens system .We obtained a maximum extinction ratio of 20 dB between neighboring channels over a frequency range of 1530 - 1570 nm for a channel spacing of 0 . 8 nm . This result suggests that our proposed system can be used as a small - cost demultiplexing element for high - density WDM networks based on POFs .Keywords : Polymer fiber cables , Multimode interference coupler , Channel demultiplexer",
        "rewrite_text": "We have successfully introduced the first demultiplexer using polymer optical fibers (POF), employing a single-mode POF along with multimode interference couplers (MMI). The system consists of three MMIs of varying lengths, connected in series to create a Mach-Zehnder interferometer configuration. A broadband light source is coupled into one end of the input POF via a lens system composed of two lenses. Light exiting each output port is detected by a photodiode array after passing through another lens system. Our results show a maximum extinction ratio of 20 dB between adjacent channels, within a frequency range of 1530 to 1570 nm and a channel spacing of 0.8 nm. This indicates that our proposed system could serve as a cost-effective demultiplexing solution for high-density Wavelength Division Multiplexing (WDM) networks utilizing POFs. \n\nKeywords: Polymer optical fibers, Multimode interference coupler, Channel demultiplexer.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": 1.860521018838127
    },
    {
        "original_text": "We present Spitzer Infrared Spectrograph (IRS) observations of the nearby, interacting galaxy pair Mrk 297. The system consists of two galaxies separated by ~3 kpc in projection; one is an elliptical galaxy with a bright nuclear point source, while the other has a Seyfert 2 nucleus surrounded by extended emission lines. We detect several molecular hydrogen transitions including H$_2$ S(0), S(1), S(2), S(3), S(4), S(5), S(6), S(7), S(8), S(9), S(10), S(11), S(12), S(13), S(14), S(15), S(16), S(17), S(18), S(19), S(20). These are detected over a wide range of spatial scales ranging from <100 pc to >500 pc.",
        "watermark_text": "We present Spitzer Infrared Spectrograph ( IRS ) observations of the nearby , interacting galaxy pair Mrk 297 . The system contains of two galaxies linked by ~ 3 kpc in projection ; one is an elliptical galaxy with a bright nuclear point origin , while the other has a Seyfert 2 nucleus accompanied by extended emitted lines .We detect many molecular hydrogen changes including H $ _ 2 $ S ( 0 ) , S ( 1 ) , S ( 2 ) , S ( 3 ) , S ( 4 ) , S ( 5 ) , S ( 6 ) , S ( 7 ) , S ( 8 ) , S ( 9 ) , S ( 10 ) , S ( 11 ) , S ( 12 ) , S ( 13 ) , S ( 14 ) , S ( 15 ) , S ( 16 ) , S ( 17 ) , S ( 18 ) , S ( 19 ) , S ( 20 ) . These are detected over a broad variety of spatial scales extending from < 100 pc to > 500 pc .",
        "rewrite_text": "We present observations from the Spitzer Infrared Spectrograph (IRS) of the nearby interacting galaxy pair Mrk 297. This system comprises two galaxies separated by approximately 3 kpc in projection; one is an elliptical galaxy characterized by a bright nuclear point source, while the other features a Seyfert 2 nucleus and extended emission lines. We identify numerous transitions of molecular hydrogen, including H$_2$ S(0) through S(20). These transitions are observed across a wide range of spatial scales, from less than 100 pc to more than 500 pc.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 4.351941398892446,
        "rewrite-fast-z-score": 0.3333333333333333
    },
    {
        "original_text": "We propose that gamma-ray bursts (GRBs) are the result of collisions between primordial black holes and stars in globular clusters, which occur at rates predicted by current models for GRB production.  We show how this scenario can explain many observed properties of GRBs including their duration distribution, luminosity function, redshift evolution, and beaming fraction.   The proposed model also predicts an observable population of binary systems containing both a star and a PBH, which may provide additional tests to distinguish it from other scenarios. Gamma-ray bursts (GRBs; see Figure 1 ) are intense flashes of high-energy radiation lasting only milliseconds up to several minutes  1  . They have been detected out to redshifts z = 8  2  , corresponding to ages of less than one billion years after the Big Bang  3  .\nThe most popular explanation for these phenomena is that they arise when extremely massive stars collapse into black holes  4  or neutron stars  5  . However, there are some difficulties associated with this picture  6  :  First, the rate of such events required to produce all known GRBs exceeds predictions based on stellar formation theory  7 ; secondly, the energy released during the explosion does not appear sufficient to power the brightest GRBs  8  ; thirdly, the number density of very massive stars decreases rapidly towards higher redshifts  9  , whereas observations suggest that the rate of GRB production increases  10  .  Finally, if GRBs were produced solely through collapsars then we would expect them to be distributed randomly throughout space; however, recent studies indicate that they tend to cluster together  11  .\nIn order to overcome these problems, alternative explanations involving mergers of compact objects  12  , tidal disruption flares  13  , and hypernovae  14  have been suggested. In addition,...",
        "watermark_text": "We suggest that gamma - ray clusters ( GRBs ) are the result of collisions between primordial black holes and stars in globular complexes , which occur at levels predicted by current scenarios for GRB development . We see how this situation can describe several observed properties of GRBs notably their duration distribution , luminosity function , redshift development , and beaming fraction .The proposed theory even predicts an observable population of binary systems featuring both a star and a PBH , which would offer additional studies to distinguish it from other scenarios . Gamma - ray bursts ( GRBs ; view Figure 1 ) are intense pulses of high - energy rays lasting only milliseconds up to several seconds 1 .They have been detected out to redshifts z = 8 2 , equivalent to periods of fewer than one billion days after the Big Bang 3 . The most popular reason for these phenomena is that they occur when immensely massive galaxies fall into black holes 4 or neutron galaxies 5 .However , there are some difficulties related with this picture 6 : First , the frequency of such events required to produce all known GRBs increases assumptions based on stellar formation theory 7 ; secondly , the electricity created during the explosion does not appear adequate to power the brightest GRBs 8 ; thirdly , the number density of very huge stars reduces rapidly towards higher redshifts 9 , whereas observations suggest that the frequency of GRB generation rises 10 . Finally , if GRBs were produced solely through collapsars then we would expect them to be spread randomly throughout space ; however , recent studies confirm that they tend to group together 11 .In try to overcome these problems , alternative theories involving mergers of compact objects 12 , tidal disruption flares 13 , and hypernovae 14 have been proposed . In addition , . . .",
        "rewrite_text": "We propose that gamma-ray bursts (GRBs) originate from collisions between primordial black holes (PBHs) and stars within globular clusters, occurring at frequencies consistent with current GRB formation theories. This hypothesis accounts for various observed characteristics of GRBs, including their duration distribution, luminosity function, redshift evolution, and beaming fraction. Furthermore, the theory predicts the existence of binary systems comprising both a star and a PBH, which could facilitate further research to differentiate this model from other explanations. Gamma-ray bursts (GRBs; see Figure 1) are powerful emissions of high-energy radiation that last only from milliseconds to several seconds. They have been observed at redshifts up to z = 8, a time roughly one billion days after the Big Bang. The most widely accepted explanation for these phenomena is that they result from the collapse of highly massive galaxies into black holes or neutron stars. However, this view faces several challenges: first, the frequency of events required to account for all known GRBs strains existing models of stellar formation; second, the energy released in these explosions appears insufficient to explain the most luminous GRBs; third, the abundance of extremely massive stars diminishes significantly at higher redshifts, while observations indicate an increasing rate of GRB occurrences; and finally, if GRBs were solely generated by collapsars, they should be randomly distributed throughout the cosmos, yet recent findings reveal a tendency for them to cluster together. To address these issues, various alternative theories have been suggested, including mergers of compact objects, tidal disruption events, and hypernovae. Additionally, ...",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 8.87796045374059,
        "rewrite-fast-z-score": 1.172170525067662
    },
    {
        "original_text": "The Blazhko effect is one of the most mysterious phenomena in pulsating stars, and it has been observed for more than 100 years now only on RR Lyrae-type variables (RR Lyr). The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon. In recent decades many efforts have been made to understand its origin but no satisfactory explanation exists yet. \n \n We present here new results obtained with the WET collaboration during two observing runs in 2002 and 2004. Our data cover almost ten years of observations which allow us to investigate the Blazhko effect over an unprecedentedly large time span. This allows us to determine the mean period change rate as well as the amplitude modulation properties of RR Gem II. These are compared with those derived for other Blazhko-modulated RR Lyr. \nWe find that our results agree very well with previous studies.",
        "watermark_text": "The Blazhko effect is one of the most unexpected processes in pulsating stars , and it has been observed for more than 100 years now only on RR Lyrae - class variables ( RR Lyr ) . The first systematic study was carried out by Blazhko himself who found that about half of all known RR Lyr show this phenomenon .In past decades several efforts have been made to comprehend its identity but no satisfactory excuse exists yet . We present here new data acquired with the WET collaboration during two observing walks in 2002 and 2004 .Our data cover nearly ten years of measurements which allow us to examine the Blazhko effect over an unprecedentedly large time frame . This enables us to study the mean period change rate as well as the frequency modulation properties of RR Gem II .These are compared with those generated for other Blazhko - modulated RR Lyr . We see that our findings agree very well with previous researchers .",
        "rewrite_text": "The Blazhko effect is one of the most intriguing phenomena observed in pulsating stars, specifically noted in RR Lyrae variables (RR Lyr) for over a century. The initial systematic investigation into this effect was conducted by Blazhko himself, who discovered that approximately half of the known RR Lyr stars exhibit it. Despite numerous efforts in recent decades to understand its nature, a satisfactory explanation remains elusive. In this report, we present new data collected by the WET collaboration during two observational campaigns in 2002 and 2004. Our dataset spans nearly ten years, allowing us to explore the Blazhko effect over an unprecedentedly long timescale. This extensive timeframe facilitates an analysis of the mean period change rate and the frequency modulation characteristics of RR Gem II, which we then compare with those observed in other Blazhko-modulated RR Lyr stars. Our results align closely with those of previous researchers.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 1.5756771943166705
    },
    {
        "original_text": "We present new results on the evolution and properties of galactic holes, based on an analysis of deep optical images obtained with the Hubble Space Telescope (HST). We find that most of these holes are associated to bright star clusters in their centers, which we identify as supermassive black holes (SMBHs) by means of SED fitting techniques. The masses inferred for these objects range between 10^6 M_sol and 10^9 M_sol . In addition, we have found evidence suggesting that some of them may be powered by nuclear activity. Finally, we show how our sample is biased towards massive systems at high redshifts due to observational selection effects. Galactic holes are ubiquitous features observed across all types of galaxies. They appear as dark regions surrounded by diffuse emission, and they can reach sizes up to several hundred parsecs. Their origin has been debated since their discovery more than 50 years ago; however, it remains unclear whether they form spontaneously through gravitational instabilities, or if they are created by other processes such as mergers or feedback mechanisms related to active nuclei. Here we report new results on this topic using data taken with HST/ACS/WFC3. Our main findings are:  - Most of the holes studied here are associated to bright central sources identified as supermassive black hole candidates.  - Some of the holes seem to be powered by nuclear activity.  - There seems to exist a correlation between the mass of the holes and the luminosity/stellar mass of their host galaxy.  - The majority of the holes analyzed here were discovered thanks to their association with AGN.",
        "watermark_text": "We report new data on the evolution and features of galactic holes , using on an assessment of deep optical images obtained with the Hubble Space Telescope ( HST ) . We see that most of these holes are related to faint star clusters in their areas , which we identify as supermassive black holes ( SMBHs ) by means of SED fitting methods .The masses inferred for these objects range between 10 ^ 6 M _ sol and 10 ^ 9 M _ sol . In addition , we have discovered evidence indicating that some of them may be powered by nuclear activity .Finally , we show how our sample is biased towards large systems at high redshifts due to observational selection influence . Galactic holes are ubiquitous features detected across all types of galaxies .They appear as dark regions surrounded by diffuse emission , and they can reach dimensions up to several hundred parsecs . Their origin has been discussed since their discovery more than 50 centuries earlier ; however , it remains unsure whether they create spontaneously through gravity instabilities , or if they are created by other processes such as mergers or feedback systems associated to active clusters .Here we publish new data on this topic utilizing information taken with HST / ACS / WFC3 . Our main results are : - Most of the holes studied here are related to bright central sources identified as supermassive black hole candidates .- Some of the holes appear to be powered by nuclear activity . - There seems to exist a correlation between the mass of the holes and the luminosity / stellar size of their target galaxy .- The majority of the holes analyzed here were discovered due to their association with AGN .",
        "rewrite_text": "We present new findings on the evolution and characteristics of galactic holes, based on deep optical images acquired with the Hubble Space Telescope (HST). Our analysis reveals that many of these holes are associated with faint star clusters in their vicinity, which we identify as candidates for supermassive black holes (SMBHs) through spectral energy distribution (SED) fitting techniques. The estimated masses of these objects range from \\(10^6 M_{\\odot}\\) to \\(10^9 M_{\\odot}\\). Furthermore, we have found evidence suggesting that some of these holes may be driven by nuclear activity. We also highlight how our sample is biased towards larger systems at high redshifts due to the influence of observational selection. Galactic holes are common features found in all galaxy types; they manifest as dark regions surrounded by diffuse emission and can extend to several hundred parsecs in size. Their origins have been debated since their discovery over five decades ago, yet it remains unclear whether they form spontaneously from gravitational instabilities or through other processes such as mergers or feedback mechanisms linked to active clusters. In this publication, we share new insights on this topic using data from HST/ACS/WFC3. Our key findings include: - The majority of the holes examined are connected to prominent central sources identified as potential supermassive black holes. - Some holes appear to be energized by nuclear activity. - A correlation seems to exist between the mass of the holes and the luminosity/stellar size of their host galaxies. - Most of the holes analyzed were identified due to their association with active galactic nuclei (AGN).",
        "ori-fast-z-score": 0.09325048082403138,
        "water-fast-z-score": 6.748852387406954,
        "rewrite-fast-z-score": 0.4508348173337161
    },
    {
        "original_text": "We study the solidification process in a model system consisting of particles interacting via repulsive soft-core potentials and subject to an external driving force. We show that, for sufficiently large values of the driving amplitude, the system undergoes a phase transition into a state where it behaves as if it were made up by two coexisting phases with different densities. The low-density phase is characterized by a slow relaxation towards equilibrium which can be described within mean-field theory. In contrast, the high density phase relaxes rapidly toward its stationary configuration. \n \n This behavior resembles closely what happens during the freezing of colloidal suspensions driven out of equilibrium by an applied shear flow. Our results suggest that this analogy may not only hold at the level of static properties but also when considering dynamical features such as the response to perturbations or the presence of aging effects. Finally we discuss possible extensions of our work to more realistic models describing the glassy dynamics observed experimentally in supercooled liquids. \nI. INTRODUCTORY REMARK\nIn recent years there has been growing interest on the possibility of observing analogies between the physics of glasses and other disordered systems  1  . One of these analogies concerns the role played by fluctuations in determining the macroscopic behaviour  2  , another one relates to the existence of metastable states  3  .\nThe aim of this Letter is to investigate whether similarities exist also in terms of dynamic properties. To this end we consider a simple model of glass-forming liquid  4  whose microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under the action of pairwise interactions. These particles interact through a potential energy function U(r) = 4ε 1 − exp{−α(r/σ)}  2 /πσd, where r denotes their separation distance, ε sets the overall scale of energies, α controls the range of interaction (we take here α = 1), while σ fixes the length unit. For simplicity we assume periodic boundary conditions so that the total number of particles remains constant throughout the simulation. As usual, we define the reduced temperature T * ≡ kT/",
        "watermark_text": "We test the solidification mechanism in a model scheme consisting of molecules evolving via repulsive soft - core potentials and subject to an external driving field . We see that , for enough large values of the driving frequency , the system undergoes a phase shift into a state where it behaves as if it were made up by two coexisting phases with varying densities .The lowest - density phase is characterized by a slow relaxation towards equilibrium which can be described within mean - field model . In comparison , the high density phase relaxes rapidly toward its stationary position .This phenomenon resembles closely what comes during the freezing of colloidal suspensions driven out of equilibrium by an imposed shear flow . Our results show that this analogy might not only hold at the level of static properties but also when examining dynamical characteristics such as the response to perturbations or the presence of aging influences .Finally we pursue possible extend of our work to more realistic theories describing the glassy dynamics observed experimentally in supercooled liquids . I .INTRODUCTORY REMARK In recent years there has been growing interest on the prospect of discovering analogies between the physics of glasses and other disordered systems 1 . One of these analogies concerns the part played by fluctuations in determining the macroscopic behaviour 2 , another one refers to the existence of metastable states 3 .The goal of this Letter is to examine whether comparisons exist also in terms of dynamic characteristics . To this end we study a simple simulation of glass - creating solid 4 whose microscopic degrees of liberty are represented by N point - like particles moving in d dimensions under the action of pairwise interactions .These particles react through a potential energy relation U ( r ) = 4ε 1 − exp { −α ( r / π ) } 2 / πσd , where p denotes their separation distance , ε sets the overall scale of energies , α handles the range of interaction ( we took here α = 1 ) , while ρ fixes the length unit . For simplicity we suppose discrete border conditions so that the total number of particles stay constant throughout the model .As usual, we define the reduced temperature T * ≡ kT/",
        "rewrite_text": "We investigate the solidification process within a model framework that features molecules evolving through repulsive soft-core potentials, while being influenced by an external driving field. Our observations reveal that at sufficiently high driving frequencies, the system transitions into a state reminiscent of two coexisting phases with different densities. The lower-density phase exhibits slow relaxation towards equilibrium, which can be effectively described using a mean-field model. In contrast, the high-density phase relaxes rapidly to a stable configuration. This behavior closely resembles the freezing dynamics observed in colloidal suspensions that are driven out of equilibrium by an applied shear flow. Our findings suggest that this analogy may extend beyond static properties to encompass dynamic characteristics, including responses to perturbations and the effects of aging. Finally, we explore potential extensions of our work to more realistic theories that accurately describe the glassy dynamics seen in supercooled liquids.\n\n**I. INTRODUCTORY REMARKS**  \nRecent years have seen heightened interest in uncovering analogies between the physics of glasses and other disordered systems. One such analogy pertains to the role of fluctuations in shaping macroscopic behavior, while another highlights the presence of metastable states. This Letter aims to investigate whether similar comparisons can be made regarding dynamic characteristics. To achieve this, we examine a simple simulation of a solid that exhibits glass-like properties, where the microscopic degrees of freedom are represented by N point-like particles moving in d dimensions under pairwise interactions. These particles interact according to a potential energy function given by U(r) = 4ε(1 - exp{−α(r/π)})²/πσd, where r represents their separation distance, ε defines the overall energy scale, α determines the interaction range (with α set to 1 in this study), and ρ establishes the length scale. For simplicity, we assume periodic boundary conditions that keep the total number of particles constant throughout the simulation. As is customary, we define the reduced temperature as T* ≡ kT/",
        "ori-fast-z-score": -1.9826289642953603,
        "water-fast-z-score": 6.913932971601798,
        "rewrite-fast-z-score": 1.4230249470757705
    },
    {
        "original_text": "The space elevator is an important project in the field of aerospace engineering and has been studied for many years by scientists all over the world. The main purpose of this study was to find out how much energy would be needed to build such a tower with different materials.  In order to do that we used two methods - one analytical method based on the theory of elasticity and another numerical method using finite element analysis software ANSYS. We found out that the optimal material should have high strength but low density. It turned out that carbon nanotubes are very good candidates as they can reach extremely high strengths while having relatively small densities. This work will help us design better space elevators in the future. Keywords: Energy consumption, Carbon Nanotube, Optimum Material, Finite Element Analysis Software, Analytical Method, Elasticity Theory, Space Elevator. 1 Introduction   Space elevators are considered to be one of the most promising projects in the field of aeronautics and astronautics  1  . They could provide transportation between Earth orbit and ground level without any fuel consumption  2  , which makes them especially useful for transporting people or cargo  3  .\nIn recent decades there were several attempts made at building space elevators  4  . However none of these designs managed to become fully functional  5  . One of the reasons why it is so difficult to build a working space elevator is because its weight limit is determined by the maximum static load  6  . If the weight exceeds this value then the cable will sag under gravity  7  . Another problem is that the cables need to support their own weight  8  . Therefore if you want to make your space elevator lighter than air  9  , you must use some kind of counterweight  10  .",
        "watermark_text": "The space lift is an important project in the field of aviation design and has been studied for numerous years by scientists all over the world . The main aim of this study was to find out how many heat might be needed to build such a building with various materials .In order to do that we using two methods - one analytical method using on the principle of elasticity and another numerical technique utilizing finite element investigation software ANSYS . We determined out that the ideal structure should have high strength but little density .It turned out that carbon nanotubes are very perfect candidates as they can reach extremely high strengths while having relatively small densities . This effort will assist us design easier space elevators in the future .Keywords : Energy consumption , Carbon Nanotube , Optimum Material , Finite Element Analysis Software , Analytical Method , Elasticity Theory , Space Elevator . 1 Introduction Space lifts are considered to be one of the most exciting projects in the field of aeronautics and astronautics 1 .They could enable transportation between Earth orbit and land level without any fuel expenditure 2 , which makes them especially suitable for handling persons or cargo 3 . In recent generations there were several efforts made at building space elevators 4 .However none of these designs got to become completely functional 5 . One of the causes why it is so difficult to build a working space lift is because its weight limit is chosen by the maximum static load 6 .If the gravity reaches this limit then the line will sag under gravity 7 . Another difficulty is that the ropes require to support their own weight 8 .Therefore if you want to make your space lift lighter than air 9 , you must use some kind of counterweight 10 .",
        "rewrite_text": "The space elevator represents a significant undertaking in aviation design, having been explored by researchers worldwide for many years. The primary objective of this research was to determine the thermal energy requirements for constructing such a structure using various materials. To achieve this, we employed two approaches: an analytical method based on the principles of elasticity, and a numerical technique utilizing finite element analysis through ANSYS software. Our findings suggest that the optimal structure should feature a high strength-to-weight ratio. We discovered that carbon nanotubes are excellent candidates for this application, as they can achieve exceptionally high strengths while maintaining relatively low densities. This research will facilitate the design of more efficient space elevators in the future. \n\n**Keywords:** Energy consumption, Carbon Nanotube, Optimum Material, Finite Element Analysis Software, Analytical Method, Elasticity Theory, Space Elevator. \n\n**1 Introduction**  \nSpace elevators are regarded as one of the most fascinating projects in aeronautics and astronautics. They have the potential to provide transportation between Earth's orbit and the surface without fuel consumption, making them particularly advantageous for transporting people or cargo. In recent years, various attempts have been made to develop space elevators; however, none have succeeded in becoming fully operational. One of the key challenges in constructing a functional space elevator is adhering to its weight limit, which is constrained by the maximum static load; exceeding this limit can cause the cable to sag under gravity. Additionally, the cables must support their own weight, necessitating a counterweight for design efficiency to ensure the space elevator remains buoyant.",
        "ori-fast-z-score": 0.7986208584745025,
        "water-fast-z-score": 8.716463972623675,
        "rewrite-fast-z-score": 0.3592106040535498
    },
    {
        "original_text": "We present the results of an analysis of galaxy cluster data in terms of their gravitational lensing properties and X-ray emission, with particular emphasis on the comparison between observed and predicted values for the mass-to-light ratio M/L. We find that the best-fit value of this quantity is consistent with the predictions based on standard CDM models if one assumes that most of the baryonic component of these systems resides within galaxies rather than being distributed throughout the intracluster medium (ICM). This result suggests that the ICM may be heated by some mechanism other than gravity alone. \n \n Keywords: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission \n \n \n \n 1 Introduction \n \n The study of galaxy clusters has been instrumental to our understanding of cosmology over the past few decades. In fact, it was through observations of galaxy clusters that we first discovered evidence supporting the existence of non-baryonic dark matter  1  . Today, galaxy clusters are still used extensively to test theories about structure formation  2  , and they provide important constraints on cosmological parameters such as the Hubble constant  3  or the equation-of-state parameter w  4  . \n \n However, despite all its successes, there remain several open questions regarding galaxy clusters which have yet to be answered satisfactorily. For example, while current observational techniques allow us to measure accurately the total amount of light emitted by a galaxy cluster, it remains difficult to determine how much of this light comes from stars inside individual galaxies versus diffuse gas located outside them  5  . Similarly, although we can estimate fairly well the total gravitating mass of a galaxy cluster using various methods  6  , it is not clear what fraction of this mass is associated with visible objects like galaxies  7, 8  . Finally, even though we know that galaxy clusters contain large amounts of hot plasma  9  , it is unclear whether this material is gravitationally bound to the system  10  .\n \nIn order to address these issues, we will use two different datasets obtained from the Chandra Observatory  11  : the sample of galaxy clusters studied by Vikhlinin et",
        "watermark_text": "We present the conclusion of an assessment of galaxy cluster data in terms of their gravitational lensing behavior and X - ray radiation , with particular emphasis on the comparison between seen and anticipated readings for the mass - to - light value M / L . We see that the best - fitting value of this quantity is compatible with the estimates based on normal CDM models if one suppose that most of the baryonic core of these systems resides within stars rather than being dispersed throughout the intracluster medium ( ICM ) .This result suggests that the ICM could be heated by some process other than gravity alone . Keywords : Galaxy cluster , Dark Matter Halo , Gravitational Lensing , Mass - to - Light Ratio , X - Ray Emission 1 Introduction The investigation of galaxy galaxies has been instrumental to our understanding of cosmology over the previous few years .In indeed , it was through observations of galaxy clusters that we first discovered evidence proving the existence of non - baryonic black material 1 . Today , galaxy rings are still used heavily to test models about structure formation 2 , and they give important restrictions on cosmological factors such as the Hubble constant 3 or the equation - of - state variable f 4 .However , despite all its victories , there remain many open questions regarding galaxy clusters which have yet to be answered satisfactorily . For instance , while contemporary observational techniques permit us to measure correctly the total amount of light emitted by a galaxy cluster , it remains impossible to predict how many of this light originates from stars inside individual stars vs diffuse gas located outside them 5 .Similarly , although we can calculate fairly good the total gravitating mass of a galaxy cluster using numerous technique 6 , it is not clear what fraction of this mass is associated with visible objects like stars 7 , 8 . Finally , even though we know that galaxy regions include significant amounts of bright plasma 9 , it is uncertain whether this material is gravitationally bound to the system 10 .In order to meet these problems , we will use two different datasets obtained from the Chandra Observatory 11 : the sample of galaxy regions investigated by Vikhlinin et",
        "rewrite_text": "We present the conclusion of an analysis of galaxy cluster data, focusing on their gravitational lensing properties and X-ray emissions, particularly by comparing observed and expected mass-to-light ratios (M/L). Our findings indicate that the best-fit value for this ratio aligns with estimates derived from standard cold dark matter (CDM) models, assuming that a significant portion of the baryonic matter in these systems is concentrated in stars rather than being spread out in the intracluster medium (ICM). This observation implies that the heating of the ICM may be influenced by processes beyond mere gravitational effects.\n\n**Keywords**: Galaxy cluster, Dark Matter Halo, Gravitational Lensing, Mass-to-Light Ratio, X-Ray Emission  \n\n**1 Introduction**  \nThe study of galaxy clusters has greatly enhanced our understanding of cosmology in recent years. Notably, observations of these clusters were pivotal in providing the first evidence for the existence of non-baryonic dark matter. Currently, galaxy clusters continue to play a crucial role in testing models of structure formation and offer significant constraints on cosmological parameters such as the Hubble constant and the equation-of-state variable. However, despite many breakthroughs, numerous unresolved questions regarding galaxy clusters persist. For example, although modern observational techniques allow us to accurately measure the total light emitted by a galaxy cluster, it remains challenging to determine how much of that light is sourced from individual stars compared to the diffuse gas that surrounds them. Similarly, while we can estimate the total gravitational mass of a galaxy cluster using various methods, it is unclear how much of that mass corresponds to visible objects like stars. Furthermore, while we recognize that galaxy clusters contain substantial amounts of luminous plasma, the extent to which this material is gravitationally bound to the system is still uncertain. To address these challenges, we will utilize two separate datasets obtained from the Chandra Observatory, specifically the sample of galaxy clusters studied by Vikhlinin et al.",
        "ori-fast-z-score": -1.5811388300841895,
        "water-fast-z-score": 8.171011693711216,
        "rewrite-fast-z-score": -1.113780197846022
    },
    {
        "original_text": "We present an all-sky survey for neutral hydrogen (HI) clouds associated with the Large Magellanic cloud (LMC). The LMC is known to have many small, isolated HI clouds that are not gravitationally bound and may be tidally stripped material or remnants of dwarf galaxies destroyed by tidal forces during close encounters between the Milky Way Galaxy and the LMC.  We use data obtained at Arecibo Observatory as part of the ALFALFA survey to search for new HI clouds near the LMC. In addition we combine our results with previous surveys conducted using Parkes telescope observations and single dish telescopes located on Mauna Kea, Hawaii. Our analysis reveals a total of 16 previously uncatalogued HI clouds within 10 degrees of the LMC center. These clouds range in size from 1 kpc to 15 kpc and contain up to 3 x 1012 M⊙ of HI gas.",
        "watermark_text": "We present an all - skies study for neutral hydrogen ( HI ) skies related with the Large Magellanic cloud ( LMC ) . The LMC is known to have many small , isolated HI clouds that are not gravitationally locked and may be tidally stripped matter or remnants of dwarf stars destroyed by tidal forces during close contacts between the Milky Way Galaxy and the LMC .We use data acquired at Arecibo Observatory as part of the ALFALFA survey to search for fresh HI clouds near the LMC . In addition we merge our findings with previous searches undertaken using Parkes telescope measurements and single dish telescopes located on Mauna Kea , Hawaii .Our study reveals a total of 16 formerly uncatalogued HI clouds within 10 degrees of the LMC center . These particles fall in height from 1 kpc to 15 kpc and comprise up to 3 x 1012 [UNK] of HI gas .",
        "rewrite_text": "We conduct a comprehensive survey of neutral hydrogen (HI) in the vicinity of the Large Magellanic Cloud (LMC). The LMC is characterized by numerous small, isolated HI clouds that are not gravitationally bound, potentially resulting from tidal stripping or the remnants of dwarf stars that were disrupted by tidal forces during close encounters with the Milky Way Galaxy. Our research utilizes data from the Arecibo Observatory, part of the ALFALFA survey, to identify new HI clouds near the LMC. Additionally, we integrate our results with prior investigations conducted using measurements from the Parkes Telescope and single-dish telescopes on Mauna Kea, Hawaii. Our findings uncover 16 previously uncharted HI clouds within 10 degrees of the LMC's center, with altitudes ranging from 1 kpc to 15 kpc and containing up to 3 x 10^12 solar masses of HI gas.",
        "ori-fast-z-score": 0.39056673294247163,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": 0.9615239476408232
    },
    {
        "original_text": "We present new measurements of the angular diameter variations for two classical Cepheids: Y Ophiuchi (Y Oph) and Alpha Persei (alpha Per). These stars are among the brightest in their class, making them ideal targets to study using infrared interferometry. We used the FLUOR instrument on the CHARA array at Mount Wilson Observatory to obtain high-precision visibility data over several pulsation cycles. The observations were performed simultaneously in H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns).\nThe results show that both stars have extended atmospheres which vary significantly during the pulsations cycle. In particular we find that:  - For Y Oph, our best-fit model is consistent with an atmosphere extending up to about 1 AU above its photosphere.  - For alpha Per, our best-fit models suggest that the star has an extended atmosphere reaching out to more than 2 AU above its photosphere.",
        "watermark_text": "We present new studies of the angular distance changes for two classical Cepheids : Y Ophiuchi ( Y Oph ) and Alpha Persei ( beta Per ) . These galaxies are among the brightest in their category , making them ideal candidates to study using infrared interferometry .We utilized the FLUOR instrument on the CHARA antenna at Mount Wilson Observatory to obtain high - precision visibility data over several pulsation periods . The surveys were performed simultaneously in H - band ( 1 . 6 microns ) , K - band ( 2 . 0 microns ) , and L - band ( 3 . 8 microns ) .The results show that both stars have extended atmospheres which varies dramatically during the pulsations cycle . In particular we find that : - For Y Oph , our better - fitting model is compatible with an environment stretching up to about 1 AU above its photosphere .- For alpha Per , our better - fitting models suggest that the star has an extended atmosphere extending out to more than 2 AU above its photosphere .",
        "rewrite_text": "We present new research on the changes in angular distance for two classical Cepheids: Y Ophiuchi (Y Oph) and Alpha Persei (β Per). These stars are among the brightest in their class, making them excellent candidates for study through infrared interferometry. We used the FLUOR instrument on the CHARA array at Mount Wilson Observatory to gather high-precision visibility data over multiple pulsation cycles. Observations were conducted simultaneously in the H-band (1.6 microns), K-band (2.0 microns), and L-band (3.8 microns). Our findings indicate that both stars possess extended atmospheres that fluctuate significantly throughout their pulsation cycles. Specifically, we found that for Y Oph, our best-fitting model suggests an atmosphere extending approximately 1 AU above its photosphere. In the case of Alpha Per, our optimal models indicate that the star's extended atmosphere reaches over 2 AU above its photosphere.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.528438872198585,
        "rewrite-fast-z-score": 0.4120816918460671
    },
    {
        "original_text": "We introduce the concept of brane tilings, which are generalizations of tiling groups and can be used to describe many different types of physical systems in terms of their symmetries.  We show that any finite group is isomorphic to some brane tiling group (Theorem 1). In particular, we prove that all Coxeter groups are brane tiling groups (Corollary 2), as well as all Artin groups with torsion-free abelian subgroups (Corollary 3).  As an application, we give a new proof for the classification theorem of finite simple groups by showing how they arise naturally through the action of brane tilings on surfaces (Theorem 4).\nIntroduction\n\nTilings have been studied extensively over the past few decades due to their connections to various branches of mathematics such as number theory, combinatorics, geometry, and physics.   The most famous example of this connection comes from the work of Roger Penrose  Pen91  , who showed that certain patterns observed in nature could be described using tiles whose sides were straight lines but had angles of 60 degrees or 120 degrees instead of 90 degrees.   These so-called  non-standard  tilings are now known as Penrose tilings.    Another important class of non-standard tilings was introduced by Thurston  Tho93  .    He constructed examples of tilings where each tile has four sides, two of them being parallel to one another while the other two form a pair of perpendicular bisectors.   This type of tiling is called a quadrilateral tiling.   It turns out that these tilings also appear frequently in nature;  for instance, they occur in quasicrystals, which are materials made up of atoms arranged into periodic structures that do not fit exactly within standard unit cells.   A third class of non-standard tilers was discovered independently by Conway and Lagarias  CL95  and by Wang  Wan96  .\nIn recent years there has been growing interest in studying mathematical models of quantum gravity, string theory, and M-theory.   One approach towards understanding these theories involves constructing spaces...",
        "watermark_text": "We introduce the notion of brane tilings , which are generalizations of tiling groups and can be used to explain much various types of physical structures in terms of their symmetries . We see that any finite group is isomorphic to some brane tiling group ( Theorem 1 ) .In particular , we prove that all Coxeter bands are brane tiling bands ( Corollary 2 ) , as well as all Artin groups with torsion - stable abelian subgroups ( Corollary 3 ) . As an application , we give a new proof for the classification principle of finite simple groups by showing how they occur readily through the operation of brane tilings on surfaces ( Theorem 4 ) .Introduction Tilings have been studied frequently over the previous few years owing to their connections to several parts of math such as number theory , combinatorics , mathematics , and mechanics . The most famous example of this link coming from the work of Roger Penrose Pen91 , who demonstrated that particular patterns observed in nature might be described using tiles whose sides were straight lines but had angles of 60 degrees or 120 degrees instead of 90 degrees .These so - called non - normal tilings are now termed as Penrose tilings . Another important group of non - standard tilings was introduced by Thurston Tho93 .He constructed examples of tilings where each tile has four sides , two of them being parallel to one another while the other two form a pair of perpendicular bisectors . This kind of tiling is dubbed a quadrilateral tiling .It turns out that these tilings also appear often in nature ; for instance , they occur in quasicrystals , which are materials made up of atoms arranged into periodic arrangements that do not fit precisely within normal unit cells . A third category of non - basic tilers was described independently by Conway and Lagarias CL95 and by Wang Wan96 .In past decades there has been growing interest in investigating mathematical models of quantum gravitational , string theory , and M - theory . One approach towards studying these theories involves generating spaces . . .",
        "rewrite_text": "We present the concept of brane tilings, which extend the idea of tiling groups and can be employed to elucidate various physical structures through their symmetries. Notably, we demonstrate that every finite group is isomorphic to some brane tiling group (Theorem 1). In particular, we establish that all Coxeter bands qualify as brane tiling bands (Corollary 2), as do all Artin groups with torsion-stable abelian subgroups (Corollary 3). As a practical application, we provide a new proof of the classification principle for finite simple groups, illustrating how they manifest through the operation of brane tilings on surfaces (Theorem 4). \n\nIntroduction: Over the past few years, tilings have garnered considerable attention due to their relationships with diverse areas of mathematics, including number theory, combinatorics, and mechanics. A significant example is the work of Roger Penrose (Pen91), who showed that specific patterns found in nature can be represented using tiles with straight sides but angles of 60 degrees or 120 degrees instead of the traditional 90 degrees. These non-standard arrangements are now known as Penrose tilings. Another influential category of non-standard tilings, the quadrilateral tiling, was introduced by Thurston (Tho93), featuring tiles with four sides where two sides are parallel and the other two act as perpendicular bisectors. Such tilings frequently appear in nature, including in quasicrystals—materials composed of atoms arranged in periodic patterns that do not fit perfectly within conventional unit cells. A further category of non-standard tilers was developed independently by Conway and Lagarias (CL95) and by Wang (Wan96). In recent decades, there has been an increasing interest in exploring mathematical models of quantum gravity, string theory, and M-theory, with one approach involving the generation of intricate spaces.",
        "ori-fast-z-score": -0.26211121699831136,
        "water-fast-z-score": 7.7759661042832375,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We propose to cool fermionic atoms in optical lattices by using the pairing mechanism between two different hyperfine states, which is analogous to Cooper pair formation in superconductors. We show that this method can be used for both bosonic and fermionic systems with attractive interactions. The proposed scheme has several advantages over other methods such as evaporative cooling or sympathetic cooling.  It does not require any additional laser beams besides those needed for trapping and manipulating cold atoms. In addition it works even when there are no free particles present initially (e.g., at zero temperature). Finally we discuss how our proposal could be realized experimentally. Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging problems in atomic physics today  1  . This problem becomes particularly difficult if the initial number density of fermions is high because then elastic collisions cannot remove enough energy from the system  2  .\nIn recent years, however, new experimental techniques have been developed  3, 4  , allowing us to trap and manipulate cold atoms on an unprecedented level  5  . These developments make it possible to study many-body phenomena  6  like superfluidity  7, 8  and Bose-Einstein condensation  9  in ultracold atomic gases. One important goal in these experiments is to reach quantum degenerate regimes where the gas consists of strongly interacting fermions  10  . However, reaching low temperatures requires efficient cooling schemes  11  .\nOne promising approach towards achieving this goal is to use the pairing mechanism  12  . Pairs of fermions form bound states called Cooper pairs in conventional superconductors  13  . Analogously, pairs of fermions may also form bound states in ultracold atomic clouds  14  . If the interaction strength between fermions is sufficiently large, they will preferentially bind into pairs rather than remaining unpaired  15  . Therefore, cooling fermions via pairing should work well even",
        "watermark_text": "We suggest to cool fermionic atoms in laser lattices by using the pairing principle between two different hyperfine states , which is analogous to Cooper couple formation in superconductors . We see that this process can be used for both bosonic and fermionic systems with interesting interactions .The proposed system has numerous benefits over other methods such as evaporative cooling or sympathetic heating . It does not require any additional laser electrons besides those required for trapping and manipulating cool ions .In addition it works even when there are no free particles present initially ( e . g . , at zero temperature ) . Finally we talk how our proposal possible be realized experimentally .Cooling fermions down to quantum degeneracy temperatures below 1 microkelvin remains one of the most challenging difficulties in nuclear science today 1 . This problem remains particularly challenging if the first number density of fermions is high because then elastic collisions cannot eliminate much energy from the system 2 .In recent years , however , new experimental methods have been created 3 , 4 , allowing us to trapping and manipulate cold molecules on an remarkable level 5 . These advances give it able to study many - bodies phenomena 6 like superfluidity 7 , 8 and Bose - Einstein condensation 9 in ultracold atomic atoms .One important aim in these experiments is to reach quantum degenerate regimes where the gas consists of highly interacting fermions 10 . However , finding low temperatures involves optimal cooling schemes 11 .One promising alternative towards reaching this goal is to use the pairing principle 12 . Pairs of fermions form bound states called Cooper pairs in standard superconductors 13 .Analogously , pairs of fermions may sometimes form bound states in ultracold nuclear clouds 14 . If the interaction strength between fermions is sufficiently great , they will preferentially bind into couples rather than keeping unpaired 15 .Therefore , cooling fermions via pairing should work better even",
        "rewrite_text": "We propose a method for cooling fermionic atoms in laser lattices by leveraging the pairing principle between two distinct hyperfine states, akin to the formation of Cooper pairs in superconductors. This technique can be effectively applied to both bosonic and fermionic systems exhibiting intriguing interactions. The suggested approach offers numerous advantages over traditional methods, such as evaporative cooling or sympathetic heating, as it eliminates the need for supplementary laser electrons apart from those used for the trapping and manipulation of cooled ions. Moreover, this method is effective even in the absence of free particles at the outset, such as at zero temperature. We also discuss the potential for experimental realization of our proposal.\n\nAchieving quantum degeneracy temperatures below 1 microkelvin for fermions remains one of the most pressing challenges in nuclear science. This task becomes particularly difficult at high initial fermionic number densities, where elastic collisions are insufficient to significantly reduce system energy. However, recent advancements in experimental techniques have enabled remarkable trapping and manipulation of cold molecules. These innovations allow for the exploration of many-body phenomena such as superfluidity and Bose-Einstein condensation in ultracold atomic systems. A critical objective of these experiments is to attain quantum degenerate regimes comprising highly interacting fermions. Reaching these low temperatures necessitates optimal cooling strategies. One promising alternative method for achieving this aim involves utilizing the pairing principle. Just as Cooper pairs form bound states in conventional superconductors, pairs of fermions can also form bound states within ultracold atomic clouds. When the interaction strength among fermions is sufficiently strong, they tend to bind into pairs rather than remain unpaired. Consequently, utilizing pairing as a cooling mechanism should enhance the efficiency of cooling fermions.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.723229626397817,
        "rewrite-fast-z-score": 1.1832159566199232
    },
    {
        "original_text": "We study damage propagation in an interface between two ordered phases of the confined Ising model (CIM) with random fields and quenched disorder, which is known to undergo a localization-delocalization transition as its temperature T crosses Tc = 1. We show that this transition can be observed by measuring the average size of avalanches triggered by local perturbations. The results are compared with those for the unperturbed CIM obtained using Monte Carlo simulations on large lattices. In particular we find that the distribution of avalanche sizes changes drastically across the transition point. This behavior is explained within the framework of the mean-field theory developed recently for the CIM. Finally, we discuss possible experimental realizations of our system. Introduction:-The phenomenon of phase coexistence has been studied extensively both theoretically  1  -  4  and experimentally  5  . It occurs when different thermodynamic states coexist in equilibrium  6  , or metastable states exist simultaneously  7  . A typical example is provided by water  8  where ice Ih and liquid water co-exist below 0 o C  9  .\nIn recent years there have been several studies  10  -  12  devoted to understanding how interfaces separating different phases evolve under external driving forces such as thermal fluctuations  13  , magnetic field  14  , mechanical stress  15  etc.. These investigations were motivated mainly by experiments performed on various materials  16  including ferroelectrics  17  , ferromagnets  18  , superconductors  19  , colloids  20  , granular media  21  , glasses  22  , foams  23  , and biological systems  24  . For instance, it was found  25  that the dynamics of domain walls in magnets  26  depends crucially on whether they are pinned  27  or not  28  . Similarly, the response of glassy  29  and jammed  30  systems to shear stresses  31  strongly depends on their preparation history  32  . On the other hand, the effect of quenched disorder  33  on the properties of interfaces  34  remains poorly understood  35  despite numerous theoretical  36   38  and numerical  39  attempts made over the past few decades.\nRecently, the problem of interface evolution attracted renewed interest due to the discovery of new types of transitions occurring in spatially extended systems  40   41 :",
        "watermark_text": "We test damage propagation in an interface between two organized stages of the confined Ising model ( CIM ) with random fields and quenched instability , which is known to undergo a localization - delocalization transition as its temperature T crosses Tc = 1 . We see that this shift can be described by observing the average size of avalanches caused by regional perturbations .The results are compared with those for the unperturbed CIM achieved using Monte Carlo simulations on huge lattices . In particular we find that the distribution of avalanche sizes changes significantly across the transfer point .This behavior is studied within the framework of the mean - field model established recently for the CIM . Finally , we explain possible experimental realizations of our system .Introduction : - The phenomenon of cycle coexistence has been studied thoroughly both theoretically 1 - 4 and experimentally 5 . It happens when distinct thermodynamic states coexist in equilibrium 6 , or metastable states arise simultaneously 7 .A typical example is provided by water 8 where glacier Ih and fluid water co - operate below 0 o C 9 . In recent years there have been numerous research 10 - 12 devoted to investigating how interfaces separating different components develop under external driving forces such as heat fluctuations 13 , magnetic force 14 , thermal strain 15 etc . .These studies were driven mainly by research performed on various structures 16 notably ferroelectrics 17 , ferromagnets 18 , superconductors 19 , colloids 20 , granular material 21 , glasses 22 , foams 23 , and biological environments 24 . For instance , it was shown 25 that the dynamics of domain walls in magnets 26 differs crucially on whether they are locked 27 or not 28 .Similarly , the response of glassy 29 and jammed 30 systems to shear forces 31 strongly depends on their preparation history 32 . On the other hand , the impact of quenched disorder 33 on the properties of interfaces 34 remains poorly realized 35 despite several experimental 36 38 and numerical 39 tries made over the previous few years .Recently , the issue of interface evolution enjoyed increased interest due to the discovery of new types of transitions happening in spatially extended systems 40 41 :",
        "rewrite_text": "We investigate the propagation of damage in the interface between two organized phases of the confined Ising model (CIM) subject to random fields and quenched instability, which is known to experience a localization-delocalization transition when its temperature T surpasses Tc = 1. Our findings reveal that this transition can be characterized by examining the average size of avalanches resulting from local perturbations. We contrast these results with those derived from Monte Carlo simulations conducted on large lattices of the unperturbed CIM. Notably, we observe a significant alteration in the distribution of avalanche sizes across the transition threshold. This behavior is analyzed within the framework of a recently established mean-field model for the CIM. Additionally, we discuss potential experimental implementations of our system.\n\n**Introduction:** The phenomenon of coexisting cycles has been extensively explored both theoretically and experimentally. This occurs when different thermodynamic states are in equilibrium, or when multiple metastable states emerge simultaneously. A classic example is the coexistence of glacier ice (Ih) and liquid water below 0°C. In recent years, much research has focused on understanding how interfaces separating different components evolve under external influences, such as thermal fluctuations, magnetic fields, or thermal strain. These investigations have primarily been motivated by studies on various materials, including ferroelectrics, ferromagnets, superconductors, colloids, granular materials, glasses, foams, and biological systems. For example, it has been demonstrated that the dynamics of domain walls in magnets varies significantly depending on whether they are locked or free. Likewise, the response of glassy and jammed systems to shear forces is heavily influenced by their preparation history. In contrast, the effects of quenched disorder on interface properties remain inadequately understood, despite several experimental and numerical efforts in recent years. The topic of interface evolution has gained renewed attention lately due to the discovery of new types of transitions occurring in spatially extended systems.",
        "ori-fast-z-score": -0.827605888602368,
        "water-fast-z-score": 8.16496580927726,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We present the results of our analysis on the supersymmetric parameter space, taking into account all available experimental data including those from LHC experiments as well as electroweak precision observables (EWPO). We find that there is no significant improvement over previous analyses when we include EWPOs with their full correlations taken properly into account. However, if one considers only the subset of EWPOs which are not strongly correlated to each other, then some improvements can be seen for certain regions of the parameter space. In particular, this applies to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV. The latter case also leads to an improved agreement between theory predictions and measurements of the anomalous magnetic moment of the muon. Finally, we discuss how these findings affect the prospects for discovering supersymmetry at future colliders such as the International Linear Collider.",
        "watermark_text": "We report the results of our analysis on the supersymmetric parameter room , using into consideration all available observation information including those from LHC experiments as well as electroweak accuracy observables ( EWPO ) . We see that there is no considerable progress over past analyses when we incorporate EWPOs with their full correlations took correctly into consideration .However , if one looks only the subset of EWPOs which are not closely correlated to each other , then some improvements can be shown for particular regions of the parameter space . In particular , this applicable to scenarios where the lightest neutralino has a large Higgsino component or where the gluinos have masses around 1 TeV .The last example also leads to an better agreement between theoretical estimates and measurements of the anomalous magnetic motion of the muon . Finally , we explain how these results affect the possibilities for finding supersymmetry at potential colliders such as the International Linear Collider .",
        "rewrite_text": "We present the findings of our analysis on the supersymmetric parameter space, taking into account all available observational data, including results from LHC experiments and electroweak precision observables (EWPOs). Our evaluation shows that there has not been significant advancement over previous analyses when we consider EWPOs along with their complete correlations. However, if we focus solely on a subset of EWPOs that are relatively uncorrelated, we can observe some improvements in specific regions of the parameter space. This is especially relevant for scenarios where the lightest neutralino exhibits a substantial Higgsino component or where gluinos have masses around 1 TeV. In the latter case, we also achieve a better alignment between theoretical predictions and the measured anomalous magnetic moment of the muon. Lastly, we discuss the implications of these results for the search for supersymmetry at potential colliders, such as the International Linear Collider.",
        "ori-fast-z-score": -3.6765801200722312,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": -1.0886621079036347
    },
    {
        "original_text": "We study the nonstationarity and synchronization properties of an ensemble of coupled oscillators with time-varying coupling strengths, which are modeled by Kuramoto-like phase equations on directed networks. We show that there exists a critical value for the average degree above which all nodes synchronize to each other if they have identical natural frequencies. However, we find that even when the system is synchronized at some initial moment, it may become desynchronized after a finite amount of time due to the change of the underlying topology. In particular, we prove analytically that this phenomenon occurs only in systems whose coupling strength distribution has infinite variance. Finally, numerical simulations confirm our theoretical results. The work was supported by NSF under Grant No. DMS-0805040. PACS numbers: 05.45.Mt, 02.10.Yn, 87.19 .Hc, 89.70.+c",
        "watermark_text": "We explore the nonstationarity and synchronization properties of an ensemble of coupled oscillators with time - differing correlation strengths , which are modeled by Kuramoto - like phase equations on directed networks . We see that there exists a critical value for the average degree above which all nodes synchronize to each other if they have equal natural bandwidth .However , we find that even when the scheme is synchronized at some initial moment , it could become desynchronized after a finite quantity of time due to the shift of the underlying topology . In particular , we prove analytically that this phenomenon occurs only in systems whose coupling strength distribution has endless variance .Finally , numerical simulations confirm our theoretical results . The project was supported by NSF under Grant No .DMS - 0805040 . PACS codes : 05 . 45 . Mt , 02 . 10 . Yn , 87 . 19 . Hc , 89 . 70 . + c",
        "rewrite_text": "We investigate the nonstationarity and synchronization characteristics of an ensemble of coupled oscillators that exhibit time-varying correlation strengths, modeled using Kuramoto-like phase equations on directed networks. Our findings reveal that there is a critical average degree at which all nodes can achieve synchronization, provided they share the same natural bandwidth. However, we also discover that a system that is synchronized at a given moment can become desynchronized after a finite period, influenced by changes in the underlying topology. Specifically, we analytically demonstrate that this desynchronization occurs only in systems with a coupling strength distribution that has infinite variance. Additionally, numerical simulations corroborate our theoretical predictions. This research was supported by the NSF under Grant No. DMS-0805040. PACS codes: 05.45.Mt, 02.10.Yn, 87.19.Hc, 89.70.+c.",
        "ori-fast-z-score": 0.816496580927726,
        "water-fast-z-score": 4.27617987059879,
        "rewrite-fast-z-score": 1.3608276348795434
    },
    {
        "original_text": "The cosmic code comparison project is an effort to compare the results produced by different cosmological codes, and thereby test their accuracy.  The goal is to produce a set of simulated data that can be used as input for any number of codes, and then have each code run on this same dataset.   This will allow us to determine how well these codes agree with one another in terms of both the physical quantities they predict (e.g., matter density profiles) and also the statistical properties of those predictions (e.g., power spectra).   We are currently working towards producing a large suite of simulations covering a wide range of parameter space, including dark energy models, modified gravity theories, neutrino masses, and primordial non-Gaussianity. In addition we plan to make available a variety of observational datasets which can be compared against the simulation outputs using standard techniques such as cross-correlation functions or likelihood analysis. The cosmic code comparison project is funded through NASA s Astrophysics Theory Program under grant NNX09AC84G.",
        "watermark_text": "The universe coding comparison program is an initiative to compare the results produced by various cosmological rules , and consequently test their authenticity . The goal is to produce a list of simulated evidence that can be used as input for any number of codes , and then have each code run on this same dataset .This will provide us to estimate how best these codes comply with one another in terms of both the physical quantities they predict ( e . g . , matter density levels ) and also the statistical characteristics of those predictions ( e . g . , power spectra ) . We are currently working towards creating a large suite of simulations covering a broad variety of parameter room , notably dark energy theories , modified gravity theories , neutrino masses , and primordial non - Gaussianity .In addition we plan to make accessible a variety of observational datasets which can be analyzed against the model outputs using conventional methods such as cross - correlation functions or likelihood analysis . The universe code comparison program is funded through NASA s Astrophysics Theory Program under grant NNX09AC84G .",
        "rewrite_text": "The Universe Coding Comparison Program is an initiative aimed at evaluating the outputs generated by different cosmological models to verify their validity. The objective is to create a comprehensive list of simulated evidence that can serve as input for various codes, enabling each code to operate on the same dataset. This will allow us to assess how well these codes align in terms of the physical parameters they predict, such as matter density levels, as well as the statistical features of those predictions, like power spectra. Currently, we are developing an extensive suite of simulations that span a wide range of parameter space, particularly focusing on dark energy theories, modified gravity models, neutrino masses, and primordial non-Gaussianity. Additionally, we intend to provide access to several observational datasets, which can be analyzed alongside the model outputs using standard techniques such as cross-correlation functions or likelihood analysis. The program is supported by NASA’s Astrophysics Theory Program under grant NNX09AC84G.",
        "ori-fast-z-score": -0.6469966392206304,
        "water-fast-z-score": 5.673665146135802,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "The Axion Dark Matter Search (ADMX) experiment is designed to search for dark matter in the form of axions, which are hypothetical particles predicted by theories beyond the Standard Model.  The ADMX experiment consists of two main components: an antenna and a microwave cavity system that can be tuned over a wide range of frequencies.   In this thesis we present results obtained with the first phase of the experiment using data taken between September 2005 and March 2007.    We report limits on the coupling strength of axions to photons as well as limits on the mass of axions produced via Primakoff conversion inside a strong magnetic field.  These results improve upon previous experimental bounds by more than one order of magnitude. This work was performed under the auspices of the U.S Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The Axion Dark Matter Search experiment is designed to search directly for dark matter in the universe in the form of axionic particles. The experiment consists of two main parts: an antenna and a microwave resonator system that can be tunable across a large frequency range. In this dissertation I will discuss our recent results from the first stage of the experiment.",
        "watermark_text": "The Axion Dark Matter Search ( ADMX ) experiment is designed to search for black material in the form of axions , which are hypothetical particles expected by theories beyond the Standard Model . The ADMX experiment consists of two principal portions : an antenna and a microwave cavity network that can be tuned over a broad variety of frequencies .In this dissertation we present results derived with the first phase of the program including data taken between September 2005 and March 2007 . We report limits on the interaction strength of axions to photons as well as limits on the mass of axions produced via Primakoff transformation inside a powerful magnetic force .These data improve upon former empirical bounds by more than one order of magnitude . This research was done under the auspices of the U . S Department of Energy by Lawrence Livermore National Laboratory under Contract DE - AC52 - 07NA27344 .The Axion Dark Matter Search experiment is designed to search specifically for black material in the universe in the form of axionic particles . The project consists of two principal parts : an antenna and a microwave resonator system that can be tunable across a large frequency spectrum .In this dissertation I will explore our latest findings from the first phase of the project .",
        "rewrite_text": "The Axion Dark Matter Search (ADMX) experiment aims to detect dark matter in the form of axions, which are hypothetical particles anticipated by theories extending beyond the Standard Model. The ADMX setup comprises two main components: an antenna and a microwave cavity network that can be adjusted across a wide range of frequencies. In this dissertation, we present findings from the first phase of the program, which includes data collected between September 2005 and March 2007. We detail the constraints on the interaction strength of axions with photons, as well as limits on the mass of axions produced through the Primakoff effect within a strong magnetic field. These results enhance previous empirical limits by over an order of magnitude. This research was conducted under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. In this dissertation, I will delve into our recent discoveries from the initial phase of the project.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 6.413001035278336,
        "rewrite-fast-z-score": -0.39056673294247163
    },
    {
        "original_text": "We report the detection of luminous (>10^7 Lsun) HCN(1-0), HNC(1-0), and CH3CN(5-4)\nline emission toward the nuclear region of the nearby Seyfert 2 galaxy NGC 4418\n(D = 20 Mpc). The observed luminosities are comparable to those found for ultraluminous infrared galaxies, but the ratios between these lines suggest that this is not an active galactic nucleus (AGN) dominated source. Instead we propose that the high-excitation molecular gas may be associated with a young massive starburst which has recently been triggered by tidal interactions and/or mergers. We also find evidence for a compact radio continuum source at the center of the galaxy, consistent with previous observations. This suggests that there could be a deeply-embedded AGN present within the central few hundred parsecs.  These results provide new insights into how such powerful sources can form during the early stages of galaxy evolution.",
        "watermark_text": "We report the observation of luminous ( > 10 ^ 7 Lsun ) HCN ( 1 - 0 ) , HNC ( 1 - 0 ) , and CH3CN ( 5 - 4 ) line emission toward the nuclear zone of the nearby Seyfert 2 galaxy NGC 4418 ( D = 20 Mpc ) . The observed luminosities are comparable to those observed for ultraluminous infrared galaxies , but the ratios between these lines indicate that this is not an active galactic nucleus ( AGN ) centered source .Instead we propose that the high - excitation molecular gas may be involved with a young massive starburst which has recently been caused by tidal interactions and / or mergers . We additionally find proof for a compact radio continuum source at the center of the universe , compatible with previous findings .This implies that there might be a highly - anchored AGN present within the central few hundred parsecs . These data provide fresh insights into how such powerful sources can form during the early stages of galaxy evolution .",
        "rewrite_text": "We report the detection of luminous HCN (1-0), HNC (1-0), and CH3CN (5-4) line emissions, exceeding 10^7 Lsun, in the nuclear region of the nearby Seyfert 2 galaxy NGC 4418 (D = 20 Mpc). The luminosities observed are comparable to those found in ultraluminous infrared galaxies; however, the line ratios suggest that this emission does not originate from an active galactic nucleus (AGN). Instead, we propose that the high-excitation molecular gas is likely linked to a young, massive starburst triggered by tidal interactions and/or mergers. Furthermore, we find evidence of a compact radio continuum source at the galaxy's center, consistent with previous studies. This suggests the possible presence of a well-connected AGN within the central few hundred parsecs. These findings offer new insights into the formation of such powerful sources during the early phases of galaxy evolution.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "We study gravitational wave signals from chaotic systems by using the point mass model with an accretion disk around it, which is one of the most promising candidates for gravitational wave sources in astrophysics.  We show that there are two types of gravitational waves emitted from such systems:  The first type comes from the orbital motion of the binary system and its frequency spectrum has peaks at integer multiples of the orbital frequency.  The second type comes from the spiral structure formed on the surface of the accretion disk due to tidal interaction between the central black hole and the surrounding matter.  Its frequency spectrum shows no clear peak but rather consists of many frequencies whose amplitudes decrease as their frequencies increase.  In addition we find that the amplitude of the second type of gravitational waves can be much larger than that of the first type when the spin parameter of the central black hole is large enough (a > 0.9).  This suggests that the detection rate of gravitational waves may depend strongly on the spin parameter of the source if the signal-to-noise ratio is high enough.",
        "watermark_text": "We work gravitational wave signals from chaotic systems by using the point mass model with an accretion disk around it , which is one of the most attractive candidates for gravitational wave sources in astrophysics . We see that there are two forms of gravitational waves emitted from such systems : The first sort comes from the orbital movement of the binary system and its frequency spectrum has peaks at integer multiples of the orbital frequency .The second kind comes from the spiral shape formed on the surface of the accretion ring due to tidal interaction between the central black hole and the nearby matter . Its frequency spectrum displays no clear peak but rather consists of several spectrum whose amplitudes decrease as their frequencies increase .In addition we find that the frequency of the second kind of gravitational waves can be much larger than that of the first type when the spin vector of the central black hole is huge enough ( a > 0 . 9 ) . This implies that the detection rate of gravitational waves may depend greatly on the spin vector of the source if the signal - to - noise proportion is high enough .",
        "rewrite_text": "We analyze gravitational wave signals from chaotic systems by employing a point mass model with an accretion disk surrounding it, which is a leading candidate for gravitational wave sources in astrophysics. Our findings indicate the presence of two types of gravitational waves emitted by these systems. The first type arises from the orbital motion of the binary system, characterized by a frequency spectrum with peaks at integer multiples of the orbital frequency. The second type results from the spiral structure formed on the surface of the accretion disk due to tidal interactions between the central black hole and surrounding matter. This frequency spectrum is not marked by distinct peaks; instead, it consists of several spectra whose amplitudes diminish with increasing frequency. Moreover, we discover that the frequency of the second type of gravitational waves can significantly exceed that of the first type when the central black hole's spin vector is sufficiently high (a > 0.9). This suggests that the detection rate of gravitational waves may be heavily influenced by the spin vector of the source, provided the signal-to-noise ratio is sufficiently favorable.",
        "ori-fast-z-score": -0.10153461651336192,
        "water-fast-z-score": 4.365988510074562,
        "rewrite-fast-z-score": 0.8
    },
    {
        "original_text": "We present an analysis of the evolution of early-type galaxies (ETGs) in hydrodynamical cosmological simulations, focusing on their formation history as well as their internal structure at z = 0. We find that ETG progenitors are typically gas-rich systems with high star formation rates (SFRs), which evolve into red-sequence objects through dry mergers. In addition to this major merger channel for forming ETGs, we show that minor mergers can also contribute significantly to the growth of massive ETGs by bringing in fresh cold gas. Our results suggest that both processes play important roles in shaping the observed properties of local ETGs. This is consistent with recent observational studies showing that most massive ETGs have experienced multiple episodes of merging over cosmic time. \n \n Keywords: galaxy evolution, galaxy morphology, galaxy scaling relations, galaxy clusters, semi-analytic models, hydrodynamics, dark matter",
        "watermark_text": "We present an assessment of the evolution of early - class stars ( ETGs ) in hydrodynamical cosmological simulations , concentrating on their structure history as also as their internal structure at z = 0 . We see that ETG progenitors are typically gas - rich complexes with high star formation rates ( SFRs ) , which evolution into dark - sequence bodies through dry mergers .In addition to this main merger channel for forming ETGs , we indicate that minor mergers can also contribute greatly to the development of large ETGs by bringing in fresh cold energy . Our results show that both processes hold important roles in shaping the known characteristics of local ETGs .This is consistent with recent observational research indicating that most large ETGs have experienced several bouts of merging over cosmic time . Keywords : universe formation , galaxy shape , galaxy scaling relations , galaxy regions , semi - analytic models , hydrodynamics , darkness matter",
        "rewrite_text": "We provide an analysis of the evolution of early-type galaxies (ETGs) using hydrodynamical cosmological simulations, focusing on their structural history and internal composition at redshift z = 0. Our findings indicate that the progenitors of ETGs are generally gas-rich systems with elevated star formation rates (SFRs), which evolve into quiescent galaxies through dry mergers. In addition to this primary merger mechanism for ETG formation, we highlight that minor mergers can significantly enhance the growth of large ETGs by introducing new cold gas. Our results suggest that both processes play crucial roles in shaping the characteristics of present-day ETGs, aligning with recent observational studies that reveal most large ETGs have undergone multiple merger events over cosmic time. \n\nKeywords: universe formation, galaxy structure, galaxy scaling relations, galaxy regions, semi-analytic models, hydrodynamics, dark matter.",
        "ori-fast-z-score": -1.7556172079419585,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 0.5933908290969266
    },
    {
        "original_text": "We present an effective method to conserve the total energy and linear momentum in molecular dynamics (MD) simulations by introducing two types of potentials: one is used during the MD run, while another is only used when calculating the forces on particles at each time step. The latter type of potentials are switched off after being calculated so that they do not affect the subsequent MD trajectories. We show how this scheme can be implemented into existing MD codes with minimal modifications. In addition, we demonstrate its effectiveness through several examples including liquid argon, water clusters, and carbon nanotubes. Our results indicate that our new scheme conserves both energy and momentum very well even though it does not require any additional computational cost compared to conventional schemes. This work was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011 . Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics",
        "watermark_text": "We present an efficient method to conserve the total energy and linear momentum in polymer mechanics ( MD ) simulations by using two forms of potentials : one is utilized during the MD run , while another is only employed when calculating the forces on particles at each time step . The latter type of potentials are switched off after being calculated so that they do not alter the subsequent MD trajectories .We see how this scheme can be applied into older MD codes with minimal modifications . In addition , we test its effectiveness through several examples representing liquid argon , water complexes , and carbon nanotubes .Our results show that our new project conserves both heat and momentum very best even though it does not require any additional mathematical price compared to conventional proposals . This research was supported by the National Natural Science Foundation of China under Grants No .10874145 and No.10934011 .Keywords : Energy - momentum conservation ; Switching potentials ; Molecular dynamics",
        "rewrite_text": "We propose an efficient approach for conserving total energy and linear momentum in polymer mechanics (MD) simulations by employing two types of potentials: one used during the MD simulation and another utilized solely for calculating forces on particles at each time step. The second type of potential is deactivated after the calculations to prevent it from influencing subsequent MD trajectories. We demonstrate how this method can be integrated into existing MD codes with minimal alterations. Furthermore, we evaluate its effectiveness through various examples involving liquid argon, water complexes, and carbon nanotubes. Our findings indicate that this new approach effectively conserves both energy and momentum without incurring additional computational costs compared to traditional methods. This research was supported by the National Natural Science Foundation of China under Grants No. 10874145 and No. 10934011. Keywords: Energy-momentum conservation; Switching potentials; Molecular dynamics.",
        "ori-fast-z-score": -1.1523319193960637,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": -0.3841106397986879
    },
    {
        "original_text": "We report the Langmuir Blodgett (LB) deposition of highly ordered, dense arrays of vertically-aligned single-walled carbon nanotube films on solid substrates using an aqueous dispersion containing surfactant and sodium dodecyl sulfate as dispersing agents. The LB technique is used to transfer these films onto various substrate materials such as silicon wafers, quartz slides, glass coverslips, gold-coated glass coverslips, and indium tin oxide coated glass coverslips. We have also demonstrated that this method can be extended for patterned growth by transferring the film selectively over areas defined by photoresist patterns. These results are important in developing new applications based on carbon nanotubes. \n \n Carbon nanotubes (CNTs), which were discovered about ten years ago, have attracted considerable attention because they possess unique physical properties including high electrical conductivity, mechanical strength, thermal stability, chemical inertness, etc., making them promising candidates for many potential applications ranging from field emission devices to sensors and optoelectronic devices1-5. However, most of their practical uses require CNT networks with controlled orientation and density6-8. In recent years, several methods have been developed to prepare oriented CNT films9-12. Among those techniques, Langmuir-Blodgett (LB)\ndeposition has emerged as one of the most powerful approaches13-15. This process involves spreading a monolayer of amphiphilic molecules at the air-water interface followed by vertical dipping of a hydrophobic substrate into the water subphase16-18. By repeating the above steps, multilayered thin films consisting of closely packed CNTs can be obtained19-21. Compared to other methods22-24, LB deposition offers advantages such as precise control of layer thickness25-27, easy fabrication of large-area uniform films28-30, and possibility of fabricating patterned structures31-33.",
        "watermark_text": "We report the Langmuir Blodgett ( LB ) precipitation of highly ordered , dense arrays of vertically - stacked single - walled carbon nanotube bands on solid substrates using an aqueous dispersion containing surfactant and potassium dodecyl sulfate as dispersing agents . The LB technique is utilized to move these films atop numerous substrate media such as silicon wafers , quartz slides , glass coverslips , platinum - glazed ceramic coverslips , and indium tin oxide coated glass coverslips .We have also demonstrated that this process can be improved for patterned expansion by directing the film selectively over areas defined by photoresist patterns . These results are important in building new applications based on carbon nanotubes .Carbon nanotubes ( CNTs ) , which were discovered about ten years previously , have garnered considerable scrutiny because they possess unique physical properties including high thermal conductivity , thermal strength , thermal stability , chemical inertness , etc . , making them promising candidates for numerous likely use ranging from field emission sensors to sensors and optoelectronic devices1 - 5 . However , most of their practical applications need CNT connections with restricted orientation and density6 - 8 .In recent years , various methods have been used to produce focused CNT films9 - 12 . Among those techniques , Langmuir - Blodgett ( LB ) exposure has emerged as one of the most popular approaches13 - 15 .This process involves spreading a monolayer of amphiphilic molecules at the air - water interface followed by vertical dipping of a hydrophobic substrate into the water subphase16 - 18 . By repeating the above steps , multilayered narrow films consisting of closely packed CNTs can be obtained19 - 21 .Compared to other methods22 - 24 , LB deposition gives benefits such as careful management of layer thickness25 - 27 , easy fabrication of large - area uniform films28 - 30 , and possibility of fabricating patterned structures31 - 33 .",
        "rewrite_text": "We present the Langmuir Blodgett (LB) technique for the precipitation of highly ordered, dense arrays of vertically-stacked single-walled carbon nanotube bands on solid substrates using an aqueous dispersion that includes surfactant and potassium dodecyl sulfate as dispersing agents. This LB method enables the transfer of these films onto a variety of substrates, including silicon wafers, quartz slides, glass coverslips, platinum-coated ceramic coverslips, and indium tin oxide-coated glass coverslips. Additionally, we have shown that this process can be optimized for patterned expansion by selectively directing the film over areas defined by photoresist patterns. These findings are significant for advancing new applications that utilize carbon nanotubes. First discovered about a decade ago, carbon nanotubes (CNTs) have attracted substantial interest due to their exceptional physical properties, including high thermal conductivity, strength, stability, and chemical inertness, making them promising candidates for a wide range of applications, from field emission sensors to optoelectronic devices. However, most practical applications require CNTs to be arranged with specific orientations and densities. In recent years, various methods have been developed to produce well-defined CNT films. Among these, the Langmuir-Blodgett (LB) technique has gained popularity. This method involves spreading a monolayer of amphiphilic molecules at the air-water interface, followed by the vertical dipping of a hydrophobic substrate into the water subphase. By repeating this process, multilayered narrow films of closely packed CNTs can be achieved. Compared to other techniques, LB deposition offers advantages such as precise control over layer thickness, ease of fabricating large-area uniform films, and the capability to create patterned structures.",
        "ori-fast-z-score": 1.660037707655972,
        "water-fast-z-score": 8.30018853827986,
        "rewrite-fast-z-score": 2.455762454059682
    },
    {
        "original_text": "We report on spectropolarimetric observations of the M8+M9 binary star LHS 1070A,B (GJ 436) with ESPaDOnS at CFHT. The two stars are separated by only 0. ′′ 1 and have been known to be magnetically active for many years. We find that both stars show significant circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields. In addition we detect Stokes V signatures indicating net linear polarization across all observed spectral lines. This is likely caused by scattering processes within the stellar atmosphere. Using our new data set together with previously published photometric measurements we derive rotation periods of P A = 3.6 ± 0.1 days and P B = 4.2 ± 0.3 days for the primary and secondary component respectively. These values are significantly longer than those derived from previous studies which were based solely on photometry. Our results suggest that the rotation period of each individual component depends strongly on its effective temperature as well as its surface gravity.",
        "watermark_text": "We report on spectropolarimetric studies of the M8 + M9 binary star LHS 1070A , B ( GJ 436 ) with ESPaDOnS at CFHT . The two stars are apart by only 0 .′ ′ 1 and have been known to be magnetically active for thousands decades . We see that both stars show considerable circularly polarized emission lines indicative of Zeeman splitting due to their magnetic fields .In addition we perceive Stokes V signatures suggesting net linear polarization across all observed spectral lines . This is probably due by scattering mechanisms within the stars air .Using our new data set combined with previously reported photometric surveys we derive rotation periods of P A = 3 . 6 ± 0 . 1 months and P B = 4 . 2 ± 0 . 3 days for the primary and secondary component respectively . These measurements are greatly longer than those generated from previous analyses which were based primarily on photometry .Our results show that the rotation cycle of each individual component relies highly on its effective heat as well as its surface gravity .",
        "rewrite_text": "We present spectropolarimetric observations of the M8 + M9 binary star system LHS 1070A, B (GJ 436) using ESPaDOnS at CFHT. The two stars are separated by just 0.1 arcseconds and have been recognized as magnetically active for several decades. Our findings reveal that both stars exhibit significant circularly polarized emission lines, indicating Zeeman splitting caused by their magnetic fields. Additionally, we observe Stokes V signatures that suggest net linear polarization across all spectral lines examined, likely resulting from scattering mechanisms within the stars’ atmospheres. By combining our new dataset with previously published photometric surveys, we determine the rotation periods to be P_A = 3.6 ± 0.1 months for the primary component and P_B = 4.2 ± 0.3 days for the secondary component. These rotation periods are notably longer than those derived from earlier analyses that primarily relied on photometry. Our results indicate that the rotation cycles of each star are closely linked to their effective temperature and surface gravity.",
        "ori-fast-z-score": -1.3949716649258315,
        "water-fast-z-score": 4.216541329006604,
        "rewrite-fast-z-score": -1.0392304845413263
    },
    {
        "original_text": "We report on infrared spectroscopic observations with the Infrared Spectrograph (IRS) aboard the Spitzer Space Telescope toward the nearby late-type galaxy NGC 3621, which is known to harbor a supermassive black hole at its center. The IRS spectrum shows prominent emission lines such as  Ne II 12.81 and  S III 18.71 µm that are commonly seen in active galactic nuclei (AGNs). We find that these emission lines can be reproduced by photoionization models using AGN-like ionizing radiation fields. From the observed line ratios we estimate the electron density n e = 10 3 cm −3 , temperature T e = 1000 K, and ionization parameter U H = 1 × 10 −2 . These results suggest that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies. This work was supported by NASA through grant number GO-08460.01-A awarded by the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA.",
        "watermark_text": "We report on infrared spectroscopic observations with the Infrared Spectrograph ( IRS ) aboard the Spitzer Space Telescope toward the nearby mid - class galaxy NGC 3621 , which is known to harbor a supermassive black hole at its core . The IRS spectrum displays large emitted lines such as Ne II 12 . 81 and S III 18 . 71 µm that are often observed in active galactic nuclei ( AGNs ) .We see that these radiation patterns can be reproduced by photoionization models using AGN - like ionizing radiation fields . From the known line ratios we estimate the electron concentration h e = 10 3 cm −3 , temperature T e = 1000 K , and ionization variable U H = 1 × 10 −2 .These results show that the central region of NGC 3621 has properties similar to those found for Seyfert galaxies . This work was supported by NASA through grant number GO - 08460 . 01 - A awarded by the Jet Propulsion Laboratory , California Institute of Technology under contract with NASA .",
        "rewrite_text": "We present infrared spectroscopic observations obtained using the Infrared Spectrograph (IRS) on the Spitzer Space Telescope, focused on the nearby mid-class galaxy NGC 3621, which is known to host a supermassive black hole at its center. The IRS spectrum reveals prominent emission lines, including Ne II at 12.81 µm and S III at 18.71 µm, commonly associated with active galactic nuclei (AGNs). Our analysis indicates that these emitted radiation patterns can be modeled using photoionization approaches that simulate AGN-like ionizing radiation fields. By examining the known line ratios, we estimate the electron density to be \\( h_e = 10^3 \\, \\text{cm}^{-3} \\), the electron temperature to be \\( T_e = 1000 \\, \\text{K} \\), and the ionization parameter \\( U_H = 1 \\times 10^{-2} \\). These findings suggest that the central region of NGC 3621 has characteristics akin to those of Seyfert galaxies. This research was funded by NASA under grant number GO-08460.01-A, which was awarded by the Jet Propulsion Laboratory, California Institute of Technology, under a contract with NASA.",
        "ori-fast-z-score": 0.2672612419124244,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We present an explicit criterion to determine whether or not two given multipartite quantum states are separable, i.e., can be written as convex combinations of product states. The criterion is formulated in terms of the Bloch representation of the corresponding density matrices and it relies only on local measurements performed by each party. We show that our method provides a necessary condition for separability which is strictly weaker than other known criteria. Finally we illustrate its usefulness with some examples. Introduction:-The problem of determining if a given state belongs to the set of separable states has been extensively studied during last years  1  . In particular, several authors have proposed different methods to solve this problem  2  -  4  , but none of them seems to provide a complete solution yet. Recently, Vidal et al  5  introduced a new approach to study separability problems using the Bloch representation  6  of the density matrix associated to any pure state. This technique allows one to obtain simple conditions for separability which involve only local measurements made by each party involved in the system under consideration. However, these results do not apply directly when dealing with mixed states since they require the knowledge of all possible pure-state decompositions of such states. Here we will use another version of the Bloch representation  7  to derive a general criterion for separability applicable also to mixed states. Our main result consists of showing that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state. As a consequence, we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than previous ones  8  .\nPreliminaries:-In what follows we consider N-partite systems described by Hilbert spaces H 1 ,H 2 ...H N . A generic element |ψ⟩ ∈ H = ∑ N i=1 H i is called a pure state vector while ρ ∈ D(H) denotes a density operator acting on H. Any density operator can always be expressed in terms of its spectral decomposition  9  \nwhere {|λ⟩} λ=1,...,d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 (p",
        "watermark_text": "We present an explicit criterion to measure whether or not two given multipartite quantum states are separable , i . e . , can be written as convex combinations of product states . The requirement is formulated in terms of the Bloch representation of the resulting density matrices and it rely only on local observations performed by each party .We see that our system provides a necessary condition for separability which is strictly weaker than other established parameters . Finally we explain its usefulness with some examples .Introduction : - The question of determining if a given state belongs to the group of separable states has been heavily examined during last years 1 . In particular , various papers have proposed different methods to treat this question 2 - 4 , but none of them appears to provide a complete solved yet .Recently , Vidal et al 5 invented a new approach to study separability phenomena using the Bloch representation 6 of the density graph identified to any pure state . This method enables one to obtain simple conditions for separability which require only local observations made by each party involved in the scheme under consideration .However , these results do not apply directly when dealing with mixed states since they demand the knowledge of all possible pure - state decompositions of such states . Here we will use another version of the Bloch representation 7 to derive a general criterion for separability applied also to mixing states .Our main consequence consists of finding that there exists at least one decomposition into pure states compatible with the Bloch representation of every separable state . As a consequence , we prove that the criterion presented here constitutes a necessary condition for separabilty which is strictly weaker than prior ones 8 .Preliminaries : - In what follows we define N - partite structures described by Hilbert spaces H 1 , H 2 . . . H N . A generic element | ψ ⟩ ∈ H = [UNK] N i = 1 H i is called a pure state vector while ρ ∈ D ( H ) denotes a density operator acting on H . Any density operator can always be stated in terms of its spectral transformation 9 where { | λ ⟩ } λ = 1 , . . . , d is an orthonormal basis of eigenvectors of ρ and p λ ≥ 0 ( p",
        "rewrite_text": "We introduce a clear criterion for assessing whether two multipartite quantum states are separable, meaning they can be expressed as convex combinations of product states. This criterion is formulated using the Bloch representation of the resulting density matrices and relies solely on local observations made by each participant. Our approach offers a necessary condition for separability that is notably weaker than previously established parameters. We also illustrate its practical utility through various examples. \n\n**Introduction:** The issue of determining whether a given state is part of the set of separable states has been a focal point of research in recent years. Numerous studies have proposed different methodologies to tackle this problem; however, a definitive solution remains elusive. Recently, Vidal et al. introduced a novel approach to exploring separability phenomena by employing the Bloch representation of density graphs associated with pure states. This method allows for the establishment of straightforward conditions for separability based solely on local observations from each party involved. Unfortunately, these findings do not extend directly to mixed states, as they require knowledge of all potential pure-state decompositions of such states. In this paper, we utilize an alternative version of the Bloch representation to derive a general criterion for separability that is applicable to mixed states. A key conclusion is that at least one pure-state decomposition compatible with the Bloch representation exists for every separable state. Consequently, we demonstrate that the criterion we present serves as a necessary condition for separability, which is less stringent than previously recognized conditions.\n\n**Preliminaries:** In the following sections, we define N-partite structures represented by Hilbert spaces \\(H_1, H_2, \\ldots, H_N\\). A generic element \\(| \\psi \\rangle \\in H = \\bigotimes_{i=1}^N H_i\\) is referred to as a pure state vector, while \\( \\rho \\in D(H) \\) denotes a density operator acting on \\(H\\). Every density operator can be expressed through its spectral decomposition, with \\{ | λ \\rangle \\}_{λ=1, \\ldots, d} representing an orthonormal basis of eigenvectors of \\( \\rho \\) and \\( p_λ \\geq 0 \\).",
        "ori-fast-z-score": 1.5713484026367723,
        "water-fast-z-score": 7.496340570653091,
        "rewrite-fast-z-score": 1.5261167249147478
    },
    {
        "original_text": "Dans cet article, nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux. Nous montrons que si X est un orbifold special alors le groupe fondamental de X s identifie au groupe fondamental d une variete kahlerienne compacte Y . _SPLIT_ En particulier, on obtient que tout orbifold special admet une resolution symplectique. On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage aux groupes fondamentaux une application holomorphe sur leurs resolutions respectives. Enfin, on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles. Keywords: Orbifoldes spéciaux, Variétés Kähleriennes Compactes, Groupes fondamentaux, Résolutions symplectiques. 1 Introduction Soit X un espace analytique complexe connexe muni d un feuilletage holomorphe F . Un tel objet sera appelé dans ce texte un orbifold spécial (voir  V ). Dans cette note, nous allons donner une nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0.1. (Verbitsky) Tout orbifold spécial admet une résolution symplectique.",
        "watermark_text": "Dans cet article , nous donnons une nouvelle preuve de la classification bimeromorphee des varietes kahleriennes compactes en utilisant les orbifolds speciaux . Nous montrons que si X est un orbifold special alors la groupe fondamental de X s identifie au groupe fondamental d la variete kahlerienne compacte Y ._ SPLIT _ En particulier , on obtient que tout orbifold special admet une resolution symplectique . On montre aussi que toute application holomorphe entre deux orbifolds speciaux induit par passage des groupes fondamentaux la applied holomorphe sur leurs resolutions respectives .Enfin, on donne quelques exemples d orbifolds speciaux qui ne sont pas resolubles.Keywords: Orbifoldes spéciaux, Variétés Kähleriennes Compactes, Groupes fondamentaux, Résolutions symplectiques.1 Introduction Soit X un espace analytique complexe connexe muni d en feuilletage holomorphe F . Un tel objet sera appelé dans ce texte un orbifold spécial ( voir V ) .Dans cette report , nous allons donner à nouvelle démonstration du théorème suivant dû à Verbitsky : Théorème 0 . 1 . ( Verbitsky ) Tout orbifold spécial admet une résolution symplectique .",
        "rewrite_text": "Dans cet article, nous présentons une nouvelle démonstration de la classification bimeromorphe des variétés compactes Kähleriennes en utilisant des orbifolds spéciaux. Nous prouvons que si X est un orbifold spécial, alors le groupe fondamental de X se distingue par une identification au groupe fondamental de la variété Kählerienne compacte Y. En particulier, nous montrons que chaque orbifold spécial possède une résolution symplectique. De plus, nous établissons que toute application holomorphe entre deux orbifolds spéciaux entraîne, par le passage aux groupes fondamentaux, une application holomorphe sur leurs résolutions respectives. Enfin, nous présentons quelques exemples d'orbifolds spéciaux qui ne sont pas résolubles. \n\nMots-clés : Orbifolds spéciaux, Variétés Kähleriennes compactes, Groupes fondamentaux, Résolutions symplectiques.\n\n1 Introduction : Considérons X, un espace analytique complexe connexe équipé d'un feuilletage holomorphe F. Cet objet sera désigné dans ce texte comme un orbifold spécial (voir V). Dans ce rapport, nous fournirons une nouvelle démonstration du théorème suivant, énoncé par Verbitsky : Théorème 0.1. (Verbitsky) Tout orbifold spécial possède une résolution symplectique.",
        "ori-fast-z-score": -0.1690308509457033,
        "water-fast-z-score": 2.263009527424072,
        "rewrite-fast-z-score": 0.6859943405700353
    },
    {
        "original_text": "We present the results of searches for gravitational wave signals from compact binary coalescences (CBCs) using data collected by the third and fourth science runs (S3, S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). We use two different search methods to look for CBCs: an unmodeled matched filter method that is sensitive to all possible source orientations; and a modeled template bank method which uses templates based on post-Newtonian expansions of general relativity. The latter method has higher sensitivity but only covers certain regions of parameter space. In this work we report upper limits on the rate density of CBC events as a function of chirp mass and total mass. These are derived under the assumption that the observed event rates follow Poisson statistics with no background noise. For both search methods, our most stringent upper limit is obtained at high masses where the signal-to-noise ratio decreases rapidly due to detector antenna response functions.",
        "watermark_text": "We present the results of investigations for gravitational wave signals from compact binary coalescences ( CBCs ) using data taken by the third and fifth science ran ( S3 , S4 ) of the Laser Interferometer Gravitational - Wave Observatory ( LIGO ) . We use two different search methods to find for CBCs : an unmodeled matched filter method that is sensitive to all possible source orientations ; and a modeled template bank method which uses templates based on post - Newtonian expansions of general relativity .The last approach has larger resolution but only covers certain regions of parameter space . In this research we publish higher restrictions on the frequency density of CBC events as a function of chirp mass and total mass .These are derived under the assumption that the seen event frequencies follow Poisson statistics with no background noise . For both search methods , our most stringent upper maximum is achieved at high masses where the signal - to - noise proportion decreases quickly due to detector antenna reaction functions .",
        "rewrite_text": "We present the findings from our investigations into gravitational wave signals originating from compact binary coalescences (CBCs) using data collected during the third and fifth science runs (S3 and S4) of the Laser Interferometer Gravitational-Wave Observatory (LIGO). Our analysis employs two distinct search methods for detecting CBCs: an unmodeled matched filter approach that is sensitive to all possible orientations of the sources, and a modeled template bank method that utilizes templates derived from post-Newtonian expansions in general relativity. While the latter method offers greater resolution, it is limited to specific regions of parameter space. In this study, we present tighter constraints on the frequency density of CBC events as a function of chirp mass and total mass. These constraints are based on the assumption that the observed event frequencies adhere to Poisson statistics without any background noise. For both search methods, our most stringent upper limits are found at high masses, where the signal-to-noise ratio diminishes rapidly due to the detector's antenna response characteristics.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 4.777777777777778,
        "rewrite-fast-z-score": -0.44172610429938614
    },
    {
        "original_text": "We present the results for squark-antisquark, gluino-gluon and gaugino-gauge boson production at hadron colliders within the framework of non-minimal flavour violating supersymmetric models (NMFV). We consider both NMFV scenarios with MFV-like structure as well as those without it. In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks due to the presence of new sources of flavour violation beyond minimal supergravity. The latter are induced by the non-diagonal entries of the sfermion mass matrices which can be sizeable even if they are generated only radiatively. Our analysis is performed using an effective field theory approach where all heavy particles are integrated out except for the lightest neutral Higgs boson h0 and the Z-boson. This allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "watermark_text": "We present the results for squark - antisquark , gluino - gluon and gaugino - gauge boson development at hadron colliders within the framework of non - reduced flavour violating supersymmetric theories ( NMFV ) . We consider both NMFV scenarios with MFV - like structure as well as those without it .In particular we study the impact on the decay branching ratios of neutralinos into leptons and quarks owing to the presence of new sources of flavour violation beyond minimal supergravity . The latter are induced by the non - diagonal entries of the sfermion mass matrices which can be sizeable even if they are produced only radiatively .Our study is conducted using an efficient field model approach where all heavy ions are integrated out except for the lightest neutral Higgs boson h0 and the Z - boson . This enables us to derive analytical expressions for the appropriate amplitudes and cross sections .",
        "rewrite_text": "We present results concerning the development of squarks and antisquarks, gluinos and gluons, as well as gauginos and gauge bosons at hadron colliders, analyzed through the lens of non-reduced flavor-violating supersymmetric theories (NMFV). Our study encompasses both NMFV models that exhibit minimal flavor violation (MFV)-like structures and those that do not. Specifically, we examine how new sources of flavor violation, which go beyond minimal supergravity, influence the decay branching ratios of neutralinos into leptons and quarks. These sources are generated by non-diagonal elements in the sfermion mass matrices, which can be significant even when produced via radiative processes. We employ an efficient field model approach in our analysis, where all heavy ions are integrated out except for the lightest neutral Higgs boson \\( h_0 \\) and the \\( Z \\)-boson. This methodology allows us to derive analytical expressions for the relevant amplitudes and cross sections.",
        "ori-fast-z-score": -0.13736056394868904,
        "water-fast-z-score": 2.8845718429224694,
        "rewrite-fast-z-score": 1.270001270001905
    },
    {
        "original_text": "We report on the observation of extremely slow hole spin dephasing and relaxation times T2* = 1 ms, T1 = 0.5 s at low temperatures (T < 20 K) for single holes confined to InAs/GaAs self-assembled quantum dots. The observed values are more than one order of magnitude longer than those reported previously for electrons or holes in other semiconductor nanostructures such as quantum wells or wires. We show that this is due to the strong spin-orbit interaction combined with the large effective mass of heavy holes which leads to an enhanced coupling between the hole spins and nuclear magnetic moments. This results in a very efficient suppression of the hyperfine-induced spin relaxation by means of the Overhauser effect. Our findings demonstrate that self-assembled quantum dots can be used as ideal systems for studying fundamental physics phenomena related to the dynamics of individual carriers in semiconductors. They also open up new possibilities for applications based on optically addressable spin qubits in solid-state devices operating at cryogenic temperatures. \n \n Self-assembled quantum dots have been widely studied over recent years because they provide a unique opportunity to investigate carrier confinement effects in three dimensions  1  . These structures allow us to study various physical properties of charge carriers including their optical  2  , electrical  3  , transport  4  and spin  5  characteristics. Quantum dot-based photonic  6  and electronic  7  devices have already been demonstrated experimentally. However, despite significant progress made during last decade there still remain many challenges associated with understanding basic mechanisms governing the behavior of these artificially created nanometer-sized objects  8  .\n \nIn particular, it has recently become clear that the spin degree of freedom plays a crucial role in determining the performance of quantum information processing schemes  9  . Therefore, detailed studies of spin relaxation processes in quantum dots are important both from theoretical point of view and for practical applications  10  . \n \n It was shown theoretically  11  and confirmed experimentally  12  that the electron spin relaxation time T2 * in quantum dots should be limited only by phonon scattering. On the contrary, the hole spin relaxation rate strongly depends on the strength of the spin-orbit interaction  13  . For example, in Ga",
        "watermark_text": "We report on the observation of incredibly slow hole spin dephasing and relaxation period T2 * = 1 ms , T1 = 0 . 5 s at low temperatures ( T < 20 K ) for single holes localized to InAs / GaAs self - assembled quantum dots . The observed values are more than one order of magnitude greater than those noted earlier for electrons or holes in other semiconductor nanostructures such as quantum wells or wires .We see that this is due to the strong spin - orbit interaction combined with the huge effective mass of heavy holes which results to an accelerated interaction between the hole spins and nuclear magnetic moments . This results in a very efficient restoration of the hyperfine - caused spin relaxation by means of the Overhauser effect .Our findings show that self - assembled quantum dots can be used as optimal systems for studying basic physics phenomena related to the dynamics of individual carriers in semiconductors . They also bring up new possibilities for applications based on optically addressable spinning qubits in soft - state systems operating at cryogenic temperatures .Self - assembled quantum dots have been widely explored over recent history because they give a unique opportunity to examine carrier confinement properties in three dimensions 1 . These structures enable us to study various mechanical effects of charge carriers namely their electronic 2 , electromagnetic 3 , transport 4 and spin 5 characteristics .Quantum dot - based photonic 6 and electronic 7 machines have already been shown experimentally . However , despite considerable progress made during ago decade there still continue several challenges associated with studying basic mechanisms governing the activity of these artificially created nanometer - sized particles 8 .In particular , it has recently become clear that the spin level of liberty plays a crucial role in determining the performance of quantum information processing schemes 9 . Therefore , detailed analyses of spin relaxation processes in quantum dots are important both from theoretical point of view and for useful use 10 .It was shown theoretically 11 and reported experimentally 12 that the electron spinning contraction time T2 * in quantum dots should be restricted only by phonon scattering . On the contrary , the hole spin relaxation frequency strongly depends on the strength of the spin - orbit interaction 13 .For instance , in Ga",
        "rewrite_text": "We present findings on the remarkably slow spin dephasing and relaxation times for single holes localized in InAs/GaAs self-assembled quantum dots, with T2* measured at 1 ms and T1 at 0.5 s at low temperatures (T < 20 K). These values exceed previous measurements for electrons and holes in other semiconductor nanostructures, such as quantum wells and wires, by over an order of magnitude. This enhancement is attributed to the strong spin-orbit interaction coupled with the large effective mass of heavy holes, which facilitates a rapid interaction between the hole spins and nuclear magnetic moments. Consequently, this leads to an effective restoration of hyperfine-induced spin relaxation via the Overhauser effect. Our results indicate that self-assembled quantum dots serve as ideal platforms for exploring fundamental physical phenomena related to the dynamics of individual carriers in semiconductors. Additionally, these insights open up new avenues for applications involving optically addressable spin qubits in soft-state systems operating at cryogenic temperatures. Over the years, self-assembled quantum dots have been extensively studied due to their unique capacity to investigate carrier confinement in three dimensions. These structures allow for the examination of various mechanical effects associated with charge carriers, including their electronic, electromagnetic, transport, and spin properties. Quantum dot-based photonic and electronic devices have already seen experimental validation. However, despite significant advancements made over the last decade, numerous challenges persist in understanding the fundamental mechanisms governing the behavior of these artificially engineered nanometer-sized particles. Recently, it has become evident that spin dynamics play a pivotal role in determining the effectiveness of quantum information processing schemes. Therefore, in-depth investigations of spin relaxation processes in quantum dots are critical from both theoretical and practical perspectives. It has been theoretically predicted and experimentally verified that the electron spin relaxation time T2* in quantum dots is primarily limited by phonon scattering. In contrast, the spin relaxation of holes is highly sensitive to the strength of the spin-orbit interaction.",
        "ori-fast-z-score": 1.0910894511799618,
        "water-fast-z-score": 8.460169506877884,
        "rewrite-fast-z-score": -0.15075567228888181
    },
    {
        "original_text": "The author presents an alternative approach to teaching special relativity by using the concept of transformation groups and their generators, which are used in physics for describing symmetry properties of physical systems.  The main idea is that students can learn about the foundations of special relativity without having to deal with complicated mathematical concepts such as tensors or spinor fields. This article describes how this method works on examples related to the Lorentz transformations. It also discusses some possible applications of these ideas in other areas of physics. In particular, it shows how one can use them to explain the origin of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at many universities since its discovery in 1905  1  . However, despite numerous attempts  2  , there still exists no generally accepted way of introducing SR into undergraduate courses  3  .\nIn recent years, several authors have proposed new approaches to teaching SR  4  -  8  . These methods usually involve presenting the basic principles of SR through simple experiments performed in different reference frames  9  -  11  . They often require only minimal knowledge of mathematics  12  -  14  . Some of these proposals were inspired by Feynman s lectures  15  . Other authors tried to develop similar techniques based on modern computer technology  16  -  18  .",
        "watermark_text": "The author presents an different approach to teaching regular gravity by using the idea of transformation groups and their generators , which are needed in science for describing symmetry properties of physical structures . The main idea is that students can know about the foundations of regular gravity without having to deal with difficult numerical notions such as tensors or spinor fields .This page describes how this process uses on examples related to the Lorentz transformations . It additionally outlines some possible applied of these ideas in other areas of physics .In particular , it demonstrates how one can using them to explain the origin of gauge symmetries in quantum field theory . Special relativity ( SR ) has been taught at many universities since its discovery in 1905 1 .However , despite several efforts 2 , there still exists no usually agreed way of introducing SR into undergraduate courses 3 . In recent years , various authors have proposed additional strategies to teaching SR 4 - 8 .These methods usually include presenting the fundamental principles of SR through ordinary observations performed in different reference spaces 9 - 11 . They often use only limited knowledge of math 12 - 14 .Some of these proposals were inspired by Feynman s lectures 15 . Other scholars try to develop similar method using on contemporary computer technology 16 - 18 .",
        "rewrite_text": "The author offers a novel approach to teaching regular gravity by incorporating the concept of transformation groups and their generators, which play a crucial role in science for describing the symmetry properties of physical structures. The core idea is that students can grasp the fundamentals of regular gravity without grappling with complex numerical concepts like tensors or spinor fields. This article explains how this method employs examples related to Lorentz transformations and also highlights potential applications of these concepts in other areas of physics. Notably, it illustrates how they can be used to elucidate the origins of gauge symmetries in quantum field theory. Special relativity (SR) has been taught at numerous universities since its inception in 1905. However, despite various attempts, there remains no widely accepted method for introducing SR in undergraduate courses. In recent years, a number of authors have suggested alternative strategies for teaching SR. These approaches typically involve presenting the core principles of SR through everyday observations made in different reference frames, often utilizing only basic mathematical knowledge. Some of these proposals draw inspiration from Feynman's lectures, while others aim to develop similar methods through contemporary computer technology.",
        "ori-fast-z-score": 0.769800358919501,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 1.872125628512157
    },
    {
        "original_text": "We study the lepton-flavor-violating (LFV) decays of charged leptons induced by an exchange of heavy particles with masses above 1 TeV, which are referred to as  unparticles . We show that these LFV decays can be enhanced significantly if there is mixing between ordinary and exotic fermions. In particular, we find that the branching ratio for muon decay into electron plus photon may reach 10 −8 . This result implies that such LFV decays could be observed at future experiments like Mu3e or COMET. Introduction -Lepton Flavor Violation (LFV), i.e., the process where one observes a transition between different flavors of leptons, has been studied extensively both theoretically  1  and experimentally  2  , since it was first proposed more than thirty years ago  3  . The current experimental bounds on various LFV processes have reached impressive levels  4  .\nTheoretically speaking, many extensions beyond the Standard Model predict sizable rates for LFV processes  5  . For example, supersymmetric models  6  , left-right symmetric models  7  , and extra-dimensional theories  8  all contain new sources of LFV interactions. However, most of them require some fine-tuning and/or introduce additional parameters so that their predictions agree well with existing data  9  . Therefore, any observation of LFV would provide strong evidence against those theoretical frameworks  10  .\nIn this work, we consider another class of models known as  unparticle physics   11  . These models assume that there exist new degrees of freedom whose mass scale lies far beyond the energy range accessible to present-day accelerators  12  . Such states cannot be directly produced but they can affect low-energy observables through virtual effects  13  . It turns out that the presence of these new states leads to interesting phenomenological consequences  14  . One particularly important consequence is that they induce LFV transitions among ordinary leptons  15  . As shown below, these LFV processes can occur at observable rates even when the corresponding couplings are extremely small  16  .",
        "watermark_text": "We test the lepton - flavor - violating ( LFV ) decays of charged leptons induced by an exchange of heavy ions with masses above 1 TeV , which are termed to as unparticles . We see that these LFV decays can be enhanced substantially if there is mixing between ordinary and rare fermions .In particular , we find that the branching ratio for muon decay into atom plus photon may reach 10 −8 . This result suggests that such LFV decays might be found at possible experiments like Mu3e or COMET .Introduction - Lepton Flavor Violation ( LFV ) , i . e . , the process where one sees a shift between various flavors of leptons , has been studied thoroughly both theoretically 1 and experimentally 2 , since it was first suggested more than thirty years previously 3 . The present scientific limits on various LFV processes have achieved impressive levels 4 .Theoretically speaking , various extensions beyond the Standard Model predict sizable levels for LFV processes 5 . For instance , supersymmetric theories 6 , left - right symmetric models 7 , and extra - dimensional theories 8 all contain new sources of LFV interactions .However , most of them require some fine - tuned and / or introduce extra values so that their models comply good with existing information 9 . Therefore , any study of LFV might give strong evidence against those theoretical frameworks 10 .In this research , we define another class of models termed as unparticle physics 11 . These designs assume that there exist new degrees of freedom whose mass scale lies much beyond the power range available to today - day accelerators 12 .Such states cannot be directly produced but they can affect small - energy observables through virtual interactions 13 . It turns out that the presence of these new states gives to unusual phenomenological consequences 14 .One especially essential characteristic is that they generate LFV transitions among ordinary leptons 15 . As seen below , these LFV processes can occur at observable rates even when the associated couplings are extremely tiny 16 .",
        "rewrite_text": "We investigate lepton flavor-violating (LFV) decays of charged leptons caused by the exchange of heavy particles, referred to as unparticles, with masses exceeding 1 TeV. Our findings indicate that these LFV decays can be significantly enhanced in the presence of mixing between standard and rare fermions. Specifically, we discover that the branching ratio for muon decay into a neutrino and a photon could reach as high as 10^(-8). This implies that these LFV decays might be detectable in upcoming experiments such as Mu3e or COMET. \n\nLepton Flavor Violation (LFV) involves the observation of transitions between different lepton flavors and has been rigorously explored both theoretically and experimentally since its proposition over thirty years ago. Current scientific limits on various LFV processes have reached impressive levels. Theoretically, several extensions beyond the Standard Model predict significant LFV processes. For example, models like supersymmetry, left-right symmetric theories, and extra-dimensional frameworks all introduce new sources of LFV interactions. However, many of these models necessitate fine-tuning or the introduction of additional parameters to align with existing data. As a result, studying LFV could provide compelling evidence against these theoretical frameworks.\n\nIn this work, we introduce a new class of models known as unparticle physics. These models posit the existence of new degrees of freedom with mass scales far beyond the capabilities of current accelerators. While these states cannot be produced directly, they can influence low-energy observables through virtual interactions. Notably, these new states can lead to interesting phenomenological consequences, including inducing LFV transitions among standard leptons. As we will demonstrate, such LFV processes can occur at observable rates, even with very small associated couplings.",
        "ori-fast-z-score": -0.5980503604017327,
        "water-fast-z-score": 7.88815704602373,
        "rewrite-fast-z-score": 1.2780193008453875
    },
    {
        "original_text": "We study the dynamics of free fermions hopping between sites of an arbitrary connected graph, with no restriction to nearest-neighbor hopping. We show that this system is equivalent to a collection of independent random walks evolving in parallel and interacting via pairwise collisions at vertices. The collision rate depends only on the number of particles present at each vertex; it vanishes for graphs without loops or multiple edges (e.g., trees), but can be arbitrarily large otherwise. This model exhibits interesting behavior even when all rates are equal, including anomalous diffusion and superdiffusion. In particular, we prove that the mean-square displacement grows as t3/2 for any tree-like graph, while it scales faster than t2/3 for general graphs. Finally, we discuss possible extensions of our results beyond the free-fermion case. Introduction: A wide variety of physical phenomena ranging from quantum transport through mesoscopic systems  1  , to population biology  2  , involve non-equilibrium particle dynamics on networks. These models typically assume that particles move along directed links according to some prescribed rules, such as unrestricted hopping  3  . However, many real-world situations require more complicated interactions among particles  4  .\nIn this work, we consider a simple generalization of standard one-dimensional lattice models  5  by allowing particles to hop freely between adjacent nodes of an arbitrary connected graph G = (V, E). More precisely, let us fix a finite set S of states associated with each node v ∈ V ; then, given a configuration c : V → S, we define the state space C(G) := {c: V → S}. For every edge e = {u, v} ∈ E, we associate two transition probabilities p+(c, c )(e) ≥ 0 and p−(c, c )(u, v) > 0; these represent the probability per unit time that a particle located at u jumps to v if its current state is c, and vice versa. Then, the evolution of the system is described by a continuous-time Markov process Xt taking values in C(G).\nThe main goal of this Letter is to analyze the",
        "watermark_text": "We explore the dynamics of free fermions hopping between locations of an arbitrary linked graph , with no limitation to nearest - neighbor hopping . We see that this scheme is analogous to a collection of independent random tours advancing in concurrent and communicating via pairwise collisions at vertices .The crash time depends only on the quantity of particles present at each vertex ; it vanishes for graphs without loops or multiple edges ( e . g . , trees ) , but can be arbitrarily huge otherwise . This theory exhibits unusual phenomena even when all rates are equal , notably anomalous absorption and superdiffusion .In particular , we prove that the mean - square displacement grows as t3 / 2 for any forest - like graph , while it scales faster than t2 / 3 for general graphs . Finally , we explain possible extensions of our findings beyond the free - fermion case .Introduction : A wide multitude of natural concepts ranging from particle transport through mesoscopic systems 1 , to population physics 2 , employ non - equilibrium molecule interactions on networks . These models usually assumption that particles moving along coordinated links according to some prescribed rules , such as unrestricted hopping 3 .However , many actual - time situations involve more complicated relationships among particles 4 . In this research , we define a simple generalization of standard one - dimensional crystal models 5 by requiring particles to hop freely between neighboring vertices of an arbitrary linked graph G = ( V , E ) .More specifically , let us fix a finite collection S of states associated with each node v ∈ V ; then , given a configuration c : V → S , we define the state collection C ( G ) : = { c : V → S } . For every edge e = { u , v } ∈ E , we associate two transition probabilities p + ( c , c ) ( e ) ≥ 0 and p− ( c , c ) ( v , v ) > 0 ; these denote the probability per unit time that a particle situated at u escapes to v if its current state is p , and vice versa .Then , the evolution of the process is characterized by a continuous - time Markov process Xt assuming variables in C ( G ) . The main goal of this Letter is to analyze the",
        "rewrite_text": "We investigate the behavior of free fermions as they hop between various locations on an arbitrary linked graph, without restricting the movement to nearest-neighbor interactions. This framework can be compared to a series of independent random tours that progress simultaneously and interact through pairwise collisions at vertices. The time until collisions occur is solely dependent on the number of particles at each vertex; it disappears entirely for graphs that lack loops or multiple edges (such as trees), but can become arbitrarily long in other cases. Our theory reveals intriguing phenomena, even when all rates are uniform, particularly notable are the anomalous absorption and superdiffusion observed. Specifically, we demonstrate that the mean-square displacement increases at a rate of \\( t^{3/2} \\) for any forest-like graph, while it scales faster than \\( t^{2/3} \\) for more general graphs. Furthermore, we discuss potential extensions of our results beyond the realm of free fermions.\n\nIntroduction: Numerous natural concepts, spanning from particle transport in mesoscopic systems to population dynamics, rely on non-equilibrium molecular interactions on networks. These models typically assume that particles move along defined links according to certain specified rules, such as unrestricted hopping. However, many real-time scenarios involve more complex relationships between particles. In this study, we introduce a straightforward generalization of conventional one-dimensional crystal models by allowing particles to hop freely between adjacent vertices of an arbitrary linked graph \\( G = (V, E) \\). More precisely, we establish a finite set \\( S \\) of states for each node \\( v \\in V \\); given a configuration \\( c: V \\to S \\), we define the state collection \\( C(G) := \\{ c: V \\to S \\} \\). For each edge \\( e = \\{ u, v \\} \\in E \\), we associate two transition probabilities \\( p^+(c, c')(e) \\geq 0 \\) and \\( p^-(c, c') \\) (for \\( v \\)), indicating the probability per unit time that a particle at \\( u \\) moves to \\( v \\) given its current state \\( c \\), and vice versa. The dynamic of this process is represented by a continuous-time Markov process \\( X_t \\) that takes values in \\( C(G) \\). The primary aim of this letter is to analyze the...",
        "ori-fast-z-score": -1.3000224919331833,
        "water-fast-z-score": 7.429360827073253,
        "rewrite-fast-z-score": 1.819435304589368
    },
    {
        "original_text": "We study the stability of viscous shock waves for an ideal gas with viscosity and heat conduction, using the method of characteristics.  We show that if the initial data are sufficiently small then there exists a unique global solution to this problem which converges exponentially fast as t → ∞ towards the corresponding inviscid solution (the Rankine-Hugoniot condition). If the initial data have compact support we prove that the solution remains uniformly bounded on any finite time interval  0,T . Finally, under some additional assumptions about the initial data, we establish the exponential decay of solutions at infinity. The results obtained here generalize those known previously only for the case when the pressure p(ρ) = ρ γ , where 0 < γ ≤ 1.1. Viscous shock waves play important role in many physical phenomena such as combustion theory or plasma physics. In particular they appear in connection with the so-called Riemann problems describing the interaction between two different flows separated by a discontinuity surface. For example, one can consider the following problem:  Find a function u(x,t), x ∈ R n , t > 0 satisfying the system of equations",
        "watermark_text": "We research the stability of viscous blast currents for an suitable gas with viscosity and heat conduction , using the method of characteristics . We see that if the first data are sufficiently small then there exists a unique worldwide answer to this question which converges exponentially rapidly as t → ∞ towards the equivalent inviscid solving ( the Rankine - Hugoniot condition ) .If the first data have compact support we prove that the solve remains uniformly bounded on any finite time interval 0 , T . Finally , under some additional constraints about the initial data , we determine the exponential decay of solutions at infinity .The results derived here generalize those known previously only for the case when the pressure p ( ρ ) = ρ γ , where 0 < γ ≤ 1 . 1 . Viscous blast currents play important role in many mechanical phenomena such as combustion theory or plasma physics .In particular they appear in connection with the so - called Riemann problems regarding the interaction between two different flows divided by a discontinuity terrain . For instance , one can consider the following issue : Find a function v ( x , t ) , x ∈ R n , t > 0 satisfying the scheme of equations",
        "rewrite_text": "We investigate the stability of viscous blast currents in a suitable gas characterized by viscosity and heat conduction, employing the method of characteristics. Our findings indicate that if the initial data is sufficiently small, there exists a unique globally defined solution that converges exponentially fast towards the corresponding inviscid solution (as described by the Rankine-Hugoniot condition) as \\( t \\rightarrow \\infty \\). Additionally, if the initial data has compact support, we demonstrate that the solution remains uniformly bounded over any finite time interval \\( [0, T] \\). Furthermore, under certain additional conditions on the initial data, we establish the exponential decay of the solutions at infinite time. The results presented here extend previous findings that were limited to the case where the pressure \\( p(\\rho) = \\rho^\\gamma \\), with \\( 0 < \\gamma \\leq 1 \\). Viscous blast currents are significant in various mechanical phenomena, including combustion theory and plasma physics. They are particularly relevant in the context of Riemann problems, which examine the interaction between two distinct flows separated by a discontinuous interface. For example, we pose the following problem: Find a function \\( v(x, t) \\) for \\( x \\in \\mathbb{R}^n \\) and \\( t > 0 \\) that satisfies the corresponding system of equations.",
        "ori-fast-z-score": -1.078327732034384,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 0.7258661863112977
    },
    {
        "original_text": "The European Commission has been funding research in universities, public research institutes (PRIs) and private companies since 1984 under its Framework Programmes for Research & Development (FP). The fifth framework programme (FP5), which ran between 1998-2002, was particularly successful at bringing together researchers across Europe to work on large scale projects with industrial partners.  This article describes how this collaboration worked by examining one such project - the Network of Excellence in Wireless Communications (NEWCOM#).  NEWCOM# brought together more than 100 academic and industrial organisations from all over Europe into an integrated research consortium that developed new technologies for wireless communications systems. It is shown here that the success of NEWCOM# can be attributed to three main factors:  Firstly, it had a clear vision of what needed to be achieved; secondly, there were strong links between academia and industry; thirdly, it benefited from a supportive policy environment within the EU.",
        "watermark_text": "The European Commission has been supporting research in schools , public research institutes ( PRIs ) and public companies since 1984 under its Framework Programmes for Research & Development ( FP ) . The fifth framework scheme ( FP5 ) , which ran between 1998 - 2002 , was especially successful at bringing together researchers across Europe to work on huge scale studies with production partners .This page describes how this partnership helped by examining one such initiative - the Network of Excellence in Wireless Communications ( NEWCOM # ) . NEWCOM # brought together more than 100 academic and industrial organisations from all over Europe into an unified research consortium that developed novel systems for wireless communications systems .It is demonstrated here that the result of NEWCOM # can be due to three principal factors : Firstly , it had a clear view of what needed to be achieved ; secondly , there were strong links between academia and enterprise ; thirdly , it benefited from a supportive policy environment within the EU .",
        "rewrite_text": "Since 1984, the European Commission has been funding research in schools, public research institutes (PRIs), and public companies through its Framework Programmes for Research & Development (FP). The fifth framework, known as FP5, which took place from 1998 to 2002, was particularly effective in uniting researchers across Europe to collaborate on large-scale studies with industry partners. This page illustrates the impact of such collaboration by highlighting the Network of Excellence in Wireless Communications (NEWCOM #). NEWCOM # brought together over 100 academic and industrial organizations from across Europe into a cohesive research consortium focused on developing innovative wireless communication systems. The success of NEWCOM # can be attributed to three key factors: first, it had a well-defined vision of its goals; second, there were robust connections between academia and industry; and third, it operated within a favorable policy framework provided by the EU.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 5.249512077248736,
        "rewrite-fast-z-score": -0.13018891098082389
    },
    {
        "original_text": "The authors describe how they have created an artificial life form that self-assembles into a structure similar to the fruiting body of the Dictyostelium discoideum, or  dictyos  for short.  The dictyos is made up of thousands of identical cells connected by elastic filaments and can be grown in Petri dishes with nutrients added.   It has been shown to grow and divide indefinitely without any external control signals (such as hormones).   This work was done at Harvard University s Wyss Institute for Biologically Inspired Engineering.   ... Self-assembly refers to the spontaneous organization of matter into ordered structures such as crystals, snowflakes, and living organisms like bacteria colonies and animal tissues. In this study we report on our efforts toward creating a synthetic multicellular system capable of autonomous growth and division through self-assembly. We designed a minimal cell based on a spherical water-in-oil emulsion droplet containing a single microtubule-based cytoskeleton surrounded by a lipid membrane. These cells are able to attach to each other via flexible polymeric tethers and assemble into three-dimensional aggregates called “dictyos” which resemble the fruiting bodies formed by the social amoeba Dictyostelium discoidium. Our results demonstrate that these simple cellular units can autonomously organize themselves into complex 3D shapes reminiscent of natural systems.",
        "watermark_text": "The authors report how they have created an artificial life form that self - assembles into a structure similar to the fruiting bodies of the Dictyostelium discoideum , or dictyos for short . The dictyos is made up of millions of identical cells connected by elastic filaments and can be grown in Petri dishes with minerals added .It has been shown to expand and divide indefinitely without any external regulating signals ( such as hormones ) . This research was done at Harvard University s Wyss Institute for Biologically Inspired Engineering .. . . Self - assembly describes to the spontaneous organization of matter into organized objects such as bubbles , snowflakes , and live organisms like bacteria populations and human flesh . In this study we review on our work toward building a artificial multicellular system efficient of automatic development and division through self - assembly .We built a minimal cell based on a spherical water - in - oil emulsion droplet containing a single microtubule - based cytoskeleton enclosed by a lipid membrane . These cells are able to connect to each other via flexible polymeric tethers and arrange into three - dimensional aggregates called “ dictyos ” which mimic the fruiting bodies produced by the social amoeba Dictyostelium discoidium .Our results show that these simple cellular divisions can autonomously arrange themselves into complex 3D shapes reminiscent of natural units .",
        "rewrite_text": "The authors describe their development of an artificial life form that self-organizes into a structure resembling the fruiting bodies of Dictyostelium discoideum, commonly known as dictyos. These dictyos consist of millions of identical cells interconnected by elastic filaments and can be cultivated in Petri dishes supplemented with minerals. Remarkably, they can grow and divide indefinitely without any external regulatory signals, such as hormones. This research was conducted at Harvard University's Wyss Institute for Biologically Inspired Engineering. \n\nSelf-assembly refers to the natural organization of matter into structured forms, such as bubbles, snowflakes, and living organisms like bacterial colonies and human tissue. In this study, we detail our efforts to create an artificial multicellular system capable of automatic growth and division through self-assembly. We constructed a minimal cell using a spherical water-in-oil emulsion droplet that contains a single microtubule-based cytoskeleton, all enclosed by a lipid membrane. These cells can connect via flexible polymeric tethers and form three-dimensional aggregates known as “dictyos,” which mimic the fruiting bodies of the social amoeba Dictyostelium discoideum. Our findings demonstrate that these simple cellular units can autonomously arrange themselves into complex 3D shapes reminiscent of natural structures.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.7461923416925424,
        "rewrite-fast-z-score": 2.4379951240146283
    },
    {
        "original_text": "The formation of dipoles in self-assembled monolayers (SAMs) on metal surfaces is investigated by scanning tunneling microscopy/spectroscopy (STM/STS). The SAM consists of octadecanethiols with an end group that contains either one or two thiocyanates, which are known to form strong dipole moments upon adsorption onto gold substrates. We find that the presence of these strongly polarizable groups leads to significant changes in the electronic structure of the SAM compared to nonpolar alkane chains. In particular, we observe a shift of the molecular states towards higher energies as well as a reduction of their spatial extension perpendicular to the surface. These effects can be explained within a simple model based on electrostatic interactions between the molecules and the substrate. Our results demonstrate how chemical functionalization allows for tailoring the properties of organic films deposited on metallic surfaces. Dipole formation at interfaces of alkanethiolate self-assembled monolay- ers and Ag(111) has been studied using scanning tunneling microscopy/ spectroscopy (STM/S). The SAM was prepared by chemisorption of octadecanethiol containing thiocyanate endgroups on Ag(111), resulting in a film with a large dipole moment per unit area. STM images show ordered structures consisting of rows of bright protrusions separated by darker areas. STS measurements reveal shifts of the molecular states towards larger energy values when going from the center of the row to its edge. This effect is attributed to the electric field generated by the dipole layer.",
        "watermark_text": "The formation of dipoles in self - assembled monolayers ( SAMs ) on metal surfaces is investigated by scanning tunneling microscopy / spectroscopy ( STM / STS ) . The SAM consists of octadecanethiols with an ending group that contains either one or two thiocyanates , which are known to form strong dipole moments upon adsorption onto gold surfaces .We see that the presence of these strongly polarizable groups results to significant improvements in the chemical structure of the SAM compared to nonpolar alkane rings . In particular , we study a shift of the molecular states towards higher energies as well as a reduction of their spatial extension lateral to the surface .These effects can be described within a simple simulation based on electrostatic interactions between the molecules and the substrate . Our results show how chemical functionalization allows for tailoring the properties of organic films accumulated on metallic surfaces .Dipole form at interfaces of alkanethiolate self - assembled monolay - ers and Ag ( 111 ) has been studied utilizing scanning tunneling microscopy / spectroscopy ( STM / S ) . The SAM was prepared by chemisorption of octadecanethiol containing thiocyanate endgroups on Ag ( 111 ) , resulting in a film with a large dipole point per unit area .STM pictures show ordered compounds consisting of columns of bright protrusions separated by deeper spots . STS measurements reveal shifts of the molecular states towards higher energy quantities when going from the center of the row to its boundary .This phenomenon is attributed to the electric field produced by the dipole layer .",
        "rewrite_text": "The investigation of dipole formation in self-assembled monolayers (SAMs) on metal surfaces is conducted using scanning tunneling microscopy/spectroscopy (STM/STS). The SAM is composed of octadecanethiols, featuring either one or two thiocyanate end groups, which are known to generate significant dipole moments upon adsorption on gold surfaces. Our findings indicate that the incorporation of these highly polarizable groups leads to notable enhancements in the chemical structure of the SAM when compared to nonpolar alkane rings. Specifically, we observe an upward shift in the molecular states toward higher energy levels, along with a decrease in their lateral spatial extension relative to the surface. These phenomena can be explained through a straightforward simulation utilizing electrostatic interactions between the molecules and the substrate. Our results highlight how chemical functionalization can effectively tailor the properties of organic films deposited on metallic surfaces. Additionally, the formation of dipoles at the interfaces of alkanethiolate self-assembled monolayers on Ag(111) has been explored through scanning tunneling microscopy/spectroscopy (STM/STS). The SAM was formed by chemisorbing octadecanethiol with thiocyanate end groups onto Ag(111), leading to a film characterized by a high dipole point density per unit area. STM images reveal ordered structures made up of columns of bright protrusions, interspersed with deeper regions. STS measurements indicate that the molecular states shift to higher energy levels when moving from the center of the row to its edges, a phenomenon attributed to the electric field generated by the dipole layer.",
        "ori-fast-z-score": 1.3862065601673441,
        "water-fast-z-score": 6.8657566124489255,
        "rewrite-fast-z-score": 2.7196004146003396
    },
    {
        "original_text": "The thermal stability and melting behavior of metallic single-wall carbon nanotubes (SWCNTs) are investigated by using an efficient tight-binding molecular dynamics simulation method with the Tersoff potential function. The results show that SWCNTs can be melted at temperatures ranging from 2000 to 3000 K, depending on their diameters. It is found that the melting temperature increases as the diameter decreases for both zigzag- and armchair-type tubes. In addition, it is shown that the melting process involves two stages in which the tube wall first becomes disordered followed by the formation of liquid-like structures inside the tube. Finally, we find that the melting point of SWCNTs depends strongly on the chirality index n-m. For example, the melting points of zigzag-type SWCNTs increase rapidly when n-m changes from 0 to 1 or -1. \n \n Keywords: Thermal stability, Melting behavior, Carbon nanotube",
        "watermark_text": "The mechanical stability and melting behavior of metallic single - wall carbon nanotubes ( SWCNTs ) are examined by using an efficient tight - binding molecular mechanics model model with the Tersoff potential function . The results show that SWCNTs can be melted at conditions ranging from 2000 to 3000 K , depending on their diameters .It is found that the melting temperature increases as the radius decreases for both zigzag - and armchair - type tubes . In addition , it is demonstrated that the melting cycle involves two stages in which the tube floor first gets disordered preceded by the formation of liquid - like structures inside the pipe .Finally , we find that the melting point of SWCNTs differs strongly on the chirality index n - m . For instance , the melting points of zigzag - class SWCNTs increase quickly when n - m changes from 0 to 1 or - 1 . Keywords : Thermal stability , Melting behavior , Carbon nanotube",
        "rewrite_text": "The mechanical stability and melting characteristics of metallic single-wall carbon nanotubes (SWCNTs) are analyzed using an effective tight-binding molecular mechanics model employing the Tersoff potential function. The findings reveal that SWCNTs can melt at temperatures ranging from 2000 to 3000 K, influenced by their diameters. It has been observed that the melting temperature rises as the radius decreases for both zigzag and armchair configurations. Furthermore, the melting process is shown to occur in two stages: initially, the tube's base becomes disordered, followed by the emergence of liquid-like structures within the nanotube. Additionally, our research indicates that the melting point of SWCNTs is significantly affected by the chirality index (n - m). For example, the melting points of zigzag-type SWCNTs increase sharply when n - m shifts from 0 to 1 or -1. \n\nKeywords: Thermal stability, Melting behavior, Carbon nanotube",
        "ori-fast-z-score": 2.0,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "We study the effect of disorder on the electronic structure in Bi2Sr2CaCu2O8+δ (Bi-2212) by using the t-matrix approximation with realistic parameters for the impurity scattering potential. We find that, even at low temperatures where the system is expected to be fully gapped, there are still states inside the gap which can lead to an enhanced density of states near the Fermi level. This result suggests that the pseudogap may not be due solely to pairing correlations but also has contributions from other effects such as disorder or magnetic fluctuations. The results presented here provide further evidence against the existence of a true energy gap in this material. \n \n Introduction: In recent years, much attention has been paid to the phenomenon known as  pseudogap  observed in high-Tc cuprates  1  . It refers to the suppression of the density of states around the Fermi surface below T* ~ 150 K  2  , which cannot be explained within BCS theory  3  . Pseudogap behavior was first discovered in angle-resolved photoemission spectroscopy experiments  4  , and later confirmed by scanning tunneling microscopy  5  .\nIn order to explain these experimental observations, several theoretical models have been proposed  6  -  8  . However, it remains unclear whether the pseudogap originates from pairing correlations  9  , antiferromagnetic spin fluctuations  10  , charge ordering  11  , or some combination of all three  12  .",
        "watermark_text": "We research the impact of disorder on the electronic configuration in Bi2Sr2CaCu2O8 + δ ( Bi - 2212 ) by using the t - vector algorithm with realistic variables for the impurity absorption potential . We see that , even at low temperatures where the system is expected to be properly gapped , there are still states inside the gap which can lead to an increase concentration of states near the Fermi level .This result suggests that the pseudogap may not be due exclusively to pairing correlations but also has influence from other effects such as disorder or magnetic fluctuations . The results presented here provide further evidence against the existence of a genuine energy deficit in this material .Introduction : In recent recently , much attention has been paid to the process known as pseudogap observed in high - Tc cuprates 1 . It involves to the disruption of the density of states around the Fermi boundary below T * ~ 150 K 2 , which cannot be described within BCS theory 3 .Pseudogap activity was first discovered in angle - resolved photoemission spectroscopy experiments 4 , and later confirmed by scanning tunneling microscopy 5 . In order to explain these research studies , various experimental scenarios have been proposed 6 - 8 .However , it remains unsure whether the pseudogap originates from pairing correlations 9 , antiferromagnetic spin fluctuations 10 , charge ordering 11 , or some mix of all three 12 .",
        "rewrite_text": "We investigate the effect of disorder on the electronic configuration of Bi2Sr2CaCu2O8 + δ (Bi-2212) using the t-vector algorithm with realistic parameters for the impurity absorption potential. Our findings reveal that even at low temperatures, where the system is expected to exhibit a proper energy gap, there are still states present within the gap that could result in an increased concentration of states near the Fermi level. This outcome implies that the pseudogap may not solely be attributed to pairing correlations but could also be influenced by additional factors such as disorder or magnetic fluctuations. These results provide further evidence against the notion of a true energy deficit in this material.\n\n**Introduction:** Recently, significant focus has been directed toward the pseudogap phenomenon observed in high-Tc cuprates. This involves a disruption in the density of states around the Fermi boundary below T* ~ 150 K, which cannot be adequately explained by BCS theory. The pseudogap effect was initially identified through angle-resolved photoemission spectroscopy experiments and later corroborated by scanning tunneling microscopy. To account for these findings, various experimental scenarios have been suggested. However, it remains uncertain whether the pseudogap derives from pairing correlations, antiferromagnetic spin fluctuations, charge ordering, or a combination of these elements.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 6.394537504003702,
        "rewrite-fast-z-score": 0.3110855084191276
    },
    {
        "original_text": "We present an analytical model for calculating the aperture efficiency of a reflector antenna with circularly polarized feeds, based on ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to calculate the electric field distribution over the aperture surface by solving Maxwell s equations using Green functions. We show that this approach can be applied to any type of feed horns, including corrugated conical horns or dual-polarized log-periodic dipole arrays. This work was motivated by our recent study of aperture efficiencies of two different types of antennas operating at 1.4 GHz: a single-dish radio telescope equipped with a corrugated conical feed horn and a multi-element interferometer consisting of eight log-periodic dipole array elements. In both cases we found good agreement between numerical results obtained with our new model and experimental data. \n \n Keywords: Antenna design",
        "watermark_text": "We present an analytical model for determining the lens energy of a reflector antenna with circularly polarized feeds , using on ray tracing through the feed - horn optics and physical optics ( PO ) at the aperture plane . The PO technique is utilized to estimate the electric field flow over the lens surface by solving Maxwell s equations using Green functions .We see that this methodology can be applied to any type of feed horns , particularly corrugated conical horns or multiple - polarized log - periodic dipole arrays . This research was sparked by our latest research of aperture efficiencies of two different kinds of antennas active at 1 . 4 GHz : a single - dish radio telescope fitted with a corrugated conical feed trumpet and a multi - component interferometer composed of eight log - periodic dipole array modules .In both cases we concluded excellent agreement between mathematical findings obtained with our new model and experimental evidence . Keywords : Antenna design",
        "rewrite_text": "We introduce an analytical model for calculating the lens energy of a reflector antenna equipped with circularly polarized feeds. This model employs ray tracing through the feed-horn optics and physical optics (PO) at the aperture plane. The PO method is used to estimate the electric field distribution across the lens surface by solving Maxwell's equations with Green's functions. Notably, this approach can be applied to various types of feed horns, especially corrugated conical horns and multi-polarized log-periodic dipole arrays. Our research was motivated by recent investigations into the aperture efficiencies of two distinct antennas operating at 1.4 GHz: a single-dish radio telescope with a corrugated conical feed trumpet and a multi-component interferometer made up of eight log-periodic dipole array modules. In both cases, we found strong agreement between the theoretical results produced by our new model and the experimental data. Keywords: Antenna Design",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.714045207910317,
        "rewrite-fast-z-score": 0.8427009716003844
    },
    {
        "original_text": "We report on an unexpected reversal of temperatures between two components of a brown-dwarf eclipsing binary system, which we have discovered using infrared photometry and spectroscopy obtained with Spitzer Space Telescope (Werner et al., 2004) and Gemini Observatory (Gemini North telescope). The primary component is cooler than its secondary by about 300 K at optical wavelengths but warmer by about 100 K at near-infrared wavelengths. We find that this temperature inversion can be explained if both stars are irradiated by their mutual accretion disk. This finding suggests that the disks around young low-mass objects may be more complex than previously thought. \n \n Keywords: Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric variability, Stellar radius, Temperature inversion, Young star \n \n \n \n 1 Introduction \n \n An important goal for understanding how planets form is to determine what happens during the earliest stages of planet formation when protoplanetary disks surround young stellar systems. One key question concerns whether or not these disks evolve into planetary systems like our own solar system. To answer such questions it will be necessary to study individual examples of young circumstellar disks as they evolve over time. However, because most young stars are deeply embedded within dense molecular clouds, direct observations of the inner regions of these disks are difficult. Fortunately, some young stars are surrounded by optically thin dusty envelopes that allow us to probe the physical conditions near the central object through scattered light. These so-called transitional disks show evidence of clearing out large amounts of material inside several AU of the central star while still retaining significant quantities of gas farther away (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007) . \n \n A number of studies suggest that the outer edges of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "watermark_text": "We report on an unexpected change of temperatures between two parts of a brown - giant eclipsing binary system , which we have discovered using infrared photometry and spectroscopy acquired with Spitzer Space Telescope ( Werner et al . , 2004 ) and Gemini Observatory ( Gemini North telescope ) . The main component is warmer than its primary by about 300 K at visual wavelengths but cooler by about 100 K at near - infrared wavelengths .We see that this heat inversion can be described if both stars are irradiated by their mutual accretion disk . This found shows that the disks around old minimum - density items might be more sophisticated than previously thought .Keywords : Accretion Disk , Inverse P - Cygni profile , Irradiation , Low - Mass Star , Near - Infrared Spectroscopy , Photometric variability , Stellar radius , Temperature inversion , Young star 1 Introduction An key goal for knowledge how planets occur is to study what comes during the earliest periods of planet development when protoplanetary disks invade young stellar systems . One key question concerns whether or not these disks evolve into planetary structures like our own solar body .To answer such problems it will be required to study individual examples of young circumstellar disks as they develop over time . However , because most young galaxies are deeply embedded within dense molecular clouds , direct observations of the inner regions of these disks are problematic .Fortunately , some young galaxies are surrounded by optically thin dusty envelopes that enable us to probe the physical conditions near the main object through dispersed light . These so - called transitional disks show proof of sweeping out large quantities of debris inside several AU of the main star while nevertheless supporting significant amounts of gas farther distant ( Strom et al . , 1989 ; Skrutskie et al . , 1990 ; Calvet et al . , 2002 ; Muzerolle et al . , 2003 ; Sicilia - Aguilar et al . , 2006 ; Espaillat et al . , 2007 ) .A variety of studies imply that the exterior corners of transitional disks are sculpted by photoevaporative winds driven off the surface of the disk by intense ultraviolet radiation from nearby",
        "rewrite_text": "We present findings of an unexpected temperature variation between two components of a brown giant eclipsing binary system, discovered through infrared photometry and spectroscopy conducted with the Spitzer Space Telescope (Werner et al., 2004) and the Gemini North telescope at the Gemini Observatory. The primary star is approximately 300 K warmer than its companion at visual wavelengths, yet it is about 100 K cooler at near-infrared wavelengths. This temperature inversion appears to result from both stars being irradiated by their mutual accretion disk. This discovery suggests that the disks surrounding old, low-density objects may be more complex than previously recognized. \n\n**Keywords:** Accretion Disk, Inverse P-Cygni profile, Irradiation, Low-Mass Star, Near-Infrared Spectroscopy, Photometric Variability, Stellar Radius, Temperature Inversion, Young Star.\n\n**1 Introduction**  \n\nA key objective in understanding planet formation is to investigate the early stages of planet development when protoplanetary disks infiltrate young stellar systems. A crucial question is whether these disks evolve into planetary structures akin to our solar system. To address these inquiries, it is essential to analyze individual examples of young circumstellar disks as they evolve over time. However, most young galaxies are heavily obscured within dense molecular clouds, making direct observations of the inner regions of these disks challenging. \n\nFortunately, some young galaxies are surrounded by optically thin dusty envelopes that allow us to examine the physical conditions near the primary object via scattered light. These so-called transitional disks demonstrate evidence of significant debris clearing within a few astronomical units from the central star, while still containing considerable amounts of gas at greater distances (Strom et al., 1989; Skrutskie et al., 1990; Calvet et al., 2002; Muzerolle et al., 2003; Sicilia-Aguilar et al., 2006; Espaillat et al., 2007). Various studies suggest that the outer edges of transitional disks are shaped by photoevaporative winds created by intense ultraviolet radiation from nearby sources.",
        "ori-fast-z-score": -0.16666666666666666,
        "water-fast-z-score": 8.387593465227736,
        "rewrite-fast-z-score": 1.4316582658130823
    },
    {
        "original_text": "The ability to identify individual nanoparticles is essential for understanding their properties, but this task can be challenging due to the high degree of structural similarity between different types of particles. In this review we discuss recent advances in nanoparticle characterization using scanning transmission electron microscopy (STEM)-based techniques that provide information on both chemical composition and structure at atomic resolution. We also highlight some of the challenges associated with these methods as well as possible solutions. Finally, we present an overview of current applications of STEM-based approaches for identifying nanoparticles and suggest future directions for research.  Keywords: Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition. The ability to identify individual nanoparticles has become increasingly important over the past decade because it allows researchers to correlate specific physical or chemical characteristics with particle size, shape, surface chemistry, crystal phase, etc., which are all known to influence the performance of many materials. However, despite significant progress made during the last few years, there remains considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor. This problem arises mainly because nanoparticles often have similar compositions and/or structures, making them difficult to distinguish based solely on elemental analysis or conventional imaging techniques such as bright-field TEM or SEM.",
        "watermark_text": "The capacity to identify individual nanoparticles is crucial for studying their characteristics , but this job can be challenging due to the high degree of structural similarity between various types of molecules . In this review we explain latest advances in nanoparticle characterization utilizing scanning transmission electron microscopy ( STEM ) - based techniques that provide details on both chemical composition and shape at molecular resolution .We additionally discuss some of the challenges associated with these models as well as possible solutions . Finally , we present an overview of recent uses of STEM - based methods for finding nanoparticles and suggest current directions for research .Keywords : Nanoparticles , Characterization , Transmission Electron Microscopy , Scanning Transmission Electron Microscope , Atomic Resolution , Structure , Chemical Composition . The capacity to identify individual nanoparticles has become rapidly key over the previous decade because it allows researchers to correlate specific biological or chemical qualities with particle size , shape , surface chemistry , crystal phase , etc . , which are all known to affect the performance of several substances .However , despite considerable progress made during the last few years , there exists considerable uncertainty about how best to characterize nanoparticles by combining multiple experimental parameters into one single descriptor . This problem arises principally because nanoparticles often have related compositions and / or structures , making them harder to distinguish based primarily on elemental examination or conventional optical techniques such as bright - area TEM or SEM .",
        "rewrite_text": "The ability to identify individual nanoparticles is vital for investigating their properties, yet this task can be complex due to the significant structural similarities among different molecular types. In this review, we detail recent advancements in nanoparticle characterization through scanning transmission electron microscopy (STEM) techniques, which provide insights into both chemical composition and morphology at a molecular level. We also address some challenges associated with these methodologies and propose potential solutions. Additionally, we summarize recent applications of STEM-based approaches for nanoparticle detection and outline future research directions. \n\nKeywords: Nanoparticles, Characterization, Transmission Electron Microscopy, Scanning Transmission Electron Microscope, Atomic Resolution, Structure, Chemical Composition.\n\nOver the past decade, the ability to pinpoint individual nanoparticles has become increasingly essential, as it enables researchers to link specific biological or chemical properties with factors such as particle size, shape, surface chemistry, and crystal phase—all of which influence the efficacy of various materials. However, despite significant advancements in recent years, there remains considerable uncertainty regarding the optimal approach to characterize nanoparticles by integrating multiple experimental parameters into a single descriptor. This challenge largely stems from the fact that nanoparticles frequently exhibit similar compositions and/or structures, complicating their differentiation based solely on elemental analysis or conventional optical techniques like bright-field transmission electron microscopy (TEM) or scanning electron microscopy (SEM).",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 9.260129588726066,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "We study the elastic moduli and their fluctuations in dense, disordered packings of frictionless spheres by means of molecular dynamics simulations. We find that both shear and bulk modulus are strongly correlated with the local coordination number z(r) at contact points r between particles. The correlation function C(z1, z2) = ⟨K⟩δ z1 − z2  decays exponentially for large |z1 − z2|, where K denotes either the shear or the bulk modulus. This exponential decay can be explained within an effective medium theory which takes into account correlations among neighboring contacts. In addition to this short-range behavior we observe a power-law tail ∝ (z1 − z2)−2.5 for small values of |z1 − z|. These results suggest that the elastic moduli of disordered packings may be described as random variables whose probability distribution depends on the local packing geometry.",
        "watermark_text": "We explore the elastic moduli and their fluctuations in dense , disordered packings of frictionless spheres by means of molecular mechanics simulations . We see that both shear and bulk modulus are strongly correlated with the local coordination number z ( r ) at contact points r between particles .The correlation function C ( z1 , z2 ) = ⟨ K ⟩ δ z1 − z2 decays exponentially for large | z1 − z2 | , where K indicates either the shear or the bulk modulus . This exponential decay can be described within an efficient medium theory which gives into consideration correlations among neighboring contacts .In addition to this short - range behavior we observe a power - law tail [UNK] ( z1 − z2 ) −2 . 5 for small values of | z1 − z | . These results show that the elastic moduli of disordered packings may be described as random variables whose probability distribution depends on the local packing geometry .",
        "rewrite_text": "We investigate the elastic moduli and their variations in dense, disordered packings of frictionless spheres using molecular mechanics simulations. Our findings reveal that both the shear and bulk moduli are closely linked to the local coordination number \\( z(r) \\) at the contact points \\( r \\) between particles. The correlation function \\( C(z_1, z_2) = \\langle K \\rangle \\delta_{z_1 - z_2} \\) exhibits an exponential decay for large \\( |z_1 - z_2| \\), where \\( K \\) represents either the shear or bulk modulus. This exponential behavior can be effectively described by an efficient medium theory that accounts for correlations among neighboring contacts. Furthermore, we observe a power-law tail \\( \\sim (z_1 - z_2)^{-2.5} \\) for small values of \\( |z_1 - z_2| \\). These results indicate that the elastic moduli of disordered packings can be characterized as random variables, with their probability distribution influenced by the local packing geometry.",
        "ori-fast-z-score": 0.2581988897471611,
        "water-fast-z-score": 2.581988897471611,
        "rewrite-fast-z-score": 2.108590488016544
    },
    {
        "original_text": "We study the dynamics of string cosmologies with nontrivial dilaton potentials, focusing on their chaotic behavior. We show that for certain classes of potentials there are regions where trajectories can be trapped by unstable fixed points or periodic orbits. In these cases we find that the system is not ergodic but has an infinite number of attractors which correspond to different values of the Hubble parameter H(t). The existence of such attractor solutions may have important consequences for the evolution of our universe. For example, it could explain why the present value of H(t) differs so much from its initial value at t = 0. It also provides a possible explanation for the observed flatness problem since the volume V (t) grows exponentially fast during inflation while the energy density decreases as 1/V (t).\nThe results presented here were obtained using numerical methods based on the fourth-order Runge-Kutta algorithm combined with Newton s method for finding roots.",
        "watermark_text": "We research the dynamics of string cosmologies with nontrivial dilaton potentials , concentrating on their chaotic dynamics . We see that for particular categories of potentials there are areas where trajectories can be trapped by unstable fixed points or periodic orbits .In these circumstances we find that the system is not ergodic but has an endless number of attractors which belong to different values of the Hubble parameter H ( t ) . The existence of such attractor solutions may have important implications for the evolution of our universe .For instance , it could explain why the present value of H ( t ) changes so greatly from its initial value at t = 0 . It additionally offers a possible reason for the seen flatness problem since the volume V ( t ) rises exponentially rapidly during inflation while the electricity capacity reduces as 1 / V ( t ) .The results presented here were obtained using numerical technique based on the fourth - order Runge - Kutta algorithm coupled with Newton s method for finding roots .",
        "rewrite_text": "We investigate the dynamics of string cosmologies featuring complex dilaton potentials, with a focus on their chaotic behavior. Our studies reveal that certain classes of potentials exhibit regions where trajectories can become entrapped by unstable fixed points or periodic orbits. Under these conditions, the system is found to be non-ergodic, possessing an infinite number of attractors corresponding to various values of the Hubble parameter H(t). The presence of these attractor solutions may have significant implications for the evolution of our universe. For example, it could account for the considerable variation in the current value of H(t) compared to its initial value at t = 0. Additionally, this framework may provide an explanation for the observed flatness problem, as the volume V(t) increases exponentially during inflation while the energy density diminishes as 1/V(t). The findings presented here were achieved through numerical methods utilizing the fourth-order Runge-Kutta algorithm in conjunction with Newton's method for root-finding.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": 0.11704114719613057
    },
    {
        "original_text": "We present optical and near-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007) . The supernova is located at an unusually large distance for its host galaxy, with a recession velocity of ~1000 km/sec. \n \n We find that the light curve can be well fit using a model consisting of three components: shock breakout emission, radioactive decay powered luminosity, and dust extinction. Using this model we derive physical parameters such as the progenitor radius, mass loss rate, and explosion energy. Our results are consistent with those found for other type-II SNe but suggest that the progenitor star had a lower initial mass than previously thought. This may indicate that there exists more diversity among progenitors of type-II SNe than has been realized so far. In addition to these findings, our observations provide new insights into the physics of shock breakout and early-time evolution of type-II SNe.",
        "watermark_text": "We present visual and far - infrared photometry of SN 2006bp , which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett ( Puckett et al . , 2007 ) . The supernova is situated at an exceptionally wide distance for its host galaxy , with a collapse speeds of ~ 1000 cm / sec .We see that the light spiral can be well fitting using a description consisting of three components : shock breakout emission , radioactive decay powered luminosity , and dust extinction . Using this description we derive physical factors such as the progenitor diameter , mass loss rate , and explosion power .Our results are compatible with those detected for other class - II SNe but suggest that the progenitor star had a smaller original mass than previously thought . This might suggest that there exists more diversity among progenitors of type - II SNe than has been realized so far .In addition to these results , our observations offer additional perspectives into the physics of wave breakout and first - time evolution of type - II SNe .",
        "rewrite_text": "We present visual and far-infrared photometry of SN 2006bp, which was discovered on September 24th in NGC 6946 by amateur astronomer Brian Puckett (Puckett et al., 2007). This supernova is located at an unusually large distance from its host galaxy and is characterized by collapse speeds of approximately 1000 cm/sec. Our analysis reveals that the light curve can be accurately modeled with a combination of three components: shock breakout emission, luminosity powered by radioactive decay, and dust extinction. From this model, we derive key physical parameters including the progenitor's diameter, mass loss rate, and explosion energy. Our findings align with those of other class-II supernovae but indicate that the progenitor star may have had a lower original mass than previously assumed. This suggests a greater diversity among the progenitors of type-II supernovae than has been recognized. Furthermore, our observations provide new insights into the dynamics of wave breakout and the early evolution of type-II supernovae.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 1.4342743312012722
    },
    {
        "original_text": "We report water vapor (H2O) and silicon monoxide (SiO) maser emission toward the central star of the protoplanetary  nebula OH231.8+4.2, which is associated with an infrared source IRAS 18286-1231. The H2O masers are distributed over a region of ~0.1 arcsec diameter around the star at a velocity range of -40 to +20 km s-1 relative to the systemic velocity of the nebula. We detected SiO masers only on one side of the star within 0.05 arcsec radius at velocities ranging between -50 and -30 km s-1. These results suggest that the H2O masers trace shocked gas near the stellar surface while the SiO masers arise from outflowing material along the polar axis.  This work was supported by Grants-in-Aid for Scientific Research (No. 15740160)  from MEXT Japan.",
        "watermark_text": "We report wood vapor ( H2O ) and silicon monoxide ( SiO ) maser emission toward the main star of the protoplanetary nebula OH231 . 8 + 4 . 2 , which is associated with an infrared source IRAS 18286 - 1231 . The H2O masers are distributed over a region of ~ 0 . 1 arcsec diameter around the star at a speed range of - 40 to + 20 km s - 1 relative to the systemic speed of the nebula .We observed SiO masers only on one side of the star within 0 . 05 arcsec radius at velocities ranging between - 50 and - 30 kilometers s - 1 . These data suggest that the H2O masers trace shocked liquid near the stellar surface while the SiO masers occur from outflowing matter along the polar axis .This project was supported by Grants - in - Aid for Scientific Research ( No . 15740160 ) from MEXT Japan .",
        "rewrite_text": "We present observations of water vapor (H2O) and silicon monoxide (SiO) maser emissions emanating from the primary star of the protoplanetary nebula OH 231.8+4.2, which corresponds to the infrared source IRAS 18286-1231. The H2O masers are found within a region approximately 0.1 arcseconds in diameter around the star, displaying velocities that range from -40 to +20 km/s relative to the systemic speed of the nebula. In contrast, SiO masers were detected only on one side of the star within a radius of 0.05 arcseconds, with velocities between -50 and -30 km/s. This evidence indicates that the H2O masers are associated with shock-excited gas near the stellar surface, while the SiO masers originate from outflowing material along the polar axis. This research was funded by Grants-in-Aid for Scientific Research (No. 15740160) from MEXT Japan.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 3.794733192202055,
        "rewrite-fast-z-score": 1.0674899923282326
    },
    {
        "original_text": "We present results on the first phase of an unbiased survey for high-mass protostars (HMPSs) using the Red MSX Source (RMS) database and the NRAO 12m telescope at Kitt Peak Observatory, Arizona. The sample consists of all sources with infrared excesses that are associated with radio emission within the Galactic latitude range |b| < 5 degrees. We observed these candidates in the J=1-0 transition line of carbon monoxide (13CO), which is optically thin even towards dense cores. Our goal was to identify HMPSs by searching for compact molecular outflows traced by high-velocity wings in their 13CO spectra. In total we detected 16 HMPS candidates among our sample of 61 targets. These objects have luminosities between 10^6-10^7 Lsun and masses ranging from 8 Msun up to >100 Msun. They show evidence for bipolar outflow activity as well as infall motions.",
        "watermark_text": "We release results on the first phase of an unbiased survey for high - mass protostars ( HMPSs ) using the Red MSX Source ( RMS ) database and the NRAO 12m telescope at Kitt Peak Observatory , Arizona . The sample consists of all sources with infrared excesses that are related with radio emission within the Galactic latitude range | b | < 5 degrees .We observed these finalists in the J = 1 - 0 transition line of carbon monoxide ( 13CO ) , which is optically thin even towards dark cores . Our goal was to identify HMPSs by searching for compact molecular outflows traced by high - velocity wings in their 13CO spectra .In total we identified 16 HMPS candidates among our sample of 61 targets . These particles have luminosities between 10 ^ 6 - 10 ^ 7 Lsun and masses ranging from 8 Msun up to > 100 Msun .They show proof for bipolar outflow development as well as infall motions .",
        "rewrite_text": "We present findings from the initial phase of an unbiased survey focused on high-mass protostars (HMPSs), utilizing the Red MSX Source (RMS) database along with observations from the NRAO 12m telescope at Kitt Peak Observatory in Arizona. Our study includes all sources exhibiting infrared excesses associated with radio emissions, located within the Galactic latitude range of |b| < 5 degrees. We concentrated on the J = 1 - 0 transition line of carbon monoxide (13CO), which remains optically thin even in dark cores. The primary aim was to identify HMPSs by detecting compact molecular outflows characterized by high-velocity wings in their 13CO spectra. Overall, we identified 16 candidates for HMPSs from a total of 61 targets. These objects possess luminosities between 10^6 and 10^7 L☉ and masses ranging from 8 M☉ to more than 100 M☉. They provide evidence for the development of bipolar outflows as well as infall motions.",
        "ori-fast-z-score": 1.6641005886756874,
        "water-fast-z-score": 3.6055512754639896,
        "rewrite-fast-z-score": 0.674199862463242
    },
    {
        "original_text": "We present the results of long-term monitoring observations with the Rossi X-Ray Timing Explorer (RXTE) Proportional Counter Array and All-Sky Monitor for anomalous X-ray pulsar 4U0142+61, which is one of the most active accreting neutron stars in our Galaxy. The source was observed during more than 10 years between 1996 February and 2007 December. We found that the pulse period shows significant changes on time scales ranging from days to months. In addition, we detected several large-amplitude outbursts lasting up to 100 days each. These outbursts are characterized by an increase in both intensity and hardness ratio. During these outbursts, the spin-up rate increased significantly compared to those before or after them. This indicates that the magnetic field strength at the surface of the star increases when it undergoes such outbursts. \n \n Keywords: Accretion disk - Neutron star - Outburst",
        "watermark_text": "We publish the conclusion of long - term surveillance observations with the Rossi X - Ray Timing Explorer ( RXTE ) Proportional Counter Array and All - Sky Monitor for anomalous X - ray pulsar 4U0142 + 61 , which is one of the most intense accreting neutron galaxies in our Galaxy . The source was seen during more than 10 years between 1996 February and 2007 December .We showed that the pulse interval shows significant variations on time ranges varied from weeks to months . In addition , we spotted many wide - frequency outbursts lasting up to 100 days each .These outbursts are marked by an increase in both intensity and hardness factor . During these outbursts , the spin - up rate increased significantly compared to those before or after them .This implies that the magnetic force power at the surface of the star increases when it undergoes such outbursts . Keywords : Accretion plasma - Neutron star - Outburst",
        "rewrite_text": "We present the findings from a decade-long observation study conducted with the Rossi X-Ray Timing Explorer (RXTE) Proportional Counter Array and All-Sky Monitor focusing on the anomalous X-ray pulsar 4U 0142+61, one of the most powerful accreting neutron stars in our Galaxy. Our observations spanned more than ten years, from February 1996 to December 2007. We discovered that the pulse intervals exhibit significant fluctuations over time scales ranging from weeks to months. Additionally, we identified numerous wide-frequency outbursts, each lasting up to 100 days, characterized by increases in both intensity and hardness. Notably, during these outbursts, the spin-up rate of the pulsar significantly increased compared to the periods before and after, suggesting that the magnetic energy at the star's surface intensifies during such events. \nKeywords: Accretion, plasma, neutron star, outburst.",
        "ori-fast-z-score": -2.262741699796952,
        "water-fast-z-score": 3.857142857142857,
        "rewrite-fast-z-score": -0.7001400420140048
    },
    {
        "original_text": "We report on the fabrication and characterization of electro-optically tunable microresonator devices based on proton exchange (PE) grown lithium niobate-on-insulator substrates. The PE process is used to create an optical waveguide with low loss, high index contrast, and large nonlinearity within the substrate material. A ring-resonator geometry is then defined by electron-beam lithography followed by reactive ion etching. Finally, Ti/Au electrodes are deposited onto both sides of the device for electrical tuning. We demonstrate continuous wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device. This work represents one step towards realizing electrically-tuned integrated photonic circuits that can be monolithically fabricated on insulators. Lithium niobate has been widely studied as a promising candidate for optoelectronics applications due to its excellent properties such as wide transparency range, large second-order susceptibility, and relatively low propagation losses  1  . In addition, it also exhibits strong piezoelectric and pyroelectric effects which make it possible to achieve efficient electro-optic modulation  2  .\nIn this letter we present our recent results on the development of electro-optically tuned microring resonators made out of lithium niobate. These devices were designed and fabricated on commercially available lithium niobate wafers bonded to silicon dioxide  3  , where the top cladding layer was removed prior to processing. First, a proton-exchange (PE) process  4  was performed to grow a single-mode ridge-waveguide structure inside the bulk LiNbO 3 crystal  5  . Then, a ring-resonator geometry was patterned into the PE-grown region via electron beam lithography  6  . Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to provide electrical access to the device  7, 8  . Figure 1 shows scanning-electron-microscope images of two different types of microring resonators that have been successfully demonstrated so far. Both devices consist of",
        "watermark_text": "We report on the fabrication and identification of electro - optically tunable microresonator devices using on proton exchange ( PE ) grown lithium niobate - on - insulator substrates . The PE method is utilized to create an optical waveguide with little loss , low index contrast , and large nonlinearity within the substrate material .A ring - resonator configuration is then established by electron - laser lithography preceded by reactive ion etching . Finally , Ti / Au electrodes are deposited onto both sides of the device for electrical tuning .We demonstrate constant wavelength tuning over more than 30 nm at 1555 nm using only 1 V reverse bias applied across the device . This research provides one step towards developing electrically - tuned integrated photonic devices that can be monolithically manufactured on insulators .Lithium niobate has been widely explored as a desirable candidate for optoelectronics applications due to its exceptional properties such as wide clarity range , large second - order susceptibility , and fairly little propagation losses 1 . In addition , it also exhibits strong piezoelectric and pyroelectric influences which make it able to achieve effective electro - optic modulation 2 .In this letter we present our latest findings on the development of electro - optically modified microring resonators made out of lithium niobate . These systems were built and manufactured on commercially used lithium niobate wafers bonded to silicon dioxide 3 , where the bottom cladding surface was eliminated prior to processing .First , a proton - transfer ( PE ) process 4 was done to expand a single - mode ridge - waveguide structure inside the bulk LiNbO 3 crystal 5 . Then , a ring - resonator configuration was patterned into the PE - grown region via electron beam lithography 6 .Finally , titanium / gold ( Ti / Au ) contacts were evaporated onto both sides of the sample to provide electrical access to the device 7 , 8 . Figure 1 shows scan - electron - microscope photographs of two different kinds of microring resonators that have been successfully shown so far .Both devices consist of",
        "rewrite_text": "We present the fabrication and characterization of electro-optically tunable microresonator devices created using proton exchange (PE) on lithium niobate-on-insulator substrates. The PE technique enables the formation of an optical waveguide with minimal loss, low index contrast, and high nonlinearity within the substrate material. A ring-resonator structure is then constructed through electron-beam lithography followed by reactive ion etching. Subsequently, Ti/Au electrodes are deposited on both sides of the device to facilitate electrical tuning. We demonstrate consistent wavelength tuning exceeding 30 nm at 1555 nm using just a 1 V reverse bias across the device. This work marks a significant advancement toward the creation of electrically-tuned integrated photonic devices that can be monolithically fabricated on insulator substrates. Lithium niobate has been extensively studied as a prime candidate for optoelectronic applications due to its remarkable properties, including a wide transparency range, large second-order susceptibility, and minimal propagation losses. Furthermore, it exhibits strong piezoelectric and pyroelectric characteristics, enabling effective electro-optic modulation. In this letter, we share our latest results on the development of electro-optically modified microring resonators made from lithium niobate, fabricated on commercially available lithium niobate wafers bonded to silicon dioxide, with the bottom cladding layer removed prior to processing. Initially, a proton-exchange process was performed to create a single-mode ridge-waveguide structure within the bulk LiNbO3 crystal. Following this, a ring-resonator configuration was patterned into the PE-grown area using electron beam lithography. Finally, titanium/gold (Ti/Au) contacts were evaporated onto both sides of the sample to enable electrical access to the device. Figure 1 displays scanning electron microscope images of two distinct types of microring resonators that have been successfully fabricated thus far. Both devices consist of...",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 8.172062695283987,
        "rewrite-fast-z-score": 2.7240208984279954
    },
    {
        "original_text": "We study the possibility that leptons acquire their masses through an effective dimension-5 operator, which is suppressed by some heavy scale M . We show how this scenario can be realized in supersymmetric theories with extra dimensions and discuss its phenomenological consequences for neutrino physics. \n \n In particular we consider the case where the lepton number violating interactions are localized on a brane at y = 0 while the Standard Model fields live in the bulk. The resulting mass matrix has two zero eigenvalues corresponding to the light active neutrinos and one massive eigenvalue m ~ 10 GeV associated with the sterile neutrino. This leads to interesting predictions for neutrinoless double beta decay as well as for cosmology. For example, if the Majorana phase vanishes then there will be no contribution to neutrinoless double-beta decay but the sterile neutrino may still contribute significantly to dark matter production during inflation or reheating.",
        "watermark_text": "We explore the prospect that leptons gain their masses through an efficient dimension - 5 operator , which is suppressed by some heavy scale M . We see how this situation can be realized in supersymmetric theories with extra dimensions and consider its phenomenological consequences for neutrino physics .In particular we investigate the case where the lepton number violating interactions are localized on a brane at y = 0 while the Standard Model fields reside in the bulk . The resulting mass vector has two zero eigenvalues corresponding to the light active neutrinos and one massive eigenvalue m ~ 10 GeV associated with the sterile neutrino .This leads to useful predictions for neutrinoless double β decay as well as for cosmology . For instance , if the Majorana phase vanishes then there will be no contribution to neutrinoless single - beta decay but the sterile neutrino may still reduce considerably to dark matter production during inflation or reheating .",
        "rewrite_text": "We investigate the possibility that leptons acquire their masses via an efficient dimension-5 operator, which is suppressed by a high energy scale M. This scenario can be realized within supersymmetric theories that include extra dimensions, and we analyze its implications for neutrino physics. Specifically, we consider the situation where lepton number-violating interactions occur on a brane at y = 0, while Standard Model fields exist in the bulk. The resulting mass matrix yields two zero eigenvalues corresponding to the light active neutrinos, and one massive eigenvalue of approximately m ~ 10 GeV related to the sterile neutrino. This framework leads to significant predictions for neutrinoless double beta decay and cosmological implications. For example, if the Majorana phase is zero, there will be no contribution to neutrinoless single beta decay; however, the sterile neutrino could still play a crucial role in diminishing dark matter production during inflation or reheating.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 3.7754784184438925,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "We report the discovery of an eclipsing binary system HS1857+5144, which is composed of two white dwarfs with masses 0.6 M⊙and 0.8M⊙respectively. The orbital period of this system is 1.5 hours. We find that the mass ratio q = M2/M1=0.84 ±0.04 for this system by using the method of spectral disentangling. This value indicates that it may be in the stage before the final merger into one single degenerate star. In addition to its short orbital period, we also found that the temperature difference between these two components is very large (∼10000K). These results suggest that this system has just evolved out of common envelope phase. \n \n Keywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling",
        "watermark_text": "We report the discovery of an eclipsing binary system HS1857 + 5144 , which is composed of two white dwarfs with masses 0 . 6 [UNK] 0 . [UNK] . The orbital period of this system is 1 . 5 hours .We see that the mass ratio q = M2 / M1 = 0 . 84 ±0 . 04 for this system by using the method of spectral disentangling . This value indicates that it could be in the stage before the last collapse into one single degenerate star .In addition to its short orbital period , we also found that the temperature difference between these two components is very large ( [UNK] ) . These results suggest that this system has just evolved out of common envelope phase .Keywords : Hot subdwarf B stars ; Eclipsing ; Pre - Cataclysmic Variables ; White Dwarfs ; Spectral Disentangling",
        "rewrite_text": "We are pleased to announce the discovery of the eclipsing binary system HS1857 + 5144, which consists of two white dwarfs with masses of approximately 0.6 and 0. **[UNK]**. The system has an orbital period of 1.5 hours. By employing the technique of spectral disentangling, we have determined a mass ratio of q = M2 / M1 = 0.84 ± 0.04 for this system. This mass ratio suggests that it is approaching the final phase before merging into a single degenerate star. In addition to its brief orbital period, we observed a significant temperature disparity between the two components, indicated by **[UNK]**. These findings imply that the system has recently emerged from the common envelope phase. \n\nKeywords: Hot subdwarf B stars; Eclipsing; Pre-Cataclysmic Variables; White Dwarfs; Spectral Disentangling.",
        "ori-fast-z-score": 0.5773502691896258,
        "water-fast-z-score": 1.3127849234810511,
        "rewrite-fast-z-score": -0.14285714285714285
    },
    {
        "original_text": "We present new results on the properties of X-ray selected, optically bright (MAB <-21) galaxies in the CDF-S field with spectroscopic redshifts between 1<z<3.5. We use deep Chandra data to identify sources that are obscured by column densities NH>1022 cm-2 as well as those which have intrinsic luminosities above 1043 erg s-1 Hz-1. The majority of these objects show evidence for being heavily absorbed active galactic nuclei (AGNs). Using Spitzer mid-infrared spectroscopy we find that most of our sample is dominated by star formation activity rather than an old stellar population. In addition, we detect strong Fe Kα emission lines in many of the obscured AGNs indicating high accretion rates onto supermassive black holes. By combining optical and infrared photometry with SED modeling techniques we derive accurate estimates of the total stellar mass content within each galaxy. Our analysis shows that there exists a tight correlation between the masses of central supermassive black holes and their host spheroids over more than four orders of magnitude in BH mass.",
        "watermark_text": "We report new data on the properties of X - ray selected , optically bright ( MAB < - 21 ) clusters in the CDF - S field with spectroscopic redshifts between 1 < z < 3 . 5 . We use deep Chandra data to identify sources that are blurred by column densities NH > 1022 centimetres - 2 as well as those which have intrinsic luminosities above 1043 erg s - 1 Hz - 1 .The majority of these objects show proof for being greatly absorbed active galactic nuclei ( AGNs ) . Using Spitzer mid - infrared spectroscopy we find that most of our sample is dominated by galaxy formation activity rather than an old stellar community .In addition , we perceive strong Fe Kα emission lines in many of the obscured AGNs suggesting high accretion levels onto supermassive black holes . By combining optical and infrared photometry with SED modeling strategies we derive exact predictions of the total stellar mass content within each constellation .Our study shows that there exists a tight correlation between the masses of central supermassive black holes and their host spheroids over more than four orders of magnitude in BH weight .",
        "rewrite_text": "We present new findings on the characteristics of X-ray selected, optically luminous (MAB < -21) clusters in the CDF-S field, with spectroscopic redshifts in the range of 1 < z < 3.5. Utilizing extensive Chandra data, we identify sources that exhibit column densities NH > 10^22 cm^-2, as well as those with intrinsic luminosities exceeding 10^43 erg s^-1 Hz^-1. Most of these objects show evidence of being heavily absorbed active galactic nuclei (AGNs). Through Spitzer mid-infrared spectroscopy, we discover that the majority of our sample is primarily influenced by galaxy formation activity rather than by an older stellar population. We also observe strong Fe Kα emission lines in many obscured AGNs, indicating significant accretion onto supermassive black holes. By integrating optical and infrared photometry with spectral energy distribution (SED) modeling techniques, we accurately estimate the total stellar mass within each cluster. Our research reveals a strong correlation between the masses of central supermassive black holes and their host spheroids, spanning more than four orders of magnitude in black hole mass.",
        "ori-fast-z-score": 1.585187847802434,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We consider sparse multipath channels with random delays, where each path is modeled as an independent Rayleigh fading channel. We derive exact expressions for ergodic capacity and outage probability over such channels under wideband regime (i.e., when bandwidth grows without bound). Our results show that both ergodic capacity and outages are determined by the number of paths N , their average power Pn = E{|hn(t)|2}, and delay spread T . In particular, we find that ergodic capacity scales linearly with bandwidth W at high SNR if N > 2W/πT or N < πT /4W; otherwise it saturates to a constant value. Moreover, our analysis shows that outage probability decays exponentially fast with increasing bandwidth W if N > 4W/πT ; otherwise it decreases only polynomially. Finally, numerical examples illustrate how these scaling laws depend on various system parameters.",
        "watermark_text": "We consider sparse multipath channels with random delays , where each track is modeled as an free Rayleigh fading signal . We derive exact expressions for ergodic strength and outage likelihood over such channels under wideband regime ( i . e . , when bandwidth grows without bound ) .Our results show that both ergodic strength and outages are decided by the number of paths N , their average power Pn = E { | hn ( t ) | 2 } , and delay spread T . In particular , we find that ergodic density scales linearly with capacity W at high SNR if N > 2W / πT or N < πT / 4W ; otherwise it saturates to a constant value .Moreover , our analysis shows that outage likelihood decays exponentially rapidly with expanding bandwidth W if N > 4W / πT ; otherwise it decreases only polynomially . Finally , numerical examples explain how these scaling laws depend on various system parameters .",
        "rewrite_text": "We examine sparse multipath channels characterized by random delays, where each path is represented by a free Rayleigh fading signal. We derive precise expressions for both ergodic capacity and outage probability in these channels within a wideband framework (i.e., as the bandwidth approaches infinity). Our findings indicate that the ergodic capacity and the likelihood of outages are influenced by the number of paths \\( N \\), the average power \\( P_n = E\\{ | h_n(t) |^2 \\} \\), and the delay spread \\( T \\). Specifically, we discover that the ergodic capacity scales linearly with bandwidth \\( W \\) at high signal-to-noise ratios (SNR) when \\( N > \\frac{2W}{\\pi T} \\) or \\( N < \\frac{\\pi T}{4W} \\); otherwise, it reaches a saturation point. Furthermore, our analysis reveals that the probability of outage decreases exponentially with increasing bandwidth \\( W \\) when \\( N > \\frac{4W}{\\pi T} \\); in contrast, it decreases at a polynomial rate otherwise. Finally, numerical simulations illustrate how these scaling behaviors are influenced by various system parameters.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 4.370956778314644,
        "rewrite-fast-z-score": -0.9847319278346618
    },
    {
        "original_text": "We report on electrical transport measurements through individual biphenyl dithiol molecules connected to gold electrodes by using scanning tunneling microscopy break-junction technique at room-temperature (RT) and low temperatures down to 4 K. The current–voltage characteristics show clear signatures for molecular conduction, such as Coulomb blockade peaks and negative differential resistance regions. We find that the junction conductance depends strongly on the tilt angle between the molecule backbone and the substrate normal. This is explained by an anisotropic coupling strength between the molecule and the metal contacts which leads to different transmission probabilities along the two main axes of the molecule. In addition we observe a strong temperature dependence of the junction conductance with a maximum around 50 K. These results are discussed within a model based on electron-phonon scattering processes. Biphenyl dithiol (BDT), one of the most studied organic semiconductors, has been used extensively in field-effect transistors1-5 and other electronic devices6-8 due to its high charge carrier mobility9-11. However, despite intensive research efforts12-17 only few studies have investigated the electrical properties of BDT-based single-molecule junctures18-20. Herein, we present detailed investigations of the electrical transport behavior of individual BDT molecules contacted via Au(111) surfaces by means of scanning tunneling microscopy breakjunction experiments21-23. Our data reveal pronounced features characteristic for molecular conduction24-26 including Coulomb blockade peaks27-29 and negative differential resistance30-32. Furthermore, our study reveals a strong influence of the molecular orientation relative to the substrate surface on the junction conductance33-35. Finally, we demonstrate a significant temperature dependent change of the junction conductance36-38.",
        "watermark_text": "We report on electrical travel data through individual biphenyl dithiol molecules linked to gold electrodes by using scan tunneling microscopy break - junction technique at room - temperature ( RT ) and low temperatures down to 4 K . The current – voltage parameters show visible signatures for protein conduction , such as Coulomb blockade spikes and negative integral resistance regions . We see that the junction conductance depends strongly on the tilt distance between the molecule backbone and the substrate normal .This is explained by an anisotropic interaction strength between the molecule and the metal contacts which results to different communication probabilities along the two principal axes of the molecule . In addition we study a weak thermal influence of the junction conductance with a maximum around 50 K . These data are discussed within a theory based on electron - phonon absorption processes .Biphenyl dithiol ( BDT ) , one of the most studied synthetic semiconductors , has been used heavily in field - effect transistors1 - 5 and other electronic devices6 - 8 due to its high charge carrier mobility9 - 11 . However , despite intensive study efforts12 - 17 only few researchers have researched the electrical properties of BDT - based multi - molecule junctures18 - 20 .Herein , we present detailed investigations of the electrical travel dynamics of individual BDT atoms contacted via Au ( 111 ) surfaces by means of scanning tunneling microscopy breakjunction experiments21 - 23 . Our data reveal pronounced features characteristic for molecular conduction24 - 26 including Coulomb blockade peaks27 - 29 and negative differential resistance30 - 32 .Furthermore , our research reveals a powerful impact of the molecular orientation compared to the substrate surface on the junction conductance33 - 35 . Finally , we obtain a substantial heat dependent change of the junction conductance36 - 38 .",
        "rewrite_text": "We present findings on the electrical transport properties of individual biphenyl dithiol (BDT) molecules connected to gold electrodes, utilizing the scanning tunneling microscopy break-junction technique at room temperature (RT) and at low temperatures down to 4 K. The current-voltage characteristics exhibit distinct indicators of protein-like conduction, such as Coulomb blockade spikes and regions of negative differential resistance. Our observations indicate that the conductance of the junction is significantly influenced by the tilt distance between the molecular backbone and the normal of the substrate. This dependency can be attributed to the anisotropic interaction strength between the molecule and the metal contacts, leading to varying communication probabilities along the two primary axes of the molecule. Additionally, we examine the minor thermal effects on junction conductance, with a notable peak around 50 K. These findings are contextualized within a theoretical framework based on electron-phonon interaction processes. BDT, a widely studied synthetic semiconductor, has been extensively used in field-effect transistors and other electronic devices due to its high charge carrier mobility. However, despite significant research efforts, only a limited number of studies have focused on the electrical properties of BDT-based multi-molecule junctions. In this work, we provide a comprehensive analysis of the electrical transport dynamics of individual BDT molecules interfaced with Au (111) surfaces, supported by scanning tunneling microscopy break-junction experiments. Our results reveal clear signatures typical of molecular conduction, including Coulomb blockade features and negative differential resistance. Furthermore, our investigation demonstrates the substantial influence of the molecular orientation in relation to the substrate surface on junction conductance. Finally, we observe significant heat-dependent variations in the junction conductance.",
        "ori-fast-z-score": 1.4253932901995967,
        "water-fast-z-score": 7.839663096097782,
        "rewrite-fast-z-score": -0.939793423488437
    },
    {
        "original_text": "We present new calculations for the radiative transfer in stellar winds, including the effects of line-blanketing and clumping on the wind structure. We find that neglecting the porosity of the wind leads to an overestimate of the mass loss rate by up to two orders of magnitude compared with our results. This is because the density contrast between the dense clumps and the interclump medium increases as one moves out into the wind, leading to more absorption at large distances than would be expected if the wind were smooth. The effect becomes stronger when the wind velocity decreases or the metallicity increases. \n \n Keywords: Stellar winds, Radiation hydrodynamics, Line-driven winds, Porosity, Clumping \n \n Hot stars lose mass through their stellar winds driven by radiation pressure on spectral lines (Castor et al., 1975; Abbott & Lucy, 1985) . In order to understand how these winds are structured it is necessary to solve the coupled equations of radiative transfer and fluid dynamics simultaneously. However, this problem has proved extremely difficult to solve numerically due to its multi-scale nature - both spatially and temporally - which requires very high resolution grids to resolve all relevant scales correctly. As such, most previous studies have used simplified treatments of either the radiative transfer or the fluid dynamics, but not both together. For example, some authors assume that the wind consists entirely of optically thin gas (e.g. Friend & Castor, 1983) while others use simple prescriptions for the radial dependence of the optical depth (e.g. Pauldrach et al., 1986) , or even ignore the effects of opacity altogether (e.g. Lamers & Cassinelli, 1999 ) . Other authors make simplifying assumptions about the flow itself, e.g. assuming spherical symmetry (e.g. Puls et al., 1996 ) , steady state (e.g. Owocki et al. , 1988 ) and/or stationarity (e.g. Runacres & Owocki , 2002 ) . \n \n Here we present new calculations for the structure of line-driven winds",
        "watermark_text": "We present new models for the radiative transfer in stellar winds , particularly the effects of line - blanketing and clumping on the wind structure . We see that neglecting the porosity of the wind leads to an overestimate of the mass loss rate by up to two orders of magnitude compared with our findings .This is because the density contrast between the sparse clumps and the interclump medium tends as one moves out into the wind , leading to more scattering at large distances than would be anticipated if the wind were smooth . The phenomenon grows stronger when the wind velocity reduces or the metallicity increases .Keywords : Stellar storms , Radiation hydrodynamics , Line - powered winds , Porosity , Clumping Hot objects losing mass through their stellar winds driven by radiation stress on spectral lines ( Castor et al . , 1975 ; Abbott & Lucy , 1985 ) . In order to comprehend how these storms are structured it is required to solve the coupled equations of radiative transfer and fluid dynamics simultaneously .However , this question has become highly hard to overcome numerically due to its multi - scale nature - both spatially and temporally - which requires very high resolution grids to resolution all relevant dimensions correctly . As such , most prior studies have utilized simplified solutions of either the radiative transfer or the liquid mechanics , but not both together .For instance , some writers suppose that the wind consists entirely of optically thin gas ( e . g . Friend & Castor , 1983 ) while many use simple prescriptions for the radial dependence of the optical thickness ( e . g .Pauldrach et al . , 1986 ) , or even ignore the effects of opacity altogether ( e . g . Lamers & Cassinelli , 1999 ) .Other scientists make simplifying statements about the flow itself , e . g . assuming spherical symmetry ( e . g .Puls et al . , 1996 ) , steady state ( e . g . Owocki et al ., 1988 ) and / or stationarity ( e . g . Runacres & Owocki , 2002 ) .Here we present new values for the composition of line - coupled winds",
        "rewrite_text": "We introduce new models for radiative transfer in stellar winds, specifically examining the impact of line blanketing and clumping on wind structure. Our findings indicate that failing to account for the wind's porosity can lead to an overestimation of the mass loss rate by as much as two orders of magnitude. This discrepancy arises because the density contrast between the scattered clumps and the interclump medium increases with distance from the star, resulting in greater scattering at large distances than would be expected in a smooth wind. This effect becomes more pronounced as wind velocity decreases or metallicity increases.\n\nKeywords: Stellar storms, Radiation hydrodynamics, Line-powered winds, Porosity, Clumping. Hot objects lose mass through their stellar winds, driven by radiation pressure on spectral lines (Castor et al., 1975; Abbott & Lucy, 1985). To understand the structure of these winds, it is essential to simultaneously solve the coupled equations of radiative transfer and fluid dynamics. However, due to the multi-scale nature of the problem—both spatially and temporally—numerical solutions have proven challenging, requiring high-resolution grids to accurately capture all relevant dimensions. Consequently, previous studies often relied on simplified models of either radiative transfer or fluid mechanics, rather than addressing them together. For example, some researchers have assumed the wind consists solely of optically thin gas (e.g., Friend & Castor, 1983), while others have employed basic approximations for the radial dependence of optical thickness (e.g., Pauldrach et al., 1986), or have disregarded opacity effects entirely (e.g., Lamers & Cassinelli, 1999). Additionally, some scientists have made assumptions about the flow itself, such as maintaining spherical symmetry (e.g., Puls et al., 1996), steady state (e.g., Owocki et al., 1988), and/or stationarity (e.g., Runacres & Owocki, 2002). In this work, we present new insights into the composition of line-coupled winds.",
        "ori-fast-z-score": -0.7474093186836597,
        "water-fast-z-score": 6.6530001764255,
        "rewrite-fast-z-score": -0.08137884587711594
    },
    {
        "original_text": "We present an absolute calibration of MIPS photometry at 24, 70, and 160 microns using stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. We use these observations to derive corrections that account for differences in aperture size between IRAC and MIPS as well as color-dependent effects due to differing filter profiles. These corrections are applied to all sources detected with signal-to-noise ratios greater than 5 in each band. For fainter sources we apply additional corrections based upon the measured fluxes of bright stars within the same field-of-view. This method is used to calibrate over 1 million objects across the sky. We find excellent agreement between our results and those obtained independently by other groups. Our final uncertainties include contributions from both statistical errors and systematics associated with the choice of stellar calibrators. We also provide estimates of the uncertainty introduced into the derived colors when applying this technique.",
        "watermark_text": "We present an absolute calibration of MIPS photometry at 24 , 70 , and 160 microns using stellar calibrators observed by the Infrared Array Camera ( IRAC ) onboard the Spitzer Space Telescope . We use these observations to derive corrections that explain for variations in aperture size between IRAC and MIPS as well as color - based effects due to distinct filter profiles .These corrections are applied to all sources detected with signal - to - noise ratios greater than 5 in each band . For fainter sources we apply additional corrections based upon the determined fluxes of bright stars within the same field - of - view .This method is utilized to calibrate over 1 million items across the sky . We see better agreement between our findings and those achieved independently by other organizations .Our last uncertainties include contributions from both statistical mistakes and systematics associated with the selection of stars calibrators . We additionally offer estimates of the uncertainty introduced into the derived colors when applying this methodology .",
        "rewrite_text": "We provide an absolute calibration of MIPS photometry at 24, 70, and 160 microns, utilizing stellar calibrators observed by the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. These observations allow us to derive corrections that account for variations in aperture size between IRAC and MIPS, as well as color-related effects stemming from different filter profiles. We apply these corrections to all sources detected with signal-to-noise ratios exceeding 5 in each band. For weaker sources, we implement additional corrections based on the fluxes of brighter stars within the same field of view. This approach enables the calibration of over 1 million objects throughout the sky. Our findings show improved agreement with results obtained independently by other organizations. The final uncertainties we report encompass both statistical errors and systematic issues related to the selection of stellar calibrators. Additionally, we provide estimates of the uncertainty introduced in the derived colors when applying this methodology.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 6.077701994871215,
        "rewrite-fast-z-score": 0.4923659639173309
    },
    {
        "original_text": "The fabrication, structure and properties of uranium oxide (UO 2 )/uranium nitride (UN) superlattices are reported in this work. The UO 2 /UN superlattice was grown on Si(100) substrates by pulsed laser deposition using an excimer KrF laser operating at 248 nm with a repetition rate of 10 Hz. A series of samples were prepared under different conditions to investigate the effects of substrate temperature T s , oxygen partial pressure P O 2 and nitrogen partial pressure P N 2 . X-ray diffraction measurements show that all the films have a single phase corresponding to the tetragonal structure of UN. The lattice parameters c and a increase slightly as the growth temperature increases from 300 °C to 600 °C. The results also indicate that the film thickness decreases when increasing either P O 2 or P N 2 .\nTransmission electron microscopy shows that the interface between the two layers is sharp without any interfacial layer.",
        "watermark_text": "The fabrication , structure and properties of uranium alloy ( UO 2 ) / uranium nitride ( UN ) superlattices are observed in this project . The UO 2 / UN superlattice was grown on Si ( 100 ) substrates by pulsed laser extraction using an excimer KrF laser running at 248 nm with a repetition rate of 10 Hz .A series of samples were prepared under various circumstances to examine the effects of substrate elevation T s , oxygen partial pressure P O 2 and oxygen partial pressure P N 2 . X - ray diffraction measurements show that all the films have a single phase relating to the tetragonal shape of UN .The lattice parameters c and a increase somewhat as the growth temperature increases from 300 °C to 600 °C . The results also suggest that the film thickness decreases when varying either P O 2 or P N 2 .Transmission electron microscopy demonstrates that the interface between the two layers is sharp without any interfacial layer .",
        "rewrite_text": "This project investigates the fabrication, structure, and properties of uranium alloy (UO2) and uranium nitride (UN) superlattices. The UO2/UN superlattice was grown on Si (100) substrates through pulsed laser deposition, utilizing a KrF excimer laser operating at 248 nm with a repetition rate of 10 Hz. A series of samples were created under different conditions to analyze the impact of substrate temperature (Ts), oxygen partial pressure (PO2), and nitrogen partial pressure (PN2). X-ray diffraction measurements reveal that all films exhibit a single phase corresponding to the tetragonal structure of UN. It was observed that the lattice parameters 'c' and 'a' slightly increase as the growth temperature rises from 300 °C to 600 °C. Additionally, the findings indicate that film thickness decreases with variations in either PO2 or PN2. Transmission electron microscopy confirms that the interface between the two layers is sharp, with no interfacial layer present.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": -0.7385489458759964
    },
    {
        "original_text": "The Megapie (Megavoltage Ionization Projection Imaging Experiment) is an experiment designed to study the feasibility and performance of proton radiography for medical applications. The main goal of this project was to develop a compact, high intensity ion source based on laser-plasma interaction in order to produce protons with energies up to several hundred MeV. In addition, it has been shown that such sources can be used as targets for neutron production by spallation reactions induced by energetic ions. This work presents results obtained during experiments performed at GSI Darmstadt using a pulsed deuteron beam accelerated by the SIS-18 synchrotron accelerator. Neutrons produced by the D+D reaction were detected by means of two fission chambers placed around the target chamber. A detailed analysis of these data allowed us to determine the number of neutrons emitted per incident deuteron particle and their energy distribution.",
        "watermark_text": "The Megapie ( Megavoltage Ionization Projection Imaging Experiment ) is an project meant to study the feasibility and performance of proton radiography for medical uses . The main goal of this project was to develop a compact , large intensity particle channel relying on laser - plasma interaction in order to produce protons with intensity up to several hundred MeV .In addition , it has been shown that such sources can be used as targets for neutron production by spallation reactions generated by energetic ions . This research provides findings obtained during experiments conducted at GSI Darmstadt involving a pulsed deuteron light accelerated by the SIS - 18 synchrotron accelerator .Neutrons created by the D + D process were detected by means of two fission chambers put around the target chamber . A precise analysis of these information permitted us to predict the quantity of neutrons emitted per incident deuteron particle and their power distribution .",
        "rewrite_text": "The Megapie (Megavoltage Ionization Projection Imaging Experiment) project aims to investigate the feasibility and performance of proton radiography for medical applications. The primary objective of this initiative is to develop a compact, high-intensity particle channel based on laser-plasma interactions to generate protons with energies reaching several hundred MeV. Furthermore, it has been demonstrated that such proton sources can serve as targets for neutron production through spallation reactions triggered by energetic ions. This research presents findings from experiments conducted at GSI Darmstadt, utilizing a pulsed deuteron beam accelerated by the SIS-18 synchrotron. Neutrons produced through the D + D reaction were detected using two fission chambers positioned around the target chamber. A detailed analysis of this data enabled us to predict the number of neutrons emitted per incident deuteron and their energy distribution.",
        "ori-fast-z-score": 0.762000762001143,
        "water-fast-z-score": 7.112007112010668,
        "rewrite-fast-z-score": 1.2909944487358056
    },
    {
        "original_text": "We report on observations made in March and April 2002 at 345 GHz using the Atacama Submillimiter Telescope Experiment (ASTE). We detected no significant emission associated with the host galaxy of the gamma-ray burst GRB 980425, which was discovered by BeppoSAX on Apr. 25, 1998 . The 3 sigma upper limit to the flux density is < 0.5 Jy beam-1 , corresponding to an integrated line intensity ICO< 1 x 10^9 K km s^-1 pc^2 or Mgas < 2 x 10^8 Msun .\nThe non-detection suggests that either the molecular gas mass is much lower than previously estimated or that it has been destroyed by the intense UV radiation field produced during the burst.  These results are consistent with those obtained recently by other groups who have searched for CO emission from this source. If our result can be confirmed then it will provide important constraints on models of the formation of massive stars.",
        "watermark_text": "We report on observations made in March and April 2002 at 345 GHz use the Atacama Submillimiter Telescope Experiment ( ASTE ) . We observed no major radiation associated with the target galaxy of the gamma - ray burst GRB 980425 , which was discovered by BeppoSAX on Apr .25 , 1998 . The 3 sigma higher limit to the flux concentration is < 0 . 5 Jy beam - 1 , equivalent to an integrated line intensity ICO < 1 x 10 ^ 9 K km s ^ - 1 pc ^ 2 or Mgas < 2 x 10 ^ 8 Msun .The non - discovery suggests that either the molecular gas mass is much lower than previously predicted or that it has been destroyed by the strong UV rays field produced during the explosion . These data are compatible with those acquired previously by other groups who have searched for CO radiation from this source .If our outcome can be verified then it will provide important restrictions on estimates of the formation of large stars .",
        "rewrite_text": "We present observations conducted in March and April 2002 at 345 GHz using the Atacama Submillimeter Telescope Experiment (ASTE). Our findings indicate that no significant radiation was detected from the target galaxy of the gamma-ray burst GRB 980425, which was identified by BeppoSAX on April 25, 1998. The 3-sigma upper limit for the flux density is < 0.5 Jy beam^-1, which corresponds to an integrated line intensity ICO < 1 x 10^9 K km s^-1 pc^2, or a molecular gas mass Mgas < 2 x 10^8 M☉. The lack of detection implies that the molecular gas mass may be considerably lower than earlier estimates, or it may have been destroyed by the intense UV radiation generated by the explosion. Our results align with earlier data obtained by other research teams investigating CO radiation from this source. If these findings are confirmed, they will impose significant constraints on models of large star formation.",
        "ori-fast-z-score": -1.9867985355975657,
        "water-fast-z-score": 4.106050306901635,
        "rewrite-fast-z-score": -2.5
    },
    {
        "original_text": "We present the results for the dynamics of baryons in terms of string theory, vector dominance model (VDM) and chiral perturbation theory (ChPT). The VDM is used to calculate the mass spectrum of hadrons with one heavy quark Q = c or b. In this approach we have calculated masses of charmonium states J/ψ , ψ′ , Υ and their radial excitations as well as bottomonium states Υ(1S), Υ(2S), Υ(3S), Υ(4S) and their radial excitations using potential parameters determined by fitting experimental data on meson masses. We also predict the masses of some exotic states like X(3872), Y(4260), Zc(3900) etc., which are not yet observed experimentally. Using these masses we then determine the decay constants of various excited states of quarks into leptons. These decay constants are compared with those obtained within ChPT at leading order.",
        "watermark_text": "We present the results for the dynamics of baryons in terms of string theory , tensor dominance description ( VDM ) and chiral perturbation theory ( ChPT ) . The VDM is utilized to estimate the mass spectrum of hadrons with one heavy quark Q = b or c .In this methodology we have predicted masses of charmonium states J / ψ , ψ ′ , [UNK] and their radial excitations as also as bottomonium states [UNK] ( 1S ) , [UNK] ( 2S ) , [UNK] ( 3S ) , [UNK] ( 4S ) and their radial excitations using potential parameters calculated by fitting experimental records on meson masses . We additionally calculate the masses of some exotic states like X ( 3872 ) , Y ( 4260 ) , Zc ( 3900 ) etc . , which are not already detected experimentally .Using these masses we then calculate the decay constants of several excited states of quarks into leptons . These decay constants are compared with those generated within ChPT at leading order .",
        "rewrite_text": "We present our findings on baryon dynamics through the lenses of string theory, tensor dominance (VDM), and chiral perturbation theory (ChPT). VDM is applied to estimate the mass spectrum of hadrons containing a single heavy quark, either Q = b or c. Using this approach, we have predicted the masses of charmonium states such as J/ψ, ψ′, and their radial excitations, as well as bottomonium states like [UNK] (1S), [UNK] (2S), [UNK] (3S), [UNK] (4S) and their corresponding radial excitations. These predictions are based on potential parameters derived from fitting to experimental meson mass data. We also calculate the masses of several exotic states, including X(3872), Y(4260), and Zc(3900), which have yet to be experimentally confirmed. Using these mass estimates, we calculate the decay constants for various excited quark states into leptons and compare these values with those obtained from ChPT at leading order.",
        "ori-fast-z-score": -2.0604084592303353,
        "water-fast-z-score": 2.1773242158072694,
        "rewrite-fast-z-score": 0.9113223768657671
    },
    {
        "original_text": "We present the results of our Spitzer Space Telescope survey for protostars and young stellar objects (YSOs) in three nearby, intermediate mass star-forming regions: NGC 1333, Serpens South, and Perseus North. We identify over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and/or envelopes. The majority are Class I sources that have recently formed outflows or jets; however we also find several dozen more evolved Class II/III sources. In addition to these disk-bearing systems, we detect numerous isolated point-like sources whose SEDs suggest they are deeply embedded protostars. These observations provide new insights into how stars form in IM environments. Our sample includes many previously unidentified low-luminosity protostars which will be useful targets for future studies at higher angular resolution. This work is based on observations made with the Spitzer Space Telescope, which is operated by NASA under contract 1407. Support for this work was provided by NASA through an award issued by JPL/Caltech. \n \n Keywords: Protostar",
        "watermark_text": "We publish the conclusion of our Spitzer Space Telescope survey for protostars and young stellar bodies ( YSOs ) in three nearby , intermediate mass star - creating areas : NGC 1333 , Serpens South , and Perseus North . We distinguish over 100 candidate YSOs with infrared excesses indicative of circumstellar disks and / or envelopes .The majority are Class I sources that have newly formed outflows or jets ; however we also find several dozen more evolved Class II / III sources . In addition to these disk - bearing components , we locate many isolated point - like sources whose SEDs suggest they are deeply embedded protostars .These measurements give novel knowledge into how stars create in IM environments . Our specimen includes several recently unidentified low - luminosity protostars which will be valuable targets for future research at higher angular resolution .This project is based on observations made with the Spitzer Space Telescope , which is controlled by NASA under contract 1407 . Support for this project was provided by NASA through an grant issued by JPL / Caltech .Keywords: Protostar",
        "rewrite_text": "We present the findings from our Spitzer Space Telescope survey of protostars and young stellar objects (YSOs) in three nearby regions of intermediate-mass star formation: NGC 1333, Serpens South, and Perseus North. Our study identifies over 100 candidate YSOs exhibiting infrared excesses, which are characteristic of circumstellar disks and/or envelopes. Most of these candidates are classified as Class I sources, showing newly formed outflows or jets; however, we also observe several dozen more advanced Class II/III sources. Beyond these disk-bearing objects, we uncover numerous isolated point-like sources whose spectral energy distributions (SEDs) indicate they are deeply embedded protostars. These findings enhance our understanding of star formation in intermediate-mass environments. Our sample includes several previously unidentified low-luminosity protostars, which may serve as important targets for future high-resolution studies. This research is based on data obtained through the Spitzer Space Telescope, managed by NASA under contract 1407, with support from a grant provided by JPL/Caltech. Keywords: Protostar",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 5.019960159204453,
        "rewrite-fast-z-score": -0.22645540682891913
    },
    {
        "original_text": "We present the results on searches for new physics beyond the Standard Model (SM) in ttbar events at sqrt(s) = 7 TeV, using data collected by ATLAS during 2011 corresponding to an integrated luminosity of 4.7 fb-1 . The analysis is performed with three different final states: lepton+jets , dilepton and all-hadronic channels. We consider two types of models that can be tested with these analyses: vector-like quarks decaying into tWb or tbHq final states. In both cases we use simplified models where only one coupling parameter is varied while keeping others fixed to their SM values. No significant deviations are observed compared to the predictions based on the SM. Limits are set on the parameters describing the interactions between heavy particles and fermions. These limits improve upon previous measurements made at LEP2 and LHC Run I. \nThe ATLAS Collaboration",
        "watermark_text": "We publish the conclusion on investigations for latest physics beyond the Standard Model ( SM ) in ttbar events at sqrt ( s ) = 7 TeV , using data derived by ATLAS during 2011 resulting to an integrated luminosity of 4 . 7 fb - 1 . The investigation is conducted with three different final states : lepton + jets , dilepton and all - hadronic channels .We consider two forms of models that can be evaluated with these calculations : matrix - like quarks decaying into tWb or tbHq final states . In both cases we utilize simplified theories where only one coupling parameter is varied while staying others set to their SM parameters .No considerable deviations are observed compared to the assumptions based on the SM . Limits are set on the variables describing the interactions between heavy ions and fermions .These limits enhance upon recent observations made at LEP2 and LHC Run I . The ATLAS Collaboration",
        "rewrite_text": "We present our findings on the latest investigations into physics beyond the Standard Model (SM) in ttbar events at sqrt(s) = 7 TeV, utilizing data collected by ATLAS in 2011, resulting in an integrated luminosity of 4.7 fb^-1. The study focuses on three different final states: lepton + jets, dilepton, and all-hadronic channels. We explore two types of models that can be analyzed using these data: matrix-like quarks decaying into tWb or tbHq final states. In both scenarios, we apply simplified models that vary only one coupling parameter while keeping others fixed at their SM values. No significant deviations from SM expectations are detected. Additionally, we establish limits on the parameters that describe interactions between heavy ions and fermions, which improve upon recent results from LEP2 and LHC Run I. The findings are reported by the ATLAS Collaboration.",
        "ori-fast-z-score": -1.7856873313329573,
        "water-fast-z-score": 4.714951667914447,
        "rewrite-fast-z-score": -0.4120816918460671
    },
    {
        "original_text": "The present work is devoted to the study of the relationship between the orbital motion and the energy levels in atoms, ions or molecules. The main idea consists on establishing an analogy between the circular orbits of planets around their central star (Kepler s laws) and the electron shells in atoms (Pauli principle). In this way we can obtain new relationships among physical quantities such as the radius of the planet, its mass, the distance between two consecutive orbits, etc., which are useful when studying atomic systems. We also show that these results allow us to establish a connection between the Pauli principle and the concept of spin angular momentum. Finally, we propose some applications of our formalism to solve problems related to molecular physics. This article has been published in Journal of Physics A, Volume 44, Issue 1, page 015101-1-151105, 2015. DOI: 10.1088/1361-6111/44/1/015101",
        "watermark_text": "The present work is devoted to the observation of the relationship between the orbital movement and the power concentrations in atoms , electrons or compounds . The main idea focuses on establishing an analogy between the circular orbits of stars around their central star ( Kepler s rules ) and the electron shells in atoms ( Pauli concept ) .In this way we can obtain new interactions among physical components such as the radius of the planet , its mass , the distance between two consecutive orbits , etc . , which are helpful when examining nuclear systems . We additionally find that these results allow us to obtain a connection between the Pauli concept and the notion of spin angular velocity .Finally , we propose some applications of our formalism to solve difficulties related to molecular physics . This section has been publication in Journal of Physics A , Volume 44 , Issue 1 , page 015101 - 1 - 151105 , 2015 .DOI: 10.1088/1361-6111/44/1/015101",
        "rewrite_text": "This study focuses on exploring the relationship between orbital movements and power concentrations in atoms, electrons, or compounds. The core concept involves drawing an analogy between the circular orbits of stars around a central star as described by Kepler's laws and the electron shells in atoms as explained by Pauli's principles. This approach enables us to derive new interactions among various physical components, such as the radius of a planet, its mass, and the distance between successive orbits, which are beneficial for analyzing nuclear systems. Furthermore, we discover that these findings establish a link between Pauli's concepts and the idea of spin angular velocity. Finally, we suggest potential applications of our formalism to address challenges in molecular physics. This section was published in the Journal of Physics A, Volume 44, Issue 1, page 015101, in 2015. DOI: 10.1088/1361-6111/44/1/015101.",
        "ori-fast-z-score": -0.8682431421244593,
        "water-fast-z-score": 4.923659639173309,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "We study the formation and dynamics of bright-dark solitons in spin-1 condensates with spin-orbit coupling, which are described by the Gross-Pitaevskii equation for two coupled fields. We show that dark-bright solitons can be formed when one field is initially localized at the center of the trap while the other has an extended profile. The resulting solitonic states have been observed experimentally. \n \n In addition to their fundamental interest as nonlinear excitations, these structures may also play important roles in quantum information processing applications such as atom interferometry or quantum logic gates based on matter waves. Finally we discuss how our results could be generalized beyond the mean-field approximation. \nI. INTRODUCTORY REMARK\nThe recent experimental realization of spinor BECs  1  , i.e., atomic gases trapped in magnetic potentials where each atom carries a well-defined internal degree of freedom (spin), has opened up new avenues towards the investigation of novel physical phenomena  2  . Among them, the possibility of creating stable spin textures  3  , topological defects  4  , and vortex lattices  5  has attracted considerable attention over the past few years  6  .\nIn this work we focus on another interesting class of solutions recently predicted theoretically  7, 8  : Bright-Dark Soliton Complex (BDSC) pairs. These consist of a pair of spatially separated bright and dark solitons whose relative phase varies continuously across the system  9  . They were first proposed in the context of optics  10  but later found to exist in various systems including superfluids  11  , plasmas  12  , and semiconductor microcavities  13  . Their existence was confirmed experimentally in optical fibers  14  and more recently in ultracold atoms  15  . \nII. MODEL AND METHODS\n\nA. Mean-Field Model\nSpinor BECs are modeled within the framework of the meanfield theory  16  using the following set of coupled Gross-Pitaevski equations  17  :",
        "watermark_text": "We research the formation and dynamics of bright - dark solitons in spin - 1 condensates with spin - orbit bonding , which are explained by the Gross - Pitaevskii equation for two coupled fields . We see that dark - dim solitons can be formed when one field is initially localized at the center of the trap while the other has an extended shape .The produced solitonic states have been observed experimentally . In addition to their inherent interest as nonlinear excitations , these structures could also play important roles in quantum information processing applications such as atom interferometry or quantum logic gates based on matter waves .Finally we talk how our findings may be generalized beyond the mean - field approximation . I .INTRODUCTORY REMARK The recent experimental realization of spinor BECs 1 , i . e . , atomic atoms trapped in magnetic potentials where each molecule carries a well - defined internal degree of liberty ( spin ) , has opened up new avenues towards the exploration of new physical phenomena 2 . Among them , the prospect of creating stable spinning textures 3 , topological errors 4 , and vortex lattices 5 has garnered considerable scrutiny over the previous few years 6 .In this research we focus on another important family of solutions recently suggested theoretically 7 , 8 : Bright - Dark Soliton Complex ( BDSC ) pairs . These consist of a pair of spatially separated brilliant and dark solitons whose relative phase differs continuously across the system 9 .They were first suggested in the context of optics 10 but later confirmed to arise in different structures including superfluids 11 , plasmas 12 , and semiconductor microcavities 13 . Their existence was confirmed experimentally in laser fibers 14 and more recently in ultracold atoms 15 .II . MODEL AND METHODS A . Mean - Field Model Spinor BECs are modeled within the framework of the meanfield physics 16 utilizing the following setting of coupled Gross - Pitaevski equations 17 :",
        "rewrite_text": "We investigate the formation and dynamics of bright-dark solitons in spin-1 condensates with spin-orbit coupling, described by the Gross-Pitaevskii equation for two coupled fields. Our findings indicate that dark-dim solitons can emerge when one field is initially concentrated at the center of a trap, while the other field has a more extended profile. These solitonic states have been confirmed through experimental observation. Beyond their fundamental interest as nonlinear excitations, these structures may also have significant implications for quantum information processing, including applications in atom interferometry and quantum logic gates utilizing matter waves. Finally, we discuss how our results can potentially extend beyond the mean-field approximation. \n\nI. INTRODUCTORY REMARKS\n\nThe recent experimental development of spinor Bose-Einstein condensates (BECs), which involves atoms trapped in magnetic potentials with a specific internal degree of freedom (spin), has opened up new pathways for exploring novel physical phenomena. This includes the creation of stable spinning textures, topological defects, and vortex lattices, all of which have received considerable attention in recent years. Our research focuses on another crucial set of solutions that have been proposed theoretically: the Bright-Dark Soliton Complex (BDSC) pairs. These pairs consist of a spatial separation between bright and dark solitons with a continuously varying relative phase across the system. Initially suggested in the field of optics, BDSCs have been subsequently confirmed in various contexts, including superfluids, plasmas, and semiconductor microcavities. Their experimental validation has been achieved in laser fibers and more recently in ultracold atomic systems. \n\nII. MODEL AND METHODS\n\nA. Mean-Field Model\n\nSpinor BECs are analyzed within the mean-field framework using a set of coupled Gross-Pitaevskii equations.",
        "ori-fast-z-score": -0.6211495565912797,
        "water-fast-z-score": 6.300231216854408,
        "rewrite-fast-z-score": 0.2683281572999747
    },
    {
        "original_text": "We present the results of our analysis on the statistical properties of dust FIR emission in nearby galaxies, based on data obtained by ISO and Spitzer space telescopes. We find that the distribution function of dust FIR luminosity is well described by a log-normal form with an exponential tail at high luminosities. The mean value of the logarithmic luminosity dispersion for all samples considered here is 0.3 dex (factor of 2). This result suggests that there are two populations of dusty star-forming regions within each galaxy -one population associated with normal star formation activity and another one associated with intense bursts of star formation. Our study also shows that the fraction of galaxies containing such extreme objects increases towards higher redshifts. These findings have important implications for understanding the physical processes responsible for the evolution of distant galaxies as well as their contribution to the cosmic infrared background radiation. \n \n Keywords: Infrared, Galaxy",
        "watermark_text": "We present the conclusion of our analysis on the statistical characteristics of dust FIR pollution in nearby galaxies , using on evidence derived by ISO and Spitzer space telescopes . We see that the distribution function of dust FIR luminosity is well described by a log - normal shape with an exponential tail at high luminosities .The mean value of the logarithmic luminosity dispersion for all specimens considered here is 0 . 3 dex ( factor of 2 ) . This result suggests that there are two communities of dusty star - creating areas within each galaxy - one community associated with normal star formation activity and another one linked with aggressive bursts of galaxy formation .Our study also shows that the fraction of stars bearing such extreme objects increases towards higher redshifts . These studies have important implications for studying the physical processes responsible for the evolution of distant galaxies as also as their impact to the cosmic infrared background radiation .Keywords: Infrared, Galaxy",
        "rewrite_text": "We conclude our analysis of the statistical characteristics of far-infrared (FIR) dust pollution in nearby galaxies, drawing on data from the ISO and Spitzer space telescopes. Our findings indicate that the distribution function of dust FIR luminosity exhibits a log-normal shape, complete with an exponential tail at higher luminosities. The average logarithmic luminosity dispersion across all samples analyzed is 0.3 dex (a factor of 2). This suggests the existence of two distinct communities of dusty star-forming regions within each galaxy: one associated with standard star formation activities and the other linked to intense bursts of galaxy formation. Additionally, our study reveals that the proportion of stars exhibiting such extreme characteristics increases with higher redshifts. These findings have significant implications for understanding the physical processes driving the evolution of distant galaxies and their influence on the cosmic infrared background radiation. \nKeywords: Infrared, Galaxy.",
        "ori-fast-z-score": 0.3464101615137754,
        "water-fast-z-score": 5.427092530382482,
        "rewrite-fast-z-score": 1.1785113019775793
    },
    {
        "original_text": "We present constraints on models of neutrino mass, mixing angles, and interactions using data from the Planck satellite experiment. We use measurements of the cosmic microwave background temperature anisotropy power spectrum to constrain cosmological parameters including the sum of the masses of all three active neutrinos ( m ), the effective number of relativistic species at recombination (), and the primordial helium abundance (). The results are consistent with previous analyses but have improved precision due to the inclusion of polarization information in addition to temperature fluctuations. \n \n In particular we find that the total neutrino mass is constrained by Planck to be less than 0.23 eV at 95% confidence level for normal hierarchy or less than 0.28 eV for inverted hierarchy. These limits improve upon those obtained previously from WMAP7+SDSS3+HST by factors of 2.5-3.0. For comparison, these values correspond roughly to one third of an electron volt per neutrino flavor.",
        "watermark_text": "We create limits on estimates of neutrino mass , mix angles , and interactions using data from the Planck satellite experiment . We use measurements of the cosmic microwave background altitude anisotropy energy spectrum to constrain cosmological factors including the sum of the masses of all three active neutrinos ( m ) , the effective number of relativistic species at recombination ( ) , and the primordial helium abundance ( ) .The results are compatible with previous analyses but have improved precision thanks to the introduction of polarization info in addition to heat fluctuations . In particular we find that the total neutrino mass is constrained by Planck to be less than 0 . 23 eV at 95 % confidence rate for normal hierarchy or less than 0 . 28 eV for inverted hierarchy .These limits enhance upon those achieved formerly from WMAP7 + SDSS3 + HST by factors of 2 . 5 - 3 . 0 . For comparison , these estimates approximate roughly to one third of an electron volt per neutrino flavor .",
        "rewrite_text": "We establish constraints on neutrino mass estimates, mixing angles, and interactions using data from the Planck satellite experiment. By analyzing measurements of the cosmic microwave background's altitude anisotropy energy spectrum, we can refine our understanding of several cosmological parameters, including the total mass of the three active neutrinos (m), the effective number of relativistic species at the time of recombination (Neff), and the primordial helium abundance (Yp). Our findings align with previous analyses but exhibit greater precision due to the incorporation of polarization data alongside temperature fluctuations. Specifically, we determine that the total neutrino mass is constrained by Planck to be below 0.23 eV at a 95% confidence level for normal hierarchy and below 0.28 eV for inverted hierarchy. These limits surpass those previously obtained from WMAP7, SDSS3, and HST, improving by factors of 2.5 to 3.0. For perspective, these mass estimates correspond to approximately one-third of an electron volt per neutrino flavor.",
        "ori-fast-z-score": 1.3858697343671664,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": 2.108590488016544
    },
    {
        "original_text": "We present an analytical model to study the effect of mass loading feedback on particle concentration and enstrophy in fully developed turbulence. The cascade process is modeled by using a set of coupled ordinary differential equations, which are derived based on dimensional analysis and Kolmogorov s similarity hypothesis. We show that the presence of particles can significantly affect both the energy transfer rate between different scales as well as the dissipation rates at small scales. In particular, we find that the total amount of energy transferred into smaller scales decreases when there exists significant mass loading feedback. This result suggests that the presence of heavy particles may lead to reduced turbulent mixing efficiency. Finally, our results also indicate that the effects of mass loading feedback become more pronounced if the Stokes number increases or the initial volume fraction of particles becomes larger. Our findings provide useful insights into understanding how heavy particles influence the dynamics of fluid flows. C \nAuthor(s): Yi-Chun Chen , Shih-Chieh Hwang , Chia-Hui Wu , Yu-Ting Lin , Ming-Yuan Liu , Chao-Lin Wang , Jie-Sheng Huang , Wen-Ju Tsai , Tzi-Chao Chan , Chin-Fa Lee , Kuo-Yang Chang , Chung-Ming Yeh , Yuan-Kang Chiou , Chien-Nan Chu , Cheng-Wei Hsieh , Chien-Wen Lu , Chien-Chung Wu , Chien-Shu Chen , Chien-Chin Wu , Chien-Chin Yang , Chien-Chin Lai , Chien-Chin Su , Chien-Chin Hung , Chien-Chin Chen , Chien-Ching Wu , Chien-Ching Tai , Chien-Ching Li , Chien-Ching Sun , Chien-Ching Liang , Chien-Ching Chen , Chien-Chong Wu , Chien-Chung Chen , Chien-Chung Lai , Chien-Chung Su , Chien-Chung Hung , Chien-Chung",
        "watermark_text": "We present an analytical theory to study the impact of mass displacement feedback on particle concentration and enstrophy in fully developed turbulence . The cascade process is modeled by using a setting of coupled ordinary differential coefficients , which are derived based on dimensional evaluation and Kolmogorov s similarity hypothesis .We see that the presence of particles can significantly affect both the power transfer frequency between various scales as well as the dissipation levels at small scales . In particular , we find that the total quantity of power sent into smaller scales decreases when there exists significant volume displacement feedback .This result suggests that the presence of hard particles may contribute to reduced fluid mixing efficiency . Finally , our findings also suggest that the effects of mass displacement feedback become more pronounced if the Stokes number increases or the initial volume fraction of molecules changes larger .Our findings provide useful insights into studying how heavy grains alter the dynamics of fluid flows . C Author ( s ) : Yi - Chun Chen , Shih - Chieh Hwang , Chia - Hui Wu , Yu - Ting Lin , Ming - Yuan Liu , Chao - Lin Wang , Jie - Sheng Huang , Wen - Ju Tsai , Tzi - Chao Chan , Chin - Fa Lee , Kuo - Yang Chang , Chung - Ming Yeh , Yuan - Kang Chiou , Chien - Nan Chu , Cheng - Wei Hsieh , Chien - Wen Lu , Chien - Chung Wu , Chien - Shu Chen , Chien - Chin Wu , Chien - Chin Yang , Chien - Chin Lai , Chien - Chin Su , Chien - Chin Hung , Chien - Chin Chen , Chien - Ching Wu , Chien - Ching Tai , Chien - Ching Li , Chien - Ching Sun , Chien - Ching Liang , Chien - Ching Chen , Chien - Chong Wu , Chien - Chung Chen , Chien - Chung Lai , Chien - Chung Su , Chien - Chung Hung , Chien - Chung",
        "rewrite_text": "We introduce an analytical framework to investigate the effects of mass displacement feedback on particle concentration and enstrophy in fully developed turbulence. Our model employs a system of coupled ordinary differential equations, which are derived through dimensional analysis and Kolmogorov's similarity hypothesis. Our results indicate that the presence of particles significantly influences both the transfer of energy between different scales and the dissipation rates at smaller scales. Notably, we observe that the overall power transferred to smaller scales diminishes when substantial volume displacement feedback is present. This suggests that hard particles may reduce the efficiency of fluid mixing. Furthermore, our findings indicate that the impact of mass displacement feedback is amplified with increasing Stokes numbers or larger initial volume fractions of particles. Overall, these insights enhance our understanding of how heavy grains modify the dynamics of fluid flows.",
        "ori-fast-z-score": 0.7844645405527362,
        "water-fast-z-score": 7.060180864974626,
        "rewrite-fast-z-score": 0.6793662204867574
    },
    {
        "original_text": "We present new results on the long-term evolution of solar magnetic fields, obtained by applying an advanced data analysis technique to observations made with the Wilcox Solar Observatory (WSO) magnetograph between 1976 and 2009. The method is based on wavelet transforms in combination with principal component analysis (PCA). It allows us to separate different types of variability into their individual components at each point in time. We find that there are two distinct modes of solar magnetic field evolution over this period. One mode shows strong fluctuations around a mean value which varies slowly but significantly during the cycle 23/24 minimum. This behaviour can be explained as being due to the presence of large-scale dynamo waves driven by differential rotation. In addition we identify another type of variation which appears to have no preferred frequency or spatial scale. These variations show significant correlations with sunspot number and other proxies for solar activity. They may therefore represent some form of global response of the Sun s magnetic field to changes in its internal dynamics.",
        "watermark_text": "We report new data on the long - term expansion of solar magnetic waves , obtained by using an sophisticated data analysis technique to observations made with the Wilcox Solar Observatory ( WSO ) magnetograph between 1976 and 2009 . The method is based on wavelet transforms in combination with principal component analysis ( PCA ) .It enables us to separate distinct types of variability into their individual parts at each point in time . We see that there are two different paths of solar magnetic force evolution over this time .One mode displays large fluctuations around a mean value which varies steadily but significantly during the period 23 / 24 minimum . This behaviour can be understood as being owing to the presence of large - scale dynamo waves generated by differential rotation .In addition we identify another type of variation which appears to have no dominant amplitude or geographic range . These changes demonstrate considerable correlations with sunspot number and other proxies for solar activity .They might hence indicate some kind of worldwide response of the Sun s magnetic force to changes in its internal motion .",
        "rewrite_text": "We present new findings on the long-term evolution of solar magnetic waves, derived from advanced data analysis techniques applied to observations from the Wilcox Solar Observatory (WSO) magnetograph spanning 1976 to 2009. Our approach utilizes wavelet transforms combined with principal component analysis (PCA), allowing us to decompose various types of variability into their individual components over time. Our analysis reveals two distinct pathways for the evolution of solar magnetic forces during this period. One mode exhibits significant fluctuations around a mean value that changes consistently yet noticeably during the solar minimum between cycles 23 and 24. This behavior is attributed to large-scale dynamo waves driven by differential rotation. Additionally, we identify another form of variation that lacks a dominant amplitude or geographic distribution. These fluctuations show strong correlations with sunspot counts and other indicators of solar activity, suggesting a possible global response of the Sun's magnetic forces to alterations in its internal dynamics.",
        "ori-fast-z-score": 0.43133109281375365,
        "water-fast-z-score": 7.763959670647566,
        "rewrite-fast-z-score": 0.6625891564490792
    },
    {
        "original_text": "The aim of this work is to present the results obtained by applying the method of R-matrix theory for solving problems related with scattering processes on complex systems, such as atomic nuclei and molecules. The main idea behind the approach presented here consists in using an effective interaction potential which takes into account all possible interactions between particles involved in the process under consideration. This allows one to obtain solutions that are valid at any distance between interacting objects. In particular we have applied our formalism to study elastic electron-atom collisions within the framework of the Born approximation. We show how it can be used to calculate cross sections for different types of atoms (hydrogen-like ions) and compare them with those calculated within other approaches. \n \n Keywords: Elastic Scattering, Cross Section, Electron Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-Like Ions. 1 Introduction \n \n The problem of calculating cross section for elastic electron-atom collision has been studied extensively during last decades both theoretically and experimentally  1  . It was shown  2  , however, that even if the exact wave function describing the system is known, the calculation of the corresponding cross section requires very complicated numerical procedures. Therefore various approximate methods were developed  3  -  6  . Among these approximations the most popular ones are: the first Born approximation  7  , the second Born approximation  8  , the distorted-wave Born approximation  9  , the Kohn variational principle  10  , etc.. All these methods allow one to find analytical expressions for the total cross section but they require some additional assumptions about the form of the wave functions or potentials describing the system under investigation. For example, in order to apply the first Born approximation one needs to know the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation requires knowledge of the exact wave function for final states only  11  . On the other hand, the distorted-wave Born",
        "watermark_text": "The goal of this study is to provide the results derived by using the method of R - matrix theory for solving cases linked with scattering phenomena on complex systems , such as atomic atoms and atoms . The main idea behind the approach given here consists in utilizing an efficient interaction potential which gives into consideration all possible interactions between particles interested in the process under consideration .This enables one to obtain solve that are applicable at any distance between interacting objects . In particular we have applied our formalism to study elastic electron - atom collisions within the framework of the Born method .We see how it can be used to estimate cross sections for different kinds of atoms ( hydrogen - like ions ) and compare them with those estimated within other methods . Keywords : Elastic Scattering , Cross Section , Electron Atom Collision , R - Matrix Theory , Effective Interaction Potential , Hydrogen - Like Ions .1 Introduction The question of calculating cross section for elastic electron - atom collision has been studied frequently during last decades both theoretically and experimentally 1 . It was shown 2 , however , that even if the exact wave function describing the system is known , the determination of the associated cross area needs very complicated mathematical procedures .Therefore several exact approaches were developed 3 - 6 . Among these approximations the most popular ones are : the first Born approximation 7 , the second Born approximation 8 , the distorted - wave Born approximation 9 , the Kohn variational theory 10 , etc . . All these algorithms provide one to find analytical expressions for the total cross area but they demand some additional constraints about the form of the wave functions or potentials describing the system under inquiry .For instance , in order to apply the first Born approximation one needs to knowledge the exact solution of Schrödinger equation for the initial state of the system while the second Born approximation involves knowledge of the exact wave function for eventual states only 11 . On the other hand , the distorted - wave Born",
        "rewrite_text": "The aim of this study is to present the results obtained using R-matrix theory to analyze scattering phenomena in complex systems, such as atomic structures. The approach focuses on employing an effective interaction potential that accounts for all possible interactions among the particles involved in the examined process. This methodology allows for obtaining solutions that are relevant at any distance between the interacting entities. Specifically, we have applied our formalism to investigate elastic electron-atom collisions within the framework of the Born method. Our analysis demonstrates how this approach can be utilized to estimate cross sections for various types of atoms, particularly hydrogen-like ions, and how these estimates compare with those derived using alternative methods.\n\n**Keywords**: Elastic Scattering, Cross Section, Electron-Atom Collision, R-Matrix Theory, Effective Interaction Potential, Hydrogen-like Ions.\n\n**1. Introduction**  \nThe calculation of cross sections for elastic electron-atom collisions has been extensively explored in both theoretical and experimental contexts over the past few decades. It has been demonstrated, however, that even when the precise wave function of the system is available, determining the corresponding cross section can be mathematically complex. Consequently, several exact approaches have been developed. Among these methodologies, the most widely used include the first Born approximation, the second Born approximation, and the distorted-wave Born approximation, along with Kohn variational theory. While these techniques enable the derivation of analytical expressions for total cross sections, they require specific conditions concerning the forms of the wave functions or potentials governing the system under investigation. For example, applying the first Born approximation necessitates knowledge of the exact solution to the Schrödinger equation for the initial state, while the second Born approximation requires the exact wave function only for the final states. Conversely, the distorted-wave Born approximation...",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 7.069156454130364,
        "rewrite-fast-z-score": -1.4216114170120542
    },
    {
        "original_text": "We present the latest advances in optical frequency combs and their applications to precision metrology, including calibration techniques for high-resolution spectroscopy instruments such as Fourier transform spectrometers (FTS). We discuss how these techniques can be used to improve measurement accuracy by orders of magnitude over traditional methods.  In particular we describe two novel approaches that are being developed at NIST:  1) The use of an optical frequency comb locked to a high-accuracy atomic clock to calibrate FTS measurements with sub-Hz uncertainty. 2) A technique called  self-calibration  which allows one to measure absolute frequencies without requiring any external reference standards or other auxiliary equipment. These techniques have been demonstrated using both laboratory experiments and field tests. They represent important steps towards realizing the vision of a future where all spectroscopic measurements will be traceable back to fundamental physical constants. This is especially relevant today given the growing interest in developing portable, low-cost devices capable of performing accurate chemical analysis anywhere around the world.",
        "watermark_text": "We address the latest advances in imaging bandwidth combs and their applications to precision metrology , notably calibration methods for high - resolution spectroscopy instruments such as Fourier shift spectrometers ( FTS ) . We discuss how these tools can be used to achieve observation reliability by orders of magnitude over traditional techniques .In particular we explain two novel approaches that are being pioneered at NIST : 1 ) The using of an optical frequency comb fixed to a high - speed atomic clock to calibrate FTS calculations with sub - Hz uncertainty . 2 ) A methodology called self - calibration which allows one to measure absolute frequencies without using any external reference standards or other auxiliary machinery .These methods have been shown using both laboratory experiments and field trials . They represent crucial progress towards realizing the vision of a future where all spectroscopic observations will be traceable back to essential scientific constants .This is especially applicable today due the increasing interest in making portable , low - cost devices capable of performing exact chemical analysis anywhere around the world .",
        "rewrite_text": "We explore the recent advancements in imaging bandwidth combs and their applications in precision metrology, particularly in the calibration of high-resolution spectroscopy instruments like Fourier transform spectrometers (FTS). We illustrate how these innovations enhance observation reliability by several orders of magnitude compared to traditional methods. In particular, we detail two innovative techniques being developed at NIST: 1) the use of an optical frequency comb linked to a high-speed atomic clock for calibrating FTS measurements with sub-Hz uncertainty, and 2) a self-calibration methodology that enables the measurement of absolute frequencies without dependence on external reference standards or additional equipment. These approaches have been validated through both laboratory experiments and field trials, marking significant progress toward a future where all spectroscopic measurements are traceable to fundamental scientific constants. This is particularly relevant today, given the growing demand for portable, low-cost devices capable of conducting precise chemical analyses anywhere in the world.",
        "ori-fast-z-score": -0.6396021490668313,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 1.118033988749895
    },
    {
        "original_text": "We report on measurements of spin-transfer-torque vector (STTV) in magnetic tunnel junctions with perpendicular anisotropy and in-plane magnetization. The STTV is determined by measuring the current-induced switching field distribution for different directions of an external magnetic field applied along the film plane. We find that the magnitude of the STTV depends strongly on the direction of the external magnetic field, which can be explained by taking into account the dependence of the spin polarization efficiency on the angle between the magnetizations of ferromagnetic electrodes. Our results demonstrate that the measurement of the STTV provides information about the angular dependence of the spin polarization at interfaces between magnetic materials. \n \n Spin transfer torque (STT), arising due to the flow of spin-polarized electrons across a magnetic interface, has been extensively studied both theoretically and experimentally over past decade  1  . In particular, it was shown that the application of a charge current through a magnetic tunnel junction (MTJ) leads to the reversal of its free layer via the action of the so-called spin-transfer-torque vector  2  , whose components are given by: \n \n where  is the unit vector pointing along the electron s momentum,  is the unit vector pointing in the direction of the local magnetization, , and are the magnitudes of the spin polarization efficiencies at the left and right interfaces respectively, and is the thickness of the MTJ barrier  3  .\n \nIn this work we present experimental data demonstrating how the magnitude of the STTT changes as function of the orientation of the external magnetic field Hext applied parallel to the film plane. This allows us to determine the angular dependences of the spin polarization effciency at each interface separately.",
        "watermark_text": "We report on observations of spin - transfer - torque velocity ( STTV ) in magnetic tunnel junctions with perpendicular anisotropy and in - plane magnetization . The STTV is calculated by monitoring the current - caused switching field flow for different angles of an external magnetic current introduced along the film plane .We see that the magnitude of the STTV depends strongly on the direction of the external magnetic force , which can be described by take into consideration the dependence of the spin polarization efficiency on the angle between the magnetizations of ferromagnetic electrodes . Our results show that the determination of the STTV provides knowledge about the angular influence of the spin polarization at connections between magnetic materials .Spin transfer torque ( STT ) , arising owing to the movement of spin - polarized ions across a magnetic interface , has been heavily examined both theoretically and experimentally over past year 1 . In particular , it was shown that the introduction of a charge current through a magnetic tunnel junction ( MTJ ) results to the reversal of its free layer via the operation of the so - called spin - transfer - torque velocity 2 , whose components are given by : where is the unit matrix pointing along the electron s momentum , is the unit matrix pointing in the direction of the local magnetization , , and are the magnitudes of the spin polarization efficiencies at the left and left interfaces respectively , and is the length of the MTJ barrier 3 .In this research we present experimental evidence showing how the magnitude of the STTT shifts as function of the orientation of the external magnetic force Hext introduced parallel to the film plane . This enables us to predict the angular dependences of the spin polarization effciency at each connection separately .",
        "rewrite_text": "We present observations of spin-transfer-torque velocity (STTV) in magnetic tunnel junctions characterized by perpendicular anisotropy and in-plane magnetization. STTV is determined by tracking the current-induced switching field flow at various angles of an external magnetic current applied along the film plane. Our findings indicate that the STTV magnitude is significantly influenced by the orientation of the external magnetic force, which can be understood by considering the relationship between spin polarization efficiency and the angle between the ferromagnetic electrodes' magnetizations. This research demonstrates that measuring STTV offers insights into the angular dependence of spin polarization at the interfaces of magnetic materials. Spin transfer torque (STT), which results from the movement of spin-polarized electrons across a magnetic interface, has been extensively studied both theoretically and experimentally over the past year. Specifically, it has been established that passing a charge current through a magnetic tunnel junction (MTJ) can reverse its free layer through the mechanism of spin-transfer-torque velocity. The components of this velocity are defined by the unit matrix aligned with the electron's momentum, the unit matrix corresponding to the local magnetization direction, the magnitudes of spin polarization efficiencies at both interfaces, and the thickness of the MTJ barrier. In this study, we provide experimental evidence showing how the magnitude of STTV varies with the orientation of the external magnetic field introduced parallel to the film plane. This allows us to forecast the angular dependence of spin polarization efficiency at each junction individually.",
        "ori-fast-z-score": -0.9918365981341755,
        "water-fast-z-score": 6.5821883330722555,
        "rewrite-fast-z-score": 2.37346441585572
    },
    {
        "original_text": "We have analyzed the shapes of interstellar extinction curves in the infrared through ultraviolet wavelength range using data for more than 100 sight lines with known distances and reddenings, including those obtained by us at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). We find that all observed extinction curves can be fitted well by a single power law function A(lambda) = lambda -alpha , where alpha is an index ranging between 1.5 to 2.0. This result suggests that there are no significant differences among various types of interstellar dust grains as far as their optical properties are concerned. In addition, we show that the value of alpha correlates strongly with the total-to-selective extinction ratio Rv . These results suggest that the shape of interstellar extinction curve may provide important information on the physical conditions of interstellar matter along individual sight lines. Keywords: Interstellar extinction; Power-law index; Reddening; Dust grain",
        "watermark_text": "We have analyzed the shapes of interstellar extinction lines in the infrared through ultraviolet frequency range utilizing information for more than 100 sight lines with reported distances and reddenings , notably those acquired by us at Kitt Peak National Observatory ( KPNO ) and Cerro Tololo Inter - American Observatory ( CTIO ) . We see that all observed extinction lines can be fit well by a single power law relation A ( lambda ) = lambda - alpha , where alpha is an index ranging between 1 . 5 to 2 . 0 .This result suggests that there are no major changes among various types of interstellar dust grains as long as their optical properties are concerned . In addition , we find that the value of α correlates heavily with the total - to - selective extinction ratio Rv .These data suggest that the form of interstellar extinction curve might give important information on the physical conditions of interstellar matter along individual sight lines . Keywords : Interstellar extinction ; Power - law index ; Reddening ; Dust grain",
        "rewrite_text": "We have conducted an analysis of the shapes of interstellar extinction lines across the infrared to ultraviolet spectrum, using data from over 100 sight lines with known distances and reddenings, particularly those gathered at Kitt Peak National Observatory (KPNO) and Cerro Tololo Inter-American Observatory (CTIO). Our findings indicate that all observed extinction lines can be accurately described by a single power law relation, A(λ) = λ^(-α), where α is an index ranging from 1.5 to 2.0. This observation implies that there are no significant variations among the different types of interstellar dust grains regarding their optical properties. Furthermore, we observe a strong correlation between the value of α and the total-to-selective extinction ratio Rv. These findings suggest that the characteristics of the interstellar extinction curve could provide crucial insights into the physical conditions of interstellar matter along specific sight lines. Keywords: Interstellar extinction; Power-law index; Reddening; Dust grain.",
        "ori-fast-z-score": 1.0834726777719228,
        "water-fast-z-score": 5.820855000871991,
        "rewrite-fast-z-score": 1.3054598240132387
    },
    {
        "original_text": "We study the properties of Bose-Einstein condensates (BECs) with an arbitrary number N of particles trapped by a harmonic oscillator and exposed to a disordered external potential. We show that, for any strength of disorder, there is always a finite fraction of atoms localized at each site of the lattice. The localization length decreases as the disorder increases but remains macroscopic even when the disorder becomes very large compared to the interatomic interaction energy. This result holds true both in one dimension and higher dimensions.  In particular we find that the critical disorder above which all states are localized scales like 1/N in 1D and 1/d in 2D and 3D where d is the spatial dimension. Our results provide a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices. Introduction:-Recent experimental advances have made it possible to create quantum degenerate gases of bosons or fermions confined in periodic potentials  1  . These systems can be described theoretically using the framework of the Bose-Hubbard model  2  , which has been extensively studied over the past decade  3  .\nIn this work we consider the case of a gas of interacting bosons in a disordered potential. Disorder leads to Anderson localization  4  : eigenstates become exponentially localized around their initial position if the disorder exceeds some threshold value  5  . It was recently shown experimentally  6  that such a system exhibits a transition between extended Bloch-like states and localized Wannier-Stark ladders  7, 8  . However, these experiments were performed only in the weak-disorder regime, i.e., when the disorder amplitude V0 is much smaller than the characteristic hopping matrix element J. Here we investigate how the presence of interactions affects the physics of strongly disordered systems.",
        "watermark_text": "We explore the properties of Bose - Einstein condensates ( BECs ) with an arbitrary number N of atoms trapped by a harmonic oscillator and exposed to a disordered external potential . We see that , for any strength of disorder , there is usually a finite fraction of atoms confined at each site of the crystal .The localization width decreases as the disorder advances but continues macroscopic even when the disorder becomes very huge compared to the interatomic interaction power . This result holds true both in one dimension and upper dimensions .In particular we find that the fundamental disorder above which all states are localized scales like 1 / N in 1D and 1 / d in 2D and 3D where d is the spatial dimension . Our results present a microscopic understanding of recent experiments on ultracold atomic gases in optical lattices .Introduction : - Recent research developments have enabled it able to create quantum degenerate gases of bosons or fermions localized in periodic potentials 1 . These systems can be described theoretically utilizing the framework of the Bose - Hubbard theory 2 , which has been heavily explored over the previous decade 3 .In this research we study the case of a gas of interacting bosons in a disordered potential . Disorder leads to Anderson localization 4 : eigenstates grow exponentially localized around their early position if the disorder approaches some threshold parameter 5 .It was recently shown experimentally 6 that such a system displays a shift between advanced Bloch - like states and localized Wannier - Stark ladders 7 , 8 . However , these experiments were performed only in the weak - disturbance regime , i . e . , when the disorder amplitude V0 is much smaller than the typical hopping matrix element J .Here we investigate how the presence of interactions affects the physics of highly disordered systems .",
        "rewrite_text": "We investigate the characteristics of Bose-Einstein condensates (BECs) containing an arbitrary number \\( N \\) of atoms, which are confined by a harmonic oscillator and subject to a disordered external potential. Our findings indicate that, regardless of the disorder's strength, a significant fraction of atoms remains localized at each site within the crystal. Although the localization width diminishes as disorder increases, it remains macroscopic even when the disorder becomes significantly stronger than the interatomic interaction strength. This behavior is consistent in one dimension as well as in higher dimensions. Notably, we determine that the critical disorder level at which all states become localized scales as \\( 1/N\\) in one dimension and \\( 1/d\\) in two and three dimensions, where \\( d \\) denotes the spatial dimension. Our results contribute to a microscopic understanding of recent experiments conducted on ultracold atomic gases within optical lattices. \n\n**Introduction:** Recent advancements have made it possible to create quantum degenerate gases of either bosons or fermions that are localized in periodic potentials. These systems can be theoretically described using the Bose-Hubbard model, which has been extensively examined in the past decade. In this study, we focus on a gas of interacting bosons exposed to a disordered potential. Disorder induces Anderson localization, where eigenstates exhibit exponential localization around their original positions once the disorder surpasses a certain threshold. Recent experimental work has demonstrated a transition between advanced Bloch-like states and localized Wannier-Stark ladders. However, these experiments have primarily been limited to the weak disturbance regime, where the disorder amplitude \\( V_0 \\) is significantly smaller than the typical hopping matrix element \\( J \\). In this paper, we explore how interactions influence the behavior of highly disordered systems.",
        "ori-fast-z-score": 0.09166984970282113,
        "water-fast-z-score": 6.937819061732104,
        "rewrite-fast-z-score": 1.2888044650576527
    },
    {
        "original_text": "We present new constraints on planet formation timescales by combining infrared photometry and spectroscopy with optical data to study the circumstellar environments of young stars in nearby open clusters. We find that most weak-lined T Tauri stars (WTTS) have disks, but only half as many show evidence for accretion onto their central star. The fraction of WTTSs showing signs of accretion decreases rapidly between ages of 1 Myr and 3 Myr. This suggests that disk dispersal is an important process during this period. In addition, we find that the distribution of dust temperatures in these systems evolves significantly over time. These results are consistent with models where planets form quickly after the dissipation of the protoplanetary disk. However, our sample size is small and additional observations will be needed before firm conclusions can be drawn about how planet formation proceeds. \n \n Keywords: Open cluster, Circumstellar disk",
        "watermark_text": "We introduce novel constraints on planet development timescales by combining laser photometry and spectroscopy with optical data to study the circumstellar environments of young galaxies in nearby open complexes . We see that most soft - lined T Tauri stars ( WTTS ) have disks , but only nearly as much demonstrate proof for accretion onto their central star .The percentage of WTTSs displaying signs of accretion decreases quickly between ages of 1 Myr and 3 Myr . This implies that disk dispersal is an important process during this time .In addition , we find that the distribution of dust altitudes in these systems evolves greatly over time . These conclusions are compatible with models where planets occur swiftly after the dissipation of the protoplanetary disk .However , our sample volume is tiny and additional observations will be needed before firm conclusions can be drawn about how planet development proceeds . Keywords : Open cluster , Circumstellar disk",
        "rewrite_text": "We present new constraints on the timescales of planet formation by integrating laser photometry and spectroscopy with optical data to examine the circumstellar environments of young galaxies in nearby open clusters. Our findings indicate that while most weak-lined T Tauri stars (WTTS) possess disks, only a small fraction exhibit evidence of accretion onto their central stars. Notably, the percentage of WTTS showing signs of accretion declines rapidly between the ages of 1 million years and 3 million years, suggesting that disk dispersal is a significant process during this period. Furthermore, we observe substantial changes in the distribution of dust altitudes within these systems over time. These results align with models proposing that planet formation occurs swiftly after the protoplanetary disk dissipates. However, our sample size is limited, and further observations are necessary before we can draw definitive conclusions about the progression of planet formation. Keywords: Open cluster, Circumstellar disk.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 6.063390625908324,
        "rewrite-fast-z-score": -0.5852057359806528
    },
    {
        "original_text": "We study the spatial and kinematic alignment between central and satellite halos in cosmological N-body simulations, focusing on their dependence on halo mass ratio (q = Msat/Mcen) and redshift z. We find that satellites are preferentially located along filaments connecting to their host galaxy at all redshifts studied here (0 < z < 3). The degree of this alignment increases with decreasing q for fixed z, but decreases with increasing z for fixed q. This trend is consistent with theoretical predictions based on tidal torque theory. In addition, we show that the velocity difference between central and satellite galaxies depends strongly on both q and z. For example, when q > 0.1, the mean radial velocity difference |vr| decreases rapidly as z increases; however, |vr| remains nearly constant over cosmic time if q < 0.01. These results suggest that the observed evolution of the luminosity function may be affected by the dynamical friction acting on satellite galaxies.",
        "watermark_text": "We explore the spatial and kinematic alignment between central and satellite halos in cosmological N - bodies simulations , concentrating on their dependence on halo weight ratio ( q = Msat / Mcen ) and redshift z . We see that planets are preferentially positioned along filaments connecting to their target galaxy at all redshifts studied here ( 0 < z < 3 ) .The degree of this alignment varies with varying q for fixed w , but decreases with expanding z for fixed q . This trend is compatible with theoretical estimates based on wave tilt theory .In addition , we find that the velocity difference between central and satellite galaxies depends strongly on both q and z . For instance , when q > 0 . 1 , the mean radial speed difference | vr | decreases quickly as z increases ; however , | vr | remains virtually constant over cosmic time if g < 0 . 01 .These data suggest that the known development of the luminosity function could be altered by the dynamical friction act on satellite galaxies .",
        "rewrite_text": "We investigate the spatial and kinematic alignment between central and satellite halos using cosmological N-body simulations, focusing on how these relationships depend on the halo weight ratio (q = Msat / Mcen) and the redshift (z). Our findings indicate that satellite galaxies tend to be positioned along filaments that connect to their central galaxies across all redshifts examined (0 < z < 3). The strength of this alignment fluctuates based on different q values while holding w constant, but it diminishes as redshift increases for fixed q values. This observed trend aligns with theoretical predictions derived from wave tilt theory. Additionally, we observe that the velocity difference between central and satellite galaxies is significantly influenced by both q and z. For example, when q > 0.1, the mean radial velocity difference |vr| decreases sharply with increasing z; conversely, |vr| remains nearly constant over cosmic time when q < 0.01. These results imply that the evolution of the luminosity function may be affected by the dynamical friction experienced by satellite galaxies.",
        "ori-fast-z-score": -1.585187847802434,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": -0.3333333333333333
    },
    {
        "original_text": "We present the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold, including both classical and quantum aspects. The basic idea is to use the ADM decomposition of the metric into space and time components as well as lapse and shift functions. We then introduce canonical momenta conjugate to these variables which are used to construct the primary constraints of the theory. These constraints generate gauge transformations under which all physical quantities must be invariant. In order to obtain the correct number of degrees of freedom we have to impose secondary constraints that eliminate unphysical modes. Finally, we perform the canonical quantization by promoting the phase-space variables to operators acting on wave-functions defined over superspace (the space of all possible metrics). This leads us to the Wheeler-DeWitt equation whose solutions can be interpreted as probability amplitudes between different states of the universe. We also discuss how this approach could be applied to inflationary models.",
        "watermark_text": "We introduce the Hamiltonian formulation for general relativity with matter fields on an arbitrary spacetime manifold , covering both classical and quantum aspects . The basic idea is to use the ADM decomposition of the metric into space and time parts as well as lapse and shift variables .We then introduce canonical momenta conjugate to these parameters which are applied to build the primary constraints of the theory . These restrictions produce gauge transformations under which all physical components must be invariant .In order to obtain the appropriate number of degrees of liberty we have to introduce secondary constraints that eliminate unphysical modes . Finally , we perform the canonical quantization by expanding the phase - space variables to functions acted on wave - functions defined over superspace ( the space of all possible metrics ) .This leads us to the Wheeler - DeWitt equation whose solutions can be interpreted as probability amplitudes between various states of the universe . We also discuss how this methodology possible be applied to inflationary theories .",
        "rewrite_text": "We present the Hamiltonian approach to general relativity incorporating matter fields within an arbitrary spacetime manifold, addressing both classical and quantum dimensions. The core concept involves the ADM decomposition of the metric, separating it into spatial and temporal components along with lapse and shift variables. Next, we introduce canonical momenta that are conjugate to these parameters, which serve as the foundation for establishing the primary constraints of the theory. These constraints yield gauge transformations that ensure all physical components remain invariant. To achieve the correct number of degrees of freedom, we must introduce secondary constraints to remove unphysical modes. Ultimately, we carry out canonical quantization by expanding the phase-space variables into functions that act on wave functions defined over superspace (the space of all possible metrics). This process culminates in the Wheeler-DeWitt equation, whose solutions can be understood as probability amplitudes representing transitions between different states of the universe. Additionally, we explore how this framework can potentially be applied to inflationary theories.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 3.849741916091625,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "We present an atomistic multiscale approach to the study of charge transport through stretched single-stranded DNA (ssDNA). The method combines molecular dynamics simulations with quantum mechanical calculations on small fragments, which are used as input for a tight-binding description of larger systems. We show that this scheme allows us to reproduce experimental results obtained by scanning tunneling microscopy experiments performed at room temperature. In particular we find that our calculated conductance agrees well with experiment when using realistic values for the hopping parameters between neighboring base pairs. Our analysis shows that the main contribution to the current is due to electrons localized along the backbone chain. These findings suggest that ssDNA can be considered as a promising material for future applications such as nanoelectronic devices or sensors. \n \n Introduction \n \n Single stranded DNA has been studied extensively over many years both experimentally  1 - 3  and theoretically  4 - 6  . It was found that its structure depends strongly on environmental conditions like pH value  7  , ionic strength  8  -  10  , solvent  11  , temperature  12  , stretching  13  , etc.. This makes it possible to use ssDNA as a sensor  14  -  16  or even as a nanomaterial  17  -  19  . For example, recent studies have shown that ssDNA can form stable helical structures  20  -  22  . Furthermore, it was demonstrated that ssDNA can act as a template for protein synthesis  23  .\n \nIn addition to these structural properties there is growing interest in understanding how charge carriers move through ssDNA  24  -  26  . Recent theoretical investigations showed that electron transfer rates depend sensitively on the conformation of the molecule  27  -  29  . Experimentally, it was observed that the conductivity decreases exponentially with increasing length  30  -  32  . However, the exact mechanism behind this effect remains unclear  33  . \n \n Here we propose a new computational scheme combining classical molecular dynamics (MD) simulations  34  with density functional theory (DFT)  35  based quantum chemical calculations  36    Fig. 1(a)  . Using this approach we calculate the transmission function T(E), i.e., the probability amplitude for an electron injected into one end of the system to reach the other end",
        "watermark_text": "We present an atomistic multiscale approach to the study of charge flow through stretched single - stranded DNA ( ssDNA ) . The method mixes molecular mechanics simulations with quantum mechanical calculations on small fragments , which are using as input for a tight - binding characterization of bigger systems .We see that this scheme allows us to publish empirical results derived by scanning tunneling microscopy experiments conducted at room temperature . In particular we find that our measured conductance agrees well with experiment when using reasonable values for the hopping characteristics between neighboring base pairs .Our study shows that the main contribution to the current is due to electrons concentrated along the link chain . These conclusions show that ssDNA can be regarded as a viable substrate for future applications such as nanoelectronic applications or sensors .Introduction Single stranded DNA has been studied extensively over numerous years both experimentally 1 - 3 and theoretically 4 - 6 . It was shown that its composition depends strongly on environmental conditions like pH value 7 , ionic intensity 8 - 10 , solvent 11 , temperature 12 , stretching 13 , etc . .This gives it able to use ssDNA as a sensor 14 - 16 or even as a nanomaterial 17 - 19 . For instance , recent studies have shown that ssDNA can form stable helical structures 20 - 22 .Furthermore , it was demonstrated that ssDNA can work as a template for protein development 23 . In addition to these functional characteristics there is growing interest in understanding how charge carriers go through ssDNA 24 - 26 .Recent conceptual investigations indicated that electron transfer rates differ sensitively on the conformation of the molecule 27 - 29 . Experimentally , it was seen that the conductivity decreases exponentially with increasing long 30 - 32 .However , the exact mechanism behind this effect remains unsure 33 . Here we propose a new computational scheme incorporating traditional molecular mechanics ( MD ) simulations 34 with density functional theory ( DFT ) 35 based quantum chemical calculations 36 Fig .1 ( a ) . Using this methodology we determine the transmission function T ( E ) , i . e . , the probability probability for an electron pumped into one end of the system to reach the other end",
        "rewrite_text": "We introduce an atomistic multiscale methodology for investigating charge flow through stretched single-stranded DNA (ssDNA). This approach combines molecular mechanics simulations with quantum mechanical calculations on small fragments, which serve as inputs for a tight-binding analysis of larger systems. Our results align well with empirical data obtained from scanning tunneling microscopy experiments conducted at room temperature. Notably, we find that our measured conductance is consistent with experimental values when reasonable hopping parameters between adjacent base pairs are employed. Our findings indicate that the primary contribution to the current arises from electrons localized along the linking chain. These insights suggest that ssDNA could be a promising substrate for future applications in areas such as nanoelectronics and sensors. \n\n**Introduction:** Single-stranded DNA has been extensively investigated over the years, both experimentally and theoretically. It has been established that its characteristics are highly influenced by environmental factors, including pH levels, ionic strength, solvent types, temperature, and stretching. This variability positions ssDNA as a potential sensor or even as a nanomaterial. Recent research has demonstrated that ssDNA can form stable helical structures and can serve as a template for protein development. Alongside these functionalities, there is an increasing interest in understanding the charge carrier dynamics in ssDNA. Recent theoretical studies have suggested that electron transfer rates are highly sensitive to the conformation of the molecule. Experimentally, it has been observed that conductivity decreases exponentially with increasing length. However, the exact mechanisms driving this phenomenon remain uncertain. In this study, we propose a novel computational framework that integrates traditional molecular dynamics (MD) simulations with density functional theory (DFT) based quantum chemical calculations. Through this methodology, we aim to determine the transmission function T(E), representing the probability of an electron injected at one end of the system reaching the opposite end.",
        "ori-fast-z-score": -0.07980868844676221,
        "water-fast-z-score": 7.851210698609627,
        "rewrite-fast-z-score": 1.3834403799109711
    },
    {
        "original_text": "We present an open-source framework, called Bubblesched, that allows to build portable thread schedulers for hierarchical multiprocessors (HMPs). The main idea is to use bubbles as scheduling units and schedule them on different levels in HMP hierarchy using a set of rules. We have implemented two schedulers: one based on work stealing and another one based on load balancing. Both schedulers are able to run efficiently on top of Bubblesched without any modifications. Our experimental results show that both schedulers outperform state-of-the-art solutions by up to 3Â times when running parallel applications with fine-grained tasks. In addition, we demonstrate how our scheduler can be used to implement efficient task-parallel algorithms such as graph coloring or matrix multiplication. This research was supported by Russian Science Foundation grant 14-50-00040. We present an opensource framework, called Bubblesched: it allows to build portable threadschedulers for hierarchicalmultiprocessors(HMPs)andrunefficientlyonthemwithoutanymodifications.Theframeworkusesbubblesastheschedu-lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules.Wehaveimplementedtwo-schedulers:onebasedonstealingworkandanotheronesupportedbyloadbalancing.BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified.Ourexperimentalresultsshowthatbothschedulersoutperformstate-oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks.Inaddition,weillustratethattheframeworkcanbeusedtomakeefficienttask-parallelandalgorithmssuchastask-coloringanoregularmatrixmultiplication.",
        "watermark_text": "We create an free - source framework , entitled Bubblesched , that enables to build portable thread schedulers for hierarchical multiprocessors ( HMPs ) . The main idea is to use bubbles as booking units and pattern them on various levels in HMP hierarchy following a setting of rules .We have adopted two schedulers : one based on work stealing and another one based on load balancing . Both schedulers are able to run efficiently on top of Bubblesched without any modifications .Our research results show that both schedulers outperform state - of - the - art applications by up to 3Â times when ran parallel programs with fine - grained assignments . In addition , we prove how our scheduler can be used to execute easy problem - parallel techniques such as graph coloring or vector multiplication .This research was supported by Russian Science Foundation award 14 - 50 - 00040 . We create an opensource platform , entitled Bubblesched : it allows to build portable threadschedulers for hierarchicalmultiprocessors ( HMPs ) andrunefficientlyonthemwithoutanymodifications . Theframeworkusesbubblesastheschedu - lingsunitsandschedulethemonthelevelsofHMPhierarchyusingasetofrules . Wehaveimplementedtwo - schedulers : onebasedonstealingworkandanotheronesupportedbyloadbalancing . BothschedulersexecutesuccessfullyontopofBubbleschedwithouthavingtobemodified . Ourexperimentalresultsshowthatbothschedulersoutperformstate - oftheartsolutionsupto3timeswhenrunningparallelapplicationswithfinegrainetasks . Inaddition , weillustratethattheframeworkcanbeusedtomakeefficienttask - parallelandalgorithmssuchastask - coloringanoregularmatrixmultiplication .",
        "rewrite_text": "We have developed an open-source framework called Bubblesched, designed to facilitate the creation of portable thread schedulers for hierarchical multiprocessors (HMPs). The core concept revolves around using bubbles as scheduling units, which are arranged at various levels within the HMP hierarchy according to a defined set of rules. We have implemented two schedulers: one utilizing work stealing and another focused on load balancing. Both schedulers operate effectively on Bubblesched without requiring any modifications. Our research findings indicate that these schedulers can outperform leading-edge applications by up to three times when running parallel programs with fine-grained tasks. Furthermore, we demonstrate the ability of our framework to efficiently execute straightforward parallel techniques, such as graph coloring and matrix multiplication. This research was funded by the Russian Science Foundation under award number 14-50-00040.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": -0.12403473458920847
    },
    {
        "original_text": "Weak gravitational lensing is an important tool to study dark matter halos in galaxy clusters, but it requires large telescopes or space-based observatories with high angular resolution cameras. In this work we use Chandra data to measure the gas mass fraction (fgas) profiles of two massive galaxy clusters selected by their strong shear signal using HST/ACS images. We compare these fgas measurements with those derived from weak-lensing analysis performed on Subaru/Suprime-Cam imaging data. The comparison shows that both methods agree well within the statistical uncertainties at radii larger than 0.5 r500. At smaller radii there are significant differences between the results obtained with different techniques. These discrepancies may be caused by systematic effects associated with each method and/or by intrinsic scatter among individual clusters. This project was supported by NASA grant NNX10AD65G. We thank J. Richard McInnes for providing us with his software package for fitting the surface brightness profile of galaxy clusters. \n \n Keywords: galaxy cluster, Chandra, weak lensing",
        "watermark_text": "Weak gravitational lensing is an important tool to study dark matter halos in galaxy regions , but it requires large telescopes or space - based observatories with high angular resolution cameras . In this research we utilize Chandra data to measure the gas mass fraction ( fgas ) profiles of two huge galaxy galaxies determined by their powerful stress signal using HST / ACS images .We link these fgas measurements with those generated from soft - lensing research performed on Subaru / Suprime - Cam scan data . The comparison shows that both approaches agree well within the statistical uncertainties at radii larger than 0 . 5 r500 .At lower radii there are significant variations between the results derived with various methods . These discrepancies may be caused by systematic effects involved with each approach and / or by intrinsic scatter among specific clusters .This project was supported by NASA grant NNX10AD65G . We praise J . Richard McInnes for providing us with his computer package for fitting the surface brightness profile of galaxy regions .Keywords : galaxy region , Chandra , soft lensing",
        "rewrite_text": "Weak gravitational lensing serves as a crucial method for investigating dark matter halos in galaxy regions; however, it necessitates the use of large telescopes or high-resolution cameras on space-based observatories. In this study, we utilize data from the Chandra satellite to measure the gas mass fraction (fgas) profiles of two massive galaxies, determined by their strong lensing signals using HST/ACS imagery. We also correlate these fgas measurements with those obtained from soft-lensing studies conducted with Subaru/Suprime-Cam survey data. Our comparison indicates that the results from both methods align well within the statistical uncertainties for radii greater than 0.5 r500. However, at smaller radii, we observe notable discrepancies between results produced by different techniques. These variations may stem from systematic effects inherent to each method and/or intrinsic variations among specific clusters. This project was funded by NASA grant NNX10AD65G, and we extend our gratitude to J. Richard McInnes for providing his software package for fitting the surface brightness profiles of galaxy regions. \nKeywords: galaxy region, Chandra, soft lensing.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 5.888888888888889,
        "rewrite-fast-z-score": -0.10976425998969035
    },
    {
        "original_text": "We study the dynamics of three agent games with two strategies each, where agents are connected by an underlying network and play pairwise interactions according to their strategy choices. We show that for any initial state there is always at least one absorbing state in which all agents have the same strategy choice. In addition we find that if the number of nodes with either strategy exceeds 1 then this state can be reached within finite time. Finally, we provide bounds on how fast such convergence occurs as well as conditions under which it will occur exponentially quickly. The results presented here generalize previous work done on two-agent evolutionary games to multi-agent systems. Evolutionary game theory has been used extensively over the past decade to model competition between different species or individuals competing for limited resources  1  . A common approach taken when modeling these types of problems is to consider a population consisting of many interacting agents who choose among several possible strategies  2  , and then use mathematical tools developed in statistical physics  3  to analyze the resulting system behavior  4  .\nIn recent years researchers have begun studying more complex models involving multiple populations  5  , spatial structure  6  , and heterogeneous environments  7, 8  . However, most existing research focuses only on two-player games  9  , while less attention has been paid to multi-agent systems  10  . Here we present new results showing that even simple multi-agent systems exhibit richer dynamical behaviors than previously thought  11  .",
        "watermark_text": "We explore the dynamics of three agent games with two strategies each , where agents are connected by an underlying network and play pairwise relationships according to their strategy choices . We see that for any initial state there is usually at least one absorbing state in which all agents have the same strategy selection .In addition we find that if the number of nodes with either strategy approaches 1 then this state can be reached within finite period . Finally , we provide bounds on how rapid such convergence occurs as well as conditions under which it will occur exponentially rapidly .The results presented here generalize past work done on two - agent evolutionary games to multi - agent systems . Evolutionary game theory has been used heavily over the previous decade to model competition between various populations or individuals competing for limited supplies 1 .A typical approach took when solving these kinds of problems is to consider a population consisting of several interacting agents who decide among various possible strategies 2 , and then use numerical tools developed in statistical physics 3 to analyze the resulting system interaction 4 . In recent years investigators have begun investigating more sophisticated models involving several populations 5 , temporal formation 6 , and heterogeneous environments 7 , 8 .However , most existing study emphasis only on two - player players 9 , while worse attention has been paid to multi - agent systems 10 . Here we present new data demonstrating that even basic multi - agent systems exhibit richer dynamical traits than previously thought 11 .",
        "rewrite_text": "We investigate the dynamics of three-agent games, each having two strategies, where agents are interconnected through an underlying network and engage in pairwise interactions based on their chosen strategies. Our findings indicate that for any starting condition, there is typically at least one absorbing state where all agents adopt the same strategy. Additionally, we discover that if the number of nodes adopting either strategy converges to one, this state can be achieved within a finite timeframe. Furthermore, we establish bounds on the speed of this convergence and identify conditions that facilitate exponential convergence. The results we present here extend prior research on two-agent evolutionary games to involve multi-agent systems. Over the past decade, evolutionary game theory has been extensively applied to model competitive interactions among various populations or individuals contending for limited resources. Traditionally, these problems have been approached by examining populations with several interacting agents making strategic decisions, often utilizing numerical techniques developed in statistical physics to analyze the dynamics of the interactions. Recently, researchers have begun to explore more complex models that incorporate multiple populations, temporal dynamics, and heterogeneous environments. However, most existing studies have focused primarily on two-player interactions, with significantly less attention given to multi-agent systems. In this work, we provide new evidence that even simple multi-agent systems exhibit more complex dynamical behaviors than previously recognized.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.789028582272215,
        "rewrite-fast-z-score": 0.9838699100999074
    },
    {
        "original_text": "We propose an excitable convolutional neural network (CNN) model for solving the 2D path planning problem in this work. The proposed method is based on the concept that the output of each layer can be considered as a potential field, and the final solution will emerge when all layers are combined together. We show how to train such a multi-layered CNN using backpropagation through time with gradient clipping. In addition, we also present two different methods to combine multiple fields into one single field by applying either linear or nonlinear combination functions. Finally, we demonstrate our approach on several benchmark problems including maze navigation, robotics motion planning, and autonomous driving. Keywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving. 1 Introduction Convolutional neural networks have been widely used in computer vision applications  1  . Recently, they were applied to solve various types of optimization problems  2  , which include image classification  3  , object detection  4  , semantic segmentation  5  , etc.. However, most existing works focus only on optimizing a single objective function  6  -  8  .\nIn many real-world applications, there may exist more than one objective function  9  . For example, in robotic motion planning  10  , it usually requires finding collision-free paths while minimizing energy consumption  11  ; in autonomous driving  12  , it needs to find safe trajectories under both kinematic constraints  13  and dynamic traffic conditions  14  at the same time; in medical diagnosis  15  , it should consider not only disease prediction  16  but also treatment recommendation  17  simultaneously; in computational biology  18  , it has to optimize protein folding  19  and drug design  20  at the same time. Therefore, it becomes necessary to develop new algorithms to handle multi-objective optimization problems  21  .\nRecently, deep reinforcement learning  22  was introduced to address multiobjective optimization problems  23  . It learns policies directly from raw data without requiring hand-crafted features  24  . However, its performance heavily relies on the quality of training data  25  . Moreover, it often suffers from high sample complexity  26  due to the large number of",
        "watermark_text": "We suggest an excitable convolutional neural network ( CNN ) model for solving the 2D route planning problem in this project . The proposed approach is based on the idea that the output of each layer can be regarded as a potential field , and the finished problem will emerge when all layers are united together .We see how to train such a multi - layered CNN use backpropagation through time with gradient clipping . In addition , we also demonstrate two different methods to mix multiple fields into one single field by using either linear or nonlinear combination operations .Finally , we prove our approach on numerous benchmark problems namely maze navigation , robotics motion plan , and autonomous driving . Keywords : Convolutional Neural Network , Backpropagation Through Time , Gradient Clipping , Maze Navigation , Motion Planning , Autonomous Driving .1 Introduction Convolutional neural systems have been widely using in computer vision solutions 1 . Recently , they were applied to solve many kinds of algorithms problems 2 , which include image characterization 3 , object discovery 4 , semantic segmentation 5 , etc . .However , most existing works concentrate only on optimizing a single objective function 6 - 8 . In many real - time users , there may contain more than one objective function 9 .For instance , in robotic motion plan 10 , it generally needs finding collision - safe paths while minimizing electricity demand 11 ; in autonomous steering 12 , it must to find safe trajectories under both kinematic limits 13 and dynamic transportation conditions 14 at the same time ; in medical treatment 15 , it should consider not only disease prediction 16 but also treatment recommendation 17 simultaneously ; in computational genetics 18 , it has to optimize protein folding 19 and drug design 20 at the same time . Therefore , it becomes necessary to develop new algorithms to manage multi - goal optimization problems 21 .Recently , deep reinforcement testing 22 was introduced to address multiobjective optimization problems 23 . It learns policies directly from pure data without using hand - crafted functionality 24 .However , its reliability strongly depends on the performance of training data 25 . Moreover , it often suffers from high sample complexity 26 related to the huge amount of",
        "rewrite_text": "We propose an innovative convolutional neural network (CNN) model designed to tackle the 2D route planning problem in this project. Our method hinges on the concept that the output of each layer can be visualized as a potential field, and the solution to the problem arises when all layers work in unison. We explain how to train this multi-layered CNN using backpropagation through time, incorporating gradient clipping techniques. Additionally, we present two distinct methods for merging multiple potential fields into a single field, utilizing either linear or nonlinear combination techniques. We validate our approach against several benchmark challenges, including maze navigation, robotic motion planning, and autonomous driving. \n\nKeywords: Convolutional Neural Network, Backpropagation Through Time, Gradient Clipping, Maze Navigation, Motion Planning, Autonomous Driving.\n\n1. Introduction \nConvolutional neural networks have found extensive applications in computer vision. Recently, they have been employed to solve various algorithmic problems, such as image classification, object detection, and semantic segmentation. However, most current research focuses solely on optimizing a single objective function. In many real-world scenarios, multiple objective functions may be present. For example, in robotic motion planning, it is often necessary to identify collision-free paths while minimizing energy consumption; in autonomous navigation, safe trajectories must be determined while considering both kinematic constraints and dynamic environmental conditions; in healthcare, disease prediction must be balanced with treatment recommendations; and in computational genetics, simultaneous optimization of protein folding and drug design is required. Consequently, there is a growing need for new algorithms capable of addressing multi-objective optimization problems. Recently, deep reinforcement learning has emerged as a solution for multi-objective optimization tasks, allowing for policy learning directly from raw data without the need for manually crafted features. However, its effectiveness is heavily reliant on the quality of the training data and often faces challenges related to high sample complexity due to the substantial amount of data required.",
        "ori-fast-z-score": 0.6172133998483676,
        "water-fast-z-score": 9.14991421995628,
        "rewrite-fast-z-score": 1.0613372610104648
    },
    {
        "original_text": "We present an analysis of the temperature dependence of thermally stimulated luminescent (TSL) glow curves in terms of the nonstationary electron-phonon relaxation theory, which does not assume that the system is close to equilibrium at any time during its evolution.  We show how this approach can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL data obtained on different types of materials. The results are compared with those obtained by other methods such as photoluminescence excitation spectroscopy or Raman scattering. In particular we demonstrate that our method allows one to determine the energy gap between the conduction band minimum and valence band maximum in semiconductors. This work was supported by Russian Science Foundation grant No. 14-50-00040. DOI: 10.1063/1.4935190 \nI. INTRODUCTORY REMARK\nThe study of luminescence phenomena has been attracting considerable attention over many years because it provides valuable information about electronic structure and optical properties of solids  1  . Thermal stimulation luminescence (TSL), also known as optically stimulated luminescence (OSL), is particularly useful since it enables us to probe the distribution function of electrons excited into the conduction band  2  .\nIn recent decades there have been numerous attempts to develop theoretical models describing various aspects of luminescence processes  3  , including thermal stimulation luminescence  4  -  8  . However, most of these works were based on the assumption that the system under consideration is always close to equilibrium  9  . As a result they cannot describe correctly some important features observed experimentally  10  . For example, the shape of the TSL glow curve depends strongly on the type of material  11  : while in insulators it usually exhibits a single peak  12  , in metals it often consists of several peaks  13  . Moreover, even within the same class of materials, e.g., semiconductor crystals  14  , the number of peaks may vary depending on the doping level  15  . These observations cannot be explained using existing theories  16  .",
        "watermark_text": "We present an assessment of the temperature dependence of thermally stimulated luminescent ( TSL ) glow curves in terms of the nonstationary electron - phonon relaxation hypothesis , which does not assume that the system is close to equilibrium at any time during its evolve . We see how this methodology can be used for extracting information about the phonon spectrum and the density of states of charge carriers from TSL information obtained on various types of substances .The results are compared with those achieved by other methods such as photoluminescence excitation spectroscopy or Raman absorption . In particular we prove that our technique permits one to estimate the electricity gap between the conduction band minimum and valence band maximum in semiconductors .This project was supported by Russian Science Foundation gift No . 14 - 50 - 00040 .DOI : 10 . 1063 / 1 . 4935190 I . INTRODUCTORY REMARK The investigation of luminescence effects has been drawing tremendous attention over numerous years because it gives valuable info about electronic properties and electronic properties of solids 1 .Thermal stimulation luminescence ( TSL ) , sometimes called as optically stimulated luminescence ( OSL ) , is especially useful since it allows us to probe the distribution behavior of electrons excited into the conduction band 2 . In recent generations there have been numerous attempts to develop conceptual models explaining various parts of luminescence events 3 , notably heat stimulation luminescence 4 - 8 .However , most of these works were based on the assumption that the process under consideration is usually nearly to equilibrium 9 . As a result they cannot describe correctly some important features discovered experimentally 10 .For instance , the form of the TSL flicker curve varies strongly on the kind of material 11 : while in insulators it generally exhibits a single peak 12 , in metals it often consists of several peaks 13 . Moreover , even within the same category of substances , e . g . , semiconductor crystals 14 , the number of peaks may differ depending on the doping level 15 .These measurements cannot be understood using existing models 16 .",
        "rewrite_text": "We present an evaluation of the temperature dependence of thermally stimulated luminescence (TSL) glow curves using the nonstationary electron-phonon relaxation hypothesis. This approach does not assume that the system remains close to equilibrium during its evolution. We demonstrate how this methodology can be employed to extract information about the phonon spectrum and the density of states of charge carriers from TSL data collected from various materials. We compare these findings with results obtained through other techniques such as photoluminescence excitation spectroscopy and Raman absorption. Notably, we show that our technique allows for the estimation of the energy gap between the conduction band minimum and valence band maximum in semiconductors. This project was supported by the Russian Science Foundation, grant No. 14-50-00040. DOI: 10.1063/1.4935190. \n\nINTRODUCTORY REMARK\nThe study of luminescence phenomena has garnered significant attention over the years due to its ability to provide crucial insights into the electronic properties of solids. Thermal stimulation luminescence (TSL), often referred to as optically stimulated luminescence (OSL), is particularly valuable as it enables probing the behavior of electrons excited into the conduction band. In recent years, numerous attempts have been made to formulate conceptual models that explain different aspects of luminescence phenomena, especially thermal stimulation luminescence. However, most of these models rely on the assumption that the processes involved are close to equilibrium. Consequently, they fail to accurately capture some important experimentally observed features. For example, the shape of the TSL glow curve varies significantly depending on the material: in insulators, it typically shows a single peak, while in metals, it often presents multiple peaks. Additionally, even within the same category of materials, such as semiconductor crystals, the number of peaks can vary based on the doping level. These observations cannot be adequately explained using the existing models.",
        "ori-fast-z-score": -2.3664319132398464,
        "water-fast-z-score": 7.437357441610946,
        "rewrite-fast-z-score": -1.1272429603813559
    },
    {
        "original_text": "The influence of pulsed magnetic fields on the relaxation processes in HTSC was investigated by measuring the temperature dependence of resistance and Hall coefficient for samples with different oxygen content (d = 0, 1). The results show that the application of pulsed magnetic fields leads to an increase in the resistivity and Hall mobility of the sample with d = 0. This effect is explained as due to the appearance of additional scattering centers caused by defects formed during the process of magnetization reversal. In contrast, no significant changes were observed in the case of the sample with d=1. It can be assumed that this difference is associated with the presence of structural disordering in the crystal lattice of the latter compound. \n \n Keywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defects Formation, Magnetoresistance, Hall Effect. Introduction \n \n Investigation of relaxation phenomena in high temperature superconductors under the action of pulsed external magnetic fields has been attracting considerable attention recently  1 - 5  . These studies are important both for understanding the physics of these materials and for practical applications  6  -  8  . \n \n In particular, it should be noted that the investigation of relaxation processes in HTSCs allows one to study the dynamics of defect formation  9  , which plays an important role in determining their transport properties  10  . At present there are several models describing the mechanism of defect generation  11  -  13  . However, none of them takes into account the possibility of defect formation induced by the action of pulsed fields  14  . \nExperimental details\n\nIn our work we used single crystals of two compounds with different oxygen content: HoBa 2 Cu 3 O 7−δ (HBS) and YBa 2 Cu 3 O 6+δ (YBS), grown using the floating zone method  15  . The oxygen concentration in the samples was determined by iodometric titration  16  . The typical size of the samples was about 5 × 4 mm 2 . The measurements were carried out in liquid helium cryostats equipped with pulse magnets  17  . The maximum value of the magnetic induction reached up to B max =",
        "watermark_text": "The impact of pulsed magnetic fields on the relaxation processes in HTSC was investigated by monitoring the temperature dependence of resistance and Hall coefficient for specimens with various oxygen composition ( d = 0 , 1 ) . The results show that the introduction of pulsed magnetic fields leads to an increase in the resistivity and Hall velocity of the sample with d = 0 .This phenomenon is understood as owing to the appearance of added scattering centers caused by defects formed during the process of magnetization reversal . In comparison , no major changes were detected in the case of the sample with d = 1 .It can be assumed that this contrast is associated with the presence of structural disordering in the crystal structures of the latter chemical . Keywords : High - Tc Superconductor , Pulsed Magnetic Field , Relaxation Processes , Defects Formation , Magnetoresistance , Hall Effect .Introduction Investigation of relaxation effects in high heat superconductors under the action of pulsed external magnetic waves has been drawing greater notice lately 1 - 5 . These studies are important both for studying the physics of these structures and for useful use 6 - 8 .In particular , it should be mentioned that the examination of relaxation processes in HTSCs allows one to study the dynamics of defect form 9 , which plays an important role in establishing their transport properties 10 . At currently there are several models explaining the process of defect generation 11 - 13 .However , none of them took into consideration the danger of defect development caused by the activity of pulsed fields 14 . Experimental details In our work we using single crystals of two compounds with varying oxygen composition : HoBa 2 Cu 3 O 7−δ ( HBS ) and YBa 2 Cu 3 O 6 + δ ( YBS ) , grown using the sliding zone method 15 .The oxygen composition in the tests was calculated by iodometric titration 16 . The typical size of the samples was about 5 × 4 mm 2 .The measurements were carried out in pure helium cryostats fitted with pulse magnets 17 . The maximum value of the magnetic induction reached up to B max =",
        "rewrite_text": "The effects of pulsed magnetic fields on relaxation processes in high-temperature superconductors (HTSC) were examined by assessing the temperature dependence of resistance and Hall coefficient in samples with varying oxygen compositions (d = 0, 1). The findings indicate that applying pulsed magnetic fields increases the resistivity and Hall velocity of the sample with d = 0. This increase is attributed to the creation of additional scattering centers due to defects that emerge during the magnetization reversal process. Conversely, no significant changes were observed in the sample with d = 1, suggesting that the differences might be linked to the presence of structural disorder in the crystal structure of that compound. \n\nKeywords: High-Tc Superconductor, Pulsed Magnetic Field, Relaxation Processes, Defect Formation, Magnetoresistance, Hall Effect. \n\nIntroduction: Recent studies investigating the relaxation effects in high-temperature superconductors under the influence of pulsed external magnetic fields have garnered increased attention. These investigations are crucial for understanding the underlying physics of these materials and have practical applications. Notably, exploring relaxation processes in HTSCs enhances our understanding of defect dynamics, which are pivotal for determining their transport properties. Several models exist that explain defect generation; however, none have addressed the risks posed by pulsed fields on defect development.\n\nExperimental Details: In this study, we utilized single crystals of two compounds with differing oxygen compositions: HoBa2Cu3O7−δ (HBS) and YBa2Cu3O6+δ (YBS), both synthesized using the sliding zone method. The oxygen content in the samples was determined through iodometric titration. The typical dimensions of the samples were approximately 5 × 4 mm². Measurements were conducted in a pure helium cryostat equipped with pulse magnets, achieving a maximum magnetic induction of Bmax = ...",
        "ori-fast-z-score": -1.1832159566199232,
        "water-fast-z-score": 7.268326590665242,
        "rewrite-fast-z-score": -0.5222329678670935
    },
    {
        "original_text": "We have recently shown that the primordial spectrum of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space-time, which is known as the cosmological slingshot effect (CSE). In this work we show how to match the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3-year data release (WMAP3) and compare them against other models. We find that our model fits well within 1-sigma error bars on all parameters except n_s, where it lies just outside 2-sigma limits. The best-fit values are given by:  H_0 = 72.6 +/- 0.9 km/s/Mpc,  Omega_m = 0.26 +/- 0.01,   Omega_Lambda = 0.74 +/- 0.02,    n_s = 0.96 +/- 0.06.   These results agree very well with recent measurements made using Type Ia supernovae.  Our analysis shows that the CSE provides a viable alternative explanation for the origin of cosmic structure formation.",
        "watermark_text": "We have recently shown that the primordial range of density fluctuations can be obtained by solving an initial value problem for a massless scalar field in deSitter space - time , which is known as the cosmological slingshot phenomenon ( CSE ) . In this research we find how to tie the CSE predictions with those of the Wilkinson Microwave Anisotropy Probe 3 - month data release ( WMAP3 ) and compare them against other models .We see that our model fits well within 1 - sigma error bars on all parameters except k _ s , where it lies just outside 2 - sigma limits . The best - fitting values are given by : H _ 0 = 72 . 6 + / - 0 . 9 km / s / Mpc , Omega _ m = 0 . 26 + / - 0 . 01 , Omega _ Lambda = 0 . 74 + / - 0 . 02 , n _ s = 0 . 96 + / - 0 . 06 .These conclusions agree very best with recent observations made using Type Ia supernovae . Our study shows that the CSE provides a viable alternative theory for the origin of universe mechanism formation .",
        "rewrite_text": "We have recently demonstrated that the primordial spectrum of density fluctuations can be derived by addressing an initial value problem for a massless scalar field in de Sitter spacetime, a phenomenon known as the cosmological slingshot effect (CSE). In this study, we explore how to connect the predictions of the CSE with the data from the three-month release of the Wilkinson Microwave Anisotropy Probe (WMAP3) and compare them to other theoretical models. Our findings indicate that our model aligns well within the 1-sigma confidence intervals for all parameters except for k_s, where it slightly exceeds the 2-sigma limits. The best-fit parameters are as follows: H_0 = 72.6 ± 0.9 km/s/Mpc, Ω_m = 0.26 ± 0.01, Ω_Λ = 0.74 ± 0.02, and n_s = 0.96 ± 0.06. These results are in strong agreement with recent observations made using Type Ia supernovae. Our research suggests that the CSE presents a robust alternative theory for the mechanisms underlying the formation of the universe.",
        "ori-fast-z-score": -0.9271726499455306,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We study theoretically how the conformational properties of flexible and semiflexible chains are affected by electrostatic interactions with ions dissolved in solution, using Monte Carlo simulations for different values of ionic strength I. We find that the persistence length increases as a function of I due to screening effects between charged monomers along the chain backbone. The effect is more pronounced when increasing the charge density per unit length ql or decreasing the Bjerrum length lB = e2/4πε0kBT . In addition we show that the radius of gyration scales differently depending on whether the chain is flexible (Rg ∝ N1/2) or semiflexible (Rg ∝ N1). Finally, we discuss our results within the context of recent experiments performed on DNA molecules immersed into an electrolyte solution. \n \n Introduction \n \n Polymeric systems play important roles in many biological processes such as protein folding  1  , gene regulation  2  , cell division  3  , etc., where they often interact strongly with other macromolecules and/or solvents  4  . For example, it has been shown experimentally  5  that the structure of double stranded DNA can be significantly altered upon interaction with cations  6  . This behavior is also observed in synthetic biopolymers like polypeptides  7, 8  which have attracted considerable attention recently  9  .\n \nIn this work we focus on the role played by electrostatics on the conformation of polymer chains immersed in salt solutions. To do so, we use Monte Carlo simulations  10  to investigate the dependence of the persistence length lp  11  and the radius of gyration: Rg2 = ⟨r2⟩ − ⟨r⟩2 /N  12  on the concentration of added salt C = ∑iZiCi/V , where Zi denotes the valence of species i and Ci its molar concentration  13  . Here V stands for the volume occupied by the system under consideration. \nThe main goal of this investigation is twofold. First, we want to understand how the presence of counterions affects the structural properties of flexible and semifflexible chains. Second, we aim at comparing these theoretical predictions against experimental data obtained for",
        "watermark_text": "We research theoretically how the conformational characteristics of flexible and semiflexible chains are influenced by electrostatic interactions with ions suspended in solution , using Monte Carlo simulations for different values of ionic resistance I . We see that the persistence length changes as a function of I due to screening influences between charged monomers along the chain backbone .The phenomenon is more pronounced when varying the charge density per unit size ql or decreasing the Bjerrum size lB = e2 / 4πε0kBT . In addition we find that the radius of gyration scales differently depending on whether the chain is flexible ( Rg [UNK] N1 / 2 ) or semiflexible ( Rg [UNK] N1 ) .Finally , we explain our findings within the context of recent experiments conducted on DNA molecules immersed into an electrolyte solution . Introduction Polymeric materials act key roles in different biological pathways such as protein folding 1 , gene control 2 , cell division 3 , etc . , where they frequently interact heavily with other macromolecules and / or solvents 4 .For instance , it has been shown experimentally 5 that the composition of double stranded DNA can be substantially altered upon activity with cations 6 . This phenomenon is also observed in artificial biopolymers like polypeptides 7 , 8 which have garnered considerable scrutiny recently 9 .In this research we focus on the importance played by electrostatics on the conformation of polymer bars immersed in salt solutions . To do so , we utilize Monte Carlo simulations 10 to examine the dependence of the persistence length lp 11 and the radius of gyration : Rg2 = ⟨ r2 ⟩ − ⟨ r ⟩ 2 / N 12 on the density of added sodium C = [UNK] / V , where Zi denotes the valence of genus i and Ci its molar density 13 .Here V stands for the quantity occupied by the process under consideration . The main goal of this study is twofold .First , we try to explain how the presence of counterions affects the structural properties of flexible and semifflexible networks . Second , we focus at comparing these theoretical estimates against empirical data received for",
        "rewrite_text": "We conduct a theoretical investigation into how the conformational features of flexible and semiflexible chains are affected by electrostatic interactions with ions in solution, using Monte Carlo simulations across various levels of ionic resistance (I). Our findings indicate that the persistence length varies as a function of I due to screening effects among charged monomers along the chain's backbone. This effect becomes more significant when altering the charge density per unit length (ql) or when reducing the Bjerrum length (lB = e² / 4πε₀kBT). Furthermore, we discover that the radius of gyration scales differently based on whether the chain is flexible (Rg ∝ N¹/²) or semiflexible (Rg ∝ N). We contextualize these results in light of recent experimental studies involving DNA molecules in electrolyte solutions. \n\nIntroduction: Polymeric materials are crucial in various biological processes such as protein folding, gene regulation, and cell division, often interacting closely with other macromolecules or solvents. For example, experimental research has demonstrated that the composition of double-stranded DNA can be significantly altered by interactions with cations. This behavior is also evident in artificial biopolymers like polypeptides, which have recently attracted considerable attention. In this study, we emphasize the role of electrostatics in influencing the conformation of polymer chains immersed in salt solutions. To accomplish this, we apply Monte Carlo simulations to explore how the persistence length (lp) and the radius of gyration (Rg² = ⟨r²⟩ - ⟨r⟩² / N) depend on the concentration of added sodium (C = [UNK] / V), where Zi represents the valence of species i and Ci its molar density. Here, V denotes the volume occupied by the process in question. The primary objectives of this research are twofold: first, to elucidate how counterions influence the structural properties of flexible and semiflexible networks, and second, to compare our theoretical estimates with empirical data obtained for similar systems.",
        "ori-fast-z-score": -2.138089935299395,
        "water-fast-z-score": 7.542530330036968,
        "rewrite-fast-z-score": 0.0873704056661038
    },
    {
        "original_text": "We have studied the rheology of isotropic networks formed by crosslinking actin filaments with two different concentrations of biotin-avidin linkers, using microrheology experiments on single filament dynamics in combination with macrorheology measurements performed at low frequencies (0.01-10 Hz). We find that both microand macro-rheology are consistent with an elastic network model for which we can extract values for the number density of links between filaments as well as their stiffness. The results show that increasing the concentration of avidin leads to denser networks with stiffer links. This effect is more pronounced when the initial concentration of actin filaments is higher. Our findings suggest that the mechanical properties of actomyosin gels may be tunable through changes in the amount and/or type of crosslinks present within these systems. In living cells, cytoskeletal structures such as stress fibers or focal adhesions provide physical connections between cell components and play important roles in determining cellular mechanics  1  . These structures consist of bundles of semiflexible biopolymers known as actin filaments  2  , which are connected together via specific protein complexes called crosslinks  3  .\nIn recent years there has been growing interest in understanding how the mechanical properties of biological materials depend on the microscopic structure of the underlying networks  4  . For example, it was shown recently that the viscoelasticity of reconstituted actomyosin gels depends strongly on the presence of myosins  5  . However, despite this progress our knowledge about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building blocks remains limited  6  .",
        "watermark_text": "We have researched the rheology of isotropic bands formed by crosslinking actin filaments with two different amounts of biotin - avidin linkers , using microrheology experiments on double filament dynamics in combination with macrorheology measurements completed at low frequencies ( 0 . 01 - 10 Hz ) . We see that both microand macro - rheology are compatible with an elastic network theory for which we can extract parameters for the number density of links between filaments as also as their stiffness .The results show that raising the density of avidin leads to denser networks with stiffer links . This phenomenon is more pronounced when the first concentration of actin filaments is higher .Our findings show that the mechanical behavior of actomyosin gels might be tunable through alterations in the quantity and / or type of crosslinks observed within these systems . In living cells , cytoskeletal structures such as stress fibers or focal adhesions contribute physical bridges between tissue systems and play essential roles in establishing cellular dynamics 1 .These structures composed of bundles of semiflexible biopolymers known as actin filaments 2 , which are connected together via particular protein complexes called crosslinks 3 . In recent years there has been growing interest in understanding how the mechanical behavior of biological materials depend on the microscopic shape of the underlying systems 4 .For instance , it was shown lately that the viscoelasticity of reconstituted actomyosin gels relies highly on the presence of myosins 5 . However , despite this progress our information about the relationship between the macroscopic behavior of complex fluids and the microstructure of the constituent building components remains restricted 6 .",
        "rewrite_text": "We conducted research on the rheological properties of isotropic bands created by crosslinking actin filaments with varying amounts of biotin-avidin linkers. This was achieved through microrheology experiments focusing on the dynamics of double filaments, alongside macrorheology measurements carried out at low frequencies (0.01 - 10 Hz). Our findings indicate that both micro- and macrorheological data align with an elastic network theory, allowing us to derive parameters such as the number density of links between filaments and their stiffness. The results demonstrate that increasing avidin density results in denser networks with stiffer connections, a trend that is especially pronounced at higher initial concentrations of actin filaments. These observations suggest that the mechanical properties of actomyosin gels may be adjustable by modifying either the quantity or type of crosslinks within these systems. In living cells, cytoskeletal structures, like stress fibers and focal adhesions, serve as physical connections between tissue systems and are crucial for cellular dynamics. These structures consist of bundles of semiflexible biopolymers known as actin filaments, which are linked by specific protein complexes called crosslinks. Recently, there has been increased interest in uncovering how the mechanical properties of biological materials are influenced by the microscopic architecture of their underlying structures. For example, it has been demonstrated that the viscoelasticity of reconstituted actomyosin gels is significantly affected by the presence of myosins. Nonetheless, our understanding of how the macroscopic behavior of complex fluids correlates with the microstructure of their constituent components remains limited.",
        "ori-fast-z-score": -0.19611613513818404,
        "water-fast-z-score": 7.19290373059934,
        "rewrite-fast-z-score": 1.9291577137538762
    },
    {
        "original_text": "We present here an application of the Scenario Machine (SM) to calculate the luminosity functions for different types of binaries in our Galaxy, including black hole and neutron star systems as well as white dwarf - main sequence stars. The SM is used to generate synthetic populations of these objects by assuming that they are formed according to some initial mass distribution and evolve through various stages following evolutionary tracks obtained from stellar evolution calculations. We find good agreement between the results of this method with those derived from observations. This work was supported by NASA grant NAG5-10842. Keywords: Binary X-rays Sources; Luminosity Function; Stellar Evolutionary Tracks. 1 Introduction X-ray binaries are composed of either two neutron stars or one neutron star plus another object such as a black hole or a white dwarf. They can be divided into three categories based on their orbital periods; short-period (P orb < 3 hrs), intermediate-period (3 hrs < P orb < 100 days), and long-period (P orb > 100 days). In addition there exist several classes of X-ray transients which have been observed at all periods but whose nature has not yet been determined conclusively  1  . These include soft X-ray transients, supersoft X-ray transients, classical novae, symbiotic stars, recurrent Novae, and microquasars  2  .\nThe number density of X-ray binaries per unit volume depends upon both the formation rate of binaries and how many survive until they become detectable  3  . Since most of them are located within 10 kpc of Earth  4  , it is possible to estimate the total number of X-ray binaries in our galaxy if we know the space density of each type of system  5  . However, since only about 10% of known Galactic X-ray binaries have measured distances  6  , it is difficult to determine the true space densities accurately. Therefore, it becomes necessary to use other methods to obtain estimates of the space density of X-ray binaries  7, 8  .",
        "watermark_text": "We see here an use of the Scenario Machine ( SM ) to estimate the luminosity functions for different kinds of binaries in our Galaxy , notably black hole and neutron star systems as well as black dwarf - principal sequence stars . The SM is utilized to produce synthetic populations of these objects by assuming that they are created due to some early mass distribution and evolve through several stages following evolutionary tracks derived from planetary evolution calculations .We get good agreement between the results of this algorithm with those generated from measurements . This research was supported by NASA grant NAG5 - 10842 .Keywords : Binary X - radiation Sources ; Luminosity Function ; Stellar Evolutionary Tracks . 1 Introduction X - ray binaries are composed of either two neutron galaxies or one neutron galaxy plus another object such as a black hole or a black dwarf .They can be grouped into three categories based on their orbital periods ; small - time ( P orb < 3 hrs ) , intermediate - time ( 3 hrs < P orb < 100 months ) , and long - time ( P orb > 100 months ) . In addition there exist several classes of X - ray transients which have been observed at all periods but whose nature has not already been determined conclusively 1 .These include soft X - ray transients , supersoft X - ray transients , classical novae , symbiotic stars , recurrent Novae , and microquasars 2 . The amount density of X - ray binaries per unit volume depends upon both the formation rate of binaries and how many live until they become detectable 3 .Since most of them are situated within 10 kpc of Earth 4 , it is easy to estimate the total number of X - ray binaries in our universe if we determine the space density of each type of system 5 . However , since only about 10 % of known Galactic X - ray binaries have recorded distances 6 , it is harder to predict the true space densities accurately .Therefore , it becomes necessary to use other methods to obtain calculations of the space density of X - ray binaries 7 , 8 .",
        "rewrite_text": "In this study, we utilize the Scenario Machine (SM) to estimate the luminosity functions of various binary systems within our Galaxy, particularly focusing on black hole and neutron star systems, as well as black dwarf and main sequence stars. The SM generates synthetic populations of these celestial objects by assuming their formation is influenced by early mass distributions and that they evolve through various stages, which are modeled using evolutionary tracks derived from planetary evolution calculations. Our findings show strong agreement between the outputs of this algorithm and observational measurements. This research was made possible through support from NASA grant NAG5-10842. \n\n**Keywords**: Binary X-ray Sources; Luminosity Function; Stellar Evolutionary Tracks. \n\n**1 Introduction**: X-ray binaries consist of pairs that may include two neutron stars or a neutron star combined with another object, such as a black hole or a black dwarf. These systems can be classified into three categories based on their orbital periods: short-period (P_orb < 3 hours), intermediate-period (3 hours < P_orb < 100 months), and long-period (P_orb > 100 months). Moreover, several classes of X-ray transients have been observed across all periods, though their specific nature remains unresolved. These include soft X-ray transients, supersoft X-ray transients, classical novae, symbiotic stars, recurrent novae, and microquasars. The space density of X-ray binaries per unit volume relies on both the rate of binary formation and the lifespan until their detection. Given that most are located within 10 kpc of Earth, estimating the total number of X-ray binaries in our universe becomes feasible once the space density for each system type is established. However, since only about 10% of known Galactic X-ray binaries have recorded distances, accurately predicting true space densities proves challenging. Hence, additional methods are necessary to calculate the space density of X-ray binaries.",
        "ori-fast-z-score": 0.3481553119113957,
        "water-fast-z-score": 7.3704347409550195,
        "rewrite-fast-z-score": 2.2901101101359216
    },
    {
        "original_text": "We present near-infrared (NIR) spectroscopy obtained with Subaru/HDS on day +16 after explosion for the peculiar type Ib supernova SN2006jc, which shows prominent dust formation in its dense shell. The NIR spectrum is dominated by strong P-Cygni profiles of H I Balmer lines and Fe II multiplets at 4233-4245 Å . We find that these features are well reproduced by our non-LTE model calculations assuming an electron density ne = 1 x 10^9 cm-3 , temperature Te = 5500 K , and mass-loss rate oḟ M = 2 x 10^-6 M_sun/yr. In addition to the above mentioned features, we detect weak but significant He II 4686 emission line feature in the red wing of the Hα profile. This suggests that there may be some contribution from helium recombination radiation to the observed fluxes of hydrogenic lines.",
        "watermark_text": "We use near - infrared ( NIR ) spectroscopy acquired with Subaru / HDS on week + 16 after explosion for the unusual type Ib supernova SN2006jc , which shows significant dust form in its dense shell . The NIR spectrum is dominated by strong P - Cygni profiles of H I Balmer bands and Fe II multiplets at 4233 - 4245 Å .We see that these characteristics are better illustrated by our non - LTE model calculations assuming an electron concentration ne = 1 x 10 ^ 9 centimeters - 3 , temperature Te = 5500 K , and mass - loss rate [UNK] M = 2 x 10 ^ - 6 M _ sun / yr . In addition to the above mentioned properties , we perceive slight but significant He II 4686 absorption line feature in the red wing of the Hα profile .This implies that there may be some influence from helium recombination emission to the seen fluxes of hydrogenic lines .",
        "rewrite_text": "We conducted near-infrared (NIR) spectroscopy using the Subaru/HDS instrument during week +16 after the explosion of the unusual Type Ib supernova SN2006jc, which exhibits significant dust formation within its dense shell. The NIR spectrum prominently features strong P-Cygni profiles of the H I Balmer series as well as Fe II multiplets between 4233 and 4245 Å. Our non-LTE model calculations, assuming an electron concentration of ne = 1 x 10^9 cm^-3, a temperature of Te = 5500 K, and a mass-loss rate of [UNK] M = 2 x 10^-6 M_sun/yr, better illustrate these characteristics. Additionally, we detect a subtle yet notable absorption line feature of He II at 4686 Å in the red wing of the Hα profile, suggesting a potential influence from helium recombination emission on the observed hydrogenic line fluxes.",
        "ori-fast-z-score": -1.4832396974191326,
        "water-fast-z-score": 4.08248290463863,
        "rewrite-fast-z-score": 1.0690449676496976
    },
    {
        "original_text": "We present an approach to decision tree learning that uses views over the data in order to improve efficiency and scalability. The proposed method is based on the idea of using multiple views, each one capturing different aspects or features of the same dataset. We show how this can be achieved by defining a set of views for each node in the decision tree model being learned. These views are then used as input to a modified version of the standard ID3 algorithm which learns the decision tree structure. Our experimental results demonstrate significant improvements in terms of both accuracy and execution time when compared against existing approaches. Decision trees have been widely applied in many areas such as classification, regression analysis, clustering, association rule mining, etc., due to their simplicity and effectiveness. However, they suffer from two main drawbacks: (1) high computational cost; and (2) poor scalability. In particular, the number of possible splits at any given internal node grows exponentially with respect to the size of its parent s training sample. This makes it difficult to build large decision trees efficiently. To address these problems we propose a novel approach called Relational Views-based Decision Tree Learning (RV-DTL). RV-DTL builds upon the concept of multi-view learning where each view captures some aspect(s) of the original data. More specifically, our approach defines a set of views for every internal node in the decision tree being built. Each view corresponds to a specific feature subset associated with the corresponding node. Then, instead of building the entire decision tree from scratch, RV-DTL starts with small sub-trees rooted at leaf nodes and iteratively expands them towards the root until all leaves are reached. At each expansion step, RV-DTL selects the best split among those available according to the information gain criterion. Finally, the final decision tree is constructed by merging together...",
        "watermark_text": "We suggest an approach to decision tree learning that using views over the information in attempts to promote efficiency and scalability . The proposed approach is based on the idea of using multiple views , each one capturing different components or elements of the same dataset .We see how this can be obtained by creating a setting of views for each node in the decision tree model being taught . These views are then used as input to a altered version of the standard ID3 algorithm which understands the reasoning tree structure .Our research results show considerable improvements in terms of both precision and execution time when compared against existing techniques . Decision trees have been widely applied in different areas such as classification , regression evaluation , clustering , association rule extraction , etc . , owing to their simplicity and effectiveness .However , they suffer from two principal drawbacks : ( 1 ) great computational cost ; and ( 2 ) poor scalability . In particular , the number of possible splits at any certain internal node grows exponentially with regard to the length of its parent s training sample .This creates it difficult to build large decision forests quickly . To address these problems we propose a new approach titled Relational Views - based Decision Tree Learning ( RV - DTL ) .RV - DTL relies upon the idea of multi - view testing where each vision preserves some aspect ( s ) of the original material . More specifically , our approach establishes a setting of views for every internal node in the decision tree being built .Each view belongs to a certain characteristic subset associated with the respective node . Then , rather of building the entire choice tree from scratch , RV - DTL began with little sub - forests rooted at leaf nodes and iteratively builds them towards the root until all leaves are reached .At each expansion stage , RV - DTL selects the best split among those available based to the information gain criterion . Finally , the finished choice table is built by merging together . . .",
        "rewrite_text": "We propose a new approach to decision tree learning that leverages multiple views of the information to enhance both efficiency and scalability. This method is founded on the concept of utilizing various views, where each view captures distinct aspects of the same dataset. Specifically, we establish a set of views for each node in the decision tree being constructed. These views serve as input for a modified version of the standard ID3 algorithm, which understands the structure of the reasoning tree. Our research demonstrates significant improvements in both accuracy and execution speed when compared to existing methods. Decision trees are widely used across various fields, including classification, regression evaluation, clustering, and association rule extraction, due to their simplicity and effectiveness. However, they face two main challenges: high computational costs and limited scalability. The number of potential splits at any given internal node increases exponentially with the size of its parent training sample, making it difficult to quickly construct large decision forests. To tackle these issues, we present a novel approach called Relational Views-based Decision Tree Learning (RV-DTL). RV-DTL is based on the idea of multi-view testing, where each view highlights certain aspects of the original data. Our approach creates a set of views for each internal node in the decision tree. Each view corresponds to a specific characteristic subset related to that node. Rather than building the entire decision tree from the ground up, RV-DTL initiates small sub-forests at the leaf nodes and iteratively constructs them towards the root until all leaves are included. During each expansion phase, RV-DTL selects the optimal split based on the information gain criterion. Ultimately, the completed decision tree is formed by merging these sub-forests.",
        "ori-fast-z-score": -1.7905475715715027,
        "water-fast-z-score": 7.916735362204594,
        "rewrite-fast-z-score": -0.6531972647421809
    },
    {
        "original_text": "The Coulomb drag effect is the phenomenon where one charge carrier can transfer energy to another by exchanging virtual phonons, leading to an electric current in the second carrier that opposes its own motion.  In this work we study the Coulomb drag between two graphene sheets separated by a dielectric spacer layer and subject to different gate voltages. We find that for small separation distances (less than 10 nm) there are significant deviations from the predictions based on the standard theory developed for bulk materials. These deviations arise due to the presence of evanescent modes which couple strongly with the carriers at low energies. For larger separations these effects become negligible as expected. The results presented here provide useful information about how to design devices such as transistors or thermoelectric generators using graphene layers. \nI. INTRODUCTIO N\nGraphene has attracted considerable attention recently because it exhibits unique electronic properties  1  . It consists of carbon atoms arranged into a honeycomb lattice structure and behaves like a two-dimensional electron gas when doped  2  .\nOne interesting property of graphene is the so-called Coulomb drag effect  3  , i.e., the generation of an electric current in a second sheet of electrons moving through a first sheet of electrons even if they do not interact directly  4  . This effect arises because both carriers exchange virtual phonons via their mutual interaction mediated by the substrate  5  . As a result, the current density in the second carrier depends on the velocity of the first carrier  6  . Since the discovery of the Coulomb drag effect in semiconductors  7, 8  many theoretical studies have been performed  9  -  11  . However, only very few experiments were carried out so far  12  -  14  mainly due to difficulties associated with fabricating samples with high quality interfaces  15  . Recently, several groups succeeded in growing high-quality epitaxial graphene  16  -  18  opening up new possibilities for studying the Coulomb drag effect experimentally  19  -  21  .",
        "watermark_text": "The Coulomb drag effect is the situation where one charge carrier can carry energy to another by exchanging virtual phonons , leading to an electric current in the second carrier that opposes its own movement . In this research we study the Coulomb drag between two graphene strands divided by a dielectric spacer coating and subject to different gate voltages .We see that for short separation distances ( fewer than 10 nm ) there are significant deviations from the estimates based on the standard theory created for bulk materials . These deviations occur due to the presence of evanescent modes which couple strongly with the carriers at low energies .For larger separations these changes become negligible as anticipated . The results presented here provide useful details about how to build electronics such as transistors or thermoelectric turbines using graphene layers .I . INTRODUCTIO N Graphene has garnered considerable scrutiny lately because it displays unusual electronic properties 1 . It consists of carbon atoms arranged into a honeycomb lattice structure and behaves like a two - dimensional electron gas when doped 2 .One interesting property of graphene is the so - called Coulomb drag effect 3 , i . e . , the generation of an electric current in a second sheet of electrons moved through a second sheet of electrons even if they do not interact directly 4 . This phenomenon arises because both carriers trade virtual phonons via their mutual interaction mediated by the substrate 5 .As a result , the current density in the second carrier varies on the velocity of the first carrier 6 . Since the discovery of the Coulomb drag effect in semiconductors 7 , 8 numerous conceptual research have been performed 9 - 11 .However , only very few experiments were carried out so far 12 - 14 chiefly due to difficulties related with fabricating samples with high quality interfaces 15 . Recently , various groups helped in growing high - grade epitaxial graphene 16 - 18 opening up new possibilities for studying the Coulomb drag effect experimentally 19 - 21 .",
        "rewrite_text": "The Coulomb drag effect refers to the phenomenon in which one charge carrier transfers energy to another by exchanging virtual phonons, resulting in an electric current in the second carrier that opposes its own motion. In this study, we investigate the Coulomb drag between two graphene strands separated by a dielectric spacer and subjected to varying gate voltages. We observe that at short separation distances (less than 10 nm), there are notable deviations from predictions based on standard theories developed for bulk materials. These deviations are attributed to the presence of evanescent modes that strongly couple with low-energy carriers. For larger separation distances, these effects diminish as expected. The findings presented here offer valuable insights for the development of electronic devices such as transistors and thermoelectric turbines using graphene layers.\n\nI. INTRODUCTION\n\nRecently, graphene has attracted significant attention due to its remarkable electronic properties. Composed of carbon atoms arranged in a honeycomb lattice, graphene behaves like a two-dimensional electron gas when doped. A noteworthy feature of graphene is the Coulomb drag effect—the generation of an electric current in a second sheet of electrons moved by a first sheet, despite the absence of direct interaction. This effect arises from the carriers exchanging virtual phonons through their interaction mediated by the substrate. Consequently, the current density in the second carrier depends on the velocity of the first carrier. Since the discovery of the Coulomb drag effect in semiconductors, numerous theoretical studies have been conducted. However, only a limited number of experiments have been performed, primarily due to challenges in fabricating samples with high-quality interfaces. Recently, advancements in the growth of high-quality epitaxial graphene have opened new avenues for experimental investigations of the Coulomb drag effect.",
        "ori-fast-z-score": 1.7439550769285392,
        "water-fast-z-score": 8.333333333333334,
        "rewrite-fast-z-score": 0.34299717028501764
    },
    {
        "original_text": "We report the observation of electron-hole puddles in graphene using scanning single-electron transistors (SETs). The SET is fabricated on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K. We observe that the conductance through the SET depends strongly on its position with respect to the underlying graphene sheet, which we attribute to local variations in charge carrier density induced by charged impurities trapped between the substrate and the graphene layer. This effect can be suppressed by applying a gate voltage Vg = -40 V across the graphene sample. Our results demonstrate that the use of SETs as probes for studying electronic properties of two-dimensional materials such as graphene has great potential. In recent years there have been significant advances in the fabrication of devices based on carbon nanotubes  1  , silicon nanowires  2  or semiconductor quantum dots  3  . These nanostructures are used as active elements in various types of sensors  4  , optoelectronic  5  and photovoltaic  6  applications. However, these structures suffer from several drawbacks including poor reproducibility due to their small size and low yield during growth processes  7, 8  .\nIn contrast, graphene  9  offers many advantages over other two dimensional materials  10  : it is mechanically flexible  11  , chemically stable  12  , biocompatible  13  and electrically conductive  14  . Moreover, it can be produced in large quantities via chemical vapor deposition  15  or mechanical exfoliation  16  techniques  17  . Recently, graphene-based field-effect transistors  18  were demonstrated  19, 20  opening up new avenues towards high-performance electronics  21  . Despite all these attractive features, however, one major challenge remains in achieving high-quality electrical contacts to graphene  22  .",
        "watermark_text": "We report the observation of electron - hole puddles in graphene using scanning single - ion transistors ( SETs ) . The SET is manufactured on top of an exfoliated monolayer graphene flake and operated at cryogenic temperatures down to 4 K . We establish that the conductance through the SET depends strongly on its position with regard to the underlying graphene cover , which we attribute to local differences in charge carrier density resulting by charged impurities stored between the substrate and the graphene layer .This phenomenon can be suppressed by using a gate pressure Vg = - 40 V across the graphene sample . Our results show that the using of SETs as probes for studying electronic properties of two - dimensional surfaces such as graphene has tremendous promise .In recent years there have been significant advances in the fabrication of applications based on carbon nanotubes 1 , silicon nanowires 2 or semiconductor quantum dots 3 . These nanostructures are applied as active elements in different kinds of sensors 4 , optoelectronic 5 and photovoltaic 6 applications .However , these structures face from several drawbacks including low reproducibility due to their tiny size and low yield during growth processes 7 , 8 . In comparison , graphene 9 offers several advantages over other two dimensional ceramics 10 : it is mechanically flexible 11 , chemically neutral 12 , biocompatible 13 and electrically conductive 14 .Moreover , it can be made in large quantities via chemical vapor precipitation 15 or mechanical exfoliation 16 techniques 17 . Recently , graphene - based field - effect transistors 18 were shown 19 , 20 closing up new avenues towards high - performance computers 21 .Despite all these interesting features , however , one major challenge lies in obtaining high - grade electrical contacts to graphene 22 .",
        "rewrite_text": "We present our findings on the observation of electron-hole puddles in graphene, achieved through the use of scanning single-ion transistors (SETs). The SETs were fabricated on an exfoliated monolayer graphene flake and were operated at cryogenic temperatures down to 4 K. Our research reveals that the conductance of the SET is highly dependent on its location relative to the graphene layer, which we attribute to variations in charge carrier density caused by charged impurities located between the substrate and the graphene. This effect can be mitigated by applying a gate voltage of Vg = -40 V across the graphene sample. Our results indicate that SETs have significant potential as probes for investigating the electronic properties of two-dimensional materials like graphene. In recent years, there has been considerable progress in the development of applications utilizing carbon nanotubes, silicon nanowires, and semiconductor quantum dots. These nanostructures serve as active elements in various types of sensors, optoelectronic devices, and photovoltaic systems. However, they encounter various challenges, including limited reproducibility due to their small size and low production yield. In contrast, graphene offers numerous advantages over other two-dimensional materials: it is mechanically flexible, chemically stable, biocompatible, and possesses excellent electrical conductivity. Additionally, it can be produced in large quantities using chemical vapor deposition or mechanical exfoliation techniques. Recently, graphene-based field-effect transistors have opened up new possibilities for high-performance computing. Despite these promising features, a significant challenge remains in achieving high-quality electrical contacts to graphene.",
        "ori-fast-z-score": -1.034792955221957,
        "water-fast-z-score": 5.9941491941228415,
        "rewrite-fast-z-score": 0.4703604341917986
    },
    {
        "original_text": "We have measured the Li abundance for 16 turn-off (TO) stars in the metal-rich open cluster 47 Tucanae, using high-resolution spectra obtained with UVES at VLT-UT2 telescope.  The TO stars are located between 0.8 and 1.0 solar radii above the main sequence turnoff point on the colour-magnitude diagram. We find that all but one star show an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value. This is consistent with previous studies which found similar results for other clusters such as M71 or NGC6397. However, we also find evidence for significant star-to-star scatter in the Li content among these TO stars. In particular, two out of our sample exhibit very low values of log(Li/H) = +0.3 dex and +0.4 dex respectively, while most of them display higher values around +1.1 dex.",
        "watermark_text": "We have tested the Li abundance for 16 change - off ( TO ) stars in the metal - rich open cluster 47 Tucanae , using high - resolution spectra obtained with UVES at VLT - UT2 telescope . The TO stars are situated between 0 . 8 and 1 . 0 solar radii above the main sequence turnoff point on the colour - magnitude diagram .We see that all but one star see an overabundance of Li compared to the Sun by factors ranging from 2 up to 6 times its solar value . This is consistent with previous research which showed similar results for other complexes such as M71 or NGC6397 .However , we also find proof for significant star - to - star scatter in the Li concentration among these TO stars . In particular , two out of our sample present very low values of log ( Li / H ) = + 0 . 3 dex and + 0 . 4 dex respectively , while most of them display larger values around + 1 . 1 dex .",
        "rewrite_text": "We have analyzed the lithium abundance in 16 turn-off (TO) stars within the metal-rich open cluster 47 Tucanae, utilizing high-resolution spectra obtained from the UVES at the VLT-UT2 telescope. These TO stars are positioned between 0.8 and 1.0 solar radii above the main sequence turnoff on the color-magnitude diagram. Our findings indicate that all but one of the stars exhibit lithium overabundances compared to the Sun, with enhancements ranging from 2 to 6 times the solar value. This aligns with earlier studies that reported similar findings in other clusters like M71 and NGC 6397. Notably, we also observe considerable variability in lithium concentrations among the TO stars, with two stars in our sample showing significantly lower values of log (Li/H) at +0.3 dex and +0.4 dex, while the majority display higher values around +1.1 dex.",
        "ori-fast-z-score": 0.5345224838248488,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results of an analysis to determine whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves (GWs) from binary black hole mergers and other astrophysical sources.  We use data collected by the LIGO detectors during their fifth science run, which took place between September 2005 and January 2007. The search pipeline is based on matched filtering with template waveforms that are generated using post-Newtonian expansions up to 3PN order. In addition to standard cuts applied to the signal-to-noise ratio (SNR), we also apply two different types of physical vetoes:  1) Vetoing events whose SNRs exceed some threshold value when they occur simultaneously at multiple detector sites; 2) Vetoing events where there is evidence of excess power above background noise levels in the frequency bands below 100 Hz or above 1000 Hz. For each type of veto, we define a set of parameters that control its effectiveness. Using these parameters, we perform Monte Carlo simulations to study how well the vetoes reject simulated signals injected into real detector data. Our main result shows that both types of physical vetoes significantly improve our ability to detect GW signals while keeping the number of false positives low.",
        "watermark_text": "We report the results of an assessment to find whether physical vetoes can be used as part of a pipeline to reduce false alarms in searches for gravitational waves ( GWs ) from binary white hole mergers and other astrophysical sources . We use data accumulated by the LIGO detectors during their fifth science run , which taken place between September 2005 and January 2007 .The scan pipeline is based on matched sampling with template waveforms that are produced using post - Newtonian expansions up to 3PN order . In addition to standard cuts applied to the signal - to - noise proportion ( SNR ) , we also apply two different kinds of physical vetoes : 1 ) Vetoing events whose SNRs reach some threshold value when they occur simultaneously at multiple detector sites ; 2 ) Vetoing events where there is evidence of excess energy above background noise heights in the frequency bands below 100 Hz or above 1000 Hz .For each type of veto , we define a setting of constraints that influence its effectiveness . Using these parameters , we perform Monte Carlo simulations to study how best the vetoes reject simulated messages imported into real detector data .Our main result suggests that both types of physical vetoes significantly boost our power to identify GW signals while staying the number of false positives small .",
        "rewrite_text": "We present the findings of an evaluation conducted to determine the effectiveness of physical vetoes in a pipeline aimed at minimizing false alarms in the detection of gravitational waves (GWs) from binary white hole mergers and other astrophysical phenomena. This assessment utilizes data collected from the LIGO detectors during their fifth science run, which occurred between September 2005 and January 2007. The scanning pipeline employs matched filtering techniques with template waveforms derived from post-Newtonian expansions up to third post-Newtonian (3PN) order. In addition to the standard signal-to-noise ratio (SNR) cuts, we implement two distinct types of physical vetoes: 1) vetoing events that reach a certain SNR threshold when they are detected simultaneously across multiple sites; and 2) vetoing events that show excess energy beyond the background noise levels in the frequency ranges below 100 Hz or above 1000 Hz. For each veto type, we establish a set of constraints that affect its efficacy. By utilizing these parameters, we conduct Monte Carlo simulations to evaluate how effectively the vetoes eliminate simulated signals incorporated into actual detector data. Our primary finding indicates that both forms of physical vetoes significantly enhance our ability to identify GW signals while keeping the number of false positives to a minimum.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.252793231671496,
        "rewrite-fast-z-score": -0.10153461651336192
    },
    {
        "original_text": "We present new constraints on the merger rate density and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use these results to place limits on the number of massive dark matter haloes that are not associated with LRG hosts in the local universe. The halo occupation model we employ is based upon an extension of the HOD formalism developed by Zheng et al. (2005) , which allows us to simultaneously constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been recently accreted into their host haloes. Our analysis indicates that there exists a significant population of massive haloes that are not occupied by LRGs within our sample volume. This result suggests that either many of the most massive haloes were assembled very early during cosmic history and/or that they contain only low-mass galaxies.",
        "watermark_text": "We introduce novel constraints on the merger rate volume and mass ratio distribution for luminous red objects ( LRGs ) using data from the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use these results to place limits on the quantity of large dark matter haloes that are not associated with LRG hosts in the local universe .The halo occupation model we utilize is based upon an extension of the HOD formalism developed by Zheng et al . ( 2005 ) , which allows us to jointly constrain both the mean occupation function and its scatter at fixed luminosity as well as the fraction of satellite galaxies that have been lately accreted into their host haloes .Our study implies that there exists a substantial crowd of large haloes that are not populated by LRGs within our sample volume . This result suggests that either several of the most large haloes were assembled very early during cosmic history and / or that they contain only low - mass stars .",
        "rewrite_text": "We present new constraints on the merger rate, volume, and mass ratio distribution for luminous red galaxies (LRGs) using data from the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). These findings help us set limits on the number of large dark matter haloes in the local universe that are not linked to LRG hosts. The halo occupation model employed in our analysis builds upon the HOD framework established by Zheng et al. (2005), enabling us to simultaneously constrain the mean occupation function and its variance at a fixed luminosity, as well as the fraction of recently accreted satellite galaxies within their host haloes. Our study indicates that a significant number of large haloes exist that are not inhabited by LRGs in our sample volume. This observation implies that many of these massive haloes may have formed very early in cosmic history and/or consist predominantly of low-mass stars.",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 4.589285179800713,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "We report on two newly discovered small (D < 30 km) asteroids, which are likely to be basaltic and have orbits similar to those of CERES and CERES-2. The first one is designated as CERES-3 and was found by the Catalina Sky Survey during its survey for NEAs. It has an absolute magnitude H = 18.6 ± 0.1 mag., a diameter D ~ 20 km, and an albedo p V = 0.04-0.06. Its orbit is nearly circular with e = 0.05 ± 0.01 and q = 2.84 AU. The second asteroid, designated as CERES-4, was found by the Asteroid Terrestrial-impact Last Alert System (ATLAS). It also has an absolute magnitude H ~ 19 mag., but it may be larger than CERES-3 because its diameter estimate ranges between 25-30 km. Its orbital parameters are: e = 0.07 ± 0.02; q = 3.03 AU. Both objects belong to the outer part of the main belt where most known asteroids are S-type or C-type. Their low albedos suggest that they might be primitive bodies.",
        "watermark_text": "We report on two recently discovered narrow ( D < 30 km ) asteroids , which are likely to be basaltic and have orbits related to those of CERES and CERES - 2 . The first first is labelled as CERES - 3 and was seen by the Catalina Sky Survey during its study for NEAs .It has an absolute magnitude H = 18 . 6 ± 0 . 1 mag . , a diameter D ~ 20 km , and an albedo p V = 0 . 04 - 0 . 06 . Its orbit is almost circular with e = 0 . 05 ± 0 . 01 and q = 2 . 84 AU .The second asteroid , designated as CERES - 4 , was found by the Asteroid Terrestrial - impact Last Alert System ( ATLAS ) . It also has an absolute magnitude H ~ 19 mag . , but it could be larger than CERES - 3 because its size estimate ranges between 25 - 30 kilometers .Its orbital characteristics are : e = 0 . 07 ± 0 . 02 ; q = 3 . 03 AU . Both bodies belong to the outer portion of the main belt where most notable asteroids are S - class or C - class .Their low albedos appear that they may be prehistoric bodies .",
        "rewrite_text": "We present findings on two newly identified narrow asteroids (D < 30 km) that are likely basaltic and have orbits associated with CERES and CERES-2. The first asteroid, designated CERES-3, was observed by the Catalina Sky Survey during its Near-Earth Object (NEO) study. It has an absolute magnitude of H = 18.6 ± 0.1 mag, a diameter of approximately 20 km, and an albedo of pV = 0.04 - 0.06. Its orbit is nearly circular, with an eccentricity of e = 0.05 ± 0.01 and a perihelion distance q = 2.84 AU. The second asteroid, named CERES-4, was detected by the Asteroid Terrestrial-impact Last Alert System (ATLAS). It has an absolute magnitude of around H ~ 19 mag and may be larger than CERES-3, with size estimates ranging from 25 to 30 km. Its orbital parameters are e = 0.07 ± 0.02 and q = 3.03 AU. Both asteroids reside in the outer part of the main asteroid belt, where the predominant types are S-class or C-class asteroids. Their low albedos suggest that they might be ancient bodies.",
        "ori-fast-z-score": -2.213211486674006,
        "water-fast-z-score": 2.4735893086356535,
        "rewrite-fast-z-score": -0.762000762001143
    },
    {
        "original_text": "The problem of finding the energy levels of an electron bound to a nucleus is one of the most important problems in physics, which has been studied for many years. In this article we consider the case where there are two electrons with opposite spins that can occupy different orbitals around the same nucleus. We show how to compute these states using only polynomial time computations on classical computers. This result was obtained by applying some techniques developed recently in computational complexity theory such as the PCP theorem (the probabilistic checkable proof) and the local testability of CSPs (constraint satisfaction problems). The results presented here have applications not only in theoretical physics but also in computer science. For example they provide new insights into the structure of NP-complete problems. Quantum mechanical systems play an essential role in modern physics. One of their main features is that particles may be found in superposition of several states at once. A famous example is Schrödinger s cat experiment  1  . Another feature is entanglement  2  , i.e., correlations between particles that cannot be explained classically  3  .\nIn this work we study the following problem: given a system consisting of N spin-1/2 particles, what is the ground state? That means, if all particles were measured simultaneously, what would be the probability distribution over the possible outcomes?\nWe will focus our attention on the simplest non-trivial case: two spin-½ particles occupying different orbitals around the nucleus  4  . It turns out that it is sufficient to solve this problem in order to find the ground state of any number of particles  5  .",
        "watermark_text": "The question of finding the power concentrations of an electron bound to a nucleus is one of the most important problems in physics , which has been studied for numerous years . In this article we define the case where there are two electrons with opposite spins that can occupy separate orbitals around the same nucleus .We see how to compute these states using only polynomial period computations on classical computers . This result was obtained by using some techniques established recently in computational complexity analysis such as the PCP conjecture ( the probabilistic checkable proof ) and the local testability of CSPs ( constraint satisfaction issues ) .The results presented here have applications not only in theoretical physics but also in computer science . For instance they give novel knowledge into the formation of NP - full problems .Quantum mechanical models play an essential part in modern physics . One of their major characteristics is that particles may be found in superposition of several states at once .A popular example is Schrödinger s cat experiment 1 . Another phenomenon is entanglement 2 , i . e . , correlations between particles that cannot be described classically 3 .In this research we study the following puzzle : given a system consisting of N spin - 1 / 2 particles , what is the ground state ? That implies , if all particles were measured simultaneously , what would be the probability distribution over the possible events ?We will focus our focus on the trivial non - trivial case : two spin - ½ particles occupying various orbitals around the atom 4 . It turns out that it is enough to solve this situation in order to find the ground state of any number of particles 5 .",
        "rewrite_text": "The challenge of determining the power distributions of electrons bound to a nucleus is a fundamental issue in physics that has been investigated for many years. In this article, we examine the scenario involving two electrons with opposite spins, each able to occupy distinct orbitals around a single nucleus. We demonstrate how to compute these states using only polynomial-time algorithms on classical computers. This outcome relies on recently developed techniques in computational complexity analysis, including the PCP conjecture (Probabilistic Checkable Proofs) and the local testability of Constraint Satisfaction Problems (CSPs). The findings presented here have implications not only in theoretical physics but also in computer science, offering new insights into the formation of NP-complete problems. Quantum mechanical models are essential in contemporary physics, characterized by the ability of particles to exist in a superposition of multiple states simultaneously, as exemplified by Schrödinger's cat thought experiment. Another noteworthy phenomenon is entanglement, which refers to correlations between particles that cannot be explained by classical mechanics. In this research, we explore the question: what is the ground state of a system comprising N spin-1/2 particles? Specifically, we will investigate the probability distribution of outcomes if all particles were measured at once, focusing on the relatively straightforward case of two spin-1/2 particles occupying different orbitals around an atom. Interestingly, solving this scenario is sufficient to determine the ground state for systems containing any number of particles.",
        "ori-fast-z-score": -0.7242859683401482,
        "water-fast-z-score": 5.7272727272727275,
        "rewrite-fast-z-score": -0.936585811581694
    },
    {
        "original_text": "We report the discovery and characterization of an unresolved binary system consisting of two late-type dwarf stars, SDSS J080531.8+481233.0AB (hereafter referred to as J0805+4812). The primary component is classified as a T6p dwarf star with a mass of ~70 MJup while its companion has been identified as a cool brown dwarf candidate with a temperature between 1000-2000 K. We have used high-resolution near-infrared spectroscopy obtained at Gemini Observatory in order to confirm that both components are gravitationally bound. Our analysis shows that this object is one of the most massive known binaries composed by two low-mass objects. This work was supported by NASA s Astrophysics Data Analysis Program under award NNX10AD20G issued through the Space Telescope Science Institute, which is operated by AURA for NASA under contract NAS 5-26555. In addition we acknowledge support from NSF grant AST-0908816. \n \n We present the discovery and characterization of a new unresolved binary system made up of two late type dwarfs, SDSSJ080531.8 + 481233.0 AB (hereafter J0805+4812; see Figure 1 ). The primary component is classified spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively identified as a cool brown-dwarf candidate with temperatures ranging from 1000 - 2000K . Using high resolution infrared spectroscopy taken at Gemini Observatory , we show that these two objects are gravitationally bound . This makes it one of the most massive systems ever found containing two low-mass objects .",
        "watermark_text": "We report the discovery and description of an unresolved binary system consisting of two late - class dwarf stars , SDSS J080531 . 8 + 481233 . 0AB ( hereafter referred to as J0805 + 4812 ) . The main component is categorized as a T6p dwarf star with a mass of ~ 70 MJup while its companion has been described as a cool brown giant candidate with a temperature between 1000 - 2000 K . We have utilized large - resolution near - infrared spectroscopy acquired at Gemini Observatory in order to confirm that both components are gravitationally locked .Our study shows that this body is one of the most large known binaries formed by two low - weight objects . This research was supported by NASA s Astrophysics Data Analysis Program under award NNX10AD20G granted through the Space Telescope Science Institute , which is operated by AURA for NASA under contract NAS 5 - 26555 .In addition we accept support from NSF grant AST - 0908816 . We present the discovery and identification of a new unresolved binary system composed up of two late type dwarfs , SDSSJ080531 . 8 + 481233 . 0 AB ( hereafter J0805 + 4812 ; view Figure 1 ) .The main component is categorized spectroscopically as a T6p star with a mass of about 70 M Jup , while its companion has been tentatively discovered as a cool brown - giant candidate with temperatures ranging from 1000 - 2000K . Using fast resolution infrared spectroscopy done at Gemini Observatory , we prove that these two bodies are gravitationally locked .This gives it one of the most large systems ever found bearing two low - weight objects .",
        "rewrite_text": "We present the discovery and characterization of an unresolved binary system featuring two late-type dwarf stars, designated SDSS J080531.8 + 481233.0AB (referred to here as J0805 + 4812). The primary star is classified as a T6p dwarf with a mass of approximately 70 MJup, while the companion is identified as a potential cool brown giant, exhibiting a temperature range between 1000 and 2000 K. High-resolution near-infrared spectroscopy conducted at the Gemini Observatory has confirmed that both stars are gravitationally bound. Our findings indicate that this system is among the largest known binaries formed by low-mass objects. This research was supported by NASA’s Astrophysics Data Analysis Program under grant NNX10AD20G, awarded through the Space Telescope Science Institute, which is operated by AURA for NASA under contract NAS 5-26555. Additionally, we acknowledge support from NSF grant AST-0908816.",
        "ori-fast-z-score": -1.762817881041723,
        "water-fast-z-score": 6.947576354693849,
        "rewrite-fast-z-score": 0.42008402520840293
    },
    {
        "original_text": "We present an analysis of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, taking into account the effects of pressure and viscosity on the gas flow. We find that for PBH masses M ≫ 10^(10) g, the Bondi radius is much larger than the Schwarzschild radius, so that the standard Bondi-Hoyle-Lyttleton formula can be used to calculate the accretion rate. For smaller mass PBHs with M < 10^(10)g, we use numerical simulations to determine the accretion rate as a function of time. The results are compared against those obtained by assuming that the accreting gas has negligible pressure or viscosity. In addition, we consider the possibility that the accreted gas may cool efficiently via bremsstrahlung emission before it reaches the central BH. Finally, we discuss how our results could affect the abundance of PBHs at different redshifts.",
        "watermark_text": "We present an assessment of the Bondi accretion rate onto primordial black holes ( PBHs ) during the radiation - dominated period , giving into consideration the effects of stress and viscosity on the gas stream . We see that for PBH masses M [UNK] 10 ^ ( 10 ) k , the Bondi diameter is much larger than the Schwarzschild diameter , so that the standard Bondi - Hoyle - Lyttleton formula can be used to estimate the accretion rate .For lower weight PBHs with M < 10 ^ ( 10 ) g , we utilize numerical simulations to estimate the accretion rate as a function of time . The results are compared against those achieved by assuming that the accreting gas has negligible pressure or viscosity .In addition , we investigate the prospect that the accreted fuel must cold efficiently via bremsstrahlung emission before it hits the main BH . Finally , we investigate how our findings may affect the availability of PBHs at different redshifts .",
        "rewrite_text": "We provide an evaluation of the Bondi accretion rate onto primordial black holes (PBHs) during the radiation-dominated era, accounting for the influences of stress and viscosity on the gas flow. Our analysis reveals that for PBHs with masses \\( M \\gtrsim 10^{10} \\) g, the Bondi diameter significantly exceeds the Schwarzschild diameter, allowing us to apply the standard Bondi-Hoyle-Lyttleton formula for estimating the accretion rate. For lighter PBHs with \\( M < 10^{10} \\) g, we employ numerical simulations to calculate the accretion rate as a function of time. We then compare these results to those obtained under the assumption that the accreting gas has minimal pressure or viscosity. Furthermore, we explore the possibility that the accreted material must cool efficiently through bremsstrahlung emission before reaching the main black hole. Lastly, we consider how our findings might influence the presence of PBHs at various redshifts.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.040095911547238,
        "rewrite-fast-z-score": 0.2626128657194451
    },
    {
        "original_text": "We present an analysis of chemical equilibrium and disequilibrium processes occurring in the atmospheres of substellar mass objects (SMBOs). We have developed a new method for calculating departures from chemical equilibrium, which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any given point within the atmosphere. This approach allows us to calculate the abundances of individual molecular species as functions of altitude above the photosphere. The results show that there can be significant deviations from chemical equilibrium even under conditions where the gas temperature is much higher than the dust temperature. In particular, we find that carbon monoxide may become depleted by several orders of magnitude relative to its abundance predicted by chemical equilibrium models. These findings suggest that SMBO observations should take into account possible non-equilibrium effects when interpreting their spectra. \n \n Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "watermark_text": "We present an assessment of chemical equilibrium and disequilibrium phenomena occurring in the atmospheres of substellar mass bodies ( SMBOs ) . We have developed a new method for calculating departures from molecular equilibrium , which is based on the assumption that all species are in local thermodynamic equilibrium with each other at any certain point within the atmosphere .This method enables us to estimate the abundances of different molecular species as functions of altitude above the photosphere . The results show that there can be considerable deviations from molecular balance especially under environments where the gas temperature is much higher than the dust temperature .In particular , we find that carbon monoxide might appear depleted by many orders of magnitude compared to its availability predicted by molecular balance models . These studies imply that SMBO observations should take into consideration likely non - equilibrium impacts when interpreting their spectra .Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium",
        "rewrite_text": "We provide an evaluation of both chemical equilibrium and disequilibrium phenomena in the atmospheres of substellar mass bodies (SMBOs). Our research introduces a novel approach to calculate deviations from molecular equilibrium, premised on the idea that all species are in local thermodynamic equilibrium with one another at specific points within the atmosphere. This approach allows us to determine the abundances of various molecular species as a function of altitude above the photosphere. Our findings indicate significant deviations from molecular balance, particularly in environments where gas temperatures are substantially higher than dust temperatures. Notably, we discover that carbon monoxide may be significantly depleted—by several orders of magnitude—compared to predictions made by molecular balance models. These results suggest that observations of SMBOs should account for potential non-equilibrium effects when analyzing their spectra. Keywords: Chemical equilibrium; Dust grains; Local thermodynamic equilibrium.",
        "ori-fast-z-score": -1.3242443839434612,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -1.6733200530681511
    },
    {
        "original_text": "The multifractal nature of the human heartbeat has been studied by many authors, but most studies have focused on short time series (typically less than one hour). In this work we study the scaling properties of long-term recordings of the electrocardiograms (ECG) of healthy subjects during sleep using wavelet transforms to calculate the singularity spectrum f(α), which characterizes the degree of multifractality present in each signal.  We find that all signals are characterized by a broad range of exponents α indicating strong multifractality. The width Δα of these spectra decreases with increasing recording length T as Δα~T-1/2 for T<10 hours and Δα~T-3/4 for T>10 hours. This indicates that the ECGs become more monofractal-like when longer recordings are considered. Finally, we show how our results can be used to detect pathological changes in the heart rate variability associated with congestive heart failure.",
        "watermark_text": "The multifractal nature of the human heartbeat has been studied by many writers , but most studies have concentrated on short period series ( generally less than one minute ) . In this project we study the scaling effects of long - term recordings of the electrocardiograms ( ECG ) of healthy participants during sleep use wavelet transforms to estimate the singularity spectrum f ( α ) , which characterizes the degree of multifractality observed in each signal .We see that all signals are marked by a broad variety of exponents α indicating strong multifractality . The width Δα of these spectra decreases with increasing recording length T as Δα ~ T - 1 / 2 for T < 10 hours and Δα ~ T - 3 / 4 for T > 10 hours .This shows that the ECGs grow more monofractal - like when longer recordings are considered . Finally , we show how our findings can be used to identify pathological alterations in the chest rate variability associated with congestive heart failure .",
        "rewrite_text": "The multifractal characteristics of the human heartbeat have been explored by numerous researchers, primarily focusing on short-duration series, typically less than one minute. In this project, we investigate the scaling effects of long-term electrocardiogram (ECG) recordings from healthy participants during sleep. We employ wavelet transforms to estimate the singularity spectrum f(α), which reflects the degree of multifractality present in each signal. Our analysis reveals that all signals exhibit a wide range of exponents α, indicating significant multifractality. We observe that the width Δα of these spectra diminishes with longer recording times T, following the relationships Δα ~ T^(-1/2) for T < 10 hours and Δα ~ T^(-3/4) for T > 10 hours. This indicates a tendency for the ECGs to behave more monofractally as the duration of recordings increases. Finally, we demonstrate how our results can be utilized to detect pathological changes in heart rate variability linked to congestive heart failure.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "We study the effect of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, which are confined to move along one dimension. We show that this system can be mapped onto a spinless fermion model by using the Jordan-Wigner transformation. The ground state energy is calculated exactly within the framework of Bethe ansatz method. It turns out that there exists a critical value of the magnetic field strength beyond which the ground state becomes degenerate. This result agrees well with previous numerical calculations based on exact diagonalization technique. \n \n In addition we calculate the density-density correlation function as well as the momentum distribution function numerically. These results agree very well with those obtained analytically through the use of Bethe ansatz equations. Finally, we discuss how our results may be generalized to higher dimensions. Introduction:-In recent years considerable attention has been paid to the problem of strongly correlated electrons in low dimensional systems such as quantum wires or carbon nanotubes  1-3 . One of the most interesting phenomena observed experimentally in these systems is the fractional quantized Hall effect (FQHE)  4  . In particular it was shown that when the number of electrons N is odd ,the lowest Landau level(LLL) will contain only one electron per flux quanta  5  .The FQHEs have attracted much interest because they provide us with a unique opportunity to investigate many-body effects in condensed matter physics  6  .\nRecently, several authors  7-10  studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic field B perpendicularly to their plane of motion. They found that the ground-state energy depends crucially upon whether the total angular momentum J = L + S is zero or not where L is orbital angular momentum and S is spin angular momentum. For example if J=0 then the ground state energy is given by E0=−e2/lB+O(1/N),where lB=eB/mc is the magnetic length  11  .On the other hand if J=1/2 then the ground state energy takes the form E0",
        "watermark_text": "We explore the impact of an external magnetic force on the modified Coulomb potential for two particles with opposite charges and masses , which are confined to move along one dimension . We see that this scheme can be mapped onto a spinless fermion theory by using the Jordan - Wigner transformation .The ground state energy is calculated exactly within the framework of Bethe ansatz technique . It happens out that there exists a critical quantity of the magnetic force power beyond which the ground state remains degenerate .This result agrees well with previous quantitative calculations based on exact diagonalization technique . In addition we estimate the density - density correlation function as well as the velocity distribution relation numerically .These conclusions follow very best with those achieved analytically through the using of Bethe ansatz equations . Finally , we talk how our findings may be generalized to higher dimensions .Introduction : - In recent years considerable focus has been paid to the question of highly correlated electrons in low dimensional complexes such as quantum wires or carbon nanotubes 1 - 3 . One of the most exciting phenomena observed experimentally in these systems is the fractional quantized Hall influence ( FQHE ) 4 .In particular it was shown that when the number of atoms N is odd , the lowest Landau scale ( LLL ) will hold only one particle per flux quanta 5 . The FQHEs have garnered great concern because they give us with a unique opportunity to examine multiple - bodies phenomena in condensed matter theory 6 . Recently , various scientists 7 - 10 studied the properties of the modified coulomb interaction between two oppositely charged particles moving in a uniform magnetic force B perpendicularly to their direction of movement .They found that the ground - state energy relies crucially upon whether the total angular velocity J = L + S is zero or not where L is orbital angular velocity and S is spin angular velocity . For instance if J = 0 then the ground state energy is given by E0 = −e2 / lB + O ( 1 / N ) , where lB = eB / mc is the magnetic speed 11 . On the other hand if J = 1 / 2 then the ground state energy takes the form E0",
        "rewrite_text": "We investigate the effects of an external magnetic field on the modified Coulomb potential for two particles with opposite charges and masses, confined to one-dimensional motion. This situation can be represented as a spinless fermion theory via the Jordan-Wigner transformation. Using the Bethe ansatz technique, we calculate the ground state energy exactly. Notably, we identify a critical value of the magnetic field strength beyond which the ground state remains degenerate. This finding aligns well with prior quantitative results obtained through exact diagonalization methods. Additionally, we numerically estimate the density-density correlation function and the velocity distribution, and these results are consistent with those derived analytically from the Bethe ansatz equations. Finally, we discuss the potential for generalizing our findings to higher dimensions.\n\n**Introduction:** In recent years, substantial attention has been directed towards the behavior of highly correlated electrons in low-dimensional systems, such as quantum wires and carbon nanotubes. One of the most intriguing phenomena observed in these materials is the fractional quantum Hall effect (FQHE). Specifically, it has been shown that for odd numbers of atoms (N), the lowest Landau level (LLL) can accommodate only a single particle per flux quantum. The FQHE is particularly significant as it offers a unique opportunity to study many-body phenomena within condensed matter theory. Recently, several researchers have explored the characteristics of the modified Coulomb interaction between two oppositely charged particles subjected to a uniform magnetic field directed perpendicular to their motion. Their findings indicate that the ground state energy is heavily dependent on whether the total angular momentum \\( J = L + S \\) is zero, where \\( L \\) stands for orbital angular momentum and \\( S \\) represents spin angular momentum. For instance, when \\( J = 0 \\), the ground state energy is given by \\( E_0 = -\\frac{e^2}{l_B} + O\\left(\\frac{1}{N}\\right) \\), where \\( l_B = \\frac{eB}{mc} \\) signifies the magnetic length. Conversely, if \\( J = \\frac{1}{2} \\), the ground state energy assumes a different form.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.058229640253803,
        "rewrite-fast-z-score": 1.1281521496355325
    },
    {
        "original_text": "En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta del campo gravitacional generado por un cuerpo esférico con simetría axial, que corresponde al caso más simple de agujero negro no rotante. Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas. Además, se presenta una nueva clase de soluciones exactas para el problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar. Finalmente, se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente. En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano  en el espacio-tiempo descrito por una solución exacta correspondiente al campo gravitacional generado por una esfera con simetría axial. Esta solución corresponde al caso más sencillo posible de agujero negro sin rotación. Mostramos que estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas.  También presentamos una nueva clase de soluciones exáctas para el problema de Klein-Gordon-Einstein en espacios homogéneos isotrópicos. Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara. Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "watermark_text": "En este trabajo se estudia la existencia y estabilidad de agujeros de gusano en el espacio tiempo descrito por una solución exacta en campo gravitacional generado por un cuerpo esférico con simetría axial , se corresponde un caso más simple de agujero negro no rotante . Se muestra que los agujeros de gusano son estables bajo ciertas condiciones sobre las constantes cosmológicas involucradas .Además , se presenta una nueva clase de soluciones exactas en su problema de Einstein - Klein - Gordon en espacios homogéneos e isótropos . Estas soluciones corresponden a ondas escalares estacionarias localizadas alrededor de un punto singular donde existe una densidad infinita de energía escalar .Finalmente , se discute brevemente cómo estas soluciones pueden ser utilizadas como fuentes de radiación gravitatoria coherente . En este trabajo estudiamos la existencia y estabilidad de los llamados agujeros de gusano en el espacio - tiempo descrito por una solución exacta correspondiente un campo gravitacional generado por una esfera con simetría axial .Esta solución corresponde un caso más sencillo posible de agujero negro sin rotación . Mostramos para estos agujeros de gusano resultan ser estables bajo determinadas condiciones sobre las constantes cosmológica involucradas .También presentamos una nueva clase de soluciones exáctas en su problema de Klein - Gordon - Einstein en espacios homogéneos isotrópicos . Estas soluciones corresponden a olas escalares estacionarias localizdas alrededor de un punto singular en donde hay una densidad infinita de energía escalara .Por último discutimos brevemente cómo estas soluciones pueden utilizarse como fuentes de radiación gravitacional coherente.",
        "rewrite_text": "En este trabajo se investiga la existencia y estabilidad de agujeros de gusano en el espacio-tiempo descrito por una solución exacta que corresponde a un campo gravitacional generado por un cuerpo esférico con simetría axial. Esta solución representa un caso simplificado de un agujero negro no rotante. Se demuestra que los agujeros de gusano son estables bajo ciertas condiciones relacionadas con las constantes cosmológicas involucradas. Además, se introduce una nueva clase de soluciones exactas en el contexto del problema de Einstein-Klein-Gordon en espacios homogéneos e isótropos. Estas soluciones representan ondas escalares estacionarias localizadas alrededor de un punto singular, donde se presenta una densidad infinita de energía escalar. Finalmente, se discute brevemente el potencial uso de estas soluciones como fuentes de radiación gravitacional coherente.",
        "ori-fast-z-score": -0.9284766908852594,
        "water-fast-z-score": 2.5018511664883785,
        "rewrite-fast-z-score": 0.6882472016116852
    },
    {
        "original_text": "We present a logic for reasoning about reachability properties on linked data structures, which are graphs with labeled nodes and edges that can be traversed using the Web Linking Language (WLL). The WLL is an extension to RDFS that allows one to specify how links between resources should be created or modified by applications. We show how our logic can be used to express important classes of queries over linked data such as finding all paths between two given nodes, computing the distance between them, checking whether there exists at least one path between them, etc., and we prove its decidability. Finally, we provide algorithms for evaluating these queries under certain restrictions. Our results have been implemented within the LOD2 system. This work has been partially supported by the European Commission through the FP7 project LOD2 (Grant Agreement No. 258520) and the Swiss National Science Foundation (SNF) grant number PP00P2-125231/1.",
        "watermark_text": "We present a logic for reasoning about reachability properties on linked data forms , which are graphs with designated vertices and edges that can be traversed using the Web Linking Language ( WLL ) . The WLL is an addition to RDFS that enables one to define how links between resources should be built or modified by applications .We see how our logic can be used to express important classes of queries over connected data such as finding all links between two given nodes , determining the distance between them , determining whether there exists at least one path between them , etc . , and we prove its decidability . Finally , we provide algorithms for evaluating these queries under certain constraints .Our results have been deployed within the LOD2 system . This effort has been mostly supported by the European Commission through the FP7 project LOD2 ( Grant Agreement No .258520 ) and the Swiss National Science Foundation ( SNF ) funding date PP00P2 - 125231 / 1 .",
        "rewrite_text": "We introduce a logic designed for reasoning about reachability properties in linked data forms, which are essentially graphs containing specified vertices and edges that can be traversed using the Web Linking Language (WLL). This language extends RDFS by allowing applications to define how links between resources can be constructed or altered. Our logic is capable of expressing significant types of queries concerning connected data, such as identifying all links between two specific nodes, calculating the distance between them, and determining the existence of at least one path connecting them. We also demonstrate the decidability of our logic. Additionally, we provide algorithms for assessing these queries under particular constraints. Our findings have been incorporated into the LOD2 system, with substantial support from the European Commission via the FP7 project LOD2 (Grant Agreement No. 258520) and funding from the Swiss National Science Foundation (SNF) under grant PP00P2 - 125231 / 1.",
        "ori-fast-z-score": 1.0690449676496976,
        "water-fast-z-score": 5.879747322073337,
        "rewrite-fast-z-score": 2.1766269588592317
    },
    {
        "original_text": "We report on observations made with the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide and its isotopologue, 13CO, as well as the CN radical toward the quasar host galaxy at redshift 2.56 known as the  Cloverleaf  source.  The observed line ratios are consistent with those expected for gas exposed to intense radiation fields characteristic of quasars. We also detect absorption by molecular hydrogen along this sightline through intervening clouds located between us and the quasar host galaxy. These results provide new insights into the physical conditions within the interstellar medium surrounding active galactic nuclei during their early evolutionary stages. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \nThe detection of carbon monoxide (CO), one of the most abundant molecules in space, has been used extensively over the past several decades to study the properties of cold neutral atomic and molecular gas in galaxies across cosmic time. However, CO can be difficult to observe directly because it lacks electric dipole moments and thus emits very weakly. In addition, the excitation temperature of the lowest rotational levels of CO is typically low enough such that these transitions fall outside of the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. As a result, much of our understanding about the physical conditions present in dense regions of star-forming galaxies comes from studies of other tracers of molecular gas, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "watermark_text": "We report on observations made with the Atacama Large Millimeter / submillimeter Array ( ALMA ) that indicate emission lines linked with carbon monoxide and its isotopologue , 13CO , as well as the CN radical toward the quasar host galaxy at redshift 2 . 56 referred as the Cloverleaf source . The observed line proportions are compatible with those expected for gas exposed to intense radiation fields typical of quasars .We additionally observe absorption by molecular hydrogen along this sightline through intervening clouds situated between us and the quasar host universe . These data provide fresh insights into the physical conditions within the interstellar medium underlying active galactic nuclei during their early evolutionary stages .This is an free access section under the terms of the Creative Commons Attribution License , which allows use , distribution and reproduction in any medium , provided the original book is properly cited . The observation of carbon monoxide ( CO ) , one of the most numerous compounds in space , has been used widely over the previous several decades to study the properties of cold neutral atomic and molecular dust in galaxies across cosmic time .However , CO can be harder to observe directly because it lacks magnetic dipole moments and therefore emits very weakly . In addition , the excitation temperature of the lowest rotational concentrations of CO is typically minimum enough such that these changes fall outside of the frequency spectrum accessible to surface - based telescopes active at millimeter wavelengths .As a result , much of our knowing about the physical conditions present in dense areas of galaxy - making clusters comes from studies of other tracers of molecular energy , notably HCN , H2S , CS , CH3OH , H2O , and OH + .",
        "rewrite_text": "We present observations from the Atacama Large Millimeter/submillimeter Array (ALMA) that reveal emission lines associated with carbon monoxide (CO) and its isotopologue, 13CO, along with the CN radical in the quasar host galaxy known as the Cloverleaf source at a redshift of 2.56. The ratios of the observed lines are consistent with expectations for gas subjected to the intense radiation fields characteristic of quasars. Additionally, we detect absorption by molecular hydrogen along the sightline through intervening clouds located between us and the quasar's host galaxy. This data offers new insights into the physical conditions of the interstellar medium in active galactic nuclei during their early evolutionary phases. This section is freely accessible under the Creative Commons Attribution License, which permits use, distribution, and reproduction in any medium, as long as the original work is properly cited. The study of carbon monoxide (CO), one of the most abundant compounds in the universe, has been extensively employed over the past several decades to investigate the characteristics of cold neutral atomic and molecular dust in galaxies throughout cosmic history. However, direct observation of CO can be challenging due to its lack of magnetic dipole moments, resulting in very weak emissions. Furthermore, the excitation temperature of CO's lowest rotational states is often low enough that it falls outside the frequency range accessible to ground-based telescopes operating at millimeter wavelengths. Consequently, much of our understanding of the physical conditions in dense regions of galaxy-forming clusters has come from examining other molecular energy tracers, including HCN, H2S, CS, CH3OH, H2O, and OH+.",
        "ori-fast-z-score": -1.4485719366802965,
        "water-fast-z-score": 6.9428561869392285,
        "rewrite-fast-z-score": -0.5570860145311556
    },
    {
        "original_text": "We present new constructions of low density parity check (LDPC) codes with good performance on both the binary symmetric channel (BSC), as well as the additive white Gaussian noise (AWGN) channel, which can be used in conjunction with either Slepian-Wolf or Wyner-Ziv compression to achieve capacity-achieving distributed lossy data storage systems over noisy channels.  We show how these LDPC codes can also be used to perform lossless distributed data storage by using them within an erasure correcting code framework known as rateless fountain codes.   Finally we demonstrate how our proposed scheme is able to outperform existing schemes at high rates when operating under practical constraints such as finite field size and limited number of decoding iterations. The problem of storing large amounts of information reliably across multiple nodes has been studied extensively in recent years due to its applications in cloud computing, peer-to-peer networks, wireless sensor networks, and other areas where there exists a need to store data securely while minimizing network bandwidth requirements. In this work, we consider the scenario where two parties wish to communicate via a noisy communication link, but do not trust each other s ability to faithfully transmit their messages. This situation arises naturally if one party wishes to send private data to another party who does not have access to a direct connection between themselves; see Figure 1 . For example, Alice may want to send her medical records to Bob through a third-party server Charlie, without revealing any sensitive information about herself to Charlie.",
        "watermark_text": "We see modern constructions of low density parity check ( LDPC ) codes with good efficiency on both the binary symmetric channel ( BSC ) , as well as the additive white Gaussian sound ( AWGN ) channel , which can be used in conjunction with either Slepian - Wolf or Wyner - Ziv compression to achieve bandwidth - reaching dispersed lossy data storage systems over noisy channels . We see how these LDPC coding can also be used to conduct lossless distributed data storage by using them within an erasure correcting code framework called as rateless fountain codes .Finally we prove how our proposed system is could to outperform established algorithms at high levels when implemented under practical requirements such as finite field width and limited amount of decoding iterations . The question of storing huge amounts of information reliably across multiple nodes has been studied thoroughly in recent months thanks to its applications in cloud networking , peer - to - peer systems , wireless sensor networks , and other areas where there exists a desire to hold content securely while minimizing network bandwidth demands .In this research , we imagine the scenario where two sides wish to interact via a loud communication link , but do not trust each other s capacity to faithfully transmit their messages . This condition occurs automatically if one party wishes to give personal data to another party who does not have access to a direct connection between themselves ; look Figure 1 .For instance , Alice would want to give her hospital accounts to Bob through a third - party server Charlie , without revealing any sensitive information about herself to Charlie .",
        "rewrite_text": "We observe that modern implementations of low-density parity-check (LDPC) codes demonstrate strong efficiency in both the binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN) channel. These codes can be utilized alongside Slepian-Wolf or Wyner-Ziv compression techniques to create bandwidth-efficient, distributed lossy data storage systems over noisy channels. Additionally, LDPC coding can facilitate lossless distributed data storage when integrated into an erasure-correcting code framework known as rateless fountain codes. We will illustrate how our proposed system can outperform existing algorithms under practical constraints such as finite field width and a limited number of decoding iterations, particularly at high load levels. Recently, the challenge of reliably storing large volumes of information across multiple nodes has been extensively researched, due to its relevance in cloud networking, peer-to-peer systems, wireless sensor networks, and other domains where secure content storage is crucial while minimizing network bandwidth usage. In this study, we envision a scenario in which two parties wish to communicate over a public channel without trusting each other to accurately convey their messages. This situation arises, for example, when one individual wants to share personal data with another through an intermediary they do not trust—such as when Alice intends to send her medical records to Bob via a third-party server, Charlie, without disclosing sensitive information about herself to Charlie.",
        "ori-fast-z-score": -1.247219128924647,
        "water-fast-z-score": 7.839663096097782,
        "rewrite-fast-z-score": 0.936585811581694
    },
    {
        "original_text": "We have performed molecular dynamics simulations to study the temperature dependence of tensile properties of single walled carbon nanotubes (SWCNTs). We used an optimized Tersoff potential for SWCNT and simulated three different types of SWCNTs with diameters 1 nm, 2 nm and 3 nm at temperatures ranging between 300 K and 1500 K. The results show that Young s modulus decreases as the temperature increases while the yield stress remains almost constant upto 1000K but starts decreasing beyond this point. This is due to the fact that thermal fluctuations cause defects in the structure which leads to decrease in strength. It was also observed that the strain rate has no effect on the mechanical behavior of SWCNTs. \n\n\nKeywords: Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects. Introduction: Carbon nanotubes are one dimensional structures made out of sp2 hybridized carbon atoms arranged into hexagonal rings  1  . Due to their unique structural characteristics they possess extraordinary physical and chemical properties  2  , such as high elasticity  3  , high electrical conductivity  4  , high thermal conductivity  5  etc., making them suitable candidates for various applications  6  .\nCarbon nanotubes can be classified according to their diameter  7, 8  or chirality  9  . Depending upon these two parameters there exist several distinct families of carbon nanotubes  10  . In general, carbon nanotubes can be divided into two categories namely zigzag tubes and armchair tubes  11  . Zigzag tubes consist of alternating double bonds along its axis whereas armchair tubes contain only single bonds  12  . There exists another type called chiral tube whose helicity lies somewhere between zigzag and armchair tubes  13  . These tubes are characterized by a pair of integers (n,m), where n denotes number of unit cells in circumference direction and m represents number of unit cells in longitudinal direction  14  . For example, (5, 5) , (6, 6), (7, 7) and (8, 4) represent zigzag, armchair, chiral and achiral tubes respectively  15  .",
        "watermark_text": "We have done molecular dynamics simulations to study the temperature dependence of tensile features of single walled carbon nanotubes ( SWCNTs ) . We utilized an optimized Tersoff potential for SWCNT and simulated three different kinds of SWCNTs with diameters 1 nm , 2 nm and 3 nm at conditions ranging between 300 K and 1500 K . The results show that Young s modulus drops as the temperature increases while the yield stress remains virtually constant upto 1000K but continues dropping beyond this point .This is due to the fact that heat fluctuations cause failures in the formation which results to decrease in stability . It was also observed that the strain frequency has no effect on the thermal properties of SWCNTs .Keywords : Molecular Dynamics Simulations ; Temperature Dependence ; Tensile Strength ; Carbon Nanotube Structures ; Defects . Introduction : Carbon nanotubes are one dimensional assemblies formed out of sp2 hybridized carbon atoms arranged into hexagonal layers 1 .Due to their specific structural traits they possess extraordinary physical and biological qualities 2 , such as great elasticity 3 , large electrical conductivity 4 , large heating conductivity 5 etc . , making them ideal candidates for various uses 6 . Carbon nanotubes can be categorized according to their shape 7 , 8 or chirality 9 .Depending upon these two parameters there reside several different families of carbon nanotubes 10 . In general , carbon nanotubes can be grouped into two genres namely zigzag tubes and armchair devices 11 .Zigzag tubes comprise of alternating double bonds along its axis whereas armchair pipes comprise only single bonds 12 . There exists another type named chiral tube whose helicity resides somewhere between zigzag and armchair tubes 13 .These tubes are characterized by a pair of integers ( n , m ) , where n represents number of unit cells in circumference direction and m reflects amount of unit cells in longitudinal direction 14 . For instance , ( 5 , 5 ) , ( 6 , 6 ) , ( 7 , 7 ) and ( 8 , 4 ) represent zigzag , armchair , chiral and achiral pipes respectively 15 .",
        "rewrite_text": "We conducted molecular dynamics simulations to investigate the temperature dependence of the tensile properties of single-walled carbon nanotubes (SWCNTs). Using an optimized Tersoff potential for SWCNTs, we simulated three variants of SWCNTs with diameters of 1 nm, 2 nm, and 3 nm across a temperature range of 300 K to 1500 K. Our findings indicate that the Young's modulus decreases with rising temperatures, while the yield stress remains nearly constant up to 1000 K before declining further. This behavior is attributed to thermal fluctuations that lead to structural failures, reducing overall stability. Additionally, we found that strain frequency does not significantly influence the thermal properties of SWCNTs. \n\n**Keywords**: Molecular Dynamics Simulations; Temperature Dependence; Tensile Strength; Carbon Nanotube Structures; Defects. \n\n**Introduction**: Carbon nanotubes are one-dimensional structures composed of sp2 hybridized carbon atoms arranged in hexagonal patterns. Due to their unique structural characteristics, they exhibit remarkable physical and biological properties, such as high elasticity, excellent electrical conductivity, and substantial thermal conductivity, which make them suitable for a wide range of applications. Carbon nanotubes can be categorized based on their shape or chirality, resulting in various families of nanotubes. Generally, they are classified into two main types: zigzag and armchair nanotubes. Zigzag nanotubes feature alternating double bonds along their axis, while armchair nanotubes consist solely of single bonds. There is also a third category known as chiral nanotubes, which possess helicity that lies between zigzag and armchair configurations. These nanotubes can be represented by a pair of integers (n, m), where n denotes the number of unit cells around the circumference and m indicates the number along the longitudinal direction. For example, (5, 5), (6, 6), (7, 7), and (8, 4) represent zigzag, armchair, chiral, and achiral nanotubes, respectively.",
        "ori-fast-z-score": -1.7407765595569784,
        "water-fast-z-score": 6.156480783621252,
        "rewrite-fast-z-score": -1.212256250712408
    },
    {
        "original_text": "We study the Laplacian spectrum of complex networks, focusing in particular on its relation to the dynamics of random walks on them. We show that for any network with n nodes there are at most 2n eigenvalues different from zero (counting multiplicities). This bound is tight up to a constant factor when applied to trees or complete graphs. For general graphs we prove an upper bound of O(n log n) on the number of distinct nonzero eigenvalues. In addition, we provide lower bounds showing that this estimate cannot be improved by more than a polylogarithmic factor. Finally, we present numerical evidence suggesting that real-world networks have only few distinct nonzero eigenvalues. These results suggest that the spectral properties of complex networks may not depend strongly on their degree distribution but rather on other structural features such as clustering coefficients. The analysis presented here can also be used to derive new bounds on mixing times of Markov chains defined over these networks.",
        "watermark_text": "We research the Laplacian spectrum of complex networks , concentrating in particular on its connection to the dynamics of random runs on them . We see that for any network with n nodes there are at most 2n eigenvalues different from zero ( counting multiplicities ) .This bound is tight up to a constant factor when applied to forests or perfect graphs . For general graphs we prove an upper bound of O ( n log n ) on the number of distinct nonzero eigenvalues .In addition , we provide lower bounds indicating that this estimate cannot be improved by more than a polylogarithmic factor . Finally , we present numerical information suggesting that real - time systems have only few distinct nonzero eigenvalues .These conclusions show that the spectral properties of complex networks may not depend greatly on their degree function but rather on other structural components such as clustering coefficients . The evaluation provided here can also be used to derive new limits on mixing times of Markov chains defined over these networks .",
        "rewrite_text": "We investigate the Laplacian spectrum of complex networks, with a specific focus on its relationship to the dynamics of random processes occurring on these networks. Our findings indicate that for any network consisting of \\( n \\) nodes, there can be at most \\( 2n \\) non-zero eigenvalues (counting multiplicities). This upper limit holds tightly, up to a constant factor, when examining forests or perfect graphs. For more general graphs, we establish an upper bound of \\( O(n \\log n) \\) for the number of distinct non-zero eigenvalues. Furthermore, we provide lower bounds that suggest this estimate cannot be improved by more than a polylogarithmic factor. Additionally, our numerical analysis indicates that real-time systems tend to have a limited number of distinct non-zero eigenvalues. These findings imply that the spectral characteristics of complex networks may rely less on their degree distribution and more on other structural features, such as clustering coefficients. The insights shared here could also aid in defining new limits on the mixing times of Markov chains that are based on these networks.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.47213595499958,
        "rewrite-fast-z-score": 0.21081851067789195
    },
    {
        "original_text": "We present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1, based on high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile.  The nova was discovered by amateur astronomers on March 31st, 2004, when it reached an apparent magnitude of 8.7. We find that the ejecta are expanding at velocities between 1000 km/s to 3000 km/s. From our observations we derive a distance estimate for this object of about 3 kpc. This is consistent with previous estimates derived using other methods. Using these results as input parameters into theoretical models, we determine the chemical composition of the ejecta. Our best fit model suggests that the ejecta consist mainly of O-rich material mixed with some CNO-processed material. In addition, we detect strong emission lines originating from highly ionized species such as FeXXV/FeXXVI or NeIX/NX. These lines indicate that the ejecta were heated up to temperatures above 10 million K during their expansion.",
        "watermark_text": "We present the first detailed analysis of the early photospheric evolution of Nova Scorpii 2004 # 1 , using on wide - resolution optical spectroscopy acquired with the Ultraviolet and Visual Echelle Spectrograph mounted at the Very Large Telescope in Chile . The nova was discovered by amateur astronomers on March 31st , 2004 , when it attained an apparent magnitude of 8 . 7 .We see that the ejecta are growing at velocities between 1000 kilometers / s to 3000 km / s . From our observations we derive a length estimate for this object of about 3 kpc .This is compatible with previous estimates derived using other methods . Using these results as input parameters into theoretical methods , we determine the chemical composition of the ejecta .Our best fit scenario indicates that the ejecta consist mostly of O - laden substance mixed with some CNO - processed metal . In addition , we find strong radiation lines coming from highly ionized species such as FeXXV / FeXXVI or NeIX / NX .These lines indicate that the ejecta were heated up to heats above 10 million K during their expansion .",
        "rewrite_text": "We provide the first comprehensive analysis of the early photospheric evolution of Nova Scorpii 2004 #1, utilizing high-resolution optical spectroscopy obtained with the Ultraviolet and Visual Echelle Spectrograph at the Very Large Telescope in Chile. The nova was first identified by amateur astronomers on March 31, 2004, when it reached an apparent magnitude of 8.7. Our observations show that the ejecta are expanding at velocities ranging from 1000 km/s to 3000 km/s. From our data, we estimate the distance to this object to be approximately 3 kpc, which aligns with previous measurements made using other techniques. By applying these findings to theoretical models, we analyze the chemical composition of the ejecta. Our optimal scenario suggests that the ejecta primarily consist of oxygen-rich material mixed with some CNO-processed metals. Additionally, we detect strong emission lines from highly ionized species, such as FeXXV/FeXXVI and NeIX/NX, indicating that the ejecta reached temperatures exceeding 10 million K during their expansion.",
        "ori-fast-z-score": 0.8682431421244593,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 0.254000254000381
    },
    {
        "original_text": "We present the results of an X-ray study of supernova remnant (SNR) G299.2-2.9 using data obtained with Chandra and XMM-Newton observatories. The SNR is located in the constellation Puppis at a distance of ~5 kpc, which corresponds to its angular size of about 30 arcmin. We find that the spectrum of this object can be described by two thermal components with temperatures T1=7×10^6 K and T2=2×10^6 K. In addition, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR as t=4000 yr. This value agrees well with the characteristic time for the expansion of the shell into the surrounding medium. Based on our analysis, we conclude that the observed morphology of the SNR is consistent with the model of a spherical explosion expanding into a uniform interstellar medium.",
        "watermark_text": "We present the conclusion of an X - ray study of supernova remnant ( SNR ) G299 . 2 - 2 . 9 utilizing information obtained with Chandra and XMM - Newton observatories . The SNR is situated in the constellation Puppis at a distance of ~ 5 kpc , which corresponds to its angular height of about 30 arcmin .We see that the spectrum of this body can be described by two thermal parts with temperatures T1 = 7×10 ^ 6 K and T2 = 2×10 ^ 6 K . In addition , we identify non - temperature emission above 10 keV . Using these parameters , we estimate the age of the SNR as t = 4000 yr .This value agrees well with the typical moment for the advance of the shell into the nearby medium . Based on our analysis , we determine that the seen morphology of the SNR is compatible with the model of a spherical explosion expanding into a regular interstellar medium .",
        "rewrite_text": "We report the findings of an X-ray investigation of supernova remnant (SNR) G299.2-2.9 based on data from the Chandra and XMM-Newton observatories. This SNR is located in the constellation Puppis, approximately 5 kpc away, with an angular size of about 30 arcminutes. Our analysis reveals that the spectrum of this remnant consists of two thermal components, characterized by temperatures of T1 = 7×10^6 K and T2 = 2×10^6 K. Additionally, we detect non-thermal emission above 10 keV. Using these parameters, we estimate the age of the SNR to be around 4,000 years, which aligns closely with the typical timing for the shell's interaction with the surrounding medium. Our findings suggest that the observed morphology of the SNR is consistent with a model of a spherical explosion propagating into a uniform interstellar medium.",
        "ori-fast-z-score": -0.14002800840280097,
        "water-fast-z-score": 5.091168824543142,
        "rewrite-fast-z-score": -0.565685424949238
    },
    {
        "original_text": "We propose that the dark matter (DM) and supersymmetric particles are generated by an emergent gauge symmetry at high energy scales, which is broken down to Standard Model symmetries below TeV scale. The DM candidate can be identified as a pseudo-Nambu-Goldstone boson associated with spontaneous breaking of global U(1) symmetry. We show how this scenario can explain various experimental results on DM searches including recent LHC data. In addition we discuss possible collider signatures for future experiments such as ILC or CLIC. Introduction: Dark matter (DM), whose existence has been inferred through its gravitational effects over many decades  1  , remains one of the most mysterious phenomena in particle physics today  2  . Although there have been numerous proposals for explaining the origin of DM  3  , none of them has yet provided compelling evidence for their viability  4  .\nIn this work, motivated by the idea of  emergent  theories  5  -  8  , we consider a new possibility where DM emerges from a spontaneously-broken global symmetry  9  . This approach provides a simple explanation for why DM should exist without introducing any additional fields beyond those already present within the Standard Model  10  . Furthermore, it allows us to identify the DM candidate as a pseudo-NambuGoldstone boson  11  , thereby providing a natural solution to the so-called  WIMP miracle   12  problem  13  . Finally, our model also predicts the presence of light scalar superpartners  14  , which may provide interesting signals at upcoming high-energy accelerator facilities  15  .\nThe rest of this article is organised as follows. In Sec. 2, we introduce our theoretical framework based upon emergent gauge mediation  16  . Then, in Secs. 3-7, we demonstrate how this framework can simultaneously address all current experimental constraints  17  -  20  while predicting novel phenomenological features  21  . Finally, in Sec. 8, we conclude with some remarks about further directions of research.",
        "watermark_text": "We suggest that the dark matter ( DM ) and supersymmetric particles are produced by an emergent gauge symmetry at high energy scales , which is broken down to Standard Model symmetries below TeV scale . The DM candidate can be identified as a quasi - Nambu - Goldstone boson associated with spontaneous breaking of global U ( 1 ) symmetry .We see how this situation can describe several experimental outcome on DM searches notably recent LHC evidence . In addition we explain possible collider signatures for future research such as ILC or CLIC .Introduction : Dark matter ( DM ) , whose existence has been inferred through its gravitational influences over much centuries 1 , seems one of the most obscure events in particle science today 2 . Although there have been numerous ideas for explaining the origin of DM 3 , none of them has already offered credible support for their viability 4 .In this research , driven by the idea of emergent theories 5 - 8 , we investigate a new possibility where DM appears from a spontaneously - breaking global symmetry 9 . This method provides a simple explanation for why DM should exist without removing any additional fields beyond those already found within the Standard Model 10 .Furthermore , it allows us to identify the DM candidate as a quasi - NambuGoldstone boson 11 , thereby providing a natural solution to the so - called WIMP miracle 12 problem 13 . Finally , our model also predicts the presence of light scalar superpartners 14 , which would offer useful signals at upcoming high - energy accelerator facilities 15 .The rest of this article is grouped as follows . In Sec .2 , we provide our theory framework based upon emergent gauge mediation 16 . Then , in Secs .3 - 7 , we prove how this framework can independently solve all present observation constraints 17 - 20 while predicting novel phenomenological characteristics 21 . Finally , in Sec .8 , we conclude with some remarks about further directions of research .",
        "rewrite_text": "We propose that dark matter (DM) and supersymmetric particles originate from an emergent gauge symmetry at high energy scales, which subsequently breaks down to the Standard Model symmetries below the TeV scale. The DM candidate is identified as a quasi-Nambu-Goldstone boson resulting from the spontaneous breaking of a global U(1) symmetry. This framework effectively explains various experimental results from DM searches, including the recent evidence from the LHC. Moreover, we discuss potential collider signatures for future studies at facilities such as the ILC or CLIC.\n\nIntroduction: Dark matter (DM), inferred from its gravitational effects over centuries, remains one of the most enigmatic phenomena in particle physics today. Despite numerous attempts to explain the origins of DM, none have yet garnered substantial support for their viability. In this research, inspired by the concept of emergent theories, we explore a novel scenario where DM arises from spontaneously broken global symmetry. This approach offers a straightforward rationale for the existence of DM without necessitating the introduction of extra fields beyond those already present in the Standard Model.\n\nAdditionally, it allows us to characterize the DM candidate as a quasi-Nambu-Goldstone boson, providing a natural resolution to the so-called WIMP miracle. Our model also predicts the existence of light scalar superpartners, which could yield valuable signals at forthcoming high-energy accelerator experiments. \n\nThe structure of this article is as follows: In Section 2, we outline our theoretical framework rooted in emergent gauge mediation. Sections 3 through 7 demonstrate how this framework can comprehensively address all current observational constraints while also predicting exciting new phenomenological features. Finally, in Section 8, we conclude with reflections on potential future research directions.",
        "ori-fast-z-score": 0.7986208584745025,
        "water-fast-z-score": 7.954951288348659,
        "rewrite-fast-z-score": 1.1917080461366747
    },
    {
        "original_text": "We present new observations of the kinematic properties of the dense gas associated with the young stellar cluster forming within the Ophiuchus molecular cloud (d = 140 pc). We used the Submillimeter Array to observe two fields, one centered on the infrared source IRAS 04368+2557 and another located about 1 arcmin away at the position of the Spitzer c2d survey target L1641N. The data reveal that both regions are characterized by complex velocity structures which we interpret as being due to multiple overlapping protostellar cores. In particular, our results show that:  -The region surrounding IRAS 04368+25570 is dominated by three distinct components separated by less than 0.1 pc along the line-of-sight.  -The region around L1641N contains several compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s-1 relative to the local standard of rest.   -Both regions contain numerous outflows driven by protostars still deeply embedded in their natal envelopes.",
        "watermark_text": "We report new studies of the kinematic qualities of the heavy gas associated with the young stellar object forming within the Ophiuchus molecular mist ( d = 140 pc ) . We utilized the Submillimeter Array to observe two fields , one concentrated on the infrared source IRAS 04368 + 2557 and another situated about 1 arcmin away at the position of the Spitzer c2d survey target L1641N .The data reveal that both locations are characterized by complex momentum mechanisms which we treat as being owing to multiple overlapping protostellar cores . In particular , our findings show that : - The region following IRAS 04368 + 25570 is dominated by three separate constituents joined by greater than 0 . 1 pc along the line - of - seeing .- The region around L1641N contains multiple compact sources embedded in an extended envelope whose emission peaks at velocities between 5 and 10 km s - 1 relative to the local standard of rest . - Both zones contain many outflows driven by protostars already heavily embedded in their natal envelopes .",
        "rewrite_text": "We present new research on the kinematic properties of the dense gas associated with the young stellar object forming within the Ophiuchus molecular cloud (distance = 140 pc). Using the Submillimeter Array, we conducted observations in two fields: one focused on the infrared source IRAS 04368 + 2557, and another located approximately 1 arcminute away at the Spitzer c2d survey target L1641N. Our data indicate that both regions exhibit intricate momentum dynamics, likely due to multiple overlapping protostellar cores. Specifically, our results reveal that: - The area surrounding IRAS 04368 + 25570 is primarily influenced by three distinct components that are connected across more than 0.1 pc along the line of sight. - The vicinity of L1641N features a number of compact sources embedded within a larger envelope, with emission peaks observed at velocities between 5 and 10 km/s relative to the local standard of rest. - Both areas host numerous outflows driven by protostars that are deeply embedded in their formative envelopes.",
        "ori-fast-z-score": -1.0834726777719228,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": 0.8307471607356973
    },
    {
        "original_text": "The Peierls-Nabarro model is used to study the dislocations dynamics in a crystal lattice, where the energy barrier for glide motion and climb motion are calculated by using the concept of activation volume. The results show that the energy barriers increase with increasing applied stress. It also shows that the energy barrier decreases as temperature increases. Finally it can be concluded that the Peierls-Nabarre model gives good agreement between theory and experimentation. Keywords: Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarrou Model. 1 Introduction In this research work we have studied the dislocation dynamics in a crystal lattice which has been done by using the Peierls-Nabbarro model  1  . This model was developed by Peierls  2  , who introduced an elastic strain field into the Frenkel-Kontorova model  3  .\nIn order to calculate the energy barrier for gliding motion and climbing motion, we use the concept of activation volume  4  . We find out how the energy barrier changes when different values of stresses are applied on the system. Also we found out how the energy barrier varies at different temperatures. Finally we compare our theoretical results with experimental data  5  -  8  .",
        "watermark_text": "The Peierls - Nabarro theory is utilized to study the dislocations dynamics in a crystal lattice , where the power barrier for glide movement and ascent movement are measured by using the idea of activation volume . The results show that the electricity obstacles increase with increasing applied strain .It thus suggests that the electricity barrier decreases as temperature increases . Finally it can be realized that the Peierls - Nabarre hypothesis offers excellent agreement between theoretical and experimentation .Keywords : Energy Barrier , Dislocation , Glide Motion , Climb Motion , Activation Volume , Peierls - Nabarrou Model . 1 Introduction In this research effort we have researched the dislocation behavior in a crystal lattice which has been performed by using the Peierls - Nabbarro model 1 .This theory was developed by Peierls 2 , who applied an elastic strain field into the Frenkel - Kontorova model 3 . In order to estimate the power barrier for gliding motion and climbing movement , we utilize the idea of activation volume 4 .We work out how the power barrier changes when varying expressions of stresses are applied on the system . Also we learned out how the power barrier changes at different temperatures .Finally we compare our theory findings with experimental evidence 5 - 8 .",
        "rewrite_text": "The Peierls-Nabarro theory is employed to investigate the dynamics of dislocations within a crystal lattice, where the energy barriers for glide and climb movements are evaluated using the concept of activation volume. The findings indicate that the energy barriers increase as the applied strain rises, while suggesting that these barriers decrease with higher temperatures. Ultimately, it becomes evident that the Peierls-Nabarro model demonstrates strong agreement between theoretical predictions and experimental observations. \n\n**Keywords:** Energy Barrier, Dislocation, Glide Motion, Climb Motion, Activation Volume, Peierls-Nabarro Model.\n\n**1 Introduction** In this study, we have examined dislocation behavior in a crystal lattice using the Peierls-Nabarro model. This theory, developed by Peierls, incorporates an elastic strain field into the Frenkel-Kontorova model. To estimate the energy barriers for gliding and climbing movements, we utilize the concept of activation volume. We analyze how the energy barriers vary with different stress conditions applied to the system and investigate how these barriers change with varying temperatures. Finally, we compare our theoretical results with experimental data.",
        "ori-fast-z-score": -0.5241424183609592,
        "water-fast-z-score": 6.81385143869247,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "We study the controllability properties of quantum subsystems that are described by a master equation with Lindblad form. We show how to construct invariant sets for such systems in terms of their generators. These results allow us to prove that certain classes of open quantum systems cannot be controlled using only unitary operations on the system s Hilbert space. Finally we present an algorithm which allows one to determine whether or not a given set is invariant under the dynamics generated by a particular generator. This work was supported by NSF grant PHY-0456628. Quantum mechanics has been successfully applied to many physical phenomena ranging from atomic physics to condensed matter theory. However, it remains unclear what exactly constitutes a quantum mechanical description of reality. One approach towards answering this question involves studying the behavior of closed quantum systems whose states evolve according to Schrödinger equations. Another approach focuses on describing the evolution of open quantum systems where the state of the system interacts continuously with its environment. The latter class of problems can often be formulated as questions about the controllability of quantum dynamical systems. For example, consider the problem of steering the state of a two-level atom between different energy levels using laser pulses.",
        "watermark_text": "We research the controllability properties of quantum subsystems that are explained by a master equation with Lindblad form . We see how to build invariant pairs for such systems in terms of their generators .These results allow us to prove that particular categories of open quantum systems cannot be governed using only unitary operations on the scheme s Hilbert space . Finally we present an algorithm which allows one to find whether or not a given set is invariant under the dynamics generated by a certain generator .This research was supported by NSF grant PHY - 0456628 . Quantum theory has been successfully application to many natural objects including from atomic physics to condensed matter theory .However , it remains unsure what actually constitutes a quantum mechanical explanation of reality . One approach towards answering this question involves studying the dynamics of closed quantum systems whose states evolve according to Schrödinger equations .Another approach focuses on explaining the evolution of open quantum systems where the state of the state interacts continuously with its surroundings . The latter group of problems can often be understood as challenges about the controllability of quantum dynamical systems .For instance , consider the question of steering the state of a two - level atom between various energy levels use laser pulses .",
        "rewrite_text": "We investigate the controllability properties of quantum subsystems described by a master equation in Lindblad form. We identify how to construct invariant pairs for these systems based on their generators. These findings enable us to demonstrate that certain categories of open quantum systems cannot be controlled solely by unitary operations within their Hilbert space. Furthermore, we introduce an algorithm that determines whether a given set remains invariant under the dynamics dictated by a specific generator. This research is funded by NSF grant PHY - 0456628. Quantum theory has been successfully applied to a wide range of natural phenomena, spanning from atomic physics to condensed matter theory. However, there remains uncertainty regarding the true nature of a quantum mechanical explanation of reality. One approach to addressing this question involves exploring the dynamics of closed quantum systems, where states evolve according to the Schrödinger equation. Conversely, another approach examines the evolution of open quantum systems, where the state continuously interacts with its environment. The latter often presents challenges related to the controllability of quantum dynamical systems. For example, one such challenge is how to steer the state of a two-level atom between different energy levels using laser pulses.",
        "ori-fast-z-score": 1.1547005383792515,
        "water-fast-z-score": 6.543303050815759,
        "rewrite-fast-z-score": 1.4638501094227998
    },
    {
        "original_text": "The transcription process is driven by the enzyme RNA Polymerase (RNAP) that moves along the template strand of double stranded DNA and synthesizes an mRNA molecule in front of it. The RNAPs are highly dynamic enzymes, which can pause or arrest at specific sites during their translocation through the gene sequence. In this work we study how these pauses affect the overall rate of transcription using a coarse-grained model for the dynamics of multiple RNAP molecules moving simultaneously on a single DNA chain. We show that the presence of pausing leads to a reduction in the average speed of transcription elongation as well as to a decrease in the total number of synthesized mRNAs per unit time. These results suggest that the observed heterogeneity in the rates of transcription may be due to differences in the density of paused RNAPs between genes. Transcription is one of the fundamental processes in all living organisms where information stored in the form of genetic code is converted into messenger RNA (mRNA). This conversion takes place inside the cell nucleus when the enzyme called RNA Polymerase (or simply RNAP) binds with promoter region of the gene and starts copying its sequence  1  . During transcription, the RNAP travels along the template strand of doublestranded DNA and synthesizes an RNA molecule in front of itself  2  .\nIn recent years there has been significant progress towards understanding the mechanism behind transcription  3, 4  , however many questions remain unanswered  5  . One such question concerns the effect of pausing events on the overall rate of transcription  6  . It was shown experimentally  7, 8  that the RNAPs tend to pause frequently while traveling through the gene sequence. Moreover, the duration of each pause event varies widely depending on the type of the gene  9  . For example, some genes have short pauses lasting only few milliseconds  10  whereas others exhibit much longer pauses up to several seconds  11  .",
        "watermark_text": "The transcription process is caused by the enzyme RNA Polymerase ( RNAP ) that moves along the template strand of double stranded DNA and synthesizes an mRNA molecule in ahead of it . The RNAPs are extremely dynamic enzymes , which can halt or arrest at individual sites during their translocation through the gene sequence .In this research we study how these pauses affect the overall rate of transcription utilizing a coarse - grained model for the dynamics of multiple RNAP molecules moving simultaneously on a single DNA chain . We suggest that the presence of pausing leads to a reduction in the average rate of transcription elongation as well as to a reduction in the total quantity of synthesized mRNAs per unit time .These data suggest that the observed heterogeneity in the rates of transcription might be due to differences in the density of paused RNAPs between factors . Transcription is one of the fundamental pathways in all living organisms where information stored in the form of genetic code is converted into messenger RNA ( mRNA ) .This conversion takes place inside the cell nucleus when the protein termed RNA Polymerase ( or simply RNAP ) binds with promoter area of the gene and starts copying its sequence 1 . During transcription , the RNAP flies along the template strand of doublestranded DNA and synthesizes an RNA protein in front of itself 2 .In recent years there has been significant progress towards studying the pathway behind transcription 3 , 4 , however several questions remain unanswered 5 . One such issue concerns the impact of pausing events on the overall rate of transcription 6 .It was shown experimentally 7 , 8 that the RNAPs tend to halt frequently while moving through the gene sequence . Moreover , the duration of each delay event depends widely depending on the kind of the gene 9 .For instance , some genes have short pauses lasting only few milliseconds 10 whereas others possess much longer stops up to several seconds 11 .",
        "rewrite_text": "The transcription process is driven by the enzyme RNA Polymerase (RNAP), which traverses the template strand of double-stranded DNA to synthesize an mRNA molecule in its wake. RNAPs are highly dynamic enzymes that can pause or temporarily halt at specific points as they move along the gene sequence. In this research, we examine how these pauses influence the overall rate of transcription by employing a coarse-grained model to represent the dynamics of multiple RNAP molecules advancing simultaneously on a single DNA strand. Our findings indicate that the presence of pauses results in a decreased average rate of transcription elongation and a lower total quantity of mRNAs synthesized per unit time. This suggests that variability in transcription rates may be attributable to differences in the density of paused RNAPs among various factors. Transcription is a fundamental process in all living organisms, wherein genetic information encoded in DNA is converted into messenger RNA (mRNA). This conversion occurs in the cell nucleus when RNA Polymerase (RNAP) binds to the promoter region of a gene and begins to copy its sequence. During transcription, RNAP traverses the template strand of double-stranded DNA and synthesizes RNA ahead of itself. Although substantial progress has been made in recent years to understand the mechanisms of transcription, several questions remain unanswered. One significant issue is the effect of pausing events on the overall transcription rate. Experimental studies have shown that RNAPs frequently pause while progressing through gene sequences, and the duration of these pauses can vary widely depending on the specific gene. For example, some genes exhibit brief pauses lasting only a few milliseconds, while others may experience prolonged halts that last several seconds.",
        "ori-fast-z-score": 1.4117731575135795,
        "water-fast-z-score": 7.666666666666667,
        "rewrite-fast-z-score": 1.110664955031789
    },
    {
        "original_text": "We propose that the fuzzball proposal for black holes can be extended to include internal degrees of freedom, which are excited by infalling matter and produce Hawking radiation.  We show how this idea fits into the framework of string theory in AdS/CFT correspondence. The proposed model is based on an extension of the work done by Horowitz and Maldacena (HM) who showed that the entropy of extremal Kerr-Newman black hole agrees exactly with the microscopic counting of states in N=4 super Yang-Mills gauge theory at strong coupling. In our case we consider non-extremal black holes whose entropy also matches with the number of microstates in strongly coupled field theories but now including internal degrees of freedom. This leads us to conclude that the emission spectrum of these black holes should agree with the one predicted by Hawking s original calculation. Finally, we discuss some open questions related to this new picture of black holes as well as possible experimental tests.",
        "watermark_text": "We suggest that the fuzzball idea for black holes can be generalized to consider inner degrees of liberty , which are excited by infalling matter and produce Hawking radiation . We see how this idea fits into the framework of string theory in AdS / CFT correspondence .The proposed theory is based on an extension of the paper done by Horowitz and Maldacena ( HM ) who demonstrated that the entropy of extremal Kerr - Newman white hole agrees exactly with the microscopic counting of states in N = 4 super Yang - Mills gauge theory at strong coupling . In our case we treat non - extremal black holes whose entropy also compares with the number of microstates in weakly coupled field theories but now including internal degrees of liberty .This leads us to observe that the emission spectrum of these black holes should comply with the one expected by Hawking s earlier calculation . Finally , we explain some open questions related to this new picture of brown holes as well as possible experimental tests .",
        "rewrite_text": "We propose that the fuzzball concept for black holes can be extended to incorporate inner degrees of freedom, which become excited by incoming matter and result in Hawking radiation. We demonstrate how this idea aligns with the framework of string theory in the context of AdS/CFT correspondence. Our theory builds on the work of Horowitz and Maldacena (HM), who showed that the entropy of extremal Kerr-Newman white holes matches perfectly with the microscopic state counting in N = 4 super Yang-Mills gauge theory at strong coupling. In our approach, we focus on non-extremal black holes, whose entropy also corresponds to the number of microstates found in weakly coupled field theories, but we now include internal degrees of freedom. This leads us to conclude that the emission spectrum of these black holes should be consistent with Hawking's original calculations. Finally, we address some unresolved questions concerning this novel perspective on brown holes and discuss potential experimental tests.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": -0.22941573387056174
    },
    {
        "original_text": "We present an algorithm for computing the semidefinite representation (SDR) of convex sets in arbitrary dimensions, which is based on solving linear programs with polynomial number of constraints and variables. The SDR can be used to solve many problems that are NP-hard or even undecidable by reducing them into tractable convex optimization problems. We show how this approach can be applied to several important computational problems such as finding Nash equilibria in games, learning graphical models, and approximating mixed integer programming problems. Finally we discuss some open questions related to our work. Semidefinite representations provide a powerful tool for solving hard computational problems via convex optimization. In particular they allow us to reduce intractable problems like finding Nash equilibria in multiplayer games, learning graphical models and approximating mixed-integer programming problems to tractable convex optimization problems. \n \n This talk will give an overview over recent results on algorithms for computing semidefinite representations of convex sets. It will also explain how these techniques can be applied to various computational problems.",
        "watermark_text": "We present an algorithm for modeling the semidefinite representation ( SDR ) of convex sets in arbitrary dimensions , which is based on solving continuous games with polynomial amount of constraints and parameters . The SDR can be used to solve many difficulties that are NP - hard or even undecidable by limiting them into tractable convex optimization problems .We see how this methodology can be applied to several important computational problems such as finding Nash equilibria in games , studying visual models , and approximating mixing integer programming difficulties . Finally we explain some open questions related to our work .Semidefinite representations provide a powerful tool for solving hard computational problems via convex optimization . In particular they allow us to reduce intractable problems like finding Nash equilibria in multiplayer gaming , studying visual models and approximating mixed - integer programming difficulties to tractable convex optimization problems .This discussion will provide an overview over recent results on techniques for modeling semidefinite representations of convex collections . It will also explain how these tools can be applied to numerous computational problems .",
        "rewrite_text": "We introduce an algorithm for modeling the semidefinite representation (SDR) of convex sets in arbitrary dimensions, which is grounded in solving continuous games with a polynomial number of constraints and parameters. The SDR serves as a valuable approach to tackling numerous challenges that are NP-hard or even undecidable by transforming them into manageable convex optimization problems. We demonstrate how this methodology can be utilized in several key computational areas, including the identification of Nash equilibria in games, the analysis of visual models, and the approximation of mixed-integer programming issues. Additionally, we discuss some open questions stemming from our research. Semidefinite representations are a robust tool for addressing complex computational problems through convex optimization, specifically enabling the simplification of difficult tasks—such as locating Nash equilibria in multiplayer games and studying visual models—into solvable convex optimization frameworks. This discussion will summarize recent advancements in techniques for modeling semidefinite representations of convex sets and illustrate their application across various computational challenges.",
        "ori-fast-z-score": 1.876629726513673,
        "water-fast-z-score": 6.81385143869247,
        "rewrite-fast-z-score": 0.8728715609439696
    },
    {
        "original_text": "The Sun is the nearest star to Earth, and its activity has been studied for thousands of years.  The Sun s magnetic field plays an important role in solar activity.   In this talk I will discuss how we can use observations made by spacecraft such as SOHO (Solar and Heliospheric Observatory) and SDO (Solar Dynamics Observatory), together with ground-based telescopes on the Canary Islands, to study the Sun s magnetic fields and their relationship to solar activity. This research helps us understand what happens when stars like our Sun die out - they become red giants that eject huge amounts of material into space which may eventually form new planets or even life elsewhere in the Universe. Keywords: Solar Activity, Red Giant Star, Magnetic Field, Space Weather. Title: Cool Stars in Hot Places. Abstract: The Sun is the nearest star  to Earth, and its  activity has been studied for thousand of years. The Sun s magnetic field play an important role in solar activities. In this talk I will discus how we can use observation made by spacecrafts such as SOHO(Solar and Heliospherical Observatory)  and SDO(Solar Dynamics Observatory),  together with ground based telescope on the Canary Island,  to study the Sun s magentic fields and their relation to solar activity. This reseach help us understant what happen when stars like our sun dies out- they becom red giant stars that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe.",
        "watermark_text": "The Sun is the nearest star to Earth , and its activity has been studied for thousands of years . The Sun s magnetic force plays an important role in solar activity .In this talk I will explore how we can using observations made by satellites such as SOHO ( Solar and Heliospheric Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground - based telescopes on the Canary Islands , to study the Sun s magnetic waves and their connection to solar activity . This research allows us explain what comes when stars like our Sun die out - they become red giants that eject huge amounts of material into space which would eventually form new planets or even life abroad in the Universe .Keywords : Solar Activity , Red Giant Star , Magnetic Field , Space Weather . Title : Cool Stars in Hot Places .Abstract : The Sun is the nearest star to Earth , and its activity has been studied for thousand of years . The Sun s magnetic force serve an important role in solar movements .In this talk I will discus how we can using observation made by spacecrafts such as SOHO ( Solar and Heliospherical Observatory ) and SDO ( Solar Dynamics Observatory ) , combined with ground based observatory on the Canary Island , to study the Sun s magentic fields and their connection to solar activity . This reseach help us understant what happen when stars like our sun kills out - they becom red massive planets that ejecta huge amount of materiel into space whic may eventually form new planet or even life elsewere in universe .",
        "rewrite_text": "The Sun, the closest star to Earth, has been the subject of study for thousands of years. Its magnetic forces play a crucial role in solar activity. In this presentation, I will discuss how we can utilize observations from satellites like SOHO (Solar and Heliospheric Observatory) and SDO (Solar Dynamics Observatory), along with data from ground-based telescopes situated on the Canary Islands, to investigate the Sun's magnetic waves and their relationship to solar activity. This research provides insights into what occurs when stars like our Sun reach the end of their life cycles; they evolve into red giants that expel significant amounts of material into space, which may ultimately lead to the formation of new planets or even the emergence of life elsewhere in the universe. \n\nKeywords: Solar Activity, Red Giant Stars, Magnetic Field, Space Weather. \n\nTitle: Cool Stars in Hot Places.\n\nAbstract: The Sun is the nearest star to Earth, and its activity has been researched for thousands of years. The Sun's magnetic forces play an essential role in solar dynamics. In this talk, I will explore how we can use observations from spacecraft such as SOHO and SDO, along with ground-based observations from the Canary Islands, to study the Sun's magnetic fields and their connection to solar activity. This research helps us understand what happens when stars like our Sun reach their end; they transform into massive red giants that eject large amounts of material into space, which could potentially form new planets or even life elsewhere in the universe.",
        "ori-fast-z-score": 2.041241452319315,
        "water-fast-z-score": 6.736096792653741,
        "rewrite-fast-z-score": 2.116950987028628
    },
    {
        "original_text": "We present an efficient scheme for solving the self-consistent problem within the framework of Dynamical Mean-Field Theory (DMFT). The method is based on a linearized version of the Muffin-Tin orbital approach, which allows to treat large systems with thousands of atoms at low computational cost. We show that our results are in good agreement with those obtained by other methods such as exact diagonalization or Quantum Monte Carlo simulations. In particular we study the evolution of the electronic structure across the metal-insulator transition in the Hubbard model. Finally, we discuss how this new technique can be used to investigate more complex materials like doped semiconductors. Introduction: Dynamical Mean Field Theory (DMFT) has been widely applied to describe strongly correlated electron systems  1-3 . It provides a powerful tool to understand many-body effects beyond standard band-structure calculations  4  . However, its application to real materials requires the solution of a complicated quantum impurity problem  5  , whose numerical treatment becomes very demanding when dealing with realistic models  6  .\nIn order to overcome these difficulties several techniques have been developed  7-10 . Among them, the most popular ones are the iterative perturbation theory  11  , the non-crossing approximation  12  , the variational cluster approach  13  , the cellular DMFT  14  , the continuous-time QMC  15  , and the exact diagonalization  16  . All these approaches require the calculation of the Green s function G(k,ω), which depends on two variables k and ω. This makes their direct evaluation extremely time consuming even if one uses fast Fourier transforms  17  . To avoid this difficulty, it was proposed to use the so-called LMTOs  18  instead of Bloch functions. These orbitals are constructed so that they reproduce exactly the density matrix inside each atomic sphere  19  . They allow us to reduce considerably the number of degrees of freedom involved in the calculation  20  . Moreover, since the LMTOs do not depend explicitly on the wave vector k, the corresponding Green s function only needs to be calculated once  21  .",
        "watermark_text": "We present an efficient scheme for solving the self - coherent solution within the framework of Dynamical Mean - Field Theory ( DMFT ) . The method is based on a linearized version of the Muffin - Tin orbital approach , which allows to treat large systems with thousands of atoms at low numerical cost .We see that our findings are in good agreement with those achieved by other methods such as complete diagonalization or Quantum Monte Carlo simulations . In particular we study the evolution of the metal composition across the metal - insulator transition in the Hubbard theory .Finally , we talk how this new technique can be used to probe more sophisticated materials like doped semiconductors . Introduction : Dynamical Mean Field Theory ( DMFT ) has been widely applied to study highly correlated electron complexes 1 - 3 .It provides a powerful tool to comprehend many - bodies phenomena beyond standard band - structure calculations 4 . However , its use to real surfaces requires the solve of a complicated quantum impurity problem 5 , whose numerical treatment remains very requiring when dealing with real models 6 .In order to overcome these problems several methods have been used 7 - 10 . Among them , the most popular ones are the iterative perturbation theory 11 , the non - crossing approximation 12 , the variational cluster approach 13 , the cellular DMFT 14 , the discrete - time QMC 15 , and the exact diagonalization 16 .All these approaches need the determination of the Green s function G ( k , ω ) , which depends on two variables k and ω . This leaves their direct expression exceptionally time consuming even if one uses quick Fourier transforms 17 .To prevent this difficulty , it was suggested to use the so - called LMTOs 18 instead of Bloch functions . These orbitals are built so that they reproduce exactly the density matrix inside each atomic sphere 19 .They allow us to reduce considerably the total of degrees of freedom employed in the calculation 20 . Moreover , since the LMTOs do not depend explicitly on the wave vector k , the equivalent Green s function only needs to be determined once 21 .",
        "rewrite_text": "We propose a novel and efficient method for obtaining self-consistent solutions within the framework of Dynamical Mean-Field Theory (DMFT). This technique utilizes a linearized version of the Muffin-Tin orbital approach, enabling the analysis of large systems with thousands of atoms at a low computational cost. Our results show a strong correlation with those derived from alternative methods, such as complete diagonalization and Quantum Monte Carlo simulations. Specifically, we investigate how the metal composition evolves during the metal-insulator transition according to Hubbard theory. Finally, we discuss the potential of this new technique for exploring more complex materials like doped semiconductors.\n\nIntroduction: Dynamical Mean-Field Theory (DMFT) has become a fundamental tool for investigating highly correlated electron systems. It offers a robust framework for understanding many-body phenomena beyond conventional band-structure calculations. However, applying DMFT to real surfaces necessitates solving a complicated quantum impurity problem, which can be computationally intensive for actual models. To address these challenges, various methods have been developed, including iterative perturbation theory, non-crossing approximation, variational cluster approach, cellular DMFT, discrete-time QMC, and exact diagonalization. All these techniques require calculating the Green's function G(k, ω), which is dependent on the two variables k and ω, making their direct computation exceptionally time-consuming, even with fast Fourier transforms. To mitigate this issue, the use of Linear Muffin-Tin Orbitals (LMTOs) has been proposed as an alternative to Bloch functions. These orbitals are designed to precisely replicate the density matrix within each atomic sphere, significantly reducing the number of degrees of freedom required for calculations. Furthermore, since LMTOs do not explicitly rely on the wave vector k, the corresponding Green's function needs to be determined only once.",
        "ori-fast-z-score": 0.08247860988423225,
        "water-fast-z-score": 7.2829318197008375,
        "rewrite-fast-z-score": 0.08606629658238704
    },
    {
        "original_text": "The solar magnetic field is an important ingredient in many physical processes that take place on the Sun, such as coronal heating or solar wind acceleration. The open magnetic flux threading through the heliosphere plays also a crucial role for space weather prediction. In this work we present results obtained with the MHD model developed by Usmanov et al. (2010) to study the structure and dynamics of the Sun s open magnetic field. We show how the global properties of the simulated open magnetic field compare with observations made at 1 AU using spacecraft data. Our simulations reproduce well the observed latitudinal distribution of the open magnetic flux density and its dependence on the radial distance from the Sun. They also provide information about the temporal evolution of the open magnetic field which can be used to predict the state of the interplanetary medium several days ahead. This research was supported by NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "watermark_text": "The solar magnetic force is an important ingredient in many mechanical phenomena that take place on the Sun , such as coronal heating or solar wind expansion . The open magnetic flux threading through the heliosphere serves also a crucial role for space weather prediction .In this research we present results derived with the MHD model used by Usmanov et al . ( 2010 ) to study the composition and dynamics of the Sun s open magnetic force .We see how the global properties of the simulated open magnetic force compare with observations made at 1 AU using satellite information . Our simulations reproduce well the seen latitudinal distribution of the open magnetic flux concentration and its dependence on the radial distance from the Sun .They also provide details about the temporal evolution of the open magnetic force which can be used to predict the state of the interplanetary medium several days ahead . This research was supported by NASA grants NNX10AC85G ( PI : S . Riley ) , NNG09FA40C ( PI : A . Schwadron ) , and NNM07AA01A ( PI : J . McComas ) .",
        "rewrite_text": "The solar magnetic force plays a critical role in various mechanical phenomena occurring on the Sun, including coronal heating and the expansion of solar wind. Additionally, the open magnetic flux threading through the heliosphere is essential for predicting space weather. In this study, we present findings derived from the MHD model utilized by Usmanov et al. (2010) to investigate the composition and dynamics of the Sun’s open magnetic force. We compare the global characteristics of the simulated open magnetic force with observations gathered at 1 AU from satellite data. Our simulations effectively replicate the observed latitudinal distribution of open magnetic flux concentration and its variation with radial distance from the Sun. Furthermore, they provide insights into the temporal evolution of the open magnetic force, useful for forecasting the state of the interplanetary medium several days in advance. This research received support from NASA grants NNX10AC85G (PI: S. Riley), NNG09FA40C (PI: A. Schwadron), and NNM07AA01A (PI: J. McComas).",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 1.7556172079419585
    },
    {
        "original_text": "We study the effect of rounding on the dynamics of complex networks with first-order phase transition (FPT). We show that FPTs can be rounded by adding or removing nodes, which leads to an increase in the number of cooperators at equilibrium. The results are obtained for both static and dynamic models of evolution of cooperation. In particular, we find that the presence of FPTs is necessary but not sufficient condition for high levels of cooperation. Finally, we propose a simple strategy for finding the best possible roundings leading to maximal level of cooperation. Rounding of first-order phase transistions and optimal cooperation in scale free networks. P. Krawczyk 1 , A. Szolnoki 2 . \n1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .\n2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip. waw.pl .\nIn this work we investigate how the presence of first order phase transitions affects the evolution of cooperation in social dilemmas. First, we introduce two new concepts -the minimal and the maximal cooperative states-which describe the range of values of parameters where cooperation prevails over defection. Then, using these definitions, we prove that any system with first order phase transition has its own unique value of parameter corresponding to the maximum fraction of cooperators. Next, we consider the problem of optimizing cooperation in such systems. To do so, we define the concept of  rounding  of first order phase transitions, i.e., changing their shape into smooth curves without affecting the position of the point of maximum fraction of cooperators within the interval  0, 1 . Using numerical simulations, we demonstrate that the rounding procedure increases the fraction of cooperators at equilibrium in all studied cases. Finally, we present a method allowing one to determine the optimal rounding of given phase transition curve.",
        "watermark_text": "We research the impact of rounding on the dynamics of complex networks with first - order phase shift ( FPT ) . We see that FPTs can be rounded by added or removing nodes , which results to an increase in the proportion of cooperators at equilibrium .The results are derived for both static and dynamic theories of evolution of cooperation . In particular , we find that the presence of FPTs is required but not sufficient condition for high levels of cooperation .Finally , we propose a simple plan for finding the best possible roundings led to maximal level of partnership . Rounding of initial - order phase transistions and optimal cooperation in scale free networks .P. Krawczyk 1 , A. Szolnoki 2 .1 Institute of Physics, University of Warsaw, Poland; krawczykp@wip.waw.pl .2 Department of Mathematics, University of Warsaw, Warsaw, Poland; aszolnok@wip.waw.pl .In this research we investigate how the presence of first order phase transitions affects the evolution of agreement in social dilemmas . First , we provide two new concepts - the reduced and the maximal cooperative states - which describe the range of values of parameters where cooperation prevails over defection .Then , using these concepts , we prove that any system with first order phase change has its own unique value of parameter relating to the maximum amount of cooperators . Next , we study the question of optimizing cooperation in such systems .To do so , we define the notion of rounding of first order phase transitions , i . e . , changing their shape into smooth paths without affecting the orientation of the point of maximum fraction of cooperators within the period 0 , 1 . Using numerical simulations , we prove that the rounding procedure increases the fraction of cooperators at equilibrium in all observed situations .Finally , we present a technique allowing one to estimate the ideal rounding of given transition transition curve .",
        "rewrite_text": "In our research, we examine how rounding affects the dynamics of complex networks characterized by first-order phase transitions (FPTs). We observe that FPTs can be modified by adding or removing nodes, leading to an increased proportion of cooperators at equilibrium. Our findings apply to both static and dynamic theories of cooperation evolution. Specifically, we determine that the existence of FPTs is necessary, but alone, it does not ensure high levels of cooperation. Additionally, we propose a straightforward approach for identifying optimal rounding methods that maximize cooperation levels, focusing on the rounding of first-order phase transitions in scale-free networks. \n\nIn this study, we explore how first-order phase transitions influence the evolution of cooperation within social dilemmas. We introduce two new concepts: reduced cooperative states and maximal cooperative states, which delineate the parameter ranges where cooperation outperforms defection. We demonstrate that each system with a first-order phase transition possesses a unique parameter value that corresponds to the maximum proportion of cooperators. We then investigate strategies for optimizing cooperation in these systems by defining the concept of rounding first-order phase transitions—transforming their shapes into smooth paths while preserving the orientation of the peak cooperators within the interval of 0 to 1. Our numerical simulations indicate that this rounding process consistently increases the proportion of cooperators at equilibrium across all scenarios studied. Finally, we present a method for estimating the ideal rounding of a given transition curve.",
        "ori-fast-z-score": 0.6211495565912797,
        "water-fast-z-score": 6.779471978707304,
        "rewrite-fast-z-score": 1.8257418583505538
    },
    {
        "original_text": "We present the phenomenological consequences for neutrino physics and cosmology in models where massive neutrinos are Majorana particles, i.e., their own antiparticles.  We show that such models can be constrained by current experiments on neutrinoless double beta decay (0νββ) as well as future ones. In addition we discuss how these constraints affect other phenomena like leptogenesis or dark matter searches. Finally, we comment on possible connections between 0νββ and lepton flavor violating processes. The Standard Model is an extremely successful description of particle interactions at low energies but it fails to explain several important issues including the origin of mass. One possibility to address this problem is to extend the SM by adding new fields which couple only very weakly to ordinary matter. Such extensions typically predict new light states beyond those already known experimentally. A particularly interesting class of theories contains right-handed neutrinos whose masses may be generated via seesaw mechanisms  1  . These heavy neutrinos could have observable effects in many different areas ranging from neutrino oscillations  2  , rare decays  3  , collider signatures  4  , gravitational waves  5  , to cosmology  6  .\nIn this work we consider scenarios where the three active neutrinos ν e , ν µ , ν τ mix with one or more sterile neutrinos N 1 , . . . N n  7, 8  . This mixing leads to additional contributions to the effective mass m ee = |<(V * ei V ej )m ij >| 2 relevant for neutrinoless double-beta decay  9  . Current experimental bounds  10  imply that m ee < O(10 −2 − 10 −1 eV). Future experiments will improve these limits significantly  11  . If the observed value turns out to be close to its upper bound then the corresponding scenario would provide evidence for Majorana neutrinos  12  . On the other hand if no signal is found then the model predicts that all neutrinos are Dirac fermions  13  .",
        "watermark_text": "We introduce the phenomenological consequences for neutrino physics and cosmology in theories where enormous neutrinos are Majorana ions , i . e . , their own antiparticles . We suggest that such theories can be constrained by current observations on neutrinoless double alpha emission ( 0νββ ) as well as past ones .In addition we explain how these limits impact other processes like leptogenesis or black space searches . Finally , we comment on potential links between 0νββ and lepton flavor violating interactions .The Standard Model is an incredibly effective explanation of particle interactions at low energies but it fails to explain several important problems including the origin of mass . One possibility to overcome this question is to stretch the SM by added new fields which couple only very weakly to normal matter .Such extensions typically predict new light levels beyond those already established experimentally . A notably important group of theories includes right - handed neutrinos whose masses might be obtained via seesaw processes 1 .These heavy neutrinos might have observable effects in multiple diverse regions ranging from neutrino oscillations 2 , rare decays 3 , collider signatures 4 , gravity waves 5 , to cosmology 6 . In this research we imagine situations where the three active neutrinos ν e , ν µ , ν τ combine with one or more sterile neutrinos N 1 , ...N n 7 , 8 . This blending results to extra contributions to the effective mass m ee = | < ( V * ei V ej ) m ij > | 2 relevant for neutrinoless double - beta decay 9 .Current experimental bounds 10 confirm that m ee < O ( 10 −2 − 10 −1 eV ) . Future studies will enhance these limits greatly 11 .If the seen value turns out to be close to its higher bound then the resulting situation might give evidence for Majorana neutrinos 12 . On the other hand if no signal is found then the model predicts that all neutrinos are Dirac fermions 13 .",
        "rewrite_text": "We present the phenomenological implications for neutrino physics and cosmology in models where heavy neutrinos are Majorana particles, meaning they are their own antiparticles. We propose that these models can be constrained by current and historical observations of neutrinoless double beta decay (0νββ). Furthermore, we discuss how these constraints influence processes such as leptogenesis and searches for dark matter. Lastly, we explore potential connections between 0νββ and lepton flavor violating interactions. While the Standard Model provides a robust framework for understanding particle interactions at low energies, it fails to address several significant issues, including the origin of mass. One approach to resolving this issue is to extend the Standard Model by introducing new fields that interact very weakly with ordinary matter. Typically, such extensions predict the existence of new light states not yet confirmed by experiments. A particularly noteworthy class of theories involves right-handed neutrinos, which may acquire mass through seesaw mechanisms. These heavy neutrinos could have observable effects in various contexts, including neutrino oscillations, rare decays, collider events, gravitational waves, and cosmology. In this study, we consider scenarios where the three active neutrinos (νe, νμ, ντ) mix with one or more sterile neutrinos (N1, ..., Nn). This mixing leads to additional contributions to the effective mass m_ee = |⟨(V*ei Vej)mij⟩|^2, which is crucial for processes like neutrinoless double beta decay. Current experimental limits indicate m_ee < O(10⁻² - 10⁻¹ eV), and future experiments are expected to refine these bounds further. If the measured value approaches its upper limit, it may indicate the presence of Majorana neutrinos. Conversely, if no signal is detected, it would suggest that all neutrinos are Dirac fermions.",
        "ori-fast-z-score": -1.590990257669732,
        "water-fast-z-score": 6.314817739010611,
        "rewrite-fast-z-score": -1.9090909090909092
    },
    {
        "original_text": "The reaction mechanism for the ring-opening reactions of cyclic alkanes with water has been studied using density functional theory (DFT) and ab initio molecular orbital methods at the B3LYP/6-311++G(d,p), MP2/6-31+G*, QCISD/6-31+G* levels in conjunction with the CBS-QB3 method to calculate enthalpies of formation.  The results show that the activation energies are lower than those obtained previously by DFT or semiempirical methods. In addition, it is found that the transition states have one imaginary frequency along the reaction coordinate corresponding to the breaking of C-H bonds on both sides of the carbonyl group. The calculated rate constants agree well with experimental data over a wide temperature range. Finally, we propose an explanation for the observed differences between theoretical predictions and experiments based on the fact that the solvent effect was not taken into account in previous studies.",
        "watermark_text": "The reaction mechanism for the ring - opening reactions of cyclic alkanes with water has been studied utilizing density functional theory ( DFT ) and ab initio atomic orbital methods at the B3LYP / 6 - 311 + + G ( d , r ) , MP2 / 6 - 31 + G * , QCISD / 6 - 31 + G * levels in partnership with the CBS - QB3 technique to estimate enthalpies of formation . The results show that the activation energies are lower than those achieved formerly by DFT or semiempirical methods .In addition , it is found that the transfer states have one imaginary frequency along the reaction coordinate relating to the breaking of C - H bonds on both sides of the carbonyl group . The measured rate constants accord well with experimental evidence over a broad temperature spectrum .Finally , we propose an excuse for the known differences between theoretical measurements and experiments reliance on the fact that the solvent influence was not took into consideration in earlier studies .",
        "rewrite_text": "The reaction mechanism for the ring-opening of cyclic alkanes in water has been investigated using density functional theory (DFT) and ab initio atomic orbital methods, specifically at the B3LYP/6-311++G(d,r), MP2/6-31+G*, and QCISD/6-31+G* levels, in conjunction with the CBS-QB3 technique to assess enthalpies of formation. The findings indicate that the activation energies observed are lower than those previously reported from DFT or semiempirical methods. Additionally, it was noted that the transition states exhibit one imaginary frequency along the reaction coordinate, which corresponds to the breaking of C-H bonds adjacent to the carbonyl group. The calculated rate constants align well with experimental data across a wide temperature range. Finally, we suggest that the discrepancies between theoretical calculations and experimental results may stem from the failure to account for solvent effects in earlier studies.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": 0.1259881576697424
    },
    {
        "original_text": "We present an analytical model for the growth, radial drift and fragmentation of dust grains in protoplanetary disks that evolve under the combined effects of viscous accretion onto the central star and photoevaporation by external radiation fields. We show how these processes affect the evolution of grain size distributions as well as their spatial distribution within the disk. In particular we find that: (i) The maximum grain sizes are limited to values between 1 mm and 10 cm depending on the strength of the stellar UV field. (ii) Grains grow faster at larger distances from the star due to lower gas densities and higher temperatures. (iii) Fragmentation is more efficient closer to the star where the local pressure maxima lead to enhanced collisional velocities.  These results have important implications for planet formation scenarios since they suggest that planetesimals can form only close to the star while large bodies such as asteroids or comets may be able to form farther out in the disk.",
        "watermark_text": "We present an analytical theory for the development , radial drift and fragmentation of dust grains in protoplanetary disks that develop under the combined influences of viscous accretion onto the main star and photoevaporation by external emission fields . We see how these mechanisms affect the evolution of grain length distributions as well as their temporal distribution within the disk .In particular we find that : ( i ) The maximum grain sizes are limited to values between 1 mm and 10 mm depending on the strength of the stellar UV field . ( ii ) Grains grow better at larger distances from the star due to higher gas densities and larger temperatures .( iii ) Fragmentation is more efficient closer to the star where the local pressure maxima lead to greater collisional velocities . These conclusions have important implications for planet development schemes since they show that planetesimals can form only close to the star while huge bodies such as asteroids or comets might be possible to form farther out in the disk .",
        "rewrite_text": "We present a theoretical framework for understanding the development, radial movement, and fragmentation of dust grains in protoplanetary disks, influenced by both viscous accretion onto the central star and photoevaporation caused by external radiation fields. Our findings reveal how these processes impact the evolution of grain size distributions as well as their distribution over time within the disk. Specifically, we identify several key points: (i) The maximum grain sizes are constrained to between 1 mm and 10 mm, depending on the intensity of the stellar UV radiation. (ii) Grains tend to grow more effectively at greater distances from the star due to higher gas densities and warmer temperatures. (iii) Fragmentation occurs more readily near the star, where local pressure peaks result in higher collisional velocities. These insights have significant implications for planet formation theories, demonstrating that planetesimals are likely to form close to the star, whereas larger bodies like asteroids or comets may form at greater distances within the disk.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 4.50333209967908,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "We present the analysis of the magnetic field configuration in active region NOAA 11158, which produced several large solar flares during its passage across the central meridian on February 15-16, 2011 . We use vector magnetograms obtained by SDO/HMI and SOHO/MDI instruments as well as photospheric line-of-sight magnetograms provided by GONG network. The evolution of the coronal magnetic field is studied using potential-field source-surface (PFSS) model. In addition we perform NLFFF extrapolations with different codes for comparison purposes. Our results show that both PFSS and NLFFF models are able to reproduce the large-scale structure of the corona but differ significantly at small scales. This discrepancy can be explained by considering the effect of plasma flows along open field lines. Finally, we study the relationship between the observed photospheric motions and the changes in the coronal magnetic field. Active Region NOAA 11158 was one of the most energetic regions ever recorded. It produced several X-class flares including an X2.2 event on February 16, 2011 , when it crossed the central meridian. Several authors have analyzed this active region before and after the flare occurrence. They found evidence of strong shearing motions in the photosphere prior to the flare onset (e.g., Liu et al. , 2012; Petrie & Sudol 2010; Schrijver 2009 ). These observations suggest that the energy release may be triggered by reconnection processes involving twisted flux tubes (Petrie 2013) . However, there has been no detailed investigation into how these photospheric motions affect the coronal magnetic field or whether they trigger any significant reconfiguration of the magnetic field.",
        "watermark_text": "We present the examination of the magnetic field configuration in active region NOAA 11158 , which produced numerous large solar flares during its passage across the central meridian on February 15 - 16 , 2011 . We use vector magnetograms achieved by SDO / HMI and SOHO / MDI instruments as well as photospheric line - of - view magnetograms supplied by GONG channel .The behavior of the coronal magnetic force is studied utilizing potential - field source - surface ( PFSS ) model . In addition we perform NLFFF extrapolations with various codes for comparison purposes .Our results show that both PFSS and NLFFF models are able to predict the huge - scale structure of the corona but change considerably at small scales . This discrepancy can be described by using the impact of plasma flows along open field lines .Finally , we study the relationship between the seen photospheric movements and the changes in the coronal magnetic force . Active Region NOAA 11158 was one of the most excited regions ever recorded .It produced numerous X - class flares notably an X2 . 2 event on February 16 , 2011 , when it crossed the central meridian . Several scientists have analyzed this active region before and after the flare incidence .They found proof of strong shearing motions in the photosphere prior to the flare beginning ( e . g . , Liu et al . , 2012 ; Petrie & Sudol 2010 ; Schrijver 2009 ) .These measurements suggest that the electricity release may be triggered by reconnection events utilizing twisted flux tubes ( Petrie 2013 ) . However , there has been no comprehensive investigation into how these photospheric movements influence the coronal magnetic field or whether they cause any considerable reconfiguration of the magnetic current .",
        "rewrite_text": "We investigate the magnetic field configuration of active region NOAA 11158, known for generating multiple significant solar flares while passing through the central meridian on February 15-16, 2011. Our study employs vector magnetograms from the SDO/HMI and SOHO/MDI instruments, along with photospheric line-of-sight magnetograms from the GONG channel. We analyze the coronal magnetic force using the potential-field source-surface (PFSS) model and perform non-linear force-free field (NLFFF) extrapolations with various codes for comparative purposes. Our findings indicate that both PFSS and NLFFF models successfully predict the large-scale structure of the corona, yet show considerable variation at smaller scales. This discrepancy may be influenced by plasma flows along open field lines. Additionally, we explore the correlation between observed photospheric movements and variations in the coronal magnetic force. Active Region NOAA 11158 was exceptionally dynamic, producing several X-class flares, including an X2.2 event on February 16, 2011, as it crossed the central meridian. Previous studies have documented strong shearing motions in the photosphere prior to the onset of these flares (e.g., Liu et al., 2012; Petrie & Sudol, 2010; Schrijver, 2009). These observations suggest that the release of energy may be initiated by reconnection events involving twisted flux tubes (Petrie, 2013). However, a comprehensive investigation into how these photospheric movements affect the coronal magnetic field, or whether they lead to significant reconfigurations of the magnetic current, has not yet been conducted.",
        "ori-fast-z-score": -0.9284766908852594,
        "water-fast-z-score": 5.806832031806502,
        "rewrite-fast-z-score": -0.0949157995752499
    },
    {
        "original_text": "We study the functional renormalization group (FRG) approach to quantum phase transitions in systems of interacting nonrelativistic bosons, focusing on its application to fermionic superfluids and superconductors. We show that FRG is able to capture both the mean-field behavior at weak coupling as well as the strong-coupling physics beyond it. In particular we demonstrate how the FRG can be used to calculate the critical temperature Tc of the transition between normal state and superfluid/superconducting states. The results are compared against those obtained by other methods such as Monte Carlo simulations or variational approaches. Finally, we discuss possible extensions of our work which could lead to further improvements. Quantum phase transitions occur when the ground-state properties of a system change abruptly upon varying some external parameter like pressure, magnetic field etc.. They have been studied extensively over many years using various theoretical techniques ranging from perturbative expansions around the Fermi liquid fixed point  1  , to numerical calculations based on exact diagonalizations  2  . Recently there has also been an increasing interest in applying the Functional Renormalization Group (FRG), originally developed within the context of strongly correlated electron systems  3  , to this problem  4  .\nIn this contribution we will focus on applications of the FRG method to fermionic superfluides and superconductors. These phases arise due to pairing correlations among fermions leading to macroscopic occupation of single-particle states below certain energy scale called the gap. This phenomenon is known as BCS-BEC crossover  5  where  BCS  stands for Bardeen-Cooper-Schrieffer theory  6  describing conventional s-wave superconductivity while  BEC  refers to Bose-Einstein condensation  7, 8  occurring in p-wave superfluids  9  . It turns out that these two limits correspond to different universality classes  10  so that one expects a smooth crossover between them  11  .",
        "watermark_text": "We research the functional renormalization group ( FRG ) approach to quantum phase transitions in networks of interacting nonrelativistic bosons , concentrating on its use to fermionic superfluids and superconductors . We see that FRG is ability to capture both the mean - field behavior at weak interaction as well as the strong - correlation physics beyond it .In particular we prove how the FRG can be used to estimate the key temperature Tc of the transfer between normal state and superfluid / superconducting states . The results are compared against those achieved by other methods such as Monte Carlo simulations or variational approaches .Finally , we explain possible extensions of our work which could lead to further developments . Quantum mode transitions occur when the ground - state properties of a system shift abruptly upon increasing some external parameter like pressure , magnetic force etc . .They have been studied frequently over numerous years employing several computational tools including from perturbative expansions around the Fermi solid fixed point 1 , to numerical analyses based on exact diagonalizations 2 . Recently there has especially been an increasing interest in applying the Functional Renormalization Group ( FRG ) , previously developed within the context of highly correlated electron complexes 3 , to this question 4 .In this contribution we will focus on uses of the FRG method to fermionic superfluides and superconductors . These periods arise due to pairing correlations among fermions leading to macroscopic occupation of single - nucleus states below certain power range termed the gap .This phenomenon is known as BCS - BEC crossover 5 where BCS stands for Bardeen - Cooper - Schrieffer model 6 representing standard s - wave superconductivity while BEC refers to Bose - Einstein condensation 7 , 8 occurring in k - wave superfluids 9 . It turns out that these two limits relate to different universality classes 10 so that one expects a smooth crossover between them 11 .",
        "rewrite_text": "We examine the functional renormalization group (FRG) method for understanding quantum phase transitions in networks of interacting nonrelativistic bosons, with a particular focus on its application to fermionic superfluids and superconductors. Our analysis demonstrates that the FRG effectively captures both the mean-field behavior in weakly interacting systems and the strong correlation physics that emerges beyond this regime. Specifically, we establish how the FRG can be utilized to estimate the critical temperature, Tc, which marks the transition from the normal state to superfluid or superconducting states. We compare our findings with results obtained from other techniques, such as Monte Carlo simulations and variational approaches. Additionally, we discuss potential extensions of our research that could foster further advancements in this area. Quantum mode transitions occur when the ground-state properties of a system abruptly change in response to variations in external parameters like pressure or magnetic fields. These transitions have been widely studied over the years, employing a variety of computational methods ranging from perturbative expansions around the Fermi solid fixed point to numerical analyses based on exact diagonalization. Recently, there has been growing interest in applying the Functional Renormalization Group (FRG), originally developed for highly correlated electron systems, to this topic. In this paper, we will concentrate on the application of the FRG method to fermionic superfluids and superconductors. These states arise due to pairing correlations among fermions, resulting in macroscopic occupancy of single-nuclei states when the system is below a threshold energy, referred to as the gap. This phenomenon, known as the BCS-BEC crossover, relates the Bardeen-Cooper-Schrieffer (BCS) model of standard s-wave superconductivity to Bose-Einstein condensation (BEC) observed in k-wave superfluids. Notably, these two limits correspond to different universality classes, suggesting a smooth crossover between them.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.601397897755385,
        "rewrite-fast-z-score": 1.590990257669732
    },
    {
        "original_text": "We present statistical results on the long-term variability (LTV) properties of active galactic nuclei (AGNs). We use data obtained by the Owens Valley Radio Observatory (OVRO), University of California, Berkeley, and Metsähovi observatories in the period between 1989 and 2009 to study LTVs with time scales ranging from months up to several years for more than 100 sources. The main goal is to investigate whether there are any differences in the LTV behavior among different classes of AGNs. In addition we also aim to find out how the observed flux density variations depend on source luminosity and redshift. \n \n Keywords: Active Galactic Nuclei, Long-term Variability, OVRO, Metsähovi, High-frequency observations \n \n \n \n 1 Introduction \n \n It has been known since the early 1980s that many extragalactic radio sources show significant flux density variations over timescales as short as days or weeks (e.g., Aller et al. 1985; Quirrenbach et al. 1991; Witzel et al. 1986 ). However, it was not until the late 1990s when systematic studies were carried out using large samples of objects monitored simultaneously at multiple wavelengths (see e.g., Heeschen et al. 1987; Edelson & Krolik 1988; Hughes et al. 1992; Carini et al. 1993; Wagner et al. 1996) . These investigations revealed that most of these variable sources have flat-spectrum cores which can be identified with quasars and/or blazars. Furthermore, they showed that the majority of these sources exhibit rapid flares superimposed onto slower trends such as linear increases/decreases or exponential decays/flares. This type of behaviour is commonly referred to as  double-duty cycles  because the light curves often contain both fast flaring activity and longer term trends (Wagner et al. 1996; . \n \n Since then, numerous multi-wavelength campaigns have been conducted to monitor the flux densities of thousands of AGNs simultaneously across wide frequency ranges (from infrared through optical bands",
        "watermark_text": "We report statistical findings on the long - term variability ( LTV ) characteristics of active galactic nuclei ( AGNs ) . We use data acquired by the Owens Valley Radio Observatory ( OVRO ) , University of California , Berkeley , and Metsähovi observatories in the period between 1989 and 2009 to study LTVs with time ranges varied from months up to several decades for more than 100 sources .The main goal is to examine whether there are any variations in the LTV behavior among different kinds of AGNs . In addition we also aim to find out how the seen density density variations depend on source luminosity and redshift .Keywords : Active Galactic Nuclei , Long - term Variability , OVRO , Metsähovi , High - frequency observations 1 Introduction It has been known since the early 1980s that several extragalactic radio sources show considerable flux concentration variations over timescales as short as days or months ( e . g . , Aller et al . 1985 ; Quirrenbach et al .1991 ; Witzel et al . 1986 ) .However , it was not until the late 1990s when comprehensive experiments were carried out utilizing large specimens of items detected simultaneously at multiple wavelengths ( see e . g . , Heeschen et al . 1987 ; Edelson & Krolik 1988 ; Hughes et al .1992 ; Carini et al . 1993 ; Wagner et al .1996 ) . These studies showed that most of these variable sources have flat - spectrum cores which can be identified with quasars and / or blazars .Furthermore , they demonstrated that the majority of these sources exhibit rapid flares superimposed onto slower events such as continuous gains / decreases or exponential decays / flares . This kind of behaviour is often referred to as double - duty cycles because the light curves often contain both rapid flaring activity and longer term changes ( Wagner et al .1996 ; . Since then , various multi - wavelength campaigns have been performed to examine the flux densities of thousands of AGNs continuously across wide wavelength ranges ( from infrared through optical bands",
        "rewrite_text": "We present statistical findings regarding the long-term variability (LTV) characteristics of active galactic nuclei (AGNs). Our study employs data collected from the Owens Valley Radio Observatory (OVRO), the University of California, Berkeley, and Metsähovi observatories from 1989 to 2009. We focus on analyzing LTVs over periods ranging from months to several decades for over 100 AGNs. The primary objective is to investigate the variability in LTV behavior among different AGN categories. Additionally, we aim to understand how the observed density variations correlate with the luminosity and redshift of the sources. \n\nKeywords: Active Galactic Nuclei, Long-term Variability, OVRO, Metsähovi, High-frequency observations.\n\n1. Introduction\n\nSince the early 1980s, it has been established that several extragalactic radio sources undergo significant flux variations over timescales as brief as days or months (e.g., Aller et al. 1985; Quirrenbach et al. 1991; Witzel et al. 1986). However, it was not until the late 1990s that exhaustive experiments were conducted using large samples detected simultaneously across multiple wavelengths (see, for example, Heeschen et al. 1987; Edelson & Krolik 1988; Hughes et al. 1992; Carini et al. 1993; Wagner et al. 1996). These investigations revealed that a majority of these variable sources possess flat-spectrum cores, which are typically associated with quasars and/or blazars. Furthermore, the studies demonstrated that most sources exhibit rapid flares, combined with slower changes such as continuous increases or decreases, as well as exponential rises and falls. This behavior is often referred to as double-duty cycles, as the light curves generally display both rapid flaring activity and longer-term variations (Wagner et al. 1996). Since then, numerous multi-wavelength campaigns have been undertaken to monitor the flux densities of thousands of AGNs continuously across a broad range of wavelengths, from infrared to optical bands.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.540737725975564,
        "rewrite-fast-z-score": 1.6994116628998401
    },
    {
        "original_text": "We study the gravity dual description of 1/2 BPS Wilson loops in N = 4 super Yang-Mills theory on R × S 3 . We show that these operators are described by D-branes wrapping an S 2 inside AdS 5 and ending at the boundary of AdS 5 , where they end on a string which is stretched between two parallel D3 branes located at different points along the radial direction of AdS 5 .\nThe expectation value of this operator can be computed using standard techniques for computing open strings attached to D-branes, and we find agreement with results obtained previously via direct gauge-theory computations. \n \n The work presented here was done while the author was visiting the Institute for Advanced Study (IAS) during the Fall semester 2005 as part of the IAS-CMS program. It has been supported in part by NSF grant PHY-0456728. This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility operated under Contract DE-AC02-05CH11231.",
        "watermark_text": "We research the gravity dual description of 1 / 2 BPS Wilson loops in N = 4 super Yang - Mills theory on R × S 3 . We see that these operators are presented by D - branes wrapping an S 2 inside AdS 5 and ending at the boundary of AdS 5 , where they end on a string which is stretched between two connected D3 branes located at different places along the radial direction of AdS 5 .The expectation value of this operator can be computed use conventional methods for modeling open strings attached to D - branes , and we find agreement with conclusions derived earlier via direct gauge - theory computations . The paper given here was done while the writer was touring the Institute for Advanced Study ( IAS ) during the Fall semester 2005 as part of the IAS - CMS program .It has been supported in part by NSF grant PHY - 0456728 . This research used resources of the National Energy Research Scientific Computing Center , a DOE Office of Science User Facility operated under Contract DE - AC02 - 05CH11231 .",
        "rewrite_text": "We investigate the gravity dual representation of 1/2 BPS Wilson loops in N = 4 super Yang-Mills theory on R × S^3. Our findings indicate that these operators correspond to D-branes that wrap an S^2 within AdS_5 and terminate at the boundary of AdS_5. Here, they intersect with a string that stretches between two connected D3-branes positioned at different points along the radial direction of AdS_5. We can compute the expectation value of this operator using standard techniques for modeling open strings attached to D-branes, and we achieve results consistent with previous conclusions obtained through direct gauge-theory calculations. This research was conducted while the author was visiting the Institute for Advanced Study (IAS) during the Fall semester of 2005, as part of the IAS-CMS program. It was partially funded by NSF grant PHY-0456728. Additionally, this research utilized resources from the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility operated under Contract DE-AC02-05CH11231.",
        "ori-fast-z-score": 0.674199862463242,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "We present an ab initio tight-binding model for calculating the optical properties of semiconductor nanocrystals, which is based on the solution of the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE describes excitonic effects and allows to calculate absorption spectra with high accuracy. We show that our approach reproduces experimental results very well. In particular we find good agreement between calculated and measured absorption cross sections at low energies where quantum confinement dominates over electron-hole exchange interactions. Our method can be applied to any type of semiconductor material including doped systems as well as core-shell structures. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes or solar cells due to their unique optoelectronic properties. However, it remains challenging to predict these properties accurately since they depend sensitively on the electronic structure of the system. Here we propose a new theoretical approach to tackle this problem by combining DFT calculations with the Bethe-Salpether equation (BSE), which takes into account excitonic effects beyond mean-field approaches like Kohn-Sham DFT. This enables us to obtain accurate predictions for the optical properties of semiconductor nanostructures.",
        "watermark_text": "We present an ab initio close - binding model for determining the optical properties of semiconductor nanocrystals , which is based on the solve of the Bethe - Salpeter equation ( BSE ) within the framework of density functional theory ( DFT ) . The BSE describes excitonic effects and allows to estimate absorption spectra with high sensitivity .We see that our approach reproduces experimental results very best . In particular we find good agreement between calculated and reported absorption cross sections at low energies where quantum confinement dominates over electron - hole exchange interactions .Our method can be applied to any type of semiconductor material especially doped systems as well as core - shell systems . Semiconductor nanocrystals are promising candidates for applications such as light - emitting diodes or solar cells due to their different optoelectronic properties .However , it remains challenging to predict these characteristics properly since they rely sensitively on the electronic structure of the device . Here we attempt a new theoretical technique to tackle this question by combining DFT calculations with the Bethe - Salpether equation ( BSE ) , which gives into consideration excitonic effects beyond mean - field methods like Kohn - Sham DFT .This enables us to obtain precise predictions for the optical properties of semiconductor nanostructures .",
        "rewrite_text": "We introduce an ab initio close-binding model designed to determine the optical properties of semiconductor nanocrystals. This model is based on solving the Bethe-Salpeter equation (BSE) within the framework of density functional theory (DFT). The BSE effectively captures excitonic effects, allowing for high-sensitivity estimation of absorption spectra. Our findings indicate that our approach closely aligns with experimental results, particularly demonstrating a strong agreement between calculated and reported absorption cross sections at low energies, where quantum confinement plays a dominant role over electron-hole exchange interactions. Our method is versatile and can be applied to various semiconductor materials, including doped and core-shell systems. Semiconductor nanocrystals are promising candidates for applications such as light-emitting diodes and solar cells due to their unique optoelectronic properties. Nonetheless, accurately predicting these characteristics remains a challenge, as they are highly sensitive to the electronic structure of the system. This work presents a new theoretical approach that combines DFT calculations with the Bethe-Salpeter equation, which effectively accounts for excitonic effects that go beyond the mean-field methods inherent in Kohn-Sham DFT. This advancement allows us to achieve precise predictions for the optical properties of semiconductor nanostructures.",
        "ori-fast-z-score": 1.4596008983995234,
        "water-fast-z-score": 6.463946835769319,
        "rewrite-fast-z-score": 2.424366106925306
    },
    {
        "original_text": "The thermal Casimir force is the quantum mechanical effect that arises when two objects are separated by vacuum, which causes them to attract each other due to zero-point fluctuations in their electromagnetic fields.  In this talk I will present some recent results for the thermal Casimir force between dielectrics as well as related problems such as the van der Waals interaction between polarizable atoms or molecules at finite temperature. The first part of my talk will be devoted to an overview of our work on the subject published recently in Physical Review Letters (PRL)  1  . This includes new exact expressions for the thermal Casimir energy density and pressure valid for arbitrary temperatures and dielectric functions. These formulas can also be used to calculate the leading order corrections to Lifshitz theory  2  , which has been widely applied to describe the Casimir force between real materials  3  .\nIn the second part of my talk I will discuss how these results have been extended to include retardation effects  4  . We find that retardation leads to additional contributions to both the energy density and pressure that depend strongly on the distance between the bodies. Finally, we will show how these results can be used to study the van der Waals interactions between polarizable atoms or molecules; i.e., systems where retardation plays no role but where the dispersion forces still give rise to non-trivial behavior  5  .  For example, we will demonstrate how one can use our formalism to obtain accurate predictions for the critical point of the liquid-vapor phase transition in water  6  .",
        "watermark_text": "The thermal Casimir force is the quantum mechanical effect that arises when two bodies are apart by vacuum , which makes them to attract each other owing to zero - point fluctuations in their electromagnetic fields . In this talk I will present some latest findings for the thermal Casimir force between dielectrics as well as related problems such as the van der Waals interaction between polarizable atoms or compounds at finite temperature .The first part of my talk will be devoted to an overview of our work on the subject released lately in Physical Review Letters ( PRL ) 1 . This contains new accurate expressions for the thermal Casimir energy density and tension applicable for arbitrary pressures and dielectric functions .These formulas can also be used to estimate the main order corrections to Lifshitz principle 2 , which has been widely applied to explain the Casimir force between real substances 3 . In the second part of my talk I will explore how these results have been extended to include retardation properties 4 .We see that retardation contributes to extra contributions to both the power concentration and tension that rely highly on the distance between the bodies . Finally , we will show how these results can be used to study the van der Waals relationships between polarizable atoms or compounds ; i . e . , systems where retardation plays no part but where the dispersion forces nevertheless give rise to non - simple properties 5 .For instance , we will prove how one can using our formalism to obtain precise predictions for the important moment of the liquid - fluid phase shift in water 6 .",
        "rewrite_text": "The thermal Casimir force is a quantum mechanical phenomenon that occurs when two bodies are separated by a vacuum, causing them to attract each other due to zero-point fluctuations in their electromagnetic fields. In this presentation, I will share some recent findings regarding the thermal Casimir force between dielectric materials, along with associated topics such as the van der Waals interaction between polarizable atoms or compounds at finite temperatures. The first segment of my talk will provide an overview of our recent work published in Physical Review Letters (PRL), which presents new accurate expressions for the thermal Casimir energy density and pressure that are applicable under varying pressures and dielectric functions. These formulas can also help estimate significant corrections to the Lifshitz principle, which has been extensively used to describe the Casimir force between real materials. In the second part, I will discuss how these findings have been adapted to incorporate retardation effects. We find that retardation introduces additional contributions to both energy density and pressure, which are highly dependent on the separation distance between the bodies. Lastly, I will demonstrate how these results can be utilized to investigate van der Waals interactions between polarizable atoms or compounds—systems where retardation is negligible, yet dispersion forces still result in complex behaviors. For example, I will show how our theoretical approach can yield precise predictions for the critical moment of the liquid-fluid phase transition in water.",
        "ori-fast-z-score": 0.4622501635210242,
        "water-fast-z-score": 7.799204203436179,
        "rewrite-fast-z-score": 0.658504607868518
    },
    {
        "original_text": "We present exact solutions for the electromagnetic field in the presence of spherical particles with arbitrary dielectric functions, including both metals and insulators. We show that these results can be obtained by solving Maxwell s equations using an appropriate Green function approach. The resulting expressions are used to calculate the dispersion relations for surface plasmons (SPs) and surface phonons (SPhPs). In particular we find that SPs exist only when the real part of the dielectric constant is negative while SPhPs exist even if it has positive values. Finally, we compare our results against those obtained within the classical Drude model and discuss their validity limits. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied over many decades  1  . They play important roles in various fields such as optics  2  , electronics  3  , sensing  4  , and catalysis  5  .\nRecently there has also been growing interest in studying surface phonon-polaritons (SPhPs), which are analogous excitations associated with longitudinal acoustic waves  6  . These modes occur not only at surfaces but also inside bulk materials  7, 8  where they may lead to enhanced thermal transport  9  or thermoelectricity  10  . Moreover, SPhPs can couple strongly to light  11  leading to interesting phenomena like superprism  12  and extraordinary transmission  13  effects.",
        "watermark_text": "We present precise solutions for the electromagnetic field in the presence of spherical objects with arbitrary dielectric functions , including both metals and insulators . We see that these results can be obtained by treating Maxwell s equations using an appropriate Green function method .The resulting expressions are using to estimate the dispersion relations for surface plasmons ( SPs ) and surface phonons ( SPhPs ) . In particular we find that SPs occur only when the real part of the dielectric constant is zero while SPhPs exist even if it has negative values .Finally , we compare our findings against those achieved within the classical Drude theory and consider their efficacy limits . Surface plasmons ( SPs ) , which are collective oscillations of conduction electrons at metal - dielectric connections , have been heavily explored over numerous years 1 .They play essential roles in different fields such as optics 2 , electronics 3 , sensing 4 , and catalysis 5 . Recently there has especially been growing interest in investigating surface phonon - polaritons ( SPhPs ) , which are analogous excitations associated with longitudinal acoustic waves 6 .These modes happen not only at surfaces but also inside bulk surfaces 7 , 8 where they may contribute to heightened thermal transport 9 or thermoelectricity 10 . Moreover , SPhPs can close intensely to light 11 contributing to curious phenomena like superprism 12 and exceptional transmission 13 phenomena .",
        "rewrite_text": "We provide accurate solutions for the electromagnetic field around spherical objects with arbitrary dielectric properties, encompassing both metals and insulators. These results are derived from applying an appropriate Green's function method to Maxwell's equations. The resulting formulas are used to analyze the dispersion relations for surface plasmons (SPs) and surface phonon polaritons (SPhPs). Notably, we discover that SPs emerge only when the real part of the dielectric constant is zero, while SPhPs can exist even with negative values. We also compare our results with those from classical Drude theory and discuss their limitations. Surface plasmons (SPs), which are collective oscillations of conduction electrons at metal-dielectric interfaces, have been extensively studied for many years. They are crucial in various domains, including optics, electronics, sensing, and catalysis. Recently, there has been an increasing interest in surface phonon polaritons (SPhPs), which represent analogous excitations linked to longitudinal acoustic waves. These modes can occur not just at surfaces but also within bulk materials, potentially enhancing thermal transport or thermoelectric efficiency. Furthermore, SPhPs can couple strongly to light, leading to intriguing phenomena such as the superprism effect and exceptional transmission.",
        "ori-fast-z-score": -0.8164965809277261,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": 0.32539568672798425
    },
    {
        "original_text": "Cirrus clouds are an important component in Earth s radiation budget, but their radiative properties remain poorly understood because they occur at high altitude where direct measurements are difficult to make. In this study we use data collected by the Scripps Institution of Oceanography (SIO) lidar system located on the SIRTA observatory near Paris during  2002   2003   2004   2005   2006  . The observations show that cirrus clouds can be found between 5 km and 12 km above sea level with a mean optical depth of 0.2 ± 0.1. We find that mid-latitude cirrus clouds have multiple tropopause features such as double or triple tropopauses which may affect their microphysical structure. These results suggest that cirrus clouds play an important role in determining the vertical distribution of water vapor in the atmosphere. Citation: \nIntroduction\n\nCirrus clouds cover about 10%-20% of the globe s surface area  Sassen et al., 2008  , yet little is known about how these clouds form and evolve. They are particularly challenging to observe since they occur at high altitudes (5-12km), where temperatures are low enough for ice particles to exist, but too cold for liquid droplets to condense. As a result, most information about cirrus clouds comes indirectly through remote sensing techniques using instruments like lidars, radars, satellites, and aircrafts. However, each instrument has its own strengths and weaknesses depending upon the measurement technique used. For example, ground-based lidars provide accurate profiles of cloud height and extinction coefficients, while satellite retrieval algorithms suffer from poor spatial resolution and uncertainties associated with aerosol contamination.",
        "watermark_text": "Cirrus winds are an important element in Earth s radiation budget , but their radiative characteristics remain little explained because they occur at high elevation where direct measurements are hard to make . In this study we using data taken by the Scripps Institution of Oceanography ( SIO ) lidar scheme located on the SIRTA observatory near Paris during 2002 2003 2004 2005 2006 .The surveys reveal that cirrus skies can be found between 5 kilometres and 12 kilometres above water level with a mean laser thickness of 0 . 2 ± 0 . 1 . We see that mid - latitude cirrus skies have multiple tropopause features such as double or triple tropopauses which may affect their microphysical structure .These data suggest that cirrus winds play an important role in measuring the vertical flow of water vapor in the air . Citation : Introduction Cirrus skies occupy about 10 % - 20 % of the globe s surface area Sassen et al . , 2008 , however less is known about how these clouds form and evolve .They are particularly challenging to observe since they occur at high altitudes ( 5 - 12km ) , where temperatures are low enough for glacier ions to form , but too cold for solid droplets to condense . As a result , most information about cirrus winds lies indirectly through remote sensing techniques using instruments like lidars , radars , satellites , and aircrafts .However , each system has its own qualities and flaws depending upon the monitoring methodology used . For instance , land - based lidars provide accurate profiles of mist height and extinction values , while satellite retrieval algorithms suffer from good temporal precision and uncertainties involved with aerosol contamination .",
        "rewrite_text": "Cirrus winds are a crucial component of Earth's radiation budget, yet their radiative properties are not well understood due to their high-altitude occurrence, which makes direct measurement challenging. This study utilizes data gathered by the Scripps Institution of Oceanography (SIO) lidar system situated at the SIRTA observatory near Paris during the years 2002 to 2006. The findings indicate that cirrus clouds are located between 5 and 12 kilometers above sea level, with an average thickness of 0.2 ± 0.1. Our observations show that mid-latitude cirrus skies exhibit multiple tropopause characteristics, such as double or triple tropopauses, which may influence their microphysical composition. This data implies that cirrus winds significantly contribute to assessing the vertical transport of water vapor in the atmosphere. \n\nIntroduction: Cirrus clouds cover approximately 10% to 20% of the Earth's surface (Sassen et al., 2008), yet the processes behind their formation and development remain largely unclear. Their observation is particularly difficult due to their high altitude (5 to 12 km), where temperatures are low enough for ice crystals to form but too cold for liquid water droplets to condense. Consequently, most information regarding cirrus winds is obtained indirectly through remote sensing technologies, including lidars, radars, satellites, and aircraft. Each of these systems has its advantages and limitations based on the methodology employed. For example, land-based lidars offer precise profiles of cloud heights and extinction values, whereas satellite retrieval algorithms may struggle with temporal accuracy and issues related to aerosol interference.",
        "ori-fast-z-score": -0.3651483716701107,
        "water-fast-z-score": 8.272727272727273,
        "rewrite-fast-z-score": 0.9918365981341755
    },
    {
        "original_text": "We consider the problem of learning an unknown distribution over binary strings, where each string is generated by sampling independently at random with replacement according to some fixed probability vector . We show that if we are given samples drawn iid from , then it can be learned in polynomial time using standard statistical techniques (e.g., maximum likelihood). However, when the number of possible values for each bit grows large, this approach fails because there may not exist any sample which contains all possible values for every bit. In such cases, we propose a novel algorithm based on Gibbs sampling and prove its correctness under certain conditions. Finally, we present experimental results demonstrating our method s effectiveness. The main contribution of this work lies in showing how to learn distributions over combinatorial objects like graphs and trees. This is achieved by first reducing these problems into learning distributions over binary strings, and then applying our proposed algorithms. Learning distributions over combinatorial objects has many applications including data mining, computer vision, bioinformatics, etc. .",
        "watermark_text": "We consider the question of knowing an unknown distribution over binary strings , where each string is generated by filtering independently at random with replacement due to some fixed probability vector . We see that if we are given samples picked iid from , then it can be learned in polynomial period using conventional statistical tools ( e . g . , maximum likelihood ) .However , when the quantity of possible values for each bit grows large , this methodology fails because there may not exist any specimen which contains all possible values for every bit . In such cases , we undertake a new algorithm based on Gibbs filtering and find its correctness under certain conditions .Finally , we present research results proving our technique s efficacy . The main impact of this research consists in teaching how to teach distributions over combinatorial objects like graphs and trees .This is achieved by first transforming these problems into knowledge distributions over binary strings , and then applying our proposed methods . Learning distributions over combinatorial objects has numerous uses including data extraction , computer vision , bioinformatics , etc ..",
        "rewrite_text": "We explore the problem of understanding an unknown distribution over binary strings, where each string is independently generated by random filtering with replacement according to a fixed probability vector. Our findings indicate that if we have access to independent and identically distributed samples, we can learn the distribution in polynomial time using traditional statistical techniques, such as maximum likelihood estimation. However, this approach encounters difficulties when the number of potential values for each binary digit is large, as it may be impossible to obtain samples that encompass all possible values for each bit. In these scenarios, we introduce a new algorithm based on Gibbs filtering and demonstrate its validity under specific conditions. Furthermore, we present research findings that validate the effectiveness of our method. The primary contribution of this research lies in its application to teaching distributions over combinatorial structures such as graphs and trees. This is accomplished by first converting these problems into knowledge distributions over binary strings, followed by the application of our proposed techniques. The ability to learn distributions over combinatorial objects has various practical applications, including data extraction, computer vision, and bioinformatics.",
        "ori-fast-z-score": 0.10369516947304253,
        "water-fast-z-score": 6.947576354693849,
        "rewrite-fast-z-score": 0.20203050891044214
    },
    {
        "original_text": "We consider the problem of scheduling jobs on parallel machines with probabilistic anonymity constraints, where each job has to be scheduled in such a way that its execution time is hidden among other jobs. We show how this problem can be solved by using admissible schedulers for the underlying deterministic problem. In particular we present two algorithms based on list-scheduling and randomization respectively. The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance guarantee of 1 + . Both results are tight up to constant factors. Finally, we provide some experimental evidence showing that our randomized algorithm performs well also in practice. Probabilistic anonymity arises naturally when considering privacy issues related to data mining or computational social choice problems. For example, suppose that there exists a set of n users who have to submit their preferences over m alternatives (e.g., candidates) before a certain deadline T. Each user i submits her preference profile Pi = {(ai1 , bi1 ), . . . , (aim , bim )}, where aij denotes the utility she derives from alternative j and bij indicates whether she likes it or not. Then, if all profiles are submitted simultaneously at time t, they should be processed so as to hide which user prefers what alternative. This means that no information about individual preferences should be revealed until after the deadline T.",
        "watermark_text": "We consider the issue of scheduling jobs on parallel machines with probabilistic anonymity limitations , where each work has to be scheduled in such a way that its execution time is hidden among other tasks . We see how this question can be answered by using admissible schedulers for the underlying deterministic problem .In particular we present two strategies using on list - planning and randomization respectively . The first algorithm achieves a competitive ratio of 2 while the second one obtains a better performance assurance of 1 + .Both results are tight up to constant factors . Finally , we provide some experimental evidence showing that our randomized algorithm performs good also in practice .Probabilistic anonymity arises readily when examining privacy issues related to data extraction or computational social selection difficulties . For instance , suppose that there exists a group of n users who have to submit their preferences over m choices ( e . g . , candidates ) before a certain deadline T . Each customer i submits her preference profile Pi = { ( ai1 , bi1 ) , ..., ( aim , bim ) } , where aij represents the utility she generates from alternative j and bij represents whether she likes it or not . Then , if all profiles are presented simultaneously at time t , they should be processed so as to hide which user prefers what alternative .This implies that no information about individual preferences should be revealed until after the date T .",
        "rewrite_text": "We address the challenge of scheduling jobs on parallel machines while adhering to probabilistic anonymity constraints, ensuring that each job's execution time remains concealed among other tasks. We explore how this problem can be tackled by employing admissible schedulers for the corresponding deterministic scenario. Specifically, we introduce two approaches: one based on list planning and the other on randomization. The first algorithm achieves a competitive ratio of 2, while the second offers an improved performance guarantee of 1+. Both results are tight within constant factors. Additionally, we present experimental evidence demonstrating that our randomized algorithm performs well in practice. Probabilistic anonymity frequently emerges in discussions of privacy concerning data collection and computational social selection challenges. For example, consider a group of n users who need to submit their preferences regarding m options (such as candidates) before a deadline T. Each user i provides her preference profile Pi = { (ai1, bi1), ..., (aim, bim) }, where aij indicates the utility she derives from option j, and bij indicates her preference for it. If all profiles are submitted simultaneously at time t, they must be processed in a manner that obscures which user favors which option, ensuring that no individual preferences are revealed until after the deadline T.",
        "ori-fast-z-score": 1.7822655773580138,
        "water-fast-z-score": 7.462778926574919,
        "rewrite-fast-z-score": 1.0206207261596576
    },
    {
        "original_text": "We present an alternative description of the electron in terms of its position and velocity, which is based on the idea that it moves along a helical trajectory around the nucleus. The new approach leads to a simple analytical expression for the energy levels of the helium atom as well as for the wave functions corresponding to these states. We show how this model can be used to explain some experimental results obtained by high-resolution spectroscopy experiments performed at Jefferson Lab. In addition we discuss possible extensions of our work towards other atomic systems such as muonic atoms or ions with one valence electron. Helium has been studied extensively over many decades both experimentally and theoretically. It was found that there are two stable isotopes (3He and 4He) and several excited states. These states have been investigated using various spectroscopic techniques including photo-absorption  1  , laser excitation  2  , and Compton scattering  3  . However, despite all efforts made so far, no satisfactory explanation exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound  4  .\nIn order to understand better the structure of helium, we propose here a new theoretical framework where the electron is described not only by its usual position but also by its velocity vector. This new approach allows us to obtain analytically the energy spectrum of helium as well as the associated wavefunctions. Our formalism is inspired by the so-called Bohmian mechanics  5  , which describes particles moving along trajectories instead of following classical equations of motions  6  .",
        "watermark_text": "We introduce an different characterization of the electron in terms of its position and speed , which is based on the idea that it travels along a helical trajectory around the nucleus . The modern perspective results to a simple analytical expression for the power concentrations of the helium atom as well as for the wave functions associated to these states .We see how this description can be used to explain some experimental results acquired by high - resolution spectroscopy investigations undertaken at Jefferson Lab . In addition we explain possible extend of our work towards other nuclear systems such as muonic atoms or ions with one valence electron .Helium has been studied frequently over numerous years both experimentally and theoretically . It was shown that there are two stable isotopes ( 3He and 4He ) and many excited states .These states have been investigated using numerous spectroscopic techniques including photo - absorption 1 , laser excitation 2 , and Compton absorption 3 . However , despite all efforts made so far , no satisfactory excuse exists yet about why the ground state of 3He is unbound while the ground state of 4He is bound 4 .In order to explain better the composition of helium , we undertake here a new theoretical framework where the electron is characterized not only by its traditional position but also by its velocity tensor . This new approach allows us to obtain analytically the power spectrum of helium as well as the associated wavefunctions .Our formalism is influenced by the so - called Bohmian theory 5 , which explains particles moving along trajectories rather of following classical equations of motions 6 .",
        "rewrite_text": "We present a novel characterization of the electron based on its position and velocity, conceptualizing its movement along a helical path around the nucleus. This contemporary viewpoint yields a straightforward analytical expression for the power distributions in the helium atom, as well as the wave functions corresponding to these states. We demonstrate how this framework can account for certain experimental findings from high-resolution spectroscopy studies conducted at Jefferson Lab. Furthermore, we discuss potential avenues for extending our work to other nuclear systems, such as muonic atoms and ions with a single valence electron. Helium has been extensively explored both experimentally and theoretically over many years, revealing two stable isotopes, 3He and 4He, along with numerous excited states. These states have been examined using various spectroscopic methods, including photoabsorption, laser excitation, and Compton absorption. Nonetheless, despite extensive research, there remains no satisfactory explanation for why the ground state of 3He is unbound while that of 4He is bound. To enhance our understanding of helium’s structure, we propose a new theoretical framework where the electron is characterized not only by its conventional position but also by its velocity tensor. This innovative approach enables us to analytically derive the power spectrum of helium and the associated wave functions. Our formalism is inspired by Bohmian theory, which depicts particles as moving along specific trajectories rather than adhering to classical equations of motion.",
        "ori-fast-z-score": -0.27975144247209416,
        "water-fast-z-score": 7.243550686553699,
        "rewrite-fast-z-score": -0.3922322702763681
    },
    {
        "original_text": "We present results on infrared sources selected by their flux densities at 11 microns (S11) using early data taken with the InfraRed Camera (IRC) on-board AKARI, which is an infrared space telescope launched into orbit in February 2006. The survey covers about 1 deg2 area centered around the north ecliptic pole and reaches to S/N = 5 limit for point source detection. We have identified more than 1000 infrared sources down to S11 ~ 0.1 Jy over the entire field-of-view. Among them we found that most are associated with galaxies or galaxy clusters. About 20% of these objects show red colors indicative of dust-obscured star formation activity. A large fraction of the remaining 80% shows blue colors indicating active galactic nuclei and/or young stellar populations. These results suggest that our sample contains various types of infrared luminous objects including normal galaxies, interacting/merging systems, obscured AGNs as well as distant quasars.",
        "watermark_text": "We report findings on infrared sources chosen by their flux densities at 11 microns ( S11 ) using early data taken with the InfraRed Camera ( IRC ) on - board AKARI , which is an infrared space telescope launched into orbit in February 2006 . The survey encompasses about 1 deg2 region focused around the north ecliptic pole and reaches to S / N = 5 limit for point source discovery .We have discovered more than 1000 infrared sources down to S11 ~ 0 . 1 Jy over the entire field - of - view . Among them we identified that most are related with galaxies or galaxy regions .About 20 % of these objects show red colors indicative of dust - obscured star formation activity . A large fraction of the remaining 80 % indicates blue colors representing active galactic nuclei and / or young stellar groups .These data suggest that our sample comprises numerous types of infrared luminous objects including typical clusters , interacting / merging systems , obscured AGNs as well as distant quasars .",
        "rewrite_text": "We present findings on infrared sources selected based on their flux densities at 11 microns (S11), utilizing early observations from the InfraRed Camera (IRC) aboard the AKARI infrared space telescope, which was launched in February 2006. The survey covers approximately 1 square degree around the north ecliptic pole and achieves a signal-to-noise ratio (S/N) limit of 5 for detecting point sources. We have identified over 1,000 infrared sources down to S11 ~ 0.1 Jy across the entire field of view. Most of these sources are associated with galaxies or galactic regions. Notably, around 20% exhibit red colors indicative of dust-obscured star formation activity, while a significant portion of the remaining 80% displays blue colors, suggesting the presence of active galactic nuclei and/or young stellar groups. These observations imply that our sample encompasses a variety of infrared-luminous objects, including typical clusters, interacting or merging systems, obscured AGNs, and distant quasars.",
        "ori-fast-z-score": 0.6882472016116852,
        "water-fast-z-score": 4.817730411281796,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We study entanglement properties of the Toric Code model in two dimensions, which is defined on a square lattice with periodic boundary conditions. We consider both ground state and thermal states for this system. In particular we calculate the von Neumann entropy S(A) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I(A; B) between any pair of disjoint regions A and B. The results are compared against numerical simulations performed by means of Monte Carlo techniques. For the ground state it turns out that there exists an area law for the von Neumann entropy, i.e., S(A) ∝ L−d−1 where d denotes the dimension of region A and L its linear size. Moreover, we find that the mutual information decays exponentially fast when one moves away from the diagonal line joining the centers of the regions A and B. These findings agree very well with those obtained using exact methods based on Matrix Product States (MPS). Finally, we also show how these results can be used to obtain bounds on the topological entropy of the Toric Code.",
        "watermark_text": "We explore entanglement properties of the Toric Code model in two dimensions , which is characterized on a square lattice with periodic border conditions . We consider both ground state and thermal states for this process .In particular we estimate the von Neumann entropy S ( A ) = −TrρA ln ρA associated to different regions A of the lattice as well as the mutual information I ( A ; B ) between any pair of disjoint regions A and B . The results are compared against quantitative simulations conducted by means of Monte Carlo methods .For the ground state it turns out that there exists an area law for the von Neumann entropy , i . e . , S ( A ) [UNK] L−d−1 where d indicates the dimension of zone A and L its linear size . Moreover , we find that the mutual intelligence decays exponentially rapidly when one moves away from the diagonal line joining the centers of the regions A and B .These conclusions follow very best with those acquired using accurate methods based on Matrix Product States ( MPS ) . Finally , we also demonstrate how these results can be used to obtain limits on the topological entropy of the Toric Code .",
        "rewrite_text": "We investigate the entanglement properties of the two-dimensional Toric Code model, which is set on a square lattice with periodic boundary conditions. Our analysis includes both ground states and thermal states. Specifically, we compute the von Neumann entropy \\( S(A) = -\\text{Tr}(\\rho_A \\ln \\rho_A) \\) for various regions \\( A \\) of the lattice, as well as the mutual information \\( I(A; B) \\) between pairs of disjoint regions \\( A \\) and \\( B \\). We compare our findings with quantitative simulations performed using Monte Carlo methods. For the ground state, we find that the von Neumann entropy satisfies an area law: \\( S(A) \\propto L^{d-1} \\), where \\( d \\) is the dimension of region \\( A \\) and \\( L \\) is its linear size. Additionally, we observe that the mutual information decays exponentially as one moves away from the diagonal connecting the centers of regions \\( A \\) and \\( B \\). These results are consistent with those obtained using precise methods based on Matrix Product States (MPS). Finally, we illustrate how our findings can be leveraged to place bounds on the topological entropy of the Toric Code.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 5.114896104728048,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "We present an analysis of the connection between different galaxy types using data from the Sloan Digital Sky Survey (SDSS). We use two methods to classify galaxies into four types: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL) and early-type galaxies without emission lines (ETGSIL).\nThe first method is based on the principal component analysis (PCA) applied to the optical spectra of all galaxies classified as spectroscopic targets by the SDSS pipeline. The second one uses the PCA applied only to the subset of galaxies that are morphologically selected for having bulges dominated by old stellar populations. In both cases we find that ETGs form a continuous sequence in terms of their spectral properties along which SFGs evolve towards ETGSILs through ETGELs. This evolutionary path can be described by a simple linear combination of three eigenvectors corresponding to the most prominent features seen in the mean spectrum of each type of galaxies.",
        "watermark_text": "We present an assessment of the link between various galaxy kinds use data from the Sloan Digital Sky Survey ( SDSS ) . We use two means to classify objects into four types : star - creating galaxies ( SFG ) , active galactic nuclei guest galaxies ( AGNHG ) , early - class galaxies with emitted lines ( ETGEL ) and early - class galaxies without absorption lines ( ETGSIL ) .The first method is based on the main component analysis ( PCA ) applied to the optical spectra of all galaxies designated as spectroscopic targets by the SDSS pipeline . The second one uses the PCA applicable only to the subset of stars that are morphologically selected for having bulges dominated by ancient stars populations .In both cases we find that ETGs form a continuous progression in terms of their spectral properties along which SFGs grow towards ETGSILs through ETGELs . This evolutionary progression can be described by a simple linear mixture of three eigenvectors corresponding to the most notable features found in the mean spectrum of each type of galaxies .",
        "rewrite_text": "We provide an evaluation of the relationship between different types of galaxies using data from the Sloan Digital Sky Survey (SDSS). Our classification approach divides galaxies into four categories: star-forming galaxies (SFG), active galactic nuclei host galaxies (AGNHG), early-type galaxies with emission lines (ETGEL), and early-type galaxies without absorption lines (ETGSIL). The first method utilizes principal component analysis (PCA) on the optical spectra of all galaxies identified as spectroscopic targets by the SDSS pipeline. The second method applies PCA exclusively to a subset of stars with bulges that are predominantly composed of ancient star populations. Our results indicate that early-type galaxies (ETGs) demonstrate a continuous progression in their spectral characteristics, with star-forming galaxies evolving into ETGSILs through ETGELs. This evolutionary process can be modeled as a simple linear combination of three eigenvectors that represent the most significant features observed in the average spectrum of each galaxy type.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 0.22360679774997896
    },
    {
        "original_text": "We present new observations made with the Cosmosoma experiment, which were designed to search for evidence of an excess in cosmic microwave background (CMB) temperature fluctuations above those predicted by standard cosmological models. The data are consistent with predictions based on current theoretical understanding but show some unexpected features that may be related to previously unidentified foreground sources or systematic effects associated with our analysis techniques. \n \n We have used these results to place limits on possible contributions from primordial gravitational waves and other exotic phenomena such as topological defects. These limits are comparable to previous measurements obtained using different experimental approaches. In addition we report the detection of a significant signal at frequencies below 10GHz, which is not expected within conventional cosmological models. This could represent either a new source of foreground contamination or a novel physical effect. Further investigation will require additional experiments to confirm this result and determine its origin. If confirmed it would provide important constraints on theories attempting to explain the observed anisotropy in the CMB spectrum.",
        "watermark_text": "We present new experiments done with the Cosmosoma study , which were built to search for indication of an amount in cosmic microwave background ( CMB ) temperature fluctuations above those predicted by traditional cosmological predictions . The data are compatible with predictions based on current theoretical knowledge but display some surprising characteristics that might be connected to formerly unidentified foreground sources or systematic effects involved with our analysis methods .We have utilized these results to place limits on potential contributions from primordial magnetic waves and other exotic processes such as topological defects . These restrictions are comparable to previous measurements obtained using separate observation approaches .In addition we report the finding of a substantial frequency at speeds below 10GHz , which is not anticipated within conventional cosmological predictions . This might represent either a new cause of foreground contamination or a novel physical impact .Further investigation will demand additional studies to confirm this result and establish its identity . If confirmed it would offer important restrictions on experiments pursuing to explain the observed anisotropy in the CMB spectrum .",
        "rewrite_text": "We present new experiments conducted as part of the Cosmosoma study, designed to investigate whether there are indications of temperature fluctuations in the cosmic microwave background (CMB) that exceed those predicted by conventional cosmological models. While the data aligns with current theoretical predictions, it also reveals some unexpected features that may be linked to unidentified foreground sources or systematic effects related to our analytical methods. We have used these findings to impose limits on potential contributions from primordial magnetic waves and other exotic phenomena, such as topological defects. These constraints are comparable to previous measurements obtained through different observational techniques. Additionally, we highlight the discovery of a significant frequency below 10 GHz, which is not anticipated by standard cosmological theories. This observation may indicate a new source of foreground contamination or suggest a novel physical effect. Further research will be needed to confirm this finding and understand its implications, as its verification could provide important constraints for experiments aiming to explain the observed anisotropy in the CMB spectrum.",
        "ori-fast-z-score": -1.0540925533894598,
        "water-fast-z-score": 7.233165373381237,
        "rewrite-fast-z-score": 1.1043152607484654
    },
    {
        "original_text": "We present the first results on the soft X-ray emission in two nearby elliptical galaxies, NGC 2992 (M60) and NGC 3081 (M84). The observations were made with the Chandra X-Ray Observatory using the Advanced CCD Imaging Spectrometer (ACIS-S3), which has an energy resolution of about 130 eV at 6 keV. We find that both galaxies show extended diffuse emission around their central regions. In addition, we detect several point sources within each galaxy s field-of-view. For these point sources, we have extracted spectra for individual source components as well as combined them into one spectrum per galaxy. Using spectral fitting techniques, we found that all but three of the detected point sources are consistent with being background AGNs or foreground stars. However, there is evidence that some of the brightest point sources may be associated with the host galaxies themselves. Finally, we also fit the diffuse component of the X-ray emission with thermal plasma models.",
        "watermark_text": "We present the first findings on the soft X - ray radiation in two nearby elliptical galaxies , NGC 2992 ( M60 ) and NGC 3081 ( M84 ) . The findings were made with the Chandra X - Ray Observatory using the Advanced CCD Imaging Spectrometer ( ACIS - S3 ) , which has an energy resolution of about 130 eV at 6 keV .We see that both nuclei display extended diffuse emission around their central regions . In addition , we locate many point sources within each galaxy s field - of - view .For these point sources , we have gathered spectra for individual source components as well as blended them into one spectrum per galaxy . Using spectral fitting methods , we concluded that all but three of the recovered point sources are consistent with being background AGNs or foreground stars .However , there is evidence that some of the brightest point sources might be identified with the host galaxies themselves . Finally , we also fitted the diffuse component of the X - ray radiation with thermal plasma estimates .",
        "rewrite_text": "We present initial findings on soft X-ray emissions from two nearby elliptical galaxies, NGC 2992 (M60) and NGC 3081 (M84). These observations were conducted using the Chandra X-Ray Observatory, specifically the Advanced CCD Imaging Spectrometer (ACIS-S3), which has an energy resolution of approximately 130 eV at 6 keV. Our analysis reveals that both galaxies exhibit extended diffuse emissions in their central regions. Additionally, we identified numerous point sources within each galaxy's field of view. For these point sources, we collected spectra for individual components and also combined them into a single spectrum for each galaxy. Through spectral fitting methods, we found that nearly all recovered point sources align with background active galactic nuclei (AGNs) or foreground stars, with only three exceptions. Notably, there are indications that some of the brightest point sources may be associated with the host galaxies themselves. Lastly, we fitted the diffuse component of the X-ray emissions with estimates of thermal plasma.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 3.3489378339086286,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "We consider the problem of successive refinement coding for layered broadcast systems, where each receiver is interested only in one out of several layers and has access to all previous layers. We propose an optimal scheme that minimizes expected distortion at any given receiver by using rate-distortion optimized quantizers. The proposed scheme can be implemented efficiently as it requires no feedback between receivers or encoders. Our results are illustrated through numerical examples. Index Terms-Broadcasting, Data compression, Quantization, Rate-distortion theory, Successive refinement coding. I. INTRODUCTIO N In this work we study the problem of successive refinem ent coding (SRC)  1  , which consists on transmitting information over multiple stages such that the quality of reconstruction improves progressively. SRC is used extensively in video broadcasting applications  2  -  4  . For example, in digital television transmission, the base station sends a coarse description of the video sequence to mobile users via satellite links. Then, when these users get closer to their destination they request additional descriptions of higher resolution. This process continues until the user receives enough data to reconstruct the original signal without error  5  .\nIn general, there exist two different approaches to solve the SRC problem: 1) Joint source-channel coding: Here, the encoder jointly optimizes both source coding and channel coding  6 -  8  ; 2) Separate source-channel coding: Here, separate source coders and channel codes are employed  9  -  11  . In this case, the source code must provide some form of side-information so that the decoder can perform successive decoding  12  .",
        "watermark_text": "We consider the issue of successive refinement compression for layered broadcast systems , where each receiver is interested only in one out of several layers and has access to all previous layers . We suggest an appropriate plan that minimizes expected distortion at any certain receiver by using frequency - noise optimized quantizers .The proposed system can be executed easily as it requires no feedback between receivers or encoders . Our results are shown through numerical examples .Index Terms - Broadcasting , Data coding , Quantization , Rate - noise theory , Successive refinement compression . I . INTRODUCTIO N In this research we study the question of successive refinem ent codes ( SRC ) 1 , which consists on distributing data over successive phases such that the quality of recovery improves progressively .SRC is utilized frequently in digital broadcasting environments 2 - 4 . For instance , in digital television broadcast , the base station provides a coarse summary of the television scene to mobile users via satellite connections .Then , when these users draw closer to their target they seek alternative descriptions of greater resolution . This process proceeds until the user receives enough data to reconstruct the previous signal without error 5 .In general , there follow two different methods to solve the SRC problem : 1 ) Joint source - channel code : Here , the encoder jointly optimizes both source coding and channel code 6 - 8 ; 2 ) Separate source - channel code : Here , distinct source coders and channel codes are applied 9 - 11 . In this instance , the source coding need provide some kind of side - information so that the decoder can conduct consecutive decoding 12 .",
        "rewrite_text": "We address the challenge of successive refinement compression in layered broadcast systems, where each receiver is focused on a single layer while having access to all preceding layers. Our proposed strategy aims to minimize the expected distortion at each receiver by utilizing frequency-noise optimized quantizers. Notably, this system is straightforward to implement as it does not require feedback between the receivers or the encoders. We demonstrate our findings through numerical examples. \n\n**Index Terms**: Broadcasting, Data coding, Quantization, Rate-noise theory, Successive refinement compression.\n\n**I. INTRODUCTION** \n\nThis research investigates successive refinement codes (SRC), which involve distributing data across multiple phases to progressively enhance recovery quality. SRC is commonly employed in digital broadcasting environments. For example, in digital television broadcasting, the base station transmits a coarse summary of the television content to mobile users via satellite. As these users approach their destination, they seek higher-resolution descriptions of the content. This process continues until the user has sufficient data to accurately reconstruct the original signal. \n\nGenerally, there are two main approaches to tackle the SRC problem: 1) **Joint source-channel coding**, which involves the encoder optimizing both source coding and channel coding together; and 2) **Separate source-channel coding**, where distinct source coders and channel codes are utilized. In this latter method, the source coding must provide some side information to enable the decoder to perform successive decoding.",
        "ori-fast-z-score": -2.057182539299806,
        "water-fast-z-score": 6.770618128448084,
        "rewrite-fast-z-score": 0.3682298471593294
    },
    {
        "original_text": "We report on the observation of strong electron-phonon interaction (EPI) in heavily boron-doped epitaxial graphene grown on SiC(0001). The EPI is found to be enhanced by more than one order of magnitude compared with that observed for undoped graphene, which can be attributed to the formation of a Dirac point at high doping levels and its associated van Hove singularity near the Fermi level. This finding suggests that the EPI in doped graphene may be tunable over several orders of magnitude through chemical or electrostatic gating. \n \n Graphene has attracted considerable attention because it exhibits unique physical properties such as extremely high carrier mobility1-5 and very large optical nonlinearities6-8. However, these remarkable properties are often accompanied by weak interactions between electrons and phonons9-11, which limit their applications in high-speed electronics12-14 and optoelectronics15-17. In this work we demonstrate that the EPI in heavily boron-doping epitaxial graphene grown by thermal decomposition of SiC18-20 can be significantly enhanced due to the presence of a Dirac point21-23 and its associated van Hov singularity24-26 near the Fermi energy EF. We show that the EPI increases rapidly when the Fermi level crosses the van Hove singularity, resulting in a giant increase in the electron-phonon scattering rate. Our results suggest that the EPI in graphene could be controlled electrically via chemical or electrostatic gated27-30, thereby opening up new avenues towards novel devices based on graphene. \nGraphene is known to have extremely high carrier mobilities1-4 but relatively small electron-phonon couplings5-9. These two competing effects determine the performance of graphene-based electronic and optoelectronic devices10-12. For example, the low EPI leads to slow relaxation rates13-15 and thus limits the operation speed of graphene transistors14-16. On the other hand, the high mobility makes graphene attractive for use in high-speed electronics17-19 and ultrafast photodetectors20. Therefore, there exists great interest in developing methods to enhance the EPI while maintaining the high mobility31",
        "watermark_text": "We report on the observation of strong ion - phonon interaction ( EPI ) in heavily boron - doped epitaxial graphene grown on SiC ( 0001 ) . The EPI is found to be enhanced by more than one order of magnitude compared with that detected for undoped graphene , which can be due to the formation of a Dirac point at high doping rates and its associated van Hove singularity near the Fermi level .This found shows that the EPI in doped graphene may be tunable over several orders of magnitude through chemical or electrostatic gating . Graphene has garnered considerable scrutiny because it displays unusual physical properties such as extremely high carrier mobility1 - 5 and very huge optical nonlinearities6 - 8 .However , these unique properties are often accompanied by weak interactions between electrons and phonons9 - 11 , which reduce their applications in high - speed electronics12 - 14 and optoelectronics15 - 17 . In this research we prove that the EPI in heavily boron - doping epitaxial graphene grown by mechanical transformation of SiC18 - 20 can be greatly enhanced owing to the presence of a Dirac point21 - 23 and its associated van Hov singularity24 - 26 near the Fermi energy EF .We suggest that the EPI increases quickly when the Fermi level crosses the van Hove singularity , leading in a huge improvement in the electron - phonon absorption rate . Our results propose that the EPI in graphene might be regulated electrically via chemical or electrostatic gated27 - 30 , thereby introducing up new avenues towards new technology based on graphene .Graphene is known to have extremely high carrier mobilities1 - 4 but fairly little electron - phonon couplings5 - 9 . These two different properties determine the performance of graphene - based electronic and optoelectronic devices10 - 12 .For instance , the small EPI contributes to small relaxation rates13 - 15 and therefore decreases the operation rate of graphene transistors14 - 16 . On the other hand , the high mobility makes graphene suitable for use in high - speed electronics17 - 19 and ultrafast photodetectors20 .Therefore , there exists much interest in improving strategies to modify the EPI while maintaining the high mobility31",
        "rewrite_text": "We present our findings on the significant electron-phonon interaction (EPI) observed in heavily boron-doped epitaxial graphene grown on SiC (0001). The EPI is enhanced by more than an order of magnitude compared to that in undoped graphene, likely due to the emergence of a Dirac point at high doping levels and the associated van Hove singularity near the Fermi level. This discovery indicates that EPI in doped graphene can be tuned across several orders of magnitude through chemical or electrostatic gating. Graphene has attracted extensive research interest due to its remarkable physical properties, including exceptionally high carrier mobility and significant optical nonlinearities. However, these beneficial traits are typically offset by weak electron-phonon interactions, which limit its applications in high-speed electronics and optoelectronics. In our study, we demonstrate that the EPI in heavily boron-doped epitaxial graphene—produced via mechanical transformation of SiC—can be substantially enhanced due to the presence of both a Dirac point and its corresponding van Hove singularity near the Fermi energy (EF). We propose that the EPI rises sharply when the Fermi level crosses the van Hove singularity, resulting in a considerable increase in the electron-phonon absorption rate. Our findings suggest that the EPI in graphene could be electrically modulated through chemical or electrostatic gating, paving the way for innovative technologies based on graphene. Although graphene is recognized for its high carrier mobility, it traditionally exhibits relatively low electron-phonon coupling. These contrasting properties influence the performance of graphene-based electronic and optoelectronic devices. For example, the low EPI contributes to diminished relaxation rates, subsequently reducing the operational efficiency of graphene transistors. Conversely, the high mobility renders graphene suitable for applications in high-speed electronics and ultrafast photodetectors. As such, there is a growing interest in strategies to enhance EPI while preserving high mobility.",
        "ori-fast-z-score": -0.8049844718999243,
        "water-fast-z-score": 8.074944235686637,
        "rewrite-fast-z-score": 1.403292830891247
    },
    {
        "original_text": "We report on new observations with Suzaku and XMM-Newton that reveal an extended component in the hard X-ray spectrum (10-40 keV) of the Coma galaxy cluster, which is located at a distance of about 100 Mpc. The observed flux density distribution shows a clear excess over the expected contribution from point sources within the field-of-view. We find that this excess can be well described by a power-law model with photon index Γ = 1.7 ± 0.1 and normalization F0 = (2.6 ± 0.3) × 10−12 erg cm−2 s−1 sr−1 at 10 keV. This result suggests that there are additional components beyond those associated with individual galaxies or AGNs contributing to the hard X-ray emission from clusters of galaxies. In particular, we discuss possible origins for such diffuse emission as thermal bremsstrahlung radiation from hot gas and/or inverse Compton scattering off cosmic microwave background photons.",
        "watermark_text": "We report on new experiments with Suzaku and XMM - Newton that discover an extended component in the hard X - ray spectrum ( 10 - 40 keV ) of the Coma galaxy cluster , which is situated at a distance of about 100 Mpc . The observed flux concentration distribution shows a clear excess over the expected contribution from point sources within the field - of - view .We see that this excess can be well described by a power - law theory with photon index Γ = 1 . 7 ± 0 . 1 and normalization F0 = ( 2 . 6 ± 0 . 3 ) × 10−12 erg cm−2 s−1 sr−1 at 10 keV . This result suggests that there are additional components beyond those associated with individual stars or AGNs causing to the hard X - ray radiation from clusters of stars .In particular , we explain possible origins for such diffuse emission as heat bremsstrahlung rays from hot gas and / or inverse Compton absorption off cosmic microwave background photons .",
        "rewrite_text": "We present findings from new experiments conducted with Suzaku and XMM-Newton, which reveal an extended component in the hard X-ray spectrum (10 - 40 keV) of the Coma galaxy cluster, located approximately 100 Mpc away. The distribution of the observed flux concentration indicates a significant excess compared to the expected contribution from point sources within the field of view. This excess can be effectively described by a power-law model with a photon index of Γ = 1.7 ± 0.1 and a normalization of F0 = (2.6 ± 0.3) × 10⁻¹² erg cm⁻² s⁻¹ sr⁻¹ at 10 keV. These results imply the existence of additional components beyond those linked to individual stars or active galactic nuclei (AGNs) contributing to the hard X-ray radiation from star clusters. In particular, we explore possible sources of this diffuse emission, including heat bremsstrahlung radiation from hot gas and inverse Compton scattering off cosmic microwave background photons.",
        "ori-fast-z-score": 1.116312611302876,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "We present new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377, obtained with the Wide Field Planetary Camera 2 (WFPC2). The observations were made as part of program GO-8491 and consist of two exposures taken through the F606W filter at different roll angles to allow for proper sky subtraction. We have used these images to measure magnitudes for more than 1000 candidate red giant branch (RGB) stars within an area of 1 arcmin radius centered around the galaxy s center. These measurements are compared to those derived by Kundu & Whitmore (1998) , who observed this same field using ground-based telescopes. Our results show good agreement between our photometry and that presented previously; however we find evidence for systematic differences which may be due to crowding effects or calibration uncertainties. \n \n Keywords: Red Giant Branch, Galaxy, WFPC2",
        "watermark_text": "We report new HST photometric data on halo stars in the nearby elliptical galaxy NGC 3377 , obtained with the Wide Field Planetary Camera 2 ( WFPC2 ) . The images were made as part of series GO - 8491 and consist of two exposures took through the F606W filter at different roll axes to allow for correct sky subtraction .We have utilized these images to measure magnitudes for more than 1000 candidate red giant line ( RGB ) stars within an area of 1 arcmin radius centered around the universe s center . These measurements are compared to those derived by Kundu & Whitmore ( 1998 ) , who studied this same field using ground - based telescopes .Our results show good agreement between our photometry and that presented previously ; however we find proof for systematic differences which may be due to crowding effects or calibration uncertainties . Keywords : Red Giant Branch , Galaxy , WFPC2",
        "rewrite_text": "We present new photometric data from the Hubble Space Telescope (HST) on halo stars in the nearby elliptical galaxy NGC 3377, acquired using the Wide Field Planetary Camera 2 (WFPC2). The observations were conducted as part of program GO-8491 and consisted of two exposures taken through the F606W filter, with different roll angles to facilitate accurate sky subtraction. Using these images, we have measured the magnitudes of over 1,000 candidate red giant branch (RGB) stars within a 1 arcminute radius centered on the galaxy's core. Our findings are compared to measurements made by Kundu & Whitmore (1998), who explored the same area with ground-based telescopes. Our results demonstrate strong agreement with their photometry; however, we also identify potential systematic discrepancies that may arise from crowding effects or calibration issues. Keywords: Red Giant Branch, Galaxy, WFPC2.",
        "ori-fast-z-score": 1.4832396974191326,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.13736056394868904
    },
    {
        "original_text": "We present an analytical model to study the formation of red elliptical galaxies in hierarchical cosmologies. We assume that supermassive black holes (SMBHs) grow by accretion at high redshift z > 6 when gas is abundant but cooling times are short. The SMBH growth rate depends on its mass M•(z), which we calculate using semi-analytical models based on merger trees extracted from N-body simulations. In our scenario, quasars shine brightly during their active phase with luminosities LQSO ~ 1045 erg s-1. They heat up surrounding cold gas clouds via photoionization heating and Compton scattering, preventing them from forming stars efficiently until they fade away after about 1 Gyr. This leads to a bimodal color distribution of early-type galaxies as observed today. Our results show good agreement with observations such as the quasar luminosity function, galaxy clustering statistics, and the fraction of red ellipticals among all massive galaxies.",
        "watermark_text": "We present an analytical theory to study the formation of red elliptical galaxies in hierarchical cosmologies . We assume that supermassive black holes ( SMBHs ) grow by accretion at high redshift z > 6 when gas is abundant but warming periods are small .The SMBH growth speed relies on its weight M • ( z ) , which we estimate utilizing semi - analytical models using on merger trees obtained from N - bodies simulations . In our scenario , quasars shine brightly during their active mode with luminosities LQSO ~ 1045 erg s - 1 .They heat up surrounding cold gas clouds via photoionization heating and Compton absorption , allowing them from creating stars efficiently until they fade away after about 1 Gyr . This leads to a bimodal color distribution of early - class stars as observed nowadays .Our results show good agreement with observations such as the quasar luminosity function , galaxy clustering statistics , and the fraction of red ellipticals among all huge objects .",
        "rewrite_text": "We introduce an analytical framework to investigate the formation of red elliptical galaxies within hierarchical cosmologies. We propose that supermassive black holes (SMBHs) experience growth through accretion at high redshifts (z > 6), a period characterized by abundant gas and brief warming intervals. The rate of SMBH growth is influenced by its mass M•(z), which we estimate using semi-analytical models based on merger trees derived from N-body simulations. In our model, quasars radiate intensely during their active phases, exhibiting luminosities of LQSO ~ 10^45 erg/s. They heat nearby cold gas clouds through photoionization and Compton absorption, enabling efficient star formation until they diminish after approximately 1 Gyr. This process results in a bimodal color distribution of early-type stars, which aligns with current observations. Our findings are consistent with various observational data sets, including the quasar luminosity function, galaxy clustering statistics, and the proportion of red elliptical galaxies among large structures.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": 0.46499055497527714
    },
    {
        "original_text": "We study the electromagnetic wave propagation through a bundle of closely-packed metallic single-walled carbon nanotubes (SWCNTs). The SWCNTs  diameter is assumed to be 1 nm and their length is taken as 10 microns. We use the finite element method for solving Maxwell s equations with appropriate boundary conditions at the ends of each tube. Our results show that there are two different regimes depending on the frequency range considered. In one regime, we find that the transmission coefficient decreases rapidly when increasing the number density of tubes. This behavior can be explained by considering the effect of multiple scattering between neighboring tubes. On the other hand, in another regime where the wavelength is much larger than the tube radius, the transmission coefficient increases slowly with respect to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes. Finally, our numerical results indicate that the presence of defects such as vacancies or impurities may significantly affect the overall transmission properties of the system.",
        "watermark_text": "We research the electromagnetic wave propagation through a bundle of closely - packed metallic single - walled carbon nanotubes ( SWCNTs ) . The SWCNTs diameter is expected to be 1 nm and their height is taken as 10 microns .We use the finite element method for solving Maxwell s coefficients with suitable boundary constraints at the ends of each tube . Our results show that there are two different regimes based on the frequency spectrum considered .In one regime , we find that the transmission coefficient drops rapidly when increasing the number density of tubes . This phenomenon can be described by using the result of multiple scattering between neighboring tubes .On the other hand , in another regime where the frequency is much larger than the tunnel radius , the propagation coefficient increases slowly with regard to the number density of tubes due to constructive interference effects among scattered waves inside individual tubes . Finally , our numerical findings show that the presence of flaws such as vacancies or impurities may significantly affect the overall transmission properties of the system .",
        "rewrite_text": "We investigate the propagation of electromagnetic waves through a tightly packed array of single-walled carbon nanotubes (SWCNTs) with a diameter of approximately 1 nm and a height of 10 microns. Utilizing the finite element method, we solve Maxwell's equations while applying appropriate boundary conditions at the ends of each nanotube. Our findings reveal two distinct regimes based on the frequency spectrum analyzed. In one regime, we observe a rapid decline in the transmission coefficient as the number density of the tubes increases, a phenomenon that can be attributed to multiple scattering effects among neighboring tubes. Conversely, in the second regime, where the frequency is substantially greater than the tube radius, the propagation coefficient exhibits a gradual increase with the number density of tubes, driven by constructive interference among scattered waves within individual tubes. Lastly, our numerical results indicate that defects such as vacancies or impurities can significantly impact the overall transmission characteristics of the system.",
        "ori-fast-z-score": 0.9649012813540153,
        "water-fast-z-score": 5.607304206578798,
        "rewrite-fast-z-score": 0.1125087900926024
    },
    {
        "original_text": "We study the linear stability properties of coronal beams in the presence of background plasma and magnetic field fluctuations, using a multi-fluid model for ions and electrons. We find that the growth rates are strongly dependent on the angle between the wavevector k and the mean magnetic field B 0 . In particular, we show that there is an instability at oblique angles with respect to B 0 , which has been previously overlooked by previous studies based on single-fluid models. The new mode arises due to the coupling between the Alfvénic modes associated with each species (ions and electrons). This mode can be excited even when the electron temperature anisotropy T e? /T ez < 1, where ? denotes directions perpendicular to B 0 .\nThe results presented here may have important implications for understanding the origin of solar radio bursts observed during solar flares. Introduction: Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the Sun s corona into interplanetary space. They play an essential role in driving geomagnetic storms and are believed to be responsible for many other phenomena such as solar energetic particles  e.g., Reames et al. (1998) , Kahler & Ragot (2007)  , solar radio bursts  e.g., Aschwanden (2004)  , and white-light flares  e.g., Benz (2008)  . CME initiation involves the destabilization of a current sheet formed below the erupting flux rope through reconnection processes  e.g., Forbes & Priest (1995) ; Lin & Forbes (2000); Aulanier et al. (2010)  . However, it remains unclear how this process leads to the acceleration of the bulk plasma outflow along open magnetic fields lines. Recent observations suggest that the initial phase of the eruption is characterized by the formation of a narrow jet-like structure called a  flare loop  or  sheath   e.g., Liu et al. (2009a Liu et al. ( , 2009b ; Cheng et al. (2011); Jiang et al. (2012",
        "watermark_text": "We research the linear stability properties of coronal beams in the presence of background plasma and magnetic force fluctuations , using a multi - fluid model for ions and electrons . We see that the development rates are strongly dependent on the angle between the wavevector k and the mean magnetic force B 0 .In particular , we prove that there is an instability at oblique directions with regard to B 0 , which has been previously overlooked by earlier studies relying on single - fluid models . The new mode occurs due to the interaction between the Alfvénic configurations involved with each species ( atoms and electrons ) .This mode can be excited even when the electron thermal anisotropy T e ? / T ez < 1 , where ?denotes directions perpendicular to B 0 . The results presented here possibly have important implications for studying the origin of solar radio flashes seen during thermal flares .Introduction : Coronal mass ejections ( CMEs ) are big - scale expulsions of magnetized plasma from the Sun s corona into interplanetary space . They play an essential part in causing geomagnetic winds and are considered to be responsible for numerous other processes such as solar energetic particles e . g . , Reames et al .( 1998 ) , Kahler & Ragot ( 2007 ) , sun wireless flare e . g . , Aschwanden ( 2004 ) , and white - light flares e . g . , Benz ( 2008 ) . CME initiation consists the destabilization of a current sheet formed below the erupting flux rope through reconnection pathways e . g . , Forbes & Priest ( 1995 ) ; Lin & Forbes ( 2000 ) ; Aulanier et al .( 2010 ) . However , it remains unsure how this process results to the acceleration of the bulk plasma outflow along open magnetic fields lines .Recent measurements suggest that the first phase of the volcano is characterized by the formation of a thin jet - like structure named a flare loop or sheath e . g . , Liu et al . ( 2009a Liu et al .( , 2009b ; Cheng et al . ( 2011 ) ; Jiang et al .(2012",
        "rewrite_text": "We investigate the linear stability characteristics of coronal beams in the context of background plasma and magnetic field fluctuations, employing a multi-fluid model for ions and electrons. Our findings indicate that the growth rates are significantly affected by the angle between the wavevector \\( \\mathbf{k} \\) and the average magnetic field \\( \\mathbf{B_0} \\). Notably, we demonstrate the existence of an instability at oblique angles relative to \\( \\mathbf{B_0} \\), a phenomenon that has been largely overlooked in previous research relying on single-fluid models. This new mode arises from the interactions between the Alfvénic configurations associated with each species (atoms and electrons). Moreover, this mode can be activated even when the electron thermal anisotropy \\( T_e^\\perp / T_e^z < 1 \\), where \\( \\perp \\) denotes directions perpendicular to \\( \\mathbf{B_0} \\). The outcomes of this study may have significant implications for understanding the origins of solar radio emissions observed during thermal flares.\n\n**Introduction:** Coronal mass ejections (CMEs) are large-scale expulsions of magnetized plasma from the solar corona into interplanetary space. They play a crucial role in generating geomagnetic winds and are believed to be responsible for various phenomena, including solar energetic particles (e.g., Reames et al. 1998, Kahler & Ragot 2007), solar flares (e.g., Aschwanden 2004), and white-light flares (e.g., Benz 2008). The initiation of a CME involves the destabilization of a current sheet formed beneath an erupting flux rope through reconnection processes (e.g., Forbes & Priest 1995; Lin & Forbes 2000; Aulanier et al. 2010). However, the exact mechanism by which this process accelerates the bulk plasma outflow along open magnetic field lines remains uncertain. Recent observations suggest that the initial phase of the eruption is marked by the formation of a thin, jet-like structure known as a flare loop or sheath (e.g., Liu et al. 2009a, Liu et al. 2009b; Cheng et al. 2011; Jiang et al. 2012).",
        "ori-fast-z-score": -1.007017629956027,
        "water-fast-z-score": 5.202924421439472,
        "rewrite-fast-z-score": 0.49656353316142077
    },
    {
        "original_text": "We report on an unusual bright optical transient (OT) discovered by the Palomar Transient Factory (PTF). The OT was detected at R = 16.7 mag and peaked at R = 14.6 mag, with a rise time of about 1 day. It is located near the center of M85, one of the nearest galaxies to our own Milky Way Galaxy. We find that this event has many properties similar to those observed for supernovae Ia but it lacks spectroscopic signatures typical of these events. This suggests that we are witnessing another type of explosion which may be related to some other types of transients such as tidal disruption flares or superluminous supernovae. \n \n Keywords: Supernova, Optical transient, PTF, Tidal disruption flare, Brightest cluster galaxy \n \n Introduction \n \n In recent years there have been several discoveries of extremely luminous optical transients associated with nearby galaxies. These include the famous outbursts of Eta Carinae (Davidson & Humphreys 1997; Smith et al. 1998), SN 2005ap (Gal-Yam et al. 2005; Foley et al. 2007), ASASSN-14li (Holoien et al. 2014a), ATLAS14aaq (Dong et al. 2015), PS1-10jh (Gezari et al. 2012), iPTF16axa (Kasliwal et al. 2016), and ASASSN-15oi (Shappee et al. 2016). Many of them were found to be associated with supermassive black holes residing in galactic nuclei. However, their exact nature remains unclear. Some authors suggested that they could be caused by tidal disruptions of stars by massive black holes (TDE) (Komossa 2002; Gezari et al. 2009a; Bloom et al. 2011; Holoien et al. 2013b; Arcavi et al. 2014; Brown et al. 2017), while others argued that they might represent new classes of thermonuclear explosions (SNe Ia-like) (Valenti et al. 2009; Kas",
        "watermark_text": "We report on an strange bright optical transient ( OT ) discovered by the Palomar Transient Factory ( PTF ) . The OT was measured at R = 16 . 7 mag and peaked at R = 14 . 6 mag , with a rise time of about 1 day .It is situated near the center of M85 , one of the nearest clusters to our own Milky Way Galaxy . We see that this event has numerous characteristics similar to those observed for supernovae Ia but it lacks spectroscopic signatures common of these events .This implies that we are witnessing another type of explosion which perhaps be connected to some other types of transients such as tidal disruption flares or superluminous supernovae . Keywords : Supernova , Optical transient , PTF , Tidal disruption explosion , Brightest cluster galaxy Introduction In recent history there have been numerous discoveries of incredibly luminous optical transients associated with nearby galaxies .These include the famous outbursts of Eta Carinae ( Davidson & Humphreys 1997 ; Smith et al . 1998 ) , SN 2005ap ( Gal - Yam et al .2005 ; Foley et al . 2007 ) , ASASSN - 14li ( Holoien et al .2014a ) , ATLAS14aaq ( Dong et al . 2015 ) , PS1 - 10jh ( Gezari et al .2012 ) , iPTF16axa ( Kasliwal et al . 2016 ) , and ASASSN - 15oi ( Shappee et al .2016 ) . Many of them were found to be identified with supermassive black holes residing in galactic nuclei .However , their exact form continues unclear . Some authors proposed that they may be caused by tidal disruptions of stars by massive black holes ( TDE ) ( Komossa 2002 ; Gezari et al .2009a ; Bloom et al . 2011 ; Holoien et al .2013b ; Arcavi et al . 2014 ; Brown et al .2017 ) , while others argued that they may signal new classes of thermonuclear explosions ( SNe Ia - like ) ( Valenti et al . 2009 ; Kas",
        "rewrite_text": "We present findings on an unusual bright optical transient (OT) discovered by the Palomar Transient Factory (PTF). This OT was recorded at R = 16.7 mag and reached a peak at R = 14.6 mag, with a rise time of approximately one day. It is located near the center of M85, one of the closest galaxy clusters to our Milky Way. While this event shares many characteristics with Type Ia supernovae, it does not display the typical spectroscopic signatures associated with these explosions. This indicates that we may be observing a different kind of explosion, potentially related to other transient phenomena such as tidal disruption flares or superluminous supernovae. \n\n**Keywords:** Supernova, Optical transient, PTF, Tidal disruption explosion, Brightest cluster galaxy\n\n**Introduction:** In recent years, there have been numerous discoveries of exceptionally bright optical transients linked to nearby galaxies. Notable examples include the dramatic eruptions of Eta Carinae (Davidson & Humphreys 1997; Smith et al. 1998), SN 2005ap (Gal-Yam et al. 2005; Foley et al. 2007), ASASSN-14li (Holoien et al. 2014a), ATLAS14aaq (Dong et al. 2015), PS1-10jh (Gezari et al. 2012), iPTF16axa (Kasliwal et al. 2016), and ASASSN-15oi (Shappee et al. 2016). Many of these events have been associated with supermassive black holes located in the centers of galaxies; however, their precise nature remains unclear. Some researchers suggest that these phenomena might be the result of tidal disruptions of stars by massive black holes (TDE) (Komossa 2002; Gezari et al. 2009a; Bloom et al. 2011; Holoien et al. 2013b; Arcavi et al. 2014; Brown et al. 2017), while others propose they may indicate new classes of thermonuclear explosions resembling Type Ia supernovae (Valenti et al. 2009; Kasliwal et al.).",
        "ori-fast-z-score": -0.9434563530497265,
        "water-fast-z-score": 3.249682993837947,
        "rewrite-fast-z-score": -0.6831300510639733
    },
    {
        "original_text": "The nonlinear dynamics of phonon stimulated emission (PSE) is investigated for microwave solid-state resonators with nonautonomous phase-locked loop generators. The PSE process is described by means of an extended version of the Lang-Kobayashi model, which takes into account the influence of the external driving field on the gain medium and includes additional terms describing the effect of spontaneous emission noise. It has been shown that the presence of these new effects leads to significant changes in the behavior of the system under consideration as compared to autonomous systems. In particular, it was found that the generation of chaotic regimes becomes possible even at relatively small values of the pumping parameter. This fact can be used to develop novel types of chaos-based devices based on microwave solid-state oscillators. \n \n Keywords: Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid state oscillator. PACS: 42.65.Tt; 42.65.Pq; 42.65.Re",
        "watermark_text": "The nonlinear dynamics of phonon stimulated emission ( PSE ) is investigated for microwave solid - state resonators with nonautonomous phase - locked loop components . The PSE transition is characterized by means of an extended version of the Lang - Kobayashi model , which gives into consideration the impact of the external driving field on the gain medium and incorporates additional terms describing the impact of induced emission interference .It has been shown that the presence of these new changes leads to significant improvements in the attitude of the process under consideration as compared to autonomous systems . In particular , it was shown that the generation of turbulent regimes appears possible even at fairly little values of the pumping function .This fact can be used to develop new types of chaos - based units based on microwave solid - state oscillators . Keywords : Nonlinear dynamics ; Chaos ; Phase synchronization ; Phonons ; Microwave liquid state oscillator .PACS: 42.65.Tt; 42.65.Pq; 42.65.Re",
        "rewrite_text": "The study explores the nonlinear dynamics of phonon-stimulated emission (PSE) in microwave solid-state resonators that incorporate nonautonomous phase-locked loop components. An extended version of the Lang-Kobayashi model is employed to characterize the PSE transition, taking into account the influence of the external driving field on the gain medium and adding terms that describe the effects of induced emission interference. The findings reveal that these modifications lead to notable enhancements in the system's behavior compared to autonomous configurations. Notably, the research indicates that turbulent regimes can be generated even with relatively low values of the pumping function. This insight could facilitate the development of new chaos-based devices utilizing microwave solid-state oscillators. Keywords: Nonlinear dynamics; Chaos; Phase synchronization; Phonons; Microwave solid-state oscillator. PACS: 42.65.Tt; 42.65.Pq; 42.65.Re.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 5.185449728701349,
        "rewrite-fast-z-score": -0.8551861104941365
    },
    {
        "original_text": "In this work, we propose an evolutionary algorithm to evolve ensemble classifiers in the context of multi-class classification problems. The proposed approach is based on the combination of two well-known techniques: genetic algorithms and bagging. We have conducted experiments using several datasets taken from UCI Machine Learning Repository. Our results show that our method outperforms other state-of-the-art methods such as Bagging or Random Forests. In addition, it has been shown that the use of ensembles can improve the performance of single models evolved by Genetic Programming (GP). This fact suggests that GP could be used not only to evolve individual solutions but also to evolve whole ensembles of solutions. Keywords: Ensemble learning, Multi-Class Classification, Genetic Algorithm, Bagging. 1 Introduction Ensembles are collections of multiple base learners whose outputs are combined into one final prediction  1  . They are widely used because they often provide better accuracy than any of their constituent members  2  .\nThe most popular approaches to combine predictions include voting schemes  3  , stacking  4  , boosting  5  , and blending  6  . However, these approaches require some knowledge about how to combine the output of each member of the ensemble  7?   . For example, if there are three classes, then the simplest way would be to assign equal weights to all the classifiers; however, this may lead to poor performance when dealing with imbalanced data  8  . Other more sophisticated ways involve assigning different weights according to the confidence level of each classifier  9  ; however, finding optimal values for those parameters requires additional effort  10  .\nRecently, researchers have started exploring new ways to automatically generate ensembles without requiring prior information  11  . One of them involves combining genetic algorithms  12  and bagging  13  . These two techniques were first applied separately  14, 15  , but later they were integrated together  16  .",
        "watermark_text": "In this research , we propose an phylogenetic algorithm to evolve ensemble classifiers in the context of multi - class classification difficulties . The proposed approach is based on the combination of two well - famous strategies : genetic methods and bagging .We have done research utilizing multiple datasets taken from UCI Machine Learning Repository . Our results show that our technique outperforms other state - of - the - art methods such as Bagging or Random Forests .In addition , it has been shown that the using of ensembles can increase the performance of single models adapted by Genetic Programming ( GP ) . This fact suggests that GP may be used not only to evolve individual solutions but also to evolve entire groups of solutions .Keywords : Ensemble learning , Multi - Class Classification , Genetic Algorithm , Bagging . 1 Introduction Ensembles are collections of multiple base learners whose outputs are united into one finished prediction 1 .They are widely using because they frequently provide better precision than any of their constituent groups 2 . The most popular methods to mix predictions include voting strategies 3 , stacking 4 , boosting 5 , and blending 6 .However , these perspectives need some understanding about how to mix the output of each member of the group 7 ? .For instance , if there are three categories , then the simplest manner might be to give equivalent weights to all the classifiers ; however , this might lead to bad performance when dealing with imbalanced data 8 . Other more sophisticated ways include assigning multiple weights based to the confidence rate of each classifier 9 ; however , finding ideal values for those variables requires added effort 10 .Recently , researchers have started researching new ways to automatically generate ensembles without using prior information 11 . One of them requires merging genetic methods 12 and bagging 13 .These two procedures were first applied separately 14 , 15 , but later they were integrated together 16 .",
        "rewrite_text": "In this study, we introduce a phylogenetic algorithm aimed at evolving ensemble classifiers to tackle challenges in multi-class classification. Our approach synthesizes two prominent strategies: genetic algorithms and bagging. We conducted experiments using various datasets from the UCI Machine Learning Repository, and our findings indicate that our method surpasses existing state-of-the-art techniques, including Bagging and Random Forests. Additionally, our results demonstrate that employing ensembles can enhance the performance of individual models optimized by Genetic Programming (GP). This insight suggests that GP can be leveraged not only for evolving singular solutions but also for developing entire ensembles of solutions. \n\n**Keywords:** Ensemble learning, Multi-Class Classification, Genetic Algorithm, Bagging. \n\n**1. Introduction** \nEnsembles consist of multiple base learners whose outputs are combined into a single final prediction. They are widely utilized due to their ability to deliver superior accuracy compared to any individual model. Common methods for combining predictions include voting strategies, stacking, boosting, and blending. However, these approaches require an understanding of how to aggregate the outputs from each member of the ensemble. For instance, in a scenario with three classes, a straightforward method would be to assign equal weights to all classifiers, yet this can lead to suboptimal performance when faced with imbalanced datasets. More advanced techniques involve assigning different weights based on the confidence level of each classifier, though determining optimal values for these weights demands additional effort. Recently, researchers have begun investigating methods to automatically generate ensembles without prior information, including the integration of genetic algorithms and bagging. These techniques were initially applied in isolation but have since been combined to enhance performance.",
        "ori-fast-z-score": -0.42717882885838043,
        "water-fast-z-score": 8.397070403831712,
        "rewrite-fast-z-score": -1.84894690328381
    },
    {
        "original_text": "We consider the dynamics of stochastic interacting particle systems in which particles evolve according to an overdamped Langevin equation with multiplicative noise and interact via pair potentials that decay exponentially fast at large distances.  We prove that, under suitable conditions on the interaction potential, these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator. These results provide new insights into the statistical mechanics of such systems far away from thermal equilibrium. Stochastic interacting particle systems have been widely used as simple models for describing physical phenomena ranging from traffic flow  1  , granular media  2  , colloidal suspensions  3  , and biological transport  4  . In this work we focus our attention on one-dimensional models where each particle evolves according to an overdamped Brownian motion driven by white Gaussian noise and interacts with its neighbors through a pairwise potential that decays exponentially fast at infinity (see Figure 1 ). The resulting system is described by the following set of Itô SDE s:",
        "watermark_text": "We consider the dynamics of stochastic interacting particle structures in which particles evolve according to an overdamped Langevin equation with multiplicative sound and interact via pair potentials that decay exponentially rapidly at large distances . We prove that , under suitable conditions on the interaction potential , these models admit unique stationary states whose density profiles are given by solutions of nonlinear integral equations involving fractional powers of the Laplacian operator .These data provide novel knowledge into the statistical mechanics of such systems distant distant from temperature equilibrium . Stochastic interacting particle networks have been widely using as simple models for describing physical phenomena ranging from street passage 1 , granular material 2 , colloidal suspensions 3 , and biological transport 4 .In this research we focus our focus on one - dimensional theories where each particle evolves due to an overdamped Brownian movement generated by white Gaussian interference and interacts with its friends through a pairwise potential that decays exponentially rapidly at infinity ( see Figure 1 ) . The resulting system is characterized by the following setting of Itô SDE s :",
        "rewrite_text": "We investigate the dynamics of stochastic interacting particle systems where particles evolve according to an overdamped Langevin equation with multiplicative noise and interact through pair potentials that decay rapidly at large distances. Under specific conditions on the interaction potential, we demonstrate that these models possess unique stationary states, with their density profiles described by solutions to nonlinear integral equations that involve fractional powers of the Laplacian operator. This research contributes new insights into the statistical mechanics of systems far from thermal equilibrium. Stochastic interacting particle networks have been widely utilized as simplified models to depict a variety of physical phenomena, including pedestrian movement, granular materials, colloidal suspensions, and biological transport. In this study, we concentrate on one-dimensional models, where each particle undergoes overdamped Brownian motion influenced by white Gaussian noise and interacts with neighboring particles through a rapidly decaying pairwise potential (see Figure 1). The system can be characterized by a specific formulation of Itô stochastic differential equations (SDEs):",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 0.6625891564490792
    },
    {
        "original_text": "We present new U BVRI photometric observations for the barred galaxy NGC 3367, obtained with the 1 m telescope at Cerro Tololo Inter-American Observatory (CTIO). The main goal is to study the stellar structures in this galaxy and their relation to its nuclear activity. We find that there are two bright knots along the major axis of the galaxy which may be associated with star formation regions. These knots have colors similar to those found in HII regions. In addition we detect several other faint knots on both sides of the nucleus. Their color indices suggest that they could also be related to recent star formation events. Finally, we identify an extended structure towards south-east direction whose nature remains unclear. This work was supported by CONACyT grant 36586-E. We thank J. M. Alloin for his help during our observing run at CTIO. Keywords: Starburst galaxies; Nuclear activity",
        "watermark_text": "We report new U BVRI photometric surveys for the barred star NGC 3367 , obtained with the 1 m observatory at Cerro Tololo Inter - American Observatory ( CTIO ) . The main goal is to study the stars formations in this galaxy and their connection to its nuclear activity .We see that there are two bright knots along the main axis of the galaxy which may be identified with star formation regions . These threads have colors similar to those present in HII centers .In addition we find several other slight knots on both sides of the nucleus . Their color indices indicate that they may also be connected to recent star formation changes .Finally , we identify an extended structure towards south - eastward direction whose nature remains unsure . This project was supported by CONACyT grant 36586 - E . We praise J . M . Alloin for his help during our observing run at CTIO .Keywords : Starburst galaxies ; Nuclear activity",
        "rewrite_text": "We present new U BVRI photometric surveys of the barred galaxy NGC 3367, conducted with the 1 m telescope at the Cerro Tololo Inter-American Observatory (CTIO). The primary objective of this research is to investigate the star formation phenomena within this galaxy and their relationship to its nuclear activities. Our observations reveal two prominent bright knots along the galaxy's main axis, which are likely associated with star formation regions. These features exhibit color characteristics similar to those found in HII regions. Additionally, we have identified several fainter knots on either side of the nucleus, with color indices suggesting a connection to recent changes in star formation. Moreover, we observe an extended structure extending southeast, although its exact nature remains uncertain. This work was funded by CONACyT grant 36586-E, and we would like to express our gratitude to J. M. Alloin for his assistance during our observing session at CTIO. Keywords: Starburst galaxies; Nuclear activity.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 0.6974858324629157
    },
    {
        "original_text": "We present an analysis of Hawking radiation in terms of Bogoliubov coefficients, which are calculated using WKB wavefunctions for scalar fields on Schwarzschild backgrounds. We show that these results agree with those obtained by other methods when the backreaction is neglected and we also find agreement between our method and previous calculations including backreaction effects at leading order in perturbation theory. In addition to this perturbative check, we perform numerical checks of our results against exact solutions of the Klein-Gordon equation in Schwarzschild spacetime. Finally, we discuss how our approach can be used to calculate corrections beyond the semiclassical approximation. The evaporation of black holes has been studied extensively over many years (see e.g. ), but there remain some open questions about its detailed behaviour. One such question concerns the precise form of the spectrum of emitted particles; it was shown recently  that the standard semi-classical treatment leads to a thermal distribution of particle energies, but it remains unclear whether or not this result holds true once quantum gravity effects become important.",
        "watermark_text": "We present an assessment of Hawking radiation in terms of Bogoliubov coefficients , which are measured using WKB wavefunctions for scalar fields on Schwarzschild backgrounds . We see that these results agree with those achieved by other methods when the backreaction is neglected and we also find agreement between our technique and previous analyses using backreaction effects at leading order in perturbation theory .In addition to this perturbative search , we perform numerical checks of our findings against exact solutions of the Klein - Gordon equation in Schwarzschild spacetime . Finally , we explain how our approach can be used to estimate corrections beyond the semiclassical approximation .The evaporation of black holes has been studied thoroughly over numerous years ( see e . g . ) , but there remain some open questions about its precise behaviour .One such issue concerns the exact form of the spectrum of emitted particles ; it was shown lately that the standard semi - classical treatment leads to a heat distribution of particle intensity , but it remains unsure whether or not this consequence holds true once quantum gravitational changes become crucial .",
        "rewrite_text": "We provide an evaluation of Hawking radiation through the lens of Bogoliubov coefficients, utilizing WKB wavefunctions for scalar fields in Schwarzschild backgrounds. Our findings align with those obtained through other approaches when disregarding backreaction effects, and we also establish consistency with prior analyses that account for backreaction effects at leading order in perturbation theory. In addition to this perturbative investigation, we conduct numerical verifications of our results by comparing them to exact solutions of the Klein-Gordon equation within Schwarzschild spacetime. Furthermore, we discuss how our methodology can be employed to estimate corrections that extend beyond the semiclassical approximation. Although the evaporation of black holes has been extensively studied for many years, certain questions about its precise dynamics remain unresolved. One such question pertains to the specific form of the emitted particle spectrum; recent studies have indicated that the conventional semiclassical approach yields a thermal distribution of particle intensities, yet it remains uncertain whether this conclusion holds when quantum gravitational effects become significant.",
        "ori-fast-z-score": 1.3251783128981585,
        "water-fast-z-score": 6.555555555555555,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We present the first version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which is based on the recently developed weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in one dimension and multi-dimensions.  The main idea behind this method is to use high-order spatial reconstruction with an adaptive mesh refinement technique to achieve high accuracy at low computational cost. We have implemented several different versions of the WENO algorithm into our code including the fifth-order WENO-Z scheme as well as the third- and fifth-order WENO-JS schemes. In addition we also implement the fourth-order Runge-Kutta time integration scheme along with the Harten-Lax-van Leer contact discontinuity capturing scheme to handle discontinuities arising during hydrodynamic evolution. Our results show that all these algorithms are able to produce accurate solutions when compared against exact or reference solutions.",
        "watermark_text": "We introduce the first generation of our new code , WHAM ( Weno - Hybrid Arbitrary Mesh ) , which is based on the recently advanced weighted essentially non - oscillatory ( WENO ) schemes for solving hyperbolic conservation forces in one dimension and multi - dimensions . The main idea behind this algorithm is to use large - order spatial reconstruction with an adaptive mesh refinement technique to achieve high sensitivity at low numerical cost .We have integrated various different versions of the WENO algorithm into our code including the fifth - order WENO - Z plan as also as the third - and fifth - order WENO - JS schemes . In addition we also execute the third - order Runge - Kutta time integration scheme along with the Harten - Lax - van Leer contact discontinuity capturing scheme to manage discontinuities resulting during hydrodynamic evolution .Our results show that all these algorithms are able to produce accurate answers when compared against exact or reference solutions .",
        "rewrite_text": "We are excited to present the inaugural version of our new code, WHAM (Weno-Hybrid Arbitrary Mesh), which utilizes the latest advancements in weighted essentially non-oscillatory (WENO) schemes for solving hyperbolic conservation laws in both one and multiple dimensions. The core concept of this algorithm involves employing high-order spatial reconstruction combined with an adaptive mesh refinement technique, allowing for high sensitivity at a low numerical cost. Our code incorporates several different versions of the WENO algorithm, including the fifth-order WENO-Z scheme, as well as the third and fifth-order WENO-JS schemes. Additionally, we implement a third-order Runge-Kutta time integration method alongside the Harten-Lax-van Leer scheme for capturing contact discontinuities that arise during hydrodynamic evolution. Our results demonstrate that all these algorithms can deliver accurate solutions when benchmarked against exact or reference solutions.",
        "ori-fast-z-score": 2.1652509527331207,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 1.5011106998930268
    },
    {
        "original_text": "We present optical BVRI imaging, near-infrared JHKs photometry, and radio continuum observations at 1.4 GHz for the dwarf irregular galaxy ESO 364-G 029 (UGC 6456). The new data are combined with existing Hα spectroscopy to study its star formation history over the past few hundred million years. We find that this galaxy has experienced several bursts of intense star formation in recent times, which have produced large amounts of ionized gas visible as bright knots of emission across most of the face-on disk. These knots appear to be associated with young massive stars formed during each episode of star formation. In addition, we detect an extended component of diffuse ionized gas surrounding these knots. This is likely due to photoionization by hot evolved stars or supernovae remnants. Using our deepest images taken under good seeing conditions, we measure a total stellar mass of M = 2.1 × 10^7 M_sol within a radius of 5 kpc.",
        "watermark_text": "We use optical BVRI imaging , near - infrared JHKs photometry , and radio continuum measurements at 1 . 4 GHz for the dwarf irregular star ESO 364 - G 029 ( UGC 6456 ) . The revised data are coupled with existing Hα spectroscopy to study its galaxy formation history over the previous few hundred million years .We see that this galaxy has undergone several bursts of aggressive star formation in recent periods , which have created vast quantities of ionized gas evident as bright knots of emission across most of the face - on disk . These knots appear to be identified with young massive galaxies formed during each season of galaxy formation .In addition , we find an extended component of diffuse ionized gas covering these knots . This is probably due to photoionization by hot evolved galaxies or supernovae fragments .Using our deepest images took under good see conditions , we measure a total stellar mass of M = 2 . 1 × 10 ^ 7 M _ sol within a diameter of 5 kpc .",
        "rewrite_text": "We employ optical BVRI imaging, near-infrared JHKs photometry, and 1.4 GHz radio continuum measurements to investigate the dwarf irregular galaxy ESO 364-G 029 (UGC 6456). This updated data, combined with existing Hα spectroscopy, allows us to explore the galaxy's formation history over the last few hundred million years. Our observations reveal that this galaxy has experienced several intense bursts of star formation recently, resulting in the production of significant amounts of ionized gas, which is manifested as bright emission knots scattered throughout its face-on disk. These knots are thought to correspond to young massive stars formed during each phase of star formation activity. Additionally, we identify an extended region of diffuse ionized gas surrounding these knots, likely attributed to photoionization from hot evolved stars or supernova remnants. With our deepest images taken under favorable seeing conditions, we estimate the total stellar mass to be M = 2.1 × 10^7 M_sun within a diameter of 5 kpc.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 1.811643254631353
    },
    {
        "original_text": "We present results for MHD simulations of the magneto-rotational instability (MRI) in a stratified, Keplerian disk threaded by an initially vertical magnetic field using the ZEUS-2D code. We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas. In all cases we find that the growth rate of the fastest growing mode agrees well with linear theory predictions when normalized appropriately. However, there are significant differences between our runs depending upon whether or not they have reached steady-state equilibrium. For example, the saturated level of stress achieved at late times varies significantly among different models. This suggests that it may be difficult to accurately predict the saturation amplitude of the MRI unless one can perform very high resolution calculations which evolve over many orbital periods. Finally, we show that the inclusion of radiative cooling has little effect on the properties of the turbulence generated by the MRI.",
        "watermark_text": "We see results for MHD simulations of the magneto - rotational disturbance ( MRI ) in a stratified , Keplerian disk threaded by an initially vertical magnetic force using the ZEUS - 2D coding . We consider both isothermal and adiabatic equations of state to examine how the MRI depends on the thermodynamics of the gas .In all situations we find that the development frequency of the fastest growing mode agrees well with continuous theory expectations when normalized appropriately . However , there are significant variations between our runs depending upon whether or not they have achieved steady - state balance .For instance , the saturated amount of stress attained at late times changes significantly among different models . This implies that it could be impossible to correctly calculate the saturation amplitude of the MRI unless one can conduct very high resolution measurements which change over numerous orbital periods .Finally , we prove that the introduction of radiative cooling has little impact on the properties of the turbulence generated by the MRI .",
        "rewrite_text": "We present results from MHD simulations of the magneto-rotational instability (MRI) within a stratified, Keplerian disk influenced by an initially vertical magnetic field, conducted using the ZEUS-2D code. To investigate the effect of thermodynamics on the MRI, we analyze both isothermal and adiabatic equations of state. In all cases, we find that the development frequency of the fastest-growing mode aligns closely with the predictions of continuous theory when appropriately normalized. However, notable differences arise between our simulations based on whether they have reached a steady-state balance. For example, the level of stress achieved in the saturated state varies significantly across different models. This suggests that accurately determining the saturation amplitude of the MRI may be challenging without conducting high-resolution measurements over many orbital periods. Lastly, we demonstrate that the inclusion of radiative cooling has a minimal effect on the characteristics of the turbulence produced by the MRI.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 5.737097324805089,
        "rewrite-fast-z-score": -0.48507125007266594
    },
    {
        "original_text": "We present new near-infrared (NIR) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope, which allow us to study the dwarf galaxy population in this rich environment for the first time at wavelengths longer than 1 micron. We identify and classify all galaxies detected within an area of 0.5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red galaxies that have been missed by previous optical surveys due to their low surface brightnesses. Using photometric redshift estimates we find that most of them lie between z=0.1 and z=1.0. By comparing our sample to existing spectroscopic data sets we show that our NIR selection is complete up to M* ~ - 17 + 5 log h70. This corresponds roughly to L*(z=0), but it should be noted that there may still exist some fainter dwarfs below our detection limit.",
        "watermark_text": "We present new near - infrared ( NIR ) observations of the Coma cluster using the Infrared Array Camera on board the Spitzer Space Telescope , which allow us to study the dwarf galaxy community in this rich environment for the first time at wavelengths greater than 1 micron . We recognize and classify all galaxies found within an area of 0 . 5 deg2 centered around the center of the Coma cluster down to a limiting magnitude of Ks = 18 mag .The majority of these sources are faint red objects that have been missed by current visual observations due to their low exterior brightnesses . Using photometric redshift estimates we find that most of them reside between z = 0 . 1 and z = 1 . 0 .By matching our sample to existing spectroscopic data sets we indicate that our NIR selection is complete up to M * ~ - 17 + 5 log h70 . This equals roughly to L * ( z = 0 ) , but it should be mentioned that there may still lie some fainter dwarfs below our detection limit .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the Coma cluster, conducted with the Infrared Array Camera on the Spitzer Space Telescope. These observations allow us to explore the dwarf galaxy population in this densely populated environment for the first time at wavelengths exceeding 1 micron. We identify and classify all galaxies located within a 0.5 deg² area centered on the Coma cluster, down to a limiting magnitude of Ks = 18 mag. The majority of these sources are faint red objects that previous visual observations have missed due to their low surface brightness. By utilizing photometric redshift estimates, we find that most of these galaxies lie between z = 0.1 and z = 1.0. By comparing our findings with existing spectroscopic data, we confirm that our NIR selection is complete for galaxies with M* approximately -17 + 5 log h70, which corresponds to roughly L* (z = 0). However, it is important to note that there may still be some fainter dwarf galaxies that are below our detection threshold.",
        "ori-fast-z-score": 1.5491933384829668,
        "water-fast-z-score": 5.422176684690384,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "The search is performed in the context of the Minimal Supersymmetric Standard Model (MSSM) using data collected by the Compact Muon Solenoid experiment at sqrt(s) = 7 TeV, corresponding to an integrated luminosity of 5 fb-1 . The results are interpreted as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance. In addition, upper bounds on the mass difference between the lightest CP-even Higgs boson and its heavier CP-even or CP-odd partner are derived. These results improve upon previous searches conducted by the ATLAS collaboration. \n \n A summary of this work has been presented at: \n \n \n \n \n \n This document contains additional information that may be useful to readers interested in reproducing our analysis or applying it to other datasets. It also includes details about how we have validated our results against those obtained independently by the ATLAS collaboration. \n \nIntroduction\n\nThe discovery of a new particle consistent with the Standard Model (SM) Higgs boson  1–3  has opened up a new era in particle physics. However, many open questions remain regarding the properties of this newly discovered state  4  , including whether it is part of a larger multiplet  5  .\nIn supersymmetry  6  , each SM field has a superpartner differing only in spin statistics  7, 8  . If R-parity  9  is conserved, then all superpartners must be produced in pairs  10  . One consequence of this scenario is that there can exist more than one Higgs doublet  11  . In particular, if the lighter scalar Higgs boson observed at the LHC  12–18  corresponds to the lightest CP-eigenstate h0 of such a model  19, 20  , then the next-to-lightest CP-eigenstates H0 and A0 could both couple strongly to fermions  21  . Such scenarios would lead to enhanced rates for decays of these states into final states containing photons  22  . \n \n In order to explore possible deviations from the SM predictions  23  , precise measurements of the masses and couplings of the Higgs bosons predicted by",
        "watermark_text": "The scan is conducted in the context of the Minimal Supersymmetric Standard Model ( MSSM ) using data taken by the Compact Muon Solenoid experiment at sqrt ( s ) = 7 TeV , corresponding to an integrated luminosity of 5 fb - 1 . The results are seen as limits on the production cross section times branching fraction into two photons of neutral Higgs bosons decaying within the detector acceptance .In addition , upper limits on the mass ratio between the lightest CP - even Higgs boson and its lighter CP - even or CP - even partner are derived . These data improve upon recent searches undertaken by the ATLAS collaboration .A description of this research has been presented at : This text includes added details that might be valuable to readers interested in reproducing our analysis or applying it to other datasets . It additionally contains details about how we have validated our findings against those acquired independently by the ATLAS collaboration .Introduction The discovery of a new particle compatible with the Standard Model ( SM ) Higgs boson 1 – 3 has opened up a new period in particle science . However , many open questions remain regarding the properties of this freshly found state 4 , particularly whether it is part of a greater multiplet 5 .In supersymmetry 6 , each SM field has a superpartner varying only in spin statistics 7 , 8 . If R - parity 9 is conserved , then all superpartners must be made in pairs 10 .One result of this situation is that there can occur more than one Higgs doublet 11 . In particular , if the heavier scalar Higgs boson seen at the LHC 12 – 18 corresponds to the lightest CP - eigenstate h0 of such a theory 19 , 20 , then the second - to - lightest CP - eigenstates H0 and A0 could both couple strongly to fermions 21 .Such scenarios would result to accelerated rates for decays of these states into last states carrying photons 22 . In order to examine possible deviations from the SM predictions 23 , detailed observations of the masses and couplings of the Higgs bosons predicted by",
        "rewrite_text": "The scan is performed within the framework of the Minimal Supersymmetric Standard Model (MSSM) using data obtained from the Compact Muon Solenoid experiment at a center-of-mass energy of \\( \\sqrt{s} = 7 \\) TeV, corresponding to an integrated luminosity of 5 fb\\(^{-1}\\). The findings are interpreted as upper limits on the production cross section multiplied by the branching fraction into two photons for neutral Higgs bosons that decay within the detector's acceptance. Additionally, upper limits on the mass ratio between the lightest CP-even Higgs boson and its lighter CP-even or CP-odd partners are established. These results enhance recent searches conducted by the ATLAS collaboration. A comprehensive description of this research has been shared, which includes valuable insights for readers interested in replicating our analysis or applying it to alternative datasets. It also provides information on how we have cross-validated our findings with independent results from the ATLAS collaboration.\n\nThe introduction of a new particle consistent with the Standard Model (SM) Higgs boson has ushered in a new era in particle physics. Nevertheless, several unresolved questions persist about the characteristics of this newly discovered state, particularly regarding its potential membership in a larger multiplet. In the context of supersymmetry, each SM field has an associated superpartner that differs only in spin statistics. If R-parity is conserved, all superpartners must exist in pairs. This leads to the possibility of having multiple Higgs doublets. Notably, if the heavier scalar Higgs boson observed at the LHC corresponds to the lightest CP eigenstate \\( h_0 \\) in such a theoretical framework, the second-lightest CP eigenstates \\( H_0 \\) and \\( A_0 \\) could exhibit significant coupling to fermions. In such scenarios, higher decay rates could occur for these states into final states that include photons. To investigate any potential deviations from SM predictions, meticulous measurements of the masses and couplings of the Higgs bosons anticipated by this model are essential.",
        "ori-fast-z-score": -2.658425641381813,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": -1.0776318121606494
    },
    {
        "original_text": "We report on the detection by HESS of an exceptional flaring activity in the very-high-energy (VHE) gamma-ray band for the blazar PKS 2155-304, which was observed between September and November 2007 with a flux doubling time scale as short as ~1 day. The source reached its highest recorded state ever detected at TeV energies during this period. We present results based on data taken simultaneously with Swift-XRT and UVOT instruments that show no significant change in X-ray or optical emission properties over the course of the outburst. This suggests that the high-energy emission is produced via inverse Compton scattering off relativistic electrons located close to the central black hole rather than synchrotron radiation emitted by particles accelerated up to high Lorentz factors within the jet itself. These findings are consistent with theoretical models where particle acceleration takes place in shocks formed when jets collide with surrounding clouds. Keywords: Blazars, Very High Energy Emission",
        "watermark_text": "We report on the discovery by HESS of an exceptional flaring activity in the very - large - energy ( VHE ) gamma - ray band for the blazar PKS 2155 - 304 , which was seen between September and November 2007 with a flux doubling time scale as short as ~ 1 hour . The source achieved its highest confirmed state yet detected at TeV energies during this era .We present results based on evidence drawn independently with Swift - XRT and UVOT instruments that demonstrate no major shift in X - ray or optical emission behavior over the course of the outburst . This implies that the high - energy emission is produced via inverse Compton absorption off relativistic electrons located close to the main white hole rather than synchrotron emission generated by particles advanced up to low Lorentz factors within the jet itself .These conclusions are compatible with theoretical theories where particle motion takes occur in shocks formed when jets collide with nearby clouds . Keywords : Blazars , Very High Energy Emission",
        "rewrite_text": "We report the remarkable discovery by HESS of intense flaring activity in the very-high-energy (VHE) gamma-ray band from the blazar PKS 2155-304, observed between September and November 2007, with a flux doubling time of approximately 1 hour. During this period, the source reached its highest confirmed state ever detected at TeV energies. Our findings, based on independent observations from Swift-XRT and UVOT instruments, indicate that there was no significant change in the behavior of X-ray or optical emissions throughout the outburst. This suggests that the high-energy emissions arise from inverse Compton scattering by relativistic electrons situated near the central black hole, rather than from synchrotron emissions produced by particles accelerated to low Lorentz factors within the jet itself. These results align with theoretical models where particle dynamics occur in shocks formed when jets interact with nearby clouds. Keywords: Blazars, Very High Energy Emission.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.23904572186687872
    },
    {
        "original_text": "We present here an overview of our numerical method for propagating cosmic ray particles through the Galactic magnetic field and interstellar medium, including all relevant physical processes such as diffusion, convection, adiabatic deceleration/acceleration, nuclear fragmentation and radioactive decay. We discuss how we treat the various sources of uncertainty associated with these processes (e.g., uncertainties in the strength and structure of the Galactic magnetic field) by performing several different calculations using different assumptions about each process. Finally, we describe some preliminary results obtained from this code which demonstrate that it is capable of reproducing many observed features of the diffuse gamma-ray emission produced by cosmic ray interactions with gas throughout the Milky Way. The main goal of this work is to develop a new technique for studying the origin and propagation of cosmic rays on cosmological scales. \n \n High-energy cosmic rays are believed to be accelerated at astrophysical shocks driven by supernova remnants or active galactic nuclei. These energetic particles then propagate diffusively through space until they interact with matter or radiation fields along their trajectories. In doing so, they produce secondary photons and neutrinos via hadronuclear reactions and inverse Compton scattering respectively. Cosmic rays also contribute significantly to the total pressure support within galaxies and may play an important role in regulating star formation rates therein. However, despite decades of theoretical study, there remain significant uncertainties regarding both the acceleration mechanisms responsible for producing cosmic rays and the transport properties of those same cosmic rays once they have been accelerated. This situation has led to considerable debate over whether cosmic rays can account for the bulk of the pressure required to maintain the regular rotation curves of spiral galaxies without violating observational constraints imposed by the non-detection of dark matter halos around most nearby galaxies.  \n \n To address these questions, we have developed a new computational tool called GALPROP (Galaxy Propagator), which solves numerically the time-dependent transport equation describing the evolution of cosmic ray distributions in three spatial dimensions under the influence of large-scale magnetic fields and turbulent motions. Our approach involves discretizing the distribution function into a large number of",
        "watermark_text": "We present here an overview of our numerical technique for propagating cosmic ray molecules through the Galactic magnetic field and interstellar medium , covering all relevant physical processes such as diffusion , convection , adiabatic deceleration / acceleration , nuclear fragmentation and radioactive decay . We discuss how we treat the various sources of uncertainty associated with these mechanisms ( e . g . , uncertainties in the strength and shape of the Galactic magnetic force ) by performing numerous separate measurements involving varying constraints about each system .Finally , we explain some preliminary results acquired from this code which demonstrate that it is capable of reproducing many observed features of the diffuse gamma - ray radiation generated by gamma radiation interactions with gas throughout the Milky Way . The main goal of this project is to develop a new technique for studying the origin and propagation of gamma radiation on cosmological scales .High - energy cosmic rays are said to be advanced at astrophysical shocks driven by supernova remnants or active galactic nuclei . These energetic particles then propagate diffusively through space until they interact with matter or emission fields along their trajectories .In doing so , they produce secondary photons and neutrinos via hadronuclear reactions and inverse Compton diffusion respectively . Cosmic rays additionally help greatly to the total stress support within stars and may play an important role in controlling galaxy formation rates therein .However , despite decades of theoretical investigation , there remain considerable uncertainties regarding both the acceleration mechanisms involved for producing cosmic rays and the travel properties of those same cosmic rays once they have been accelerated . This problem has led to considerable debate over whether cosmic rays can provide for the majority of the pressure required to keep the standard rotation curves of spiral galaxies without violating observational restrictions imposed by the non - observation of bright matter halos around most nearby galaxies .To address these problems , we have developed a new computational tool called GALPROP ( Galaxy Propagator ) , which solves numerically the time - dependent transport equation explaining the evolution of cosmic ray distributions in three spatial dimensions under the impact of large - scale magnetic fields and turbulent movements . Our solution involves discretizing the distribution relation into a large number of",
        "rewrite_text": "We provide an overview of our numerical approach for tracking cosmic ray particles as they move through the Galactic magnetic field and interstellar medium. This method takes into account all key physical processes, including diffusion, convection, adiabatic deceleration and acceleration, nuclear fragmentation, and radioactive decay. We address the various uncertainties related to these processes, such as those concerning the strength and configuration of the Galactic magnetic field, by conducting multiple measurements with different constraints on each system. Additionally, we present preliminary results obtained from this code, which show its ability to replicate many of the observed characteristics of diffuse gamma-ray radiation resulting from interactions between gamma rays and gas in the Milky Way. The primary objective of this project is to develop a novel technique for exploring the origin and propagation of gamma radiation on cosmological scales. High-energy cosmic rays are thought to be generated at astrophysical shocks caused by supernova remnants or active galactic nuclei. These energetic particles then undergo diffusive propagation through space until they encounter matter or emission fields along their paths, producing secondary photons and neutrinos through hadronuclear interactions and inverse Compton scattering, respectively. Cosmic rays also significantly contribute to the total pressure support within stars and may influence galaxy formation rates. However, despite decades of theoretical research, substantial uncertainties remain regarding the mechanisms responsible for cosmic ray acceleration and their subsequent travel properties. This ambiguity has sparked ongoing debate about whether cosmic rays can provide enough pressure to sustain the standard rotation curves of spiral galaxies without contradicting observational evidence for the absence of bright matter halos around many nearby galaxies. To tackle these challenges, we have developed a new computational tool named GALPROP (Galaxy Propagator), which numerically solves the time-dependent transport equation that describes the evolution of cosmic ray distributions across three spatial dimensions under the influence of large-scale magnetic fields and turbulent motions. Our solution involves discretizing the distribution relation into a vast number of elements.",
        "ori-fast-z-score": 1.0377490433255416,
        "water-fast-z-score": 7.909667599213929,
        "rewrite-fast-z-score": 0.45226701686664544
    },
    {
        "original_text": "Epitaxial thin films of the multiferroic compound Bi2FeCrO 6 were grown on (001)-oriented SrTiO3 substrates by pulsed laser deposition at 750 °C in an oxygen partial pressure of 0.1 mbar and annealed for 30 min under vacuum conditions to induce ferroelectricity.  The structural properties of these epitaxial films are investigated using X-ray diffraction, transmission electron microscopy, scanning probe techniques as well as Raman spectroscopy. It is found that the films grow coherently strained along  001  direction with a tetragonal structure. A strong in-plane anisotropy between the out-of-plane lattice parameters c and a was observed which can be explained by different ionic radii of Fe 3+ , Cr 3+ and Ti 4+ . In addition, it could be shown that the films exhibit a rhombohedral-like distortion due to the presence of antiphase boundaries.",
        "watermark_text": "Epitaxial thin sheets of the multiferroic compound Bi2FeCrO 6 were cultivated on ( 001 ) - directed SrTiO3 substrates by pulsed infrared deposition at 750 °C in an oxygen partial pressure of 0 . 1 mbar and annealed for 30 min under vacuum environments to create ferroelectricity . The structural properties of these epitaxial films are examined utilizing X - ray diffraction , transmission electron microscopy , scanning probe methods as well as Raman spectroscopy .It is found that the films increase coherently strained along 001 direction with a tetragonal shape . A strong in - plane anisotropy between the out - of - plane lattice parameters c and a was seen which can be described by various ionic radii of Fe 3 + , Cr 3 + and Ti 4 + .In addition , it could be shown that the films show a rhombohedral - like degradation due to the presence of antiphase borders .",
        "rewrite_text": "Epitaxial thin films of the multiferroic compound Bi2FeCrO6 were grown on (001)-oriented SrTiO3 substrates using pulsed infrared deposition at 750 °C under an oxygen partial pressure of 0.1 mbar, followed by a 30-minute annealing in a vacuum to induce ferroelectricity. The structural characteristics of these epitaxial films were analyzed employing X-ray diffraction, transmission electron microscopy, scanning probe techniques, and Raman spectroscopy. The results indicate that the films exhibit coherent strain along the 001 direction and adopt a tetragonal structure. A significant in-plane anisotropy between the out-of-plane lattice parameters c and a was observed, which can be attributed to the varying ionic radii of Fe3+, Cr3+, and Ti4+. Furthermore, it was demonstrated that the films exhibit rhombohedral-like degradation due to the presence of antiphase boundaries.",
        "ori-fast-z-score": -1.9051586888313607,
        "water-fast-z-score": 2.9938207967349952,
        "rewrite-fast-z-score": -1.632993161855452
    },
    {
        "original_text": "We present new spectroscopic observations of galaxies at z ~ 1.5-2.0 selected by their UVJ colors and optical morphologies, obtained with VLT/VIMOS on the Very Large Telescope (VLT). We find that these objects are mostly early-type galaxies showing signs of recent star formation activity. The observed properties suggest that they may be progenitors of local massive elliptical galaxies. These results provide further evidence supporting the scenario where most massive galaxies grow through mergers between gas-rich disk systems during the first half of cosmic time. This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium provided the original work is properly cited. \n \n Keywords: galaxy evolution; merger remnants; young ellipticals; CDF-S field \n \n Massive galaxies evolve rapidly over cosmic time as a result of merging processes involving smaller companions. In particular, it has been suggested that many of today s brightest cluster galaxies were formed via major mergers of two or more gas-rich disks at redshifts around one to three  1  . However, direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift  2  .\n \nIn order to study the physical mechanisms driving galaxy growth we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph  3  . Our sample consists of about 100 galaxies selected based on their ultraviolet J (UVJ) color  4  , morphological type  5  , and apparent magnitude  6  . Most of them show strong emission lines characteristic of active star-forming regions  7, 8  . Their stellar masses range from 10^10 M_sol to 10^11 M_sol  9  . \n\n\nThe main goal of our project was to identify possible candidates for progenitor populations of local massive elliptical/S0 galaxies  10  . To do so, we used several selection criteria designed to select galaxies with similar characteristics to those found among nearby massive spheroids  11  : \n\n\n1. Morphological type: all targets must",
        "watermark_text": "We report new spectroscopic observations of clusters at z ~ 1 . 5 - 2 . 0 selected by their UVJ colors and optical morphologies , obtained with VLT / VIMOS on the Very Large Telescope ( VLT ) . We see that these objects are mostly early - class stars displaying signs of recent star formation activity .The observed properties suggest that they may be progenitors of local heavy elliptical galaxies . These data provide further evidence supporting the scenario where most large galaxies grow through mergers between gas - rich disk systems during the first half of cosmic time .This is an Open Access article distributed under the terms of the Creative Commons Attribution License 2 . 0 , which allows unrestricted use , distribution , and reproduction in any medium provided the original book is properly cited . Keywords : galaxy evolution ; collision remnants ; young ellipticals ; CDF - S field Massive stars develop rapidly over cosmic time as a product of combining processes involving smaller companions .In particular , it has been proposed that several of today s brightest cluster objects were created via large mergers of two or more gas - rich disks at redshifts around one to three 1 . However , direct observational evidence for this process remains elusive because of the difficulty in identifying such events at high redshift 2 .In order to study the physical mechanisms governing galaxy formation we have carried out deep spectroscopy of galaxies at intermediate redshifts using the VLT - VIMOS spectrograph 3 . Our specimen consists of about 100 galaxies chose based on their ultraviolet J ( UVJ ) color 4 , morphological class 5 , and apparent magnitude 6 .Most of them show intense emission lines typical of active star - creating areas 7 , 8 . Their stellar masses range from 10 ^ 10 M _ sol to 10 ^ 11 M _ sol 9 .The main goal of our work was to identify possible nominees for progenitor populations of local heavy elliptical / S0 galaxies 10 . To do so , we using numerous selection categories modified to select clusters with similar characteristics to those detected among neighboring massive spheroids 11 : 1 .Morphological type : all targets must",
        "rewrite_text": "We present new spectroscopic observations of galaxy clusters at redshifts around 1.5 to 2.0, selected based on their UVJ colors and optical morphologies, obtained using the VLT/VIMOS on the Very Large Telescope (VLT). Our findings indicate that these galaxies are predominantly early-type stars showing evidence of recent star formation activity. The characteristics observed suggest they could be the progenitors of present-day massive elliptical galaxies. These observations add further support to the hypothesis that most large galaxies evolve through mergers of gas-rich disk systems during the early phases of cosmic history. This article is published under Open Access terms of the Creative Commons Attribution License 2.0, which permits unrestricted use, distribution, and reproduction in any medium, provided the original source is cited appropriately. Keywords: galaxy evolution; collision remnants; young ellipticals; CDF-S field. \n\nMassive stars evolve quickly over cosmic time as a result of interactions with smaller companions. It has been theorized that many of today’s prominent cluster objects were formed through major mergers of two or more gas-rich disks at redshifts of about one to three. Nonetheless, obtaining direct observational evidence for this phenomenon has proven challenging due to the difficulties in detecting such events at high redshifts. To investigate the underlying physical processes of galaxy formation, we conducted deep spectroscopy of galaxies at intermediate redshifts using the VLT-VIMOS spectrograph. Our sample comprises approximately 100 galaxies selected based on their ultraviolet J (UVJ) colors, morphological classifications, and apparent magnitudes. The majority exhibit strong emission lines typical of active star-forming regions, with stellar masses ranging from 10^10 M_sol to 10^11 M_sol. The primary objective of our study was to identify potential progenitor populations of local heavy elliptical/S0 galaxies. To achieve this, we utilized various selection criteria aimed at isolating clusters with characteristics similar to those of nearby massive spheroids.",
        "ori-fast-z-score": 0.16116459280507606,
        "water-fast-z-score": 6.98800816145174,
        "rewrite-fast-z-score": 0.08192319205190406
    },
    {
        "original_text": "We discuss the concept of particle number in quantum mechanics, with particular emphasis on its role as an indicator for quantum indistinguishability between particles. We show that this is not always true by considering two examples where it does not hold. The first example involves fermions which are distinguishable but have different spin states; we argue that these should be considered to belong to distinct species rather than being regarded as identical particles. In our second example, we consider bosons which can occupy the same state (e.g., photons) but are nevertheless distinguishable; here too they do not constitute a single species. Finally, we present arguments against identifying particle number with entropy or information content. This article is part of a series of articles intended to introduce some key concepts in physics through informal discussions. It may also serve as a useful guide for students preparing their own presentations. For more detailed explanations see the original papers cited below. \n \n Quantum mechanics has been developed over many decades since its birth at the beginning of 20th century. One of the most important aspects of quantum mechanics is the concept of particle number. Particles are fundamental building blocks of matter and energy. They come in various forms such as electrons, protons, neutrons, atoms, molecules etc.. Each type of particle carries certain properties like charge, mass, spin etc... \n \n What exactly is particle number? How is it related to other physical quantities? Why is it so important? These questions will be answered in this article.",
        "watermark_text": "We discuss the notion of particle number in particle theory , with particular stress on its role as an measure for quantum indistinguishability between particles . We see that this is not always true by examining two examples where it does not hold .The first instance involves fermions which are distinguishable but have different spinning states ; we claim that these should be regarded to belong to distinct species rather than being regarded as identical particles . In our second example , we treat bosons which can occupy the same state ( e . g . , photons ) but are nevertheless distinguishable ; here too they do not constitute a single species .Finally , we present arguments against identifying particle number with entropy or information content . This section is part of a string of papers meant to introduce some important concepts in physics through informal talks .It might additionally help as a helpful guide for students preparing their own presentations . For more complete explanations view the original papers cited below .Quantum theory has been refined over numerous years since its establishment at the beginning of 20th millennium . One of the most important elements of quantum mechanics is the notion of particle number .Particles are fundamental construction foundations of matter and energy . They come in different shapes such as atoms , protons , neutrons , atoms , atoms etc . . Each sort of particle contains certain characteristics like charge , mass , spin etc . . . What really is particle size ?How is it linked to other physical substances ? Why is it so significant ?These questions will be answered in this article .",
        "rewrite_text": "In this article, we explore the concept of particle number within particle theory, emphasizing its importance as a measure of quantum indistinguishability among particles. However, our examination reveals that this notion is not universally applicable. For instance, we first consider fermions that are distinguishable due to their differing spin states; we argue that these should be classified as separate species rather than identical particles. Next, we analyze bosons, such as photons, which are capable of occupying the same quantum state but can still be distinguished from one another; again, this indicates that they do not form a single species. Furthermore, we present arguments against equating particle number with entropy or information content. This discussion is part of a series of papers aimed at introducing key concepts in physics through informal discussions, which may also serve as a useful resource for students preparing their own presentations. For a more thorough understanding, readers are encouraged to consult the original papers referenced below. Since its inception in the early 20th century, quantum theory has been significantly refined, with particle number standing out as one of its fundamental components. Particles are the basic building blocks of matter and energy, existing in various forms, such as atoms, protons, and neutrons, each possessing unique properties like charge, mass, and spin. This raises important questions: What exactly constitutes particle number? How does it relate to other physical entities? And why is it so vital? We aim to address these inquiries in this article.",
        "ori-fast-z-score": -0.7242859683401482,
        "water-fast-z-score": 7.242859683401482,
        "rewrite-fast-z-score": -1.9250668437592438
    },
    {
        "original_text": "We study the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial-vector or tensor meson in the framework of QCD factorization with generalized form factors at large recoil.  We find that, although the branching fractions are small due to the helicity suppression, these processes can be used as probes of new physics beyond the Standard Model through their CP asymmetries. \nPACS numbers: 11.15.Tk, 12.38.Qk, 13 .25.Hw \nI. INTRODUCTORY REMAR K\nIn this work we will consider the following two types of charmless hadronic:  B → S V (S = P , A 0 ;V = T 1 )andB → SV(S=P;V=A1). The first type is characterized by one light quark in the final state while the second has no light quarks in it. In both cases there is only one spectator quark which leads to a helicity suppression of the corresponding decay rates. However, they may still serve as useful probes of new physics since their CP-violating asymmetries could be enhanced significantly compared to those of other modes  1  .\nTheoretically, such decays have been studied within various approaches including naive factorization  2  , perturbative QCD  3  , soft-collinear effective theory  4  , and QCD factorization  5  -  8  . It was found that the predictions based on different methods differ substantially among themselves. For example, using naive factorization, Ref.  2  predicted Br(B − →K * 0 π − )/Br(B − →Kπ)=0.27 ±0.04, whereas Refs.  6, 7  obtained values around 0.1−0.2. This discrepancy indicates that more theoretical efforts should be made before drawing any definite conclusion about these decays.",
        "watermark_text": "We research the decay amplitudes for charmless hadronic B decays into a scalar meson and an axial - vector or vector meson in the framework of QCD factorization with generalized form factors at large recoil . We see that , although the branching fractions are small owing to the helicity suppression , these mechanisms can be used as probes of new dynamics beyond the Standard Model through their CP asymmetries .PACS numbers : 11 . 15 . Tk , 12 . 38 . Qk , 13 . 25 . Hw I . INTRODUCTORY REMAR K In this study we will explore the following two forms of charmless hadronic : B → S V ( S = P , A 0 ; V = T 1 ) andB → SV ( S = P ; V = A1 ) .The first sort is characterized by one light quark in the last position while the second has no light quarks in it . In both cases there is only one spectator quark which results to a helicity suppression of the associated decay rates .However , they may still provide as helpful probes of new theory since their CP - breaking asymmetries may be enhanced considerably compared to those of other modes 1 . Theoretically , such decays have been studied within various approaches including naive factorization 2 , perturbative QCD 3 , soft - collinear effective theory 4 , and QCD factorization 5 - 8 .It was shown that the estimates based on various methods varies dramatically among themselves . For instance , using naive factorization , Ref .2 observed Br ( B − →K * 0 π − ) / Br ( B − →Kπ ) = 0 . 27 ±0 . 04 , whereas Refs . 6 , 7 obtained values around 0 . 1−0 . 2 .This discrepancy implies that more theoretical efforts should be made before drew any explicit conclusion about these decays .",
        "rewrite_text": "We investigate the decay amplitudes for charmless hadronic B decays into a scalar meson and either an axial-vector or vector meson, using the QCD factorization framework with generalized form factors in the case of large recoil. Our findings reveal that although the branching fractions are small due to helicity suppression, these decay modes can serve as valuable probes for potential new dynamics beyond the Standard Model, particularly through their CP asymmetries. \n\nIn this paper, we will examine two specific forms of charmless hadronic decays: \\( B \\to SV \\) (with \\( S = P, A_0 \\) and \\( V = T_1 \\)) and \\( B \\to SV \\) (with \\( S = P \\) and \\( V = A_1 \\)). The first category features one light quark in the final state, while the second contains no light quarks. In both scenarios, the presence of a single spectator quark leads to helicity suppression in the decay rates. Nevertheless, these decays could provide significant insights regarding new theoretical frameworks since their CP-violating asymmetries may be notably enhanced compared to those of other decay modes.\n\nTheoretically, these decays have been analyzed using several approaches, including naive factorization, perturbative QCD, and soft-collinear effective theory, as well as QCD factorization. Previous estimates have shown considerable variability depending on the method employed. For instance, naive factorization used in reference 2 found \\( Br(B^- \\to K^{*0} \\pi^-) / Br(B^- \\to K \\pi) = 0.27 \\pm 0.04 \\), while references 6 and 7 reported values around 0.1 to 0.2. This inconsistency highlights the need for further theoretical investigation before reaching definitive conclusions about these decay processes.",
        "ori-fast-z-score": 0.3375263702778072,
        "water-fast-z-score": 6.484597134749389,
        "rewrite-fast-z-score": 1.744163198544762
    },
    {
        "original_text": "We report on the detection of suppressed radio emission in two interacting galaxy clusters, Abell 2146 and Abell 2390 (z = 0.23). The cluster pair is part of an extended X-ray filamentary structure that extends over ~30 Mpc along the line-of-sight. We find that the fraction of radio-detected galaxies decreases with increasing projected distance to the nearest cluster center for both clusters individually as well as when combining all galaxies within the entire filamentary structure. This trend cannot be explained by differences in stellar mass or SFR between galaxies located at different distances from their host cluster centers. Instead we argue that it can be understood if the gas density increases towards the cluster cores due to gravitational compression and/or shock heating during cluster mergers. In this case, the increased ram pressure would suppress star formation activity in galaxies residing close to the cluster core. Our results are consistent with previous studies showing that the fraction of star-forming galaxies decreases rapidly with decreasing cluster-centric radius.",
        "watermark_text": "We report on the discovery of suppressed radio emission in two interacting galaxy regions , Abell 2146 and Abell 2390 ( z = 0 . 23 ) . The cluster couple is part of an extended X - ray filamentary structure that extends over ~ 30 Mpc along the line - of - view .We see that the fraction of radio - detected galaxies decreases with increasing projected altitude to the nearest cluster center for both clusters separately as well as when combining all galaxies within the entire filamentary structure . This trend cannot be described by differences in stellar size or SFR between nuclei located at different distances from their host cluster sites .Instead we feel that it can be understood if the gas density increases towards the cluster cores due to gravitational fusion and / or shock heating during core mergers . In this example , the increased ram tension may reduce galaxy formation activity in galaxies residing nearest to the cluster core .Our results are compatible with previous research indicating that the fraction of galaxy - creating stars reduces rapidly with decreasing cluster - centric radius .",
        "rewrite_text": "We report the discovery of suppressed radio emission in two interacting galaxy regions, Abell 2146 and Abell 2390 (z = 0.23). These clusters are part of an extensive X-ray filamentary structure that stretches approximately 30 Mpc along the line of sight. Our observations reveal that the proportion of radio-detected galaxies decreases as the projected distance to the nearest cluster center increases, both for each cluster individually and when considering all galaxies within the entire filamentary network. This trend cannot be attributed to variations in stellar size or star formation rate (SFR) among nuclei located at differing distances from their respective cluster centers. Instead, we propose that the phenomenon can be explained by an increase in gas density towards the cluster cores, likely due to gravitational contraction and/or shock heating during core mergers. In this context, the heightened ram pressure may inhibit galaxy formation activity in those galaxies situated closest to the cluster core. Our findings align with previous studies that suggest a rapid decline in the fraction of star-forming galaxies as one moves away from the cluster center.",
        "ori-fast-z-score": -0.6324555320336759,
        "water-fast-z-score": 5.136595699937399,
        "rewrite-fast-z-score": 0.5184758473652127
    },
    {
        "original_text": "We present new numerical schemes to solve the Einstein equations in Kerr-Schild coordinates with multiple black holes and an outer boundary, which are based on high order accurate finite difference discretizations combined with efficient solvers using multigrid techniques. The main difficulty is that the computational domain contains several disconnected components (holes) where no grid points can be placed due to the presence of singularities at the locations of the black holes. We show how this problem can be overcome by introducing ghost points outside each component and applying appropriate boundary conditions there. In addition we use a special treatment near the horizon of each black hole to avoid problems related to coordinate singularities. Finally, we demonstrate our method by computing stationary axisymmetric solutions corresponding to two equal mass black holes located along the z-axis. Our results agree very well with previous computations obtained with other codes. This work was supported by NSF grant PHY-0555611.",
        "watermark_text": "We create novel numerical schemes to solve the Einstein equations in Kerr - Schild coordinates with many black holes and an outer edge , which are based on well order accurate finite difference discretizations combined with fast solvers using multigrid techniques . The main problems is that the theoretical domain contains multiple disconnected components ( holes ) where no grid points can be placed due to the presence of singularities at the places of the dark holes .We see how this question can be overcome by using ghost points outside each component and using appropriate boundary constraints there . In addition we using a unique treatment near the horizon of each dark hole to eliminate problems related to coordinate singularities .Finally , we prove our technique by modeling stationary axisymmetric solutions associated to two equal mass black holes located along the z - axis . Our results agree very well with previous computations achieved with other codes .This project was supported by NSF grant PHY - 0555611 .",
        "rewrite_text": "We develop innovative numerical methods to address the Einstein equations in Kerr-Schild coordinates, accommodating multiple black holes and an outer boundary. Our approach utilizes high-order accurate finite difference discretizations paired with efficient multigrid solvers. A major challenge arises from the theoretical domain, which contains several disconnected regions (holes) where grid points cannot be placed due to singularities at the locations of the black holes. We tackle this issue by implementing ghost points outside each component and applying suitable boundary conditions. Additionally, we employ a specialized treatment near the horizons of each black hole to resolve issues related to coordinate singularities. To demonstrate the effectiveness of our technique, we model stationary axisymmetric solutions for two equal-mass black holes situated along the z-axis. Our findings align closely with previous results obtained using other computational methods. This project received support from NSF grant PHY-0555611.",
        "ori-fast-z-score": 0.10976425998969035,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": -0.7683498199278324
    },
    {
        "original_text": "We present the results for charged hadron multiplicity density and transverse momentum fluctuations obtained by using the AMPT model with string melting mechanism at RHIC energies. The effect of initial parton distribution is studied by varying the initial temperature T0, which controls the initial entropy content of the system. We find that both the mean value and fluctuation of the multiplicity density increase as we decrease the initial temperature T0. This indicates that the initial state fluctuations play an important role in determining the final observables. It has been observed experimentally that the ratio of variance to mean square (V /M) increases with decreasing beam energy. In our study also it is found that V /M decreases with increasing initial temperature T0. However, this trend changes when we include the contribution due to resonance decays. Finally, we have calculated the scaled variances S2/S1 and S3/S2 for different values of T0.",
        "watermark_text": "We present the results for charged hadron multiplicity density and longitudinal momentum fluctuations derived by using the AMPT theory with string melting system at RHIC intensity . The impact of initial parton distribution is studied by varying the first temperature T0 , which influences the first entropy concentration of the system .We see that both the mean value and fluctuation of the multiplicity density increase as we decrease the initial temperature T0 . This implies that the first state fluctuations play an important role in determining the finished observables .It has been observed experimentally that the proportion of variance to mean square ( V / M ) increases with decreasing beam energy . In our research also it is found that V / M decreases with expanding initial temperature T0 .However , this decline changes when we include the contribution owing to resonance decays . Finally , we have adjusted the scaled variances S2 / S1 and S3 / S2 for different values of T0 .",
        "rewrite_text": "We present our findings on charged hadron multiplicity density and longitudinal momentum fluctuations, obtained through the AMPT theory using a string melting framework at RHIC intensity. The impact of the initial parton distribution is explored by varying the initial temperature \\( T_0 \\), which affects the initial entropy concentration of the system. Our results indicate that both the mean value and the fluctuations of the multiplicity density increase as \\( T_0 \\) decreases. This suggests that fluctuations in the initial state significantly influence the final observables. Experimental observations have shown that the ratio of variance to mean square (V/M) rises with decreasing beam energy. Our study also reveals that V/M decreases as the initial temperature \\( T_0 \\) increases. However, this trend changes when we account for contributions from resonance decays. Lastly, we have adjusted the scaled variances \\( S_2/S_1 \\) and \\( S_3/S_2 \\) for various values of \\( T_0 \\).",
        "ori-fast-z-score": -1.5882027766319677,
        "water-fast-z-score": 3.542913886332851,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "The Discrete Dipole Approximation (DDA) is used to simulate light scattering by large spherical particles in this study. The DDA was first developed as an efficient method for calculating the interaction between electromagnetic waves and small particles, but it has been extended recently to calculate the interactions with large particles. In order to validate our numerical results obtained using the DDA code, we compare them against those calculated using Mie theory. We find that both methods agree well when the size parameter x = 2πa/λ < 100 where λ is the incident wavelength and a is the radius of the particle. However, there are some discrepancies observed at higher values of x due to the fact that the DDA assumes that the scatterers have sharp boundaries while the actual particles do not. Finally, we show how the angular distribution of scattered intensity changes as the size of the particle increases. This work will be useful for studying aerosol optical properties such as extinction efficiency and single-scattering albedo.",
        "watermark_text": "The Discrete Dipole Approximation ( DDA ) is utilized to simulate light scattering by large spherical objects in this study . The DDA was first developed as an efficient model for calculating the interaction between electromagnetic signals and tiny particles , but it has been extended recently to estimate the interactions with large particles .In order to validate our numerical findings obtained using the DDA code , we compare them against those estimated using Mie theory . We see that both algorithms accord well when the size variable x = 2πa / λ < 100 where α is the incident wavelength and a is the radius of the particle .However , there are some discrepancies noted at higher values of x thanks to the fact that the DDA assumes that the scatterers have jagged boundaries while the actual particles do not . Finally , we study how the angular distribution of scattered intensity shifts as the height of the particle increases .This research will be valuable for studying aerosol imaging properties such as extinction efficiency and single - absorption albedo .",
        "rewrite_text": "In this study, we employ the Discrete Dipole Approximation (DDA) to simulate light scattering by large spherical objects. Originally developed as an efficient model for analyzing the interaction between electromagnetic waves and small particles, the DDA has recently been adapted to estimate interactions involving larger particles. To validate the numerical results obtained from our DDA code, we compare them with estimates derived from Mie theory. Our findings indicate that both methods yield consistent results when the size parameter \\( x = \\frac{2\\pi a}{\\lambda} < 100 \\), where \\( \\lambda \\) represents the incident wavelength and \\( a \\) denotes the particle radius. However, discrepancies arise at larger values of \\( x \\) due to the DDA's assumption that scatterers possess irregular boundaries, unlike the actual particles. Additionally, we investigate how the angular distribution of scattered intensity changes with increasing particle height. This research has significant implications for understanding aerosol imaging properties, including extinction efficiency and single-absorption albedo.",
        "ori-fast-z-score": -0.46499055497527714,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 0.7977240352174656
    },
    {
        "original_text": "We study the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium after being quenched across the superfluid-Mott insulator transition. We show that this system exhibits universal behavior at late times which is characterized by power-law decaying correlations and algebraic growth of entanglement entropy. The exponents are determined analytically using a mapping onto a classical statistical mechanics problem for a driven diffusive system. This work was supported by NSF grant PHY-0960291 (M.S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A.K.). \nI. INTRODUCTORY REMARkS\n\nThe recent experimental realization of quantum degenerate gases has opened up new avenues towards understanding strongly correlated many-body systems  1  . In particular, ultracold atomic gases have been used as model systems to explore phenomena such as fermionization  2  , supersolidity  3  , and Mott-insulating states  4  .\nIn this article we consider a particularly interesting class of experiments where the properties of these systems can be probed through their response to sudden changes in parameters  5  . For example, if the strength of inter-particle repulsion or density of particles is suddenly changed then it takes some time before the system reaches thermal equilibrium  6  . During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling  7, 8  and non-thermal fixed points  9  . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing novel phases of matter  10  .\nRecently there has been considerable interest in studying the nonequilibrium dynamics of bosonic systems  11  . A particularly well studied case is when the initial state corresponds to a highly excited state above the ground state  12  . It turns out that even though the initial state is far away from equilibrium, the system relaxes to a steady state described by a Gibbs ensemble  13  . However, if the initial state is prepared deep inside the ordered phase, then the system does not",
        "watermark_text": "We explore the dynamics of an interacting Bose gas with repulsive contact interactions in one dimension , concentrating on its relaxation to equilibrium after being quenched across the superfluid - Mott insulator transition . We see that this system displays universal behavior at late times which is characterized by power - law decaying correlations and algebraic growth of entanglement entropy .The exponents are estimated analytically using a mapping onto a traditional statistical mechanics problem for a driven diffusive system . This research was supported by NSF grant PHY - 0960291 ( M . S . )and DOE funds DE - FG03 - 92 - ER40701 and DE - SC0012704 ( A . K . ) . I .INTRODUCTORY REMARkS The recent experimental realization of quantum degenerate gases has opened up new avenues towards studying strongly interacting multiple - bodies systems 1 . In particular , ultracold nuclear gases have been used as model structures to examine processes such as fermionization 2 , supersolidity 3 , and Mott - insulating states 4 .In this article we imagine a particularly exciting group of studies where the properties of these systems can be probed through their response to unexpected changes in parameters 5 . For instance , if the strength of inter - atom repulsion or density of molecules is suddenly changed then it takes some time before the system reaches heat equilibrium 6 .During this nonequilibrium evolution , the system might exhibit new characteristics like dynamical scaling 7 , 8 and non - cooling fixed points 9 . These effects are not only important for our fundamental understanding of quantum matter but also provide useful insights into possible routes to realizing new phases of matter 10 .Recently there has been substantial interest in investigating the nonequilibrium dynamics of bosonic systems 11 . A notably well discussed case is when the first state corresponds to a highly excited state above the ground state 12 .It happens out that even though the first state is far back from equilibrium , the system relaxes to a steady state described by a Gibbs ensemble 13 . However , if the initial system is prepared deep inside the ordered phase , then the system does not",
        "rewrite_text": "We investigate the behavior of an interacting Bose gas with repulsive contact interactions in one dimension, focusing on its relaxation to equilibrium following a quench across the superfluid-Mott insulator transition. Our findings reveal that, at long times, the system exhibits universal characteristics marked by power-law decay of correlations and algebraic growth of entanglement entropy. We analytically estimate the exponents by mapping the problem onto a conventional statistical mechanics scenario involving a driven diffusive system. This research was supported by NSF grant PHY-0960291 (M. S.) and DOE grants DE-FG03-92-ER40701 and DE-SC0012704 (A. K.).\n\nI. INTRODUCTORY REMARKS\nThe recent achievements in realizing quantum degenerate gases have paved the way for new opportunities to study strongly interacting many-body systems. Ultracold atomic gases, in particular, have served as model systems for exploring phenomena such as fermionization, supersolidity, and Mott insulating states. In this article, we propose a particularly intriguing avenue of research that examines the properties of these systems in response to abrupt changes in parameters. For example, when the inter-atom repulsion strength or the density of the molecules is suddenly altered, it takes some time for the system to reach thermal equilibrium. During this nonequilibrium evolution, the system may exhibit novel features like dynamical scaling and non-cooling fixed points. These phenomena are not only crucial for enhancing our fundamental understanding of quantum matter but also offer valuable insights into potential pathways for achieving new phases of matter. Recently, there has been a significant surge of interest in studying the nonequilibrium dynamics of bosonic systems. A particularly well-explored scenario is when the system starts in a highly excited state, far removed from equilibrium, yet relaxes to a steady state characterized by a Gibbs ensemble. However, if the initial conditions find the system deeply entrenched within the ordered phase, the relaxation path significantly differs.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 7.175639059928206,
        "rewrite-fast-z-score": 1.2893167424406085
    },
    {
        "original_text": "We present new observational constraints on the cosmic ray (CR) energy density and its evolution with redshift, based on gamma-ray observations by Fermi/LAT in the range 0 < z < 1.5. We find that CRs contribute at most 10% to the total pressure budget of the universe at redshifts below 2. This upper limit is consistent with theoretical expectations for the contribution of CRs accelerated by supernovae. The results are also compatible with previous measurements using radio data. These limits can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or weak lensing. Cosmic rays (CRs), charged particles which fill space uniformly over large volumes, have been observed throughout our Galaxy and beyond. They play an important role in many astrophysical phenomena including galactic winds, star formation, and possibly even the acceleration of ultra-high-energy cosmic rays  1  . However, their origin remains unknown  2  .\nIn this work we use gamma-ray observations made by the Large Area Telescope (LAT) aboard the Fermi satellite  3  , to place tight constraints on the amount of CRs contributing to the overall pressure budget of the Universe  4  . In particular, we consider two different models for the CR distribution function f(p,z). First, we assume that it follows a power law spectrum dN/dE ~ E^{-alpha} between energies Emin = 10 GeV and Emax = 100 TeV; secondly, we adopt a broken power-law model where the spectral index changes from alpha1 = -2.2 to alpha2 = -3 above some break energy Eb = 50 GeV. For both cases, we fix the normalization factor A by requiring that the integral of f(p,z) over all momenta equals unity. \nThe resulting CR distributions are shown in Figure 1 . \nTo calculate the effect of these CR populations on the expansion history of the universe, we solve numerically the coupled system of equations describing the time-evolution of the background...",
        "watermark_text": "We introduce novel observational restrictions on the cosmic ray ( CR ) energy density and its progression with redshift , using on gamma - ray observations by Fermi / LAT in the range 0 < z < 1 . 5 . We see that CRs contribute at most 10 % to the total pressure budget of the universe at redshifts below 2 .This upper maximum is compatible with theoretical expectations for the impact of CRs accelerated by supernovae . The results are also consistent with previous measurements involving radio data .These restrictions can be used as priors when modeling the effects of CRs on cosmological observables such as galaxy clustering or strong lensing . Cosmic rays ( CRs ) , charged particles which fill space uniformly over large quantities , have been observed throughout our Galaxy and beyond .They play an important role in different astrophysical processes including galactic winds , sun formation , and maybe even the acceleration of ultra - large - energy cosmic rays 1 . However , their source remains unidentified 2 .In this research we utilize gamma - ray observations made by the Large Area Telescope ( LAT ) aboard the Fermi satellite 3 , to place secure constraints on the proportion of CRs causing to the overall pressure budget of the Universe 4 . In particular , we consider two different models for the CR distribution relation h ( p , z ) .First , we suppose that it takes a power law spectrum dN / dE ~ E ^ { - alpha } between frequencies Emin = 10 GeV and Emax = 100 TeV ; secondly , we adopt a broken power - law description where the spectral index shifts from alpha1 = - 2 . 2 to alpha2 = - 3 above some broken power Eb = 50 GeV . For both cases , we solve the normalization factor A by requiring that the integral of f ( p , z ) over all momenta equals unity .The resulting CR variables are shown in Figure 1 . To estimate the impact of these CR populations on the expansion history of the universe , we solve numerically the coupled system of equations explaining the period - progression of the background . . .",
        "rewrite_text": "We present new observational constraints on the energy density of cosmic rays (CRs) and their evolution with redshift, utilizing gamma-ray observations from the Fermi Large Area Telescope (LAT) within the redshift range of 0 < z < 1.5. Our findings indicate that CRs contribute at most 10% to the universe's total pressure budget at redshifts below 2, which aligns well with theoretical predictions regarding CRs accelerated by supernovae. These results are also consistent with earlier measurements that used radio data. Our constraints can serve as priors for modeling the influence of CRs on cosmological observables, including galaxy clustering and strong lensing. Cosmic rays, which are charged particles distributed uniformly across vast regions, have been detected throughout our Galaxy and beyond. They are pivotal in various astrophysical processes, such as galactic winds, star formation, and possibly in the acceleration of ultra-high-energy cosmic rays. However, their origins remain unknown. In this study, we employ gamma-ray data from the Fermi LAT to establish robust limits on the fraction of CRs contributing to the universe's overall pressure budget. Specifically, we analyze two models for the CR distribution relation h(p, z). In the first model, we assume a power-law spectrum dN/dE ~ E^{-α} over the energy range from 10 GeV to 100 TeV. In the second model, we use a broken power-law approach where the spectral index transitions from α1 = -2.2 to α2 = -3 above a break energy of 50 GeV. For both models, we determine the normalization factor A by ensuring that the integral of f(p, z) across all momenta equals unity. The resulting CR parameters are illustrated in Figure 1. To assess the influence of these CR populations on the universe's expansion history, we numerically solve the coupled equations that describe the evolution of the background.",
        "ori-fast-z-score": 1.0215078369104984,
        "water-fast-z-score": 8.342314001435737,
        "rewrite-fast-z-score": 1.4967665407535604
    },
    {
        "original_text": "We present an analysis of the X-ray properties of a sample of 12 hyper-luminous infrared galaxies (HLIRGs) observed with XMM-Newton, using data obtained in AO-1 and AO-2. The HLIRG sample is selected to have L(8-1000um)>10^12L_sun , where L(8-1000um), is derived by integrating over the best-fit SEDs for each source. We find that all sources are detected at >5 sigma significance in the 0.3-10 keV band; however only two objects show evidence for significant absorption above Galactic levels. For these two absorbed systems we derive column densities NH = 1.7 x 10^23 cm^{-2} and 2.1 x 10^22 cm^{-2} respectively. Using the hardness ratio HR=H-S/H+S, where H and S represent counts in the 3-7keV and 0.3-2keV bands respectively, we find no correlation between HR and either luminosity or redshift. This suggests that there may be little evolution in the intrinsic spectral shape of this population out to z=2.6.",
        "watermark_text": "We present an assessment of the X - ray characteristics of a sample of 12 hyper - luminous infrared galaxies ( HLIRGs ) detected with XMM - Newton , using data acquired in AO - 1 and AO - 2 . The HLIRG sample is selected to have L ( 8 - 1000um ) > 10 ^ 12L _ sun , where L ( 8 - 1000um ) , is calculated by combining over the best - fitting SEDs for each source .We see that all sources are detected at > 5 sigma significance in the 0 . 3 - 10 keV band ; however only two bodies exhibit indication for significant absorption above Galactic concentrations . For these two reflected systems we derive column densities NH = 1 . 7 x 10 ^ 23 centimetres ^ { - 2 } and 2 . 1 x 10 ^ 22 mm ^ { - 2 } respectively .Using the hardness factor HR = H - S / H + S , where H and S represent counts in the 3 - 7keV and 0 . 3 - 2keV bands respectively , we find no correlation between HR and either luminosity or redshift . This implies that there may be little development in the intrinsic spectral structure of this community out to z = 2 . 6 .",
        "rewrite_text": "We provide an analysis of the X-ray properties of a sample of 12 hyper-luminous infrared galaxies (HLIRGs) detected with XMM-Newton, utilizing data from AO-1 and AO-2. The selected HLIRG sample has an infrared luminosity, L (8 - 1000 µm), greater than 10^12 L_sun, calculated using the best-fitting spectral energy distributions (SEDs) for each galaxy. All sources in the sample are detected with greater than 5 sigma significance in the 0.3 - 10 keV band; however, only two show signs of significant absorption beyond Galactic levels. For these two sources, we measure column densities of NH = 1.7 x 10^23 cm^-2 and 2.1 x 10^22 cm^-2, respectively. By employing the hardness ratio HR = (H - S) / (H + S), where H and S represent count rates in the 3 - 7 keV and 0.3 - 2 keV bands, respectively, we observe no correlation between HR and either luminosity or redshift. This suggests that there is minimal change in the intrinsic spectral properties of this population up to z = 2.6.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 5.598123172175427,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "We present the results of first-principles calculations for structural, electronic, magnetic properties of LaMnO3 in its rhombohedral phase (R3c). We show that the local spin density approximation (LSDA) fails to reproduce correctly both the lattice constants and the band gap energy. The latter is underestimated by more than one order of magnitude as compared with experiment. In contrast, our self-consistent full-potential linearized augmented plane wave method gives good agreement between calculated and experimental values of these quantities. \n \n To understand better the origin of this discrepancy we have performed additional calculations using an effective tight-binding approach based on Wannier functions obtained within the framework of the LSDA+U formalism. Our analysis shows that the main reason why the LSDA fails to describe properly the electronic structure of LaMnO3 is due to strong hybridization effects which are not taken into account adequately within the standard LSDA scheme.",
        "watermark_text": "We present the conclusion of first - principles measurements for structural , electronic , magnetic properties of LaMnO3 in its rhombohedral phase ( R3c ) . We see that the local spin density algorithm ( LSDA ) fails to predict correctly both the crystal constants and the band gap energy .The latter is underestimated by more than one order of magnitude as compared with observation . In comparison , our self - consistent full - potential linearized augmented plane wave method gives excellent agreement between calculated and observation readings of these quantities .To understand better the origin of this discrepancy we have done additional calculations using an efficient tight - binding approach using on Wannier functions obtained within the framework of the LSDA + U formalism . Our study shows that the main explanation why the LSDA fails to explain adequately the optical composition of LaMnO3 is due to powerful hybridization factors which are not took into consideration appropriately within the standard LSDA scheme .",
        "rewrite_text": "We present our findings from first-principles measurements concerning the structural, electronic, and magnetic properties of LaMnO3 in its rhombohedral phase (R3c). Our results indicate that the local spin density approximation (LSDA) inadequately predicts both the crystal constants and the band gap energy, underestimating the latter by over an order of magnitude compared to experimental observations. In contrast, our self-consistent full-potential linearized augmented plane wave method demonstrates excellent agreement between calculated values and observed data for these properties. To gain a deeper understanding of this discrepancy, we conducted further calculations using an efficient tight-binding approach based on Wannier functions derived from the LSDA + U framework. Our analysis reveals that the primary reason for the LSDA's inability to accurately represent the optical properties of LaMnO3 lies in significant hybridization effects that are not appropriately addressed within the conventional LSDA framework.",
        "ori-fast-z-score": -0.9299811099505543,
        "water-fast-z-score": 4.4174102722651325,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We study plasma modes along open field lines in neutron stars using kinetic theory and numerical simulations. We find that there are two types of instabilities, one driven by parallel electric fields (the electron firehose instability) and another driven by perpendicular magnetic fields (the mirror instability). The growth rates for these instabilities depend on the local density gradient scale length as well as the strength of the background magnetic field. For typical parameters expected near the polar cap region of a pulsar we show that both instabilities can grow rapidly enough to be important for particle acceleration processes at the stellar surface. \n \n Keywords: Plasma physics; Kinetic theory; Instability; Pulsar emission mechanism; Acceleration mechanisms; Mirror instability; Firehose instability; Polar cap acceleration; Magnetosphere; Neutrino emission; Magnetic reconnection; Force-free magnetohydrodynamics; Astrophysics",
        "watermark_text": "We research plasma mechanisms along open field lines in neutron galaxies using kinetic theory and mathematical simulations . We see that there are two forms of instabilities , one driven by perpendicular electric forces ( the electron firehose instability ) and another driven by perpendicular magnetic fields ( the mirror instability ) .The growth rates for these instabilities depend on the local concentration gradient scale length as also as the strength of the background magnetic force . For typical values expected near the polar cap region of a pulsar we find that both instabilities can develop rapidly sufficiently to be crucial for particle acceleration processes at the stars surface .Keywords : Plasma mechanics ; Kinetic physics ; Instability ; Pulsar radiation mechanism ; Acceleration mechanisms ; Mirror instability ; Firehose instability ; Polar cap acceleration ; Magnetosphere ; Neutrino emission ; Magnetic reconnection ; Force - free magnetohydrodynamics ; Astrophysics",
        "rewrite_text": "We investigate plasma mechanisms along open field lines in neutron stars using kinetic theory and mathematical simulations. Our findings reveal two types of instabilities: one induced by perpendicular electric forces, known as the electron firehose instability, and the other caused by perpendicular magnetic fields, termed the mirror instability. The growth rates of these instabilities are influenced by the local concentration gradient scale length and the intensity of the background magnetic field. In typical conditions expected near the polar cap region of a pulsar, we observe that both instabilities can develop quickly enough to play a significant role in particle acceleration processes at the star's surface. \n\nKeywords: Plasma mechanics; Kinetic physics; Instability; Pulsar radiation mechanism; Acceleration mechanisms; Mirror instability; Firehose instability; Polar cap acceleration; Magnetosphere; Neutrino emission; Magnetic reconnection; Force-free magnetohydrodynamics; Astrophysics.",
        "ori-fast-z-score": -1.4770978917519928,
        "water-fast-z-score": 3.938927711338647,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "We present new near-infrared (NIR) and millimeter-wave observations of the starless dense core FeSt 1-457, which is located in the Taurus molecular cloud complex at a distance of 140 pc. The NIR data were obtained with the Subaru telescope using the SofI instrument on 2005 May 24-25 UT. We detected two sources within the central 0.5 arcmin region; one source was found to be associated with an infrared dark cloud (IRDC), while another source was not. Both sources are embedded deeply inside the dusty envelope surrounding the dense core. In addition, we observed this object simultaneously with the Nobeyama 45 m radio telescope at 1 mm wavelength during the same night as our NIR observation. No significant emission line features were seen in either spectrum. Using these observational results, we discuss possible scenarios for the formation of stars in such a young dense core.",
        "watermark_text": "We report new near - infrared ( NIR ) and millimeter - wave studies of the starless rich core FeSt 1 - 457 , which is situated in the Taurus molecular mist complex at a distance of 140 pc . The NIR data were obtained with the Subaru observatory using the SofI camera on 2005 May 24 - 25 UT .We observed two sources within the inner 0 . 5 arcmin region ; one source was reported to be involved with an infrared shadow cloud ( IRDC ) , while another source was not . Both sources are lodged deeply inside the dusty envelope surrounding the dense core .In addition , we studied this body simultaneously with the Nobeyama 45 m radio telescope at 1 mm frequency during the same night as our NIR observation . No notable emission line characteristics were witnessed in either spectrum .Using these observational results , we explain possible strategies for the formation of stars in such a young dense core .",
        "rewrite_text": "We present new near-infrared (NIR) and millimeter-wave observations of the starless rich core FeSt 1-457, located in the Taurus molecular cloud complex at a distance of 140 parsecs. The NIR data were collected on May 24-25, 2005, using the SofI camera at the Subaru Observatory. Within the inner 0.5 arcminute region, we identified two sources; one was associated with an infrared dark cloud (IRDC), while the other was not. Both sources are situated deep within the dusty envelope encompassing the dense core. Additionally, we conducted simultaneous observations with the Nobeyama 45-meter radio telescope at a 1 mm frequency on the same night. However, we did not detect any significant emission line features in either spectrum. Based on these findings, we propose potential mechanisms for star formation within such a young dense core.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 1.1920791213585393
    },
    {
        "original_text": "We present an analysis of the distribution of gas, stars and dust in two nearby edge-on spirals with prominent bars (NGC 1365 and NGC 1530). We use high-resolution observations obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems. The main results are as follows:  - In both cases we find that the molecular hydrogen is concentrated on the leading edges of the bar, while atomic hydrogen follows closely the stellar light.  - The star formation rate peaks at the ends of the bar where the density of molecular hydrogen increases significantly. This suggests that the gravitational torques induced by the bar can trigger the collapse of dense clouds into new generations of young stars.  - The infrared emission associated with polycyclic aromatic hydrocarbons shows a clear correlation between the location of this component and the regions of active star formation. - The comparison of our data with hydrodynamical simulations indicates that the observed structure of the ISM may be explained if the bar potential has been able to drive significant amounts of cold gas towards its inner Lindblad resonance.",
        "watermark_text": "We present an assessment of the distribution of gas , stars and dust in two distant edge - on spirals with prominent bars ( NGC 1365 and NGC 1530 ) . We use large - resolution measurements obtained by the Herschel Space Observatory to study the physical conditions of the interstellar medium along these systems .The main results are as follows : - In both cases we find that the molecular hydrogen is confined on the led corners of the bar , while nuclear hydrogen takes closely the stellar radiation . - The galaxy formation rate peaks at the ends of the bar where the density of molecular hydrogen rises considerably .This implies that the gravitational torques induced by the bar can cause the decay of dense clouds into new generations of young stars . - The infrared absorption associated with polycyclic aromatic hydrocarbons indicates a clear correlation between the location of this constituent and the regions of active star formation .- The comparison of our information with hydrodynamical simulations indicates that the known composition of the ISM may be described if the bar potential has been able to drive considerable portions of cold gas towards its internal Lindblad resonance .",
        "rewrite_text": "We provide an evaluation of the distribution of gas, stars, and dust in two distant edge-on spiral galaxies featuring prominent bars, specifically NGC 1365 and NGC 1530. Utilizing high-resolution data from the Herschel Space Observatory, we analyze the physical conditions of the interstellar medium within these galaxies. Our main findings include: - In both galaxies, molecular hydrogen is concentrated at the leading edges of the bar, while nuclear hydrogen closely follows the stellar radiation. - The star formation rate peaks at the ends of the bar, where there is a significant increase in molecular hydrogen density. This suggests that the gravitational torques generated by the bar may facilitate the transformation of dense gas clouds into new generations of young stars. - We observe a strong correlation between the infrared absorption linked to polycyclic aromatic hydrocarbons and regions undergoing active star formation. - Additionally, a comparison of our findings with hydrodynamical simulations indicates that the known composition of the interstellar medium can be explained if the bar potential has effectively driven substantial amounts of cold gas toward its internal Lindblad resonance.",
        "ori-fast-z-score": 1.4924050144892729,
        "water-fast-z-score": 6.88998622004134,
        "rewrite-fast-z-score": 1.6681153124565982
    },
    {
        "original_text": "We study the effect of local optical phonons on the electronic transport properties of a quantum dot system by using the nonequilibrium Green s function method combined with the density functional theory (DFT). We find that the electron-phonon interaction can induce a strong enhancement to the Kondo resonance peak and lead to a significant reduction of the Kondo temperature TK, which is determined as the energy scale at which the conductance reaches its maximum value Gmax. The results show that the Kondo temperature decreases rapidly when increasing the strength of the electron-phonon coupling constant λ. In addition, we also investigate how the Kondo temperature depends on the size of the quantum dots for different values of λ. Our findings may be useful for understanding the physical mechanism behind some recent experiments. Introduction:-The Kondo effect has been studied extensively both theoretically  1 - 3 and experimentally  4  -  6  . It occurs due to the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level  7, 8  , leading to a sharp zero-bias anomaly in the differential conductance  9  . Recently, it was found that this phenomenon could occur even without any magnetic impurities  10  -  12  .\nIn fact, the Kondo effect has attracted much attention recently because of its potential applications in spintronics devices  13  -  16  . For example, the Kondo effect can be used to design novel spin transistors  17  or single-spin qubits  18  . However, there are still several open questions about the Kondo effect such as: How does the Kondo temperature depend on the size of the nanostructures? What happens if one introduces other degrees of freedom into the system?\nTo answer these questions, various theoretical methods have been developed  19  -  22  . Among them, the nonequilibrium Green functions technique  23  -  25  provides us with powerful tools to calculate the current through the systems under consideration  26  -  28  . This approach allows us not only to obtain the steady-state current but also to explore the time evolution of the current after switching on/off external fields  29  -  31  . Moreover, combining the nonequilibrium Green",
        "watermark_text": "We research the impact of local optical phonons on the electronic transport properties of a quantum dot network by using the nonequilibrium Green s function method combined with the density functional theory ( DFT ) . We see that the electron - phonon interaction can induce a weak enhancement to the Kondo resonance peak and lead to a substantial drop of the Kondo temperature TK , which is calculated as the power range at which the conductance reaches its highest value Gmax .The results show that the Kondo temperature falls fast when increasing the strength of the electron - phonon coupling constant κ . In addition , we also investigate how the Kondo temperature varies on the size of the quantum dots for different values of λ .Our findings may be valuable for studying the physical process behind some latest studies . Introduction : - The Kondo phenomenon has been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 .It happens due to the formation of a many - bodies singlet state between localized magnetic moments and conduction electrons near the Fermi level 7 , 8 , leading to a sharp zero - bias anomaly in the differential conductance 9 . Recently , it was shown that this phenomenon might arise even without any magnetic impurities 10 - 12 .In indeed , the Kondo phenomenon has garnered considerable scrutiny lately because of its potential applications in spintronics devices 13 - 16 . For instance , the Kondo phenomenon can be used to build novel spin transistors 17 or single - spinning qubits 18 .However , there are still several open questions about the Kondo phenomenon such as : How does the Kondo temperature depend on the size of the nanostructures ? What happens if one introduces other degrees of liberty into the system ?To answer these problems , various theoretical methods have been used 19 - 22 . Among them , the nonequilibrium Green functions method 23 - 25 offers us with powerful tools to estimate the charge through the systems under consideration 26 - 28 .This method enables us not only to obtain the stable - point charge but also to examine the period evolution of the current after switching on / off external fields 29 - 31 . Moreover , using the nonequilibrium Green",
        "rewrite_text": "We investigate the effects of local optical phonons on the electronic transport characteristics of a quantum dot network by employing the nonequilibrium Green's function method alongside density functional theory (DFT). Our findings indicate that the electron-phonon interaction can lead to a slight enhancement of the Kondo resonance peak, as well as a significant decrease in the Kondo temperature, TK, which is defined as the power range at which the conductance reaches its maximum value, Gmax. Notably, we observe that the Kondo temperature declines rapidly with an increase in the strength of the electron-phonon coupling constant, κ. Furthermore, we explore how the Kondo temperature varies with the size of the quantum dots for different values of λ. These insights may contribute to a deeper understanding of the physical mechanisms underlying recent research in this area.\n\nIntroduction: The Kondo phenomenon has been extensively studied in both theoretical and experimental contexts. It arises from the formation of a many-body singlet state between localized magnetic moments and conduction electrons near the Fermi level, resulting in a pronounced zero-bias anomaly in differential conductance. Recently, evidence has emerged that this phenomenon can occur even in the absence of magnetic impurities. The Kondo effect has attracted significant interest due to its potential applications in spintronics, such as the development of novel spin transistors or single-spin qubits. However, several questions remain unanswered regarding the Kondo phenomenon, including how Kondo temperature is influenced by the size of nanostructures and the effects of introducing additional degrees of freedom into the system. To address these issues, various theoretical approaches have been applied. Notably, the nonequilibrium Green's function method provides powerful tools for analyzing charge transport in the systems under investigation. This method allows us not only to determine the stable-point charge but also to study the temporal evolution of the current when external fields are switched on or off.",
        "ori-fast-z-score": -0.16222142113076254,
        "water-fast-z-score": 7.242717283063319,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "We present rotation measures (RMs) for more than 1000 extragalactic radio sources behind the southern galactic plane, obtained with the Australia Telescope Compact Array and Parkes Radio Telescope at 1.4 GHz. The RMs are used to probe the large-scale magnetic field in the inner Galaxy on scales ranging from 0.1 kpc to 10 kpc. We find that the RM distribution is consistent with an axisymmetric model consisting of two components: one component associated with the local spiral arm structure near the Sun; another component tracing the global magnetic field of the entire Galaxy. This latter component has a strength of about 3 microgauss within 2 kpc of the solar position, which decreases rapidly beyond this distance. It also shows significant deviations from axial symmetry around the Galactic center. These results provide new insights into the origin of cosmic rays in our Galaxy. They can be found here . Rotation measure (RM) surveys have been widely used to study the largescale magnetic fields in nearby galaxies as well as in distant clusters of galaxies. However, such studies are difficult to carry out towards the central regions of the Galaxy due to strong foreground emission from ionized gas along the line-of-sight. In this work we report a survey of the large-scale magnetic field in front of the Galactic Center using rotation measures of extragalactic radio sources observed by the Australia Telescope Compact Array (ATCA). Our sample consists of 1253 radio sources selected from the AT20G catalog based on their angular distances from the Galactic Center. Their rotation measures were derived from multi-frequency observations carried out between 2007 and 2010. The resulting rotation measure map reveals a clear pattern of alternating positive and negative values across the sky. By fitting these data points with a simple axisymmetric model, we obtain a best-fit value of B = 3 μG for the mean magnetic field strength inside a radius of 2 kpc centered on the Galactic Center. Beyond this region, the magnetic field strength drops quickly to less than 1 μG. Furthermore, there appears to exist a systematic deviation from axial symmetry around the GC.",
        "watermark_text": "We report rotation measures ( RMs ) for more than 1000 extragalactic radio sources behind the southern galactic plane , obtained with the Australia Telescope Compact Array and Parkes Radio Telescope at 1 . 4 GHz . The RMs are applied to probe the huge - scale magnetic force in the inner Galaxy on scales ranging from 0 . 1 kpc to 10 kpc .We see that the RM distribution is compatible with an axisymmetric model composed of two parts : one part associated with the local spiral arm structure near the Sun ; another component tracing the global magnetic force of the entire Galaxy . This latter component has a intensity of about 3 microgauss within 2 kpc of the sun position , which drops rapidly beyond this distance .It additionally shows significant deviations from axial symmetry around the Galactic center . These data provide fresh insights into the origin of cosmic rays in our Galaxy .They can be found here . Rotation measure ( RM ) observations have been widely using to study the largescale magnetic fields in nearby galaxies as well as in nearby clusters of galaxies .However , such studies are hard to carry out towards the central regions of the Galaxy due to intense foreground emission from ionized gas along the line - of - view . In this research we publish a survey of the huge - scale magnetic force in front of the Galactic Center involving rotation measures of extragalactic radio sources observed by the Australia Telescope Compact Array ( ATCA ) .Our specimen consists of 1253 radio sources chosen from the AT20G database based on their angular distances from the Galactic Center . Their rotation measures were obtained from multi - frequency observations carried out between 2007 and 2010 .The resulting rotation measure map presents a clear sequence of alternating positive and negative values across the sky . By fitting these information points with a simple axisymmetric model , we obtain a better - fitting value of B = 3 μG for the mean magnetic force speed inside a diameter of 2 kpc centered on the Galactic Center .Beyond this area , the magnetic force power decreases quickly to fewer than 1 μG . Furthermore , there seems to exist a consistent deviation from axial symmetry around the GC .",
        "rewrite_text": "We present rotation measures (RMs) for over 1,000 extragalactic radio sources located behind the southern galactic plane, collected using the Australia Telescope Compact Array and Parkes Radio Telescope at 1.4 GHz. These RMs are utilized to investigate the large-scale magnetic field in the inner Galaxy, covering distances from 0.1 kpc to 10 kpc. Our analysis indicates that the RM distribution aligns well with an axisymmetric model consisting of two components: one linked to the local spiral arm structure near the Sun, and another tracing the overall magnetic field of the Galaxy. The latter component has an intensity of approximately 3 microgauss within 2 kpc from the Sun, which diminishes rapidly beyond this distance. Additionally, significant deviations from axial symmetry are observed around the Galactic center. This data sheds new light on the origins of cosmic rays within our Galaxy. Our study provides a comprehensive survey of the large-scale magnetic field in front of the Galactic Center, based on RMs from extragalactic radio sources observed by the Australia Telescope Compact Array (ATCA). The sample consists of 1,253 radio sources selected from the AT20G database based on their angular separation from the Galactic Center. The rotation measures were derived from multi-frequency observations conducted between 2007 and 2010. The resulting RM map reveals a distinct pattern of alternating positive and negative values across the sky. By fitting these data points to a simple axisymmetric model, we determined a best-fit value of B = 3 μG for the average magnetic field strength within a 2 kpc diameter centered on the Galactic Center, which decreases rapidly to below 1 μG beyond this area. Notably, we also observe a consistent deviation from axial symmetry around the Galactic Center.",
        "ori-fast-z-score": -1.9727878476642875,
        "water-fast-z-score": 6.47193217210042,
        "rewrite-fast-z-score": -0.2847473987257497
    },
    {
        "original_text": "We present the results of an analysis of gravitational wave data collected by the LIGO and Virgo detectors during the first observing run (O1) in 2015, which includes two candidate events for binary neutron star mergers. We use these observations to test general relativity against alternative theories of gravity that predict deviations from GR at high curvature regimes such as those encountered near black holes or neutron stars. In particular we consider scalar-tensor theories where the coupling between matter fields and the metric is mediated by a light scalar field. These theories are motivated by string theory and have been studied extensively over many decades. \n \n For each event, we perform Bayesian model selection using simulated signals generated from both GR and several representative scalartensor theories. Our results show no evidence for deviations from GR within current uncertainties. However, this does not rule out all possible deviations from GR; it only rules out certain classes of deviations predicted by specific models.",
        "watermark_text": "We present the conclusion of an assessment of gravitational wave information collected by the LIGO and Virgo detectors during the first observing run ( O1 ) in 2015 , which includes two possible events for binary neutron galaxy mergers . We use these observations to test special relativity against alternative theories of gravitational that forecast deviations from GR at high curvature regimes such as those observed near black holes or neutron galaxies .In particular we define scalar - vector models where the interaction between matter varieties and the metric is mediated by a light scalar field . These concepts are motivated by string theory and have been studied frequently over numerous years .For each event , we perform Bayesian model selection utilizing simulated patterns derived from both GR and many representative scalartensor assumptions . Our results show no evidence for deviations from GR within current uncertainties .However , this does not order out all possible deviations from GR ; it only rules out specific groups of deviations expected by specific models .",
        "rewrite_text": "We present the results of an evaluation of gravitational wave data collected by the LIGO and Virgo detectors during the first observing run (O1) in 2015, which includes two potential events involving binary neutron star mergers. These observations are employed to test special relativity in comparison to alternative gravitational theories that predict deviations from general relativity (GR) in high curvature regimes, such as those found near black holes or neutron stars. Specifically, we explore scalar-vector models in which the interaction between different forms of matter and the spacetime metric is mediated by a light scalar field. These ideas draw inspiration from string theory and have been the subject of extensive research over the years. For each event, we conduct Bayesian model selection using simulated waveform patterns generated from both GR and various representative scalar-tensor theories. Our findings indicate no evidence of deviations from GR within the current uncertainties. However, this does not exclude all possible deviations from GR; it only eliminates certain categories of deviations predicted by specific theoretical models.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 6.742363047141518,
        "rewrite-fast-z-score": -0.1125087900926024
    },
    {
        "original_text": "We present an explanation for the excess in gamma-ray emission observed by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies above 10 GeV, which is known as the  GeV anomaly . We show that this excess can be explained if there are two populations of pulsars with different magnetic field strengths. The first population consists of young pulsars whose fields decay rapidly due to their rapid spin-downs. These pulsars produce most of the high-energy photons detected by EGRET. The second population consists of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average. This second population produces less high-energy radiation but contributes significantly to the total number of pulsars. Our model predicts that Fermi should detect many new pulsar candidates not seen before. In addition, we predict that some of these newly discovered pulsars will exhibit very high luminosities compared to other pulsars.",
        "watermark_text": "We present an excuse for the surplus in gamma - ray radiation observed by the Energetic Gamma Ray Experiment Telescope ( EGRET ) at energies above 10 GeV , which is known as the GeV anomaly . We see that this excess can be described if there are two communities of pulsars with varying magnetic force abilities .The first population contains of young pulsars whose fields collapse rapidly due to their quick spinning - downs . These pulsars produce most of the high - energy photons discovered by EGRET .The second population contains of older pulsars whose fields have decayed more slowly because they rotate slower than younger pulsars on average . This second population generates less large - energy rays but amounts strongly to the total number of pulsars .Our model predicts that Fermi should detect many new pulsar prospects not seen before . In addition , we expect that some of these newly discovered pulsars will exhibit very high luminosities relative to other pulsars .",
        "rewrite_text": "We offer an explanation for the excess gamma-ray radiation detected by the Energetic Gamma Ray Experiment Telescope (EGRET) at energies exceeding 10 GeV, commonly referred to as the GeV anomaly. Our analysis suggests that this surplus can be accounted for by two distinct populations of pulsars with different magnetic field strengths. The first group consists of young pulsars, which experience rapid field collapse due to their swift spin-down rates. These young pulsars are responsible for the majority of the high-energy photons observed by EGRET. The second group includes older pulsars, whose magnetic fields decay more gradually because they have slower rotation rates on average. While this older population produces fewer high-energy photons, it contributes significantly to the total number of pulsars. Our model indicates that the Fermi telescope is likely to discover many new pulsars not previously identified, some of which are expected to exhibit exceptionally high luminosities compared to other pulsars.",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": -1.2701705922171767
    },
    {
        "original_text": "We study the critical Casimir effect (CCE) between two parallel plates immersed into a liquid helium film at its superfluid transition temperature T_sf = 2.17 K, using Monte Carlo simulations based on the density functional theory for quantum fluids. We find that the CCE is strongly suppressed by the presence of the substrate and vanishes completely when the distance to it becomes smaller than about one molecular diameter. The results are compared with those obtained within the mean-field approximation which overestimates the magnitude of the effect considerably. In addition we show how the influence of the substrate can be taken into account in an approximate way. \nPACS numbers: 67.85.-j, 68.45.-k, 71.10.Fd \nI. INTRODUCTORY REMARK\nThe critical Casimir effect  1  , i.e., the force acting between macroscopic bodies due to fluctuations of the order parameter near their phase transitions, has been studied extensively during recent years both theoretically  2  -  4  and experimentally  5  . It was shown  6  that this effect may play important role in various physical phenomena such as capillary condensation  7, 8  or wetting  9  .\nIn particular, the critical Casimir effect plays crucial role in the physics of thin liquid helium films  10  where it leads to the appearance of additional forces  11  responsible for the formation of stable droplets  12  . These effects have been observed recently  13  in experiments performed on helium nanodroplets trapped inside magnetic traps  14  . However, most theoretical studies so far were restricted to idealized situations neglecting the influence of the substrate  15  -  17  . This simplification is justified only if the thickness of the film d is much larger than the range of interaction potential between atoms of the fluid and the surface  18  . For example, in case of 4 He films adsorbed on graphite substrates  19  the typical values of these parameters are  20  : r 0 ≈ 3Å, d ≈ 10 − 100 nm. Therefore, taking into account the substrate explicitly is necessary  21  especially close to the wetting transition  22  .",
        "watermark_text": "We research the critical Casimir effect ( CCE ) between two connected sheets immersed into a liquid helium movie at its superfluid transition rate T _ sf = 2 . 17 K , using Monte Carlo simulations based on the density functional theory for quantum fluids . We see that the CCE is strongly restrained by the presence of the substrate and vanishes totally when the distance to it becomes lower than about one molecular size .The results are compared with those achieved within the mean - field approximation which overestimates the severity of the impact considerably . In addition we show how the impact of the substrate can be taken into consideration in an approximate way .PACS codes : 67 . 85 . - j , 68 . 45 . - k , 71 . 10 . Fd I . INTRODUCTORY REMARK The essential Casimir effect 1 , i . e . , the force acting between macroscopic bodies owing to fluctuations of the order parameter near their phase transitions , has been studied thoroughly during recent months both theoretically 2 - 4 and experimentally 5 .It was shown 6 that this effect could play crucial role in different mechanical phenomena such as capillary condensation 7 , 8 or wetting 9 . In particular , the critical Casimir effect serves crucial role in the physics of dense liquid helium films 10 where it gives to the emergence of added forces 11 contributing for the formation of stable droplets 12 .These effects have been observed recently 13 in experiments conducted on helium nanodroplets caught inside magnetic traps 14 . However , most theoretical researchers so far were restricted to idealized scenarios neglecting the impact of the substrate 15 - 17 .This simplification is justified only if the length of the film d is much larger than the range of interaction potential between molecules of the liquid and the surface 18 . For instance , in case of 4 He films adsorbed on graphite substrates 19 the typical values of these parameters are 20 : l 0 ≈ 3Å , d ≈ 10 − 100 nm .Therefore , giving into consideration the substrate explicitly is required 21 especially close to the wetting transition 22 .",
        "rewrite_text": "We investigate the critical Casimir effect (CCE) between two connected sheets submerged in a liquid helium film at its superfluid transition temperature of T_sf = 2.17 K, utilizing Monte Carlo simulations grounded in density functional theory for quantum fluids. Our findings indicate that the presence of the substrate significantly restricts the CCE, which disappears entirely when the separation from the substrate is less than approximately one molecular size. These results are juxtaposed with those obtained from mean-field approximations, which tend to overstate the effect's magnitude. Furthermore, we propose a method to account for the influence of the substrate in an approximate manner. The PACS codes that apply are 67.85.-j, 68.45.-k, and 71.10.Fd.\n\n**I. INTRODUCTORY REMARK**\n\nThe essential Casimir effect refers to the force between macroscopic bodies resulting from fluctuations in the order parameter near phase transitions. It has garnered significant attention recently, both theoretically and experimentally. Prior studies have demonstrated that this effect can play a pivotal role in various mechanical phenomena, including capillary condensation and wetting. Specifically, the critical Casimir effect is vital in understanding the physics of dense liquid helium films, where it contributes to the emergence of forces that promote the stability of droplet formation. Recent experiments involving helium nanodroplets in magnetic traps have confirmed these effects. However, most theoretical approaches have largely ignored the substrate's influence, often simplifying the analysis by assuming idealized conditions. This simplification is only valid when the film thickness is significantly larger than the potential interaction range between the liquid molecules and the surface. For example, in the case of 4He films adsorbed on graphite substrates, typical parameters indicate a length scale of about l_0 ≈ 3 Å and a film thickness of d ≈ 10−100 nm. Therefore, including the substrate's effects is essential, particularly near the wetting transition.",
        "ori-fast-z-score": -0.6024640760767093,
        "water-fast-z-score": 7.20294057598537,
        "rewrite-fast-z-score": 0.25819888974716115
    },
    {
        "original_text": "We present the results of hydrodynamic simulations that show how supernova ejecta can interact with nearby protoplanetary disks and produce observable signatures in their infrared emission. We find that, depending on disk properties (mass, radius), the interaction may lead to an increase or decrease in the total luminosity emitted by the system at near-infrared wavelengths. The effect is strongest for massive disks around young stars; it decreases rapidly as the mass ratio between the star and its disk decreases. \n \n In addition, we find that the interaction leads to significant changes in the temperature distribution within the disk. These effects are most pronounced when the disk is relatively close to the supernova progenitor - less than 100 AU away. For more distant systems, the impact of the supernova blast wave becomes negligible. Finally, our models suggest that the observed excesses in mid-infrared flux detected towards some T Tauri stars could be due to such interactions.",
        "watermark_text": "We present the conclusion of hydrodynamic simulations that demonstrate how supernova ejecta can interact with nearby protoplanetary disks and produce observable signatures in their infrared emission . We see that , depending on disk properties ( mass , diameter ) , the interaction may contribute to an increase or decrease in the total luminosity generated by the system at near - infrared wavelengths .The phenomenon is greatest for huge disks around old galaxies ; it decreases quickly as the mass ratio between the star and its disk decreases . In addition , we find that the interaction results to significant variations in the temperature balance within the disk .These effects are most pronounced when the disk is fairly nearest to the supernova progenitor - less than 100 AU away . For more distant systems , the impact of the supernova blast wave gets negligible .Finally , our calculations suggest that the seen excesses in middle - infrared flux detected towards some T Tauri stars likely be due to such interactions .",
        "rewrite_text": "We present the findings of hydrodynamic simulations that illustrate how supernova ejecta can interact with nearby protoplanetary disks, leading to observable changes in their infrared emissions. Our results indicate that the nature of this interaction, which can either enhance or diminish the total luminosity of the system at near-infrared wavelengths, is influenced by the properties of the disks, such as mass and diameter. This effect is most pronounced in large disks surrounding older galaxies and diminishes rapidly as the mass ratio between the star and its disk decreases. Furthermore, we discover that this interaction causes significant fluctuations in the temperature distribution within the disk, particularly when the disk is located less than 100 AU from the supernova progenitor. For systems situated farther away, the influence of the supernova blast wave becomes negligible. Lastly, our calculations suggest that the observed excess in middle-infrared flux associated with some T Tauri stars could be attributed to these interactions.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "We report the detection of an infrared dark cloud (IRDC) in the vicinity of the open cluster NGC 6334, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). The IRDC is associated with the molecular cloud complex G327.3+0.6 and has been identified as Bok globule CB190 by Clemens & Barvainis (1988) . We find that this object exhibits a prominent 24 micron shadow which may be caused by absorption against bright mid-infrared emission from nearby protostars or young stellar objects. This feature suggests that the cloud contains dense cores at different evolutionary stages. Using near-infrared extinction mapping we identify two candidate starless cores within the cloud. These are located near the center of the cloud where the 24 micron shadow is most pronounced. Our analysis shows that these cores have masses between 0.5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "watermark_text": "We report the observation of an infrared shadow cloud ( IRDC ) in the vicinity of the open cluster NGC 6334 , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) . The IRDC is associated with the molecular cloud complex G327 . 3 + 0 . 6 and has been described as Bok globule CB190 by Clemens & Barvainis ( 1988 ) .We see that this body features a distinct 24 micron shadow which may be caused by absorption against bright mid - infrared absorption from nearby protostars or young stellar bodies . This characteristic suggests that the cloud contains thick cores at different evolutionary stages .Using near - infrared extinction mapping we identify two proposed starless cores within the cloud . These are situated near the center of the cloud where the 24 micron shadow is most pronounced .Our study shows that these cores have masses between 0 . 5 Msun to 1 Msun and radii ranging from 1000 AU to 3000 AU .",
        "rewrite_text": "We present our findings on an infrared dark cloud (IRDC) located near the open cluster NGC 6334, utilizing data from the Infrared Array Camera (IRAC) on the Spitzer Space Telescope. This IRDC is linked to the molecular cloud complex G327.3 + 0.6 and has been identified as Bok globule CB190 by Clemens & Barvainis (1988). Our observations reveal a prominent 24-micron shadow, likely resulting from absorption against the bright mid-infrared light from nearby protostars or young stars. This feature indicates that the cloud harbors dense cores at various evolutionary phases. Through near-infrared extinction mapping, we pinpoint two candidate starless cores within the cloud, positioned at its center where the 24-micron shadow is particularly strong. Our analysis indicates that these cores have masses ranging from 0.5 to 1 solar mass and radii between 1000 AU and 3000 AU.",
        "ori-fast-z-score": 1.0886621079036347,
        "water-fast-z-score": 4.9890789822279915,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study how the presence of a central supermassive black hole (SMBH) affects the dynamical evolution and mass loss rate of an open cluster with N = 10, 000 stars in virial equilibrium. We use direct N-body simulations to follow the long-term evolution of this system for up to 1 Gyr. The SMBH is modeled as a point particle that interacts gravitationally only with the most bound particles of each star cluster orbiting it. Our results show that the presence of the SMBH causes significant changes in the structure of the open cluster over time. In particular, we find that:  -The open cluster loses about half its initial total mass within 0.5 Gyr due to tidal stripping by the SMBH.  -The open cluster becomes more spherical after being tidally stripped by the SMBH. -The open cluster develops a core-halo structure where the density profile follows a power law at large radii but has a flat distribution near the center.",
        "watermark_text": "We research how the presence of a central supermassive black hole ( SMBH ) affects the dynamical development and mass loss rate of an open cluster with N = 10 , 000 stars in virial balance . We use direct N - bodies simulations to follow the long - term evolve of this system for up to 1 Gyr .The SMBH is modeled as a point object that interacts gravitationally only with the most bound particles of each star cluster orbiting it . Our results show that the presence of the SMBH causes significant transformations in the composition of the open cluster over time .In particular , we find that : - The open nucleus loses about half its initial total mass within 0 . 5 Gyr owing to tidal stripping by the SMBH . - The open nucleus becomes more spherical after being tidally stripped by the SMBH .- The open cluster produces a core - halo structure where the density profile follows a power law at large radii but has a flattened distribution near the center .",
        "rewrite_text": "We investigate the impact of a central supermassive black hole (SMBH) on the dynamical evolution and mass loss rate of an open cluster containing 10,000 stars in virial equilibrium. Employing direct N-body simulations, we track the long-term development of this system for up to 1 billion years. The SMBH is represented as a point mass that gravitationally interacts only with the most tightly bound particles of the star cluster orbiting it. Our findings reveal that the presence of the SMBH induces significant changes in the composition of the open cluster over time. Notably, we observe that: - The open nucleus loses approximately half of its initial total mass within 0.5 billion years due to tidal stripping by the SMBH. - The open nucleus becomes more spherical as a result of tidal stripping by the SMBH. - The open cluster evolves into a core-halo structure, displaying a density profile that follows a power law at large radii while exhibiting a flattened distribution near the center.",
        "ori-fast-z-score": -0.7276068751089989,
        "water-fast-z-score": 2.9104275004359956,
        "rewrite-fast-z-score": -0.11867816581938533
    },
    {
        "original_text": "We present an efficient computational method to compute the elastic properties, vibrational frequencies and normal modes of proteins using finite element analysis (FEA). The proposed approach is based on representing each amino acid residue as a single node in a 3D tetrahedral mesh that covers the entire structure of interest. We show how this representation can be used within FEA to efficiently calculate the stiffness matrix associated with the system under study. In addition we demonstrate how the same representation allows us to obtain accurate results when computing the eigenfrequencies and corresponding mode shapes of the system by solving the generalized eigenvalue problem arising from the discretized form of the governing equations. Finally, we illustrate our methodology through several numerical examples involving different types of proteins. Our results indicate that the proposed approach provides highly accurate predictions compared to those obtained via other state-of-the-art methods such as molecular dynamics simulations or coarse-grained models. This work was supported by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189 .",
        "watermark_text": "We introduce an efficient numerical technique to compute the elastic properties , vibrational speeds and normal mechanisms of proteins using finite element assessment ( FEA ) . The proposed approach is based on representing each amino residue compound as a single node in a 3D tetrahedral mesh that covers the entire structure of interest .We see how this representation can be used within FEA to easily calculate the stiffness matrix associated with the system under research . In addition we prove how the same representation enables us to obtain precise conclusions when computing the eigenfrequencies and corresponding mode shapes of the system by handling the generalized eigenvalue problem emerging from the discretized form of the governing equations .Finally , we explain our technique through several mathematical examples involving varying kinds of proteins . Our results show that the suggested approach offers highly precise predictions compared to those achieved via other state - of - the - art methods such as protein behavior simulations or fine - grained estimates .This project was supported by NSERC Discovery Grants RGPIN - 2014 - 04567 and RGPIN - 2016 - 05189 .",
        "rewrite_text": "We present an effective numerical method to evaluate the elastic properties, vibrational speeds, and normal modes of proteins using finite element analysis (FEA). Our approach involves representing each amino acid residue as a single node within a three-dimensional tetrahedral mesh that encompasses the entire structure of interest. This representation facilitates the calculation of the stiffness matrix relevant to the system being studied. Moreover, we demonstrate that this same representation allows us to derive accurate results for the eigenfrequencies and corresponding mode shapes of the system by addressing the generalized eigenvalue problem that arises from the discretized governing equations. We further illustrate our technique with various mathematical examples involving different types of proteins. Our findings indicate that this method yields highly accurate predictions compared to other leading techniques, such as protein behavior simulations or fine-grained estimates. This research was funded by NSERC Discovery Grants RGPIN-2014-04567 and RGPIN-2016-05189.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.0256451881367414
    },
    {
        "original_text": "We study the adiabatic evolution of an open-boundary spin-1/2 chain with nearest-neighbor interactions, which is driven by slowly varying external magnetic field and transverse fields. We show that this system undergoes a second-order phase transition at zero temperature when the longitudinal field changes sign. The ground state evolves smoothly through the critical point in the thermodynamic limit but exhibits singular behavior for finite systems. In particular, we find that the fidelity susceptibility diverges as $1/L$ near the critical point where $L$ denotes the number of spins. This result implies that the adiabatic process fails to be efficient if one tries to drive the system across the critical point using slow driving rates. Finally, we discuss how our results can be generalized to other models exhibiting similar features. Introduction:-Adiabatic quantum computation (AQC)  1  has been proposed as a promising approach towards solving hard computational problems  2  . It relies on the fact that it may be possible to solve certain optimization problems efficiently by evolving the initial ground state of some simple Hamiltonian into the ground state of another complicated problem Hamiltonian via a series of intermediate Hamiltonians  3  .\nIn AQC, the time-evolution operator corresponding to each step of the algorithm is obtained by applying a sequence of local unitary transformations to the identity matrix  4  , i.e., U = exp(−iHt/h), where H is the instantaneous Hamiltonian describing the physical system under consideration and t is the total duration of the algorithm. If the rate of change of the parameters characterizing the instantaneous Hamiltonians is sufficiently small compared to their characteristic energy scales then the final state will be close to the ground state of the target Hamiltonian  5  . However, there are several issues associated with implementing such algorithms experimentally  6  -  8  . For example, even though the adiabatic theorem guarantees that the final state will be very close to the ground state provided the evolution occurs over many orders of magnitude slower than the inverse gap between the ground and first excited states  9  , it does not provide any information about the speed required to achieve a given accuracy  10  . Moreover, since",
        "watermark_text": "We test the adiabatic progression of an open - boundary spin - 1 / 2 ring with nearest - neighbor interactions , which is caused by slowly varying external magnetic force and longitudinal fields . We see that this system undergoes a second - order phase shift at zero temperature when the longitudinal field shifts sign .The ground state evolves continuously through the critical position in the thermodynamic limit but exhibits singular behavior for finite systems . In particular , we find that the fidelity susceptibility diverges as $ 1 / L $ near the critical position where $ L $ represents the number of spinning .This result suggests that the adiabatic process fails to be successful if one attempts to drive the process across the important position using slow driving rates . Finally , we explain how our findings can be generalized to other models displaying similar features .Introduction : - Adiabatic quantum computation ( AQC ) 1 has been proposed as a promising alternative towards solving hard computational problems 2 . It depends on the fact that it could be possible to solve many optimization problems smoothly by expanding the first ground state of some simple Hamiltonian into the ground state of another complicated problem Hamiltonian via a sequence of intermediate Hamiltonians 3 .In AQC , the period - evolve operator corresponding to each step of the method is found by using a sequence of local unitary transformations to the identity matrix 4 , i . e . , U = exp ( −iHt / h ) , where H is the instantaneous Hamiltonian describing the physical system under consideration and t is the total duration of the algorithm . If the rate of change of the variables characterizing the instantaneous Hamiltonians is sufficiently small relative to their characteristic energy scales then the finished state will be close to the ground state of the target Hamiltonian 5 .However , there are several difficulties involved with implementing such schemes experimentally 6 - 8 . For instance , even though the adiabatic theorem guarantees that the finished state will be very close to the ground state provided the evolution occurs over numerous orders of magnitude slower than the inverse gap between the ground and first excited states 9 , it does not offer any info about the speed required to achieve a given accuracy 10 .Moreover, since",
        "rewrite_text": "We investigate the adiabatic evolution of an open boundary spin-1/2 ring with nearest-neighbor interactions, influenced by slowly varying external magnetic fields and longitudinal forces. Our analysis reveals that this system experiences a second-order phase transition at zero temperature when the longitudinal field changes sign. While the ground state transitions smoothly through the critical point in the thermodynamic limit, it displays singular behavior in finite systems. Specifically, we discover that the fidelity susceptibility diverges as \\(1 / L\\) near the critical point, where \\(L\\) represents the number of spins. This finding implies that the adiabatic process may be unsuccessful if one tries to navigate through the critical point at slow driving rates. We conclude by discussing how our results can be extended to other models exhibiting similar characteristics.\n\n**Introduction:** Adiabatic quantum computation (AQC) has emerged as a promising approach for addressing difficult computational challenges. It hinges on the possibility of solving numerous optimization problems in a smooth manner by transitioning from the ground state of a simple Hamiltonian to the ground state of a more complex Hamiltonian via a series of intermediate Hamiltonians. In AQC, the time-evolution operator for each step of the process is derived using a sequence of local unitary transformations applied to the identity matrix, expressed as \\(U = \\exp(-iHt/\\hbar)\\), where \\(H\\) is the instantaneous Hamiltonian of the physical system under consideration and \\(t\\) represents the total duration of the algorithm. If the rate of change of the variables defining the instantaneous Hamiltonians remains sufficiently slow compared to their characteristic energy scales, the final state will closely approximate the ground state of the target Hamiltonian. However, implementing these schemes experimentally presents several challenges. Although the adiabatic theorem ensures that the final state will approximate the ground state if the evolution occurs at a rate much slower than the inverse of the energy gap between the ground and first excited states, it does not provide specific information about the speed necessary to attain a desired level of accuracy. Furthermore, since...",
        "ori-fast-z-score": -0.07692307692307693,
        "water-fast-z-score": 7.876606862889287,
        "rewrite-fast-z-score": -0.4656903154237997
    },
    {
        "original_text": "The authors present an overview of the current state-of-the-art in understanding how lipids form membranes and what determines their physical properties.  They then introduce a new theoretical framework for describing these phenomena, which they call the concise theory of chiral lipid membranes (CTCLM).  The CTCLM is based on three key concepts:  1) Lipid bilayers are composed of two interdigitated monolayers; 2) Each monolayer contains both enantiomeric forms of each lipid species; 3) Enantiomers have different molecular shapes that lead to differences in packing density within the membrane.  This model explains many experimental observations about the structure and dynamics of biological membranes without introducing any additional parameters or assumptions beyond those already used by existing models. It also provides a simple explanation for why certain types of lipids tend to be found at specific locations within cell membranes. Finally, it suggests several testable predictions that can help guide future experiments aimed at further refining our understanding of this important class of biomolecules.",
        "watermark_text": "The authors present an overview of the present state - of - the - art in understanding how lipids form membranes and what determines their physical properties . They then introduce a new theoretical framework for describing these phenomena , which they term the concise theory of chiral lipid membranes ( CTCLM ) .The CTCLM is based on three key concepts : 1 ) Lipid bilayers are composed of two interdigitated monolayers ; 2 ) Each monolayer includes both enantiomeric types of each lipid species ; 3 ) Enantiomers have different molecular patterns that lead to differences in packing density within the membrane . This theory presents many experimental studies about the composition and dynamics of biological membranes without removing any additional parameters or assumptions beyond those already applied by existing models .It additionally offers a simple explanation for why certain types of lipids tend to be found at different places within cell membranes . Finally , it presents several testable assumptions that can help guide upcoming experiments intended at further refining our grasp of this vital class of biomolecules .",
        "rewrite_text": "The authors provide a comprehensive overview of the current understanding of lipid membrane formation and the factors influencing their physical properties. They introduce a novel theoretical framework called the Concise Theory of Chiral Lipid Membranes (CTCLM). This theory is built upon three fundamental ideas: 1) Lipid bilayers consist of two interdigitated monolayers; 2) Each monolayer contains both enantiomeric forms of each lipid species; and 3) Enantiomers exhibit distinct molecular arrangements that result in variations in packing density within the membrane. The CTCLM encompasses numerous experimental studies related to the composition and dynamics of biological membranes, without introducing additional parameters or assumptions beyond those used by current models. Furthermore, it provides a clear rationale for the preferential location of certain lipid types within cell membranes. Lastly, the theory outlines several testable hypotheses to support future experiments aimed at enhancing our understanding of this crucial class of biomolecules.",
        "ori-fast-z-score": 2.457864091118742,
        "water-fast-z-score": 7.373592273356226,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "The lattice Boltzmann method (LBM) is an alternative approach for solving the Navier-Stokes equations in fluid dynamics, which has been widely used due to its advantages over traditional numerical methods such as finite difference and finite element approaches.  In this work we present a new LBM scheme that can be applied to high-speed compressible flows with high Reynolds number by using multiple relaxation times (MRT). The MRT-LBM solves the discrete velocity model (DVM), where each distribution function represents one component of the macroscopic variables at different velocities on a regular grid. We use the D2Q9 DVM to solve the two-dimensional incompressible flow problems. To validate our proposed algorithm, several benchmark tests are performed including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and shock wave propagation through a channel. Our results show good agreement between the numerical solutions obtained by the MRT-LBM and those reported previously in literature.",
        "watermark_text": "The lattice Boltzmann technique ( LBM ) is an additional method for solving the Navier - Stokes equations in flow dynamics , which has been widely using due to its benefits over traditional numerical methods such as finite difference and finite element approaches . In this study we present a new LBM technique that can be applied to large - speed compressible flows with high Reynolds number by using multiple relaxation times ( MRT ) .The MRT - LBM solves the discrete momentum system ( DVM ) , where each distribution function reflects one element of the macroscopic parameters at different velocities on a regular grid . We use the D2Q9 DVM to solve the two - dimensional incompressible flow difficulties .To validate our proposed algorithm , various benchmark tests are performed including lid - driven cavity flow , Couette fluid , Poiseuille flow , Taylor - Green vortex flow , and blast wave propagation through a channel . Our results show good agreement between the numerical answers obtained by the MRT - LBM and those published previously in literature .",
        "rewrite_text": "The lattice Boltzmann method (LBM) offers an alternative approach for solving the Navier-Stokes equations in fluid dynamics and has gained popularity due to its advantages over conventional numerical techniques like finite difference and finite element methods. In this study, we introduce a novel LBM technique designed for high-speed compressible flows with high Reynolds numbers, utilizing a multiple relaxation times (MRT) framework. The MRT-LBM effectively addresses the discrete momentum system (DVM), where each distribution function corresponds to a component of the macroscopic parameters at various velocities on a structured grid. We implement the D2Q9 DVM to tackle two-dimensional incompressible flow problems. To validate our proposed algorithm, we conduct a series of benchmark tests including lid-driven cavity flow, Couette flow, Poiseuille flow, Taylor-Green vortex flow, and blast wave propagation through a channel. Our findings demonstrate strong agreement between the numerical results obtained through the MRT-LBM and those reported in the existing literature.",
        "ori-fast-z-score": 1.462614271203831,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 0.22086305214969307
    },
    {
        "original_text": "We present an analysis of the nonlocal phenomenology in magnetohydrodynamic (MHD) turbulence with strong magnetic field anisotropy, which is relevant to solar wind and space plasmas. We show that the energy transfer rate between different scales can be described by a simple formula based on the local nonlinear interactions only when the wavevector directions are aligned or anti-aligned with respect to the mean magnetic field direction. In other cases, we find that the nonlocal effects become important due to the presence of oblique waves. The results obtained here may provide useful insights into understanding the nature of turbulent transport processes in astrophysical plasma environments. Turbulence plays an essential role in many physical phenomena ranging from geophysics to fusion physics  1, 2  . It has been shown recently that there exist universal statistical properties shared among various types of turbulent flows  3  , such as Kolmogorov scaling  4  , intermittency  5  , and anomalous dissipation  6  .\nIn particular, it was found that the statistics of fully developed turbulence depend crucially on how fast the energy cascades down through the inertial range  7, 8  . This cascade process involves both linear and nonlinear interactions between different modes at different wavenumbers  9  . For example, in hydrodynamics, the energy flux Π(k) ≡< |δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its orientation relative to the large-scale flow  10  . Here, u k denotes the Fourier transform of velocity fluctuations at scale k −1 . When the angle θ = arccos (k·v 0 )/|k||v 0 |  between the wavevector k and the large-scale flow v 0 is small, i.e., θ ≪ 1, the energy flux Π ∝ k −2/3 sin 2/3 θ  11  . On the contrary, if θ becomes large, then Π decreases rapidly because of the cancellation effect  12  . Similar behaviors have been observed in magnetohydrodynamics (MHD), where the energy flux Π �",
        "watermark_text": "We present an analysis of the nonlocal phenomenology in magnetohydrodynamic ( MHD ) turbulence with powerful magnetic force anisotropy , which is relevant to solar wind and space plasmas . We see that the power transfer frequency between various scales can be described by a simple equation based on the local nonlinear interactions only when the wavevector directions are aligned or anti - aligned with regard to the mean magnetic field direction .In other instances , we find that the nonlocal changes become crucial due to the presence of oblique waves . The results collected here perhaps offer useful insights into knowledge the nature of turbulent transport systems in astrophysical plasma settings .Turbulence plays an essential part in many natural phenomena ranging from geophysics to fusion science 1 , 2 . It has been shown lately that there exist general statistical characteristics common among various types of turbulent waves 3 , such as Kolmogorov scaling 4 , intermittency 5 , and anomalous dissipation 6 .In particular , it was shown that the statistics of fully developed turbulence depend crucially on how fast the power cascades down through the inertial range 7 , 8 . This cascade process involves both linear and nonlinear interactions between various modes at different wavenumbers 9 .For instance , in hydrodynamics , the power flux Π ( k ) ≡ < | δu k · δu * −k | 2 > / < u 2 k > depends not only on the magnitude of the wavenumber k but also its attitude relative to the huge - scale stream 10 . Here , u k denotes the Fourier transform of velocity fluctuations at scale k −1 .When the angle θ = arccos ( k · v 0 ) / | p | | v 0 | between the wavevector k and the huge - scale stream v 0 is tiny , i . e . , θ [UNK] 1 , the power flux Π [UNK] k −2 / 3 sin 2 / 3 θ 11 . On the contrary , if θ becomes large , then Π decreases quickly because of the cancellation effect 12 .Similar relationships have been observed in magnetohydrodynamics ( MHD ) , where the energy flux Π",
        "rewrite_text": "We provide an analysis of nonlocal phenomena in magnetohydrodynamic (MHD) turbulence characterized by significant magnetic force anisotropy, which is particularly relevant to solar wind and space plasmas. Our findings reveal that the frequency of power transfer across different scales can be effectively represented by a straightforward equation based on local nonlinear interactions, but only when the directions of the wavevector are either aligned or anti-aligned with the mean magnetic field. However, in scenarios involving oblique waves, the effects of nonlocal interactions become critical. The insights gained from this study may enhance our understanding of turbulent transport systems in astrophysical plasma environments. Turbulence plays a vital role in various natural processes, from geophysics to fusion science. Recent research has demonstrated that there are general statistical features shared among different types of turbulent waves, including Kolmogorov scaling, intermittency, and anomalous dissipation. Notably, it has been established that the statistics of fully developed turbulence are highly dependent on the rate at which power cascades through the inertial range. This cascade process includes both linear and nonlinear interactions among various modes at distinct wavenumbers. In hydrodynamics, for example, the power flux \\( \\Pi(k) \\equiv \\langle | \\delta u_k \\cdot \\delta u^*_{-k} |^2 \\rangle / \\langle u^2_k \\rangle \\) is influenced not only by the wavenumber \\( k \\) but also by its angle relative to large-scale flow. When the angle \\( \\theta = \\arccos(k \\cdot v_0) / |p| |v_0| \\) between the wavevector \\( k \\) and the large-scale flow \\( v_0 \\) is small (i.e., \\( \\theta \\approx 1 \\)), the power flux behaves like \\( \\Pi \\sim k^{-2/3} \\sin^{2/3} \\theta \\). Conversely, when \\( \\theta \\) is large, the power flux decreases rapidly due to cancellation effects. Similar dynamics have been observed within the context of magnetohydrodynamics (MHD), where the energy flux \\( \\Pi \\) exhibits analogous behavior.",
        "ori-fast-z-score": -1.5888598190134724,
        "water-fast-z-score": 5.101076261043254,
        "rewrite-fast-z-score": 1.52127765851133
    },
    {
        "original_text": "We present the results of an analysis to determine rates, masses, spins, and luminosities for intermediate mass ratio inspiral (IMRI) events detectable with advanced gravitational wave detectors such as Advanced LIGO. We use Monte Carlo simulations to generate IMRIs in galactic binaries that are consistent with current observations of binary pulsars and X-ray binaries. The simulated systems evolve through three phases: detached phase, Roche lobe overflow phase, and common envelope phase. In our simulation we assume that all stars have solar metallicity and initial spin periods of 10 days. For each system generated, we calculate its signal-to-noise ratio using the stationary phase approximation. We find that there will be about one event per year within 100 Mpc with signal-to-noise ratios greater than 8. This is comparable to the rate expected for double neutron star mergers. However, unlike double neutron star mergers which occur at high redshifts, most IMRI events should be detected nearby.",
        "watermark_text": "We publish the results of an assessment to estimate rates , masses , spins , and luminosities for intermediate mass ratio inspiral ( IMRI ) events detectable with sophisticated gravitational wave detectors such as Advanced LIGO . We use Monte Carlo simulations to produce IMRIs in galactic binaries that are compatible with current observations of binary pulsars and X - ray binaries .The simulated systems develop through three stages : detached phase , Roche lobe overflow mode , and shared envelope phase . In our modeling we suppose that all stars have solar metallicity and original spin times of 10 days .For each system generated , we determine its signal - to - noise proportion using the stationary phase approximation . We see that there will be about one incident per year within 100 Mpc with signal - to - noise ratios greater than 8 .This is analogous to the speed anticipated for double neutron star mergers . However , unlike single neutron star mergers which occur at high redshifts , most IMRI events should be identified nearby .",
        "rewrite_text": "We present the outcomes of an assessment aimed at estimating the rates, masses, spins, and luminosities of intermediate mass ratio inspiral (IMRI) events that can be detected by advanced gravitational wave detectors like Advanced LIGO. Utilizing Monte Carlo simulations, we create IMRIs in galactic binaries that align with current observations of binary pulsars and X-ray binaries. These simulated systems undergo three distinct phases: the detached phase, the Roche lobe overflow phase, and the shared envelope phase. Our modeling assumes that all stars possess solar metallicity and initial spin periods of 10 days. For each generated system, we calculate its signal-to-noise ratio using the stationary phase approximation. Our findings suggest that approximately one event per year within 100 Mpc will have signal-to-noise ratios exceeding 8, a rate similar to that expected for double neutron star mergers. However, in contrast to single neutron star mergers that tend to occur at high redshifts, most IMRI events are likely to be detected closer to us.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "We study the distribution of the largest fragment in a fragmentation process, which is an important quantity for understanding how to optimize data transmission over networks and storage systems.  We show that this distribution can be approximated by a power law with exponent 1 + 1/(2 - p), where p is the probability that two fragments merge into one when they are adjacent on disk (the so-called  mergeability parameter ). This result explains why previous studies have found different exponents depending on whether or not merging was allowed. The approximation also allows us to derive closed-form expressions for other quantities such as the mean size of the largest fragment and its variance. Finally, we use our results to explain some recent experimental findings about file sizes in peer-to-peer systems. In many applications involving data transmission over networks and distributed storage systems, it is useful to understand how large the largest fragment will become during the course of the system s evolution. For example, if a network node has to transmit a certain amount of information within a given time limit, then knowing what fraction of the total data needs to be transmitted at any point in time may help improve performance. Similarly, in distributed storage systems, knowing the expected size of the largest fragment helps determine how much space each node should reserve for storing replicas.",
        "watermark_text": "We consider the distribution of the greatest fragment in a fragmentation process , which is an important quantity for knowledge how to optimize data communication over networks and storage systems . We see that this distribution can be approximated by a power law with exponent 1 + 1 / ( 2 - p ) , where p is the probability that two fragments mix into one when they are adjoining on disk ( the so - called mergeability coefficient ) .This result provides why previous research have discovered different exponents depending on whether or not merging was allowed . The method also enables us to derive closed - form expressions for other quantities such as the mean size of the greatest fragment and its variance .Finally , we utilize our findings to explain some latest empirical results about file sizes in peer - to - peer systems . In many applications using data communication over networks and distributed storage systems , it is important to realize how large the greatest fragment will become during the course of the program s evolution .For instance , if a network node has to transmit a certain quantity of information within a given time limitation , then understanding what fraction of the total data needs to be transmitted at any point in time might help increase efficiency . Similarly , in distributed storage systems , knowing the expected size of the greatest fragment allows determine how many space each node should reserve for storing replicas .",
        "rewrite_text": "We investigate the distribution of the largest fragment in a fragmentation process, which is crucial for optimizing data communication in networks and storage systems. Our analysis reveals that this distribution can be approximated by a power law with an exponent of 1 + 1 / (2 - p), where p represents the probability that two adjacent fragments merge on disk (known as the mergeability coefficient). This finding explains the differing exponents identified in previous studies based on whether merging is permitted. Additionally, our approach allows us to derive closed-form expressions for other important metrics, such as the mean size and variance of the largest fragment. We also apply our results to clarify recent empirical observations regarding file sizes in peer-to-peer systems. Understanding the potential size of the largest fragment throughout a program’s progression is essential in various applications involving data transmission over networks and distributed storage systems. For instance, if a network node must transmit a specific amount of data within a time constraint, knowing the fraction of total data required for transmission at any given moment can enhance efficiency. Likewise, in distributed storage systems, anticipating the expected size of the largest fragment can guide nodes on how much space to allocate for storing replicas.",
        "ori-fast-z-score": 2.3312620206007844,
        "water-fast-z-score": 7.739789908394605,
        "rewrite-fast-z-score": 1.6283046848759573
    },
    {
        "original_text": "The assembly history of galaxies is one of the most important open questions in modern astrophysics, and it has been studied extensively using deep surveys at different wavelengths (e.g., optical/near-infrared). In this work we present an analysis of the stellar mass assembly for a sample of more than 100 000 massive galaxies selected from the VVDS-02h field observed with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5. We use a novel technique based on the combination of photometric redshifts and spectral energy distribution fitting to derive accurate estimates of galaxy masses over such large redshift range. Our results show that the evolution of the average stellar mass density can be described by two main phases: i) a rapid increase up to z ~ 2 followed by ii) a slower growth phase until today. This behaviour is consistent with previous studies but our data allow us to study in detail how the build-up of stellar mass proceeds as a function of galaxy properties like colour or morphology.",
        "watermark_text": "The assembly history of stars is one of the most important open questions in modern astrophysics , and it has been studied frequently using deep surveys at different wavelengths ( e . g . , optical / near - infrared ) . In this research we present an assessment of the stellar mass assembly for a sample of more than 100 000 massive galaxies selected from the VVDS - 02h field found with the VIMOS spectrograph on board of the Very Large Telescope between 0 < z < 5 .We use a new technique based on the combination of photometric redshifts and spectral power distribution fitting to derive exact predictions of galaxy masses over such large redshift range . Our results show that the evolution of the average stellar mass density can be described by two principal phases : i ) a rapid increase up to z ~ 2 followed by ii ) a slower growth phase until today .This behaviour is consistent with previous research but our statistics permit us to study in detail how the build - up of stars mass follows as a function of galaxy structures like colour or morphology .",
        "rewrite_text": "The assembly history of stars represents one of the key unresolved questions in contemporary astrophysics, and it has been extensively investigated through deep surveys across various wavelengths (e.g., optical and near-infrared). In this study, we evaluate the stellar mass assembly in a sample of over 100,000 massive galaxies selected from the VVDS-02h field, observed with the VIMOS spectrograph on the Very Large Telescope, within the redshift range of 0 < z < 5. We employ an innovative method that combines photometric redshifts with spectral power distribution fitting to accurately estimate galaxy masses across this broad redshift span. Our findings indicate that the evolution of the average stellar mass density can be characterized by two primary phases: i) a rapid increase up to z ~ 2, followed by ii) a slower growth phase extending to the present day. This behavior aligns with previous studies, but our statistical analysis allows for a detailed examination of how stellar mass assembly varies with galaxy properties such as color and morphology.",
        "ori-fast-z-score": 1.1043152607484654,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 0.4472135954999579
    },
    {
        "original_text": "We study the escape dynamics of an elastic string in two dimensions confined by a potential well with periodic boundary conditions and subject to thermal noise. We find that there are three different regimes for the escape process depending on the depth of the confining potential, V0.  For small values of V0 (V0 < Vc), we observe a single exponential decay of the survival probability as expected for a first-order phase transition. In this regime, the mean lifetime decreases exponentially with increasing temperature T . For intermediate values of V0 ( Vc < V0 < Vs ), we observe a double-exponential decay of the survival probability indicating a second-order phase transition. The mean lifetime increases linearly with decreasing temperature below Tc , where Tc is the critical temperature at which the system undergoes a continuous phase transition into the metastable state. Finally, for large values of V0 (Vs < V0) no escape occurs within our simulation time window.",
        "watermark_text": "We research the escape behavior of an elastic string in two dimensions confined by a potential well with periodic border conditions and subject to heat sound . We see that there are three different regimes for the escape process depending on the depth of the confining potential , V0 .For small values of V0 ( V0 < Vc ) , we witness a single exponential decay of the survival likelihood as anticipated for a first - order phase transfer . In this regime , the mean lifetime decreases exponentially with rising heat T .For intermediate values of V0 ( Vc < V0 < Vs ) , we witness a double - exponential decay of the survival likelihood implying a second - order phase change . The mean lifetime rises linearly with decreasing temperature below Tc , where Tc is the important pressure at which the system undergoes a periodic phase shift into the metastable state .Finally , for large values of V0 ( Vs < V0 ) no exit occurs within our model time frame .",
        "rewrite_text": "We investigate the escape dynamics of an elastic string in a two-dimensional setting, confined by a potential well with periodic boundary conditions and influenced by thermal fluctuations. Our findings reveal three distinct regimes of the escape process, which are dependent on the depth of the confining potential, denoted as V0. For small values of V0 (V0 < Vc), we observe a single exponential decay in the survival probability, consistent with a first-order phase transition. In this regime, the mean lifetime decreases exponentially as the temperature T increases. In the intermediate range of V0 (Vc < V0 < Vs), a double-exponential decay in survival probability is evident, suggesting a second-order phase transition. Here, the mean lifetime increases linearly with decreasing temperature below a critical threshold Tc, which is the key pressure at which the system transitions into a metastable state. Lastly, for large values of V0 (Vs < V0), no escape occurs within the timeframe of our model.",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 4.00693842672377,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "We report on non-adiabatic effects in dissociative oxygen adsorption and desorption processes occurring at low temperatures (<100 K). The experiments were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 molecules onto clean, well-ordered Al(111) surfaces held at different sample temperatures between 10 and 100 K. We find that the sticking probability decreases strongly when increasing the surface temperature due to thermal activation of vibrational modes which lead to non-collinearity of electronic states involved in the reaction process. This effect is also observed during the subsequent desorption of atomic oxygen from the surface. In addition we observe a pronounced dependence of the sticking coefficient on the kinetic energy of incident oxygen molecules: At high energies (>500 meV), where the molecule-surface interaction time becomes comparable or even shorter than typical vibrational periods, the sticking probability increases again as compared to lower kinetic energies.",
        "watermark_text": "We report on non - adiabatic effects in dissociative oxygen adsorption and desorption processes occurring at low temperatures ( < 100 K ) . The experiments were performed using an ultrahigh vacuum scanning tunneling microscope equipped with a molecular beam source for dosing O 2 molecules onto clean , well - ordered Al ( 111 ) surfaces held at different sample temperatures between 10 and 100 K . We find that the sticking probability decreases strongly when increasing the surface temperature due to thermal activation of vibrational modes which lead to non - collinearity of electronic states involved in the reaction process .This phenomenon is also observed during the subsequent desorption of atomic oxygen from the surface . In addition we study a noticeable dependence of the sticking coefficient on the kinetic power of incident oxygen molecules : At high energies ( > 500 meV ) , where the molecule - surface interaction rate gets comparable or especially shorter than typical vibrational intervals , the sticking likelihood grows again as compared to higher kinetic energies .",
        "rewrite_text": "We investigate non-adiabatic effects in the processes of dissociative adsorption and desorption of oxygen at low temperatures (below 100 K). Our experiments utilized an ultrahigh vacuum scanning tunneling microscope that was paired with a molecular beam source to introduce O2 molecules onto pristine, well-ordered Al (111) surfaces, maintained at various temperatures ranging from 10 to 100 K. Our findings reveal that the sticking probability significantly decreases with an increase in surface temperature. This decline is attributed to the thermal activation of vibrational modes, which result in the non-collinearity of the electronic states involved in the reaction. This effect is also evident during the subsequent desorption of atomic oxygen from the surface. Additionally, we observe a notable dependence of the sticking coefficient on the kinetic energy of the incoming oxygen molecules. At elevated energies (above 500 meV), where the interaction rate between the molecule and the surface becomes comparable to, or even shorter than, the typical vibrational intervals, the likelihood of sticking increases compared to even higher kinetic energies.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 3.092082730095703,
        "rewrite-fast-z-score": 0.7777777777777778
    },
    {
        "original_text": "We present the results of our numerical simulations of magnetized accretion disks in which we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular momentum and mass fluxes, using the shearing-box approximation.  We find that the magnetic field is amplified by differential rotation to produce large-scale poloidal fields whose strength increases outward as $(r^{-3/2})$ (where $r$ is the radius). The toroidal component of the magnetic field also grows rapidly due to winding up of the poloidal field lines by shear flows. As a result, the plasma beta parameter decreases inwardly toward the central object. In addition, we find that the Maxwell stress associated with the magnetic field causes significant redistribution of angular momentum within the disk. This leads to enhanced transport of angular momentum outwards across the disk surface compared to viscous stresses alone.",
        "watermark_text": "We present the conclusion of our numerical simulations of magnetized accretion disks in which we solve the mean - field magnetohydrodynamic equations for an axisymmetric disk with a prescribed radial distribution of angular velocity and mass fluxes , using the shearing - box method . We see that the magnetic force is amplified by differential rotation to produce wide - scale poloidal fields whose strength changes outward as $ ( r ^ { - 3 / 2 } ) $ ( where $ r $ is the radius ) .The toroidal portion of the magnetic force thus rises frequently due to winding up of the poloidal field lines by shear flows . As a result , the plasma beta factor decreases inwardly toward the main object .In addition , we find that the Maxwell stress involved with the magnetic force produces significant redistribution of angular velocity within the disk . This leads to heightened transport of angular velocity outwards across the disk boundary compared to viscous stresses alone .",
        "rewrite_text": "We report the findings from our numerical simulations of magnetized accretion disks, where we solve the mean-field magnetohydrodynamic equations for an axisymmetric disk characterized by a specified radial distribution of angular velocity and mass fluxes, utilizing the shearing-box method. Our results indicate that the magnetic force is enhanced by differential rotation, resulting in large-scale poloidal fields that vary in strength with the radial distance as \\( ( r^{-3/2} ) \\) (with \\( r \\) being the radius). The toroidal component of the magnetic force also frequently increases due to the winding of poloidal field lines by shear flows. Consequently, the plasma beta factor decreases as one moves inward towards the central object. Furthermore, we observe that the Maxwell stress associated with the magnetic force causes a significant redistribution of angular velocity within the disk, leading to an increased outward transport of angular velocity across the disk's boundary compared to what is driven by viscous stresses alone.",
        "ori-fast-z-score": 2.223781796726481,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 2.9068883707497264
    },
    {
        "original_text": "We present novel examples of tree-level stable non-BPS D-branes in string theory, which are not associated with spacetime fermion zero modes and thus do not require the presence of orientifolds or other sources for tadpole cancellation. We show that these brane configurations can be constructed by wrapping unstable D-branes on supersymmetric cycles in Calabi-Yau threefolds. The resulting BPS states preserve half of the original supersymmetry but carry no net charge under any gauge group factor. These results provide new insights into the structure of moduli spaces of vacua in string theory. Introduction: In recent years there has been considerable interest in studying non-BPS D-brane (NBD) configurations in type II string theories  1  . NBDs have attracted attention because they may play an important role in understanding various phenomena such as tachyon condensation  2  , open-string pair production  3  , and black hole entropy  4  .\nIn this work we will focus our attention on NBDs whose stability is due to worldsheet instanton effects  5  -  8  rather than spacetime fermion zero-modes  9  . Such NBDs were first studied in  10  where it was shown that certain wrapped D3-branes could become stable at one-loop order without requiring the presence of orientifold planes  11  . Subsequently, several authors  12  -  16  have considered similar constructions involving different types of D-branes and compactifications. However, all of these works required some form of tadpole cancellation  17  so that the total RR-charge carried by the configuration vanishes. Tadpole cancellation conditions place strong constraints on the allowed values of fluxes and charges in the background geometry  18  . It would therefore be interesting if one could find examples of stable NBDs which did not require the presence of additional sources for tadpole cancellations.",
        "watermark_text": "We introduce novel instances of forest - level stable non - BPS D - branes in string theory , which are not associated with spacetime fermion zero configurations and therefore do not require the presence of orientifolds or other sources for tadpole cancellation . We see that these brane configurations can be built by wrapping unstable D - branes on supersymmetric cycles in Calabi - Yau threefolds .The resulting BPS states preserve half of the former supersymmetry but hold no net charge under any gauge group factor . These data provide fresh insights into the formation of moduli spaces of vacua in string theory .Introduction : In recent years there has been substantial interest in investigating non - BPS D - brane ( NBD ) arrangements in type II string theories 1 . NBDs have garnered attention because they may play an important role in understanding various phenomena such as tachyon condensation 2 , empty - string pair production 3 , and dark hole entropy 4 .In this research we will focus our focus on NBDs whose stability is due to worldsheet instanton interaction 5 - 8 instead than spacetime fermion zero - modes 9 . Such NBDs were first investigated in 10 where it was shown that particular tangled D3 - branes might remain stable at one - ring order without requiring the presence of orientifold planes 11 .Subsequently , various scientists 12 - 16 have thought equivalent constructions concerning diverse kinds of D - branes and compactifications . However , all of these works involved some kind of tadpole cancellation 17 so that the total RR - charge dropped by the configuration vanishes .Tadpole cancellation conditions place powerful restrictions on the allowed parameters of fluxes and charges in the background geometry 18 . It would therefore be attractive if one could discover examples of stable NBDs which did not require the presence of additional sources for tadpole cancellations .",
        "rewrite_text": "We present new examples of forest-level stable non-BPS D-branes in string theory that do not correlate with spacetime fermion zero configurations, eliminating the need for orientifolds or other sources for tadpole cancellation. These brane configurations can be constructed by wrapping unstable D-branes around supersymmetric cycles in Calabi-Yau threefolds. The resulting BPS states preserve half of the original supersymmetry and possess no net charge under any gauge group factor. This contributes valuable insights into the development of moduli spaces of vacua within string theory. \n\nIntroduction: Recently, there has been a growing interest in exploring non-BPS D-brane (NBD) setups in type II string theories. NBDs have attracted attention due to their potential importance in understanding phenomena such as tachyon condensation, empty-string pair production, and dark hole entropy. In this study, we will concentrate on NBDs whose stability stems from worldsheet instanton interactions rather than spacetime fermion zero-modes. Initial investigations into such NBDs were conducted, demonstrating that certain tangled D3-branes could remain stable at one-loop order without the necessity of orientifold planes. Following this, several researchers have proposed similar constructions involving various kinds of D-branes and compactifications. However, all these works have involved some form of tadpole cancellation, ensuring that the total RR charge contributed by the configuration is zero. Tadpole cancellation conditions impose significant constraints on the permissible parameters of fluxes and charges in the background geometry. Therefore, it is compelling to identify examples of stable NBDs that do not require additional sources for tadpole cancellation.",
        "ori-fast-z-score": -1.247219128924647,
        "water-fast-z-score": 7.0101164243872995,
        "rewrite-fast-z-score": 2.4351231101124045
    },
    {
        "original_text": "The standard model (SM) is the most successful theory in particle physics, but it has some problems such as hierarchy problem and CP violation. In this talk I will discuss how we can solve these problems by using string theories. First let us consider the SM with three generations of quarks and leptons. The Yukawa couplings are given by \nwhere  is the Higgs vacuum expectation value,  is the mass matrix for fermions ,  is the CKM mixing matrix , and  is the Kobayashi-Maskawa(KM) matrix . We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub .\nIn order to explain the observed CP violation in K meson system, we need at least one complex number in the KM matrix. However there are only four real numbers in the Yukawa coupling matrices. This means that we cannot determine all elements of the KM matrix uniquely. Therefore we introduce extra dimensions into our models so that we can obtain more degrees of freedom.",
        "watermark_text": "The conventional model ( SM ) is the most efficient model in particle theory , but it has some problems such as hierarchy problem and CP violation . In this talk I will explore how we can answer these problems by using string theories .First let us consider the SM with three generations of quarks and leptons . The Yukawa couplings are given by where is the Higgs vacuum expectation parameter , is the mass vector for fermions , is the CKM mixing function , and is the Kobayashi - Maskawa ( KM ) matrix .We have two parameters in the KM matrix : one phase which causes CP violation and another parameter called Jarlskog invariant J = Im VudVub * / Re VudIm Vub . In order to explain the observed CP violation in K meson scheme , we require at least one complex number in the KM matrix .However there are only four real numbers in the Yukawa interaction matrices . This implies that we never determine all members of the KM matrix uniquely .Therefore we incorporate additional dimensions into our models so that we can obtain more degrees of liberty .",
        "rewrite_text": "The Standard Model (SM) is currently the most successful framework in particle physics, yet it faces several issues, including the hierarchy problem and CP violation. In this presentation, I will discuss potential solutions to these challenges through the application of string theory. We begin by examining the SM, which includes three generations of quarks and leptons. The Yukawa couplings are defined by the Higgs vacuum expectation value, the fermion mass vector, the CKM mixing matrix, and the Kobayashi-Maskawa (KM) matrix. The KM matrix contains two significant parameters: one phase responsible for CP violation and another known as the Jarlskog invariant, expressed as J = Im(Vud Vub*) / Re(Vud) Im(Vub). To account for the observed CP violation in the K meson system, at least one complex number is necessary in the KM matrix. However, the Yukawa interaction matrices only comprise four real numbers, which suggests that we cannot uniquely determine every element of the KM matrix. To address this limitation, we propose the introduction of extra dimensions in our models to provide additional degrees of freedom.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 4.093146241443879,
        "rewrite-fast-z-score": -0.9299811099505543
    },
    {
        "original_text": "We study the effect of an external magnetic field on the ground state properties in the Hubbard model with next-nearest neighbor hopping and spin-orbit coupling, which is relevant to iron-based superconductors. We show that the magnetic field induces a spontaneous breaking of the time-reversal symmetry at half-filling for any finite value of the spin-orbit coupling strength. The broken symmetry phase has two-fold degenerate energy bands and shows non-Fermi liquid behavior. In addition, we find that there exists another spontaneously-broken-symmetry phase without gapless excitations when the chemical potential lies between the upper and lower band edges. This phase also exhibits non-Fermi liquid behaviors. Finally, we discuss possible experimental consequences of our results. Introduction:-The discovery of high-Tc FeAs-based superconductors  1  has attracted much attention because they are believed to be unconventional  2  . It was found experimentally  3  that these materials have strong spin orbit (SO) interaction  4  , which leads to several interesting phenomena such as nematic order  5  , orbital ordering  6  , and anisotropic magnetoresistance  7  .\nIn this Letter, we consider the following extended Hubbard model: \nwhere c†iσ(ciσ) creates (annihilates) an electron with spin σ =↑ or ↓ at site i, nαβij= c † αji c βji denotes the density matrix element between sites j and i, t represents nearestneighbor hopping amplitude, t  stands for nextnearest-neighbor hopping amplitude, U is the local Coulomb repulsion, λ is the SO coupling constant, µ is the chemical potential, and B is the applied magnetic field along z-direction. Hereafter, we set t=1, t =0.3t, U=4t, and λ=0.1t unless otherwise stated  8  .",
        "watermark_text": "We explore the impact of an external magnetic force on the ground state properties in the Hubbard theory with next - nearest neighbor hopping and spin - orbit bonding , which is relevant to iron - based superconductors . We see that the magnetic force induces a spontaneous breaking of the period - reversal symmetry at half - filling for any finite value of the spin - orbit coupling strength .The broken symmetry phase has two - fold degenerate power bands and shows non - Fermi liquid activity . In addition , we find that there exists another spontaneously - breaking - symmetry phase without gapless excitations when the chemical potential sits between the inner and upper band edges .This phase additionally exhibits non - Fermi liquid processes . Finally , we explain possible experimental impacts of our findings .Introduction : - The discovery of high - Tc FeAs - based superconductors 1 has drew much attention because they are said to be unconventional 2 . It was shown experimentally 3 that these materials have stable spin orbit ( SO ) bonding 4 , which results to several interesting phenomena such as nematic order 5 , orbital ordering 6 , and anisotropic magnetoresistance 7 .In this Letter , we define the following modified Hubbard theory : where c † iσ ( ciσ ) produces ( annihilates ) an electron with spin σ = ↑ or [UNK] at site i , nαβij = c † αji c βji denotes the density matrix element between locations j and i , t represents nearestneighbor hopping frequency , t stands for nextnearest - neighbor hopping frequency , U is the local Coulomb repulsion , λ is the SO coupling constant , µ is the chemical potential , and B is the applied magnetic force along z - direction . Hereafter , we put t = 1 , t = 0 . 3t , U = 4t , and λ = 0 . 1t unless otherwise stated 8 .",
        "rewrite_text": "We investigate the effects of an external magnetic field on the ground state properties of the Hubbard model, incorporating next-nearest neighbor hopping and spin-orbit coupling, which are significant for iron-based superconductors. Our analysis reveals that the magnetic field leads to a spontaneous breaking of time-reversal symmetry at half-filling for any non-zero spin-orbit coupling strength. This broken symmetry phase is characterized by two-fold degenerate power bands and demonstrates non-Fermi liquid behavior. Additionally, we identify another phase with spontaneously broken symmetry that lacks gapless excitations when the chemical potential is positioned between the inner and outer band edges. This phase also shows non-Fermi liquid processes. Finally, we discuss the potential experimental implications of our results. \n\n**Introduction:** The discovery of high-temperature superconductors based on FeAs has attracted considerable interest, as they exhibit unconventional properties. Experimental studies have demonstrated that these materials possess stable spin-orbit coupling, which results in various fascinating phenomena, such as nematic order, orbital ordering, and anisotropic magnetoresistance. In this letter, we outline a modified Hubbard model where \\( c^{\\dagger}_{i\\sigma} \\) ( \\( c_{i\\sigma} \\) ) creates (annihilates) an electron with spin \\( \\sigma = \\uparrow \\) or \\( \\downarrow \\) at site \\( i \\). The density matrix element between sites \\( j \\) and \\( i \\) is denoted as \\( n_{\\alpha\\beta}^{ij} = c^{\\dagger}_{\\alpha j} c_{\\beta i} \\). We define \\( t \\) as the nearest neighbor hopping amplitude, \\( t' \\) as the next-nearest neighbor hopping amplitude, \\( U \\) as the local Coulomb repulsion, \\( \\lambda \\) as the spin-orbit coupling constant, \\( \\mu \\) as the chemical potential, and \\( B \\) as the applied magnetic field in the z-direction. For consistency, we will set \\( t = 1 \\), \\( t' = 0.3t \\), \\( U = 4t \\), and \\( \\lambda = 0.1t \\) unless specified otherwise.",
        "ori-fast-z-score": 0.09016696346674323,
        "water-fast-z-score": 6.106580268910347,
        "rewrite-fast-z-score": -0.24743582965269675
    },
    {
        "original_text": "We report the discovery of a new satellite galaxy, dubbed  A pair of bootes  (ApoBootes), orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1.5 x 10^10 M_sun . ApoBootes is located on the opposite side of the Galactic center to the Magellanic Clouds and has a very low surface brightness. We have used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this object. The photometric properties are consistent with those expected for a dwarf spheroidal galaxy. This work was supported by the Australian Research Council Discovery Project funding scheme under grant DP130104011. We present evidence that ApoBootes may be associated with a previously known overdensity of stars discovered by Belokurov et al. (2007) using SDSS data.",
        "watermark_text": "We report the discovery of a new satellite galaxy , nicknamed A couple of bootes ( ApoBootes ) , orbiting around our Galaxy at a distance of about 300 kpc in projection and with an estimated mass of 1 . 5 x 10 ^ 10 M _ sun . ApoBootes is situated on the opposite end of the Galactic center to the Magellanic Clouds and has a very low exterior brightness .We have utilized deep near - infrared images obtained by the VISTA telescope as part of the Vista Variables in the Via Lactea survey to identify this body . The photometric properties are compatible with those expected for a dwarf spheroidal galaxy .This research was supported by the Australian Research Council Discovery Project grant program under grant DP130104011 . We present evidence that ApoBootes might be identified with a previously known overdensity of stars found by Belokurov et al .( 2007 ) used SDSS information .",
        "rewrite_text": "We announce the discovery of a new satellite galaxy, affectionately called A couple of Boötes (ApoBootes), which orbits our Galaxy at an approximate distance of 300 kpc in projection and has an estimated mass of 1.5 x 10^10 M_sun. ApoBootes is located on the opposite side of the Galactic center from the Magellanic Clouds and possesses a very low surface brightness. To identify this galaxy, we used deep near-infrared images taken by the VISTA telescope as part of the Vista Variables in the Via Lactea survey. The galaxy’s photometric characteristics align with those typically associated with dwarf spheroidal galaxies. This research was funded by the Australian Research Council Discovery Project grant program under grant DP130104011. Furthermore, we present evidence suggesting that ApoBootes may correspond to a known stellar overdensity identified by Belokurov et al. (2007) using SDSS data.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.437601569801833,
        "rewrite-fast-z-score": 1.9867985355975657
    },
    {
        "original_text": "We present the results of an analysis of rise times for a sample of type Ia supernovae (SNeIa) with well-measured light curves and redshifts in order to determine whether there is one mode of SNeIa rise time, as suggested by Phillips et al. (1999), or two modes, as suggested by Riess et al. (1999). We find that our data are consistent with either model at the 2-sigma level; however, we cannot rule out the possibility of only one mode being present. The best-fit values for the parameters describing each distribution differ significantly between these models. If future observations confirm this result, it will have important implications on cosmological studies using SNeIa as distance indicators. In particular, if there really are two populations of SNeIa, then the use of a single value for the stretch parameter may lead to systematic errors in determining distances. \n \n Keywords: Supernova, Light curve",
        "watermark_text": "We present the conclusion of an assessment of rise periods for a sample of type Ia supernovae ( SNeIa ) with good - measured light curves and redshifts in order to find whether there is one mode of SNeIa rise history , as suggested by Phillips et al . ( 1999 ) , or two modes , as suggested by Riess et al .( 1999 ) . We see that our statistics are compatible with either model at the 2 - sigma level ; however , we cannot leave out the suggestion of only one mode being present .The best - fitting values for the variables describing each distribution depend greatly between these models . If future discoveries confirm this result , it will have important implications on cosmological experiments using SNeIa as distance indicators .In particular , if there really are two groups of SNeIa , then the using of a single value for the stretch variable might lead to systematic errors in calculating distances . Keywords : Supernova , Light curve",
        "rewrite_text": "We present the conclusion of an assessment regarding rise periods for a sample of Type Ia supernovae (SNeIa) with well-measured light curves and redshifts. Our aim was to determine whether a single mode of SNeIa rise history exists, as proposed by Phillips et al. (1999), or if there are two distinct modes, as suggested by Riess et al. (1999). Our statistical analysis indicates compatibility with both models at the 2-sigma confidence level; however, we cannot entirely rule out the possibility of a sole mode being present. The best-fitting values for the parameters describing each distribution vary significantly between the two models. If future observations corroborate our findings, it could have critical implications for cosmological studies that utilize SNeIa as distance indicators. Specifically, if two groups of SNeIa do exist, then applying a single value for the stretch variable could result in systematic errors in distance calculations. Keywords: Supernova, Light curve.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "We propose an alternative scenario for the evolution of our universe, which is based on the idea that the expansion rate of the universe may be driven by its curvature rather than dark energy. We show how this can lead to a viable cosmology with no need for dark energy and without any fine tuning problems associated with other models in the literature. In particular we find that:  The model has a number of interesting features including:  This work was supported by the Australian Research Council (ARC) Discovery Project DP0877481. Any opinions expressed are those of the authors only. 1 Introduction.\nThe discovery of accelerated cosmic expansion  1, 2  , as well as the recent detection of gravitational waves  3  have led to renewed interest in understanding the nature of gravity at large scales  4  . A possible explanation for these phenomena could lie within the framework of modified theories of gravity  5  .\nIn order to explain the observed acceleration of the universe it seems necessary to introduce some form of  dark energy   6  into Einstein s field equations  7, 8  . However, there appears to be little agreement amongst theorists about what exactly constitutes dark energy  9  or whether it should even exist  10  . Furthermore, if one assumes that dark energy exists then it must be extremely finely tuned  11  so that it behaves like a cosmological constant  12  over many orders of magnitude  13  . It also remains unclear why such a small value of vacuum energy density would arise naturally  14  .\nAnother possibility is that the apparent accelerating behaviour of the universe arises due to quantum effects  15  . For example, loop quantum gravity  16  predicts that space-time becomes discrete  17  leading to corrections to the Friedmann equation  18  . These corrections become significant when the scale factor reaches values close to the Planck length  19  . Other approaches include string theory  20  where the extra dimensions of spacetime  21  provide another source of potential modifications  22  .",
        "watermark_text": "We suggest an additional scenario for the evolution of our universe , which is based on the idea that the development frequency of the universe might be motivated by its curvature instead than dark energy . We see how this can lead to a viable cosmology with no requirement for black light and without any coarse tuning problems identified with other models in the literature .In particular we find that : The model has a number of interesting features including : This research was supported by the Australian Research Council ( ARC ) Discovery Project DP0877481 . Any views stated are those of the writers only .1 Introduction . The observation of rapid cosmic expansion 1 , 2 , as also as the recent discovery of gravitational waves 3 have led to renewed concern in understanding the nature of gravitational at large scales 4 .A potential explanation for these phenomena could lay within the framework of revised theories of gravitational 5 . In order to explain the known acceleration of the universe it appears necessary to introduce some kind of dark energy 6 into Einstein s field equations 7 , 8 .However , there seems to be little consensus amongst theorists about what actually constitutes black light 9 or whether it should even exist 10 . Furthermore , if one suppose that dark energy occurs then it must be extremely finely tuned 11 so that it behaves like a cosmological factor 12 over numerous orders of magnitude 13 .It therefore remains unsure why such a small measure of vacuum energy density would occur naturally 14 . Another possibility is that the actual accelerating behaviour of the universe occurs due to quantum effects 15 .For instance , loop quantum gravitational 16 predicts that space - time remains discrete 17 leading to corrections to the Friedmann equation 18 . These corrections get significant when the scale factor passes values close to the Planck size 19 .Other approaches involve string theory 20 where the extra dimensions of spacetime 21 offer another source of potential modifications 22 .",
        "rewrite_text": "We propose an alternative scenario for the evolution of our universe, suggesting that the rate of development may be influenced by its curvature rather than dark energy. This perspective can yield a viable cosmological model that eliminates the need for \"black light\" and avoids the fine-tuning issues associated with other theoretical constructs. Notably, our model features several intriguing aspects, with support from the Australian Research Council (ARC) Discovery Project DP0877481. The opinions expressed here are solely those of the authors. \n\n1. Introduction: The observation of rapid cosmic expansion and the recent detection of gravitational waves have intensified efforts to comprehend gravity's nature on cosmic scales. One potential explanation for these phenomena may lie within revised gravitational theories. To account for the observed acceleration of the universe, it seems necessary to incorporate some form of dark energy into Einstein's field equations. However, there is still considerable debate among theorists regarding the true nature of dark energy and whether it even exists. Moreover, if dark energy does exist, its characteristics must be extraordinarily finely tuned to behave like a cosmological constant across many orders of magnitude. This raises questions about why such a minuscule vacuum energy density would naturally arise. An alternative possibility is that the universe's accelerating behavior is a result of quantum effects. For example, loop quantum gravity predicts that spacetime is discrete, resulting in modifications to the Friedmann equation that become significant when the scale factor approaches values near the Planck scale. Additional frameworks, such as string theory, propose that the extra dimensions of spacetime may also provide avenues for potential modifications.",
        "ori-fast-z-score": 0.3287979746107146,
        "water-fast-z-score": 9.308061022576076,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "We present new results on the nature of dust extinction in external galaxies, based on observations with HST/ACS of type Ia supernovae (SNe) at redshifts z=0.1-0.7. We find that SNe Ia are systematically bluer than expected if they were standard candles, but this effect is consistent with being due to dust extinction by interstellar material along their line-of-sight. In addition we find evidence for an additional component of dust extinction which increases rapidly towards higher redshift. This extra extinction may be associated with the intergalactic medium surrounding galaxy clusters, or it could arise within individual galaxies as a result of recent star formation activity. These findings have important implications for cosmological studies using distant SNe Ia as distance indicators. \n \n Keywords: Galaxy cluster, Interstellar matter, Intergalactic medium, Redshift evolution, Supernova, Supernova Ia",
        "watermark_text": "We report new data on the nature of dust extinction in external galaxies , based on observations with HST / ACS of type Ia supernovae ( SNe ) at redshifts z = 0 . 1 - 0 . 7 . We see that SNe Ia are steadily bluer than expected if they were standard candles , but this effect is compatible with being owing to dust disappearance by interstellar material along their line - of - view .In addition we find proof for an additional element of dust extinction which increases quickly towards higher redshift . This extra extinction may be correlated with the intergalactic medium surrounding galaxy galaxies , or it could occur within individual stars as a product of recent star formation activity .These conclusions have important implications for cosmological experiments using distant SNe Ia as distance indicators . Keywords : Galaxy cluster , Interstellar matter , Intergalactic medium , Redshift formation , Supernova , Supernova Ia",
        "rewrite_text": "We present new findings regarding dust extinction in external galaxies, derived from HST/ACS observations of type Ia supernovae (SNe) at redshifts ranging from z = 0.1 to z = 0.7. Our analysis shows that SNe Ia appear consistently bluer than anticipated if they were considered standard candles; however, this phenomenon can be attributed to the presence of dust along their line of sight caused by interstellar material. Furthermore, we have identified evidence of an additional component of dust extinction that intensifies significantly at higher redshifts. This increased extinction may be linked to the intergalactic medium surrounding galaxies or could arise within individual stars due to recent star formation activity. These findings have significant implications for cosmological studies utilizing distant SNe Ia as distance markers. Keywords: Galaxy cluster, interstellar matter, intergalactic medium, redshift formation, supernova, supernova Ia.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": -0.36650833306891567
    },
    {
        "original_text": "We present the results of our study on common envelope evolution (CEE) in binary systems with helium white dwarfs and red giant secondaries, using detailed stellar evolutionary models for both components. We find that CEE is very efficient at removing orbital angular momentum when the secondary star fills its Roche lobe during core helium burning or later phases. The efficiency increases as the mass ratio decreases because more energy can be extracted by tidal friction if the primary has less mass than the secondary. \n \n In addition to this effect, we also found that the amount of orbital angular momentum removed depends strongly on whether the system undergoes stable or unstable mass transfer after the onset of RLOF. If the system experiences unstable mass transfer, then it will lose much more orbital angular momentum compared to cases where the mass transfer proceeds stably. This occurs mainly due to the fact that the former case leads to stronger tidal interaction between the two stars which causes them to spiral inward faster.",
        "watermark_text": "We present the conclusion of our research on common envelope development ( CEE ) in binary systems with helium white dwarfs and dark giant secondaries , using detailed stellar evolutionary estimates for both components . We see that CEE is very efficient at removing orbital angular velocity when the secondary star fills its Roche lobe during core helium burning or later periods .The efficiency increases as the mass ratio falls because more power can be extracted by tidal vibration if the primary has less mass than the secondary . In addition to this effect , we also discovered that the quantity of orbital angular velocity removed depends strongly on whether the system undergoes stable or unstable mass transfer after the beginning of RLOF .If the system encounters unstable mass transfer , then it will losing much more orbital angular velocity compared to cases where the mass transfer progresses stably . This occurs mostly owing to the fact that the former situation leads to greater tidal interaction between the two stars which makes them to spiral inward faster .",
        "rewrite_text": "We present our findings on the development of common envelopes (CEE) in binary systems that contain helium white dwarfs and dark giant secondaries, utilizing detailed stellar evolutionary models for both components. Our research indicates that CEE is highly effective at reducing orbital angular velocity, particularly when the secondary star fills its Roche lobe during core helium burning or subsequent phases. The efficiency of this process increases as the mass ratio decreases, since a less massive primary allows for more energy to be extracted through tidal interactions. Furthermore, we found that the amount of orbital angular velocity removed is significantly influenced by whether the system experiences stable or unstable mass transfer after the onset of Roche lobe overflow (RLOF). In cases of unstable mass transfer, the system loses a considerably greater amount of orbital angular velocity compared to instances of stable mass transfer. This is primarily due to the enhanced tidal interaction in unstable scenarios, which drives the stars to spiral inward at an accelerated rate.",
        "ori-fast-z-score": -0.10482848367219183,
        "water-fast-z-score": 5.136595699937399,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "In this thesis we study the problem of optimizing resource allocation in wireless networks by using non-cooperative game theory. We consider three different problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. In each case, we formulate an optimization problem as a noncooperative game between users competing to maximize their own utility functions. Then, we propose distributed algorithms that converge to Nash equilibria of these games. Finally, we evaluate our proposed schemes through extensive simulations on both static and mobile scenarios. \n \n Keywords: Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium. 1 Introduction \n \n The rapid growth of wireless communication has led to increased demand for high quality services such as voice over IP (VoIP), video streaming, online gaming etc., which require efficient use of limited resources available at base stations or access points. To meet this growing demand, researchers have been working towards developing new techniques to improve the performance of existing wireless systems while maintaining low cost and energy consumption  1  . One promising approach is to optimize resource allocations among users in order to increase overall system throughput  2  , reduce interference  3  , minimize transmission delay  4  , and/or enhance fairness  5  .\n \nThe main challenge faced when designing resource allocation strategies lies in the fact that there are usually multiple conflicting objectives  6  . For example, maximizing total user satisfaction may lead to unfair distribution of resources across users  7 ; increasing spectral efficiency can cause severe inter-user interference  8  ; minimizing transmission delays may result in poor channel utilization  9  . Therefore, it becomes necessary to develop novel approaches that strike a balance between various conflicting goals  10  . \n \n This work was supported in part by NSF under Grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "watermark_text": "In this dissertation we study the question of optimizing bandwidth allocation in mobile networks by using non - cooperative play logic . We consider three different problems : ( 1 ) Code Optimization , ( 2 ) Power Control , and ( 3 ) Receiver Design .In each situation , we formulate an optimization problem as a noncooperative game between participants competing to maximize their own utility functions . Then , we develop distributed methods that converge to Nash equilibria of these games .Finally , we assess our proposed arrangements through numerous simulations on both static and wireless situations . Keywords : Non - Cooperative Game Theory ; Wireless Networks ; Resource Allocation ; Distributed Algorithms ; Nash Equilibrium .1 Introduction The rapid increase of mobile communication has led to renewed demand for high quality services such as voice over IP ( VoIP ) , television playback , internet gaming etc . , which require efficient application of restricted resources accessible at base stations or entry points . To address this increasing demand , researchers have been pushing towards developing innovative techniques to upgrade the performance of older broadband systems while maintaining low cost and energy consumption 1 .One promising solution is to optimize resource allocations among consumers in order to expand overall network throughput 2 , avoid noise 3 , eliminate communication delay 4 , and / or enhance fairness 5 . The main challenge facing when designing resource allocation strategies rests in the fact that there are typically many conflicting aims 6 .For instance , maximizing gross consumer satisfaction would result to inappropriate distribution of assets across users 7 ; increasing spectral capacity can cause profound cross - customer interference 8 ; minimizing broadcast delays may end in poor channel utilization 9 . Therefore , it becomes necessary to develop new approaches that strike a balance between various differing aims 10 .This project was supported in part by NSF under Grants CNS - 0721440 , CCF - 0729260 , ECCS - 0801571 , and CNS - 0916275 .",
        "rewrite_text": "In this dissertation, we explore the optimization of bandwidth allocation in mobile networks through the lens of non-cooperative game theory. We address three key problems: (1) Code Optimization, (2) Power Control, and (3) Receiver Design. For each problem, we frame an optimization challenge as a non-cooperative game where participants strive to maximize their individual utility functions. We then propose distributed methods that converge towards the Nash equilibria of these games. To evaluate the effectiveness of our strategies, we conduct extensive simulations in both static and wireless environments.\n\n**Keywords:** Non-Cooperative Game Theory; Wireless Networks; Resource Allocation; Distributed Algorithms; Nash Equilibrium.\n\n**1. Introduction** \n\nThe rapid growth of mobile communication has sparked an increased demand for high-quality services such as voice over IP (VoIP), television streaming, and online gaming, which necessitate the efficient use of limited resources available at base stations or access points. In response to this growing demand, researchers are focused on developing innovative strategies to enhance the performance of existing broadband systems while keeping costs and energy usage low. One promising approach is to optimize resource allocations among users in order to improve overall network throughput, reduce noise, eliminate communication delays, and enhance fairness. \n\nHowever, the primary challenge in designing effective resource allocation strategies lies in addressing multiple conflicting objectives. For example, aiming to maximize overall user satisfaction can lead to an inequitable distribution of resources; increasing spectral capacity may cause significant interference among users; and minimizing broadcast delays could result in inefficient channel utilization. Thus, it is crucial to develop new methods that achieve a balance among these various competing goals. \n\nThis research was partially supported by the NSF under grants CNS-0721440, CCF-0729260, ECCS-0801571, and CNS-0916275.",
        "ori-fast-z-score": -1.7320508075688774,
        "water-fast-z-score": 8.825211257612851,
        "rewrite-fast-z-score": -0.16783627165933782
    },
    {
        "original_text": "We present measurements of the top quark mass in final states containing one or two jets and large missing transverse energy, based on data collected by the Collider Detector at Fermilab during Run I (1992-1996). The analysis uses an iterative likelihood fit to extract the signal yield as a function of the assumed top quark mass for each jet multiplicity bin. We find mtop = 174.3 +/- 1.7 GeV/c2(stat) +/-3.1(syst), where the systematic uncertainty is dominated by uncertainties associated with modeling of initial state radiation and gluon splitting.  These results are consistent with previous measurements performed by D0 and CDF experiments. This measurement supersedes our previously published result. \nThe full text can be found at: http://arxiv.org/abs/0907.4189v1  # Measurement of the top-quarks mass using missing E_T+jets events with secondary-vertex b-tagging at CDF-II",
        "watermark_text": "We present measurements of the top quark mass in final states involving one or two jets and large missing radial energy , using on evidence generated by the Collider Detector at Fermilab during Run I ( 1992 - 1996 ) . The calculation utilizes an iterative probability fit to extract the signal yield as a function of the expected top quark mass for each jet multiplicity bin .We get mtop = 174 . 3 + / - 1 . 7 GeV / c2 ( stat ) + / - 3 . 1 ( syst ) , where the systematic uncertainty is dominated by uncertainties involved with modeling of initial state radiation and gluon splitting . These conclusions are compatible with previous measurements completed by D0 and CDF experiments .This measurement supersedes our previously reported result . The full text can be found at : www : / / arxiv . org / abs / 0907 . 4189v1 # Measurement of the top - quarks weight used missing E _ T + jets events with secondary - vertex b - tagging at CDF - II",
        "rewrite_text": "We present measurements of the top quark mass derived from final states featuring one or two jets and significant missing transverse energy. These measurements are based on data collected by the Collider Detector at Fermilab during Run I (1992 - 1996). The analysis employs an iterative probability fit to determine the signal yield as a function of the expected top quark mass for each jet multiplicity bin. Our result is mtop = 174.3 ± 1.7 GeV/c² (stat) ± 3.1 (syst), with the systematic uncertainty primarily arising from the modeling of initial state radiation and gluon splitting. These findings are consistent with earlier measurements conducted by the D0 and CDF experiments. This measurement replaces our previously reported result. The full text is available at: www/arxiv.org/abs/0907.4189v1 # Measurement of the top-quark mass using missing E_T + jets events with secondary-vertex b-tagging at CDF-II.",
        "ori-fast-z-score": -1.9205531989934397,
        "water-fast-z-score": 3.2009219983223995,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "We present an analysis of the virialization process in protogalactic halos, using high-resolution N-body simulations with gas dynamics and radiative cooling. We find that the density profiles of dark matter halos are well described by the Navarro-Frenk-White (NFW) profile at all redshifts z < 10. The NFW concentration parameter c(z), which characterizes how concentrated is the halo mass distribution relative to its mean density, increases rapidly as redshift decreases below z = 5. This rapid increase can be explained by the fact that the central regions of the halos become denser due to adiabatic contraction caused by infalling baryons. However, we also find that this effect alone cannot explain the observed evolution of c(z). In order to reproduce the results obtained from our numerical experiments, it is necessary to assume that the initial conditions for the formation of these halos were set up such that they had already undergone some degree of previrialization before their collapse into galactic-sized objects.",
        "watermark_text": "We present an assessment of the virialization system in protogalactic halos , using high - resolution N - bodies simulations with gas mechanics and radiative cooling . We see that the density characteristics of deep material halos are better represented by the Navarro - Frenk - White ( NFW ) model at all redshifts z < 10 .The NFW concentration function c ( z ) , which characterizes how concentrated is the halo mass distribution relative to its average density , increases quickly as redshift decreases below z = 5 . This rapid increase can be described by the fact that the central regions of the halos become denser thanks to adiabatic collapse induced by infalling baryons .However , we also find that this effect alone cannot explain the known development of c ( z ) . In order to reproduce the results derived from our numerical studies , it is important to assume that the early conditions for the formation of these halos were setting up such that they had already undergone some degree of previrialization before their collapse into galactic - sized bodies .",
        "rewrite_text": "We provide an evaluation of the virialization processes in protogalactic halos through high-resolution N-body simulations that incorporate gas dynamics and radiative cooling. Our analysis indicates that the density profiles of dense material halos are more accurately represented by the Navarro-Frenk-White (NFW) model across all redshifts z < 10. Notably, the NFW concentration function c(z), which indicates the concentration of halo mass distribution relative to its mean density, increases rapidly as redshift falls below z = 5. This steep rise can be attributed to the increasing density in the central regions of the halos resulting from adiabatic collapse triggered by incoming baryons. However, we also observe that this explanation alone is insufficient to fully account for the established evolution of c(z). To align our findings with the outcomes from our numerical simulations, it is crucial to consider that the initial conditions for the formation of these halos allowed for some degree of previrialization prior to their collapse into galactic-scale structures.",
        "ori-fast-z-score": -0.9561828874675149,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 1.1952286093343936
    },
    {
        "original_text": "We report on the discovery and timing analysis of pulsar PSR J1852 + 0040, which is associated with supernova remnant (SNR) Kes 79. The pulsar has spin period P = 1.56 ms and characteristic age τc = 3 kyrs. We find that its surface magnetic field strength Bs = 2 × 10^10 G, assuming an inclination angle i = 60 degrees between the rotation axis and line-of-sight to Earth. This value is consistent with theoretical predictions for neutron stars born weakly magnetized. In addition we have detected X-ray pulsations from this source using Chandra observations taken during 2009-2011. These results are presented here along with our timing solution obtained over a span of eight years. \n \n Keywords: Pulsar, Supernova Remnant, X-Ray Pulsars, Chandra Observatory, Radio Pulsar Timing \n \n Introduction \n \n A number of young radio pulsars show very low values of their surface dipole magnetic fields inferred from their spin-down rates. Such objects include Geminga, B1951+32, B1620-26, B1509-58, B0531+21, B1757-24, B1800-21, B1853+01, B1857+09, B1913+16, B1957+50, B2224+65, B2303+46, B2334+61, B0826-34, B1133+16, B1237+25, B1929+10, B1930+42, B1932+29, B1933+16, B1944+43, B1946+35, B1947+36, B1953+50, B1954+28, B1956+54, B1959+20, B1960+03, B1962+14, B1963+27, B1968+18, B1969+22, B1970+38, B1971+02, B1973+51, B1974+14, B1975+28, B1976+44, B1977+47, B1980+12, B1981+24, B1983",
        "watermark_text": "We report on the discovery and timing examination of pulsar PSR J1852 + 0040 , which is associated with supernova remnant ( SNR ) Kes 79 . The pulsar has spin date P = 1 . 56 ms and typical age τc = 3 kyrs .We see that its surface magnetic force density Bs = 2 × 10 ^ 10 G , assuming an inclination angle i = 60 degrees between the rotation axis and line - of - view to Earth . This value is compatible with theoretical estimates for neutron galaxies born mildly magnetized .In addition we have discovered X - ray pulsations from this source using Chandra measurements taken during 2009 - 2011 . These conclusions are presented here along with our timing solution acquired over a period of eight years .Keywords : Pulsar , Supernova Remnant , X - Ray Pulsars , Chandra Observatory , Radio Pulsar Timing Introduction A number of young radio pulsars report very low readings of their surface dipole magnetic fields inferred from their spin - down rates . Such galaxies include Geminga , B1951 + 32 , B1620 - 26 , B1509 - 58 , B0531 + 21 , B1757 - 24 , B1800 - 21 , B1853 + 01 , B1857 + 09 , B1913 + 16 , B1957 + 50 , B2224 + 65 , B2303 + 46 , B2334 + 61 , B0826 - 34 , B1133 + 16 , B1237 + 25 , B1929 + 10 , B1930 + 42 , B1932 + 29 , B1933 + 16 , B1944 + 43 , B1946 + 35 , B1947 + 36 , B1953 + 50 , B1954 + 28 , B1956 + 54 , B1959 + 20 , B1960 + 03 , B1962 + 14 , B1963 + 27 , B1968 + 18 , B1969 + 22 , B1970 + 38 , B1971 + 02 , B1973 + 51 , B1974 + 14 , B1975 + 28 , B1976 + 44 , B1977 + 47 , B1980 + 12 , B1981 + 24 , B1983",
        "rewrite_text": "We present our findings on the pulsar PSR J1852 + 0040, which is linked to the supernova remnant Kes 79. This pulsar exhibits a spin period of P = 1.56 ms and is estimated to have a characteristic age of τc = 3 kyrs. Assuming an inclination angle of i = 60 degrees between its rotation axis and the line of sight to Earth, we calculate its surface magnetic field strength to be Bs = 2 × 10^10 G. This value aligns with theoretical predictions for neutron stars that are born with relatively weak magnetic fields. Furthermore, we have detected X-ray pulsations from this source through Chandra observations conducted from 2009 to 2011. This paper details our timing solution derived over an eight-year period, along with these significant findings. \n\nKeywords: Pulsar, Supernova Remnant, X-Ray Pulsars, Chandra Observatory, Radio Pulsar Timing \n\nIntroduction: Several young radio pulsars have been reported to have notably low measurements of their surface dipole magnetic fields, as inferred from their spin-down rates. Among these pulsars are Geminga, B1951 + 32, B1620 - 26, B1509 - 58, B0531 + 21, B1757 - 24, B1800 - 21, B1853 + 01, B1857 + 09, B1913 + 16, B1957 + 50, B2224 + 65, B2303 + 46, B2334 + 61, B0826 - 34, B1133 + 16, B1237 + 25, B1929 + 10, B1930 + 42, B1932 + 29, B1933 + 16, B1944 + 43, B1946 + 35, B1947 + 36, B1953 + 50, B1954 + 28, B1956 + 54, B1959 + 20, B1960 + 03, B1962 + 14, B1963 + 27, B1968 + 18, B1969 + 22, B1970 + 38, B1971 + 02, B1973 + 51, B1974 + 14, B1975 + 28, B1976 + 44, B1977 + 47, B1980 + 12, B1981 + 24, and B1983.",
        "ori-fast-z-score": -0.8962581595302719,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -0.3841106397986879
    },
    {
        "original_text": "We present broadband (radio to X-ray) observations of GRB 061121, one of the most energetic gamma-ray bursts ever detected by Swift/BAT with an isotropic equivalent energy release of 1.8 x 10^54 erg in the 15-350 keV band. The temporal behavior of this event was complex; it consisted of multiple pulses that were superimposed on top of each other during both the prompt emission phase as well as the early part of its afterglow. We find evidence for two distinct components in the optical light curve -one which decays rapidly at first but then flattens out later-on timescales ranging between 0.1-10 days post-burst. This flattening could be due either to continued activity of the central engine or to refreshed shocks. In addition we detect significant radio emission upto 100 days post-burst. Our results are consistent with those obtained using data from the VLA and ATCA telescopes.",
        "watermark_text": "We present broadband ( radio to X - ray ) observations of GRB 061121 , one of the most intense gamma - ray clusters yet detected by Swift / BAT with an isotropic equivalent energy release of 1 . 8 x 10 ^ 54 erg in the 15 - 350 keV band . The temporal response of this event was complex ; it consisted of multiple pulses that were superimposed on top of each other during both the prompt emission stage as well as the early portion of its afterglow .We get data for two different components in the optical light curve - one which decays fast at first but then flattens out eventually - on timescales ranging between 0 . 1 - 10 days post - break . This flattening may be due either to continued action of the main motor or to refreshed shocks .In addition we find considerable radio emission upto 100 months post - burst . Our results are compatible with those acquired using data from the VLA and ATCA telescopes .",
        "rewrite_text": "We present broadband observations of GRB 061121, one of the most intense gamma-ray bursts detected by Swift/BAT, with an isotropic equivalent energy release of 1.8 x 10^54 erg in the 15-350 keV range. The event exhibited a complex temporal profile, featuring multiple overlapping pulses during both the prompt emission phase and the initial part of its afterglow. Our analysis reveals two distinct components in the optical light curve: one exhibiting a rapid decay initially, followed by a gradual flattening on timescales of 0.1 to 10 days post-break. This flattening may be attributed to the ongoing activity of the primary engine or to refreshed shocks. Additionally, we observe significant radio emission persisting up to 100 months after the burst. Our findings align well with data obtained from the VLA and ATCA telescopes.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 3.810003810005715,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an algorithm for computing spherical wavelets and their associated scaling functions, which are used to analyze data defined over the unit sphere in three dimensions. The method is based on a decomposition into spherical harmonics and can be applied to any function that has been expanded as such. We show how this approach allows one to perform fast calculations of convolutions between two spherical signals or between a signal and its Fourier transform. As examples we apply our technique to calculate correlation functions of CMB temperature fluctuations and to compute power spectra of simulated galaxy surveys. Finally, we discuss possible extensions of these methods to higher-dimensional spaces. Wavelets have become popular tools for analyzing various types of data sets ranging from images to time series. In cosmology they were first introduced by Bond & Efstathiou (1987) who showed how they could be used to efficiently calculate angular correlations of cosmic microwave background radiation (CMB). Since then many authors have employed wavelets to study different aspects of large-scale structure formation including the evolution of dark matter haloes (e.g., Colombi et al. (1998) ), gravitational lensing effects (e.g., Jain et al. (2000)), weak gravitational lensing statistics (e.g., Schneider et al. (2002)) , and the clustering properties of galaxies (e.g., Percival et al. (2003)). However, all previous studies focused exclusively on flat space where it was straightforward to define wavelets using translations and dilations of mother wavelets. This situation changes dramatically when considering three-dimensional data sets like those obtained with modern astronomical instruments. Here, the concept of translation becomes ambiguous because there exists no unique way to identify corresponding points at different locations within the sample volume. Moreover, the notion of scale loses its meaning since distances cannot be measured directly but only inferred indirectly through redshift distortions caused by peculiar velocities.",
        "watermark_text": "We present an algorithm for modeling spherical wavelets and their accompanying scaling functions , which are applied to analyze information defined over the unit sphere in three dimensions . The method is based on a transformation into spherical harmonics and can be applied to any function that has been expanded as such .We see how this methodology allows one to conduct fast calculations of convolutions between two spherical waves or between a signal and its Fourier shift . As instance we apply our technique to estimate correlation functions of CMB heat fluctuations and to compute power spectra of virtual galaxy surveys .Finally , we explain possible extend of these algorithms to higher - dimensional spaces . Wavelets have developed popular tools for studying numerous types of evidence sets ranging from objects to time series .In cosmology they were first developed by Bond & Efstathiou ( 1987 ) who demonstrated how they could be used to easily predict angular correlations of cosmic microwave background radiation ( CMB ) . Since then many writers have utilized wavelets to study various details of large - scale system formation including the evolution of bright matter haloes ( e . g . , Colombi et al .( 1998 ) ) , gravity lensing effects ( e . g . , Jain et al . ( 2000 ) ) , weak gravitational lensing statistics ( e . g . , Schneider et al .( 2002 ) ) , and the clustering behavior of galaxies ( e . g . , Percival et al . ( 2003 ) ) .However , all previous research focused exclusively on soft space where it was straightforward to define wavelets using translations and dilations of parent wavelets . This condition shifts significantly when assessing three - dimensional data sets like those acquired with modern astronomical spacecraft .Here , the notion of translation becomes obscure because there exists no unique way to identify corresponding locations at different places within the sample volume . Moreover , the notion of scale loses its significance since distances never be realized directly but only inferred indirectly through redshift distortions caused by unique velocities .",
        "rewrite_text": "We introduce an algorithm for modeling spherical wavelets and their associated scaling functions, which can be utilized to analyze data defined on the unit sphere in three dimensions. This method is founded on a transformation to spherical harmonics and is applicable to any function that can be expressed in this form. Our approach demonstrates the ability to perform rapid calculations of convolutions between two spherical waves or between a signal and its Fourier transform. As an example, we employ our technique to estimate correlation functions of cosmic microwave background (CMB) temperature fluctuations and to compute power spectra for simulated galaxy surveys. Additionally, we outline potential extensions of these algorithms to higher-dimensional spaces. Wavelets have become invaluable tools for analyzing a wide range of data sets, from objects to time series. In the field of cosmology, they were first introduced by Bond & Efstathiou (1987), who showcased their utility in predicting angular correlations of CMB radiation. Since then, numerous researchers have applied wavelets to investigate various aspects of large-scale structure formation, including the evolution of luminous matter halos (e.g., Colombi et al. 1998), gravitational lensing effects (e.g., Jain et al. 2000), weak gravitational lensing statistics (e.g., Schneider et al. 2002), and galaxy clustering patterns (e.g., Percival et al. 2003). However, prior studies have primarily concentrated on flat spaces where defining wavelets through translations and dilations of parent wavelets is straightforward. This complexity increases when examining three-dimensional data from modern astronomical instruments, as the concept of translation becomes ambiguous without a clear method to match corresponding points across the sample volume. Additionally, the idea of scale loses its relevance, as distances are not directly measurable but can only be inferred through redshift distortions arising from individual velocities.",
        "ori-fast-z-score": -0.727606875108999,
        "water-fast-z-score": 8.165366042889877,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an analytical model for evaluating the effect of X-ray scattering on image quality in grazing incidence X-ray telescopes (GIXTs). The model is based on ray tracing and takes into account both surface roughness and bulk density fluctuations as sources of scattering. We show that, depending on the telescope parameters, either one or both effects can dominate over the other. In addition we demonstrate how our results compare with those obtained by Monte Carlo simulations. Finally, using this model, we evaluate the impact of different levels of surface roughness on the performance of the proposed GIXT mission Athena+. Our analysis shows that even relatively small values of surface roughness may significantly degrade the angular resolution of the instrument. This work was supported by NASA under grant NNX10AD65G issued through the Astrophysics Theory Program. Keywords: X-ray optics, Ray tracing, Surface roughness, Scattering",
        "watermark_text": "We present an analytical theory for evaluating the impact of X - ray scattering on image performance in grazing incidence X - ray telescopes ( GIXTs ) . The model is based on ray tracing and takes into consideration both exterior roughness and bulk density fluctuations as causes of scattering .We see that , depending on the telescope parameters , either one or both factors can influence over the other . In addition we prove how our findings compare with those achieved by Monte Carlo simulations .Finally , using this analysis , we assess the impact of different amounts of exterior roughness on the performance of the suggested GIXT mission Athena + . Our study shows that even surprisingly low measures of surface roughness might considerably degrade the angular resolution of the instrument .This project was supported by NASA under grant NNX10AD65G given through the Astrophysics Theory Program . Keywords : X - ray optics , Ray scanning , Surface roughness , Scattering",
        "rewrite_text": "We introduce an analytical framework for assessing the influence of X-ray scattering on image quality in grazing incidence X-ray telescopes (GIXTs). This model employs ray tracing and accounts for both surface roughness and fluctuations in bulk density as contributing factors to scattering. Our analysis reveals that, depending on the specific parameters of the telescope, one or both of these factors can affect the other. Furthermore, we demonstrate how our results align with those obtained via Monte Carlo simulations. Utilizing this analysis, we evaluate the effects of varying levels of surface roughness on the performance of the proposed GIXT mission Athena+. Our findings indicate that even minimal surface roughness can significantly impair the angular resolution of the instrument. This research was funded by NASA under grant NNX10AD65G through the Astrophysics Theory Program. Keywords: X-ray optics, ray tracing, surface roughness, scattering.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We study the diameter and clustering coefficient for several classes of random graphs with power law degree distribution, including preferential attachment (PA) networks.  We show that PA networks have small diameters if their growth is slow enough; this result holds even when we allow multiple edges between nodes. For fast growing PA networks, however, there are examples where the diameter grows exponentially as a function of n. In addition to studying the diameter, we also consider the clustering coefficient of these networks. The clustering coefficient measures how well connected neighbors of a node are among themselves. We prove upper bounds on the clustering coefficients of PA networks which hold regardless of the rate at which new nodes join the network. Finally, we present some numerical results showing that our theoretical predictions agree very well with simulations. Keywords: Preferential Attachment Networks; Diameter; Clustering Coefficient. 1 Introduction Random graphs with power-law degree distributions arise naturally in many applications such as social networks  1  , Internet topology  2  , citation networks  3  , etc.. These types of networks can be generated by using various mechanisms known as  network growth models ; see  4  for an overview. One popular model is called  preferential attachment  or  Power-Law Graphs   5  . This class of networks has been studied extensively over the past decade  6  -  8  .\nIn this work, we focus on two important properties of these graphs: the diameter and the clustering coefficient  9  . The diameter D(G) of a graph G = (V, E) is defined as max{d(u, v) | u, v ∈ V }, i.e., it is the maximum distance between any pair of vertices in G. The clustering coefficient C(v) of a vertex v is defined as the fraction of pairs of neighbors of v that are adjacent to each other  10  . It is easy to verify that both quantities depend only on the degrees of the nodes in the graph  11  ; hence they do not change under edge rewirings  12  .",
        "watermark_text": "We test the diameter and clustering density for numerous classes of random graphs with power law degree distribution , notably preferential attachment ( PA ) networks . We see that PA networks have small diameters if their development is slow enough ; this consequence holds even when we allow multiple edges between nodes .For quick expanding PA systems , however , there are examples where the diameter grows exponentially as a function of n . In addition to investigating the radius , we also consider the clustering coefficient of these networks . The clustering coefficient represents how well connected neighbors of a node are among themselves .We derive upper limits on the clustering coefficients of PA systems which hold regardless of the frequency at which additional nodes join the organization . Finally , we present some numerical findings indicating that our theoretical estimates agree very best with simulations .Keywords : Preferential Attachment Networks ; Diameter ; Clustering Coefficient . 1 Introduction Random graphs with power - law degree distributions arise naturally in different users such as social systems 1 , Internet topology 2 , citation connections 3 , etc . .These sorts of networks can be formed by using numerous mechanisms referred as network growth models ; look 4 for an overview . One popular method is known preferential attachment or Power - Law Graphs 5 .This family of networks has been studied thoroughly over the previous decade 6 - 8 . In this research , we focus on two essential properties of these graphs : the diameter and the clustering density 9 .The diameter D ( G ) of a graph G = ( V , E ) is given as max { d ( v , v ) | v , v ∈ V } , i . e . , it is the maximum length between any pair of vertices in G . The clustering coefficient C ( v ) of a node v is calculated as the fraction of pairs of neighbors of v that are adjoining to each other 10 . It is easy to confirm that both quantities rely only on the degrees of the nodes in the graph 11 ; thus they do not change under path rewirings 12 .",
        "rewrite_text": "We investigate the diameter and clustering density across various classes of random graphs characterized by power-law degree distributions, particularly focusing on preferential attachment (PA) networks. Our findings indicate that PA networks tend to exhibit small diameters if their growth is sufficiently gradual, a conclusion that remains valid even when allowing for multiple edges between nodes. Conversely, for rapidly expanding PA networks, we can identify instances where the diameter increases exponentially as a function of the number of nodes (n). Alongside our examination of diameter, we also analyze the clustering coefficient of these networks, which measures the interconnectivity among a node's neighbors. We derive upper bounds for the clustering coefficients of PA networks that are applicable regardless of the rate at which new nodes join the network. Additionally, we present numerical results that suggest a strong correlation between our theoretical predictions and simulation outcomes.\n\n**Keywords:** Preferential Attachment Networks; Diameter; Clustering Coefficient.\n\n**1 Introduction**  \nRandom graphs with power-law degree distributions are commonly found in various systems, including social networks, internet topology, and citation networks. These networks can be generated through different mechanisms known as network growth models. A widely studied model is preferential attachment, also referred to as Power-Law Graphs. This category of networks has been extensively analyzed over the past decade. In this study, we concentrate on two fundamental properties of these graphs: diameter and clustering density. The diameter \\( D(G) \\) of a graph \\( G = (V, E) \\) is defined as \\( \\max \\{ d(v_1, v_2) | v_1, v_2 \\in V \\} \\), representing the longest distance between any pair of vertices in \\( G \\). The clustering coefficient \\( C(v) \\) of a node \\( v \\) is computed as the ratio of pairs of \\( v \\)'s neighbors that are connected to one another. Notably, both the diameter and clustering coefficient depend solely on the degrees of the nodes in the graph, meaning they remain unchanged under path rewiring.",
        "ori-fast-z-score": -1.7439550769285392,
        "water-fast-z-score": 6.666666666666667,
        "rewrite-fast-z-score": 0.40689422938557973
    },
    {
        "original_text": "The authors present an experimental method for determining the probability that two photons simultaneously hit pixels in a detector, as well as its spatial resolution. The method is based on measuring correlations between pairs of photons emitted by a source with known angular distribution. It can be used to characterize any type of photon-counting detector (CCD cameras, photomultipliers etc.) without requiring knowledge about their internal structure or electronics. This information may then be used to improve the performance of imaging systems such as telescopes. The results are presented for a silicon-strip detector. They show good agreement with Monte Carlo simulations. DOI: 10.1088/1742-6596/aa5e20\nSpatial resolution and coincidence resolving time measurement of Si strip detectors using single-photon counting technique \nI. INTRODUCTIO N\nIn many applications it is important to know how accurately one can determine the position where a photon hits a detector. For example this information is needed when designing optical instruments like telescopes  1  . In order to measure the spatial resolution of a detector we need to have some reference point against which we compare our measured data  2  .\nOne way to obtain this reference point is to use a light source emitting photons at a well-defined angle relative to the normal direction  3  , see Fig.  1(a) . If the detector has no intrinsic spatial resolution, all detected photons will come from a small area around the center of the detector surface. By scanning the detector over different angles θ, we can find out what fraction of the total number of counts comes from each part of the detector  4  . We call these fractions the response function R(θ) of the detector  5  . Knowing the shape of the response function allows us to calculate the spatial resolution of the detector  6  . However, if there is more than one pixel per unit solid angle, the situation becomes complicated because now several pixels could detect a given photon  7, 8  . To solve this problem we introduce here a new concept -the joint probability P ij that i-th and j-th pixels detect a photon simultaneously  9  . Using this concept together with the response function we",
        "watermark_text": "The authors present an research technique for determining the probability that two photons simultaneously struck pixels in a detector , as well as its spatial resolution . The method is based on measuring correlations between pairs of photons generated by a source with known angular distribution .It can be used to characterize any type of photon - tracking detector ( CCD cameras , photomultipliers etc . ) without using knowledge about their internal structure or electronics .This knowledge might then be used to upgrade the performance of optical applications such as telescopes . The results are presented for a silicon - strip detector .They show good agreement with Monte Carlo simulations . DOI : 10 . 1088 / 1742 - 6596 / aa5e20 Spatial resolution and coincidence resolving time measurement of Si strip detectors using single - photon counting technique I . INTRODUCTIO N In many applications it is important to consider how accurately one can determine the position where a photon hits a detector .For instance this data is required when designing optical equipment like telescopes 1 . In order to measure the spatial resolution of a detector we require to have some reference location against which we compare our measured data 2 .One method to obtain this reference location is to use a light source emitting photons at a better - defined angle relative to the normal direction 3 , see Fig . 1 ( a ) .If the sensor has no intrinsic temporal resolution , all detected photons will coming from a small area around the center of the sensor surface . By scanning the sensor over different angles θ , we can find out what fraction of the total number of counts coming from each portion of the detector 4 .We call these fractions the response function R ( θ ) of the sensor 5 . Understanding the shape of the response function allows us to estimate the spatial resolution of the sensor 6 .However , if there is more than one pixel per unit solid angle , the situation grows difficult because now multiple pixels might detect a given photon 7 , 8 . To solve this situation we approach here a new notion - the joint probability P ij that i - th and j - th pixels detect a photon simultaneously 9 .Using this concept together with the response function we",
        "rewrite_text": "The authors introduce a research technique aimed at assessing the likelihood of two photons simultaneously hitting pixels in a detector, alongside its spatial resolution. This method relies on analyzing correlations between pairs of photons emitted by a source with a known angular distribution. It is applicable to characterize various types of photon-tracking detectors, such as CCD cameras and photomultipliers, without needing detailed knowledge of their internal structures or electronic configurations. The insights gained can be utilized to enhance the performance of optical devices, including telescopes. Results for a silicon-strip detector show strong agreement with Monte Carlo simulations. DOI: 10.1088/1742-6596/aa5e20.\n\nIn many applications, accurately determining the impact position of a photon on a detector is crucial, particularly in the design of optical equipment like telescopes. To measure the spatial resolution of a detector, a reference location for comparison is needed. One approach to establish this reference is using a light source that emits photons at a precisely defined angle relative to the normal direction. If the sensor lacks intrinsic temporal resolution, all detected photons will originate from a small area near the center of the sensor surface. By scanning the sensor at various angles θ, we can determine the fraction of total counts generated by each section of the detector. These fractions are referred to as the response function R(θ) of the sensor. By understanding the shape of this response function, we can estimate the sensor's spatial resolution. However, when multiple pixels are present within a unit solid angle, the complexity increases, as more than one pixel may register a given photon. To address this complexity, we introduce a new concept: the joint probability P_ij that the i-th and j-th pixels detect a photon simultaneously. By leveraging this concept alongside the response function, we...",
        "ori-fast-z-score": 2.2998495985826177,
        "water-fast-z-score": 8.380035799446205,
        "rewrite-fast-z-score": 3.3567254331867566
    },
    {
        "original_text": "The geometric frustration in the spin-1/2 triangular lattice is studied by means of neutron powder diffraction, magnetization measurements, specific heat data, and first-principles calculations for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both compounds are antiferromagnetic insulators with Néel temperatures TN = 5 K (Sr3NiRhO6) and T N = 7 K (Sr3NiPtO6). In addition to the expected collinear antiferromagnetism, we find evidence for noncollinear ordering in Sr3NiRhO6: First, there is an additional weak reflection at Q = 1.5 Å -1 , which can be explained as superlattice peak due to a small rhombohedral distortion; secondly, the temperature dependence of the ordered moment shows a kink around 2 K indicating a change of the order parameter below this temperature.",
        "watermark_text": "The geometric problems in the spin - 1 / 2 triangular lattice is studied by means of neutron dust diffraction , magnetization calculations , detailed heat observations , and first - principles measurements for two new compounds Sr3NiRhO 6 and Sr 3 Ni Pt O 6 . The results show that both salts are antiferromagnetic insulators with Néel pressures TN = 5 K ( Sr3NiRhO6 ) and T N = 7 K ( Sr3NiPtO6 ) .In addition to the expected collinear antiferromagnetism , we find proof for noncollinear ordering in Sr3NiRhO6 : First , there is an additional strong absorption at Q = 1 . 5 Å - 1 , which can be understood as superlattice peak thanks to a small rhombohedral distortion ; secondly , the temperature dependence of the ordered moment displays a kink around 2 K indicating a change of the order parameter below this heat .",
        "rewrite_text": "The geometric properties of the spin-1/2 triangular lattice have been investigated using neutron diffraction, magnetization calculations, detailed thermal measurements, and first-principles analyses on two new compounds: Sr3NiRhO6 and Sr3NiPtO6. The findings reveal that both compounds are antiferromagnetic insulators, exhibiting Néel temperatures of TN = 5 K for Sr3NiRhO6 and TN = 7 K for Sr3NiPtO6. Besides the anticipated collinear antiferromagnetism, evidence for noncollinear ordering has been observed in Sr3NiRhO6. Specifically, there is a significant absorption peak at Q = 1.5 Å⁻¹, interpreted as a superlattice peak resulting from a slight rhombohedral distortion. Additionally, the temperature dependence of the ordered moment shows a notable change around 2 K, suggesting a shift in the order parameter below that temperature.",
        "ori-fast-z-score": 0.5773502691896258,
        "water-fast-z-score": 5.10527470242631,
        "rewrite-fast-z-score": 0.4375949744936837
    },
    {
        "original_text": "We study the relationship between network structure, dynamics, and function using an example system that is both complex and experimentally accessible -- the brain s neuronal circuitry. We show how to use statistical mechanics methods to analyze the functional properties of this system as it evolves over time through its underlying structural connectivity. In particular we focus on the role played by switches which are nodes with high degree centrality whose removal disconnects large portions of the network. Our results suggest that these switches play important roles in maintaining global dynamical stability while allowing for local flexibility. The manuscript is organized into four sections. Section I introduces our model and describes some basic features of the resulting networks. Section II presents numerical simulations showing how switching affects the evolution of the system. Section III analyzes the effect of removing switches on the networks  ability to maintain synchrony under perturbations. Finally section IV discusses possible extensions of our work. This article has been published in final form at http://arxiv.org/abs/1306.5995",
        "watermark_text": "We research the relationship between network structure , dynamics , and function using an instance structure that is both complex and experimentally visible - - the brain s neuronal circuitry . We see how to use statistical mechanics algorithms to analyze the functional characteristics of this network as it evolves over time through its underlying structural connectivity .In particular we focus on the part played by switches which are networks with high degree centrality whose removal disconnects huge segments of the organization . Our results show that these switches act key roles in maintaining global dynamical stability while providing for central autonomy .The manuscript is organized into four components . Section I outlines our model and explains some fundamental characteristics of the resulting connections .Section II offers numerical simulations demonstrating how switching influence the evolution of the organization . Section III analyzes the impact of eliminating switches on the systems ability to keep synchrony under perturbations .Finally section IV examines possible extensions of our work . This section has been publication in finished form at http : / / arxiv . org / abs / 1306 . 5995",
        "rewrite_text": "We investigate the interplay between network structure, dynamics, and function by examining a complex and experimentally observable instance: the neuronal circuitry of the brain. Our approach employs statistical mechanics algorithms to assess the functional characteristics of this network as it evolves over time, driven by its underlying structural connectivity. In particular, we concentrate on the role of switches—high-degree centrality networks whose removal can sever significant portions of the overall architecture. Our findings indicate that these switches are crucial for preserving global dynamical stability while allowing for local autonomy. The paper is divided into four sections. Section I presents our model and discusses key properties of the resulting connections. Section II provides numerical simulations that illustrate how switches influence the network's evolution. Section III evaluates the effects of switch removal on the system's capacity to maintain synchrony amidst disturbances. Finally, Section IV explores potential extensions of our research. This section has been published in its entirety at http://arxiv.org/abs/1306.5995.",
        "ori-fast-z-score": -0.9761870601839528,
        "water-fast-z-score": 7.637626158259733,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "We report on near-infrared (NIR) photometry, optical spectroscopy, and X-ray observations of the enigmatic radio source G70.7+1. \n \n The NIR data show that this object is an extremely reddened star with A V = 25 mag located at a distance of about 5 kpc in front of the supernova remnant W28. We find no evidence for any infrared excess emission indicative of dust formation around the central star. Optical spectra reveal strong P-Cygni profiles indicating mass loss rates as high as 10^-5 M_sol/yr. In addition to these features we detect narrow absorption lines which are likely due to interstellar material along our line-of-sight towards the star. Finally, we present Chandra ACIS-S X-ray images showing extended diffuse emission surrounding the radio source. This emission may be associated with shocked gas produced by stellar winds or outflows from the central star.",
        "watermark_text": "We report on near - infrared ( NIR ) photometry , optical spectroscopy , and X - ray observations of the enigmatic radio source G70 . 7 + 1 . The NIR data reveal that this body is an incredibly reddened star with A V = 25 mag found at a distance of about 5 kpc in back of the supernova remnant W28 .We see no evidence for any infrared excess emission indicative of dust development around the main star . Optical spectra indicate strong P - Cygni profiles suggesting mass loss rates as high as 10 ^ - 5 M _ sol / yr .In addition to these characteristics we perceive close absorption patterns which are likely due to interstellar material along our line - of - view towards the star . Finally , we present Chandra ACIS - S X - ray pictures showing extended diffuse emission surrounding the radio source .This emission may be correlated with shocked energy produced by stellar winds or outflows from the primary star .",
        "rewrite_text": "We present observations of the intriguing radio source G70.7 + 1, including near-infrared (NIR) photometry, optical spectroscopy, and X-ray data. The NIR analysis indicates that this object is an exceptionally reddened star with an extinction of A_V = 25 mag, located approximately 5 kpc behind the supernova remnant W28. No indications of infrared excess emission, which would suggest dust formation around the star, were found. The optical spectra display prominent P-Cygni profiles, implying mass loss rates that could be as high as 10^(-5) M_sol/yr. Additionally, we observe distinct absorption features likely caused by interstellar material along our line of sight to the star. Moreover, we present Chandra ACIS-S X-ray images that show extended diffuse emission around the radio source, which may be linked to shocked energy created by stellar winds or outflows from the central star.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "We study the ground state properties of the spin-1/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, using exact diagonalization (ED) on finite clusters up to 12x12 sites. We find that for low concentrations of vacancies there is no significant change in the magnetic order parameter or the energy gap between singlet and triplet excitations as compared to the pure system. However, we observe an increase in the density of states at zero energy when increasing the concentration of vacancies. This effect can be explained by considering the formation of bound pairs of vacancies which are localized around each other due to their mutual interaction. The binding energies of these pairs depend strongly on the distance between them but only weakly on the size of the cluster considered. In addition, we show how this behavior changes if one considers next-nearest neighbor interactions instead of nearest-neighbor ones. Finally, we discuss possible experimental realizations of our results.",
        "watermark_text": "We research the ground state properties of the spin - 1 / 2 square lattice with nearest - neighbor antiferromagnetic interactions and vacancies , using accurate diagonalization ( ED ) on polynomial clusters up to 12x12 places . We see that for low levels of vacancies there is no major shift in the magnetic order parameter or the power gap between singlet and triplet excitations as compared to the pure system .However , we perceive an increase in the density of states at zero energy when increasing the density of vacancies . This phenomenon can be described by using the formation of bound pairs of vacancies which are localized around each other owing to their mutual interaction .The activation energies of these pairs depend greatly on the distance between them but only weakly on the length of the cluster considered . In addition , we show how this behavior changes if one considers next - nearest neighbor interactions instead of nearest - neighbor ones .Finally , we explain possible experimental realizations of our findings .",
        "rewrite_text": "We investigate the ground state properties of a spin-1/2 square lattice with nearest-neighbor antiferromagnetic interactions and vacancies, utilizing precise diagonalization (ED) on polynomial clusters of up to 12x12 sites. Our findings indicate that at low vacancy levels, there is no significant alteration in the magnetic order parameter or the energy gap between singlet and triplet excitations when compared to the pristine system. However, we do observe an increase in the density of states at zero energy as the vacancy density rises. This effect can be explained by the formation of bound pairs of vacancies that localize around one another due to their mutual interactions. The activation energies of these pairs are highly dependent on their separation, while being only weakly influenced by the size of the cluster analyzed. Furthermore, we discuss how this behavior shifts when next-nearest neighbor interactions are taken into account instead of just nearest-neighbor interactions. Lastly, we propose potential experimental implementations of our results.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 5.7350162126103985,
        "rewrite-fast-z-score": 0.11704114719613057
    },
    {
        "original_text": "The development of large space telescopes requires the use of lightweight materials to reduce launch costs and increase telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate material due to its high strength, low density, and radiation hardness at cryogenic temperatures. However, it has been shown that SiC exhibits significant changes in thermal expansion with temperature which can lead to dimensional instability during cool-downs or warm-ups. This work presents results on the measurement of the coefficient of thermal expansion (CTE) of SiC-100 over a wide range of temperatures using a novel technique based on laser interferometry. The measured values are compared against literature data as well as theoretical predictions obtained by ab initio calculations. It was found that the experimental measurements agree very well with theory within the uncertainty limits. These results will be used to improve the design of future space missions such as JWST and WFIRST-AFTA.",
        "watermark_text": "The construction of large space telescopes demands the using of lightweight materials to reduce rocket expenses and increase telescope performance in orbit . Silicon Carbide ( SiC ) is an excellent candidate material owing to its high strength , low density , and radiation hardness at cryogenic temperatures .However , it has been shown that SiC exhibits significant variations in heat expansion with temperature which can lead to dimensional instability during cool - downs or warm - ups . This research provides findings on the observation of the coefficient of thermal expansion ( CTE ) of SiC - 100 over a broad variety of conditions utilizing a innovative method based on laser interferometry .The measured measures are compared against literature statistics as well as conceptual predictions acquired by ab initio estimates . It was shown that the empirical observations coincide very best with theoretical within the uncertainty constraints .These data will be used to improve the development of potential space missions such as JWST and WFIRST - AFTA .",
        "rewrite_text": "The construction of large space telescopes requires the use of lightweight materials to lower rocket costs and enhance telescope performance in orbit. Silicon Carbide (SiC) is an excellent candidate due to its high strength, low density, and resistance to radiation at cryogenic temperatures. However, SiC has demonstrated significant variations in thermal expansion with temperature, which can result in dimensional instability during cooling and heating phases. This research presents findings on the coefficient of thermal expansion (CTE) of SiC-100 across a wide range of conditions, utilizing an innovative laser interferometry method. The measurements obtained are compared with data from existing literature and theoretical predictions derived from ab initio calculations. It was found that the empirical results align closely with theoretical expectations within the margins of uncertainty. This data will contribute to the advancement of upcoming space missions, including JWST and WFIRST-AFTA.",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We study pair production of doubly-charged scalars in association with two jets, which can be induced by both weak gauge bosons (W or Z) and photons via loops involving heavy fermions such as top quarks. We derive constraints on the masses of these particles using current experimental data for W+jets and Z+jets processes collected by ATLAS and CMS experiments at the Large Hadron Collider (LHC). In addition to the standard model backgrounds, we also consider contributions from other new physics models that may have similar signatures. The results are presented in terms of exclusion limits on the mass parameters of various new physics scenarios. Finally, we discuss possible signals of this process at future runs of the LHC. PACS numbers: 12.60.Jv, 13 .85.Rm, 14.80.Ly \nI. INTRODUCTIO N\nThe discovery of neutrinos has opened up an exciting possibility of probing beyond Standard Model (SM), especially its Majorana nature  1  , through their lepton number violating interactions  2  . One interesting scenario is the seesaw mechanism  3  where SM singlet right-handed neutrinos acquire large Majorana masses after electroweak symmetry breaking  4  .\nIn order to test whether the observed light neutrinos are indeed Majorana particles, one needs to look for lepton-number-violating processes mediated by virtual heavy neutrinos  5  . These include neutrinoless double beta decay  6  , tritium beta decay  7  , and charged-current quasielastic scattering  8  . However, it turns out that all these processes suffer from severe astrophysical and/or nuclear matrix element uncertainties  9  . On the other hand, colliders provide clean environments to probe lepton number violation directly  10  . For example, searches for same-sign dileptons  11  and trileptons  12  at hadronic colliders could lead to important information about Majorana neutrinos  13  . Another promising channel is the production of doubly-charge scalar particles  14  , which can occur either through s-channel exchange of neutral gauge bosons  15  or t-channel exchange of heavy ferm",
        "watermark_text": "We research pair production of doubly - charged scalars in association with two jets , which can be induced by both weak gauge bosons ( W or Z ) and photons via loops involving heavy fermions such as top quarks . We derive restrictions on the masses of these ions using current experimental evidence for W + jets and Z + jets interactions collected by ATLAS and CMS observations at the Large Hadron Collider ( LHC ) .In addition to the standard description backgrounds , we also consider contributions from other recent physics systems that might have related signatures . The results are presented in terms of exclusion limits on the mass parameters of several novel physics scenarios .Finally , we explain possible transmissions of this process at next ran of the LHC . PACS scores : 12 . 60 . Jv , 13 . 85 . Rm , 14 . 80 . Ly I . INTRODUCTIO N The observation of neutrinos has opened up an exciting possibility of probing beyond Standard Model ( SM ) , particularly its Majorana nature 1 , through their lepton size violating interactions 2 .One interesting scenario is the seesaw mechanism 3 where SM singlet right - handed neutrinos gain big Majorana masses after electroweak symmetry breaking 4 . In order to test whether the seen light neutrinos are indeed Majorana objects , one needs to search for lepton - number - violating reactions mediated by virtual heavy neutrinos 5 .These include neutrinoless double alpha emission 6 , tritium alpha emission 7 , and charged - current quasielastic emission 8 . However , it turns out that all these mechanisms suffer from severe astrophysical and / or radioactive matrix element uncertainties 9 .On the other hand , colliders provide clean environments to probe lepton total violation directly 10 . For instance , searches for same - sign dileptons 11 and trileptons 12 at hadronic colliders may yield to key information about Majorana neutrinos 13 .Another promising channel is the production of doubly - charge scalar particles 14 , which can occur either through s - channel exchange of neutral gauge bosons 15 or t - channel exchange of heavy ferm",
        "rewrite_text": "We investigate the pair production of doubly-charged scalars in conjunction with two jets, a process that can be triggered by weak gauge bosons (W or Z) as well as photons, through loops involving heavy fermions like top quarks. By leveraging current experimental data from W + jets and Z + jets interactions gathered by the ATLAS and CMS collaborations at the Large Hadron Collider (LHC), we establish constraints on the masses of these ions. Alongside the standard model backgrounds, we also take into account contributions from new physics scenarios that may exhibit similar signatures. Our findings are presented as exclusion limits on the mass parameters across various novel physics models. Additionally, we discuss potential implications and further investigations of this process in the next run of the LHC. \n\nPACS numbers: 12.60.Jv, 13.85.Rm, 14.80.Ly \n\nI. INTRODUCTION \n\nThe discovery of neutrinos has sparked exciting opportunities to explore phenomena beyond the Standard Model (SM), particularly their potential Majorana characteristics, via lepton number-violating interactions. Among the intriguing possibilities is the seesaw mechanism, in which right-handed neutrinos, which are SM singlets, acquire substantial Majorana masses following electroweak symmetry breaking. To ascertain whether the observed light neutrinos are indeed Majorana particles, it is essential to search for lepton number-violating reactions mediated by virtual heavy neutrinos. Such reactions may include neutrinoless double alpha emission, tritium alpha emission, and charged-current quasielastic emission. However, these mechanisms are often hindered by significant astrophysical and/or radioactive matrix element uncertainties. In contrast, colliders offer a clear environment to directly investigate lepton number violation. For example, searches for same-sign dileptons and trileptons at hadronic colliders could provide crucial insights into Majorana neutrinos. Another promising avenue is the production of doubly-charged scalar particles, which can arise through s-channel exchange of neutral gauge bosons or t-channel exchange of heavy fermions.",
        "ori-fast-z-score": 1.1188618555710317,
        "water-fast-z-score": 7.315635209502899,
        "rewrite-fast-z-score": 1.3522468075656264
    },
    {
        "original_text": "We present an approach to the description of macroscopic configurations in terms of quantum states, which is based on the concept of entanglement entropy and its generalization for mixed states. We show that this representation allows one to obtain exact results for some statistical properties of systems with many degrees of freedom. In particular, we consider the problem of calculating the partition function of classical spin models at high temperatures. The proposed method can be used as a basis for constructing approximate algorithms for solving problems related to the calculation of thermodynamic characteristics of complex systems. Introduction.\nThe main goal of statistical mechanics is to describe the behavior of macroscopic objects (for example, gases) by using microscopic information about their constituents (atoms). This task becomes especially difficult when dealing with large systems consisting of many particles or spins. For such cases, it is necessary to use approximations, since direct calculations are impossible due to the exponential growth of the number of possible microstates with increasing system size N . One of these approaches is the so-called mean-field approximation  1  , according to which each particle interacts only with all other particles simultaneously; i.e., the interaction between different pairs of particles is neglected. However, even within this simplified model, the calculation of the partition function Z = Tr exp(−βH) (1) remains extremely complicated  2  .\nIn recent years, there has been growing interest in developing new methods for describing macroscopic configurations in terms similar to those used in quantum physics  3  -  8  . These studies were inspired by the fact that both classical and quantum descriptions have certain common features  9  : they are formulated in terms of wave functions ψ(x), where x denotes either positions of particles or spins, respectively. Moreover, the evolution of these wave functions obeys the same Schrödinger equation ih∂ t |ψ(t) = H|ψ(t) , where H is the corresponding Hamiltonian operator. It should also be noted that the density matrix ρ = |ψ(t) ψ(t)| plays the role of a probability distribution in both theories  10  .",
        "watermark_text": "We present an view to the description of macroscopic configurations in terms of quantum states , which is based on the notion of entanglement entropy and its generalization for mixed states . We see that this representation enables one to obtain exact findings for some statistical characteristics of structures with many degrees of liberty .In particular , we investigate the question of calculating the splitting function of classical spin configurations at high heat . The proposed method can be used as a framework for constructing numerical algorithms for solving cases linked to the determination of thermodynamic qualities of complex systems .Introduction . The main goal of statistical mechanics is to explain the dynamics of macroscopic objects ( for example , atoms ) by using microscopic information about their components ( atoms ) .This job becomes especially difficult when dealing with large systems composed of several particles or spins . For such cases , it is important to use approximations , since direct calculations are unable due to the exponential growth of the number of possible microstates with increasing system width N .One of these method is the so - called mean - field approximation 1 , according to which each particle interacts only with all other particles concurrently ; i . e . , the interaction between multiple sets of particles is neglected . However , even within this simplified model , the determination of the partition function Z = Tr exp ( −βH ) ( 1 ) stays extremely difficult 2 .In recent years , there has been growing interest in advanced new concepts for describing macroscopic configurations in terms comparable to those utilized in quantum science 3 - 8 . These studies were inspired by the fact that both classical and quantum descriptions have many common features 9 : they are formulated in terms of wave vectors ψ ( z ) , where h indicates either positions of atoms or spins , respectively .Moreover , the evolution of these wave functions obeys the same Schrödinger equation ih∂ t | ψ ( t ) = H | ψ ( t ) , where H is the analogous Hamiltonian operator . It should additionally be mentioned that the density function ρ = | ψ ( t ) ψ ( t ) | serves the importance of a probability distribution in both models 10 .",
        "rewrite_text": "We provide a perspective on describing macroscopic configurations through quantum states, focusing on the concept of entanglement entropy and its extension to mixed states. This representation facilitates the derivation of precise results for certain statistical properties of systems with numerous degrees of freedom. Specifically, we explore the computation of the splitting function for classical spin configurations at elevated temperatures. Our proposed approach can serve as a framework for developing numerical algorithms aimed at assessing the thermodynamic properties of complex systems.\n\n**Introduction**: The primary objective of statistical mechanics is to elucidate the dynamics of macroscopic entities (such as atoms) by leveraging microscopic details about their constituents. This task becomes especially challenging when addressing large systems composed of multiple particles or spins. In such scenarios, approximations become crucial, as direct calculations are impractical due to the exponential increase in the number of possible microstates as the system size, N, expands. One common method utilized is the mean-field approximation, where each particle is considered to interact with all others simultaneously, ignoring interactions among groups of particles. However, even under this simplified framework, determining the partition function \\( Z = \\text{Tr} \\, e^{-\\beta H} \\) remains exceedingly difficult.\n\nRecently, there has been an upsurge in interest in innovative concepts for representing macroscopic configurations using frameworks akin to those in quantum science. These investigations stem from the observation that classical and quantum descriptions share many similarities: they are expressed through wave functions \\( \\psi(z) \\), where \\( z \\) represents the positions of atoms or spins, respectively. Furthermore, the evolution of these wave functions follows the same Schrödinger equation \\( i \\hbar \\partial_t | \\psi(t) \\rangle = H | \\psi(t) \\rangle \\), in which \\( H \\) denotes the corresponding Hamiltonian operator. It is also important to note that the density function \\( \\rho = | \\psi(t) |^2 \\) serves a similar role as a probability distribution in both frameworks.",
        "ori-fast-z-score": 0.23643312187173018,
        "water-fast-z-score": 8.799551054765924,
        "rewrite-fast-z-score": 0.1643989873053573
    },
    {
        "original_text": "In this work, we study the diversity-multiplexing tradeoff (DMT) for large multiple-input and multiple-output systems with independent Rayleigh fading channels. We first derive an exact expression for DMT by using the asymptotic analysis technique proposed in  1  . Then, based on our derived results, we propose two low-complexity suboptimal schemes to achieve near-optimum performance at low SNR region. Finally, simulation results are provided to verify the effectiveness of these proposed schemes. The main contributions of this paper can be summarized as follows:  1) An exact expression is obtained for the DMT of large MIMO systems; 2) Two low-complexity suboptimum schemes are proposed; 3) Simulation results show that both proposed schemes have good performances compared with existing ones. In recent years, there has been growing interest in studying large-scale antenna arrays due to their potential advantages over conventional small-scale antenna arrays  2  -  4  , such as higher data rates, lower transmit power consumption, reduced inter-cell interference, etc.. However, it should also be noted that increasing the number of antennas will lead to increased hardware cost and energy consumption  5  .\nTo fully exploit the benefits brought about by massive MIMO technology while keeping its disadvantages under control, many researchers have studied how to optimize the design parameters  6 -  8  or develop new transmission techniques  9  -  11  . Among them, one important issue is to investigate the fundamental limits of largescale antenna array systems  12  -  14  . For example, Zheng et al.  15  investigated the ergodic capacity scaling law of multi-cell multiuser massive MIMO networks. Liu et al.  16  analyzed the outage probability of downlink massive MIMO systems. Wang et al.  17  studied the achievable rate of uplink massive MIMO systems with imperfect channel state information (CSI). Moreover, some other works  18  -  20  focused on analyzing the error performance of large-scale antenna array systems.",
        "watermark_text": "In this research , we study the diversity - multiplexing tradeoff ( DMT ) for large single - input and multiple - output devices with independent Rayleigh fading sources . We first derive an precise representation for DMT by using the asymptotic analysis technique proposed in 1 .Then , using on our derived results , we propose two low - complexity suboptimal techniques to achieve near - optimum performance at low SNR region . Finally , simulation data are provided to confirm the performance of these suggested schemes .The main contributions of this paper can be summarized as follows : 1 ) An exact representation is found for the DMT of large MIMO systems ; 2 ) Two low - complexity suboptimum systems are proposed ; 3 ) Simulation data demonstrate that both proposed methods have better performances relative with existing ones . In recent years , there has been growing interest in investigating big - scale array arrays due to their potential advantages over traditional small - scale array arrays 2 - 4 , such as higher data costs , greater receive power consumption , reduced inter - cell interference , etc . .However , it should also be mentioned that expanding the number of antennas will result to higher hardware expense and energy consumption 5 . To fully utilize the advantages brought about by massive MIMO technology while maintaining its disadvantages under control , various scientists have researched how to optimize the design characteristics 6 - 8 or develop new transmission strategies 9 - 11 .Among them , one key issue is to examine the fundamental limits of largescale antenna array systems 12 - 14 . For instance , Zheng et al .15 explored the ergodic capacity scaling principle of dual - cell multiuser large MIMO connections . Liu et al .16 analyzed the outage likelihood of downlink large MIMO systems . Wang et al .17 studied the achievable rate of uplink large MIMO systems with imperfect channel state information ( CSI ) . Moreover , some other works 18 - 20 centered on examining the error behaviour of large - scale array antenna networks .",
        "rewrite_text": "In this study, we examine the diversity-multiplexing tradeoff (DMT) for large single-input multiple-output (SIMO) systems utilizing independent Rayleigh fading channels. We begin by deriving an accurate representation of the DMT through the asymptotic analysis technique introduced in previous research. Building on our findings, we then propose two low-complexity suboptimal strategies that deliver near-optimal performance in low signal-to-noise ratio (SNR) conditions. We provide simulation results to validate the effectiveness of these proposed approaches. The primary contributions of this paper are as follows: 1) an exact representation of the DMT for large MIMO systems is established; 2) two low-complexity suboptimal systems are introduced; and 3) simulation results indicate that our proposed methods outperform existing solutions. Recently, there has been increased interest in large-scale antenna arrays due to their potential benefits over conventional small-scale arrays, including higher data rates, improved reception power, and reduced inter-cell interference. However, it is important to note that increasing the number of antennas can lead to higher hardware costs and energy consumption. To maximize the advantages of massive MIMO technology while mitigating its drawbacks, researchers have been exploring ways to optimize design parameters or develop new transmission techniques. A critical aspect of this research involves evaluating the fundamental limits of large-scale antenna array systems. For instance, Zheng et al. examined the ergodic capacity scaling for dual-cell multiuser large MIMO setups, Liu et al. assessed outage probabilities in downlink large MIMO systems, and Wang et al. investigated the achievable rates in uplink large MIMO systems with imperfect channel state information (CSI). Additionally, several studies have focused on analyzing the error performance of large-scale antenna networks.",
        "ori-fast-z-score": -0.32025630761017426,
        "water-fast-z-score": 9.01838179448413,
        "rewrite-fast-z-score": 2.482817665807104
    },
    {
        "original_text": "We introduce the concept of neutral networks, which are graphs that represent relationships between sequences and shapes in an unsupervised manner. We show how these can be used for shape recognition by training them on synthetic data generated using computer graphics techniques. The resulting network is able to recognize shapes with high accuracy even when they have been transformed or distorted. Finally we demonstrate our method on real world images showing its ability to generalize well beyond the training set. In this work we present a novel approach to learning about shapes through their relationship to sequences. Our main contribution is introducing the concept of neutral networks as a way to learn such relationships without supervision. Neutral networks are graphs whose nodes correspond to sequences (e.g., strings) and edges connect similar sequences. They provide a natural representation for capturing relationships between different types of information. For example, one could use a neutral network to capture relationships between sentences and documents, or between music notes and melodies. Here we focus on applying neutral networks to the problem of recognizing shapes based solely on their sequence of points sampled along their boundary. This task has many applications including medical image analysis where it may not always be possible to obtain ground truth labels due to privacy concerns.",
        "watermark_text": "We introduce the idea of neutral networks , which are graphs that represent connections between motifs and shapes in an unsupervised manner . We see how these can be used for shape detection by testing them on synthetic information generated using computer graphics techniques .The resulting system is ability to identify images with high clarity even when they have been transformed or distorted . Finally we prove our technique on real life images showing its capacity to generalize far beyond the training set .In this project we present a unique technique to knowledge about forms through their connection to sequences . Our main achievement is pioneering the idea of neutral networks as a way to teach such interactions without supervision .Neutral networks are graphs whose nodes relate to strings ( e . g . , sequences ) and edges connect identical sequences . They offer a natural representation for capturing relationships between various types of content .For instance , one might using a neutral network to capture relationships between sentences and documents , or between music notes and melodies . Here we focus on implementing neutral networks to the question of recognizing shapes based primarily on their sequence of points sampled along their boundary .This problem has numerous uses including medical visual examination where it would not always be possible to obtain ground truth marks owing to privacy issues .",
        "rewrite_text": "We present the concept of neutral networks, which are graphs that illustrate connections between motifs and shapes in an unsupervised fashion. We demonstrate their application in shape detection by evaluating them on synthetic data created using computer graphics techniques. The resulting system can accurately identify images with high clarity, even when they have undergone transformations or distortions. Ultimately, we validate our method on real-world images, showcasing its ability to generalize well beyond the initial training set. In this project, we introduce an innovative approach to understanding forms through their connections to sequences. Our key achievement is the introduction of neutral networks as a means to learn these interactions without supervision. Neutral networks consist of nodes representing strings (e.g., sequences) and edges linking identical sequences, providing a natural framework for capturing relationships among different types of content. For example, a neutral network could be employed to illustrate relationships between sentences and documents or between music notes and melodies. In this work, we specifically focus on utilizing neutral networks to address the challenge of recognizing shapes based on the sequence of points sampled along their boundaries. This problem has wide-ranging applications, including medical visual examinations, where obtaining ground truth markings can be difficult due to privacy concerns.",
        "ori-fast-z-score": -0.8980265101338746,
        "water-fast-z-score": 8.148217143826667,
        "rewrite-fast-z-score": 1.9917864129354077
    },
    {
        "original_text": "We study the generalized Dicke model with an arbitrary number N of two-level atoms interacting with one-mode radiation field, and show that it can be mapped to a spin-1/2 system by using the Holstein-Primakoff transformation. We then use the exact diagonalization method to calculate its ground state energy spectrum for different values of the coupling constant g and the number N . The results are compared with those obtained by other methods such as perturbation theory and numerical integration. It is found that our results agree well with previous ones when the coupling strength is small but deviate significantly from them if the coupling becomes strong. Finally we discuss some possible applications of this work. PACS: 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv \nI. INTRODUCTIO N\nThe Dicke model  1  describes how many identical two-level atoms interact collectively with a single mode of electromagnetic field. In recent years there has been renewed interest in studying this model because of its potential application in quantum information processing  2  , quantum optics  3  , condensed matter physics  4  , etc.. For example, the collective spontaneous emission rate of the atomic ensemble depends on the total angular momentum J = N /2 (N being the number of atoms)  5  .\nIn fact, the Dicke model was originally proposed more than half century ago  6  . Since then various theoretical approaches have been developed to solve it  7 -10  . Among these approaches, the most successful one is probably the so-called HolsteinPrimakoff transformation  11  which maps the original problem into a spin-1/2 system  12  . This approach works very well at weak-coupling regime where the interaction between atom-field is relatively small. However, it fails completely at large-coupling limit since the mapping procedure breaks down due to the appearance of unphysical states  13  . Recently, several authors  14 -19  have tried to overcome this difficulty by introducing new transformations or approximations. Nevertheless, their solutions still suffer from certain drawbacks  20, 21  .",
        "watermark_text": "We explore the generalized Dicke model with an arbitrary number N of two - level atoms interacting with one - mode radiation field , and find that it can be mapped to a spin - 1 / 2 system by using the Holstein - Primakoff transformation . We then use the exact diagonalization technique to estimate its ground state energy spectrum for different values of the interaction factor g and the number N .The results are compared with those achieved by other methods such as perturbation theory and numerical integration . It is found that our findings agree well with previous ones when the interaction strength is tiny but deviate drastically from them if the interaction becomes strong .Finally we explain some possible use of this study . PACS : 03 . 65 . Ud , 05 . 45 . Mt , 11 . 10 . Gh , 12 . 20 . Dc , 13 . 25 . Gv I . INTRODUCTIO N The Dicke model 1 explains how many identical two - level atoms behave collectively with a single mode of electromagnetic field .In recent years there has been continued interest in investigating this model because of its potential application in quantum information processing 2 , quantum optics 3 , condensed matter science 4 , etc . . For instance , the collective spontaneous emission speed of the atomic ensemble depends on the total angular velocity J = N / 2 ( N being the number of atoms ) 5 . In reality , the Dicke concept was originally proposed more than quarter century ago 6 .Since then various theoretical methods have been constructed to solve it 7 - 10 . Among these method , the most popular one is probably the so - called HolsteinPrimakoff transformation 11 which maps the original problem into a spin - 1 / 2 system 12 .This method works very best at weak - interaction regime where the interaction between electron - field is fairly little . However , it fails totally at large - interaction range since the mapping method splits down due to the appearance of unphysical states 13 .Recently , various literature 14 - 19 have tried to overcome this challenge by using new transformations or approximations . Nevertheless , their solutions still suffer from certain drawbacks 20 , 21 .",
        "rewrite_text": "We investigate the generalized Dicke model involving an arbitrary number \\( N \\) of two-level atoms that interact with a single mode of radiation. Using the Holstein-Primakoff transformation, we demonstrate that this model can be represented as a spin-\\( \\frac{1}{2} \\) system. We then apply exact diagonalization techniques to evaluate the ground state energy spectrum for various interaction strengths \\( g \\) and atom counts \\( N \\). Our results are compared with those obtained through other approaches, including perturbation theory and numerical integration. The comparison reveals that our findings are consistent with existing results when the interaction is weak, but they diverge significantly in the strong interaction regime. Finally, we discuss potential applications of this research. \n\n**PACS:** 03.65.Ud, 05.45.Mt, 11.10.Gh, 12.20.Dc, 13.25.Gv  \n\n**I. INTRODUCTION**  \nThe Dicke model elucidates the collective behavior of numerous identical two-level atoms coupled to a single mode of the electromagnetic field. Recent years have seen sustained interest in this model due to its relevance in fields like quantum information processing, quantum optics, and condensed matter science. For example, the rate of collective spontaneous emission in the atomic ensemble is influenced by the total angular momentum \\( J = \\frac{N}{2} \\) (where \\( N \\) is the number of atoms). Originally proposed more than twenty-five years ago, the Dicke model has since been addressed using various theoretical approaches. One of the most popular methods is the Holstein-Primakoff transformation, which simplifies the original problem into that of a spin-\\( \\frac{1}{2} \\) system. This transformation is particularly effective in the weak interaction regime, where the coupling between the electron and the field is minimal. However, it fails in the strong interaction regime due to the emergence of unphysical states. Recently, new methods and approximations have been explored to tackle this limitation, though these solutions still exhibit certain drawbacks.",
        "ori-fast-z-score": -0.16222142113076254,
        "water-fast-z-score": 6.8132996874920275,
        "rewrite-fast-z-score": 0.3380617018914066
    },
    {
        "original_text": "We present the results of long-term numerical simulations of binary black hole (BBH) evolution, including gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption. We focus on binaries with total mass M = 100-1000M⊙ that evolve through collisional nuclear environments at high redshifts z > 10. Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds. In particular we investigate whether these systems can reach masses above 1000M⊙ before merging within a Hubble time. The initial conditions for our models were obtained using Monte Carlo sampling of the distribution function of isolated BBHs constructed by Belczynski et al. (2010) . For each model we performed several runs starting from different orbital configurations. All calculations were carried out assuming circular orbits. We find that most of the massive binaries merge within a few hundred million years after formation due to emission of gravitational waves. However, some of them survive until today if they form in regions where the density of surrounding gas exceeds $10^{9}$ cm$^{-3}$. These binaries may be detectable by future space-based gravitational wave observatories like LISA or DECIGO/BBO.",
        "watermark_text": "We present the conclusion of long - term numerical simulations of binary dark hole ( BBH ) development , notably gravitational radiation reaction and general relativistic effects such as frame dragging and tidal disruption . We focus on binaries with total mass M = 100 - [UNK] that develop through collisional nuclear habitats at high redshifts z > 10 .Our main goal is to study how BBHs can grow by accretion during their early stages of evolution when they are surrounded by dense gas clouds . In particular we investigate whether these systems can reach masses above [UNK] before merging within a Hubble time .The initial conditions for our models were obtained using Monte Carlo analysis of the distribution function of isolated BBHs generated by Belczynski et al . ( 2010 ) .For each model we performed numerous runs beginning from varying orbital locations . All calculations were carried out assuming circular orbits .We see that most of the huge binaries dissolve within a few hundred million months after formed due to emission of gravitational waves . However , some of them remain until today if they exist in areas where the density of neighbouring gas approaches $ 10 ^ { 9 } $ cm $ ^ { - 3 } $ .These binaries may be detectable by future space - based gravity wave observatories like LISA or DECIGO / BBO .",
        "rewrite_text": "We present the findings from extensive numerical simulations of binary black hole (BBH) evolution, specifically examining the effects of gravitational radiation and general relativistic phenomena such as frame dragging and tidal disruption. Our focus is on BBHs with a total mass of M = 100 - [UNK], which form in collisional environments at high redshifts (z > 10). The primary objective is to understand how BBHs can grow through accretion during their early evolutionary phases while surrounded by dense gas clouds. We particularly explore whether these systems can attain masses exceeding [UNK] before merging within a Hubble time. The initial conditions for our simulations were derived using Monte Carlo analysis based on the distribution function of isolated BBHs, as developed by Belczynski et al. (2010). For each model, we conducted multiple simulations starting from different orbital positions, maintaining the assumption of circular orbits. Our results indicate that most massive binaries disintegrate within a few hundred million years after formation due to gravitational wave emissions. However, a subset persists to the present day if they reside in regions where the adjacent gas density is around 10^9 cm^-3. These enduring binaries may be observable by upcoming space-based gravitational wave observatories, such as LISA or DECIGO/BBO.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 5.437685266051317,
        "rewrite-fast-z-score": -0.10259783520851541
    },
    {
        "original_text": "We present the first detection of relativistically modulated X-ray fluxes from the Galactic Center black hole candidate SgrA*, using data obtained with Chandra and XMM-Newton observatories over an eight-year period (2000-2007). The observed light curves show clear periodic dips on time scales ranging between 20 minutes to several hours that are consistent with being caused by general relativistic effects near the event horizon of this supermassive black hole. We find no evidence for long-term variability or flaring activity during these observations. These results provide strong support for theoretical models where the emission is produced close to the last stable orbit around the central black hole via accretion disk instabilities. This work was supported by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Subject headings: Black holes -accretion disks -X-rays",
        "watermark_text": "We present the first detection of relativistically modulated X - ray fluxes from the Galactic Center black hole member SgrA * , using data acquired with Chandra and XMM - Newton observatories over an eight - month period ( 2000 - 2007 ) . The observed light curves show consistent continuous dips on time ranges ranging between 20 seconds to several days that are compatible with being affected by general relativistic effects near the event horizon of this supermassive black hole .We see no evidence for large - term variability or flaring activity during these observations . These conclusions provide broad support for theoretical theories where the emission is produced nearly to the last steady orbit around the central black hole via accretion ring instabilities .This project was supported by NASA grants NAG5 - 10842 , NNG06GH50G , NNX07AH41G , and NSF grant AST - 0707765 . Subject headings : Black holes - accretion disks - X - rays",
        "rewrite_text": "We report the first detection of relativistically modulated X-ray fluxes from the black hole at the Galactic Center, known as SgrA*, utilizing data collected from the Chandra and XMM-Newton observatories over an eight-month period between 2000 and 2007. The light curves observed demonstrate consistent continuous dips over time intervals ranging from 20 seconds to several days, which align with the effects of general relativity near the event horizon of this supermassive black hole. No evidence of long-term variability or flaring activity was detected during these observations. These findings strongly support theoretical models suggesting that the emissions occur close to the last stable orbit around the central black hole due to instabilities in the accretion ring. This research was funded by NASA grants NAG5-10842, NNG06GH50G, NNX07AH41G, and NSF grant AST-0707765. Subject headings: Black holes - accretion disks - X-rays.",
        "ori-fast-z-score": -1.0327955589886444,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We present new infrared photometry for the sample of giant low surface brightness galaxies (LSBGs) studied by van Dokkum et al. (2015) . We use these data to explore their stellar populations and dust content, as well as investigate whether they are consistent with being massive analogues of local dwarf spheroidal galaxies. The LSBGs have been observed in J-(1.25 um), H-(1.65um), Ks-(2.17um), 3.6um, 4.5um, 5.8um & 8.0um bands using Spitzer/IRAC and MIPS cameras on board the Spitzer Space Telescope. Our results show that all but one galaxy has an old age population dominated by red giants at least 1 Gyr older than those found in typical early-type galaxies. This is supported by our analysis which shows that most of them have very little cold dust emission compared to other types of galaxies. These findings suggest that the majority of the LSBGs may be undergoing passive evolution without significant star formation activity over the past few billion years.",
        "watermark_text": "We report new infrared photometry for the sample of large small surface brightness galaxies ( LSBGs ) studied by van Dokkum et al . ( 2015 ) .We use these information to examine their stellar environments and dust content , as well as investigate whether they are compatible with being huge analogues of local dwarf spheroidal galaxies . The LSBGs have been observed in J - ( 1 . 25 um ) , H - ( 1 . 65um ) , Ks - ( 2 . 17um ) , 3 . 6um , 4 . 5um , 5 . 8um & 8 . 0um bands using Spitzer / IRAC and MIPS cameras on board the Spitzer Space Telescope .Our results show that all but one star has an old age population dominated by red giants at least 1 Gyr older than those observed in standard early - class objects . This is backed by our analysis which reveals that most of them have very small cold dust absorption compared to other types of stars .These studies imply that the majority of the LSBGs might be experiencing passive evolution without significant star formation activity over the previous few billion years .",
        "rewrite_text": "We present new infrared photometry for the sample of large low surface brightness galaxies (LSBGs) analyzed by van Dokkum et al. (2015). This data allows us to explore their stellar environments and dust content, as well as to determine if they can serve as large analogues to local dwarf spheroidal galaxies. The LSBGs were observed in the J (1.25 µm), H (1.65 µm), Ks (2.17 µm), 3.6 µm, 4.5 µm, 5.8 µm, and 8.0 µm bands utilizing the Spitzer/IRAC and MIPS instruments aboard the Spitzer Space Telescope. Our findings indicate that nearly all of the stars are part of an older population dominated by red giants that are at least 1 billion years older than those found in typical early-type galaxies. This is further supported by our analysis, which shows that most of these galaxies exhibit very low levels of cold dust absorption compared to other types of stars. These observations suggest that the majority of LSBGs may be undergoing passive evolution with minimal star formation activity over the last several billion years.",
        "ori-fast-z-score": 0.35603449745815596,
        "water-fast-z-score": 5.815230125149881,
        "rewrite-fast-z-score": 1.5011106998930268
    },
    {
        "original_text": "We present new observations of the molecular gas in the central region of the nearby galaxy NGC 891, obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal an extended distribution of dense (n(H2) ~ 104 cm-3), warm (T~50K) molecular gas that is associated with the optical disk of this edge-on spiral galaxy. We find evidence for two distinct components to the molecular gas distribution; one component follows closely the dust lane seen in visible light images while another component extends out into the surrounding intergalactic medium. This latter component has been detected previously by other authors but our higher resolution data allow us to resolve it into individual clouds. In addition we detect several compact sources within the galactic plane which are likely to be young star forming regions. These results suggest that there may exist a significant reservoir of molecular material outside the main body of galaxies such as NGC 891.",
        "watermark_text": "We present new images of the molecular gas in the central region of the nearby galaxy NGC 891 , obtained with the IRAM 30m telescope at 1mm and 3mm wavelengths . The data reveal an extended distribution of dense ( n ( H2 ) ~ 104 mm - 3 ) , warm ( T ~ 50K ) molecular dust that is associated with the optical disk of this edge - on spiral galaxy .We see evidence for two different components to the molecular gas distribution ; one element follows tightly the dust track seen in apparent light photographs while another component moves out into the nearby intergalactic medium . This latter component has been detected earlier by other researchers but our higher resolution data enable us to separate it into single clouds .In addition we locate many compact sources within the galactic plane which are likely to be young galaxy producing regions . These data suggest that there may contain a substantial pool of molecular matter outside the main bodies of stars such as NGC 891 .",
        "rewrite_text": "We present new images of molecular gas in the central region of the nearby galaxy NGC 891, captured using the IRAM 30m telescope at 1mm and 3mm wavelengths. The data reveal a widespread distribution of dense (n(H2) ~ 10^4 mm^-3) and warm (T ~ 50K) molecular dust associated with the optical disk of this edge-on spiral galaxy. We observe evidence of two distinct components in the molecular gas distribution: one closely follows the dust track visible in optical light images, while the other extends into the surrounding intergalactic medium. Although this latter component has been identified by previous researchers, our higher resolution data allows us to resolve it into individual clouds. Additionally, we identify numerous compact sources within the galactic plane, likely representing young star-forming regions. These findings suggest the presence of a significant reservoir of molecular material beyond the primary stellar bodies, such as NGC 891.",
        "ori-fast-z-score": -1.6666666666666667,
        "water-fast-z-score": 6.260990336999411,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "We present the results of our study on the possible existence and stability of terrestrial planets around the star Gliese 581, which is located at about 20 light-years away from Earth. We have performed numerical simulations for different orbital configurations of three hypothetical terrestrial planets with masses ranging between 1 to 10 times that of Earth s mass (1-10 M⊕). Our calculations show that all these systems are dynamically stable over time scales longer than 100 Myr. The most massive planet has an eccentric orbit with e=0.2 and its periastron distance ranges between 0.05 AU and 0.15 AU depending on the initial conditions used. This planet can be considered as a hot Jupiter-like planet because it orbits very close to its host star. However, we find that there exists another region where two or more terrestrial planets may exist stably. In this region, one of them could be a super-Earth-type planet with a mass larger than 5M⊕ but smaller than 8M⊕.",
        "watermark_text": "We present the conclusion of our research on the possible existence and dynamics of terrestrial worlds around the star Gliese 581 , which is situated at about 20 light - years distance from Earth . We have done mathematical simulations for different orbital arrangements of three hypothetical terrestrial worlds with masses differing between 1 to 10 twice that of Earth s mass ( 1 - 10 M⊕ ) .Our calculations show that all these systems are dynamically stable over time ranges longer than 100 Myr . The most large planet has an eccentric orbit with e = 0 . 2 and its periastron speed ranges between 0 . 05 AU and 0 . 15 AU depending on the early conditions utilized .This planet can be regarded as a bright Jupiter - like planet because it orbits very close to its host star . However , we find that there exists another region where two or more terrestrial worlds may arise stably .In this area , one of them could be a super - Earth - class planet with a mass greater than 5M⊕ but smaller than 8M⊕ .",
        "rewrite_text": "We present our research findings on the potential existence and dynamics of terrestrial planets around the star Gliese 581, located approximately 20 light-years from Earth. We conducted mathematical simulations to explore various orbital configurations for three hypothetical terrestrial planets, each with masses ranging from 1 to 10 times that of Earth (1 - 10 M⊕). Our results indicate that all these systems remain dynamically stable over periods exceeding 100 million years. The largest of these planets exhibits an eccentric orbit with an eccentricity of e = 0.2, with periastron speeds ranging from 0.05 AU to 0.15 AU, depending on the initial conditions considered. This planet can be classified as a bright Jupiter-like planet due to its proximity to its host star. Additionally, we identified another region where two or more terrestrial planets could exist stably. Within this region, one of these planets could be classified as a super-Earth, with a mass between 5 and 8 M⊕.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -0.9847319278346618
    },
    {
        "original_text": "We study the magnetic field amplification in supernova remnants (SNRs) due to cosmic ray streaming instability, which is driven by anisotropic diffusion of energetic particles across the mean magnetic field lines. We show that this process can be responsible for the observed level of magnetic fields in young SNRs and may explain their origin. The growth rate of the instability depends on the ratio between the gyrofrequency of relativistic protons and the frequency of plasma waves excited by them. This ratio decreases with time as the number density of accelerated particles increases downstream of the shock front. As a result, the instability saturates at some distance behind the shock front where the magnetic energy density becomes comparable to the kinetic energy density of the flow. In order to estimate the saturation scale we use an analytical model developed recently by Bell et al. (2013) . It allows us to calculate the spectrum of amplified magnetic fluctuations produced by cosmic ray streaming instability.",
        "watermark_text": "We research the magnetic force amplification in supernova remnants ( SNRs ) resulting to cosmic ray streaming instability , which is caused by anisotropic absorption of excited particles across the mean magnetic field lines . We see that this process can be responsible for the observed level of magnetic waves in young SNRs and may reason their source .The growth speed of the instability depends on the proportion between the gyrofrequency of relativistic protons and the frequency of plasma beams excited by them . This ratio falls with time as the number density of advanced waves rises downstream of the shock front .As a result , the instability saturates at some distance behind the shock wall where the magnetic energy density becomes identical to the kinetic power concentration of the flow . In order to estimate the saturation scale we utilize an analytical theory created recently by Bell et al .( 2013 ) . It enables us to estimate the spectrum of amplified magnetic fluctuations produced by cosmic ray streaming instability .",
        "rewrite_text": "We investigate the amplification of magnetic forces in supernova remnants (SNRs) due to cosmic ray streaming instability, which arises from the anisotropic absorption of excited particles in relation to the mean magnetic field lines. Our findings indicate that this mechanism can account for the levels of magnetic waves observed in young SNRs and may shed light on their origins. The rate at which the instability develops is influenced by the ratio of the gyrofrequency of relativistic protons to the frequency of the plasma beams they generate. This ratio decreases over time as the number density of advanced waves increases downstream of the shock front. Consequently, the instability reaches saturation at a certain distance behind the shock wall, where the density of magnetic energy matches the kinetic energy of the flow. To estimate the saturation scale, we apply a recently developed analytical theory by Bell et al. (2013), which allows us to determine the spectrum of magnetic fluctuations amplified by cosmic ray streaming instability.",
        "ori-fast-z-score": -0.6708203932499369,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 2.81271975231506
    },
    {
        "original_text": "We present an analysis of the observed distribution of the magnetic field geometries for stars across the upper main sequence (UMS). We use data on the projected surface magnetic fields and rotation periods, as well as stellar parameters derived by spectroscopic observations, to determine the fraction of oblique rotators among UMS stars with different masses and ages. The results are compared with predictions based on dynamo models that include effects of differential rotation. \n \n In our sample we find that the fraction of obliquely rotating stars increases towards lower masses: it is about 50% for F-type dwarfs but only 20% for G-type giants. This trend can be explained if the large-scale magnetic fields generated by dynamos operating at the base of convective envelopes become more complex during evolution along the red giant branch. Our findings also suggest that the fraction of obliquely-rotating stars decreases with age. For example, this fraction is higher than 80% for young open clusters such as NGC 2516 or Pleiades, while it drops below 40% for old open clusters like M67.",
        "watermark_text": "We present an assessment of the seen distribution of the magnetic force geometries for stars across the higher major sequence ( UMS ) . We use data on the projected surface magnetic fields and rotation periods , as also as stellar characteristics obtained by spectroscopic observations , to estimate the fraction of oblique rotators among UMS stars with various masses and periods .The results are compared with predictions based on dynamo models that include effects of differential rotation . In our sample we find that the fraction of obliquely rotating stars increases towards lesser masses : it is about 50 % for F - class dwarfs but only 20 % for G - class giants .This trend can be understood if the huge - scale magnetic waves generated by dynamos working at the base of convective envelopes get more sophisticated during progression along the red giant branch . Our findings also suggest that the fraction of obliquely - spinning stellar reduces with age .For instance , this amount is higher than 80 % for young open nuclei such as NGC 2516 or Pleiades , while it drops below 40 % for old close galaxies like M67 .",
        "rewrite_text": "We provide an evaluation of the observed distribution of magnetic force geometries for stars within the upper main sequence (UMS). Utilizing data on projected surface magnetic fields and rotation periods, alongside stellar characteristics gleaned from spectroscopic observations, we estimate the proportion of oblique rotators among UMS stars of varying masses and rotation periods. Our findings are contrasted with predictions from dynamo models that account for differential rotation effects. Our analysis reveals that the fraction of obliquely rotating stars increases for lower mass categories: approximately 50% for F-type dwarfs compared to only 20% for G-type giants. This trend may be explained by the complex large-scale magnetic waves produced by dynamos at the base of convective envelopes, which become more intricate as stars evolve along the red giant branch. Additionally, our results indicate that the prevalence of obliquely spinning stars diminishes with age; for example, this fraction exceeds 80% in young open clusters like NGC 2516 or the Pleiades, while it falls below 40% in older clusters such as M67.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.800793585191832,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "The Dalitz plot distribution for the decay D+ ->K-pi+pi+ is measured using data collected by the FOCUS experiment at Fermilab, corresponding to an integrated luminosity of 1 fb-1 . The measurement uses a sample of about 2 million events with one charged track and two neutral clusters reconstructed in the central drift chamber (CDC) and electromagnetic calorimeter (EMC). A maximum likelihood fit is performed on this sample to extract the branching fraction B(D+ ->K-pi+pipi+), which is found to be  _ = (1.55 +/- 0.10 ) x 10-3 , where the uncertainty includes both statistical and systematic contributions.  This result agrees well with previous measurements but has improved precision due to the larger number of signal events used here compared to earlier results. It also improves upon the most recent theoretical prediction based on lattice QCD calculations. The ratio Rc/D between the Cabibbo-suppressed and Cabibbo-favored decays into three pions is determined as Rc/D=(0.84+0.11-0.12)x10-2.",
        "watermark_text": "The Dalitz plot distribution for the decay D + - > K - pi + pi + is measured using data taken by the FOCUS experiment at Fermilab , corresponding to an integrated luminosity of 1 fb - 1 . The measurement involves a sample of about 2 million episodes with one charged track and two neutral nuclei reconstructed in the main drift chamber ( CDC ) and electromagnetic calorimeter ( EMC ) .A maximum likelihood fit is conducted on this data to extract the branching fraction B ( D + - > K - pi + pipi + ) , which is found to be _ = ( 1 . 55 + / - 0 . 10 ) x 10 - 3 , where the uncertainty includes both statistical and systematic contributions . This result agrees well with previous measurements but has improved precision thanks to the bigger quantity of signal events employed here compared to earlier findings .It additionally improves upon the most current theoretical estimate based on lattice QCD calculations . The ratio Rc / D between the Cabibbo - subdued and Cabibbo - preferred decays into three pions is calculated as Rc / D = ( 0 . 84 + 0 . 11 - 0 . 12 ) x10 - 2 .",
        "rewrite_text": "The Dalitz plot distribution for the decay process \\( D^+ \\to K^- \\pi^+ \\pi^+ \\) was analyzed using data from the FOCUS experiment at Fermilab, which corresponds to an integrated luminosity of 1 fb\\(^{-1}\\). This analysis utilized a sample of approximately 2 million events, each containing one charged track and two neutral particles, which were reconstructed in the main drift chamber (CDC) and the electromagnetic calorimeter (EMC). To determine the branching fraction \\( B(D^+ \\to K^- \\pi^+ \\pi^+) \\), a maximum likelihood fit was performed, resulting in a value of \\( B = (1.55 \\pm 0.10) \\times 10^{-3} \\). The reported uncertainty includes both statistical and systematic components. This result is in strong agreement with earlier measurements but benefits from enhanced precision due to a larger number of signal events compared to past studies. Additionally, it refines the latest theoretical estimate derived from lattice QCD calculations. The ratio \\( R_{c/d} \\) of Cabibbo-suppressed to Cabibbo-favored decays into three pions is calculated to be \\( R_{c/d} = (0.84^{+0.11}_{-0.12}) \\times 10^{-2} \\).",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 4.83735464897913,
        "rewrite-fast-z-score": -0.9428090415820635
    },
    {
        "original_text": "We consider the problem of scheduling multiple bag-of-work applications on parallel machines with non-cooperative tasks, where each task has its own deadline and budget constraints. We propose an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets. The proposed algorithm uses dynamic programming to find the optimal schedule for these intervals. Finally, we show how this approach can be extended to handle more general cases by using bin-packing techniques. Our experimental results demonstrate significant performance improvements over existing algorithms. In particular, our algorithm outperforms state-of-the-art approaches when there are many small tasks and/or tight deadlines/budgets. Keywords: Parallel computing, Computational complexity theory, Computational geometry, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems",
        "watermark_text": "We consider the question of scheduling multiple bag - of - job applications on parallel machines with non - cooperative tasks , where each work has its own deadline and funding limitations . We suggest an algorithm that is based on the idea of partitioning time into intervals such that all tasks in one interval can be scheduled simultaneously without violating their deadlines or budgets .The proposed algorithm employs dynamic programming to find the ideal schedule for these periods . Finally , we show how this methodology can be improved to treat more general instances by using bin - packing methods .Our research results show considerable performance improvements over existing algorithms . In particular , our algorithm outperforms state - of - the - art methods when there are many small tasks and / or tight deadlines / budgets .Keywords : Parallel computing , Computational complexity analysis , Computational topology , Data mining , Bioinformatics , High - performance computing , Grid computing , Cloud computing , Big data analysis , Distributed systems",
        "rewrite_text": "We examine the challenge of scheduling multiple bag-of-job applications on parallel machines with non-cooperative tasks, each having its own deadlines and budget constraints. We propose an algorithm that partitions time into intervals, allowing all tasks within the same interval to be scheduled concurrently without breaching their deadlines or budget limits. This algorithm utilizes dynamic programming to develop the optimal schedule for these intervals. Additionally, we demonstrate how our approach can be enhanced to handle more complex cases by integrating bin-packing techniques. Our research indicates significant performance improvements compared to existing algorithms, particularly in scenarios with numerous small tasks and/or strict deadlines and budgets. \n\nKeywords: Parallel computing, Computational complexity analysis, Computational topology, Data mining, Bioinformatics, High-performance computing, Grid computing, Cloud computing, Big data analysis, Distributed systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 0.601929265428846
    },
    {
        "original_text": "We study deviations from tri-bimaximal neutrino mixing in the type-II seesaw model with an additional U(1) gauge symmetry, which is broken at high energy scales by two Higgs doublets. We show that this scenario can be realized within the framework of supersymmetric grand unified theories (SUSY GUTs). In particular we consider SO(10) SUSY GUT models where the right-handed Majorana mass matrix for the light neutrinos arises from the vacuum expectation values of three pairs of Higgs fields transforming as 10 ⊕ 126 ⊕ 120 under SU(5), while the Dirac mass matrices are generated through Yukawa couplings to 16-plet fermions. The resulting effective neutrino mass matrix has off-diagonal entries proportional to the ratio between the masses of the heavy and light neutrinos. This leads to corrections to the tribimaximal pattern of neutrino mixings. Furthermore, we discuss leptogenesis in our model and find that it can account for the observed baryon asymmetry of the universe if the CP violating phases satisfy certain conditions.",
        "watermark_text": "We explore deviations from tri - bimaximal neutrino composition in the class - II seesaw model with an additional U ( 1 ) gauge symmetry , which is shattered at high energy scales by two Higgs doublets . We see that this situation can be realized within the framework of supersymmetric grand unified theories ( SUSY GUTs ) .In particular we define SO ( 10 ) SUSY GUT models where the right - handed Majorana mass vector for the light neutrinos emerges from the vacuum expectation values of three sets of Higgs fields converting as 10 ⊕ 126 ⊕ 120 under SU ( 5 ) , while the Dirac mass matrices are produced through Yukawa couplings to 16 - plet fermions . The resulting effective neutrino mass vector has off - diagonal entries proportional to the proportion between the masses of the heavy and light neutrinos .This leads to corrections to the tribimaximal pattern of neutrino mixings . Furthermore , we investigate leptogenesis in our model and find that it can account for the seen baryon asymmetry of the universe if the CP violating stages fulfill certain conditions .",
        "rewrite_text": "We investigate deviations from tri-bimaximal neutrino composition within the class-II seesaw model, incorporating an additional U(1) gauge symmetry that is broken at high energy scales by two Higgs doublets. This scenario can be realized in the context of supersymmetric grand unified theories (SUSY GUTs). Specifically, we focus on SO(10) SUSY GUT models, where the right-handed Majorana mass vector for the light neutrinos arises from the vacuum expectation values of three sets of Higgs fields, which transform as 10 ⊕ 126 ⊕ 120 under SU(5). Meanwhile, the Dirac mass matrices are generated through Yukawa couplings to 16-plet fermions. The resulting effective neutrino mass vector contains off-diagonal elements proportional to the ratio of the masses of heavy and light neutrinos, leading to corrections to the tribimaximal neutrino mixing pattern. Additionally, we explore leptogenesis within our model, demonstrating that it can explain the observed baryon asymmetry of the universe if certain conditions related to CP violation are met.",
        "ori-fast-z-score": 0.8962581595302719,
        "water-fast-z-score": 5.249512077248736,
        "rewrite-fast-z-score": 0.8819171036881969
    },
    {
        "original_text": "The Hubble Space Telescope has revealed the most detailed view yet of an active galactic nucleus (AGN) in the galaxy NGC6908, located about 300 million light years away.  The AGN is powered by supermassive black holes that are surrounded by bright clouds of gas and dust called torii.   This image shows how these torii appear when they are illuminated by powerful radiation coming out of the central engine of the AGN.   ... Full text here . \n \n Image credits: NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2, Digitized Sky Survey 2.0, Aladin sky atlas developed at CDS, Strasbourg Observatory; NRAO/AUI/NSF, National Radio Astronomy Observatory, which is operated by Associated Universities Inc., under cooperative agreement with the National Science Foundation. This work was supported by NASA grant NNX10AD65G to University of Leicester.",
        "watermark_text": "The Hubble Space Telescope has confirmed the most detailed view ever of an active galactic nucleus ( AGN ) in the universe NGC6908 , located about 300 million light years distance . The AGN is powered by supermassive black holes that are surrounded by bright clusters of gas and dust called torii .This image shows how these torii appear when they are illuminated by potent radiation coming out of the main motor of the AGN . . . . Full text here .Image credits : NASA , ESA , STScI , A . Simionescu et al . ( University of Leicester ) , DSS2 , Digitized Sky Survey 2 . 0 , Aladin sky atlas created at CDS , Strasbourg Observatory ; NRAO / AUI / NSF , National Radio Astronomy Observatory , which is controlled by Associated Universities Inc . , under partnership agreement with the National Science Foundation .This project was supported by NASA award NNX10AD65G to University of Leicester .",
        "rewrite_text": "The Hubble Space Telescope has captured the most detailed image to date of an active galactic nucleus (AGN) in the universe, specifically NGC 6908, which is approximately 300 million light-years away. This AGN is energized by supermassive black holes, surrounded by luminous clusters of gas and dust known as torii. The image reveals how these torii are illuminated by intense radiation emanating from the AGN's core. For complete details, refer to the full text. Image credits include NASA, ESA, STScI, A. Simionescu et al. (University of Leicester), DSS2 (Digitized Sky Survey 2.0), Aladin sky atlas created at the CDS, Strasbourg Observatory, and NRAO/AUI/NSF, which is managed by Associated Universities Inc. in partnership with the National Science Foundation. This project received funding from a NASA award (NNX10AD65G) to the University of Leicester.",
        "ori-fast-z-score": 0.9486832980505138,
        "water-fast-z-score": 5.153734142324001,
        "rewrite-fast-z-score": 1.7179113807746667
    },
    {
        "original_text": "We present the results of an analysis of all available data on cometary dust tails, including those observed by spacecraft and ground-based telescopes in recent years. We find that most of these objects are associated with Jupiter family comets (JFCs), which have orbital periods less than 20 yr. The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits. These tails appear as narrow streams of material extending outward at high speed from the parent bodies. In some cases they show evidence of being disrupted into multiple fragments or branches. Most of the tail structures we observe are consistent with models where particles are released continuously over time scales ranging from months to thousands of years. However, there is growing observational evidence suggesting that many of these tails may also contain significant amounts of freshly produced dust grains ejected during more recent outburst events. This suggests that the production rate of dust particles in these systems varies significantly both spatially and temporally. \n \n Keywords: Comet",
        "watermark_text": "We publish the conclusion of an assessment of all available data on cometary dust tails , particularly those observed by satellites and land - based telescopes in recent seasons . We see that most of these objects are identified with Jupiter class comets ( JFCs ) , which have orbital periods fewer than 20 yr .The JFCs produce dust tails that can be traced for up to several thousand AU along their orbits . These tails occur as short streams of debris extending outward at high velocity from the parent bodies .In some cases they show proof of being disrupted into multiple pieces or limbs . Most of the tail structures we study are compatible with models where objects are released constantly over time ranges varied from months to thousands of years .However , there is growing observational evidence indicating that several of these tails might additionally carry significant amounts of newly released powder grains ejected during more recent outburst events . This implies that the production frequency of dust particles in these systems vary significantly both spatially and temporally .Keywords: Comet",
        "rewrite_text": "We present the findings from an assessment of all available data on cometary dust tails, particularly those captured by satellites and ground-based telescopes in recent seasons. Our analysis indicates that the majority of these dust tails are associated with Jupiter Family Comets (JFCs), which have orbital periods of less than 20 years. These JFCs generate dust tails that can be traced for thousands of astronomical units along their orbits. The tails manifest as short streams of debris that extend outward at high velocities from the parent comets. In certain instances, there is evidence suggesting that these tails may be fragmented into multiple segments or limbs. Most of the tail structures we examine align with models indicating that debris is released consistently over various timeframes, ranging from months to thousands of years. However, increasing observational evidence suggests that several of these tails may also contain significant amounts of freshly released dust grains expelled during more recent outburst events. This indicates that the rate of dust particle production in these systems varies considerably both in space and over time. \nKeywords: Comet",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 7.979625217054442,
        "rewrite-fast-z-score": 1.9917411897716448
    },
    {
        "original_text": "We study the generic properties of discrete choice models with social influence, where agents make decisions based on their own private information and the choices made by others in their neighborhood.  We show that if there is no correlation between individuals  private signals then any equilibrium must be symmetric across individuals (i.e., all equilibria are identical up to relabeling). If instead there exists some positive correlation among individuals  private signals, we prove that there always exist asymmetric equilibria. Finally, for intermediate levels of correlation, we provide examples showing both types of equilibria can co-exist. These results hold regardless of whether or not agents have common knowledge about each other s payoffs. \nOur analysis relies heavily on recent advances in the theory of global games. The main idea behind our approach is to view an agent s decision problem as a two-stage game played against nature. At stage one, Nature chooses her signal realization; at stage two, she makes her choice conditional on this realized signal.",
        "watermark_text": "We research the generic properties of discrete choosing models with social impact , where agents form decisions based on their own private information and the decisions taken by others in their neighborhood . We see that if there is no correlation between individuals personal signals then any equilibrium must be symmetric across persons ( i . e . , all equilibria are equal up to relabeling ) .If instead there exists some positive interaction among persons private signals , we prove that there always occur asymmetric equilibria . Finally , for intermediate amounts of correlation , we provide examples demonstrating both types of equilibria can co - arise .These data hold regardless of whether or not agents have common knowledge about each other s payoffs . Our theory relies extensively on current developments in the notion of international competition .The main idea behind our approach is to see an agent s decision problem as a two - phase tournament held against nature . At phase one , Nature chooses her signal realization ; at stage two , she gives her decision conditional on this realized signal .",
        "rewrite_text": "We investigate the fundamental characteristics of discrete choice models with social impact, where individuals make decisions based on their private information along with the choices made by those in their vicinity. Our findings indicate that in the absence of correlation among individuals' personal signals, any equilibrium will be symmetric among agents (i.e., all equilibria can be considered equivalent through relabeling). However, when there is positive interaction among individuals' private signals, we demonstrate that asymmetric equilibria invariably emerge. Furthermore, in cases of moderate correlation, we provide examples showing that both types of equilibria can coexist. These results remain valid regardless of whether agents are aware of each other’s payoffs. Our analysis is deeply informed by recent advancements in the concept of international competition. The core idea of our approach frames an individual's decision-making as a two-phase tournament against nature: in the first phase, nature determines the realization of her signal; in the second phase, she makes a decision based on this realized signal.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": -0.1111111111111111
    },
    {
        "original_text": "We report on the electronic structure and magnetic properties of zigzag graphene nanoribbons (ZGNRs) with different edge structures, including hydrogenated ZGNR (H-ZGNR), fluorinated ZGNR (F-ZGNR), oxygenated ZGNR (O-ZGNR), and nitrogen-doped O-ZGNR (N-ZGNR). We find that all these ZGNRs are half-metals except for H-ZGNR which is metallic. The band gaps of F-ZGNR and N-ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at edges and their neighbors. In contrast, the band gap decreases slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from oxygen to neighboring carbon atom(s). Our results show that the spin polarization can be enhanced by introducing oxygen into the edges of ZGNRs.",
        "watermark_text": "We report on the electronic structure and magnetic properties of zigzag graphene nanoribbons ( ZGNRs ) with various edge structures , notably hydrogenated ZGNR ( H - ZGNR ) , fluorinated ZGNR ( F - ZGNR ) , oxygenated ZGNR ( O - ZGNR ) , and nitrogen - doped O - ZGNR ( N - ZGNR ) . We see that all these ZGNRs are half - metals except for H - ZGNR which is metallic .The band holes of F - ZGNR and N - ZGNR increase as compared to those of pristine ZGNR due to the electronegativity difference between carbon atoms at corners and their relatives . In comparison , the band gap falls slightly when oxygen replaces one or two carbon atoms at each edge because of charge transfer from nitrogen to neighboring carbon molecule ( s ) .Our results show that the spin polarization can be enhanced by bringing oxygen into the edges of ZGNRs .",
        "rewrite_text": "We examine the electronic structure and magnetic characteristics of zigzag graphene nanoribbons (ZGNRs) with different edge configurations, including hydrogenated ZGNR (H-ZGNR), fluorinated ZGNR (F-ZGNR), oxygenated ZGNR (O-ZGNR), and nitrogen-doped O-ZGNR (N-ZGNR). Our findings indicate that all of these ZGNRs exhibit half-metallicity, except for H-ZGNR, which behaves as a metal. The band gaps of F-ZGNR and N-ZGNR are found to be larger compared to pristine ZGNR, attributed to the difference in electronegativity between the carbon atoms at the edges and their counterparts. Conversely, when oxygen atoms substitute one or two carbon atoms at each edge, the band gap slightly decreases due to charge transfer from nitrogen to the adjacent carbon atom(s). Our results reveal that introducing oxygen into the edges of ZGNRs can enhance spin polarization.",
        "ori-fast-z-score": 0.5897678246195885,
        "water-fast-z-score": 3.8334908600273256,
        "rewrite-fast-z-score": 1.414213562373095
    },
    {
        "original_text": "We present results on the role played by the rho meson in describing pion electroproduction data obtained with the CLAS detector at Jefferson Lab (JLab). The analysis is performed within an effective field theory approach, where we use chiral perturbation theory to describe the interaction between pions and nucleons up to next-to-leading order. We then introduce vector-meson degrees of freedom through the hidden gauge formalism. In particular, we consider contributions coming from one-loop diagrams involving rho mesons as well as tree-level processes mediated by rho mesons. Our theoretical framework allows us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single-pion production off protons. Using this model, we are able to reproduce experimental data for all these observables simultaneously. Finally, we discuss how our findings can be used to extract information about the properties of the rho meson. This work was supported by the U.S. \n\n\nDepartment of Energy under Contract No. DE-SC0012704.\n\nPACS numbers: 12.38.Mh",
        "watermark_text": "We report findings on the importance played by the rho meson in representing pion electroproduction measurements obtained with the CLAS detector at Jefferson Lab ( JLab ) . The investigation is conducted within an efficient field model approach , where we using chiral perturbation theory to explain the interaction between pions and nucleons up to next - to - leading order .We then introduce vector - meson degrees of liberty through the hidden gauge formalism . In particular , we treat contributions come from one - loop diagrams using rho mesons as well as tree - level processes induced by rho mesons .Our conceptual formulation enables us to study both neutral current reactions such as elastic ep scattering or charged current reactions like single - pion production off protons . Using this model , we are able to publish empirical data for all these observables simultaneously .Finally , we talk how our findings can be used to extract information about the properties of the rho meson . This research was supported by the U . S . Department of Energy under Contract No .DE - SC0012704 . PACS numbers : 12 . 38 . Mh",
        "rewrite_text": "We present our findings on the role of the rho meson in interpreting pion electroproduction measurements recorded by the CLAS detector at Jefferson Lab (JLab). Our study employs an efficient field model approach, utilizing chiral perturbation theory to describe the interactions between pions and nucleons up to next-to-leading order. We also incorporate vector-meson degrees of freedom through the hidden gauge formalism. Specifically, we account for contributions from one-loop diagrams involving rho mesons as well as tree-level processes induced by them. Our framework allows for the examination of both neutral current reactions, such as elastic ep scattering, and charged current reactions like single-pion production from protons. Using this model, we are able to produce empirical data on all these observables concurrently. Finally, we discuss how our results can provide insights into the properties of the rho meson. This research received support from the U.S. Department of Energy under Contract No. DE-SC0012704. PACS numbers: 12.38.Mh.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 0.6201736729460423
    },
    {
        "original_text": "We report on the room-temperature ferromagnetism in Mn-doped ZnO thin films grown by pulsed laser deposition (PLD). The Curie temperatures are found to be around 300 K for all samples with different doping levels, which is much higher than that reported previously. We also find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low fields. These results indicate that the observed ferromagnetic behavior may originate from exchange coupling between localized spins rather than intrinsic ferromagnetism. \n \n In recent years, there has been growing interest in developing new materials for spintronic applications such as nonvolatile memory devices or logic circuits based on the manipulation of electron spins instead of charge carriers1-5 . Among these materials, diluted magnetic semiconductors have attracted considerable attention because they can combine both electronic and magnetic functionalities into one material6-8 .\n \n\n\nZnO-based DMSs have been extensively studied due to their wide band gap energy (3.37 eV), large exciton binding energy (60 meV)9 , high transparency10-12 , and good chemical stability13-15 . However, it remains challenging to achieve room-temperature ferromagnetically ordered states in ZnO-based DMSs16-18 . Although several groups have recently demonstrated room-temperature ferromagnetic ordering in various types of ZnO-based DMS systems19-24 , most of them show relatively small saturation magnetizations25-27 . \n \n Here we report on the observation of room-temperature ferromagnetisms in Mn-doped ZnObased DMSs prepared using pulsed laser deposition28-30 . Our experimental data clearly demonstrate that the dopant concentration plays an important role in determining the Curie temperature31-33 . For example, our sample with x = 0.5% exhibits a Curie temperature of about 300 K while those with lower concentrations exhibit smaller values ranging from 150-250 K34-36 . Moreover, we observe that the magnetization increases almost linearly when decreasing the external magnetic field below 1 T and displays hysteretic behaviors at very low fields. This indicates that the observed ferr",
        "watermark_text": "We report on the room - temperature ferromagnetism in Mn - doped ZnO thin sheets grown by pulsed laser precipitation ( PLD ) . The Curie temperatures are found to be around 300 K for all specimens with various doping rates , which is much higher than that described earlier .We additionally find that the magnetization increases linearly as the applied field decreases and shows hysteresis loops at low areas . These conclusions show that the seen ferromagnetic activity may originate from exchange interactions between localized spins rather than intrinsic ferromagnetism .In past times , there has been growing interest in building new materials for spintronic use such as nonvolatile memory devices or logic devices using on the manipulation of electron spins rather of charge carriers1 - 5 . Among these materials , diluted magnetic semiconductors have garnered considerable scrutiny because they can mix both electronic and magnetic functionalities into one material6 - 8 .ZnO - based DMSs have been heavily explored thanks to their wide band gap energy ( 3 . 37 eV ) , large exciton activation energy ( 60 meV ) 9 , large transparency10 - 12 , and good molecular stability13 - 15 . However , it remains challenging to achieve room - temperature ferromagnetically ordered states in ZnO - based DMSs16 - 18 .Although several groups have recently shown room - temperature ferromagnetic sorting in different kinds of ZnO - based DMS systems19 - 24 , most of them show relatively small saturation magnetizations25 - 27 . Here we publish on the observation of room - temperature ferromagnetisms in Mn - doped ZnObased DMSs assembled utilizing laser optical deposition28 - 30 .Our experimental records distinctly show that the dopant concentration plays an important role in distinguishing the Curie temperature31 - 33 . For instance , our sample with x = 0 . 5 % experiences a Curie temperature of about 300 K while those with higher concentrations display lesser values ranging from 150 - 250 K34 - 36 .Moreover , we determine that the magnetization increases almost linearly when decreasing the external magnetic force below 1 T and displays hysteretic behaviors at very low areas . This implies that the observed ferr",
        "rewrite_text": "We present our findings on room-temperature ferromagnetism in Mn-doped ZnO thin films developed through pulsed laser deposition (PLD). All samples exhibit Curie temperatures around 300 K, which is significantly higher than previously reported values. Additionally, we observe that the magnetization rises linearly when the applied magnetic field is reduced and exhibits hysteresis at low field strengths. These results suggest that the ferromagnetic behavior may stem from exchange interactions among localized spins rather than from intrinsic ferromagnetism. \n\nIn recent years, there has been an increasing focus on developing new materials for spintronic applications, such as nonvolatile memory and logic devices that utilize the manipulation of electron spins instead of charge carriers. Among these materials, diluted magnetic semiconductors (DMSs) have attracted considerable attention due to their ability to integrate both electronic and magnetic properties. ZnO-based DMSs are particularly promising because of their wide bandgap (3.37 eV), substantial exciton activation energy (60 meV), high transparency, and good chemical stability. However, achieving room-temperature ferromagnetically ordered states in ZnO-based DMSs has proven to be a challenge. \n\nWhile several research groups have recently reported room-temperature ferromagnetism in various ZnO-based DMS systems, most of these studies have observed relatively low saturation magnetizations. In this work, we highlight our findings on room-temperature ferromagnetism in Mn-doped ZnO DMSs synthesized using optical laser deposition. Our experimental data clearly indicate that the concentration of the dopant significantly influences the Curie temperature. For instance, our sample with a Mn concentration of 0.5% has a Curie temperature of approximately 300 K, whereas samples with higher concentrations show lower Curie temperatures ranging from 150 to 250 K. Furthermore, we find that the magnetization increases almost linearly when reducing the external magnetic field below 1 T and demonstrates hysteresis effects at very low magnetic field strengths. This suggests that the observed ferromagnetism can be attributed to localized spin interactions.",
        "ori-fast-z-score": 0.16666666666666666,
        "water-fast-z-score": 8.833333333333334,
        "rewrite-fast-z-score": 0.8058229640253802
    },
    {
        "original_text": "We propose that the prompt emission of gamma-ray bursts (GRBs) is due to high-energy protons, muons and electron-positron pairs produced by ultra-relativistic shocks in GRB jets. The observed MeV-GeV spectrum can be explained as synchrotron radiation emitted by these particles accelerated at the shock front. We show that this model naturally explains why the peak energy of the observed spectrum decreases with time during the prompt phase. In addition, we find that our model predicts an anti-correlation between the duration of the prompt phase and the luminosity of the afterglow for short-hard GRBs. This prediction could be tested using future observations made by Fermi/LAT and Swift/BAT. Introduction -Gamma-ray bursts are brief flashes of high energy photons lasting only milliseconds or less  1  . They have been detected out to redshifts z = 8.2  2  , which implies their total energy output may exceed 10^53 erg  3  . Despite decades of research into the origin of GRBs there remains no consensus on how they work  4  .\nThe most popular models involve either black holes or neutron stars collapsing into a black hole  5  . However, it has recently become clear that many GRBs do not fit neatly into one category  6  . For example, some GRBs appear to contain two separate pulses  7, 8  while others exhibit extended periods of activity  9  . Furthermore, some GRBs seem to occur when two galaxies merge  10  . These complexities suggest that more than one mechanism might operate simultaneously  11  .\nIn recent years several authors  12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59   have proposed that GRBs are powered by relativistic jets e",
        "watermark_text": "We suggest that the prompt emission of gamma - ray waves ( GRBs ) is due to large - energy protons , muons and electron - positron couples created by ultra - relativistic shocks in GRB jets . The observed MeV - GeV spectrum can be understood as synchrotron rays generated by these ions advancing at the shock back .We see that this model readily explains why the maximum energy of the seen spectrum drops with time during the prompt phase . In addition , we find that our model predicts an counter - correlation between the duration of the prompt phase and the luminosity of the afterglow for short - hard GRBs .This prediction might be evaluated using later observed made by Fermi / LAT and Swift / BAT . Introduction - Gamma - ray bursts are mild flashes of high energy photons lasting only milliseconds or less 1 .They have been detected out to redshifts z = 8 . 2 2 , which implies their total energy efficiency may exceed 10 ^ 53 erg 3 . Despite decades of research into the origin of GRBs there stands no discussion on how they use 4 .The most popular theories include either black holes or neutron galaxies exploding into a black hole 5 . However , it has recently become clear that several GRBs do not fit nicely into one category 6 .For instance , some GRBs occur to contain two separate pulses 7 , 8 while several display extended phases of activity 9 . Furthermore , some GRBs appears to arise when two galaxies merge 10 .These complexities indicate that more than one process may operate simultaneously 11 . In recent years many writers 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 have proposed that GRBs are powered by relativistic jets e",
        "rewrite_text": "We propose that the prompt emission of gamma-ray bursts (GRBs) is a result of high-energy protons, muons, and electron-positron pairs generated by ultra-relativistic shocks within GRB jets. The MeV-GeV spectrum observed can be interpreted as synchrotron radiation produced by these particles moving back from the shock front. Our model effectively accounts for the observed decline in maximum energy of the spectrum during the prompt phase over time. Furthermore, we predict a counter-correlation between the duration of the prompt phase and the luminosity of the afterglow in short-hard GRBs, which could be tested with observations from Fermi/LAT and Swift/BAT. \n\nIntroduction - Gamma-ray bursts are brief emissions of high-energy photons lasting mere milliseconds or even less. They have been detected at redshifts of z = 8.2, suggesting their total energy efficiency could surpass 10^53 erg. Despite extensive research into the origins of GRBs, there remains uncertainty regarding their mechanisms. The leading theories include scenarios involving black holes or neutron stars collapsing into black holes. However, it has recently become evident that many GRBs do not conform neatly to a single classification. For example, some GRBs feature two distinct pulses, while others exhibit prolonged periods of activity. Additionally, certain GRBs appear to originate from the merger of two galaxies. These complexities suggest that multiple mechanisms may be at play simultaneously. In recent years, numerous researchers have proposed that GRBs are fueled by relativistic jets.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.184271554937297,
        "rewrite-fast-z-score": 0.8466487815452375
    },
    {
        "original_text": "La gravite est une force fondamentale qui agit sur tous les corps materiels, et dont l action se manifeste par l attraction entre ces derniers. La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un point unique au centre du systeme solaire. Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la notion d espace-temps courbe. Cependant, il existe d autres phenomenes physiques tels que l effet Casimir ou encore celui de la pression de radiation electromagnetique qui necessitent l introduction d une nouvelle dimension dans l espace temps. Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique. Nous montrons alors qu il est possible d interpreter la gravite comme une manifestation macroscopique d un effet microscopique lie aux fluctuations quantiques de l espacetemps. Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant la dynamique des astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present.",
        "watermark_text": "La gravite est le power fondamentale qui agit à tous les corps materiels , et dont l action se manifeste par l attraction entre ces derniers . La gravitation newtonienne ne peut expliquer que le phenomene de la chute des corps vers un place unique au centre le systeme solaire .Les theories relativistes d Einstein ont permis d expliquer ce phenomene en introduisant la idea d espace - temps courbe . Cependant , el existe d autres phenomenes physiques tels se l effet Casimir ou encore celui de la pression de energy electromagnetique qui necessitent l introduction d la nouvelle depth à l espace temps .Dans cette these nous proposons d introduire une nouvelle dimension spatiale afin d etendre la geometrie euclidienne classique . Nous montrons alors qu il est possible d interpreter la gravite comme le manifestation macroscopique d un effet microscopique lie à fluctuations quantiques de l espacetemps .Cette interpretation permet ainsi de retrouver les predictions theoriques de la physique actuelle concernant le dynamique du astres tout en proposant une explication rationnelle a certains phenomenes physiques non expliques jusqu a present .",
        "rewrite_text": "La gravité est la force fondamentale qui agit sur tous les corps matériels, se manifestant par l'attraction entre eux. La gravitation newtonienne ne peut expliquer que le phénomène de la chute des corps vers un point unique situé au centre du système solaire. Les théories relativistes d'Einstein ont permis de mieux comprendre ce phénomène en introduisant le concept d'espace-temps courbe. Toutefois, d'autres phénomènes physiques, tels que l'effet Casimir et la pression de l'énergie électromagnétique, requièrent l'introduction d'une nouvelle dimension à l'espace-temps. Dans cette thèse, nous proposons d'ajouter une nouvelle dimension spatiale afin d'étendre la géométrie euclidienne classique. Nous démontrons qu'il est possible d'interpréter la gravité comme une manifestation macroscopique d'un effet microscopique lié aux fluctuations quantiques de l'espace-temps. Cette interprétation permet ainsi de retrouver les prédictions théoriques de la physique moderne concernant la dynamique des astres tout en offrant une explication rationnelle à certains phénomènes physiques encore inexpliqués à ce jour.",
        "ori-fast-z-score": -0.5443310539518174,
        "water-fast-z-score": 3.328201177351375,
        "rewrite-fast-z-score": -0.29488391230979427
    },
    {
        "original_text": "We present new characterizations for multivariable functions in the Schur class, which are defined as those analytic on the open unit disk that satisfy certain growth conditions at infinity.  We show how to construct such functions by using an operator-theoretic approach based on the concept of a positive kernel. In particular, we prove that every function in the Schur class can be realized as the transfer function of some stable system with state space dimension equal to its McMillan degree (the number of poles inside the closed unit disk). This result is obtained via a generalization of the classical theorem due to Kalman-Yakubovich-Popov. Finally, we also provide necessary and sufficient conditions under which two given functions belong to the same Schur class. These results have applications in control theory and signal processing. The research leading to these results has received funding from the European Research Council under the Seventh Framework Programme (FP7/2007-2013) / ERC Grant Agreement n",
        "watermark_text": "We introduce novel characterizations for multivariable functions in the Schur class , which are specified as those analytic on the open unit disk that fulfill certain growth conditions at infinity . We see how to build such functions by using an operator - theoretic approach based on the idea of a positive kernel .In particular , we prove that every function in the Schur class can be realized as the transfer function of some stable system with state space dimension equal to its McMillan degree ( the number of poles inside the shut unit disk ) . This result is achieved via a generalization of the classical principle attributed to Kalman - Yakubovich - Popov .Finally , we also obtain necessary and sufficient situations under which two given functions belong to the same Schur class . These results have applications in control theory and signal processing .The studies leading to these results has obtained support from the European Research Council under the Seventh Framework Programme ( FP7 / 2007 - 2013 ) / ERC Grant Agreement n",
        "rewrite_text": "We present new characterizations for multivariable functions within the Schur class, defined as those analytic in the open unit disk that satisfy specific growth conditions at infinity. We demonstrate how to construct such functions using an operator-theoretic approach centered around the concept of a positive kernel. Notably, we establish that every function in the Schur class can be expressed as the transfer function of a stable system, with the state space dimension corresponding to its McMillan degree (the number of poles located within the closed unit disk). This finding is derived from a generalization of the well-known Kalman-Yakubovich-Popov principle. Additionally, we identify the necessary and sufficient conditions for two functions to belong to the same Schur class. These findings have relevance in control theory and signal processing. The research leading to these conclusions has received support from the European Research Council under the Seventh Framework Programme (FP7/2007-2013) through ERC Grant Agreement number.",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": -0.12403473458920847
    },
    {
        "original_text": "We present new high-resolution (0.5 arcsec) radio continuum observations at 1.4 GHz and 4.8 GHz made with the Australia Telescope Compact Array (ATCA). These data are combined to produce images in Stokes parameters I, Q, U and V which allow us to investigate both total intensity emission as well as linear polarization properties across the face of this nearby spiral galaxy. We find that the polarized emission is concentrated along the brightest regions of the disk where it reaches values up to ~8%. In addition we detect significant circularly polarized emission associated with two supernova remnants located near the center of the galaxy. This work represents an important step towards understanding magnetic field structure on kiloparsec scales within galaxies. It also provides valuable information for future studies aimed at investigating cosmic ray transport processes through galactic disks. \n \n Keywords: Radio astronomy, Galaxy evolution, Magnetic fields, Polarization",
        "watermark_text": "We report new high - resolution ( 0 . 5 arcsec ) radio continuum measurements at 1 . 4 GHz and 4 . 8 GHz made with the Australia Telescope Compact Array ( ATCA ) . These data are coupled to produce images in Stokes variables I , Q , U and V which allow us to examine both total frequency emission as well as linear polarization properties across the face of this adjacent spiral galaxy .We see that the polarized emission is confined along the brightest parts of the disk where it hits values up to ~ 8 % . In addition we find considerable circularly polarized emission associated with two supernova remnants situated near the center of the galaxy .This research constitutes an important milestone towards studying magnetic force composition on kiloparsec scales within stars . It additionally offers important information for future research targeted at studying cosmic ray flow processes through galactic disks .Keywords : Radio astronomy , Galaxy evolution , Magnetic fields , Polarization",
        "rewrite_text": "We present new high-resolution (0.5 arcsec) radio continuum measurements at frequencies of 1.4 GHz and 4.8 GHz, conducted using the Australia Telescope Compact Array (ATCA). By combining these data, we generate images in Stokes parameters I, Q, U, and V, enabling us to analyze both the overall frequency emission and the linear polarization characteristics across this nearby spiral galaxy. Our observations reveal that the polarized emission is concentrated in the galaxy's brightest disk regions, reaching levels of approximately 8%. Moreover, we detect significant circularly polarized emission linked to two supernova remnants located near the galaxy's center. This study marks a significant advancement in understanding the magnetic field structure on kiloparsec scales within stars and provides valuable insights for future investigations into cosmic ray flow dynamics through galactic disks. Keywords: Radio astronomy, galaxy evolution, magnetic fields, polarization.",
        "ori-fast-z-score": -1.5882027766319677,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "We present the results of an analysis of the clustering properties of luminous red galaxies (LRGs) in the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). We use a sample of 380,000 LRGs selected to have 0.4 < zphot < 1.0 and Mr < --21.5 + 5logh. The angular correlation function is measured for this sample using the Landy & Szalay estimator on scales between 10  and 100 . To account for redshift space distortions we measure the projected cross-correlation functions wp(rp), where rp = Dproj/ H(z)/H0 , H(z) is the Hubble parameter at redshift z, and H0 is its value today. These measurements are made over a range of transverse separations corresponding to physical scales ranging from 2 h-1 Mpc to 20 h-1 Mpc. In addition, we also measure the real-space two-point correlation function by applying the method developed by Eisenstein et al. (2007) . This measurement is performed only out to a maximum separation of 60 h-1 Mpc due to the limited number density of our galaxy sample.",
        "watermark_text": "We publish the conclusion of an assessment of the clustering behavior of luminous red objects ( LRGs ) in the Sloan Digital Sky Survey Data Release 7 ( SDSS DR7 ) . We use a sample of 380 , 000 LRGs chosen to have 0 . 4 < zphot < 1 . 0 and Mr < - - 21 . 5 + 5logh .The angular correlation function is measured for this specimen using the Landy & Szalay estimator on scales between 10 and 100 . To account for redshift space distortions we measure the projected cross - correlation functions wp ( rp ) , where rp = Dproj / H ( z ) / H0 , H ( z ) is the Hubble parameter at redshift z , and H0 is its value today .These measurements are produced over a range of transverse separations corresponding to physical scales ranging from 2 h - 1 Mpc to 20 g - 1 Mpc . In addition , we also measure the real - space two - point coupling function by using the method developed by Eisenstein et al .( 2007 ) . This measurement is conducted only out to a maximum separation of 60 h - 1 Mpc owing to the limited number density of our galaxy sample .",
        "rewrite_text": "We present the results of an assessment investigating the clustering behavior of luminous red objects (LRGs) within the Sloan Digital Sky Survey Data Release 7 (SDSS DR7). Our study utilizes a sample of 380,000 LRGs selected with redshift constraints of 0.4 < zphot < 1.0 and an absolute magnitude threshold of Mr < -21.5 + 5logh. We calculate the angular correlation function for this dataset using the Landy & Szalay estimator over angular scales ranging from 10 to 100. To account for redshift space distortions, we analyze the projected cross-correlation functions wp(rp), where rp = Dproj / H(z) / H0, with H(z) representing the Hubble parameter at redshift z and H0 its value today. Our measurements cover a range of transverse separations, corresponding to physical scales from 2 h^-1 Mpc to 20 h^-1 Mpc. Additionally, we measure the real-space two-point correlation function using the method established by Eisenstein et al. (2007), although this measurement is limited to a maximum separation of 60 h^-1 Mpc due to the sparse density of our galaxy sample.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.302003302004953,
        "rewrite-fast-z-score": 0.48507125007266594
    },
    {
        "original_text": "We present new Hubble Space Telescope (HST) Advanced Camera for Survey (ACS)\ncoronagraphic observations in visible light and near-infrared wavelengths that reveal an extended dusty disk surrounding the Herbig Ae star HD 100546, which is known to harbor a protoplanetary disk with spiral arms. The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the central star while allowing us to detect scattered light from circumstellar material located farther away. We find evidence for two bright rings of emission separated by ~0.5′′ along the major axis of the disk. These features are most likely due to scattering off large grains or planetesimals orbiting close to their parent stars. \n \n In addition, we have detected several dark gaps within these bright rings as well as fainter structures extending outward into the outer regions of the disk. Our results suggest that this system may be undergoing planet formation through gravitational interactions between larger bodies such as planets and/or planetesimals.",
        "watermark_text": "We present new Hubble Space Telescope ( HST ) Advanced Camera for Survey ( ACS ) coronagraphic observations in bright light and near - infrared wavelengths that discover an extended dusty disk surrounding the Herbig Ae star HD 100546 , which is known to harbor a protoplanetary disk with spiral arms . The ACS coronagraph was used to block out direct stellar radiation at small angular separations from the main star while providing us to identify scattered radiation from circumstellar material located farther distant .We see evidence for two faint rings of emission separated by ~ 0 . 5 ′ ′ along the main axis of the disk . These features are most likely due to scattering off large grains or planetesimals orbiting close to their father stars .In addition , we have discovered numerous dark gaps within these dark rings as well as fainter objects extending outward into the inner regions of the disk . Our results propose that this system might be experiencing planet development through gravity interactions between big bodies such as planets and / or planetesimals .",
        "rewrite_text": "We present new coronagraphic observations from the Advanced Camera for Surveys (ACS) on the Hubble Space Telescope (HST) in bright light and near-infrared wavelengths, revealing an extended dusty disk around the Herbig Ae star HD 100546, which is already known to possess a protoplanetary disk with spiral arms. The ACS coronagraph effectively blocked direct stellar light at small angular separations from the star, allowing us to detect scattered light from circumstellar material located at greater distances. Our observations indicate the presence of two faint emission rings, approximately 0.5\" apart, aligned along the main axis of the disk. These features are likely due to scattering from large grains or planetesimals in close orbit around the star. Furthermore, we noted several dark gaps within these rings, as well as fainter objects extending into the inner regions of the disk. Our findings suggest that this system may be undergoing planet formation driven by gravitational interactions between large bodies, such as planets and/or planetesimals.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 0.6793662204867574
    },
    {
        "original_text": "We present deep imaging data for the nearby dwarf spheroidal galaxy, Hercules (dSph), obtained with the Large Binocular Telescope (LBT). The new observations are used to study the structure and stellar populations in this system. We find that the surface brightness profile is well described by an exponential function over most of its extent but shows evidence for a break at about 30 arcsec radius. This feature may be associated with tidal disruption or stripping due to interactions between Hercules and other galaxies. Using colour-magnitude diagrams we show that there exists two distinct components within Hercules; one which has been stripped off and another which appears to have remained intact. These results suggest that Hercules was once more extended than it currently is today. Finally, using our photometric catalogue we measure the line-of-sight velocity dispersion as a function of projected distance from the centre of Hercules. Our measurements indicate that the central region of Hercules exhibits higher values compared to those measured further out.",
        "watermark_text": "We present deep imaging information for the nearby dwarf spheroidal galaxy , Hercules ( dSph ) , obtained with the Large Binocular Telescope ( LBT ) . The newest observations are using to study the composition and stellar environments in this system .We see that the surface brightness profile is well described by an exponential function over most of its extent but shows proof for a break at about 30 arcsec radius . This phenomenon might be involved with tidal disruption or stripping due to interactions between Hercules and other stars .Using colour - magnitude diagrams we find that there exists two separate phases within Hercules ; one which has been stripped off and another which appears to have remained intact . These data suggest that Hercules was once more extended than it currently is presently .Finally , using our photometric catalogue we measure the line - of - view velocity dispersion as a function of projected distance from the centre of Hercules . Our measurements indicate that the main region of Hercules exhibits higher values compared to those observed further out .",
        "rewrite_text": "We present detailed imaging data for the nearby dwarf spheroidal galaxy Hercules (dSph), gathered with the Large Binocular Telescope (LBT). These recent observations are used to examine the galaxy's composition and stellar environment. Our analysis shows that the surface brightness profile can be accurately described by an exponential function across most of its area, but exhibits a noticeable break at approximately 30 arcseconds from the center. This break may be associated with tidal disruption or stripping resulting from interactions between Hercules and other stars. Through the use of color-magnitude diagrams, we identify two distinct phases within Hercules: one that has been stripped away and another that appears to be intact. This data implies that Hercules was once more expansive than it is now. Lastly, utilizing our photometric catalog, we measure the line-of-sight velocity dispersion as a function of the projected distance from the center of Hercules, finding that the central region of the galaxy shows higher dispersion values compared to the outer areas.",
        "ori-fast-z-score": -0.601929265428846,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The first stars in the universe were born out of primordial gas clouds, which collapsed under their own gravity to form dense cores that ignited nuclear fusion and became hot white dwarfs.  The most massive of these early stars are now known as Population III (PopIII) stars.   In this work we present results for PopIII star formation using cosmological hydrodynamic simulations with radiative transfer calculations performed on an adaptive mesh refinement grid.  We find that PopIII stars can be formed by direct collapse of metal-free gas clouds at redshifts z > 20.  These stars have masses M * = 100 - 1000M⊙ and lifetimes t < 10Myr.  They evolve into black holes or pair-instability supernovae after consuming all available fuel within their convective envelopes.  Our results show that PopIII stars may contribute significantly to reionization of the intergalactic medium around redshift z ~ 15.",
        "watermark_text": "The first stars in the universe were born out of primordial liquid clouds , which collapsed under their own gravity to form dense cores that sparked nuclear fusion and became hot white dwarfs . The most large of these first stars are now referred as Population III ( PopIII ) stars .In this research we present results for PopIII star formation combining cosmological hydrodynamic simulations with radiative transfer calculations performed on an automated mesh refinement grid . We see that PopIII stars can be formed by direct collapse of metal - loose gas clouds at redshifts z > 20 .These stars have masses M * = 100 - [UNK] and lifetimes t < 10Myr . They evolve into black holes or pair - instability supernovae after consuming all available fuel within their convective envelopes .Our results show that PopIII stars would contribute greatly to reionization of the intergalactic medium around redshift z ~ 15 .",
        "rewrite_text": "The universe's first stars emerged from primordial liquid clouds that collapsed under their own gravity, forming dense cores that ignited nuclear fusion and became hot white dwarfs. The largest of these initial stars are classified as Population III (PopIII) stars. In this study, we present findings on PopIII star formation by integrating cosmological hydrodynamic simulations with radiative transfer calculations executed on an automated mesh refinement grid. Our observations indicate that PopIII stars can form through the direct collapse of metal-poor gas clouds at redshifts greater than z > 20. These stars possess masses ranging from M* = 100 to [UNK] and have lifetimes of less than 10 million years. After exhausting all available fuel within their convective envelopes, they either evolve into black holes or undergo pair-instability supernovae. Our results suggest that PopIII stars played a significant role in the reionization of the intergalactic medium around redshift z ~ 15.",
        "ori-fast-z-score": 1.6644794391276478,
        "water-fast-z-score": 4.993438317382943,
        "rewrite-fast-z-score": 2.287331208629615
    },
    {
        "original_text": "The Spitzer Bright Field (SBF) is an all-sky survey at 3.6, 4.5, 5.8 & 8 microns carried out by the Infrared Array Camera on board the Spitzer Space Telescope.  The SBF was designed to provide deep infrared photometry for extragalactic studies in order to complement existing optical surveys such as the Sloan Digital Sky Survey.   This dataset contains images taken with IRAC channel 1 (3.6 micron), channel 2 (4.5 micron), channel 3 (5.8 micron) and channel 4 (8 micron). Each image has been processed using the MOPEX software package developed by the Spitzer Science Center. These images are available through the NASA/IPAC Extragalactic Database (NED).  For more information about this project please see http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html",
        "watermark_text": "The Spitzer Bright Field ( SBF ) is an all - skies study at 3 . 6 , 4 . 5 , 5 . 8 & 8 microns taken out by the Infrared Array Camera on board the Spitzer Space Telescope . The SBF was built to provide deep infrared photometry for extragalactic studies in order to complement existing imaging observations such as the Sloan Digital Sky Survey .This dataset contains images took with IRAC channel 1 ( 3 . 6 micron ) , channel 2 ( 4 . 5 micron ) , channel 3 ( 5 . 8 micron ) and channel 4 ( 8 micron ) . Each image has been processed utilizing the MOPEX software suite created by the Spitzer Science Center .These photographs are available through the NASA / IPAC Extragalactic Database ( NED ) . For more information about this project please view http : / / irsa . ipac . caltech . edu / data / SPITZER / docs / sbf / sbf - intro . html",
        "rewrite_text": "The Spitzer Bright Field (SBF) is a comprehensive all-skies survey conducted at wavelengths of 3.6, 4.5, 5.8, and 8 microns using the Infrared Array Camera (IRAC) onboard the Spitzer Space Telescope. Designed to enhance deep infrared photometry for extragalactic research, the SBF aims to complement existing imaging datasets, such as those from the Sloan Digital Sky Survey. This dataset includes images captured with IRAC's four channels: channel 1 (3.6 microns), channel 2 (4.5 microns), channel 3 (5.8 microns), and channel 4 (8 microns). Each image has undergone processing with the MOPEX software suite developed by the Spitzer Science Center. These images can be accessed through the NASA/IPAC Extragalactic Database (NED). For further details about the project, please visit http://irsa.ipac.caltech.edu/data/SPITZER/docs/sbf/sbf-intro.html.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 4.700096710803842,
        "rewrite-fast-z-score": 1.1832159566199232
    },
    {
        "original_text": "We present new observations at 1.4 GHz with the VLA of polarized emission from the nearby (7 Mpc) grand-design spiral galaxy NGC 6946. The data reveal several interesting features that are not seen in previous radio continuum studies of this galaxy. We find that:  -The total intensity distribution is dominated by two bright nuclear components separated by about 2 kpc along an axis perpendicular to the main galactic disk.  -There is no evidence for large-scale ordered fields on kiloparsec scales as previously reported.   -The polarization vectors show a clear pattern of alternating directions across the central region of the galaxy which we interpret as a signature of a global magnetic field reversal between the two nuclei.  -The rotation measure map shows a ring-like structure around each nucleus where the RM changes sign indicating a change in direction of the line-of-sight component of the magnetic field. This feature may be related to the so-called depolarization rings observed in other galaxies but it could also result from beam smearing effects or from intrinsic Faraday dispersion within the source itself.  -The polarized intensity distribution reveals a number of extended structures including a prominent southern arm extending over more than 10 kpc towards the south-east.",
        "watermark_text": "We present new images at 1 . 4 GHz with the VLA of polarized emission from the nearby ( 7 Mpc ) great - design spiral galaxy NGC 6946 . The data reveal numerous interesting features that are not seen in earlier radio continuum experiments of this galaxy .We see that : - The total magnitude distribution is dominated by two faint nuclear elements divided by about 2 kpc along an axis adjacent to the main galactic disk . - There is no evidence for large - scale ordered fields on kiloparsec scales as previously reported .- The polarization vectors display a clear sequence of alternating directions across the central region of the galaxy which we perceive as a signature of a global magnetic force reversal between the two nuclei . - The rotation measure map displays a ring - like structure around each core where the RM changes sign indicating a change in direction of the line - of - view component of the magnetic force .This characteristic could be connected to the so - called depolarization belts detected in other stars but it could also occur from light smearing effects or from intrinsic Faraday dispersion within the source itself . - The polarized intensity distribution reveals a number of extended features including a major southern arm reaching over more than 10 kpc towards the south - west .",
        "rewrite_text": "We present new 1.4 GHz images from the VLA showcasing polarized emission from the nearby spiral galaxy NGC 6946, located 7 Mpc away. Our data unveil several intriguing features that were not apparent in previous radio continuum studies of this galaxy. Notably: - The overall magnitude distribution is primarily influenced by two faint nuclear components situated approximately 2 kpc apart along an axis parallel to the main galactic disk. - Unlike earlier studies, we find no evidence of large-scale ordered magnetic fields on kiloparsec scales. - The polarization vectors show a distinct pattern of alternating directions across the galaxy's central region, which we interpret as a signature of a global magnetic field reversal between the two nuclei. - The rotation measure map reveals a ring-like structure around each nucleus, where the RM indicates a change in the direction of the line-of-sight component of the magnetic field. This feature may be related to so-called depolarization belts observed in other stellar sources, though it might also result from light smearing effects or intrinsic Faraday dispersion within the source itself. - Additionally, the polarized intensity distribution highlights several extended features, including a prominent southern arm that extends over 10 kpc towards the southwest.",
        "ori-fast-z-score": 0.6,
        "water-fast-z-score": 6.8,
        "rewrite-fast-z-score": 0.19802950859533489
    },
    {
        "original_text": "We present an improved quantum hard-sphere ground-state equation-of-state (EOS) for the description of dense matter in astrophysics and nuclear physics, which is based on the exact solution to the Schrödinger equation with a repulsive delta-function potential. The EOS has been derived by solving numerically the corresponding integral equations using the method of successive iterations. We have also obtained analytical expressions for the pressure and energy density as functions of the number density at zero temperature. Our results are compared with those calculated previously within various approximations such as the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. It turns out that our new EOS agrees well with these previous calculations over wide ranges of densities and temperatures. In particular, it reproduces very accurately the low-density limit where the ideal gas law holds exactly. \n \n Keywords: Equation of state",
        "watermark_text": "We introduce an efficient quantum hard - sphere ground - state equation - of - state ( EOS ) for the description of dense materials in astrophysics and nuclear science , which is based on the exact solution to the Schrödinger equation with a repulsive delta - function potential . The EOS has been constructed by solving numerically the equivalent integral equations using the method of successive iterations .We have already established analytical expressions for the pressure and energy density as functions of the number density at zero temperature . Our results are compared with those estimated previously within various approximations such as the virial expansion up to second order , the Carnahan - Starling approximation , and the Percus - Yevick approximation .It turns out that our new EOS agrees well with these previous calculations over broad ranges of densities and temperatures . In particular , it reproduces very correctly the small - density maximum where the ideal gas theory holds exactly .Keywords: Equation of state",
        "rewrite_text": "We present an efficient quantum hard-sphere equation of state (EOS) for modeling dense materials relevant to astrophysics and nuclear science. This EOS is derived from the exact solution of the Schrödinger equation with a repulsive delta-function potential. We constructed the EOS by numerically solving the corresponding integral equations using a method of successive iterations. We have already derived analytical expressions for pressure and energy density as functions of number density at zero temperature. Our findings are compared with previous estimates obtained through various approximations, including the virial expansion up to second order, the Carnahan-Starling approximation, and the Percus-Yevick approximation. Notably, our new EOS aligns well with these earlier calculations across a wide range of densities and temperatures, accurately capturing the small-density maximum where ideal gas theory is precise. Keywords: Equation of state.",
        "ori-fast-z-score": 0.8427009716003844,
        "water-fast-z-score": 3.731961445658845,
        "rewrite-fast-z-score": 2.6678918753996625
    },
    {
        "original_text": "The aim of this book is to provide an overview on the state-of-the-art in on-line condition monitoring and fault diagnosis for industrial processes, with special emphasis on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS) or evolutionary algorithms (EAs). The book covers both theoretical aspects and practical applications of these methods. It also discusses some recent advances in intelligent sensor technologies that are essential for successful implementation of on-line condition monitoring schemes. This book will be useful not only for researchers but also for engineers who want to apply computational intelligence techniques into their own research work. Contents include:  Chapter 1: Introduction to On-line Condition Monitoring.  Chapter 2: Intelligent Sensors for On-line Condition Monitoring.  Chapters 3-7: Neural Networks for Fault Diagnosis.  Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.   Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.   ...",
        "watermark_text": "The goal of this paper is to provide an overview on the state - of - the - art in on - line condition monitoring and failure detection for industrial systems , with special emphasis on computational intelligence techniques such as neural systems ( NNs ) , fuzzy logic systems ( FLS ) or ecological computers ( EAs ) . The text encompasses both theoretical components and useful users of these algorithms .It additionally outlines some latest advances in intelligent detection systems that are essential for successful implementation of on - line condition monitoring schemes . This book will be valuable not only for researchers but also for designers who desire to apply computational intelligence techniques into their own research effort .Contents comprise : Chapter 1 : Introduction to On - line Condition Monitoring . Chapter 2 : Intelligent Sensors for On - line Condition Monitoring .Chapters 3-7: Neural Networks for Fault Diagnosis.Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis.Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis....",
        "rewrite_text": "This paper aims to provide a comprehensive overview of current advancements in online condition monitoring and failure detection for industrial systems, focusing particularly on computational intelligence techniques such as neural networks (NNs), fuzzy logic systems (FLS), and evolutionary algorithms (EAs). It includes both theoretical insights and practical applications of these algorithms, along with a discussion of the latest developments in intelligent detection systems crucial for the effective implementation of online condition monitoring strategies. This work will serve as a valuable resource for researchers and designers interested in integrating computational intelligence techniques into their research. The contents include: Chapter 1: Introduction to Online Condition Monitoring; Chapter 2: Intelligent Sensors for Online Condition Monitoring; Chapters 3-7: Neural Networks for Fault Diagnosis; Chapters 8-10: Fuzzy Logic Systems for Fault Diagnosis; Chapters 11-13: Evolutionary Algorithms for Fault Diagnosis.",
        "ori-fast-z-score": 0.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 2.6887744785908154
    },
    {
        "original_text": "We present an analysis of all known short-period cataclysmic variables (CVs) in which we find that most systems have orbital periods longer than 3 hrs and are dominated by SW Sex stars, while those with shorter periods tend to be AM Her binaries. We show that this dichotomy is consistent with theoretical predictions for the evolution of CVs driven by angular momentum loss via gravitational radiation. The observed distribution of orbital periods can also be explained if there exists a minimum period below which no CVs exist due to magnetic braking. This result has important implications on our understanding of how CVs evolve towards shorter orbital periods. Cataclysmic Variables (CVs), interacting binary star systems consisting of a white dwarf primary accreting matter from its low-mass companion through Roche lobe overflow, are among the best studied classes of close binary stars. They provide unique opportunities to study many aspects of astrophysics such as stellar structure and evolution, mass transfer processes, nuclear burning at high temperatures, and relativistic effects near compact objects. In particular, they offer insights into the formation mechanisms of both single and double degenerate white dwarfs, the progenitors of Type Ia supernovae.",
        "watermark_text": "We present an assessment of all known short - period cataclysmic variables ( CVs ) in which we find that most components have orbital periods longer than 3 hrs and are dominated by SW Sex stars , while those with shorter cycles seem to be AM Her binaries . We suggest that this dichotomy is compatible with theoretical estimates for the evolution of CVs caused by angular velocity loss via gravitational rays .The observed pattern of orbital periods can also be described if there exists a minimum period below which no CVs occur due to magnetic braking . This result has crucial consequences on our understanding of how CVs develop towards shorter orbital periods .Cataclysmic Variables ( CVs ) , interacting binary star systems composed of a white dwarf secondary accreting matter from its low - weight sister through Roche lobe overflow , are among the best researched groups of close binary stars . They offer distinct options to study many aspects of astrophysics such as stellar formation and evolution , mass transfer mechanisms , nuclear burning at high temperatures , and relativistic effects near compact galaxies .In particular , they give insights into the formation patterns of both single and double degenerate white dwarfs , the progenitors of Type Ia supernovae .",
        "rewrite_text": "We provide an evaluation of all known short-period cataclysmic variables (CVs) and find that the majority exhibit orbital periods exceeding 3 hours, with SW Sex stars being the most common type. In contrast, those with shorter periods appear to be AM Her binaries. We propose that this distinction aligns with theoretical predictions regarding the evolution of CVs influenced by angular momentum loss through gravitational radiation. Furthermore, the observed distribution of orbital periods could be explained by the existence of a minimum period below which no CVs are detected due to magnetic braking. This finding has significant implications for our understanding of the evolution of CVs towards shorter orbital periods. Cataclysmic variables, which are binary star systems comprising a white dwarf that accretes material from a companion star through Roche lobe overflow, represent one of the most extensively studied groups of close binary stars. They provide valuable opportunities to explore numerous astrophysical phenomena, including stellar formation and evolution, mass transfer processes, high-temperature nuclear fusion, and relativistic effects near compact objects. Notably, they also shed light on the formation mechanisms of both single and double degenerate white dwarfs, which are the progenitors of Type Ia supernovae.",
        "ori-fast-z-score": 0.618852747755276,
        "water-fast-z-score": 5.982243228301002,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We have studied the origin of the 60 K plateau in YBa2Cu3Ox by measuring the specific heat and magnetic susceptibility as functions of temperature, field strength, oxygen content x, and doping level p. The results show that the 60 K anomaly is due to an antiferromagnetic transition at TAF = 56 K for x= 0.6 but disappears with increasing x or decreasing p. We also find that the low-temperature upturns observed in both C/T and χ(T) are caused by impurity phases which appear when x > 0.65.  These results suggest that the 60 K anomality may be related to the appearance of these impurities. In addition we found that the high-field magnetization shows a sharp increase below 50 K indicating that there exists another phase transition near this temperature. This new phase has been identified as a charge density wave state (CDW). Finally, our data indicate that the CDW state appears only if the sample contains some amount of impurities.",
        "watermark_text": "We have researched the origin of the 60 K plateau in YBa2Cu3Ox by test the specific heat and magnetic susceptibility as functions of temperature , field intensity , hydrogen content x , and doping rate r . The results show that the 60 K anomaly is due to an antiferromagnetic shift at TAF = 56 K for x = 0 . 6 but fade with rising x or decreasing p . We additionally find that the small - temperature upturns observed in both C / T and χ ( T ) are created by impurity phases which appear when x > 0 . 65 . These conclusions show that the 60 K anomality may be connected to the appearance of these impurities .In addition we learned that the high - field magnetization shows a sharp increase below 50 K indicating that there exists another phase shift near this heat . This new phase has been described as a charge density wave state ( CDW ) .Finally , our statistics indicate that the CDW state emerges only if the sample comprises some number of impurities .",
        "rewrite_text": "We investigated the origin of the 60 K plateau in YBa2Cu3Ox by examining specific heat and magnetic susceptibility as functions of temperature, magnetic field intensity, hydrogen content (x), and doping rate (r). Our results indicate that the 60 K anomaly is linked to an antiferromagnetic transition occurring at TAF = 56 K for x = 0.6, but this effect diminishes with increasing x or decreasing p. Furthermore, we discovered that the small temperature increases observed in both C/T and χ(T) are attributed to impurity phases that emerge when x exceeds 0.65. These findings suggest a connection between the 60 K anomaly and the presence of these impurities. Additionally, we observed a sharp increase in high-field magnetization below 50 K, indicating another phase transition near this temperature, which we have identified as a charge density wave (CDW) state. Finally, our analysis suggests that the formation of the CDW state requires a certain level of impurities in the sample.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": 0.75
    },
    {
        "original_text": "We test whether the observed value of lambda is consistent with the prediction that it should be equal to one third of the square root of the number density of galaxies in the universe today, as suggested by Tegmark et al. (2006) . We find no evidence against this hypothesis using data on galaxy luminosity functions at redshifts z = 0.1, 1.0 and 3.5 taken from the Sloan Digital Sky Survey (SDSS). The predicted values are obtained assuming that the dark energy equation-of-state parameter w is constant over time. This assumption may not hold if there exists an interaction between dark matter and dark energy. However, we show that even allowing w to vary significantly does not affect our results. \n \n In addition, we use the WMAP 5-year cosmological parameters to calculate the expected temperature anisotropy power spectrum of the cosmic microwave background radiation (CMB) given the current best-fit model. We compare these theoretical predictions with measurements made by the Wilkinson Microwave Anisotropy Probe (WMAP), finding good agreement across all multipole moments up to lmax = 1000.",
        "watermark_text": "We test whether the known value of lambda is compatible with the observation that it should be equal to one third of the square root of the number density of galaxies in the universe today , as suggested by Tegmark et al . ( 2006 ) .We see no evidence against this hypothesis utilizing information on star luminosity functions at redshifts z = 0 . 1 , 1 . 0 and 3 . 5 taken from the Sloan Digital Sky Survey ( SDSS ) . The predicted readings are derived assuming that the dark energy equation - of - state variable W is constant over time .This assumption must not hold if there exists an interaction between dark matter and dark energy . However , we find that even allowing W to vary significantly does not alter our findings .In addition , we utilize the WMAP 5 - day cosmological values to estimate the expected thermal anisotropy energy spectrum of the cosmic microwave background radiation ( CMB ) given the latest best - fitting model . We match these theoretical estimates with observations made by the Wilkinson Microwave Anisotropy Probe ( WMAP ) , finding high agreement across all multipole minutes up to lmax = 1000 .",
        "rewrite_text": "We examine whether the established value of lambda is consistent with the idea that it should equal one third of the square root of the current galaxy number density in the universe, as proposed by Tegmark et al. (2006). Our analysis, based on star luminosity functions at redshifts z = 0.1, 1.0, and 3.5 from the Sloan Digital Sky Survey (SDSS), reveals no evidence contradicting this hypothesis. The predicted values are based on the assumption that the dark energy equation-of-state parameter W remains constant over time. This assumption may not hold if there is an interaction between dark matter and dark energy. However, our findings indicate that even with significant variations in W, the results remain unchanged. Furthermore, we use the WMAP 5-day cosmological values to calculate the expected thermal anisotropy energy spectrum of the cosmic microwave background radiation (CMB) based on the latest best-fitting model. When we compare these theoretical predictions with observations from the Wilkinson Microwave Anisotropy Probe (WMAP), we observe a strong agreement across all multipole moments up to \\( l_{max} = 1000 \\).",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 6.340751391209736,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "We propose that the dark matter in our universe is composed of milli-charged particles, which are neutral under electromagnetism but carry an electric charge on the order of 10^(-6)e (electrons). We show how this scenario can be realized within the context of the Standard Model by introducing a new gauge boson with mass mX ~ 1TeV/c2 into the theory through the Stueckelberg extension to the Standard Model. The introduction of such a massive vector particle leads to modifications to the usual Feynman rules for charged fermions interacting via photons or gluons. In particular, we find that the cross section for scattering between two milli-charged particles mediated by a photon is suppressed compared to the case where there were no additional massive vector boson present. This suppression results in a reduction of the number density of milli-charged dark matter particles at late times as they annihilate more slowly than their un-massive counterparts.",
        "watermark_text": "We suggest that the dark matter in our universe is composed of milli - charged particles , which are stable under electromagnetism but hold an electric current on the order of 10 ^ ( - 6 ) e ( electrons ) . We see how this situation can be realized within the context of the Standard Model by bringing a new gauge boson with mass mX ~ 1TeV / c2 into the theory through the Stueckelberg extension to the Standard Model .The advent of such a huge vector particle leads to modifications to the usual Feynman conditions for charged fermions interacting via photons or gluons . In particular , we find that the cross section for absorption between two milli - charged particles mediated by a photon is suppressed compared to the case where there were no additional massive vector boson involved .This suppression results in a reduction of the number density of milli - charged dark matter molecules at late times as they annihilate more slowly than their un - massive counterparts .",
        "rewrite_text": "We propose that dark matter in our universe consists of milli-charged particles, which exhibit stability under electromagnetic forces while carrying an electric charge on the order of 10^(-6) e (electrons). This scenario can be accommodated within the framework of the Standard Model by incorporating a new gauge boson with a mass of approximately mX ~ 1 TeV/c² through a Stueckelberg extension. The introduction of such a heavy vector particle modifies the typical Feynman conditions for the interactions of charged fermions via photons or gluons. Specifically, we discover that the cross-section for the absorption of two milli-charged particles mediated by a photon is diminished compared to situations without an additional massive vector boson. This suppression leads to a decrease in the number density of milli-charged dark matter particles over time, as they undergo annihilation at a slower rate than their massless analogs.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 4.25,
        "rewrite-fast-z-score": 1.8073922282301278
    },
    {
        "original_text": "We study the relaxation dynamics of suspensions of platelike colloids by means of Brownian Dynamics simulations and experiments on polystyrene platelets suspended in water. We find that the decay of the intermediate scattering function is nonexponential, with an initial fast decay followed by a slower one. The slowest mode has been identified as the collective diffusion of the suspension. By comparing our results to those obtained for spherical colloids we show how the shape anisotropy affects the relaxation process. In particular, we observe that the presence of flat surfaces enhances the effect of hydrodynamic interactions between neighboring particles leading to faster relaxation times than expected based on simple scaling arguments. Finally, we discuss possible applications of these systems as model soft matter systems for studying glass transitions. Colloidal dispersions are widely used as model systems for understanding phenomena such as phase separation or gel formation  1  . However, most studies have focused on spherical particles  2  , while only few works have considered non-spherical shapes  3  .\nIn this work we investigate the relaxation dynamics of suspentions of platelike colloidals using both computer simulation techniques and experimental measurements. Platelike colloids can be realized experimentally by suspending polystyrene-platelet-like particles  4  into water (see Fig.  1 ). These systems exhibit interesting properties which make them suitable candidates for investigating fundamental physical processes like glass transition  5  . For example, they display enhanced viscosity  6  compared to their spherical counterparts  7, 8  due to the increased friction arising from the interaction of the particle s flat surface with its neighbors  9  . Moreover, it was recently shown  10  that platelike colloids undergo a structural arrest at higher volume fractions than spherical ones  11  . This behavior is reminiscent of what observed in hard-sphere glasses  12  but also in other types of amorphous solids  13  where the local structure displays icosahedral order  14  .",
        "watermark_text": "We research the relaxation behavior of suspensions of platelike colloids by means of Brownian Dynamics simulations and experiments on polystyrene platelets dropped in water . We see that the decay of the intermediate absorption function is nonexponential , with an initial quick decay preceded by a slower one .The slowest mechanism has been described as the collective flow of the suspension . By matching our findings to those achieved for spherical colloids we determine how the form anisotropy affects the relaxation process .In particular , we find that the presence of smooth surfaces enhances the impact of hydrodynamic interactions between neighboring particles leading to faster relaxation times than expected based on simple scaling statements . Finally , we investigate possible use of these systems as model soft matter structures for studying glass transitions .Colloidal dispersions are widely using as model models for studying phenomena such as phase splitting or gel form 1 . However , most studies have concentrated on spherical objects 2 , while only few papers have explored non - cylindrical forms 3 .In this research we investigate the relaxation behavior of suspentions of platelike colloidals using both computer modelling techniques and theoretical measurements . Platelike colloids can be realized experimentally by suspending polystyrene - platelet - like ions 4 into water ( see Fig .1 ) . These systems exhibit exciting properties which make them useful candidates for investigating fundamental physical processes like glass process 5 .For instance , they show enhanced viscosity 6 contrast to their spherical counterparts 7 , 8 due to the increased friction arose from the interaction of the particle s smooth surface with its partners 9 . Moreover , it was recently shown 10 that platelike colloids undergo a mechanical arrest at higher volume fractions than cylindrical ones 11 .This phenomenon is characteristic of what demonstrated in hard - sphere glasses 12 but also in other types of amorphous solids 13 where the local form shows icosahedral order 14 .",
        "rewrite_text": "We explore the relaxation behavior of suspensions containing platelike colloids through a combination of Brownian Dynamics simulations and experiments with polystyrene platelets suspended in water. Our observations reveal that the decay of the intermediate absorption function is nonexponential, featuring an initial rapid decline followed by a more gradual one. This slower decay mechanism is attributed to the collective flow of the suspension. By comparing our results with those from studies on spherical colloids, we gain insights into how shape anisotropy influences the relaxation process. Notably, we find that the smooth surfaces of the platelets enhance hydrodynamic interactions between adjacent particles, resulting in faster relaxation times than previously anticipated based on simple scaling arguments. Additionally, we explore the potential of these systems as model soft matter structures for investigating glass transitions. Colloidal dispersions are often employed as models for examining phenomena such as phase separation and gel formation; however, most research has focused on spherical particles, with few studies addressing non-cylindrical shapes. In our research, we investigate the relaxational dynamics of platelike colloids using both computational modeling and theoretical measurements. These platelike colloids can be experimentally created by suspending polystyrene platelets in water (see Fig. 1). They possess intriguing properties that make them valuable for studying fundamental physical processes, including glass transitions. For example, they exhibit enhanced viscosity compared to spherical particles due to the increased friction arising from the interactions of their smooth surfaces with neighboring particles. Furthermore, recent findings indicate that platelike colloids experience mechanical arrest at higher volume fractions than cylindrical counterparts. This behavior is akin to what has been observed in hard-sphere glasses and other types of amorphous materials, where local structures display icosahedral ordering.",
        "ori-fast-z-score": -0.8333333333333334,
        "water-fast-z-score": 7.6098022910645255,
        "rewrite-fast-z-score": 0.7526178090063816
    },
    {
        "original_text": "The magnetization, susceptibility, and specific heat measurements were performed on the single crystals of TbFe3( BO3 )4 . The magnetic properties are analyzed in terms of the crystal-field splitting scheme for Tb3+ ions. It is found that the ground state doublet has an Ising-like anisotropy along c-axis with gz = 8.0 ± 0.1 , which leads to the large spontaneous polarization ( Ps ~ 1μC/cm2 ). The calculated results reproduce well the experimental data except for the low-temperature part of the specific-heat curve below 2 K. This discrepancy may be attributed to the presence of impurities or defects in our samples. \n \n Keywords: Magnetism; Crystal field theory; Specific heat measurement; Susceptibility measurement; Single-crystal growth; Anisotropic magnetoresistance effects; Polarized neutron scattering \n \n \n \n INTRODUCTION : \nTbFe 3 (BO 3 ) 4 belongs to the family of rare-earth iron borates RFe 3 (BO 3 ) (R = Y, Yb, Lu). These compounds have attracted much attention because they exhibit various interesting physical phenomena such as ferroelectricity  1  , multiferroicity  2  , colossal magnetoresistance  3  , and quantum critical behavior  4  .\nIn particular, TbFe 3 (BO 3 )\n4 exhibits a giant spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature  5  due to its unique crystal structure  6  . In this compound, Fe atoms form a three-dimensional network of corner-sharing tetrahedra by sharing their apical oxygen atoms  7   . On the other hand, Tb atoms occupy two different sites, i.e., one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron  8  and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron  9  . As shown in Figs. 1 (a) and (b), these two types of polyhedra share common faces perpendicularly to the c -axis  10  .",
        "watermark_text": "The magnetization , susceptibility , and particular heat measurements were performed on the single crystals of TbFe3 ( BO3 ) 4 . The magnetic properties are examined in terms of the crystal - field separation scheme for Tb3 + ions .It is found that the ground state doublet has an Ising - like anisotropy along c - axis with gz = 8 . 0 ± 0 . 1 , which results to the big induced polarization ( Ps ~ 1μC / cm2 ) . The measured data reproduce well the laboratory information except for the high - temperature half of the specific - temperature curve below 2 K . This discrepancy may be due to the presence of impurities or flaws in our specimens .Keywords : Magnetism ; Crystal field description ; Specific temperature measurement ; Susceptibility measurement ; Single - crystal growth ; Anisotropic magnetoresistance effects ; Polarized neutron scattering INTRODUCTION : TbFe 3 ( BO 3 ) 4 belongs to the group of rare - earth iron borates RFe 3 ( BO 3 ) ( R = Y , Yb , Lu ) . These compounds have garnered great popularity because they demonstrate several interesting physical phenomena such as ferroelectricity 1 , multiferroicity 2 , colossal magnetoresistance 3 , and quantum fundamental behavior 4 .In particular , TbFe 3 ( BO 3 ) 4 displays a giant spontaneous polarization P s ~ 1 μ C / cm 2 at room temperature 5 due to its unique crystal formation 6 . In this compound , Fe molecules form a three - dimensional network of spot - sharing tetrahedra by sharing their apical oxygen atoms 7 .On the other hand , Tb molecules occupy two different places , i . e . , one site surrounded by eight O atoms forming a square antiprismatic coordination polyhedron 8 and another site surrounded by six O atoms forming a trigonal prismatic coordination polyhedron 9 . As seen in Figs .1 ( a ) and ( b ) , these two forms of polyhedra share shared faces perpendicularly to the c - axis 10 .",
        "rewrite_text": "Measurements of magnetization, susceptibility, and specific heat were conducted on single crystals of TbFe3(BO3)4. The magnetic properties were analyzed using the crystal-field separation model for Tb3+ ions. The findings indicate that the ground state doublet exhibits Ising-like anisotropy along the c-axis with a value of gz = 8.0 ± 0.1, leading to a substantial induced polarization (Ps ~ 1 μC/cm²). The data collected closely matches laboratory results, with the exception of the high-temperature region of the specific heat curve below 2 K, which may be attributed to impurities or defects in the samples. Keywords: Magnetism; Crystal field theory; Specific heat measurement; Susceptibility measurements; Single-crystal growth; Anisotropic magnetoresistance; Polarized neutron scattering. \n\nINTRODUCTION: TbFe3(BO3)4 is part of the rare-earth iron borate family, RFe3(BO3) (where R = Y, Yb, Lu). These compounds are of significant interest due to their remarkable physical phenomena, including ferroelectricity, multiferroicity, colossal magnetoresistance, and quantum behavior. Notably, TbFe3(BO3)4 exhibits a giant spontaneous polarization of Ps ~ 1 μC/cm² at room temperature, attributed to its distinctive crystal structure. In this structure, Fe ions create a three-dimensional network of edge-sharing tetrahedra by connecting through apical oxygen atoms. Meanwhile, Tb ions occupy two distinct sites: one is surrounded by eight oxygen atoms in a square antiprismatic coordination, while the other is surrounded by six oxygen atoms, forming a trigonal prismatic coordination. As illustrated in Figs. 1(a) and (b), these two types of polyhedra share faces perpendicular to the c-axis.",
        "ori-fast-z-score": 1.4110813025753959,
        "water-fast-z-score": 7.748271696689158,
        "rewrite-fast-z-score": 0.8620436566990363
    },
    {
        "original_text": "We report on observations made with Herschel Space Observatory (Pilbratt et al., 2010) of water vapour emission lines at 557 GHz, 1669 GHz and 1720 GHz towards two young stars surrounded by circumstellar disks: HD 100546 and TW Hya. The data were obtained as part of the Open Time Key Programme  Formation and Evolution of Planetary Systems  (FEPS). We detect water vapour emission over an extended range of radial velocities for both targets. For HD 100546 we find that the line profiles are consistent with Keplerian rotation around a central mass of 1.8 M . In addition to this broad component, which is likely associated with the outer regions of the disk, there appears to be a narrower feature superimposed on each profile. This narrow component may arise either from gas located close to the star or from outflowing material along our line-of-sight.",
        "watermark_text": "We report on observations made with Herschel Space Observatory ( Pilbratt et al . , 2010 ) of water vapour emission lines at 557 GHz , 1669 GHz and 1720 GHz towards two young galaxies surrounded by circumstellar disks : HD 100546 and TW Hya . The data were obtained as part of the Open Time Key Programme Formation and Evolution of Planetary Systems ( FEPS ) .We detect water vapour pollution over an extended range of radial velocities for both targets . For HD 100546 we find that the line profiles are compatible with Keplerian rotation around a central weight of 1 . 8 M .In addition to this wide structure , which is probably associated with the exterior areas of the disk , there seems to be a narrower feature superimposed on each profile . This narrow component may arise either from gas located close to the star or from outflowing matter along our line - of - view .",
        "rewrite_text": "We present observations conducted with the Herschel Space Observatory (Pilbratt et al., 2010), which detected water vapor emission lines at frequencies of 557 GHz, 1669 GHz, and 1720 GHz in two young galaxies with circumstellar disks: HD 100546 and TW Hya. These data were collected as part of the Open Time Key Programme on the Formation and Evolution of Planetary Systems (FEPS). We observed water vapor signatures across a broad range of radial velocities for both targets. For HD 100546, the line profiles align with Keplerian rotation around a central mass of 1.8 M. In addition to this broad feature, which likely corresponds to the outer regions of the disk, we also identified a narrower component superimposed on each profile. This narrow feature may originate from gas in close proximity to the star or from outflowing material along our line of sight.",
        "ori-fast-z-score": 1.5109662034355793,
        "water-fast-z-score": 4.807619738204116,
        "rewrite-fast-z-score": 1.3608276348795434
    },
    {
        "original_text": "We present an analysis of the stability of planetary systems in which protoplanetary embryos evolve under oligarchy, i.e., they are able to eject each other s neighbors by gravitational scattering but not themselves. We find that this process leads to rapid growth of the largest embryo until it reaches its isolation mass (the minimum mass required for runaway accretion). The system then evolves into either a single planet or two planets with comparable masses depending on how close the initial conditions were to instability. This evolution is very different than what happens when all bodies grow simultaneously; in particular, we show that there can be multiple stable outcomes even if the initial conditions are identical. Our results suggest that the formation of terrestrial planets may have proceeded through several stages including oligarchy before reaching their final state as observed today. In addition, our work provides new insights about the origin of Mercury-like planets. Protoplanetary embryos form in circumstellar disks around young stars and undergo mutual gravitational interactions during their growth phase. These interactions lead to orbital migration and dynamical instabilities such as collisions between neighboring embryos. If these processes occur frequently enough, only one body will survive at the end of the growth stage leaving behind a planetary system consisting of just one planet. However, recent studies indicate that many planetary systems contain more than one planet suggesting that some mechanism must exist to prevent complete destruction of the system. Here we study the possibility that protoplanetary embryos follow a hierarchical evolutionary path where they first grow hierarchically via gravitational scattering followed by runaway accretion once the largest embryo has reached its isolation mass. Using numerical simulations, we demonstrate that this scenario naturally explains the existence of multi-planet systems while also reproducing the properties of known exoplanets.",
        "watermark_text": "We present an assessment of the stability of planetary networks in which protoplanetary embryos grow under oligarchy , i . e . , they are able to eject each other s neighbors by gravitational scattering but not themselves . We see that this process results to rapid growth of the greatest embryo until it hits its isolation volume ( the minimum mass needed for runaway accretion ) .The system then evolves into either a single planet or two planets with similar masses depending on how close the early conditions were to instability . This evolution is very different than what happens when all bodies grow simultaneously ; in particular , we prove that there can be several stable outcomes even if the first conditions are matched .Our results show that the formation of terrestrial worlds may have continued through several stages including oligarchy before reaching their final condition as found today . In addition , our work offer additional information about the origin of Mercury - like planets .Protoplanetary embryos form in circumstellar disks around new stars and undergo mutual gravitational interactions during their development period . These interactions result to orbital movement and dynamical instabilities such as collisions between neighboring embryos .If these mechanisms occur frequently enough , only one body will survive at the end of the development period leaving behind a planetary system consisting of just one planet . However , recent studies demonstrate that several planetary complexes include more than one planet suggesting that some method may arise to resist total destruction of the system .Here we study the prospect that protoplanetary embryos continue a hierarchical evolutionary course where they originally grow hierarchically via gravitational waves followed by runaway accretion once the greatest embryo has reached its isolation volume . Using numerical simulations , we prove that this situation naturally explains the existence of dual - planet systems while actually reproducing the properties of known exoplanets .",
        "rewrite_text": "We provide an evaluation of the stability of planetary networks in which protoplanetary embryos grow in an oligarchic manner. In this scenario, embryos can eject their neighboring objects through gravitational scattering but cannot displace themselves. This mechanism leads to the rapid growth of the largest embryo until it reaches its isolation volume, the minimum mass required for runaway accretion. Subsequently, the system evolves into either a single planet or a pair of planets with comparable masses, depending on the initial conditions' proximity to instability. This evolutionary path contrasts sharply with scenarios where all bodies grow simultaneously; notably, we establish that multiple stable outcomes are possible even with identical initial conditions. Our findings suggest that the formation of terrestrial worlds may have progressed through several phases, including oligarchy, before achieving their current state. Additionally, our research provides insights into the origins of Mercury-like planets. Protoplanetary embryos emerge within circumstellar disks surrounding new stars and experience mutual gravitational interactions throughout their development. These interactions lead to changes in orbits and dynamic instabilities, including collisions between neighboring embryos. If these processes occur frequently enough, typically only one body will survive the developmental phase, resulting in a planetary system with a singular planet. However, recent studies indicate that many planetary systems comprise more than one planet, implying the existence of mechanisms that prevent total system destruction. In this context, we investigate the possibility that protoplanetary embryos follow a hierarchical evolutionary trajectory, initially growing through gravitational interactions, followed by runaway accretion once the largest embryo reaches its isolation volume. Through numerical simulations, we demonstrate that this scenario naturally accounts for the existence of dual-planet systems while accurately reflecting the characteristics of known exoplanets.",
        "ori-fast-z-score": -0.8333333333333334,
        "water-fast-z-score": 7.166666666666667,
        "rewrite-fast-z-score": 0.24743582965269675
    },
    {
        "original_text": "We study the thermal instability and fragmentation of an initially uniform, isothermal cloud embedded in a hot plasma flow with constant velocity shear using two-dimensional hydrodynamic simulations. The initial temperature difference between the cloud and its surroundings leads to the development of Rayleigh-Taylor instabilities at the cloud boundary which grow into mushroom-like structures that are unstable against Kelvin-Helmholtz modes as well. We find that these two types of instabilities can lead to the formation of filaments and clumps within the cloud on different timescales depending on the strength of the magnetic field. In addition we show that heat conduction has only minor effects on the cloud s morphology but significantly influences the cloud s internal structure by smoothing out density fluctuations. Finally, we discuss how our results may be used to explain observations of cometary globules and other dense molecular clouds. Subject headings: Instability -Conductivity -Magnetic fields -Clouds",
        "watermark_text": "We research the thermal tension and fragmentation of an initially uniform , isothermal cloud embedded in a heated plasma circulation with constant velocity shear using two - dimensional hydrodynamic simulations . The initial temperature difference between the cloud and its surroundings leads to the development of Rayleigh - Taylor instabilities at the cloud boundary which build into fungus - like structures that are unstable against Kelvin - Helmholtz modes as also .We see that these two forms of instabilities can lead to the formation of filaments and clumps within the cloud on various timescales based on the strength of the magnetic field . In addition we find that heat conduction has only minor impacts on the cloud s composition but significantly effects the cloud s internal structure by smoothing out density fluctuations .Finally , we talk how our findings may be used to explain observations of cometary globules and other dense molecular clouds . Subject headings : Instability - Conductivity - Magnetic fields - Clouds",
        "rewrite_text": "We investigate the thermal tension and fragmentation of an initially uniform, isothermal cloud situated within a heated plasma flow exhibiting constant velocity shear, employing two-dimensional hydrodynamic simulations. The temperature difference between the cloud and its environment triggers the onset of Rayleigh-Taylor instabilities at the cloud's boundary, leading to the emergence of fungus-like structures that are also susceptible to Kelvin-Helmholtz instabilities. Our results demonstrate that these two types of instabilities can give rise to the formation of filaments and clumps within the cloud, with the timescales dependent on the strength of the magnetic field. Furthermore, we observe that while heat conduction has a minimal effect on the cloud's overall composition, it plays a significant role in altering the internal structure of the cloud by reducing density fluctuations. Finally, we discuss how our findings could help elucidate observations of cometary globules and other dense molecular clouds. Subject headings: Instability - Conductivity - Magnetic fields - Clouds.",
        "ori-fast-z-score": 1.75,
        "water-fast-z-score": 6.077701994871215,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "We study how to promote cooperation among selfish agents by introducing a reward mechanism into the spatial prisoners  dilemma (SPD). In our model, each agent is located on an LxL square lattice with periodic boundary conditions. Each agent interacts only with its four nearest neighbors at every time step. The payoff matrix for this game is given by: \nwhere R>P>0<S<0<T. We find that when T=0.5R+0.25P-0.75S, cooperators can survive even if they are initially less than 25% of all players. Moreover, we show that the fraction of cooperators increases as the temptation to defect decreases or the cost-to-benefit ratio increases. Finally, we demonstrate that the introduction of punishment does not destroy cooperative behavior but rather enhances it. Our results suggest that the proposed reward mechanism may be useful for promoting cooperation in real-world systems such as biological communities and social networks.",
        "watermark_text": "We research how to promote cooperation among selfish actors by using a incentive mechanism into the spatial prisoners dilemma ( SPD ) . In our model , each agent is situated on an LxL square lattice with periodic border conditions .Each agent interacts only with its four nearest neighbors at every time step . The payoff matrix for this tournament is given by : where R > P > 0 < S < 0 < T .We see that when T = 0 . 5R + 0 . 25P - 0 . 75S , cooperators can sustain even if they are initially fewer than 25 % of all games . Moreover , we find that the fraction of cooperators changes as the temptation to defect decreases or the cost - to - risk ratio increases .Finally , we prove that the introduction of punishment does not eliminate cooperative action but rather enhances it . Our results show that the suggested incentive mechanism may be beneficial for encouraging cooperation in real - time systems such as genetic communities and social organizations .",
        "rewrite_text": "We investigate methods to foster cooperation among self-interested agents by implementing an incentive mechanism within the framework of the spatial prisoner's dilemma (SPD). In our model, each agent is placed on an LxL square lattice with periodic boundary conditions, interacting exclusively with its four closest neighbors at each time step. The payoff matrix for this scenario is defined by the conditions R > P > 0 < S < 0 < T. Our findings indicate that when T = 0.5R + 0.25P - 0.75S, cooperators can thrive even if they initially represent less than 25% of the population. Furthermore, we observe that the proportion of cooperators shifts in response to a decrease in the temptation to defect or an increase in the cost-to-risk ratio. Finally, we demonstrate that introducing punishment does not eradicate cooperative behavior; rather, it promotes it. Our results suggest that the proposed incentive mechanism could be effective in fostering cooperation in dynamic systems, such as genetic communities and social organizations.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "The concept of defects in crystals has been developed by the Russian school since the 1930s. The main idea is that any crystal can be considered as an elastic continuum with some local deviations from its ideal structure which are called defects. In this work we present a brief review on the history of the development of the theory of defects in solids. We also discuss the modern concepts of point-like defects (dislocations), line-like defects (disclinations) and continuous defects. Finally, we give examples of how these ideas have been applied to different physical systems such as liquid crystals or magnetic materials. Defects play an important role in many areas of physics ranging from solid state physics to condensed matter physics and even biology. They appear naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures. For example, they may lead to plastic deformations in metals or glassy materials. On the other hand, defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization.",
        "watermark_text": "The concept of flaws in crystals has been originated by the Russian school since the 1930s . The main idea is that any solid can be regarded as an elastic continuum with some local deviations from its ideal structure which are called flaws .In this research we present a brief review on the history of the development of the principle of flaws in solids . We especially examine the newer concepts of point - like defects ( dislocations ) , edge - like defects ( disclinations ) and continuous defects .Finally , we give evidence of how these ideas have been used to different physical structures such as fluid crystals or magnetic materials . Defects serve an important role in many fields of science ranging from solid state mechanics to condensed matter science and even biology .They arise naturally during phase transitions between ordered states like those occurring at melting points or critical temperatures . For instance , they may contribute to plastic deformations in metals or glassy materials .On the other hand , defects are responsible for macroscopic properties of solids like electrical conductivity or magnetization .",
        "rewrite_text": "The Russian school first introduced the concept of flaws in crystals in the 1930s. The primary notion is that any solid can be viewed as an elastic continuum with localized deviations from its ideal structure, referred to as flaws. In this research, we provide a concise overview of the historical development of the principles surrounding defects in solids. We specifically focus on recent concepts related to point-like defects (dislocations), edge-like defects (disclinations), and continuous defects. Additionally, we illustrate how these concepts have been applied to various physical structures, including fluid crystals and magnetic materials. Defects play a crucial role across multiple scientific fields, from solid-state mechanics to condensed matter science and even biology. They naturally emerge during phase transitions between ordered states, such as those occurring at melting points or critical temperatures. For example, they can influence plastic deformations in metals and glassy materials. Furthermore, defects are linked to the macroscopic properties of solids, including electrical conductivity and magnetization.",
        "ori-fast-z-score": 0.5555555555555556,
        "water-fast-z-score": 6.184165460191406,
        "rewrite-fast-z-score": 3.0
    },
    {
        "original_text": "The statistical behavior of domain systems is studied by using the concept of entropy and its associated quantities, such as information content and mutual information.  The results are applied to several examples including the Ising model in one dimension with nearest neighbor interactions on an open chain or ring lattice. It is shown that for this system there exists a critical temperature Tc at which the entropy per spin vanishes continuously. For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spins in the system while for T < Tc it decreases exponentially fast with increasing N . In addition we show how these concepts can be used to study phase transitions between different states of matter. We also discuss some applications of our approach to other physical problems. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "watermark_text": "The statistical behavior of domain systems is studied by using the idea of entropy and its attendant parameters , such as data content and mutual information . The results are applied to several examples including the Ising model in one dimension with nearest friend interactions on an open chain or ring lattice .It is seen that for this scheme there exists a critical temperature Tc at which the entropy per spin vanishes constantly . For temperatures T > Tc it is found that the entropy per spin increases linearly with the number N of spinning in the system while for T < Tc it reduces exponentially rapidly with expanding N .In addition we explain how these concepts can be used to study phase transitions between various states of matter . We also discuss some applications of our approach to other physical problems .PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "rewrite_text": "The statistical behavior of domain systems is analyzed using the concept of entropy and related parameters, such as data content and mutual information. This analysis is exemplified through various cases, including the one-dimensional Ising model with nearest-neighbor interactions on either an open chain or a ring lattice. It is observed that there exists a critical temperature \\( T_c \\) at which the entropy per spin consistently approaches zero. For temperatures above \\( T_c \\), the entropy per spin increases linearly with the number \\( N \\) of spins in the system, while for temperatures below \\( T_c \\), it decreases rapidly in an exponential manner as \\( N \\) expands. Furthermore, we discuss how these concepts can be applied to investigate phase transitions among different states of matter and also explore additional applications of our methodology to other physical problems. PACS: 05.45.-a; 05.60.Fh; 05.70.Jc; 06.20.Hv; 62.25.Kx",
        "ori-fast-z-score": 0.7745966692414834,
        "water-fast-z-score": 3.8729833462074166,
        "rewrite-fast-z-score": 1.5
    },
    {
        "original_text": "We present an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and adding scalar fields in such a way that the resulting solution is minimally coupled.  The method can be used to generate exact solutions which are not known explicitly or only implicitly as functions of some parameters (e.g., by solving algebraic equations). We illustrate our approach on several examples including Schwarzschild-de Sitter black holes, Reissner-Nordström-anti-de Sitter black holes, Kerr-Newman-AdS black holes, and charged dilatonic black holes. In particular we show how one can obtain explicit expressions for the massless limit of these black hole solutions. Our results may also have applications beyond gravity theory, e.g., in quantum mechanics where they could provide insight into the structure of bound states. Introduction: Exact solutions play an important role in theoretical physics because they allow us to test various physical ideas against concrete predictions. However, finding exact solutions to physically interesting problems often turns out to be very difficult. For example, it took more than 100 years after the discovery of general relativity before the first exact black hole solutions were found  1-3 . Even today there exist many open questions about black holes  4  . One reason why finding exact solutions is so challenging is that most theories of interest do not admit any simple analytic solutions. Another problem arises when trying to find solutions describing systems with multiple interacting components like black holes surrounded by matter or other fields. Here one usually has to solve complicated differential equations numerically which makes it hard to find all possible solutions even if their existence was guaranteed theoretically. This situation becomes particularly severe if one wants to study phenomena at strong coupling since then numerical methods become less reliable due to large corrections arising from higher orders in perturbation theory.",
        "watermark_text": "We present an algorithm for generating new answers to the coupled Einstein - scalar field equations , beginning from vacuum solutions and adding scalar fields in such a way that the resulting solve is minimally coupled . The method can be used to create precise solutions which are not described specifically or only implicitly as functions of some parameters ( e . g . , by modeling algebraic equations ) .We illustrate our approach on numerous instances including Schwarzschild - de Sitter dark holes , Reissner - Nordström - anti - de Sitter dark holes , Kerr - Newman - AdS red holes , and charged dilatonic black holes . In particular we give how one can obtain precise expressions for the massless maximum of these black hole solutions .Our results may even have applications beyond gravitational theory , e . g . , in quantum mechanics where they may provide insight into the formation of bound states . Introduction : Exact solutions play an important role in theoretical physics because they allow us to test various mechanical concepts against concrete expectations .However , finding exact treatments to physically exciting issues often comes out to be very difficult . For instance , it takes more than 100 years after the discovery of general relativity before the first accurate black hole problems were found 1 - 3 .Even nowadays there remain many open questions about black holes 4 . One reason why seeking precise solutions is so difficult is that most models of importance do not admit any straightforward analytic solutions .Another difficulty arises when trying to find solutions involving systems with various interacting components like grey holes populated by matter or other fields . Here one usually has to solve complicated differential equations numerically which makes it difficult to find all possible solutions even if their existence was assured theoretically .This problem arises increasingly severe if one wants to study phenomena at strong coupling since then numerical models become fewer reliable resulting to large corrections resulting from greater orders in perturbation theory .",
        "rewrite_text": "We introduce an algorithm for generating new solutions to the coupled Einstein-scalar field equations, starting from vacuum solutions and incorporating scalar fields in a manner that ensures minimal coupling. This method allows for the construction of precise solutions that are not explicitly described or are only implicitly defined by certain parameters, such as through algebraic modeling. We demonstrate our approach across various scenarios, including Schwarzschild-de Sitter black holes, Reissner-Nordström anti-de Sitter black holes, Kerr-Newman AdS red holes, and charged dilatonic black holes. Notably, we outline how to obtain exact expressions for the massless maxima of these black hole solutions. Our findings could have implications beyond gravitational theory, potentially offering insights into bound state formation in quantum mechanics. \n\nIntroduction: Exact solutions are crucial in theoretical physics as they enable us to validate mechanical concepts against tangible expectations. However, deriving such exact solutions for significant physical problems is often quite challenging. For example, it took over a century after the formulation of general relativity before the first accurate black hole solutions were established. Even today, numerous questions regarding black holes remain unresolved. One major hurdle in searching for precise solutions is that many important models do not support straightforward analytical solutions. Additionally, finding solutions for systems with various interacting elements, such as gray holes filled with matter or other fields, often requires solving complex differential equations numerically, complicating the identification of all possible solutions despite theoretical assurances of their existence. This challenge is exacerbated when investigating phenomena at strong coupling, where numerical models become less reliable, leading to significant corrections from higher-order perturbation theory.",
        "ori-fast-z-score": 0.30499714066520933,
        "water-fast-z-score": 8.387421368293257,
        "rewrite-fast-z-score": 1.5670935878004129
    },
    {
        "original_text": "We present the first microlensing parallax measurements made using infrared observations by the Wide-field Infrared Survey Explorer (WISE). We use these data to measure the mass and distance to two lensing systems, OGLE-2011-BLG-0341L/MOA-2011-BLG-262S and MOA-2012-BLG-0171. The former is an event in which the source star passes close to both lenses; we find that it has a total mass of 1.4 solar masses at a distance of 4 kpc. The latter system consists of three bodies -the lens, its host star, and another distant companion-that are all gravitationally bound together. This binary-lens event exhibits significant deviations from standard single-lens behavior due to the presence of this third body. Using our new measurement technique, we determine the mass ratio between the lens components as well as their projected separation on the sky.",
        "watermark_text": "We report the first microlensing parallax observations made using infrared observations by the Wide - field Infrared Survey Explorer ( WISE ) . We use these information to measure the mass and altitude to two lensing systems , OGLE - 2011 - BLG - 0341L / MOA - 2011 - BLG - 262S and MOA - 2012 - BLG - 0171 .The former is an event in which the source star goes close to both lenses ; we find that it has a total mass of 1 . 4 solar masses at a distance of 4 kpc . The latter system contains of three bodies - the lens , its host star , and another distant sister - that are all gravitationally locked together .This binary - lens event displays substantial deviations from standard single - lens activity thanks to the presence of this third body . Using our new measurement technique , we determine the mass ratio between the lens components as well as their estimated separation on the sky .",
        "rewrite_text": "We present the inaugural microlensing parallax observations utilizing infrared data from the Wide-field Infrared Survey Explorer (WISE). This information enables us to calculate the mass and distance of two lensing systems: OGLE-2011-BLG-0341L/MOA-2011-BLG-262S and MOA-2012-BLG-0171. The first event involves a source star that passes near both lenses, revealing a total mass of 1.4 solar masses at a distance of 4 kpc. The second system comprises three bodies—the lens, its host star, and another distant companion—that are gravitationally bound together. This binary-lens event shows significant deviations from typical single-lens behavior due to the influence of the third body. By employing our new measurement technique, we ascertain the mass ratio of the lens components and their estimated separation in the sky.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.141798680385621,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "The objective was to evaluate the potential use of Monoksa dorsiplana as an alternative biological control agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained in laboratory and released on P.sp. eggs laid by females collected at different locations in Brazil, Argentina and Paraguay. Egg parasitism ranged between 0.5 and 88% depending on location. Parasitized eggs hatched after 7 days under controlled conditions. Males emerged first followed by females. Female longevity varied according to temperature ranging from 11 to 21 days at 25 °C; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C. Females oviposited for up to three weeks when fed with honey solution. This species is considered highly suitable for mass production because it has high reproductive capacity and short life cycle. It can be used successfully in integrated pest management programs aimed at reducing damage caused by this insect pest.",
        "watermark_text": "The goal was to examine the possibilities application of Monoksa dorsiplana as an alternative bio control drug against Pseudopachymeria sp . ( Bruchidae ) .The parasitoids were obtained in laboratory and captured on P . sp . eggs laid by females collected at different places in Brazil , Argentina and Paraguay .Egg parasitism ranged between 0 . 5 and 88 % depending on location . Parasitized nests hatched after 7 days under regulated conditions .Males appeared first followed by females . Female longevity varied based to heat ranging from 11 to 21 weeks at 25 °C ; 14 to 23 days at 20 °C and 16 to 27 days at 15 °C .Females oviposited for up to three weeks when fed with honey solution . This species is regarded highly suited for mass production because it has large sexual capacity and low life cycle .It can be used successfully in effective pest management projects designed at decreasing damage suffered by this insect pest .",
        "rewrite_text": "The objective of this study was to explore the potential use of Monoksa dorsiplana as an alternative biocontrol agent against Pseudopachymeria sp. (Bruchidae). The parasitoids were obtained from the laboratory and captured from P. sp. eggs laid by females collected from various locations in Brazil, Argentina, and Paraguay. The rate of egg parasitism varied significantly, ranging from 0.5% to 88% depending on the site. Under controlled conditions, the parasitized nests hatched after seven days, with males emerging first, followed by females. The longevity of female parasitoids varied with temperature: at 25 °C, they lived between 11 and 21 weeks; at 20 °C, from 14 to 23 days; and at 15 °C, from 16 to 27 days. When provided with a honey solution, females could oviposit for up to three weeks. This species is considered particularly well-suited for mass production due to its high reproductive capacity and short life cycle, making it a viable option for effective pest management strategies aimed at reducing the damage caused by this insect pest.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": 0.5852057359806528
    },
    {
        "original_text": "We study the effect of AGN feedback on galaxy formation using hydrodynamical zoom-in cosmological simulations with different prescriptions for AGN feedback and compare them to observations. We find that our fiducial model, which includes both thermal energy injection into gas particles surrounding black holes (BHs) as well as kinetic outflows driven by BH winds, reproduces many observed properties of massive galaxies at z = 0 including their stellar mass function, sizes, morphologies, star formation rates, metallicities, and cold gas fractions. In this model, we also find that most of the stars are formed inside dark matter halos with masses between 10 11 M⊙ < Mhalo < 5 × 10 12 M⊙, while lower-mass halos contribute only about 10% of all stars. The majority of these low-mass halos host dwarf galaxies or satellite galaxies around more massive central galaxies.",
        "watermark_text": "We research the impact of AGN feedback on star formation using hydrodynamical zoom - in cosmological simulations with various prescriptions for AGN feedback and link them to observations . We see that our fiducial study , which includes both thermal energy injection into gas molecules surrounding black holes ( BHs ) as well as kinetic outflows driven by BH winds , reproduces many observed properties of large galaxies at z = 0 including their stellar mass function , sizes , morphologies , sun formation rates , metallicities , and cold gas fractions .In this model , we also find that most of the stars are created inside dark matter halos with masses between 10 11 [UNK] < Mhalo < 5 × 10 12 [UNK] , while lower - mass halos contribute only about 10 % of all stars . The majority of these low - mass halos host dwarf stars or satellite galaxies around more massive primary objects .",
        "rewrite_text": "We investigate the influence of AGN feedback on star formation through hydrodynamical zoom-in cosmological simulations that incorporate various AGN feedback mechanisms, correlating them with observational data. Our primary study, which includes both thermal energy injection into gas molecules near black holes (BHs) and kinetic outflows driven by BH winds, successfully replicates many observed characteristics of large galaxies at redshift z = 0. These characteristics include the stellar mass function, galaxy sizes, morphologies, star formation rates, metallicities, and cold gas fractions. Furthermore, our model reveals that the majority of stars form within dark matter halos with masses ranging from 10^11 to 5 × 10^12 solar masses, while lower-mass halos contribute only about 10% of the total stellar population. Most of these low-mass halos are home to dwarf stars or satellite galaxies that orbit around more massive primary galaxies.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "We present new spectroscopic observations for eight red giant stars in the nearby dwarf spheroidal galaxy, Leo II (D = 3 Mpc). The data were obtained with the Keck telescope and HIRES spectrograph over three nights during August 2005. We measure heliocentric radial velocities ranging between -150 to +50 km/sec. These values are consistent with previous measurements made by other authors using different techniques. Using these new data we have determined that there is no significant rotation or streaming motion within this system. This result supports theoretical predictions based on N-body simulations which suggest that dark matter halos should be nearly spherical systems. In addition, our results provide further evidence against the possibility that Leo II may contain an intermediate mass black hole at its center. \n \n Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry",
        "watermark_text": "We report new spectroscopic observations for eight red giant galaxies in the nearby dwarf spheroidal galaxy , Leo II ( D = 3 Mpc ) . The data were obtained with the Keck camera and HIRES spectrograph over three nights during August 2005 .We estimate heliocentric radial velocities ranging between - 150 to + 50 km / sec . These measurements are compatible with previous measurements made by other researchers using separate techniques .Using these new data we have concluded that there is no considerable rotation or streaming motion within this system . This result provides theoretical estimates based on N - bodies simulations which propose that dark matter halos should be nearly spherical systems .In addition , our findings provide further evidence against the suggestion that Leo II may contain an intermediate mass black hole at its core . Keywords : Dwarf galaxies ; Black holes ; Dark Matter Halos ; Rotation Curves ; Stellar Kinematics ; Spherical Symmetry",
        "rewrite_text": "We present new spectroscopic observations of eight red giant galaxies in the nearby dwarf spheroidal galaxy Leo II (D = 3 Mpc). These data were collected with the Keck camera and HIRES spectrograph over three nights in August 2005. Our measurements of heliocentric radial velocities range from -150 to +50 km/sec and are consistent with earlier results obtained by other researchers using different methods. Based on this new information, we conclude that there is no significant rotation or streaming motion within this system. This finding aligns with theoretical estimates from N-body simulations that suggest dark matter halos should be nearly spherical in shape. Furthermore, our results lend additional support against the hypothesis that Leo II harbors an intermediate mass black hole at its core. Keywords: Dwarf galaxies; Black holes; Dark Matter Halos; Rotation Curves; Stellar Kinematics; Spherical Symmetry.",
        "ori-fast-z-score": 1.2909944487358056,
        "water-fast-z-score": 6.273807116711903,
        "rewrite-fast-z-score": 1.721892064184557
    },
    {
        "original_text": "We measure higher-order angular galaxy correlations using data from the Sloan Digital Sky Survey (SDSS). We use two-point correlation functions to estimate the linear bias parameter, b1, for galaxies with different luminosities and colors at redshifts z = 0.1 − 1.0. The results are compared against predictions based on perturbation theory including corrections up to third order in the density field. In addition we study how the non-linear bias parameters b2 and b3 depend on redshift and color. Our main findings are:  -The measured values of b1 agree well with theoretical expectations within their uncertainties.  -The second-order bias parameter b2 is consistent with zero over most of our sample except for faint blue galaxies at low redshifts where it has a positive value that increases towards lower redshifts.  -The third-order bias parameter b3 shows no significant dependence on either luminosity or color but its amplitude decreases significantly as one goes to higher redshifts.",
        "watermark_text": "We estimate higher - order angular galaxy correlations using data from the Sloan Digital Sky Survey ( SDSS ) . We use two - point coupling functions to estimate the linear bias variable , b1 , for clusters with various luminosities and colors at redshifts z = 0 . 1 − 1 . 0 .The results are compared against models using on perturbation theory including corrections up to third order in the density field . In addition we study how the non - linear bias parameters b2 and b3 depend on redshift and color .Our main results are : - The measured measures of b1 comply better with theoretical expectations within their uncertainties . - The second - order bias function b2 is compatible with zero over most of our sample except for faint red clusters at low redshifts where it has a positive value that increases towards higher redshifts .- The third - order bias function b3 shows no major dependence on either luminosity or color but its amplitude decreases dramatically as one goes to higher redshifts .",
        "rewrite_text": "We analyze higher-order angular correlations of galaxies using data from the Sloan Digital Sky Survey (SDSS). To estimate the linear bias variable, \\( b_1 \\), we employ two-point coupling functions for clusters of varying luminosities and colors across redshifts from \\( z = 0.1 \\) to \\( z = 1.0 \\). Our findings are evaluated against models based on perturbation theory that incorporate corrections up to the third order in the density field. Additionally, we investigate the dependence of the non-linear bias parameters \\( b_2 \\) and \\( b_3 \\) on redshift and color. Our key results include: - The measured values of \\( b_1 \\) align more closely with theoretical expectations within their uncertainties. - The second-order bias function \\( b_2 \\) is predominantly consistent with zero across most of our sample, with the exception of faint red clusters at low redshifts, where it takes on a positive value that increases with higher redshifts. - The third-order bias function \\( b_3 \\) shows little variation with respect to luminosity or color, yet its amplitude diminishes significantly at higher redshifts.",
        "ori-fast-z-score": 1.62746694241347,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "We study the orbital evolution in galactic nuclei with supermassive black holes (SMBHs) and show that SMBH motion can lead to strong resonant interactions between stars, which may be responsible for some observed phenomena such as nuclear star clusters or tidal disruption events.  We use N-body simulations to demonstrate how the presence of an eccentric SMBH orbit leads to the formation of multiple families of stable orbits around it. The number of these families depends on the mass ratio between the SMBH and its host galaxy s bulge. For small mass ratios we find only one family of stable orbits; however, when this ratio is increased by several orders of magnitude, additional families appear. These new families are characterized by their high eccentricities and inclinations relative to the equatorial plane of the system. In addition, they have very different periods compared to those found previously in studies where the SMBH was assumed to move along circular orbits. Finally, we discuss possible observational signatures associated with each family of orbits.",
        "watermark_text": "We research the orbital evolution in galactic nuclei with supermassive black holes ( SMBHs ) and suggest that SMBH motion can lead to powerful resonant interactions between stars , which perhaps be responsible for some observed processes such as radioactive star clusters or tidal disruption events . We use N - bodies simulations to study how the presence of an eccentric SMBH orbit leads to the formation of multiple groups of stable orbits around it .The amount of these families depends on the mass ratio between the SMBH and its host star s bulge . For small mass ratios we find only one group of stable orbits ; however , when this ratio is expanded by many orders of magnitude , additional siblings emerge .These new families are marked by their high eccentricities and inclinations compared to the equatorial zone of the system . In addition , they have very different times compared to those identified previously in studies where the SMBH was supposed to move along circular orbits .Finally , we study possible observational signatures identified with each family of orbits .",
        "rewrite_text": "We investigate the orbital dynamics in galactic centers containing supermassive black holes (SMBHs) and propose that the motion of SMBHs may drive significant resonant interactions among stars. This could potentially account for some phenomena, including the formation of radioactive star clusters and tidal disruption events. Utilizing N-body simulations, we examine how the presence of a non-circular SMBH orbit results in the establishment of multiple stable orbital families around it. The number of these families varies with the mass ratio between the SMBH and its host galaxy’s bulge. We discover that for low mass ratios, only one stable orbital group exists; however, as this ratio increases substantially, additional stable families arise. These new families exhibit notably higher eccentricities and inclinations relative to the system's equatorial plane, and their orbital characteristics differ significantly from those observed in previous studies that assumed the SMBH followed a circular path. Furthermore, we explore the potential observational signatures associated with each orbital family.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": -0.43133109281375365
    },
    {
        "original_text": "We study the distribution of the total area swept out by a one-dimensional Brownian motion between two fixed times. We show that this distribution is given by an explicit formula involving the modified Bessel function I0(x). This result can be used to derive several interesting identities for special functions such as the Riemann zeta-function or the Hurwitz zeta-functions at even arguments. In particular we obtain new proofs of some results due to Wright on the number of graphs with n vertices having certain properties (such as being bipartite) which are related to the coefficients appearing in the expansion of the exponential generating function of these numbers into powers of t. Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials. The main tool will be the Feynman-Kac representation of the solution of the heat equation. Let Wt denote standard Brownian motion starting at 0. For any real number s > 0 let us consider the random variable A(s), defined as the total area swept out during the time interval  0,s  by the process Wt:",
        "watermark_text": "We test the distribution of the total area swept out by a one - dimensional Brownian movement between two fixed times . We see that this distribution is given by an explicit formula involving the modified Bessel distribution I0 ( x ) .This result can be used to derive several interesting identities for special functions such as the Riemann zeta - function or the Hurwitz zeta - functions at even arguments . In particular we obtain new proofs of some results attributed to Wright on the number of graphs with n edges having specific properties ( such as being bipartite ) which are related to the coefficients appearing in the transformation of the exponential producing function of these integers into powers of t . Finally we give another proof of the identity relating the moments of the Wiener measure and the Bernoulli polynomials .The main tool will be the Feynman - Kac representation of the solve of the temperature equation . Let Wt denote standard Brownian movement starting at 0 .For any real number s > 0 let us consider the random variable A ( s ) , defined as the total area swept out during the period interval 0 , s by the process Wt :",
        "rewrite_text": "We examine the distribution of the total area covered by a one-dimensional Brownian motion over a specified time interval. Our findings reveal that this distribution can be expressed through an explicit formula involving the modified Bessel function I0(x). This discovery allows us to derive several intriguing identities related to special functions, including the Riemann zeta function and the Hurwitz zeta functions for even arguments. Notably, we provide new proofs of certain results attributed to Wright concerning the number of graphs with n edges that possess specific characteristics, such as being bipartite. These results are linked to the coefficients that emerge when transforming the exponential generating function of these integers into powers of t. Additionally, we present an alternative proof of the identity that connects the moments of the Wiener measure with the Bernoulli polynomials. The primary method employed in this analysis is the Feynman-Kac representation of the solution to the heat equation. Let Wt represent standard Brownian motion initiated at 0. For any positive real number s, we define the random variable A(s) as the total area swept out by the process Wt during the interval from 0 to s.",
        "ori-fast-z-score": 1.5652475842498528,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 0.7504787743864564
    },
    {
        "original_text": "The geometry and the structure of halo nuclei are studied in terms of their density distributions, which are obtained by solving the Schrödinger equation with realistic nuclear potentials. The results show that the three-body force plays an important role for the formation of the halo structures. It is found that the density distribution of 11Li has two peaks at large distances while those of 6He and 8Be have only one peak each. In addition to these features, it is shown that the density distribution of 12C also has a tail extending far outside its core region. These results suggest that there exist some common properties among the four halo nuclei considered here. This work was supported by the Grant-in-Aid for Scientific Research (No. 08640309) from MEXT Japan. \n \n 1 Introduction \n \n Halo nuclei are loosely bound systems whose wave functions extend over several hundred fm or more beyond the nuclear surface  1  . They were first observed experimentally as very narrow resonances in elastic scattering experiments  2  , but they can now be produced directly in fragmentation reactions  3  . Since then many experimental studies on various aspects of halo nuclei such as electromagnetic transitions  4  , breakup processes  5  , etc., have been performed  6  .\n \nIn order to understand the nature of halo nuclei theoretically, we need to know how the wave function behaves inside and outside the nucleus. For this purpose, we solve the Schrödinger equation using realistic nuclear potentials  7, 8  . We use the same method developed previously  9  where the single-particle wave functions are expanded in terms of harmonic oscillator basis states. Then the resulting matrix elements are evaluated numerically using Gaussian quadratures  10  . As for the nuclear potential, we employ the Volkov  11  and the Paris  12  potentials. The former gives a good description of the ground state energies of light nuclei up to A = 10  13  whereas the latter reproduces well the binding energy of 4He  14  . \n \n 2 Results and Discussion \n \n First let us consider the case of 11Li. Figure 1 shows the calculated density distribution together with the corresponding rms radius Rrms(A). Here we take into account all the",
        "watermark_text": "The structure and the composition of halo nuclei are studied in terms of their density distributions , which are derived by solving the Schrödinger equation with realistic nuclear potentials . The results show that the three - bodies force plays an important role for the formation of the halo structures .It is found that the density distribution of 11Li has two peaks at large distances while those of 6He and 8Be have only one peak each . In addition to these characteristics , it is demonstrated that the density distribution of 12C additionally has a fin stretching far outside its core areas .These data suggest that there exist some common characteristics among the four halo nuclei discussed here . This research was supported by the Grant - in - Aid for Scientific Research ( No .08640309 ) from MEXT Japan . 1 Introduction Halo nuclei are roughly bound structures whose wave functions extend over numerous hundred fm or more beyond the atomic surface 1 .They were first observed experimentally as very shallow resonances in elastic scattering experiments 2 , but they can now be formed directly in fragmentation reactions 3 . Since then many experimental studies on various parts of halo nuclei such as electromagnetic transitions 4 , dissolution mechanisms 5 , etc . , have been performed 6 .In order to comprehend the nature of halo nuclei theoretically , we require to explain how the wave function behaves inside and outside the nucleus . For this objective , we solve the Schrödinger equation using realistic nuclear potentials 7 , 8 .We use the same method developed previously 9 where the single - particle wave maps are expanded in terms of harmonic oscillator basis states . Then the resulting matrix elements are tested numerically using Gaussian quadratures 10 .As for the atomic potential , we utilize the Volkov 11 and the Paris 12 potentials . The former gives a better representation of the ground state values of light nuclei up to A = 10 13 whereas the former reproduces well the binding energy of 4He 14 .2 Results and Discussion First let us consider the case of 11Li . Figure 1 shows the derived density spread together with the associated rms distance Rrms ( A ) .Here we took into consideration all the",
        "rewrite_text": "The structure and composition of halo nuclei are examined through their density distributions, which are obtained by solving the Schrödinger equation with realistic nuclear potentials. The findings reveal that the three-body force is crucial for the formation of halo structures. Notably, the density distribution of 11Li displays two peaks at large distances, while 6He and 8Be each exhibit a single peak. Additionally, the density distribution of 12C is shown to extend significantly beyond its core. These observations indicate shared characteristics among the four halo nuclei analyzed. This research was funded by the Grant-in-Aid for Scientific Research (No. 08640309) from MEXT Japan. \n\n1 Introduction  \nHalo nuclei are loosely bound structures whose wave functions can extend several hundred femtometers or more beyond the atomic surface. They were initially detected as very shallow resonances in elastic scattering experiments, but they can now be produced directly in fragmentation reactions. Following this, numerous experimental investigations have been conducted on various aspects of halo nuclei, such as electromagnetic transitions and dissolution mechanisms. To theoretically understand the nature of halo nuclei, it is essential to describe the behavior of the wave function both inside and outside the nucleus. For this purpose, we solve the Schrödinger equation using realistic nuclear potentials. We apply a method developed previously that involves expanding the single-particle wave functions in terms of harmonic oscillator basis states, and the resulting matrix elements are numerically evaluated using Gaussian quadratures. Regarding the nuclear potentials, we employ the Volkov and Paris potentials. The Volkov potential provides a more accurate representation of the ground state properties of light nuclei up to A = 10, while the Paris potential effectively reproduces the binding energy of 4He. \n\n2 Results and Discussion  \nFirst, we consider the case of 11Li. Figure 1 illustrates the calculated density distribution along with the associated root mean square distance, Rrms(A). Here, we have taken into account all relevant factors.",
        "ori-fast-z-score": -0.5853694070049635,
        "water-fast-z-score": 7.049123409692188,
        "rewrite-fast-z-score": 1.3522468075656264
    },
    {
        "original_text": "We present results on metal and molecular gas cooling in cosmological hydrodynamic simulations with the Enzo code, focusing on the effects of different numerical schemes for solving the energy equation (SPH vs. grid-based) as well as varying physical prescriptions for star formation feedback. We find that SPH codes tend to overestimate the amount of cold gas at high redshifts compared to grid-based methods due to artificial viscosity heating. Feedback models which include galactic winds are able to suppress this effect by removing low entropy material from galaxies. However, we also show that these wind models can lead to an underestimation of the total mass fraction of cool gas if they remove too much hot halo gas around massive halos. Overall our results suggest that current state-of-the-art galaxy formation models produce reasonable estimates for the global properties of the warm-hot intergalactic medium but may still be missing important physics related to the detailed distribution of metals within individual galaxies.",
        "watermark_text": "We report findings on metal and molecular vapor cooling in cosmological hydrodynamic simulations with the Enzo coding , concentrating on the effects of different numerical schemes for solving the electricity equation ( SPH vs . grid - based ) as well as varying mechanical prescriptions for star formation feedback . We see that SPH codes tend to overestimate the quantity of cold energy at high redshifts compared to grid - based methods thanks to artificial viscosity warming .Feedback models which use galactic winds are able to suppress this effect by removing lowered entropy material from stars . However , we also demonstrate that these wind predictions can lead to an underestimation of the total mass fraction of cold gas if they eliminate too much warm halo dust surrounding massive halos .Overall our findings confirm that current state - of - the - art star formation models generate reasonable estimates for the global properties of the warm - warm intergalactic medium but might nevertheless be missing crucial physics related to the detailed distribution of metals within individual stars .",
        "rewrite_text": "We present our findings on metal and molecular vapor cooling from cosmological hydrodynamic simulations conducted with the Enzo code, focusing on the impact of different numerical methods for solving the electricity equation (SPH versus grid-based approaches) and the effects of various mechanical models for star formation feedback. Our results indicate that SPH codes tend to overpredict the amount of cold energy at high redshifts relative to grid-based methods, primarily due to the effects of artificial viscosity, which induces warming. Feedback models that incorporate galactic winds are effective in mitigating this issue by expelling low-entropy material from stars. However, we also illustrate that these wind models can inadvertently underestimate the total mass fraction of cold gas if they remove excessive amounts of warm halo dust surrounding large halos. Overall, our results support the notion that contemporary star formation models provide reasonable estimates for the global characteristics of the warm intergalactic medium, yet they may overlook essential physics related to the precise distribution of metals within individual stars.",
        "ori-fast-z-score": -0.5241424183609592,
        "water-fast-z-score": 6.046918007655169,
        "rewrite-fast-z-score": 2.0851441405707476
    },
    {
        "original_text": "In this work, we consider decode-and forward (DF) cooperation among nodes in wireless networks and propose an optimal routing algorithm to maximize the network throughput by jointly optimizing source-to-destination routes as well as cooperative relay paths. We formulate the problem into mixed integer linear programming (MILP), which is NP-hard due to its combinatorial nature. To solve it efficiently, we develop two heuristic algorithms with polynomial time complexity. The first one uses a greedy approach that iteratively selects the best route between each pair of source destination pairs until all flows are routed. In order to further improve the performance, we also design another algorithm using simulated annealing technique. Extensive simulation results show that our proposed algorithms can achieve significant improvement over existing schemes. Index Terms- Cooperative communication, Optimum routing, Simulated annealing, Greedy algorithm. I. INTRODUCTIO N Cooperative communications have been widely studied recently because they provide higher data rates or longer transmission ranges than conventional non-cooperative systems  1  . In particular, decodeand-forward (DF) has attracted much attention since it does not require any extra power consumption at relays  2  .\nHowever, DF-based cooperative transmissions suffer from high end-to-end delay caused by multiple hops involved in forwarding packets  3  , especially when there exist many sources and destinations  4  . Therefore, how to find efficient cooperative relay paths becomes critical to reduce the overall end-to-end delay while maintaining good system performance  5  -  8  .",
        "watermark_text": "In this research , we investigate decode - and forward ( DF ) cooperation among nodes in wireless networks and suggest an efficient routing algorithm to maximize the network throughput by jointly optimizing source - to - destination lanes as well as cooperative relay routes . We formulate the question into mixed integer linear programming ( MILP ) , which is NP - hard thanks to its combinatorial nature .To solve it easily , we develop two heuristic algorithms with polynomial time complexity . The first first uses a greedy approach that iteratively selects the best path between each couple of source destination pairs until all flows are diverted .In try to further enhance the performance , we also design another scheme using modeled annealing technique . Extensive model results show that our proposed methods can attain tremendous improvement over existing strategies .Index Terms - Cooperative communication , Optimum routing , Simulated annealing , Greedy method . I . INTRODUCTIO N Cooperative networks have been widely examined lately because they deliver higher data levels or faster transmission ranges than conventional un - cooperative systems 1 .In particular , decodeand - forward ( DF ) has garnered considerable notice since it does not require any additional power consumption at relays 2 . However , DF - based cooperative signals suffer from high end - to - end delay due by many hops engaged in forwarding packets 3 , particularly when there exist many sources and destinations 4 .Therefore , how to find effective mutual relay routes seems vital to reduce the overall end - to - end delay while maintaining good system performance 5 - 8 .",
        "rewrite_text": "In this study, we explore decode-and-forward (DF) cooperation among nodes in wireless networks and propose an effective routing algorithm aimed at maximizing network throughput by optimizing both source-to-destination paths and cooperative relay routes. We formulate this problem as a mixed integer linear programming (MILP) challenge, which is NP-hard due to its combinatorial characteristics. To simplify the solution process, we develop two heuristic algorithms with polynomial time complexity. The first algorithm employs a greedy approach, which iteratively selects the optimal path between each pair of source and destination until all flows are rerouted. To further enhance performance, we also introduce a second method that utilizes a simulated annealing technique. Comprehensive experimental results demonstrate that our proposed methods significantly outperform existing strategies. \n\n**Index Terms** - Cooperative communication, Optimal routing, Simulated annealing, Greedy method. \n\n**I. INTRODUCTION** Cooperative networks have gained significant attention recently as they provide higher data rates and extended transmission ranges compared to traditional uncooperative systems. In particular, decode-and-forward (DF) has attracted considerable interest because it does not require additional power consumption at relay nodes. However, DF-based cooperative signals can experience high end-to-end delays due to the multiple hops involved in packet forwarding, especially in scenarios with numerous sources and destinations. Thus, identifying effective relay routes is crucial for minimizing overall end-to-end delay while ensuring robust system performance.",
        "ori-fast-z-score": -0.7924058156930615,
        "water-fast-z-score": 7.307742522502678,
        "rewrite-fast-z-score": 0.08944271909999159
    },
    {
        "original_text": "In this paper, we propose an autonomous distributed admission control scheme to improve the performance and fairness in wireless local area networks (WLANs). The proposed scheme is based on the concept that each station maintains its own queue length information by using the packet inter-arrival time at the physical layer. In addition, it uses the number of active stations as well as their transmission rates to determine whether or not new connections are admitted into the network. We show through simulation results that our scheme can achieve better throughput than existing schemes while maintaining good fairness among competing stations. Keywords: Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. 1 Introduction With the rapid development of mobile computing devices such as laptops, PDAs, smart phones etc., there has been growing interest in providing high quality services over wireless local area networks (WLANS)  1  . However, due to limited bandwidth resources available in WLANs, efficient resource management becomes crucially important  2  .\nThe most widely used medium access control protocol in current commercial WLAN products is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both contention-based channel access mechanism called Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA)  3  , and contention-free service via Point Coordinated Function (PCF)  4  . Although CSMA/CA allows multiple stations to share the same radio channel simultaneously without any centralized coordination, it suffers from poor system performance when the traffic load increases  5  . This problem is mainly caused by the hidden terminal effect  6  where two nodes may transmit packets to one another simultaneously causing collisions. To alleviate these problems, several approaches have been proposed  7 -10  . Among them, the authors in  8  introduced a simple but effective method known as Virtual Reservation Channel (VRC) to reduce the probability of collision between data frames transmitted by different stations. They also presented a modified version of VRC  9  to further enhance the performance of CSMA/CA under heavy loads. However, all these works assume that the number of active stations within the",
        "watermark_text": "In this paper , we propose an autonomous spread entry system scheme to ensure the performance and fairness in mobile regional region systems ( WLANs ) . The proposed system is based on the idea that each broadcaster keeps its own queue length information by using the message inter - arrival rate at the physical layer .In addition , it utilizes the quantity of active stations as well as their transmission rates to estimate whether or not fresh connections are admitted into the channel . We see through simulation data that our scheme can attain better throughput than existing schemes while maintaining good fairness among competing networks .Keywords : Wireless Local Area Networks , Packet Inter - Arrival Time , Fairness , Throughput Improvement . 1 Introduction With the fast development of mobile computing devices such as laptops , PDAs , smart phones etc . , there has been growing interest in providing superior grade services over wireless regional area networks ( WLANS ) 1 .However , owing to limited bandwidth assets available in WLANs , efficient resource control becomes crucially vital 2 . The most commonly used medium access control protocol in current consumer WLAN applications is the IEEE 802 . 11 Distributed Coordination Function ( DCF ) , which offers both contention - based network entry method called Carrier Sense Multiple Access with Collision Avoidance ( CSMA / CA ) 3 , and contention - free service via Point Coordinated Function ( PCF ) 4 .Although CSMA / CA enables many affiliates to share the same radio channel simultaneously without any centralized coordination , it suffers from poor system performance when the traffic burden rises 5 . This problem is mainly caused by the concealed terminal effect 6 where two nodes may transmit streams to one another simultaneously creating collisions .To reduce these problems , various approaches have been proposed 7 - 10 . Among them , the published in 8 developed a simple but effective technology called as Virtual Reservation Channel ( VRC ) to reduce the probability of collision between information frames transmitted by separate stations .They even presented a altered version of VRC 9 to further enhance the performance of CSMA / CA under heavy loads . However , all these works assume that the number of active stations within the",
        "rewrite_text": "In this paper, we introduce an autonomous spread entry system scheme aimed at enhancing performance and fairness in mobile regional area networks (WLANs). Our proposed system relies on the concept that each broadcaster monitors its own queue length by utilizing the message inter-arrival rate at the physical layer. Additionally, it assesses the number of active stations and their transmission rates to determine whether new connections can be effectively admitted to the channel. Simulation results indicate that our approach achieves superior throughput compared to existing methods while ensuring equitable access among competing networks. \n\n**Keywords:** Wireless Local Area Networks, Packet Inter-Arrival Time, Fairness, Throughput Improvement. \n\n**1 Introduction** \n\nWith the rapid advancement of mobile computing devices, including laptops, PDAs, and smartphones, there is increasing interest in delivering high-quality services over wireless local area networks (WLANs). However, due to the limited bandwidth resources available in WLANs, efficient resource management becomes essential. Currently, the most widely used medium access control protocol in consumer WLAN applications is the IEEE 802.11 Distributed Coordination Function (DCF), which provides both a contention-based access method known as Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) and a contention-free service via Point Coordination Function (PCF). \n\nWhile CSMA/CA allows many users to simultaneously share the same radio channel without centralized control, it suffers from degraded performance under heavy traffic conditions. This issue is primarily attributed to the concealed terminal problem, in which two nodes can transmit to one another at the same time, leading to collisions. Various strategies have been proposed to mitigate these challenges. Among them, a notable approach published in [8] introduced a straightforward yet effective technique called Virtual Reservation Channel (VRC), which aims to lower the likelihood of collisions between data frames sent by different stations. Moreover, an enhanced version of VRC was presented in [9] to further improve CSMA/CA performance under high-load conditions. However, all these studies operate under the assumption that the number of active stations within the network is constant.",
        "ori-fast-z-score": -1.0999438818457405,
        "water-fast-z-score": 8.485281374238571,
        "rewrite-fast-z-score": 0.722897396012249
    },
    {
        "original_text": "We consider an entropy penalization method in order to estimate the density function f of a random variable X with values on Rd . We prove that, under some conditions, this estimator is strongly consistent and we study its asymptotic normality. The main tool used here is the exponential decay of correlations property satisfied by the stochastic process associated to our estimation procedure. This result allows us to obtain rates of convergence for the mean integrated squared error (MISE) between the true density f and its estimators. Finally, numerical experiments are performed in dimension 1 and 2. Keywords: Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality. Mathematics Subject Classification (2010): 60C05, 60F10, 62G20. 1 Introduction Let X be a real-valued random vector defined on a probability space (Ω , A , P). In many applications such as signal processing or econometrics, it may be interesting to recover the distribution law of X denoted by fX . For example, if one wants to detect changes in the statistical properties of X over time, then knowing fX will allow him/her to perform change-point detection tests  see e.g., Chen et al. (2013), Fryzlewicz & Subba Rao (2014) . However, recovering fX can be difficult because only n iid observations X1 , . . . , Xn of X are available. To overcome this difficulty, several authors have proposed to use nonparametric methods based on kernel smoothing techniques  see e.g. , Silverman (1981) , Wand & Jones (1995)  . More precisely, let K : R →  0, 1  be a given kernel function satisfying certain regularity assumptions which will be specified later. Then, the classical kernel density estimator of fX at x ∈ Rd is defined bŷ fbK (x) =",
        "watermark_text": "We consider an entropy penalization techniques in order to estimate the density function f of a random variable X with values on Rd . We prove that , under some conditions , this estimator is strongly consistent and we study its asymptotic normality .The main technique useful here is the exponential decay of correlations property satisfied by the stochastic mechanism associated to our estimation method . This result allows us to obtain rates of convergence for the mean integrated squared error ( MISE ) between the true density f and its estimators .Finally , numerical studies are performed in dimension 1 and 2 . Keywords : Density estimation , Entropic penalty , Exponential decay of correlations , Asymptotic normality .Mathematics Subject Classification ( 2010 ) : 60C05 , 60F10 , 62G20 . 1 Introduction Let X be a real - valued random function characterized on a probability space ( Ω , A , P ) .In many applications such as signal filtering or econometrics , it could be interesting to extract the distribution law of X denoted by fX . For instance , if one wants to identify changes in the statistical characteristics of X over time , then understanding fX will provide him / her to conduct change - point detection tests see e . g . , Chen et al .( 2013 ) , Fryzlewicz & Subba Rao ( 2014 ) . However , returning fX can be challenging because only n iid observations X1 , ..., Xn of X are available . To solve this challenge , various published have proposed to use nonparametric techniques based on kernel smoothing methods see e . g ., Silverman ( 1981 ) , Wand & Jones ( 1995 ) . More specifically , let K : R → 0 , 1 be a given kernel map satisfying particular regularity assumptions which will be specified later .Then , the classical kernel density estimator of fX at x ∈ Rd is given bŷ fbK ( x ) =",
        "rewrite_text": "We investigate entropy penalization techniques for estimating the density function \\( f \\) of a random variable \\( X \\) that takes values in \\( \\mathbb{R}^d \\). We establish that, under certain conditions, this estimator demonstrates strong consistency and analyze its asymptotic normality. A key aspect of our approach is the exponential decay of correlations property inherent in the stochastic mechanism tied to our estimation method. This finding enables us to derive rates of convergence for the mean integrated squared error (MISE) between the true density \\( f \\) and its estimators. Additionally, we conduct numerical studies in one- and two-dimensional settings. \n\n**Keywords:** Density estimation, Entropic penalty, Exponential decay of correlations, Asymptotic normality.  \n**Mathematics Subject Classification (2010):** 60C05, 60F10, 62G20.  \n\n**1 Introduction**  \nLet \\( X \\) be a real-valued random function defined on a probability space \\( (\\Omega, \\mathcal{A}, P) \\). In various applications, such as signal filtering and econometrics, it is often desirable to extract the distribution of \\( X \\), denoted as \\( f_X \\). For example, when attempting to identify shifts in the statistical properties of \\( X \\) over time, a thorough understanding of \\( f_X \\) is crucial for conducting change-point detection tests (see, e.g., Chen et al. (2013), Fryzlewicz & Subba Rao (2014)). However, estimating \\( f_X \\) poses challenges since only \\( n \\) independent, identically distributed observations \\( X_1, \\ldots, X_n \\) of \\( X \\) are available. To address this issue, various studies have proposed nonparametric methods utilizing kernel smoothing techniques (see, e.g., Silverman (1981), Wand & Jones (1995)). Specifically, let \\( K: \\mathbb{R} \\to [0, 1] \\) be a kernel function that satisfies certain regularity conditions, which will be detailed later. The classical kernel density estimator \\( \\widehat{f}_K(x) \\) for \\( f_X \\) at a point \\( x \\in \\mathbb{R}^d \\) is formulated as follows:",
        "ori-fast-z-score": -0.5619514869490164,
        "water-fast-z-score": 5.315277406969789,
        "rewrite-fast-z-score": 2.008316044185609
    },
    {
        "original_text": "We present spectropolarimetric observations made with the Solar Optical Telescope (SOT) on board Hinode, which show that the magnetic field strength inferred from Stokes V profiles is systematically higher than those obtained by using the Zeeman splitting method for both the Ca II 8498 Å line and the Ca II 8542 Å line. The difference between these two methods increases as we go to smaller spatial scales. We also find that the magnetic fields are more inclined towards the solar surface at small spatial scales compared to larger ones. These results suggest that there may be some unknown physical processes affecting the formation of Stokes V profiles at small spatial scales. This work was supported by JSPS KAKENHI Grant-in-Aid for Scientific Research No. 16340040 . \nIntroduction\n\nThe solar atmosphere consists of various structures such as sunspots, pores, plages, prominences etc., where different physical phenomena occur. In order to understand how these phenomena take place, it is important to study their properties individually. However, this task has been difficult because most of them have very fine structure and they often overlap each other spatially. To overcome this difficulty, many observational studies have been carried out recently using high-resolution instruments such as the Swedish 1-m Solar Telescope (SST), the New Solar Telescope (NST), the Advanced Technology Solar Telescope (ATST), and the Solar Dynamics Observatory (SDO). Among others, the Hinode satellite launched in 2006 provides us with unprecedentedly high-quality data thanks to its sophisticated instrumentation including the Spectro-Polarimeter (SP) (Lites et al. (2001) ) and the Helioseismic and Magnetic Imager (HMI) (Schou et al. (2010) ), which enable us to investigate the solar photosphere down to subarcsecond resolution. Using these data sets, several authors studied the photospheric magnetic fields (e.g., Ichimoto et al. (2007) , Ishikawa & Tsuneta (2008) , Kitai et al. (2009 ), Orozco Suárez et al. (2010 , Sheminova et al. (2011))",
        "watermark_text": "We present spectropolarimetric experiments done with the Solar Optical Telescope ( SOT ) on board Hinode , which show that the magnetic force power inferred from Stokes V profiles is systematically greater than those achieved by using the Zeeman splitting method for both the Ca II 8498 Å line and the Ca II 8542 Å line . The difference between these two models increases as we went to smaller spatial dimensions .We additionally find that the magnetic waves are more oriented towards the sun surface at small spatial dimensions relative to larger ones . These conclusions show that there may be some undisclosed physical processes controlling the formation of Stokes V profiles at small spatial scales .This project was supported by JSPS KAKENHI Grant - in - Aid for Scientific Research No . 16340040 .Introduction The solar atmosphere includes of several buildings such as sunspots , pores , plages , prominences etc . , where various physical phenomena arise . In order to comprehend how these phenomena took place , it is important to study their characteristics individually .However , this job has been difficult because most of them have very fine structure and they frequently overlap each other spatially . To solve this obstacle , many observational research have been carried out recently utilizing large - resolution instruments such as the Swedish 1 - m Solar Telescope ( SST ) , the New Solar Telescope ( NST ) , the Advanced Technology Solar Telescope ( ATST ) , and the Solar Dynamics Observatory ( SDO ) .Among others , the Hinode satellite launched in 2006 offers us with unprecedentedly high - grade statistics courtesy to its sophisticated instrumentation including the Spectro - Polarimeter ( SP ) ( Lites et al . ( 2001 ) ) and the Helioseismic and Magnetic Imager ( HMI ) ( Schou et al .( 2010 ) ) , which enable us to examine the thermal photosphere down to subarcsecond resolution . Using these information sets , various scientists examined the photospheric magnetic fields ( e . g . , Ichimoto et al .( 2007 ) , Ishikawa & Tsuneta ( 2008 ) , Kitai et al . ( 2009 ) , Orozco Suárez et al .( 2010 , Sheminova et al . ( 2011 ) )",
        "rewrite_text": "We present spectropolarimetric experiments conducted with the Solar Optical Telescope (SOT) aboard Hinode, which reveal that the magnetic force power derived from Stokes V profiles consistently exceeds that obtained using the Zeeman splitting method for both the Ca II 8498 Å and Ca II 8542 Å lines. This discrepancy between the two methods increases as we examine smaller spatial scales. Furthermore, we observe that magnetic waves are more aligned with the solar surface in smaller spatial dimensions compared to larger ones. These findings suggest that there may be unknown physical processes influencing the formation of Stokes V profiles at fine spatial resolutions. This research was supported by JSPS KAKENHI Grant-in-Aid for Scientific Research No. 16340040.\n\n**Introduction:** The solar atmosphere consists of various structures such as sunspots, pores, plages, and prominences, within which different physical phenomena occur. To understand how these phenomena arise, it is essential to study their characteristics in isolation. However, this task is challenging due to their intricate structures and their tendency to overlap spatially. To address this issue, many observational studies have recently been conducted using high-resolution instruments, including the Swedish 1-m Solar Telescope (SST), the New Solar Telescope (NST), the Advanced Technology Solar Telescope (ATST), and the Solar Dynamics Observatory (SDO). Among these, the Hinode satellite, launched in 2006, provides exceptionally high-quality data thanks to its advanced instruments, such as the Spectro-Polarimeter (SP) (Lites et al. 2001) and the Helioseismic and Magnetic Imager (HMI) (Schou et al. 2010), enabling analysis of the thermal photosphere at subarcsecond resolution. Utilizing these datasets, several researchers have investigated photospheric magnetic fields (e.g., Ichimoto et al. 2007, Ishikawa & Tsuneta 2008, Kitai et al. 2009, Orozco Suárez et al. 2010, Sheminova et al. 2011).",
        "ori-fast-z-score": -1.5454545454545454,
        "water-fast-z-score": 6.454545454545454,
        "rewrite-fast-z-score": 0.4703604341917986
    },
    {
        "original_text": "We reply to the comments by A. M. Braden and J. P. Dowling in their Comment  1  . We show that our results are robust against different choices for the fitting range, and we discuss how this is related to the choice of the initial state wave function used in Ref.  2  .\nPACS numbers: 11.10.Wx, 12.20.Ds, 13.25.Gv Quantum tunneling has been studied extensively since its discovery more than half century ago  3  , but it remains an active area of research  4  . In particular, recent experiments have shown that quantum tunneling can be observed even at room temperature  5  . Theoretically, there exist two types of tunneling processes  6  : extrinsic tunneling which occurs when particles move through barriers created by external potentials  7, 8  ; and intrinsic tunneling where particles tunnel between degenerate states without any potential barrier  9  . Intrinsic tunneling plays important roles in many physical systems such as molecular vibrations  10  , nuclear fission  11  , Josephson junctions  12  , Bose-Einstein condensates  13  , and semiconductor superlattices  14  . However, distinguishing intrinsic tunneling from other effects experimentally still poses great challenges  15  .",
        "watermark_text": "We respond to the remarks by A . M . Braden and J . P . Dowling in their Comment 1 . We see that our findings are robust against different decisions for the fitting range , and we explain how this is related to the selection of the first state wave system employed in Ref .2 . PACS scores : 11 . 10 . Wx , 12 . 20 . Ds , 13 . 25 . Gv Quantum tunneling has been studied thoroughly since its discovery more than quarter century ago 3 , but it remains an active area of research 4 .In particular , recent experiments have shown that molecular tunneling can be experienced even at room temperature 5 . Theoretically , there exist two forms of tunneling processes 6 : extrinsic tunneling which occurs when molecules travel through barriers created by external potentials 7 , 8 ; and intrinsic tunneling where ions tunnel between degenerate states without any potential barrier 9 .Intrinsic tunneling performs important roles in different physical structures such as chemical vibrations 10 , nuclear fission 11 , Josephson junctions 12 , Bose - Einstein condensates 13 , and semiconductor superlattices 14 . However , distinguishing intrinsic tunneling from other effects experimentally nevertheless remains big challenges 15 .",
        "rewrite_text": "We address the comments made by A. M. Braden and J. P. Dowling in their Commentary 1. Our analysis indicates that our results are resilient under various choices for the fitting range, and we clarify how this is connected to the first state wave system chosen in Reference 2. PACS scores: 11.10.Wx, 12.20.Ds, 13.25.Gv. Quantum tunneling has been extensively investigated since its discovery over twenty-five years ago, yet it continues to be a vibrant field of study. Recent experiments have particularly highlighted that molecular tunneling can occur even at room temperature. Theoretically, there are two distinct types of tunneling processes: extrinsic tunneling, which involves molecules overcoming barriers created by external potentials, and intrinsic tunneling, where ions tunnel between degenerate states without encountering any potential barriers. Intrinsic tunneling plays significant roles in various physical phenomena, including chemical vibrations, nuclear fission, Josephson junctions, Bose-Einstein condensates, and semiconductor superlattices. Nonetheless, experimentally differentiating intrinsic tunneling from other effects remains a considerable challenge.",
        "ori-fast-z-score": -0.48507125007266594,
        "water-fast-z-score": 5.741963884746346,
        "rewrite-fast-z-score": 1.25
    },
    {
        "original_text": "We present near-infrared (NIR) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile, and compare them to previous optical studies by Smith et al. (2000) . The NIR data reveal new details on the structure of the nebular shell surrounding the open cluster Trumpler 16. We find that the brightest part of the shell is located between two dark lanes which are probably caused by dust extinction. In addition we detect several small knots embedded within the shell. These knots may be remnants of massive stars or protostars formed during an earlier phase of star formation activity in this region. Finally, we identify a number of Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our results show that the Carina Nebulae is still actively forming stars today. This work was supported by the European Southern Observatory under programme ID 072.C-0488(B).",
        "watermark_text": "We take near - infrared ( NIR ) observations of the central region of the Carina Nebulae obtained with ISAAC at the VLT in Chile , and link them to previous optical images by Smith et al . ( 2000 ) .The NIR data reveal novel features on the composition of the nebular shell surrounding the open cluster Trumpler 16 . We see that the brightest part of the shell is situated between two darkened paths which are probably created by dust extinction .In addition we find various small knots embedded within the shell . These threads may be remnants of large planets or protostars produced during an previous process of star formation activity in this area .Finally , we identify a number of Herbig - Haro objects identified with the open spaces Trumpler 14 and 15 . Our results show that the Carina Nebulae is already actively creating stars today .This project was supported by the European Southern Observatory under series ID 072 . C - 0488 ( B ) .",
        "rewrite_text": "We conducted near-infrared (NIR) observations of the central region of the Carina Nebula using the ISAAC instrument at the VLT in Chile, connecting our findings with earlier optical images from Smith et al. (2000). The NIR data unveil new insights into the composition of the nebular shell surrounding the open cluster Trumpler 16. Notably, the brightest section of the shell is located between two darker corridors likely caused by dust extinction. Additionally, we discovered several small knots within the shell that may be remnants of large planets or protostars formed during previous star formation activities in this region. Furthermore, we identified multiple Herbig-Haro objects associated with the open clusters Trumpler 14 and 15. Our findings indicate that the Carina Nebula is actively engaged in star formation today. This project received support from the European Southern Observatory under series ID 072.C-0488(B).",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 6.154574548966636,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "We present the results of our numerical simulations on planetary formation in debris disks with different masses, compositions, and structures. We find that planets can form efficiently even if they are surrounded by massive dusty disks (up to 10 times more mass than the minimum-mass solar nebula). The disk structure is found to have little effect on planet formation as long as it has enough surface density at large radii for giant impacts to occur. Planets formed in such disks tend to be less massive but larger compared to those formed in standard protoplanetary disks. This may explain why some extrasolar planets appear to be too small while others seem to be overly massive. Our results also suggest that there should exist many terrestrial planets around Sun-like stars which are not detected yet because their orbits are highly inclined or eccentric. \n \n Keywords: Planet formation; Debris disk; Giant impacts; Embryo accretion",
        "watermark_text": "We present the results of our numerical simulations on planetary formation in debris clusters with various masses , compositions , and structures . We see that planets can form smoothly even if they are surrounded by massive dusty disks ( up to 10 twice more mass than the highest - density solar nebula ) .The disk composition is found to have minimal influence on planet development as long as it has sufficient surface volume at large radii for huge impacts to occur . Planets formed in such disks tend to be less massive but bigger compared to those formed in standard protoplanetary disks .This might explain why some extrasolar stars seems to be too small while many appears to be overly huge . Our results also suggest that there should exist many terrestrial worlds around Sun - like stars which are not observed yet because their orbits are extremely inclined or eccentric .Keywords : Planet structure ; Debris disk ; Giant impacts ; Embryo accretion",
        "rewrite_text": "We report the findings from our numerical simulations on planetary formation within debris clusters that vary in mass, composition, and structure. Our results indicate that planets can form effectively even in the presence of substantial dusty disks, with masses reaching up to twice that of the densest solar nebula. The composition of these disks appears to have a limited effect on planetary development, provided that there is enough surface volume at greater distances to facilitate significant impacts. Planets that arise in such environments tend to be larger but less massive than those formed in conventional protoplanetary disks. This could account for the observed phenomenon where some extrasolar stars appear too small while others seem excessively large. Additionally, our findings imply that numerous terrestrial planets may exist around Sun-like stars, yet remain undetected due to their highly inclined or eccentric orbits. \n\nKeywords: Planet structure; Debris disk; Giant impacts; Embryo accretion.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 5.505977612893481,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "We study the vulnerability of interdependent networks under cascading failures, where nodes are subject to random attacks and their neighbors can fail as well due to loss of connectivity. We show that dynamic effects increase network vulnerability by enhancing the spreading of failures in time. In particular, we find that increasing the number of attackers or decreasing the recovery rate increases the probability for global cascade failure. Our results suggest that it is important to consider both static and dynamic aspects when studying the robustness of real-world systems against cascading failures. Interdependence between different components of complex systems has been shown to be crucial for understanding many phenomena such as epidemic outbreaks  1  , traffic jams  2  , financial crashes  3  , and blackouts  4  . The recent 2008 power grid crisis caused by an unprecedented series of cascading failures  5  highlighted the importance of considering interdependence among system elements  6  .\nIn this work, we focus on interdependent networks  7, 8  , which consist of two types of nodes: source (S) and target (T). Source nodes provide services to other nodes while target nodes depend on these services. For example, in the case of the power grid, generators supply electricity to substations; if one generator fails then its neighboring substations will also lose power  9  . Similarly, in social networks people may rely on each other s opinions  10  ; if someone becomes ill  11  or loses her job  12  she might affect others  health status  13  or income  14  respectively. Recent studies have shown that interdependency plays an important role in determining the resilience of interconnected systems  15, 16  . However, most previous works focused only on static properties  17  , i.e., they assumed that all links remain stable over time  18  . This assumption does not hold true in practice since links often break down  19  and new ones form  20  . Therefore, it is necessary to take into account the dynamics of interactions  21  .",
        "watermark_text": "We research the vulnerability of interdependent networks under cascading disasters , where nodes are subject to random attacks and their relatives can fail as well due to lack of communication . We see that dynamic effects improve network vulnerability by increased the spreading of failures in time .In particular , we find that expanding the quantity of attackers or decreasing the return speed increases the probability for global cascade failure . Our results show that it is important to consider both static and dynamic elements when examining the robustness of real - time systems against cascading disasters .Interdependence between various components of complex systems has been shown to be crucial for studying many phenomena such as epidemic outbreaks 1 , road jams 2 , financial crashes 3 , and blackouts 4 . The recent 2008 power system disaster caused by an incredible series of cascading problems 5 noted the importance of considering interdependence among system components 6 .In this research , we focus on interdependent networks 7 , 8 , which consist of two forms of nodes : source ( S ) and target ( T ) . Source nodes provide functions to other connections while target nodes depend on these services .For instance , in the case of the power system , generators supply energy to substations ; if one generator fails then its adjacent substations will also lose power 9 . Similarly , in social systems people may depend on each other s views 10 ; if someone causes illness 11 or losing her employment 12 she might impact others hospital position 13 or income 14 respectively .Recent research have shown that interdependency plays an important role in establishing the resilience of interconnected networks 15 , 16 . However , most prior papers focused only on static properties 17 , i . e . , they predicted that all links remain stable over time 18 .This assumption does not stand true in practice since links sometimes break down 19 and new ones create 20 . Therefore , it is required to take into consideration the dynamics of relationships 21 .",
        "rewrite_text": "We investigate the vulnerability of interdependent networks in the context of cascading disasters, where nodes face random attacks and related nodes may also fail due to breakdowns in communication. Our findings indicate that dynamic effects exacerbate network vulnerability by facilitating the temporal spread of failures. Specifically, we observed that increasing the number of attackers or reducing recovery rates heightens the likelihood of widespread cascade failures. This underscores the importance of accounting for both static and dynamic factors when assessing the resilience of real-time systems to cascading disasters. The interdependence among various components of complex systems is critical for understanding phenomena such as epidemic outbreaks, traffic congestion, financial crises, and power outages. The significant power system failure in 2008, triggered by a series of interconnected issues, highlighted the necessity of considering interdependencies among system components. In our research, we focus on interdependent networks characterized by two types of nodes: source nodes (S) that provide services to other connections, and target nodes (T) that rely on these services. For example, in a power system, generators supply energy to substations, so if one generator fails, adjacent substations will also experience outages. Similarly, in social networks, individuals may rely on one another’s perspectives; if one person faces illness or unemployment, it can adversely affect the hospital capabilities or income of others. Recent studies have emphasized the critical role of interdependence in enhancing the resilience of interconnected networks. However, most previous research has concentrated solely on static properties, assuming that all links remain constant over time. This assumption does not hold true in reality, as connections can fail and new ones can form. Thus, it is essential to consider the dynamics of these relationships.",
        "ori-fast-z-score": 0.48038446141526137,
        "water-fast-z-score": 9.177999171377655,
        "rewrite-fast-z-score": 1.2632278815997784
    },
    {
        "original_text": "We study the global polarization of quark gluon plasma (QGP) produced in non central heavy-ion collisions by using an effective chiral model with vector and axial-vector mesons as well as quarks and anti-quarks. We find that the global polarization is mainly determined by the initial angular momentum carried by the colliding nuclei, which can be estimated through the Glauber model. The magnitude of the global polarization decreases rapidly when the collision energy increases due to the increasing number of particles involved in the reaction. Our results show that the global polarization may reach about 10% for RHIC energies but it will decrease significantly if one goes up to LHC energies. \n \n Introduction \n \n In recent years there has been growing interest on studying the global polarization of quark-gluon plasma(QGP), especially its dependence on the collision energy  1–3  . It was found that the global polarization could reach about 20% for RHIC energies  4  , while it would drop down to less than 1% for LHC energies  5  . \n \n This phenomenon is closely related to the initial angular momenta carried by the colliding nuclei; therefore, it provides us a new way to probe the nuclear structure  6  . On the other hand, since the global polarization is also sensitive to the temperature evolution  7, 8  , it might provide some information on the thermalization process of QGP  9  .",
        "watermark_text": "We research the global polarization of quark gluon plasma ( QGP ) produced in non central heavy - ion collisions by using an efficient chiral description with vector and axial - vector mesons as well as quarks and anti - quarks . We see that the global polarization is mainly decided by the first angular velocity carried by the colliding nuclei , which can be estimated through the Glauber model .The magnitude of the global polarization decreases quickly when the interaction power grows owing to the increasing quantity of molecules active in the reaction . Our results show that the global polarization might reach about 10 % for RHIC energies but it will decrease greatly if one goes up to LHC energies .Introduction In recent years there has been growing interest on studying the global polarization of quark - gluon gas ( QGP ) , particularly its dependence on the interaction power 1 – 3 . It was shown that the global polarization could reach about 20 % for RHIC energies 4 , while it would rise down to little than 1 % for LHC energies 5 .This phenomenon is closely related to the first angular momenta carried by the colliding nuclei ; therefore , it gives us a new place to probe the atomic system 6 . On the other hand , since the global polarization is also sensitive to the temperature evolution 7 , 8 , it could give some info on the thermalization process of QGP 9 .",
        "rewrite_text": "We investigate the global polarization of quark-gluon plasma (QGP) generated in non-central heavy-ion collisions using an efficient chiral model that incorporates vector and axial-vector mesons, as well as quarks and antiquarks. Our findings indicate that the global polarization is primarily influenced by the initial angular velocity of the colliding nuclei, which can be estimated using the Glauber model. As the interaction strength increases, the magnitude of global polarization rapidly decreases due to the larger number of active particles involved in the reaction. Our results suggest that global polarization could reach approximately 10% for RHIC energies, but it significantly diminishes at LHC energies. \n\nIn recent years, there has been increased interest in examining the global polarization of the quark-gluon gas (QGP), particularly its relationship with interaction strength. Studies have indicated that global polarization may reach about 20% for RHIC energies, while it drops to below 1% at LHC energies. This phenomenon is intricately linked to the initial angular momentum of the colliding nuclei, providing a novel avenue for exploring the characteristics of the atomic system. Additionally, since global polarization is sensitive to temperature changes, it can offer valuable insights into the thermalization process of QGP.",
        "ori-fast-z-score": 0.10369516947304253,
        "water-fast-z-score": 6.18852747755276,
        "rewrite-fast-z-score": 0.10482848367219183
    },
    {
        "original_text": "We have investigated the frequency distribution of semi-major axis (SMA) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical evolution models. We used data obtained by the Two Micron All Sky Survey (2MASS), which is complete down to Ks = 12 mag, corresponding to masses as low as 0.1 M⊙ at distances up to 1 kpc. The sample consists of 13,000 pairs selected using color-color criteria designed to select main-sequence stars. Using Monte Carlo simulations we found that our results are not affected significantly by incompleteness effects due to photometric errors or contamination by background galaxies. Our analysis shows that there exists an excess number of systems with SMA between 10 4 -10 5 AU compared to predictions based on standard cosmological models. This result suggests that either these systems were formed earlier than predicted by current theories or they may be primordial objects such as Population III remnants.",
        "watermark_text": "We have analyzed the frequency distribution of semi - major axis ( SMA ) for wide binaries with separations greater than 1000 AU in order to test cosmogonies and dynamical development predictions . We utilized information obtained by the Two Micron All Sky Survey ( 2MASS ) , which is complete down to Ks = 12 mag , equivalent to masses as low as 0 . 1 [UNK] at distances up to 1 kpc .The sample consists of 13 , 000 couples chosen using color - color factors created to select primary - sequence stars . Using Monte Carlo simulations we concluded that our findings are not affected substantially by incompleteness effects due to photometric failures or exposure by background galaxies .Our study shows that there exists an excess amount of components with SMA between 10 4 - 10 5 AU compared to expectations based on normal cosmological predictions . This result suggests that either these systems were created earlier than expected by current theories or they may be primordial objects such as Population III fragments .",
        "rewrite_text": "We have examined the frequency distribution of semi-major axes (SMA) for wide binaries with separations exceeding 1000 AU to evaluate predictions from cosmogonies and dynamical evolution models. Our analysis is based on data from the Two Micron All Sky Survey (2MASS), which is complete for objects down to Ks = 12 mag, corresponding to masses as low as 0.1 [UNK] within distances up to 1 kpc. The sample includes 13,000 pairs selected using color-color criteria designed to identify main sequence stars. Through Monte Carlo simulations, we determined that our results are not significantly influenced by incompleteness due to photometric errors or exposure from background galaxies. Our findings reveal a notable excess of components with SMA in the range of 10^4 to 10^5 AU compared to what standard cosmological predictions would suggest. This indicates that these systems may have formed earlier than current theories propose, or they could represent primordial objects, such as fragments from Population III.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "The aim of this work is to study the effect of different assembly procedures, as well as different contact laws between particles in powder compaction processes using computer simulations. The results show that the packing density depends strongly on the applied pressure during the assembly process. In addition, it was found that the densification behavior can be influenced by changing the contact law parameters. Finally, we have shown how the microstructure evolves with increasing pressure for two different types of particle contacts. Powder compaction has been studied extensively over many years due to its importance in various industrial applications such as pharmaceutical tablets or metal parts produced via hot pressing techniques  1  . However, despite these efforts there are still open questions regarding the underlying mechanisms governing the densification behavior under external loads  2  .\nIn order to understand the complex phenomena occurring inside compacted materials, several experimental studies were performed  3  , which revealed important information about the evolution of the microstructure  4  . Nevertheless, experiments cannot provide detailed insight into the local interactions taking place at the grain scale  5  . Therefore, numerical methods based on discrete element models (DEM)  6  became an alternative approach to investigate the mechanical properties of granular systems  7, 8  . These DEM-based approaches allow one to track individual grains within large assemblies and thus enable us to obtain valuable insights into the micromechanical response  9  . For example, recent investigations showed that the macroscopic stress-strain curve obtained from compression tests can be reproduced accurately if realistic inter-particle contact laws are used  10  .",
        "watermark_text": "The goal of this project is to study the impact of different assembly techniques , as also as different interaction laws between particles in powder compaction processes using computer simulations . The results show that the packing density depends strongly on the applied pressure during the assembly cycle .In addition , it was shown that the densification response can be altered by varying the contact law factors . Finally , we have shown how the microstructure evolves with expanding pressure for two different kinds of particle contacts .Powder compaction has been studied thoroughly over numerous years owing to its significance in different industrial applications such as drug tablets or metal parts produced via hot pressing methods 1 . However , despite these attempts there are still close problems regarding the fundamental pathways governing the densification response under external loads 2 .In try to comprehend the complex dynamics occurring inside compacted materials , various experimental studies were performed 3 , which revealed important information about the evolution of the microstructure 4 . Nevertheless , researchers cannot offer precise insight into the local interactions taking place at the grain scale 5 .Therefore , numerical models relying on discrete element models ( DEM ) 6 became an alternative approach to examine the structural properties of granular structures 7 , 8 . These DEM - based methods provide one to track small particles within large assemblies and therefore facilitate us to obtain crucial insights into the micromechanical behavior 9 .For instance , recent studies showed that the macroscopic stress - strain model derived from compression experiments can be reproduced accurately if realistic inter - particle contact laws are using 10 .",
        "rewrite_text": "The objective of this project is to investigate the effects of various assembly techniques and different interaction laws between particles in powder compaction processes through computer simulations. The findings indicate that the packing density is significantly influenced by the pressure applied during the assembly cycle. Furthermore, the densification response can be modified by altering the factors related to the contact laws. Additionally, we have demonstrated how the microstructure evolves with increasing pressure for two distinct types of particle contacts. Powder compaction has been extensively researched over the years due to its importance in various industrial applications, such as the manufacturing of drug tablets and metal components produced through hot pressing methods. Despite these efforts, fundamental questions regarding the mechanisms that dictate the densification response to external loads remain unanswered. To better understand the intricate dynamics within compacted materials, several experimental studies have been conducted, which have provided valuable insights into microstructural evolution. However, researchers still face challenges in gaining precise understanding of the local interactions occurring at the grain scale. Consequently, numerical models based on discrete element modeling (DEM) have emerged as an alternative approach to exploring the structural properties of granular materials. These DEM-based techniques enable tracking of small particles within large assemblies, thus allowing for crucial insights into micromechanical behavior. For example, recent research has demonstrated that the macroscopic stress-strain behavior observed in compression experiments can be accurately reproduced using realistic inter-particle contact laws.",
        "ori-fast-z-score": 0.6713450866373513,
        "water-fast-z-score": 9.011025555411754,
        "rewrite-fast-z-score": 2.4959226008892244
    },
    {
        "original_text": "We present new near-infrared integral field spectroscopy (IFS) data for the brightest galaxy in the cluster Abell 2218, which is known to be interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. We find that this galaxy has an extended low-surface-brightness component surrounding it, extending out to about 10 kpc on both sides along the major axis. This feature shows no evidence of rotation but does show some velocity structure consistent with infalling gas or tidal debris. In addition we detect two compact objects within 5 kpc of the center of the galaxy. One of these appears to have a very high surface brightness and may represent a nuclear starburst; however, the other one displays much lower surface brightness and could possibly be associated with a supermassive black hole binary system. These results are discussed in terms of possible evolutionary scenarios for this interacting pair.",
        "watermark_text": "We present new near - infrared integral field spectroscopy ( IFS ) statistics for the brightest galaxy in the cluster Abell 2218 , which is known to be interacting with its closest neighbor , the radio - quiet quasar I Zw 1 at z = 0 . 0625 . We see that this galaxy has an extended low - exterior - brightness core covering it , extending out to about 10 kpc on both sides along the main axis .This structure exhibits no evidence of rotation but does display some velocity pattern correlated with infalling dust or tidal debris . In addition we find two compact entities within 5 kpc of the center of the galaxy .One of these seems to have a very high surface brightness and may indicate a nuclear starburst ; however , the other one exhibits far lower surface brightness and could possibly be identified with a supermassive black hole binary system . These conclusions are discussed in terms of possible evolved situations for this interacting pair .",
        "rewrite_text": "We present new near-infrared integral field spectroscopy (IFS) statistics for the brightest galaxy in the cluster Abell 2218, which is known to be interacting with its nearest neighbor, the radio-quiet quasar I Zw 1 at z = 0.0625. Our observations reveal that this galaxy has an extended low-exterior-brightness core, stretching approximately 10 kpc on either side along its main axis. This structure does not show any signs of rotation but displays a velocity pattern that appears to be associated with infalling dust or tidal debris. Additionally, we identify two compact entities situated within 5 kpc of the galaxy's center. One of these features a very high surface brightness, suggesting the possibility of a nuclear starburst, while the other exhibits a significantly lower surface brightness and could potentially represent a supermassive black hole binary system. These findings are discussed in the context of possible evolutionary scenarios for this interacting pair.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.8599622199011085
    },
    {
        "original_text": "The fusion cross sections for the reactions 16O+16O, 40Ca+40Ca and 48Ti+48Ti are calculated using the time-dependent Hartree-Fock (TDHF) method including the effects of the pre-equilibrium Giant Dipole Resonance (PDR). The PDR is found to play an important role in determining the fusion barrier heights as well as the fusion probabilities at sub-barrier energies. It is shown that the inclusion of the PDR leads to better agreement between theoretical predictions and experimental data than those obtained without it. \n \n In recent years there has been considerable interest in studying nuclear fusion processes by means of microscopic theories such as the Time-Dependent Hartree-Fock (TDDF) approach  1  . This is mainly due to its ability to provide detailed information on various aspects of the reaction dynamics which cannot be easily extracted experimentally  2  .\n \nIn this work we have investigated the effect of the Pre-Equilibrium Giant Dipole Resonances (PEGDRs), which appear during the initial stage of heavy ion collisions  3  , on the fusion cross section for three different systems namely 16 O + 16 O, 40 Ca + 40 Ca and 48 Ti + 48 Ti. These calculations were performed within the TDHF framework  4  where the ground state wave functions of these nuclei were generated using the Skyrme interaction Sly6  5  . \n \n We find that the PEGDR plays an important role in determining both the height and shape of the fusion barriers. Moreover, our results show that the inclusion of the pygmy resonances improves considerably the agreement between the theoretical predictions and available experimental data  6  -  8  compared to those obtained without them.",
        "watermark_text": "The fusion cross sections for the compounds 16O + 16O , 40Ca + 40Ca and 48Ti + 48Ti are measured using the time - dependent Hartree - Fock ( TDHF ) method including the effects of the pre - equilibrium Giant Dipole Resonance ( PDR ) . The PDR is found to take an important role in calculating the fusion barrier depths as well as the fusion probabilities at sub - barrier energies .It is demonstrated that the integration of the PDR leads to higher agreement between theoretical estimates and theoretical data than those achieved without it . In recent years there has been substantial interest in investigating nuclear fusion mechanisms by means of microscopic ideas such as the Time - Dependent Hartree - Fock ( TDDF ) approach 1 .This is mainly owing to its able to provide comprehensive information on various parts of the process mechanics which cannot be easily retrieved experimentally 2 . In this research we have researched the impact of the Pre - Equilibrium Giant Dipole Resonances ( PEGDRs ) , which appear during the first phase of large electron collisions 3 , on the fusion cross section for three different systems namely 16 O + 16 O , 40 Ca + 40 Ca and 48 Ti + 48 Ti .These measurements were performed within the TDHF framework 4 where the ground state wave functions of these ions were generated utilizing the Skyrme coupling Sly6 5 . We see that the PEGDR plays an important role in measuring both the height and shape of the fusion barriers .Moreover , our findings show that the introduction of the pygmy resonances improves substantially the accord between the theoretical estimates and accessible empirical data 6 - 8 compared to those achieved without them .",
        "rewrite_text": "The fusion cross sections for the reactions 16O + 16O, 40Ca + 40Ca, and 48Ti + 48Ti have been determined using the Time-Dependent Hartree-Fock (TDHF) method, which takes into account the effects of Pre-Equilibrium Giant Dipole Resonance (PDR). The PDR significantly influences the calculation of fusion barrier depths and the fusion probabilities at sub-barrier energies. Our results demonstrate that incorporating the PDR enhances the alignment between theoretical predictions and experimental data compared to analyses conducted without it. In recent years, there has been significant interest in exploring nuclear fusion processes using microscopic approaches like TDHF, as it can provide detailed insights into various aspects of the mechanisms that are difficult to obtain through experiments. This study investigates the effects of Pre-Equilibrium Giant Dipole Resonances (PEGDRs), which emerge during the initial phase of high-energy collisions, on the fusion cross sections for the aforementioned systems: 16O + 16O, 40Ca + 40Ca, and 48Ti + 48Ti. These measurements were carried out within the TDHF framework, utilizing the Skyrme interaction Sly6 to generate the ground state wave functions of these ions. Our findings indicate that the PEGDR is crucial for accurately assessing both the height and shape of the fusion barriers. Additionally, the integration of pygmy resonances significantly improves the agreement between theoretical predictions and available experimental data compared to scenarios where these resonances are neglected.",
        "ori-fast-z-score": -2.136828897185981,
        "water-fast-z-score": 6.863829072224211,
        "rewrite-fast-z-score": 0.41256849850351734
    },
    {
        "original_text": "We present an analysis of cosmogenic neutrino fluxes expected in IceCube for different models of cosmic ray sources and propagation, including recent results on anisotropy measurements by PAMELA and AMS-02 experiments. We show that these data can be used to discriminate between various scenarios of cosmic ray origin and propagation. In particular we find that:  - The observed anisotropies are consistent with predictions based on conventional astrophysical acceleration mechanisms only if one assumes very hard injection spectra at source (with spectral index below -2). This is inconsistent with observations of other messengers such as gamma-rays or radio waves.  - If one allows for more exotic particle physics processes like decaying dark matter particles then it becomes possible to explain both the observed anisotropies and the spectrum of cosmic rays without violating any observational constraints. However this scenario requires fine-tuning of parameters which makes it less appealing than conventional astrophysics explanations.",
        "watermark_text": "We present an assessment of cosmogenic neutrino fluxes anticipated in IceCube for different models of cosmic ray sources and propagation , notably recent results on anisotropy observations by PAMELA and AMS - 02 experiments . We see that these information can be used to discriminate between various circumstances of gamma radiation origin and propagation .In particular we find that : - The observed anisotropies are compatible with predictions based on conventional astrophysical acceleration mechanisms only if one takes very hard injection spectra at source ( with spectral index below - 2 ) . This is conflicting with observations of other messengers such as gamma - rays or radio beams .- If one supports for more exotic electron physics processes like decaying heavy material particles then it becomes possible to explain both the known anisotropies and the spectrum of cosmic rays without violating any observational restrictions . However this situation requires fine - tuned of values which makes it less acceptable than conventional astrophysics explanations .",
        "rewrite_text": "We provide an evaluation of the anticipated cosmogenic neutrino fluxes in IceCube, based on various models for cosmic ray sources and their propagation. This assessment notably incorporates recent findings on anisotropy from the PAMELA and AMS-02 experiments. Our analysis indicates that this data can help differentiate between different scenarios regarding the origins and propagation of gamma radiation. Specifically, we conclude that: - The detected anisotropies align with predictions derived from conventional astrophysical acceleration mechanisms only if extremely hard injection spectra at the source are considered (with a spectral index below -2). This is in conflict with data from other messenger particles, such as gamma rays and radio emissions. - Conversely, if one entertains more exotic processes involving electron physics, such as decaying heavy particles, it becomes feasible to account for both the observed anisotropies and the cosmic ray spectrum without breaching any observational constraints. However, this situation necessitates a fine-tuning of parameters, rendering it less satisfactory than traditional astrophysical explanations.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.128258770283413,
        "rewrite-fast-z-score": 0.22941573387056174
    },
    {
        "original_text": "The concept of tensegrity is used to describe the structural behavior of many biological systems, such as muscles and tendons.  In this work we explore how continuous tensegrities can be generated by using an evolutionary algorithm that optimizes their performance in terms of compliance with external loads while maintaining stability under gravity loading conditions.   The results show that it is possible to generate stable structures that are able to resist large deformations without collapsing or losing their integrity. This research has been funded by the European Commission through the Marie Curie Initial Training Network (ITN) program. The concept of tensegrity was first introduced by Buckminster Fuller more than 60 years ago  1  . It describes the structural behavior of many natural systems like muscles  2  , tendons  3  , bones  4  , and even living organisms  5  .\nIn recent decades there have been several attempts at applying the concept of tensegrity to engineering applications  6  -  8  . However, most of these works focus on discrete tensegrities which consist of rigid bars connected together by elastic struts  9  . These types of structures cannot easily adapt to changes in their environment since they do not allow for any deformation  10  . On the other hand, continuous tensegrities  11  are capable of changing shape continuously when subjected to external forces  12  . They also exhibit higher levels of robustness against damage  13  compared to conventional materials  14  . Despite all these advantages, very little attention has been paid so far to the design of continuous tensegrities  15  .\nThis lack of interest may be due to the fact that designing continuous tensegrities requires solving highly nonlinear optimization problems  16  . Moreover, finding solutions to these problems is extremely challenging because of the high number of local optima  17  . To overcome these difficulties, researchers usually use heuristic search algorithms  18  -  20  instead of exact methods  21  .",
        "watermark_text": "The concept of tensegrity is utilized to explain the structural performance of several biological systems , such as muscles and tendons . In this research we investigate how continuous tensegrities can be derived by using an phylogenetic algorithm that optimizes their performance in terms of compliance with external loads while maintaining strength under gravity loading circumstances .The results show that it is easy to produce consistent buildings that are able to resist big deformations without weakening or losing their stability . This research has been sponsored by the European Commission through the Marie Curie Initial Training Network ( ITN ) scheme .The concept of tensegrity was first described by Buckminster Fuller more than 60 years early 1 . It describes the structural function of several physical structures like nerves 2 , tendons 3 , ribs 4 , and even living organisms 5 .In past decades there have been numerous attempts at application the idea of tensegrity to engineering applications 6 - 8 . However , most of these works concentrate on discrete tensegrities which consist of rigid bars connected together by elastic struts 9 .These sorts of structures cannot effectively adapt to changes in their environment since they do not enable for any deformation 10 . On the other hand , continuous tensegrities 11 are capable of changing shape rapidly when exposed to external forces 12 .They especially display higher levels of robustness against damage 13 relative to conventional materials 14 . Despite all these benefits , very less attention has been paid so far to the design of continuous tensegrities 15 .This lack of importance may be due to the fact that designing continuous tensegrities demands modeling highly nonlinear optimization problems 16 . Moreover , finding solutions to these problems is incredibly problematic because of the high number of local optima 17 .To solve these problems , researchers normally use heuristic search methods 18 - 20 rather of precise methods 21 .",
        "rewrite_text": "The concept of tensegrity is employed to clarify the structural behavior of various biological systems, such as muscles and tendons. This research explores how continuous tensegrities can be generated using a phylogenetic algorithm that optimizes their performance in terms of their ability to withstand external loads while also maintaining strength under gravitational conditions. The findings indicate that producing consistent structures capable of withstanding significant deformations without compromising their stability is relatively straightforward. This research has received support from the European Commission through the Marie Curie Initial Training Network (ITN) initiative. Tensegrity was first introduced by Buckminster Fuller over 60 years ago. It pertains to the structural functionality of numerous physical structures, including nerves, tendons, ribs, and even living organisms. In recent decades, there have been several attempts to apply the idea of tensegrity in engineering contexts. However, most of these efforts focus on discrete tensegrities, which comprise rigid bars linked by elastic struts. Such structures lack the adaptability required to respond effectively to environmental changes, as they do not permit any deformation. In contrast, continuous tensegrities can rapidly alter their shape when subjected to external forces, demonstrating greater resilience to damage compared to conventional materials. Despite these advantages, continuous tensegrity design has received limited attention, likely due to the complexity of modeling highly nonlinear optimization problems associated with their design. Additionally, solving these problems is particularly challenging because of the numerous local optima present. Consequently, researchers often resort to heuristic search methods instead of precise approaches to address these challenges.",
        "ori-fast-z-score": -1.2722833945199565,
        "water-fast-z-score": 8.342314001435737,
        "rewrite-fast-z-score": 0.7071067811865475
    },
    {
        "original_text": "We report on observations made with Chandra and XMM-Newton that reveal an X-ray flare from the magnetar CXOU J16 47 10 . 2-45 52 16 (hereafter, J1647) located within the open cluster Westerlund 1. The flare was detected by both observatories during their respective slews to point at another target; it lasted for about one hour before fading below detectability. We find no evidence for any significant change in the spin-down rate or period derivative of this source following its outburst. \n \n This is the first time such a large event has been observed from a magnetar; we estimate that the total energy released in the flare was ~3 x 10^44 erg. Our analysis shows that the flare occurred when the star s magnetic field lines were nearly perpendicular to our line-of-sight. In addition, we detect pulsations from J1647 during the flare which are consistent with those seen prior to the flare. These results suggest that the flaring activity may be due to reconnection events occurring along the closed loops of the stellar magnetic field.",
        "watermark_text": "We report on observations made with Chandra and XMM - Newton that indicate an X - ray flare from the magnetar CXOU J16 47 10 . 2 - 45 52 16 ( hereafter , J1647 ) located within the open cluster Westerlund 1 .The flare was noticed by both observatories during their separate slews to point at another target ; it persisted for about one evening before faded below detectability . We see no evidence for any considerable shift in the spin - down frequency or duration derivative of this source following its outburst .This is the first time such a large incident has been observed from a magnetar ; we estimate that the total energy released in the flare was ~ 3 x 10 ^ 44 erg . Our study shows that the flare originated when the star s magnetic field lines were roughly perpendicular to our line - of - view .In addition , we find pulsations from J1647 during the flare which are compatible with those observed earlier to the flare . These data suggest that the flaring activity may be due to reconnection events resulting along the shut rings of the stars magnetic force .",
        "rewrite_text": "We present observations from Chandra and XMM-Newton that reveal an X-ray flare from the magnetar CXOU J164710.2-455216 (hereafter referred to as J1647), situated within the open cluster Westerlund 1. Both observatories detected the flare while conducting separate slews to another target, and it lasted for approximately one evening before fading below detectable levels. We found no significant changes in the spin-down frequency or its time derivative following the outburst. This marks the first observation of such a significant event from a magnetar, with an estimated total energy release of around 3 x 10^44 erg. Our analysis indicates that the flare occurred when the star's magnetic field lines were nearly perpendicular to our line of sight. Furthermore, we detected pulsations from J1647 during the flare that correspond to those previously recorded. These findings suggest that the flaring activity may be linked to reconnection events occurring along the closed loops of the star's magnetic field.",
        "ori-fast-z-score": -0.7171371656006361,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": -1.5882027766319677
    },
    {
        "original_text": "We present the results of our theoretical study on massive planet migration in protoplanetary disks, focusing on its dependence on disk properties such as viscosity and surface density profiles. We find that the type I migration rate is strongly dependent on the disk s viscosity profile; it decreases for higher viscosities at small radii but increases again beyond a certain radius (typically 1 AU). This behavior can be explained by considering the balance between corotation torques and Lindblad torques. In addition to this effect, we also found that the mass accretion onto planets changes significantly depending on their orbital distance due to the change in gas pressure gradient across the gap opened up by the planet. Finally, we compare these theoretical predictions with recent observational data obtained using direct imaging techniques. Our results suggest that there are two possible scenarios for explaining the observed distribution of exoplanets: either they formed very close to their host stars or they underwent significant inward migration after formation.",
        "watermark_text": "We present the conclusion of our theory study on massive planet migration in protoplanetary disks , concentrating on its dependence on disk properties such as viscosity and surface volume profiles . We see that the class I travel speed is strongly dependent on the disk s viscosity profile ; it reduces for greater viscosities at small radii but drops again beyond a certain distance ( commonly 1 AU ) .This phenomenon can be described by examining the balance between corotation torques and Lindblad torques . In addition to this effect , we also discovered that the mass accretion onto planets changes significantly depending on their orbital height owing to the shift in gas pressure gradient across the gap opened up by the planet .Finally , we compare these theoretical estimates with recent observational data acquired using direct imaging procedures . Our results propose that there are two possible strategies for explaining the seen distribution of exoplanets : either they formed very close to their host stars or they underwent considerable inward movement after formed .",
        "rewrite_text": "We present the conclusions of our theoretical study on the migration of massive planets within protoplanetary disks, focusing on how this migration is influenced by various disk properties, such as viscosity and surface density profiles. Our findings indicate that the migration speed in the Class I phase is highly sensitive to the viscosity profile of the disk. Specifically, we observe that the speed decreases with increasing viscosity at smaller radii but then experiences a notable drop beyond a certain distance, typically around 1 AU. This behavior can be understood by analyzing the balance between corotation and Lindblad torques. Furthermore, we found that the mass accretion onto planets is significantly affected by their orbital height due to variations in the gas pressure gradient across the gap created by the planet. Finally, we compare our theoretical predictions with recent observational data obtained through direct imaging techniques. Our results suggest two potential scenarios for the observed distribution of exoplanets: either they formed very close to their host stars or they have undergone significant inward migration after their formation.",
        "ori-fast-z-score": -0.7875615306482168,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": -0.8834522085987723
    },
    {
        "original_text": "We present an analytic expression for the one-loop mass-less triangle Feynman integral in terms of generalized hypergeometric functions. The results are obtained by using Mellin-Barnes representation and contour integration techniques. We also provide numerical values for some special cases which can be used to check our analytical expressions. This work is motivated by recent interest on the study of higher order corrections to various physical processes, such as Higgs decay into two photons or gluons at next-to-leading-order (NLO) accuracy. \nI. INTRODUCTORY REMARK\nThe calculation of loop diagrams plays an important role in theoretical physics. In particular, it has been shown that the inclusion of radiative corrections leads to significant changes in the predictions of many observables  1  . For example, the NLO QCD correction to the decay widths of heavy quarks  2  , top quark pair production  3  , Higgs boson decays  4  etc., have been calculated recently with great success. However, there still remain several open problems related to the evaluation of multi-loop integrals  5  .\nIn this letter we consider the following one-loop mass-less triangle Feyman integral  6  : \nwhere m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 . It should be noted here that I(q 2 ) vanishes when any three masses become equal i.e. m 1 = m 2 = m 3 = m 4 .",
        "watermark_text": "We introduce an analytic definition for the one - loop mass - less triangle Feynman integral in terms of generalized hypergeometric functions . The results are derived by using Mellin - Barnes representation and contour processing method .We additionally offer mathematical values for some particular instances which can be used to test our analytical expressions . This research is prompted by recent interest on the study of greater order corrections to several physical processes , such as Higgs decay into two photons or gluons at next - to - leading - order ( NLO ) accuracy .I . INTRODUCTORY REMARK The calculation of loop diagrams takes an important role in theoretical physics .In particular , it has been shown that the introduction of radiative corrections leads to significant improvements in the estimates of several observables 1 . For instance , the NLO QCD correction to the decay widths of hard quarks 2 , top quark pair production 3 , Higgs boson decays 4 etc . , have been measured recently with great success .However , there still continue several open problems related to the evaluation of multi - loop integrals 5 . In this letter we define the following one - loop mass - less triangle Feyman integral 6 : where m 1 = m 2 ≡ m 3 ≡ m 4 ≡ 0 and s 12 = q 2 .It should be mentioned here that I ( q 2 ) vanishes when any three masses become equal i . e . m 1 = m 2 = m 3 = m 4 .",
        "rewrite_text": "We present an analytic definition of the one-loop massless triangle Feynman integral using generalized hypergeometric functions. Our findings are obtained through the application of the Mellin-Barnes representation and contour integration techniques. Additionally, we provide numerical values for specific cases that can be employed to validate our analytical results. This work is motivated by the growing interest in examining higher-order corrections for various physical processes, such as the next-to-leading-order (NLO) precision in Higgs decay into two photons or gluons. \n\n**I. INTRODUCTORY REMARKS**  \nThe computation of loop diagrams is crucial in theoretical physics. It has been demonstrated that incorporating radiative corrections significantly enhances the accuracy of several observable estimates. For example, recent measurements of NLO QCD corrections to decay widths of heavy quarks, top quark pair production, and Higgs boson decays have yielded promising results. Nonetheless, challenges remain in the evaluation of multi-loop integrals. In this letter, we define the one-loop massless triangle Feynman integral, where we set \\( m_1 = m_2 = m_3 = m_4 = 0 \\) and \\( s_{12} = q^2 \\). Notably, the integral \\( I(q^2) \\) vanishes when all four masses are equal, i.e., \\( m_1 = m_2 = m_3 = m_4 \\).",
        "ori-fast-z-score": -0.10482848367219183,
        "water-fast-z-score": 5.421374765483944,
        "rewrite-fast-z-score": 1.0392304845413263
    },
    {
        "original_text": "We present an analysis of ellipsoidal oscillations induced in main-sequence stars by their substellar companions, and discuss prospects for detecting such signals with the Kepler mission. We find that these effects are detectable over a wide range of orbital periods (from 1 to 1000 days) and companion masses (0.1-10 MJ). The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion; it can be as large as 0.1% of the total flux at optical wavelengths. For typical parameters expected among planet-hosting systems, we estimate that this effect should produce a peak-to-peak variation in brightness on the order of 10 mmag or less. This level of precision will require several months of continuous observations using the Kepler spacecraft s photometer. However, if detected, the presence of such a signal would provide strong evidence for the existence of a low-mass stellar or planetary companion orbiting the primary star. \n \n Keywords: Ellipsoidal variations, Planetary system",
        "watermark_text": "We present an assessment of ellipsoidal oscillations induced in major - sequence stars by their substellar companions , and consider prospects for detecting such signals with the Kepler expedition . We see that these phenomena are detectable over a broad variety of orbital periods ( from 1 to 1000 days ) and companion masses ( 0 . 1 - 10 MJ ) .The amplitude of the signal is proportional to the square root of the mass ratio between the star and its companion ; it can be as big as 0 . 1 % of the total flux at optical wavelengths . For common parameters predicted among planet - hosting systems , we estimate that this effect should produce a peak - to - peak change in intensity on the order of 10 mmag or smaller .This level of precision will take many months of simultaneous measurements using the Kepler satellite s photometer . However , if detected , the presence of such a signal might give strong evidence for the existence of a small - density stellar or planetary companion orbiting the primary star .Keywords : Ellipsoidal variations , Planetary system",
        "rewrite_text": "We provide an evaluation of ellipsoidal oscillations triggered in main-sequence stars by their substellar companions and explore the potential for detecting such signals through the Kepler mission. Our analysis indicates that these phenomena can be observed across a wide range of orbital periods (from 1 to 1000 days) and companion masses (between 0.1 and 10 MJ). The signal's amplitude is proportional to the square root of the mass ratio between the star and its companion, potentially reaching up to 0.1% of the total flux at optical wavelengths. For typical parameters expected in planet-hosting systems, we estimate that this effect may result in a peak-to-peak intensity variation of around 10 mmag or less. Achieving this level of precision will require many months of simultaneous measurements with the photometer on the Kepler satellite. However, if such a signal is detected, it could provide compelling evidence for the presence of a low-density stellar or planetary companion orbiting the primary star. Keywords: Ellipsoidal variations, planetary systems.",
        "ori-fast-z-score": -1.62746694241347,
        "water-fast-z-score": 3.9524197172898554,
        "rewrite-fast-z-score": -0.5698028822981898
    },
    {
        "original_text": "We study the critical behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite-size scaling analysis. We find that the system undergoes a continuous phase transition at zero temperature, which is characterized by an infinite correlation length but no divergent susceptibility. The results are compared to those obtained for the pure 3D Ising model as well as other models with quenched disorder. In particular we show how our findings can be understood within the framework of the droplet picture. \nPACS numbers: 64.60.Cn, 64.60.J-, 64.60.Nz \nI. INTRODUCTORY REMARkS\nThe Random Field Ising Model (RFIM) has been introduced more than 50 years ago  1  . It describes a ferromagnetic material where each spin interacts only with its nearest neighbors via exchange interactions J ij , while it also feels an external magnetic field h i randomly oriented on different sites  2  .\nIn recent years there have been many studies devoted to this problem both experimentally  3  -  6  and theoretically  7  -  12  . This interest was triggered mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin-glasses  13  -  15  . For example, the presence of quenched disorder leads to frustration effects  16  similar to those observed in spin-glass materials  17  . Moreover, the RFIM displays a rich variety of phases depending on the strength of the applied magnetic field  18  . At low fields one finds a paramagnetic phase, whereas above a certain threshold value H c = O(J), the spins align along the direction of the local magnetic field leading to a ferromagnetic state  19  . Finally, if the magnitude of the external field exceeds another threshold value H t > H c , the magnetization becomes discontinuous  20  . These three regimes are separated by two second-order transitions occurring at T c1 < 0 and T c2 > 0  21  . However, despite these analogies between the RFIM and experimental systems  22  , the exact nature of the phase diagram remains controversial  23  .",
        "watermark_text": "We research the significant behavior of the 3D RFIM with Gaussian distributed disorder by means of Monte Carlo simulations and finite - length scaling processing . We see that the system undergoes a continuous phase shift at zero temperature , which is characterized by an endless correlation length but no divergent susceptibility .The results are compared to those achieved for the pure 3D Ising model as well as other models with quenched disease . In particular we show how our findings can be understood within the framework of the droplet picture .PACS codes : 64 . 60 . Cn , 64 . 60 . J - , 64 . 60 . Nz I . INTRODUCTORY REMARkS The Random Field Ising Model ( RFIM ) has been proposed more than 50 centuries earlier 1 .It describes a ferromagnetic material where each spin interacts only with its closest neighbors via transfer interactions J ij , while it also feels an external magnetic force h i randomly oriented on various places 2 . In recent seasons there have been many research devoted to this question both experimentally 3 - 6 and theoretically 7 - 12 .This interest was sparked mainly by the fact that the RFIM shares some features with real systems such as diluted antiferromagnets or spin - glasses 13 - 15 . For instance , the presence of quenched instability leads to frustration effects 16 comparable to those observed in spinning - glass materials 17 .Moreover , the RFIM displays a rich multitude of components varying on the strength of the applied magnetic force 18 . At small fields one gets a paramagnetic phase , whereas above a certain threshold factor H c = O ( J ) , the spins align along the direction of the local magnetic force leading to a ferromagnetic state 19 .Finally , if the magnitude of the external field exceeds another threshold quantity H t > H c , the magnetization becomes discontinuous 20 . These three regimes are split by two second - order transitions happening at T c1 < 0 and T c2 > 0 21 .However , despite these analogies between the RFIM and experimental systems 22 , the exact structure of the phase diagram remains disputed 23 .",
        "rewrite_text": "We investigate the notable characteristics of the 3D Random Field Ising Model (RFIM) with Gaussian-distributed disorder using Monte Carlo simulations and finite-length scaling techniques. Our findings indicate that the system experiences a continuous phase transition at absolute zero, distinguished by an infinite correlation length yet no diverging susceptibility. We compare our results with those obtained for the pure 3D Ising model as well as other models featuring quenched disorder. Specifically, we demonstrate how our observations can be interpreted within the droplet model framework. \n\nPACS codes: 64.60.Cn, 64.60.J- , 64.60.Nz \n\nI. INTRODUCTORY REMARKS \n\nThe Random Field Ising Model (RFIM) was proposed over 50 years ago and describes a ferromagnetic material where each spin interacts only with its nearest neighbors through exchange interactions J_ij, while also being influenced by an external magnetic field h_i that is randomly oriented in various locations. Recent years have seen extensive research on this topic, both experimentally and theoretically. This surge of interest is largely attributed to the RFIM's resemblance to real-world systems such as diluted antiferromagnets and spin glasses. For instance, the effect of quenched disorder results in frustration phenomena akin to those observed in spin-glass materials. Additionally, the RFIM exhibits a complex array of behaviors that depend on the strength of the applied magnetic field. At low fields, the system displays a paramagnetic phase; above a critical threshold field H_c = O(J), the spins align with the local magnetic field, resulting in a ferromagnetic state. If the external field strength surpasses another critical limit H_t > H_c, the magnetization becomes discontinuous. These three regimes are separated by two second-order phase transitions occurring at temperatures T_c1 < 0 and T_c2 > 0. However, despite the parallels between the RFIM and experimental systems, the precise structure of its phase diagram remains a topic of ongoing debate.",
        "ori-fast-z-score": -0.7986208584745025,
        "water-fast-z-score": 7.0164641544562345,
        "rewrite-fast-z-score": -1.2909944487358056
    },
    {
        "original_text": "The electric dipole moment (EDM) is an important observable in atomic physics, which can be used to test fundamental symmetries such as parity violation.  In this work we present the results for the EDM of Yb+ ions produced by laser cooling techniques at the Paul trap facility at the University of Bern.   We have measured the EDM with two different methods using either circularly polarized light or magnetic field gradients.  Our measurement yields a value of dYb+=(0.6 ± 1.1 stat ± 0.7 sys ) × 10−29 e cm where the systematic uncertainty has been reduced compared to previous measurements performed on Yb+ ions.  This result agrees well within uncertainties with theoretical predictions based on Standard Model calculations including nuclear structure corrections. A new experiment will start soon aiming at improving our current precision by one order of magnitude. The electric dipole moment (EDMs) are observables that can be used to probe fundamental symmetries like parity violation  1  . They are predicted to arise from CP-violating interactions beyond the standard model  2  , but their values are extremely small  3  .\nIn recent years there has been considerable interest in measuring EDMs of atoms  4  -  8  because they provide clean tests of fundamental theories  9  . Measurements of the electron EDM  10  -  12  yield upper limits on possible contributions from new particles  13  . On the other hand, experiments searching for permanent EDMs of nuclei  14  -  16  may lead to direct evidence of new sources of CP violation  17  .  Currently, most experimental efforts focus on heavy polarizable systems  18  -  20  since these offer higher sensitivity  21  .",
        "watermark_text": "The electric dipole point ( EDM ) is an important observable in nuclear physics , which can be used to test fundamental symmetries such as parity violation . In this research we present the results for the EDM of Yb + ions produced by radiation cooling technology at the Paul trap plant at the University of Bern .We have recorded the EDM with two different methods using either circularly polarized light or magnetic field gradients . Our measurement yields a value of dYb + = ( 0 . 6 ± 1 . 1 stat ± 0 . 7 sys ) × 10−29 e cm where the systematic uncertainty has been reduced compared to previous study performed on Yb + ions .This result agrees well within uncertainties with theoretical estimates based on Standard Model estimates including nuclear stability corrections . A second experiment will start soon aim at enhancing our existing precision by one order of magnitude .The electric dipole moment ( EDMs ) are observables that can be used to probe universal symmetries like parity violation 1 . They are expected to arise from CP - violating interactions beyond the standard theory 2 , but their values are extremely tiny 3 .In recent years there has been substantial interest in measuring EDMs of atoms 4 - 8 because they give clean tests of fundamental theories 9 . Measurements of the electron EDM 10 - 12 yield upper limits on potential contributions from new atoms 13 .On the other hand , experiments searching for permanent EDMs of nuclei 14 - 16 might lead to direct indication of new sources of CP violation 17 . Currently , most research efforts focus on heavy polarizable systems 18 - 20 since these give larger sensitivity 21 .",
        "rewrite_text": "The electric dipole moment (EDM) is a significant measurable quantity in nuclear physics that serves as a means to assess fundamental symmetries, particularly parity violation. In this study, we report on the EDM of Yb⁺ ions generated through radiation cooling techniques at the Paul trap facility at the University of Bern. We have obtained the EDM using two distinct methods that involve either circularly polarized light or magnetic field gradients. Our measurement reveals a value of dYb⁺ = (0.6 ± 1.1 stat ± 0.7 sys) × 10⁻²⁹ e cm, with reduced systematic uncertainty compared to earlier studies on Yb⁺ ions. This result is consistent with theoretical predictions from the Standard Model that account for nuclear stability corrections. A follow-up experiment is set to begin soon, aiming to improve our precision by an order of magnitude. Electric dipole moments serve as key observables for probing universal symmetries such as parity violation. These moments are anticipated to result from CP-violating interactions that extend beyond the Standard Model, although their expected values are exceedingly small. Recent years have seen a growing interest in measuring the EDMs of atoms, as these provide clear tests of fundamental theories. Measurements of the electron EDM have established upper limits on possible contributions from new atomic sources. Conversely, experiments aimed at detecting permanent EDMs of nuclei may offer direct evidence of new sources of CP violation. Presently, most research is concentrated on heavy polarizable systems, which demonstrate enhanced sensitivity.",
        "ori-fast-z-score": 0.09325048082403138,
        "water-fast-z-score": 5.501778368617852,
        "rewrite-fast-z-score": 0.6644105970267493
    },
    {
        "original_text": "We present proper motions for stars with magnitudes between 8 and 16, obtained by combining data from two epochs of photographic plates taken at the Palomar Observatory (POSS-I) and one epoch of digital images taken with the Hubble Space Telescope (HST). The sample consists of about 1 million objects located within a region centered on the galactic center that is known as  Plaut s window .  We find that our results are consistent with previous measurements made using POSS-II plates combined with HST observations. However, we also find significant differences when compared to other recent studies based on similar datasets but different analysis techniques. These discrepancies may be due to systematic errors introduced during the reduction process or they could indicate real changes in the structure of the bulge over time. Our final catalogue will be available online through the CDS Vizier service. This work was supported by NASA grant NAG5-13523.",
        "watermark_text": "We present proper motions for stars with magnitudes between 8 and 16 , obtained by combining information from two epochs of photographic sheets taken at the Palomar Observatory ( POSS - I ) and one epoch of digital photographs took with the Hubble Space Telescope ( HST ) . The sample consists of about 1 million items located within a region focused on the galactic center that is known as Plaut s window .We see that our findings are compatible with previous measurements made use POSS - II plates combined with HST observations . However , we also find considerable variations when compared to other recent studies relying on similar datasets but different analysis methods .These discrepancies may be due to systematic errors acquired during the reduction phase or they may indicate real shifts in the composition of the bulge over time . Our last catalogue will be available digital through the CDS Vizier network .This project was supported by NASA gift NAG5 - 13523 .",
        "rewrite_text": "We present proper motions for stars with magnitudes ranging from 8 to 16, derived from data collected across two epochs of photographic plates from the Palomar Observatory (POSS-I) and one epoch of digital photographs captured by the Hubble Space Telescope (HST). Our sample includes approximately 1 million stars situated in a region concentrated on the galactic center, known as Plaut's Window. Our results align with previous measurements obtained from POSS-II plates in conjunction with HST observations. However, we also observe significant discrepancies when compared to other recent research that utilized similar datasets but employed different analysis techniques. These variations may arise from systematic errors introduced during the data reduction process, or they could indicate genuine changes in the bulge's composition over time. The final catalog will be made available digitally through the CDS Vizier network. This project received support from NASA grant NAG5-13523.",
        "ori-fast-z-score": 0.24618298195866545,
        "water-fast-z-score": 6.077701994871215,
        "rewrite-fast-z-score": -0.36650833306891567
    },
    {
        "original_text": "We present results on isospin symmetry breaking effects for the ground states and excited states of the mirror nuclei 14Be, 14B, 14C, and 14N using the shell model with realistic interactions. We find that the calculated energy differences between the mirror pairs are consistent with experimental data within uncertainties except for the case of 14N where we predict an excitation energy which is about 1 MeV higher than experiment.  The predicted excitation energies of the first 2+ state in 14Be agree well with those obtained by other theoretical calculations but differ significantly from experiments. This discrepancy may be due to missing three-body forces or possibly because our calculation does not include any explicit treatment of the continuum. Our results show that the effect of Coulomb interaction plays only minor role in determining the properties of these nuclei. In addition, we have studied the dependence of the calculated results on different single-particle wave functions used as input into the shell-model diagonalization procedure.",
        "watermark_text": "We present results on isospin symmetry breaking effects for the ground states and excited states of the mirror nuclei 14Be , 14B , 14C , and 14N using the shell model with realistic interactions . We see that the derived energy differences between the mirror pairs are compatible with observation information within uncertainties except for the case of 14N where we estimate an excitation energy which is about 1 MeV higher than study .The predicted excitation energies of the first 2 + state in 14Be agree well with those achieved by other theoretical calculations but change considerably from experiments . This discrepancy may be due to missing three - bodies forces or possibly because our estimate does not include any explicit treatment of the continuum .Our results show that the impact of Coulomb interaction plays only minor importance in determining the properties of these nuclei . In addition , we have researched the dependence of the derived results on various single - nucleus wave distributions used as input into the shell - model diagonalization procedure .",
        "rewrite_text": "We present our findings on the effects of isospin symmetry breaking in the ground and excited states of the mirror nuclei 14Be, 14B, 14C, and 14N, utilizing the shell model with realistic interactions. The calculated energy differences between the mirror pairs align closely with experimental observations, within the margins of uncertainty, with the exception of 14N, where our estimated excitation energy is approximately 1 MeV higher than reported in previous studies. The predicted excitation energies for the first 2+ state in 14Be are in good agreement with results from other theoretical approaches but show significant discrepancies when compared to experimental data. This variation might be attributed to the omission of three-body forces or the lack of an explicit treatment of the continuum in our estimates. Furthermore, our findings indicate that the influence of Coulomb interactions is relatively minor in shaping the properties of these nuclei. Additionally, we have investigated how our results depend on different single-nucleus wave functions used as inputs in the shell-model diagonalization process.",
        "ori-fast-z-score": 1.4814874939752933,
        "water-fast-z-score": 5.356147093602983,
        "rewrite-fast-z-score": -0.562543950463012
    },
    {
        "original_text": "Charge ordering (CO) is one of the most important phenomena in strongly correlated electron systems, which has been observed in many transition metal oxides such as manganese perovskite compounds. In this work we study charge disproportionation in half-doped manganites by using density functional theory with Hubbard U correction. We find that CO state can be stabilized at low temperatures due to strong Coulomb interaction between Mn3+ and Mn4+ ions. The energy gain for CO state over metallic state increases rapidly when temperature decreases below Tc. Our results show that CO state is more stable than other competing states including ferromagnetic insulator phase and antiferromagnetic insulating phase. \n \n Keywords: Charge-ordering, Density-functional-theory, Correlated electrons, Transition-metal-oxides, Manganites, Energy-band-structure, Insulators, Ferromagnetism, Antiferromagnetism",
        "watermark_text": "Charge ordering ( CO ) is one of the most important phenomena in highly correlated electron structures , which has been observed in many transition metal oxides such as manganese perovskite compounds . In this research we study charge disproportionation in nearly - doped manganites by using density functional theory with Hubbard U relaxation .We see that CO state can be stabilized at low temperatures resulting to powerful Coulomb interaction between Mn3 + and Mn4 + ions . The energy gain for CO state over metallic state increases quickly when temperature falls below Tc .Our results show that CO state is more stable than other participating states including ferromagnetic insulator mode and antiferromagnetic insulating phase . Keywords : Charge - ordering , Density - functional - theory , Correlated atoms , Transition - iron - oxides , Manganites , Energy - band - structure , Insulators , Ferromagnetism , Antiferromagnetism",
        "rewrite_text": "Charge ordering (CO) is a crucial phenomenon in systems with strongly correlated electrons, and it has been observed in various transition metal oxides, including manganese perovskite compounds. In this study, we investigate charge disproportionation in nearly doped manganites using density functional theory with Hubbard U relaxation. Our findings indicate that the CO state can be stabilized at low temperatures, leading to significant Coulomb interactions between Mn3+ and Mn4+ ions. The energy difference favoring the CO state over the metallic state increases rapidly as the temperature drops below Tc. Our results demonstrate that the CO state exhibits greater stability than other competing states, such as the ferromagnetic insulator phase and the antiferromagnetic insulating phase. \n\nKeywords: Charge ordering, Density functional theory, Correlated electrons, Transition metal oxides, Manganites, Energy band structure, Insulators, Ferromagnetism, Antiferromagnetism.",
        "ori-fast-z-score": -0.5443310539518174,
        "water-fast-z-score": 3.5381518506868126,
        "rewrite-fast-z-score": 1.0504514628777804
    },
    {
        "original_text": "We report on the observation of sporadic meteors with Suprime-Cam attached to Subaru Telescope in August, 2004. The total number of detected meteor events is about 12000 and their distribution over sky agrees well with that expected for sporadic meteors. We also found some interesting features such as clustering around bright stars (probably due to fragmentation) and an excess of faint meteors near the ecliptic plane. These results will be useful for understanding the physical processes involved in the formation of meteoroid streams. \n \n Keywords: Meteor shower, Suprime-Cam, Subaru telescope, Spacecraft debris \n \n \n \n Received September 30, 2005 Accepted December 16, 2005 Published January 31, 2006 Online publication date: February 3, 2006 \n \n We have observed sporadic meteors using Suprime-Cam mounted at the Cassegrain focus of the 8-meter Subaru Telescope in August 2004 when the Perseid meteor shower was active. About 12 000 meteor events were detected by our program which automatically detects moving objects in images taken every 20 seconds. Their spatial distribution shows good agreement with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar observations. Some interesting features are also seen; e.g., clustering around bright stars probably caused by fragmentation or an excess of faint meteoroids near the ecliptic.",
        "watermark_text": "We report on the observation of sporadic meteors with Suprime - Cam attached to Subaru Telescope in August , 2004 . The total number of identified meteor events is about 12000 and their distribution over skies agrees well with that expected for sporadic meteors .We additionally found some interesting features such as clustering around bright stars ( probably due to fragmentation ) and an accumulation of bright meteors near the ecliptic plane . These data will be valuable for studying the physical processes implicated in the formation of meteoroid streams .Keywords : Meteor shower , Suprime - Cam , Subaru observatory , Spacecraft dust Received September 30 , 2005 Accepted December 16 , 2005 Published January 31 , 2006 Online publication date : February 3 , 2006 We have discovered sporadic meteors using Suprime - Cam mounted at the Cassegrain scope of the 8 - meter Subaru Telescope in August 2004 when the Perseid meteor shower was active . About 12 000 meteor events were detected by our system which automatically detects moving objects in images took every 20 seconds .Their spatial distribution shows excellent agreement with that predicted theoretically for sporadic meteors based on orbital elements obtained from radar measurements . Some interesting features are also observed ; e . g . , clustering around bright stars probably due by fragmentation or an accumulation of bright meteoroids near the ecliptic .",
        "rewrite_text": "In August 2004, during the active period of the Perseid meteor shower, we made observations of sporadic meteors using the Suprime-Cam attached to the 8-meter Subaru Telescope at the Cassegrain focus. Our automated detection system identified approximately 12,000 meteor events from images captured every 20 seconds. The distribution of these meteors aligns closely with theoretical expectations for sporadic events, based on orbital elements derived from radar measurements. Additionally, we noted intriguing phenomena, such as clustering around bright stars—likely due to fragmentation—and a higher concentration of bright meteors near the ecliptic plane. This data will contribute significantly to our understanding of the physical processes involved in the formation of meteoroid streams. \n\nKeywords: Meteor shower, Suprime-Cam, Subaru Observatory, Spacecraft dust  \nReceived: September 30, 2005  \nAccepted: December 16, 2005  \nPublished: January 31, 2006  \nOnline publication date: February 3, 2006",
        "ori-fast-z-score": 1.462614271203831,
        "water-fast-z-score": 6.260990336999411,
        "rewrite-fast-z-score": 0.6509445549041194
    },
    {
        "original_text": "We present an analysis of anisotropies in satellite galaxy locations around isolated galaxies, using data from the Sloan Digital Sky Survey (SDSS). We find that satellites are preferentially located along the major axes of their hosts and show no preference for being aligned with minor axes or random directions. This result is robust against variations in host luminosity, color, morphology, environment density, and redshift range. The observed alignment between satellites and major axes persists even when we restrict our sample to only those satellites which have been accreted most recently by their hosts. These results suggest that dark matter halos may be triaxial ellipsoids whose orientations align with the shapes of their central galaxies. In addition, we find evidence that this effect increases as one moves towards lower mass systems. Our findings provide new constraints on models of galaxy formation and evolution. Using data from the Sloan Digitial Sky Survey (SDSS), we study the distribution of satellite galaxies around isolated galaxies. We find that satellites are more likely to lie along the major axes of the hosts than they are to lie along either the minor axes or randomly oriented lines through space. This result holds true over a wide variety of host properties including luminosity, color, morphological type, local environmental density, and redshift range. \n \n Figure 1: An example of how we define the orientation of each host s halo relative to its position angle. Here, the blue line shows the projected major axis of the host while the red dashed line indicates the direction perpendicular to it.",
        "watermark_text": "We present an assessment of anisotropies in satellite galaxy locations around distant galaxies , using data from the Sloan Digital Sky Survey ( SDSS ) . We see that satellites are preferentially found along the main axes of their hosts and take no preference for being aligned with minor axes or random directions .This result is robust against variations in host luminosity , color , structure , environment density , and redshift range . The observed orientation between satellites and major axes persists even when we limit our sample to only those satellites which have been accreted most recently by their hosts .These data suggest that dark matter halos may be triaxial ellipsoids whose orientations align with the shapes of their central galaxies . In addition , we find proof that this effect grows as one moves approaching lower weight regions .Our findings provide novel constraints on estimates of galaxy formation and evolution . Using evidence from the Sloan Digitial Sky Survey ( SDSS ) , we study the spread of satellite galaxies around isolated stars .We see that satellites are more likely to lying along the main axes of the hosts than they are to lying along either the minor axes or randomly oriented lines through space . This result holds true over a broad variety of host characteristics including luminosity , color , morphological type , local environmental density , and redshift range .Figure 1 : An illustration of how we define the orientation of each host s halo compared to its position angle . Here , the blue line displays the projected major axis of the host while the red dashed line indicates the direction perpendicular to it .",
        "rewrite_text": "We provide an analysis of the distribution of satellite galaxies surrounding distant galaxies, utilizing data from the Sloan Digital Sky Survey (SDSS). Our findings indicate that satellite galaxies tend to be aligned along the main axes of their host galaxies, showing no preference for alignment with minor axes or random orientations. This pattern remains consistent across various factors such as host luminosity, color, structure, environmental density, and redshift range. Notably, the preferential alignment of satellites with major axes is sustained even when we focus solely on recently accreted satellites. These observations imply that dark matter halos may be shaped as triaxial ellipsoids that align with the orientation of their central galaxies. Additionally, we observe that this alignment effect becomes more pronounced in regions of lower density. Our results offer new insights into the processes of galaxy formation and evolution. Through our analysis of SDSS data, we examine the arrangement of satellite galaxies in relation to isolated stars. The data reveal that satellites are more commonly found along the main axes of their hosts compared to minor axes or random orientations. This trend is consistent across a range of host properties, including luminosity, color, morphological type, local environmental density, and redshift. Figure 1 illustrates how we define the orientation of each host's halo relative to its position angle, with the blue line representing the projected major axis and the red dashed line indicating the direction perpendicular to it.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 5.412874707438294,
        "rewrite-fast-z-score": 0.09016696346674323
    },
    {
        "original_text": "Epitaxial growth is the process by which atoms are deposited on top of an existing crystal structure, forming new layers that grow in registry with each other.  Epitaxy can be used to produce thin films and superlattices for electronic devices.   Graphene epitaxy refers to the deposition of carbon atoms onto a substrate such as silicon carbide (SiC) or gallium nitride (GaN).   The resulting material has many properties similar to those of single-layered graphene but also exhibits some differences due to interactions between the different atomic planes.   ... Graphene epitaxy is the deposition of carbon atoms on a substrate such as SiC or GaN. This results in a material with many properties similar to those found in single layer graphene, but there are also important differences arising from interlayer interactions. In this review we discuss recent progress towards understanding these effects using scanning probe microscopy techniques including STM, AFM and Raman spectroscopy.",
        "watermark_text": "Epitaxial growth is the process by which elements are deposited on top of an established crystal composition , forming fresh layers that grow in registry with each other . Epitaxy can be used to produce thin films and superlattices for electronic systems .Graphene epitaxy refers to the precipitation of carbon atoms onto a substrate such as silicon carbide ( SiC ) or gallium nitride ( GaN ) . The produced material has numerous characteristics similar to those of double - layered graphene but also exhibits some variations owing to interactions between the different atomic planes .. . . Graphene epitaxy is the precipitation of carbon atoms on a substrate such as SiC or GaN . This results in a material with many properties similar to those shown in single surface graphene , but there are also important differences arising from interlayer bonding .In this review we discuss latest advances towards studying these phenomena using scan probe microscopy methods notably STM , AFM and Raman spectroscopy .",
        "rewrite_text": "Epitaxial growth is the technique where elements are deposited onto an already established crystal structure, creating new layers that align perfectly with one another. This method is instrumental in the fabrication of thin films and superlattices utilized in electronic devices. Specifically, graphene epitaxy involves the deposition of carbon atoms onto substrates like silicon carbide (SiC) or gallium nitride (GaN). The resulting material shares many characteristics with bilayer graphene but also displays notable differences due to the interactions between the various atomic layers. In this review, we explore the recent advancements in understanding these phenomena through scanning probe microscopy techniques, including scanning tunneling microscopy (STM), atomic force microscopy (AFM), and Raman spectroscopy.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 6.861993625888845,
        "rewrite-fast-z-score": 1.2602520756252087
    },
    {
        "original_text": "We report the detection of periodic radial velocity variations in the spectrum of the K2V dwarf GJ 436, which are consistent with those expected for an orbiting planet. The period is 3.2 days and the semi-amplitude is about 30 m/sec. We also find evidence that this signal may be modulated on timescales longer than one year by another component whose mass we estimate to be at least 0.1 M⊕. This system has been extensively studied over many years as it lies close (5 pc) to our Sun but was not previously known to host any planets. It is therefore particularly interesting because its properties can now be compared directly with theoretical models of formation and evolution. \n \n Keywords: Planetary systems - Formation, Solar System\n\nIntroduction\n\nThe discovery of extrasolar planets has led to new insights into how planetary systems form and evolve. However, most exoplanets have been found using indirect techniques such as transit photometry or Doppler spectroscopy. These methods provide information only about the orbital parameters of the planet(s), while direct imaging provides additional constraints on their physical characteristics. In particular, high contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio relative to their parent stars.\n\nIn recent years there has been significant progress towards achieving high-contrast imaging capabilities required to detect Earth-like planets around nearby stars. For example, the Gemini Planet Imager (GPI; Macintosh et al., 2014) , SPHERE (Beuzit et al., 2008) and SCExAO (Jovanovic et al., 2015) instruments will soon begin operation on 8-10 m class telescopes. These facilities offer unprecedented sensitivity and angular resolution, allowing them to probe regions closer to the central star where terrestrial planets are more likely to exist. \nHowever, these observatories operate under different conditions and use different technologies so it remains unclear what performance they will achieve once commissioned.",
        "watermark_text": "We report the observation of periodic radial speed shifts in the spectrum of the K2V dwarf GJ 436 , which are compatible with those expected for an orbiting planet . The period is 3 . 2 days and the semi - frequency is about 30 m / sec .We additionally find proof that this signal might be modulated on timescales greater than one year by another component whose mass we estimate to be at least 0 . 1 M⊕ . This system has been heavily explored over numerous years as it lies close ( 5 pc ) to our Sun but was not originally seen to host any planets .It is consequently especially interesting because its properties can now be contrasted directly with theoretical theories of formation and evolution . Keywords : Planetary systems - Formation , Solar System Introduction The observation of extrasolar planets has led to fresh insights into how planetary structures create and evolve .However , most exoplanets have been seen use indirect approaches such as transit photometry or Doppler spectroscopy . These methods provide details only about the orbital characteristics of the planet ( s ) , while direct scanning provides additional constraints on their physical qualities .In particular , large contrast imaging allows us to measure the masses of companions down to very low levels of flux ratio compared to their parent stars . In recent history there has been significant progress towards reaching large - contrast imaging skills necessary to identify Earth - like stars around nearby planets .For instance , the Gemini Planet Imager ( GPI ; Macintosh et al . , 2014 ) , SPHERE ( Beuzit et al . , 2008 ) and SCExAO ( Jovanovic et al . , 2015 ) instruments will soon begin service on 8 - 10 m class telescopes . These systems allow extraordinary sensitivity and spatial resolution , allowing them to probe regions nearer to the main sun where terrestrial planets are more likely to appear .However , these observatories function under varying circumstances and use different technologies so it remains unsure what performance they will achieve once commissioned .",
        "rewrite_text": "We report the detection of periodic radial speed variations in the spectrum of the K2V dwarf GJ 436, which align with the expectations for an orbiting planet. This variation has a period of 3.2 days and a semi-amplitude of approximately 30 m/s. Furthermore, we provide evidence suggesting that this signal may be modulated over timescales exceeding one year by an additional component, which we estimate to have a minimum mass of 0.1 Earth masses. This system has been extensively studied over the years due to its proximity to our Sun (5 parsecs away) but was not initially observed to host any planets. Consequently, it is particularly intriguing, as its characteristics can now be directly compared with theoretical models of planetary formation and evolution. \n\nKeywords: Planetary systems - Formation, Solar System\n\nIntroduction: The discovery of exoplanets has enhanced our understanding of planetary structure and evolution. However, most exoplanets have been detected through indirect methods, such as transit photometry or Doppler spectroscopy. These approaches provide insights primarily into the orbital characteristics of the planet(s), while direct imaging offers additional constraints on their physical properties. In particular, high-contrast imaging enables the measurement of companion masses down to very low flux ratios relative to their host stars. Recently, there has been notable progress in developing the high-contrast imaging capabilities needed to identify Earth-like planets around nearby stars. For example, instruments like the Gemini Planet Imager (GPI; Macintosh et al., 2014), SPHERE (Beuzit et al., 2008), and SCExAO (Jovanovic et al., 2015) are set to become operational on 8-10 meter class telescopes. These facilities offer exceptional sensitivity and spatial resolution, allowing them to investigate regions closer to the host stars, where terrestrial planets are more likely to be found. However, as these observatories operate under various conditions and employ different technologies, their performance once fully operational remains uncertain.",
        "ori-fast-z-score": -0.2526455763199557,
        "water-fast-z-score": 8.780541105074454,
        "rewrite-fast-z-score": 0.0842151921066519
    },
    {
        "original_text": "We present results for Higgs-boson production in association with jets, including next-to-leading-order (NLO) corrections to both the total cross section and differential distributions as well as soft-gluon resummation up to NNLL accuracy. We also study the impact on these observables of varying the renormalization scale used in the perturbative expansion by considering two different prescriptions. The calculations are performed using the NNPDF2.3QED parton distribution functions and we use the CT14NNLO PDF set to estimate theoretical uncertainties due to missing higher orders. Our predictions are compared against data collected by ATLAS and CMS experiments at centre-of-mass energies of 8 TeV and 13 TeV. \nIntroduction\n\nThe Large Hadron Collider (LHC), located at CERN near Geneva, Switzerland, is currently operating at its highest energy ever achieved during Run 2. In this regime, it will be possible to produce new particles predicted by extensions of the Standard Model such as supersymmetric partners or extra gauge bosons. One of the most important goals of the LHC program is therefore to discover evidence of physics beyond the Standard Model through precision measurements of Standard-Model processes. Among them, Higgs-boson production plays an essential role since it provides crucial information about the mechanism responsible for electroweak symmetry breaking. \n \n At leading order (LO), Higgs-boson production can occur via gluon fusion, which proceeds mainly through heavy-quark loops, or vector-boson-fusion (VBF). Higher-order QCD corrections have been computed analytically  1–3  and numerically  4–6  . They increase significantly the LO prediction  7,8  , especially when going towards high values of the Higgs-boson mass mH  9  . This effect has been observed experimentally  10  . \nIn addition to the large logarithmic terms that appear at each order in perturbation theory, there exist subleading contributions associated with collinear emissions of soft gluons off initial-state quarks  11, 12  . These effects cannot be captured within fixed-order computations but must instead be included in all-order resum",
        "watermark_text": "We report findings for Higgs - boson development in association with jets , using next - to - leading - order ( NLO ) corrections to both the total cross area and integral distributions as well as hard - gluon resummation up to NNLL accuracy . We additionally study the impact on these observables of increasing the renormalization scale used in the perturbative expansion by using two different prescriptions .The calculations are performed using the NNPDF2 . 3QED parton distribution functions and we utilize the CT14NNLO PDF set to estimate theoretical uncertainties due to missing higher orders . Our predictions are compared against data taken by ATLAS and CMS observations at centre - of - mass energies of 8 TeV and 13 TeV .Introduction The Large Hadron Collider ( LHC ) , located at CERN near Geneva , Switzerland , is currently functioning at its highest power ever achieved during Run 2 . In this regime , it will be possible to produce new objects anticipated by modifications of the Standard Model such as supersymmetric partners or additional gauge bosons .One of the most important objectives of the LHC program is consequently to find proof of dynamics beyond the Standard Model through precision observations of Standard - Model processes . Among them , Higgs - boson production plays an essential part since it gives crucial data about the process controlling for electroweak symmetry breaking .At leading order ( LO ) , Higgs - boson development can occur via gluon fusion , which operates chiefly through heavy - quark loops , or vector - boson - fusion ( VBF ) . Higher - order QCD corrections have been computed analytically 1 – 3 and numerically 4 – 6 .They improve strongly the LO prediction 7 , 8 , particularly when going towards high values of the Higgs - boson weight mH 9 . This phenomenon has been observed experimentally 10 .In addition to the huge logarithmic terms that appear at each order in perturbation theory , there exist subleading contributions associated with collinear emissions of soft gluons off initial - state quarks 11 , 12 . These effects cannot be captured within fixed - order computations but need rather be included in all - order resum",
        "rewrite_text": "We present our findings on Higgs boson production in association with jets, incorporating next-to-leading order (NLO) corrections for both the total cross-section and integral distributions, as well as hard-gluon resummation with next-to-next-to-leading logarithm (NNLL) accuracy. Additionally, we examine how varying the renormalization scale utilized in the perturbative expansion affects these observables using two distinct approaches. Our calculations rely on the NNPDF2.3QED parton distribution functions, and we assess theoretical uncertainties from missing higher-order contributions using the CT14NNLO PDF set. We compare our predictions with experimental data from ATLAS and CMS at center-of-mass energies of 8 TeV and 13 TeV.\n\nIntroduction\n\nThe Large Hadron Collider (LHC) at CERN, near Geneva, Switzerland, is currently operating at its highest energy ever during Run 2. This enhanced capability allows for the potential production of new particles predicted by extensions to the Standard Model, such as supersymmetric partners or additional gauge bosons. One of the main goals of the LHC program is to uncover evidence of physics beyond the Standard Model through precise measurements of Standard Model processes. Higgs boson production is particularly significant, as it provides vital insights into the mechanisms of electroweak symmetry breaking. \n\nAt leading order (LO), Higgs boson production primarily occurs through gluon fusion, predominantly involving heavy-quark loops, or via vector-boson fusion (VBF). Previous studies have computed higher-order QCD corrections both analytically and numerically, which substantially enhance the LO predictions, especially for high Higgs boson masses (mH). This enhancement has been confirmed by experimental observations. Moreover, in each perturbative order, large logarithmic terms appear alongside subleading contributions related to collinear emissions of soft gluons from initial-state quarks. These effects cannot be adequately addressed through fixed-order calculations and must instead be accounted for using all-order resummation techniques.",
        "ori-fast-z-score": 0.34554737023254406,
        "water-fast-z-score": 7.023590753145371,
        "rewrite-fast-z-score": 0.8703882797784892
    },
    {
        "original_text": "The hypercube model is used to describe the evolution of life in terms of its complexity, speed, and stability.  The model shows that there are limits imposed by protein stability on how complex an organism can be as well as how fast it evolves molecularly.   It also suggests that these limits may have been reached during the Cambrian explosion about 540 million years ago when most animal phyla appeared simultaneously. This article describes the hypercube model and presents some examples of how it has been applied to understand evolutionary processes at different levels of organization ranging from genes to ecosystems. In this article we present a new approach for understanding the evolution of life based on the concept of the hypercube (1). We argue that the evolution of life can be described in three dimensions: complexity, speed, and stabilization. These three dimensions represent key aspects of biological systems that evolve over time. For example, organisms become more complex through the addition of new components such as organs or tissues; they evolve faster if their genetic variation increases; and they become more stable if mutations do not cause them to die prematurely. Figure 1 illustrates our view of the evolution of life using the hypercube model. Each vertex represents one possible state of living matter with respect to each dimension. As shown in Fig. 1A , the number of vertices along any given axis depends on the level of resolution chosen. At higher resolutions, the number of states increases exponentially. For instance, if we consider only two states per dimension—simple versus complex, slow versus fast, unstable versus stable—the total number of possible combinations would be four (2 x 2 x 2 = 8), which corresponds to eight types of living matter. However, if we increase the resolution so that we now include four states per dimension—very simple versus simple versus complex versus very complex, very slow versus slow versus fast versus very fast,...",
        "watermark_text": "The hypercube concept is utilized to explain the evolution of life in terms of its complexity , speed , and stability . The model shows that there are restrictions imposed by molecular stability on how difficult an organism can be as well as how fast it evolves molecularly .It additionally indicates that these limits might have been achieved during the Cambrian explosion about 540 million months previously when most animal phyla appeared simultaneously . This paragraph explains the hypercube model and provides some examples of how it has been used to explain evolutionary processes at different levels of organization ranging from genes to environments .In this article we present a new approach for studying the evolution of life based on the idea of the hypercube ( 1 ) . We argue that the evolution of life can be described in three dimensions : complexity , speed , and stabilization .These three dimensions represent crucial factors of biological systems that develop over time . For instance , animals get more sophisticated through the adding of new components such as bodies or tissues ; they develop faster if their genetic variation rises ; and they become more stable if mutations do not cause them to death prematurely .Figure 1 illustrates our view of the evolution of life using the hypercube view . Each vertex depicts one possible state of living life with regard to each dimension .As seen in Fig . 1A , the number of vertices along any certain axis determines on the level of resolution picked .At higher resolutions , the quantity of states varies exponentially . For instance , if we consider only two states per dimension — simple versus compound , slow versus fast , unstable versus stable — the total number of possible combinations may be four ( 2 x 2 x 2 = 8 ) , which equals to eight types of living matter .However , if we increase the resolution so that we now include four states per dimension — very simple versus easy versus compound versus very difficult , very slow versus slow versus fast versus very quickly , . . .",
        "rewrite_text": "The hypercube model serves as a framework to discuss the evolution of life concerning its complexity, speed, and stability. It posits that molecular stability imposes constraints on the complexity of organisms and the rate at which they evolve. According to this model, these constraints may have been reached during the Cambrian explosion, around 540 million years ago, when a significant number of animal phyla emerged simultaneously. This paragraph outlines the hypercube model and highlights its application in explaining evolutionary processes across various organizational levels, from genes to ecosystems. In this article, we introduce a novel perspective for studying the evolution of life through the lens of the hypercube (1). We propose that life’s evolution can be characterized along three dimensions: complexity, speed, and stabilization. These dimensions are critical to understanding the development of biological systems over time. For example, organisms become more complex as they acquire new features such as tissues or limbs; they evolve more rapidly with increased genetic variation; and they achieve greater stability when mutations do not lead to premature mortality. Figure 1 depicts our interpretation of life’s evolution through the hypercube framework, where each vertex represents a potential state of life in relation to the dimensions. As shown in Fig. 1A, the number of vertices along a particular axis is determined by the chosen resolution level. At higher resolutions, the number of states increases exponentially. For example, considering only two states per dimension—simple vs. complex, slow vs. fast, unstable vs. stable—the total number of combinations amounts to eight (2 x 2 x 2 = 8), representing eight types of living organisms. However, if we enhance the resolution to include four states per dimension—very simple, simple, complex, very difficult; very slow, slow, fast, very fast; etc.—the potential combinations expand significantly.",
        "ori-fast-z-score": 0.31622776601683794,
        "water-fast-z-score": 7.329426778023636,
        "rewrite-fast-z-score": -0.48349377841522817
    },
    {
        "original_text": "We consider the gravitational effect on the Casimir force between two parallel plates in an accelerating frame, and show that it is possible to obtain a repulsive force at short distances even if the initial state has no net charge or current density. We also discuss how this result can be obtained by using the concept of local energy-momentum tensor for electromagnetic fields. The possibility of obtaining such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear forces without introducing new particles. \n \n In recent years there have been several attempts to measure experimentally the Casimir force (see e.g., Refs.  1 - 6 ). However, all these experiments are performed with macroscopic objects which do not allow one to test directly the predictions of QED concerning the behavior of the vacuum fluctuations near boundaries  7, 8  . \n \n On the other hand, the measurement of the Casimir force between microscopic bodies would provide information about the properties of the vacuum  9  , and could lead to tests of QED  10  -  12  . \n \n It should be noted that the experimental determination of the Casimir force requires very accurate measurements because its magnitude is extremely small compared to gravity  13  . For example, the most precise experiment so far  6  gives only a relative accuracy of 0.1%. This means that any systematic error will dominate over statistical errors. Therefore, it is important to understand theoretically the dependence of the Casimir force upon external parameters like temperature  14  , pressure  15  , and velocity  16  .",
        "watermark_text": "We consider the gravitational impact on the Casimir force between two connected sheets in an accelerating frame , and find that it is easy to obtain a repulsive force at short distances even if the first state has no net charge or current density . We also discuss how this consequence can be obtained by using the idea of local power - momentum tensor for electromagnetic fields .The possibility of finding such a repulsive force was first suggested by Yukawa in 1951 as a way to explain nuclear pressures without introducing additional particles . In recent history there have been numerous attempts to measure experimentally the Casimir force ( saw e . g . , Refs .1 - 6 ) . However , all these experiments are performed with macroscopic objects which do not enable one to test explicitly the estimates of QED concerning the dynamics of the vacuum fluctuations near edges 7 , 8 .On the other hand , the determination of the Casimir force between microscopic bodies would offer information about the properties of the vacuum 9 , and could lead to tests of QED 10 - 12 . It should be mentioned that the empirical determination of the Casimir force requires very accurate measurements because its magnitude is incredibly small relative to gravity 13 .For instance , the most accurate experiment so far 6 offers only a relative validity of 0 . 1 % . This implies that any deliberate error will dominate over statistical mistakes .Therefore , it is important to realize theoretically the relationship of the Casimir force upon external parameters like pressure 14 , pressure 15 , and speed 16 .",
        "rewrite_text": "We investigate the gravitational effects on the Casimir force between two connected sheets in an accelerating frame and discover that a repulsive force at short distances can easily be achieved, even when the initial state has no net charge or current density. We explore how this phenomenon can be explained using the concept of a local power-momentum tensor for electromagnetic fields. The idea of such a repulsive force was first proposed by Yukawa in 1951 as a means to account for nuclear pressures without the need for additional particles. In recent years, there have been many experimental efforts to measure the Casimir force (see, for example, Refs. 1-6). However, these experiments have typically involved macroscopic objects, which make it difficult to directly test quantum electrodynamics (QED) predictions about vacuum fluctuations near edges (Refs. 7, 8). In contrast, accurately determining the Casimir force between microscopic bodies could provide insights into vacuum properties and facilitate tests of QED (Refs. 9, 10-12). It is important to note that precise measurements are essential for empirically determining the Casimir force, as its magnitude is minuscule compared to gravitational forces (Ref. 13). For example, the most precise experiment conducted to date (Ref. 6) achieves a relative accuracy of only 0.1%. This means that any systematic error can overshadow statistical uncertainties. Consequently, understanding the theoretical relationship between the Casimir force and external factors such as pressure (Ref. 14), pressure (Ref. 15), and speed (Ref. 16) is crucial.",
        "ori-fast-z-score": -0.2847473987257497,
        "water-fast-z-score": 7.370307223679931,
        "rewrite-fast-z-score": 1.1547005383792515
    },
    {
        "original_text": "We present mixed hyperbolic-second-order parabolic formulations for the Einstein field equations in vacuum and electrovacuum, which are suitable to be solved numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement (AMR). The formulation is based on an auxiliary variable that allows us to split the evolution system into two subsystems, one hyperbolic and another second-order parabolic. We show how this splitting can be used to construct stable numerical schemes using standard techniques such as Kreiss-Oliger dissipation or artificial viscosity. In addition we discuss several issues related to the implementation of these schemes within the AMR framework provided by the Cactus Computational Toolkit. Finally, we present some preliminary results obtained with our new code. This work was supported by CONACyT grant No. 164710. Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "watermark_text": "We present mixed hyperbolic - second - order parabolic formulations for the Einstein field equations in vacuum and electrovacuum , which are suitable to be answered numerically by means of finite difference methods on Cartesian grids with adaptive mesh refinement ( AMR ) . The implementation is based on an auxiliary variable that enables us to split the evolution system into two subsystems , one hyperbolic and another second - order parabolic .We see how this splitting can be used to build stable numerical theories employing conventional methods such as Kreiss - Oliger dissipation or artificial viscosity . In addition we explain several topics related to the implementation of these schemes within the AMR framework presented by the Cactus Computational Toolkit .Finally , we present some preliminary results acquired with our new code . This project was supported by CONACyT grant No .164710.Keywords: Adaptive Mesh Refinement, Numerical relativity",
        "rewrite_text": "We introduce mixed hyperbolic and second-order parabolic formulations of the Einstein field equations applicable in both vacuum and electrovacuum. These formulations are designed for numerical solutions using finite difference methods on Cartesian grids, enhanced by adaptive mesh refinement (AMR). Our approach utilizes an auxiliary variable, which allows us to decompose the evolution system into two distinct subsystems: one hyperbolic and one second-order parabolic. This decomposition facilitates the development of stable numerical theories using standard techniques like Kreiss-Oliger dissipation and artificial viscosity. Furthermore, we discuss various aspects related to the application of these schemes within the AMR framework provided by the Cactus Computational Toolkit. Lastly, we share some initial results obtained with our newly developed code. This project received support from CONACyT grant No. 164710. Keywords: Adaptive Mesh Refinement, Numerical Relativity.",
        "ori-fast-z-score": 1.632993161855452,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 0.5345224838248488
    },
    {
        "original_text": "We present new near-infrared (NIR) observations of the young stellar cluster IC 1396 N, located in the Orion Nebula Cluster region. The data were obtained with the adaptive optics system NAOS-CONICA on the VLT telescope and cover an area of 0.5 arcmin2 around the central star HD 37022. We detect more than 100 point sources down to Ks = 18 mag within this field-of-view. Using these data we have constructed colour-magnitude diagrams for different areas inside our field-of-view. From these CMDs we find that there are two populations of stars which can be separated by their position in the diagrams: one population is redder and fainter while another has bluer colours and brighter magnitudes. These results suggest that the first group consists mainly of low mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group contains mostly high mass main-sequence stars without any surrounding material.",
        "watermark_text": "We report new near - infrared ( NIR ) observations of the small stellar cluster IC 1396 N , located in the Orion Nebula Cluster region . The data were obtained with the adaptive optics network NAOS - CONICA on the VLT telescope and cover an area of 0 . 5 arcmin2 around the primary star HD 37022 .We detect more than 100 point sources down to Ks = 18 mag within this field - of - view . Using these information we have created colour - magnitude diagrams for different areas inside our field - of - view .From these CMDs we find that there are two communities of stars which can be apart by their placement in the diagrams : one community is redder and fainter while another has bluer colours and darker magnitudes . These data suggest that the first group contains primarily of high mass pre - principal sequence stars surrounded by circumstellar disks , whereas the second set contains primarily low mass primary - sequence stars without any surrounding media .",
        "rewrite_text": "We present new near-infrared (NIR) observations of the small stellar cluster IC 1396 N, situated within the Orion Nebula Cluster region. The data were collected using the adaptive optics system NAOS-CONICA on the VLT telescope, covering an area of 0.5 arcmin² around the primary star HD 37022. Within this field, we identified over 100 point sources down to Ks = 18 mag. Using this information, we constructed color-magnitude diagrams (CMDs) for various areas in our field of view. The CMDs reveal two distinct groups of stars based on their positions in the diagrams: one group appears redder and fainter, while the other exhibits bluer colors and brighter magnitudes. This data suggests that the first group mainly consists of high-mass pre-main sequence stars surrounded by circumstellar disks, whereas the second group is primarily composed of low-mass main sequence stars without any surrounding material.",
        "ori-fast-z-score": 0.12403473458920847,
        "water-fast-z-score": 5.417490779798923,
        "rewrite-fast-z-score": 0.24253562503633297
    },
    {
        "original_text": "We study the charge ordering phenomena in one-dimensional solids by using the exact diagonalization method and density matrix renormalization group (DMRG) technique. We find that there are two types of charge orderings, i.e., stripe-like and checkerboard-like orders depending on the electron filling factor n. The former is realized for 0 < n < 1 while the latter appears at half-filling with spin degeneracy lifted. In addition to these ordered states we also observe an exotic state where electrons form pairs without any net charge. This paired state can be regarded as a precursor of superconductivity. Finally, we discuss possible experimental realizations of our results. Introduction:-In recent years much attention has been paid to the physics of low dimensional systems such as carbon nanotubes  1  , semiconductor nanowires  2  , quantum wires  3  etc.. These materials have attracted considerable interest because they provide us with unique opportunities to explore novel physical properties which cannot exist in conventional three-dimensional bulk materials  4  . For example, it was predicted theoretically  5  and observed experimentally  6  that carbon nanotubes show metallic behavior even though their diameter is comparable or smaller than the Fermi wavelength. Another interesting feature of low dimensional systems is that various kinds of electronic phases may appear due to strong correlation effects  7, 8  .\nOne of the most important issues in this field is how to control the electronic phase diagram of low dimensional systems. It should be noted here that the electronic structure strongly depends not only on the geometry but also on the chemical composition  9  . Therefore, if we could change the chemical composition of low dimensional systems, then we would expect new electronic phases to emerge. Recently, several groups succeeded in synthesizing low dimensional compounds whose chemical compositions were controlled precisely  10 -12  . As a result, many fascinating phenomena have been discovered  13 -19  .",
        "watermark_text": "We research the charge ordering phenomena in one - dimensional solids by using the exact diagonalization technique and density matrix renormalization group ( DMRG ) method . We see that there are two forms of charge orderings , i . e . , stripe - like and checkerboard - like orders depending on the electron filling factor n . The first is realized for 0 < n < 1 while the former presents at half - filling with spin degeneracy lifted .In addition to these organized states we also observe an exotic state where electrons form couples without any gross charge . This paired state can be regarded as a precursor of superconductivity .Finally , we explain possible experimental realizations of our findings . Introduction : - In recent years much attention has been paid to the physics of low dimensional devices such as carbon nanotubes 1 , semiconductor nanowires 2 , quantum wires 3 etc . .These substances have garnered considerable interest because they give us with special opportunities to examine novel physical properties which cannot appear in standard three - dimensional bulk materials 4 . For instance , it was anticipated theoretically 5 and detected experimentally 6 that carbon nanotubes exhibit metallic behavior even though their distance is identical or smaller than the Fermi wavelength .Another important feature of lowest dimensional systems is that various kinds of electronic phases often emerge due to powerful correlation effects 7 , 8 . One of the most important problems in this area is how to manage the electronic phase diagram of low dimensional systems .It should be mentioned here that the chemical structure strongly depends not only on the topology but also on the chemical composition 9 . Therefore , if we could shift the chemical composition of low dimensional systems , then we may expect fresh electronic phases to emerge .Recently , various groups succeeded in synthesizing low dimensional molecules whose chemical compositions were monitored precisely 10 - 12 . As a result , various fascinating experiments have been observed 13 - 19 .",
        "rewrite_text": "We investigate charge ordering phenomena in one-dimensional solids using the exact diagonalization technique and the density matrix renormalization group (DMRG) method. Our findings indicate the presence of two types of charge orderings: stripe-like and checkerboard-like, depending on the electron filling factor \\( n \\). The stripe-like order occurs for \\( 0 < n < 1 \\), while the checkerboard order is realized at half-filling with spin degeneracy removed. In addition to these organized states, we identify an exotic state where electrons pair up without any net charge, which can be seen as a precursor to superconductivity. Lastly, we discuss potential experimental implementations of our results.\n\n**Introduction:** In recent years, there has been a growing interest in the physics of low-dimensional devices such as carbon nanotubes, semiconductor nanowires, and quantum wires. These materials have attracted attention because they provide unique opportunities to explore novel physical properties that do not arise in conventional three-dimensional bulk materials. For instance, theoretical predictions and experimental observations have suggested that carbon nanotubes exhibit metallic behavior even though their dimensions are comparable to or smaller than the Fermi wavelength. Another notable characteristic of low-dimensional systems is the emergence of various electronic phases driven by strong correlation effects. A key challenge in this field is to navigate the electronic phase diagram of low-dimensional systems. It is important to highlight that the chemical structure is influenced not only by topology but also by chemical composition. Consequently, by altering the chemical composition of low-dimensional systems, we can anticipate the emergence of new electronic phases. Recently, several research groups have successfully synthesized low-dimensional molecules with carefully controlled chemical compositions, leading to a variety of intriguing experimental results.",
        "ori-fast-z-score": -0.8778955729143844,
        "water-fast-z-score": 7.741442779335935,
        "rewrite-fast-z-score": 0.40689422938557973
    },
    {
        "original_text": "We report on two new brown dwarf candidates discovered by direct imaging with NACO/VLT at distances of ~100 AU to their host star, which are members of open clusters NGC 2423 (M67) and NGC 4349. The first object is located close to the cluster center and has an estimated mass between 0.03-0.07 M . It was found as part of our survey for low-mass companions to nearby young solar-type stars. We also present follow-up observations of this candidate using adaptive optics techniques that confirm its substellar nature. \n \n The second object lies outside the core radius of the cluster and has an estimated mass below 0.01 M . This companion may be either a planetary or very-low-mass stellar companion depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both objects have masses well above those predicted by current models of planet formation via disk accretion.",
        "watermark_text": "We report on two new brown dwarf candidates discovered by direct observation with NACO / VLT at distances of ~ 100 AU to their host star , which are members of close complexes NGC 2423 ( M67 ) and NGC 4349 . The first object is situated close to the cluster center and has an estimated mass between 0 . 03 - 0 . 07 M .It was obtained as part of our survey for low - weight companions to nearby young solar - class stars . We additionally post follow - up observations of this candidate using adaptive optics techniques that prove its substellar nature .The second object lies outside the core radius of the cluster and has an estimated mass below 0 . 01 M . This companion might be either a planetary or very - low - weight stellar companion depending on whether it created through gravity instability or fragmentation during cloud collapse .Both bodies have masses well above those predicted by current theories of planet development via disk accretion .",
        "rewrite_text": "We present two newly identified brown dwarf candidates, discovered through direct observation with NACO at the VLT, located approximately 100 AU from their host star. These candidates are part of the close complexes NGC 2423 (M67) and NGC 4349. The first candidate is positioned near the center of the cluster, with an estimated mass ranging from 0.03 to 0.07 M. This discovery is part of our survey aimed at detecting low-mass companions to nearby young solar-type stars. We also include follow-up observations utilizing adaptive optics techniques, which confirm its substellar nature. The second candidate is situated outside the core radius of the cluster, with an estimated mass of less than 0.01 M. This companion may either be a planetary object or a very low-mass stellar companion, depending on whether it formed through gravitational instability or fragmentation during cloud collapse. Both candidates have masses significantly higher than those suggested by current models of planetary formation via disk accretion.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.330522446256832,
        "rewrite-fast-z-score": 0.22360679774997896
    },
    {
        "original_text": "We study the effect of multiplicative white noise on the dynamics of a nonlinear oscillator driven by an external periodic force and subject to dissipation. We show that, depending on the parameters characterizing the driving field and the strength of the noise, there are two different regimes for the evolution of the amplitude of the oscillator: (i) exponential decay or saturation at some finite value, (ii) slow algebraic decay with time. In both cases we find analytical expressions for the corresponding relaxation rates. The results obtained can be applied to describe various physical phenomena such as damped Josephson junctions, semiconductor lasers, etc.. PACS numbers: 05.45.Mt, 05.40.+j, 05.60.Fh \nI. INTRODUCTORY REMARK\nIn this work we consider the following stochastic differential equation describing the motion of a particle in one dimension under the action of friction and a periodically varying potential V(x), which is modulated by a random process W(t)\nwhere x = {x(t), t ∈ R} denotes the position coordinate of the particle; γ > 0 stands for the coefficient of friction; f (x) describes the deterministic part of the potential energy, while g(x) represents its random component. Hereinafter, we assume that f , g : R → R + . It should be noted that Eq. (1) has been derived within several models used to describe real-world systems  1  -  4  .\nThe main goal of our investigation is to analyze how the presence of additive noise affects the behavior of solutions of Eq.\n(1). To do so, it will be convenient to rewrite Eq. (1) \nHereafter, we shall refer to Eqs. \nII. MAIN RESULTS\n\nA. Exponential decay/saturation regime\nLet us first focus on the case when the function f satisfies the condition |f ′′′ (x)| < C 3 for all x ∈ R, where C 3 is a positive constant. Then, using standard arguments based on the Ito formula  6  , one can easily prove that any solution of Eq. (",
        "watermark_text": "We test the impact of multiplicative white sound on the dynamics of a nonlinear oscillator driven by an external periodic force and subject to dissipation . We see that , depending on the variables characterizing the driving field and the strength of the noise , there are two different regimes for the evolution of the frequency of the oscillator : ( i ) exponential decay or saturation at some finite value , ( ii ) gradual geometric decay with time .In both cases we find analytical expressions for the associated relaxation frequencies . The results derived can be applied to explain different physical phenomena such as damped Josephson junctions , semiconductor lasers , etc . . PACS codes : 05 . 45 . Mt , 05 . 40 . + j , 05 . 60 . Fh I .INTRODUCTORY REMARK In this study we suppose the following stochastic differential formula describing the movement of a particle in one dimension under the action of tension and a periodically varying potential V ( x ) , which is modulated by a random process W ( t ) where x = { x ( t ) , t ∈ R } indicates the place coordinate of the particle ; γ > 0 stands for the coefficient of tension ; f ( x ) refers the deterministic part of the potential electricity , while f ( x ) indicates its random portion . Hereinafter , we suppose that f , f : R → R + .It should be mentioned that Eq . ( 1 ) has been constructed within several models used to explain real - time systems 1 - 4 .The main goal of our inquiry is to analyze how the presence of additive interference affects the actions of solutions of Eq . ( 1 ) .To do so , it will be suitable to rewrite Eq . ( 1 ) Hereafter , we shall refer to Eqs .II . MAIN RESULTS A . Exponential decay / saturation regime Let us first focus on the case when the function f satisfies the condition | f ′ ′ ′ ( x ) | < C 3 for all x ∈ R , where C 3 is a positive variable .Then , using conventional statements based on the Ito formula 6 , one can easily prove that any solution of Eq . (",
        "rewrite_text": "We investigate the influence of multiplicative white noise on the dynamics of a nonlinear oscillator that is driven by an external periodic force and experiences dissipation. Our findings indicate that the oscillator's frequency evolution displays two distinct regimes, which are dependent on the characteristics of the driving field and the intensity of the noise: (i) exponential decay or stabilization at a finite value, and (ii) gradual geometric decay over time. We derive analytical expressions for the corresponding relaxation frequencies in both scenarios. These results can be utilized to elucidate various physical phenomena, including damped Josephson junctions and semiconductor lasers, among others. \n\n**PACS codes:** 05.45.Mt, 05.40.+j, 05.60.Fh \n\n**I. INTRODUCTORY REMARKS**  \nIn this research, we propose a stochastic differential equation that models the motion of a particle in one dimension under the influence of tension and a periodically varying potential \\( V(x) \\), which is modulated by a random process \\( W(t) \\). Here, \\( x = \\{ x(t), t \\in \\mathbb{R} \\} \\) represents the position coordinate of the particle; \\( \\gamma > 0 \\) denotes the tension coefficient; \\( f(x) \\) refers to the deterministic component of the potential energy, while \\( g(x) \\) denotes its stochastic part. For clarity, we assume that \\( f, g: \\mathbb{R} \\rightarrow \\mathbb{R}^+ \\). It is important to note that Equation (1) has been developed within the framework of several models aimed at explaining real-time systems (1-4). The primary objective of our study is to examine how the presence of additive noise affects the solutions of Equation (1). To facilitate this analysis, we will rewrite Equation (1).\n\n**II. MAIN RESULTS**  \n**A. Exponential Decay/Saturation Regime**  \nFirst, we will consider the scenario where the function \\( f \\) satisfies the condition \\( | f'''(x) | < C_3 \\) for all \\( x \\in \\mathbb{R} \\), with \\( C_3 \\) being a positive constant. By employing standard methods based on Itô's lemma, we can demonstrate that any solution of Equation (1) behaves in a predictable manner.",
        "ori-fast-z-score": -0.9128709291752769,
        "water-fast-z-score": 6.6996452071463715,
        "rewrite-fast-z-score": -0.18257418583505536
    },
    {
        "original_text": "Reverberation is an important feature of brain function, but its role remains unclear. We show that reverberation can be generated by the interplay between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of spikes which are followed by periods of low firing rate due to depletion of neurotransmitter vesicles. The resulting slow recovery of transmitter release causes a build-up of residual calcium concentration  Ca  res , leading to facilitation of glutamate release during subsequent bursts. This positive feedback loop generates sustained reverberatory activity with multiple time scales. Our results suggest that reverberation may play a key role in information processing within neural circuits. Reverberation is one of the most prominent features of brain function  1  . It has been observed across different species  2  -  4  as well as in various brain areas including hippocampus  5  , neocortex  6  , thalamus  7  , striatum  8  , cerebellar cortex  9  , olfactory bulb  10  , and retina  11  .\nDespite its ubiquity, however, it still remains unknown what functional roles reverberation plays in the brain  12  . One possibility is that reverberation serves as a mechanism for memory storage  13  or retrieval  14  . Another hypothesis suggests that reverberation could serve as a substrate for working memory  15  . Yet another idea is that reverberation might provide a means for temporal coding  16  . Finally, some studies have suggested that reverberation may simply reflect ongoing spontaneous activity  17  .",
        "watermark_text": "Reverberation is an important feature of cerebral activity , but its significance remains unsure . We suggest that reverberation can be triggered by the interplay between calcium signaling and short - term plasticity ( STP ) at excitatory synapses .In our model , STP results to bursts of peaks which are preceded by periods of poor fired rate due to depletion of neurotransmitter vesicles . The resulting slow withdrawal of transmitter release forms a build - up of residual calcium concentration Ca res , leading to facilitation of glutamate production during later bursts .This positive feedback network generates sustained reverberatory behavior with various time ranges . Our results propose that reverberation may play a key importance in information processing within neural systems .Reverberation is one of the most notable features of cerebral function 1 . It has been observed across different species 2 - 4 as well as in different brain regions including hippocampus 5 , neocortex 6 , thalamus 7 , striatum 8 , cerebellar cortex 9 , olfactory bulb 10 , and retina 11 .Despite its ubiquity , however , it still remains obscure what functional functions reverberation plays in the brain 12 . One possibility is that reverberation acts as a system for memory processing 13 or retrieval 14 .Another hypothesis suggests that reverberation possibly provide as a substrate for working brain 15 . Yet another idea is that reverberation would offer a means for temporal coding 16 .Finally , some researchers have suggested that reverberation possibly simply reflect ongoing spontaneous activity 17 .",
        "rewrite_text": "Reverberation is a crucial aspect of cerebral function, but its exact role is still unclear. We propose that reverberation can be initiated by the interaction between calcium signaling and short-term plasticity (STP) at excitatory synapses. In our model, STP leads to bursts of activity, which are preceded by phases of reduced firing rates caused by the depletion of neurotransmitter vesicles. This gradual reduction in transmitter release results in an accumulation of residual calcium concentration (Ca_res), which subsequently facilitates glutamate production during later bursts of activity. This positive feedback mechanism generates sustained reverberatory behavior across various time scales. Our findings suggest that reverberation may be crucial for information processing within neural systems. Reverberation is one of the most prominent features of cerebral activity and has been observed in various species and different brain regions, including the hippocampus, neocortex, thalamus, striatum, cerebellar cortex, olfactory bulb, and retina. Despite its widespread occurrence, the specific functional roles of reverberation in the brain remain elusive. One possibility is that it serves as a mechanism for memory processing or retrieval. Another hypothesis proposes that reverberation may provide a substrate for working memory. Additionally, it may facilitate temporal coding, or alternatively, reflect ongoing spontaneous activity.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 7.514430561703706,
        "rewrite-fast-z-score": 1.5230192477004287
    },
    {
        "original_text": "Horizontal Gene Transfer (HGT) is an important evolutionary force that can increase genetic diversity and accelerate evolution, but it also has negative effects such as disrupting co-adapted gene complexes or introducing deleterious mutations into recipient genomes. In this study we investigate how HGT affects the mean fitness of unicells evolving under different environmental conditions using computational models. We find that HGT increases the mean fitness when cells are exposed to fluctuating environments with high levels of stressful events. However, if there are only mild fluctuations then HGT decreases the mean fitness because it introduces harmful mutations. Finally, for constant environments without any external stresses, HGT does not affect the mean fitness at all. Our results suggest that HGT may have played an important role during early stages of life s evolution by increasing its adaptability to changing environments. Horizontal Gene Transfer (HGT), which occurs between organisms sharing similar DNA sequences, is one of the most significant evolutionary forces known today  1  . It allows rapid acquisition of new genes and thus contributes to increased genetic diversity within species  2  , accelerates evolution  3  , and facilitates adaptation  4  .\nHowever, HGT also has some disadvantages including disruption of co-adapted gene complexes  5  and introduction of deleterious mutations  6  . Therefore, understanding the effect of HGT on population dynamics requires careful investigation  7, 8  . Previous studies suggested that HGT could be beneficial for populations living in fluctuating environments  9  while detrimental for those inhabiting stable ones  10  . Here we use computational models to explore these hypotheses further and show that HGT can either increase or decrease the mean fitness depending on the type of environment inhabited by the cell population.",
        "watermark_text": "Horizontal Gene Transfer ( HGT ) is an important genetic force that can increase genetic diversity and accelerate evolution , but it also has negative impacts such as disrupting co - adapted gene structures or introducing deleterious variants into recipient genomes . In this study we investigate how HGT affects the mean fitness of unicells evolving under various environmental conditions utilizing computational models .We see that HGT changes the mean fitness when cells are susceptible to fluctuating environments with high levels of stressful events . However , if there are only slight fluctuations then HGT decreases the mean fitness because it creates dangerous mutations .Finally , for constant environments without any external stresses , HGT does not alter the mean fitness at all . Our results show that HGT could have played an important role during initial stages of life s evolution by increasing its adaptability to changing settings .Horizontal Gene Transfer ( HGT ) , which occurs between organisms sharing comparable DNA sequences , is one of the most significant evolutionary factors known today 1 . It enables quick acquisition of new genome and therefore contributes to greater genetic fitness within genus 2 , accelerates development 3 , and facilitates adaptation 4 .However , HGT also has some disadvantages including disruption of co - adapted gene pathways 5 and entry of deleterious variants 6 . Therefore , studying the impact of HGT on population behavior needs thorough investigation 7 , 8 .Previous studies suggested that HGT could be beneficial for individuals living in fluctuating environments 9 while detrimental for those inhabiting stable ones 10 . Here we utilize computational models to examine these hypotheses further and suggest that HGT can either increase or decrease the mean fitness depending on the kind of environment populated by the cell colony .",
        "rewrite_text": "Horizontal Gene Transfer (HGT) is a significant genetic mechanism that enhances genetic diversity and accelerates evolution. However, it can also have negative consequences, such as disrupting co-adapted gene complexes or introducing harmful variants into recipient genomes. In this study, we explore how HGT influences the average fitness of unicellular organisms evolving under various environmental conditions through computational modeling. Our findings reveal that HGT increases mean fitness in unicells exposed to fluctuating environments characterized by high levels of stress. Conversely, in environments with only minor fluctuations, HGT tends to reduce mean fitness by generating harmful mutations. In stable environments devoid of external stressors, HGT does not significantly affect mean fitness. These results suggest that HGT may have played a crucial role in the early stages of the evolution of life by enhancing adaptability to changing environments. HGT enables the rapid acquisition of new genomic traits among organisms with similar DNA sequences, contributing to increased genetic fitness within a genus, accelerating evolutionary processes, and facilitating adaptation. Nonetheless, HGT also poses risks by disrupting co-adapted gene pathways and introducing harmful variants. Thus, a comprehensive investigation into the effects of HGT on population dynamics is necessary. Previous research has indicated that HGT can be advantageous for organisms in variable environments but detrimental for those in stable ones. In this study, we employ computational models to delve deeper into these hypotheses, illustrating that the impact of HGT on mean fitness can vary depending on the environmental context surrounding the cell population.",
        "ori-fast-z-score": 1.3821894809301762,
        "water-fast-z-score": 7.888934916555407,
        "rewrite-fast-z-score": 1.7817416127494958
    },
    {
        "original_text": "We study the correlation function and entanglement entropy for one-dimensional quantum systems with disordered interactions, focusing on their scaling behavior at large distances or times. We show that these quantities are related by an exact formula which is valid both in the ground state and in thermal equilibrium states. The relation can be used to obtain information about the entanglement structure of the system from measurements of correlations only. In particular we discuss how this method allows us to extract the von Neumann entropy of the reduced density matrix corresponding to half of the chain using data obtained from numerical simulations. \nI. INTRODUCTORY REMARK\nThe aim of this work is twofold. First, we want to present some new results concerning the relationship between correlation functions and entanglement entropies in disordered quantum many-body systems. Second, we would like to introduce a novel approach to calculate entanglement properties of such systems based solely on measuring correlation functions. This second aspect will be discussed in more detail below; here let us briefly summarize our main result before turning into technical details.\nConsider a generic quantum-mechanical model defined on a lattice consisting of L sites labeled by integers i = 1, ..., L. Let |0⟩ denote its ground state (which may also represent any other eigenstate) and consider the following quantity:",
        "watermark_text": "We research the interaction function and entanglement entropy for one - dimensional quantum systems with disordered relationships , concentrating on their scaling behavior at large distances or times . We see that these quantities are related by an precise formula which is valid both in the ground state and in heat equilibrium states .The relation can be used to obtain knowledge about the entanglement structure of the process from measurements of correlations only . In particular we explain how this method enables us to extract the von Neumann entropy of the reduced density matrix corresponding to part of the chain using data derived from numerical simulations .I . INTRODUCTORY REMARK The goal of this project is twofold .First , we wish to provide some fresh results relating the relationship between correlation functions and entanglement entropies in disordered quantum several - bodies systems . Second , we may like to introduce a new approach to estimate entanglement properties of such systems relying solely on measuring correlation functions .This second aspect will be mentioned in more detail below ; here allow us briefly summarize our major result before turning into technical details . Consider a generic quantum - mechanical model formulated on a lattice consisting of L locations labeled by integers i = 1 , . . . , L . Let | 0 ⟩ denote its ground state ( which may also represent any other eigenstate ) and consider the following value :",
        "rewrite_text": "We investigate the interaction function and entanglement entropy in one-dimensional quantum systems characterized by disordered interactions, focusing on their scaling behavior over large distances and times. We find that these quantities are connected by a precise relation that holds true in both the ground state and thermal equilibrium states. This relationship allows us to gain insights into the entanglement structure of the system through measurements of correlation functions alone. Specifically, we describe how this methodology enables us to determine the von Neumann entropy of the reduced density matrix for a segment of the chain using data obtained from numerical simulations.\n\nI. INTRODUCTORY REMARKS\n\nThis project has two primary objectives. First, we aim to present new findings that elucidate the connection between correlation functions and entanglement entropies in disordered many-body quantum systems. Second, we intend to propose a novel approach to estimate the entanglement properties of these systems based solely on correlation function measurements. This latter aspect will be elaborated on further below, but we will first summarize our key findings before delving into the technical details. Consider a generic quantum model defined on a lattice with L sites indexed by integers i = 1, ..., L. Let |0⟩ represent its ground state (which may also correspond to any other eigenstate), and examine the following value:",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.410486691557943,
        "rewrite-fast-z-score": 0.4926646390821466
    },
    {
        "original_text": "We study the encoding problem for iterated function systems (IFS) on solenoids by unitary representations of wavelet groups, which are infinite-dimensional Lie groups generated by affine transformations of the real line. We show that if an IFS satisfies certain conditions then it can be encoded into a unitary representation of its associated wavelet group. This result is applied to prove that every self-similar fractal set with finite local complexity has a unique invariant measure up to scaling factors. The proof relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose boundaries have zero Lebesgue measures. In particular, we obtain a new characterization of self-affine carpets as those fractals satisfying this condition. Finally, we give examples showing how our results apply to some well-known fractals including Sierpinski gasket, Koch snowflake, von Koch curve, and Menger sponge.",
        "watermark_text": "We research the encoding problem for iterated function schemes ( IFS ) on solenoids by unitary representations of wavelet groups , which are infinite - dimensional Lie groups produced by affine transformations of the real line . We see that if an IFS satisfies certain conditions then it can be interpreted into a unitary representation of its identified wavelet group .This result is applied to prove that every self - similar fractal set with finite local complexity has a unique invariant measure up to scaling factors . The confirmation relies on the fact that any such fractal set can be approximated by a sequence of compact sets whose limits have zero Lebesgue measures .In particular , we obtain a new representation of self - affine carpets as those fractals satisfying this condition . Finally , we give instance demonstrating how our findings apply to some well - famous fractals including Sierpinski gasket , Koch snowflake , von Koch graph , and Menger sponge .",
        "rewrite_text": "We investigate the encoding problem associated with iterated function schemes (IFS) on solenoids through unitary representations of wavelet groups. These wavelet groups are infinite-dimensional Lie groups generated by affine transformations of the real line. Our findings indicate that if an IFS meets specific criteria, it can be associated with a unitary representation of its corresponding wavelet group. We utilize this result to demonstrate that every self-similar fractal set with finite local complexity possesses a unique invariant measure, subject to scaling factors. This confirmation is based on the principle that such fractal sets can be approximated by a series of compact sets, the limits of which have zero Lebesgue measure. Notably, we derive a new representation of self-affine carpets as fractals that fulfill this condition. Lastly, we provide examples showcasing how our results are relevant to well-known fractals, including the Sierpinski gasket, Koch snowflake, von Koch graph, and Menger sponge.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.9847319278346618
    },
    {
        "original_text": "The zeta-function is the main object in number theory and algebraic geometry.  In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non-trivial topology.   The basic idea is that one considers the Feynman path integral over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm.    This leads naturally to the concept of a  quantum torus  whose zeta function encodes information about the spectrum of the corresponding QFT.   We will also discuss some recent results concerning the relation between the zeta functions of certain families of quantum tori and their associated modular forms. Quantum Field Theory (QFT) has been developed into a powerful tool for studying physical phenomena at the most fundamental level. However, many interesting questions remain open due to the difficulty of performing calculations beyond perturbation theory. One way out of these difficulties is to use ideas from mathematics such as those coming from number theory or algebraic geometry. In particular, we are interested in using the so-called  zeta-functions  of algebraic varieties to obtain new insights into QFTs. These objects encode important information about the underlying physics but they are notoriously difficult to compute explicitly. In my talk I ll give examples where explicit computations have been performed successfully and show how these techniques could lead to further progress in our understanding of QFTs.",
        "watermark_text": "The zeta - function is the main object in number theory and algebraic topology . In this talk I will explain how it can be used to study quantum field theories on curved spacetimes with non - trivial geometry .The basic idea is that one considers the Feynman line expansion over all fields on spacetime as an infinite dimensional functional integration which can then be regularized by replacing the space of fields by a finite dimensional vector space equipped with a suitable norm . This leads naturally to the idea of a quantum torus whose zeta function encodes data about the spectrum of the associated QFT .We will also discuss some latest findings concerning the relation between the zeta functions of certain classes of quantum tori and their associated modular forms . Quantum Field Theory ( QFT ) has been built into a powerful tool for studying physical phenomena at the most important level .However , many interesting problems continue open thanks to the difficulty of performing calculations beyond perturbation theory . One path out of these problems is to use insights from mathematics such as those coming from number theory or algebraic theory .In particular , we are concerned in use the so - called zeta - functions of algebraic fields to obtain new understanding into QFTs . These items encode crucial data about the fundamental theories but they are notoriously difficult to compute accurately .In my talk I ll offer examples where explicit computations have been performed effectively and suggest how these procedures could lead to further progress in our understanding of QFTs .",
        "rewrite_text": "The zeta function is a central focus in number theory and algebraic topology. In this talk, I will discuss its applications in studying quantum field theories (QFTs) on curved spacetimes with complex geometries. The primary concept involves considering the Feynman line expansion over all fields in spacetime as an infinite-dimensional functional integration, which can be regularized by substituting the field space with a finite-dimensional vector space that possesses an appropriate norm. This approach gives rise to the notion of a quantum torus, whose zeta function encodes important information about the spectrum of the related QFT. Additionally, we will explore recent findings regarding the connections between the zeta functions of specific classes of quantum tori and their corresponding modular forms. QFT has emerged as a powerful framework for analyzing physical phenomena at a fundamental level, yet many intriguing challenges remain due to the complexities involved in calculations beyond perturbation theory. One potential avenue to address these challenges is by leveraging mathematical insights from fields such as number theory or algebraic theory. Specifically, we aim to utilize the zeta functions of algebraic fields to enhance our understanding of QFTs. While these zeta functions contain vital information about fundamental theories, they are notoriously difficult to compute with precision. In my presentation, I will provide examples where effective explicit computations have been performed and propose how these methods could lead to further advancements in our comprehension of QFTs.",
        "ori-fast-z-score": 0.5570860145311556,
        "water-fast-z-score": 6.009252125773315,
        "rewrite-fast-z-score": 0.1889822365046136
    },
    {
        "original_text": "We study droplet excitations in the 2D spin-glass model with nearest-neighbor interactions and random ferromagnetic bonds, which is known to have an infinite number of metastable states at zero temperature. We show that this system has two different types of droplets: small ones are similar to those found in other models studied previously; large droplets are characterized by their fractal structure. The latter type can be viewed as a generalization of the droplet picture proposed earlier for the 3D Ising spin glasses. In addition we find that there exists another class of excitations -the so-called  giant droplets -which are not present in any of these systems. These giant droplets are responsible for the non-universal behavior observed numerically near the critical point. Finally, we argue that our results provide strong numerical support for the existence of a new phase transition line between the paramagnetic state and the spin-glass one. \nI. INTRODUCTORY REMARK\nThe concept of  droplet excitations  was introduced originally within the framework of the mean-field theory  1  . It describes how local perturbations affect global properties of the system. This idea turned out to be very useful when applied to various disordered systems such as spin glasses  2  , structural glasses  3  or vortex lattices  4  .\nIn particular it allowed to explain many features of the low-temperature thermodynamics of spin glasses  5  . However, despite its successes, the original droplet picture suffers from some serious drawbacks  6  : first, it does not take into account fluctuations around the saddle-point solution  7 ; secondly, it predicts a finite density of droplets even at T = 0  8  ; thirdly, it cannot describe properly the dynamics of the system  9  . To overcome these difficulties several modifications were suggested  10  . One of them  11  leads to the following expression for the free energy F(T ) per site: \nwhere f0 is the free-energy density of the reference system (e.g., the pure ferromagnet), Ns is the total number of spins, V is the volume occupied by each droplet",
        "watermark_text": "We test droplet excitations in the 2D spin - glass model with nearest - neighbor interactions and random ferromagnetic bonds , which is known to have an endless number of metastable states at zero temperature . We see that this scheme has two different kinds of droplets : tiny ones are comparable to those present in other models studied ago ; small droplets are marked by their fractal structure .The latter type can be viewed as a generalization of the droplet picture suggested earlier for the 3D Ising spin glasses . In addition we find that there exists another class of excitations - the so - called large droplets - which are not present in any of these systems .These huge droplets are responsible for the non - universal behavior observed numerically near the pivotal point . Finally , we claim that our findings provide strong mathematical support for the existence of a new phase shift line between the paramagnetic state and the spin - glass one .I . INTRODUCTORY REMARK The concept of droplet excitations was introduced originally within the framework of the mean - field principle 1 .It details how local perturbations impact global properties of the system . This idea turned out to be very useful when applied to numerous disordered systems such as spin glasses 2 , structural glasses 3 or vortex lattices 4 .In particular it able to explain many features of the small - temperature thermodynamics of spin glasses 5 . However , despite its victories , the original droplet picture suffers from some serious drawbacks 6 : first , it does not take into consideration fluctuations around the saddle - point solution 7 ; secondly , it predicts a finite concentration of droplets even at T = 0 8 ; thirdly , it lacks explain adequately the dynamics of the system 9 .To solve these problems several amendments were recommended 10 . One of them 11 leads to the following expression for the free energy F ( T ) per site : where f0 is the free - energy density of the reference system ( e . g . , the pure ferromagnet ) , Ns is the total quantity of spinning , V is the volume occupied by each droplet",
        "rewrite_text": "We investigate droplet excitations within the 2D spin-glass model characterized by nearest-neighbor interactions and random ferromagnetic bonds, which is known to exhibit an infinite number of metastable states at zero temperature. Our findings reveal two distinct types of droplets: small droplets that are comparable to those observed in previous models, and smaller droplets distinguished by their fractal structure. This latter category can be interpreted as a broader interpretation of the droplet concept previously proposed for 3D Ising spin glasses. Additionally, we identify a different class of excitations known as large droplets, which are absent in other systems. These substantial droplets are responsible for the non-universal behavior seen numerically near the critical point. Ultimately, we assert that our results provide robust mathematical evidence for the existence of a new phase transition line separating the paramagnetic and spin-glass states.\n\n**I. INTRODUCTORY REMARKS**\n\nThe idea of droplet excitations was initially formulated within the mean-field framework. This concept illustrates how localized disturbances can influence the global characteristics of the system. It has proven to be an effective tool for analyzing various disordered systems, including spin glasses, structural glasses, and vortex lattices. Notably, it has been instrumental in elucidating many aspects of low-temperature thermodynamics in spin glasses. However, despite its successes, the original droplet theory has significant limitations: it neglects fluctuations around the saddle-point solutions, predicts a finite density of droplets even at T = 0, and inadequately describes the system's dynamics. To address these issues, several modifications have been proposed. One such amendment leads to a revised expression for the free energy per site, where f0 denotes the free energy density of a reference system (such as a pure ferromagnet), and Ns represents the total number of spins, while V indicates the volume occupied by each droplet.",
        "ori-fast-z-score": 1.1666666666666667,
        "water-fast-z-score": 7.72046849632954,
        "rewrite-fast-z-score": -0.3380617018914066
    },
    {
        "original_text": "We have searched for radio transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz using archival data obtained by the Very Large Array (VLA) over the past 20 years. We find that most of these sources are extragalactic, but we also detect several Galactic objects including pulsars, supernova remnants, and flare stars. The majority of our sample is comprised of previously uncatalogued sources; however, we recover many known variable sources such as blazars and gamma-ray burst afterglows. Our results demonstrate the power of combining large amounts of archival VLA data into one coherent dataset. This work was supported by NSF grant AST-0907860. In this Letter, we present an analysis of all available archived Very Large Array (V LA) observations taken since 1990. These data were collected during various observing programs aimed primarily at studying distant galaxies or nearby star forming regions. However, they contain valuable information about fainter transient phenomena occurring within our Galaxy. By searching through more than 10 000 hours of observation time spread across nearly 2000 epochs, we identify thousands of new faint radio sources which appear only once or twice in each epoch s data set. Most of these sources are extragalaxtic, but we also detect numerous Galactic objects including pulsar wind nebulae, supernova remnants, flare stars, and other types of active galactic nuclei. Many of these newly discovered sources are not included in existing catalogs because their low signal-to-noise ratio makes them difficult to detect when observed individually. However, by combining multiple epochs together, we can boost the sensitivity of our survey enough to detect even very weak signals.",
        "watermark_text": "We have searched for signal transients with flux densities between 0 . 1 and 1 mJy at speeds ranging from 4 to 8 GHz using archival measurements obtained by the Very Large Array ( VLA ) over the previous 20 decades . We see that most of these sources are extragalactic , but we also observe numerous Galactic items including pulsars , supernova remnants , and flare stars .The majority of our sample is comprised of previously uncatalogued sources ; however , we recover several known variable sources such as blazars and gamma - ray flare afterglows . Our results show the power of combining enormous amounts of archival VLA information into one integrated dataset .This project was supported by NSF grant AST - 0907860 . In this Letter , we present an assessment of all available archived Very Large Array ( V LA ) observations made since 1990 .These data were collected during various observing programs aimed exclusively at studying nearby galaxies or neighboring star producing regions . However , they contain significant information about fainter transient phenomena occurring within our Galaxy .By looking through more than 10 000 hours of study distance distributed across nearly 2000 epochs , we identify thousands of new faint radio sources which appear only once or repeatedly in each epoch s information pool . Most of these sources are extragalaxtic , but we also identify numerous Galactic bodies including pulsar wind nebulae , supernova remnants , flare stars , and other types of active galactic nuclei .Many of these newly discovered sources are not included in existing catalogs because their low signal - to - noise proportion creates them harder to identify when observed individually . However , by combining multiple epochs combined , we can boost the sensitivity of our survey sufficiently to identify especially very weak signals .",
        "rewrite_text": "We have conducted a search for signal transients with flux densities between 0.1 and 1 mJy at frequencies ranging from 4 to 8 GHz, utilizing archival measurements from the Very Large Array (VLA) collected over the past 20 years. Our findings indicate that most of these sources are extragalactic, although we have also identified a significant number of Galactic objects, such as pulsars, supernova remnants, and flare stars. While the majority of our sample comprises previously uncatalogued sources, we did recover several known variable sources, including blazars and gamma-ray flare afterglows. This study highlights the effectiveness of integrating vast amounts of archival VLA data into a cohesive dataset. Supported by NSF grant AST-0907860, this Letter presents an analysis of all available archived VLA observations since 1990, which were collected through various programs focused mainly on nearby galaxies and regions of star formation. Nonetheless, these observations contain valuable information regarding fainter transient phenomena within our Galaxy. By analyzing over 10,000 hours of data across nearly 2,000 epochs, we have identified thousands of new faint radio sources that appear either once or multiple times in the data from each epoch. While many of these sources are extragalactic, we also discovered numerous Galactic objects, including pulsar wind nebulae, supernova remnants, flare stars, and other active galactic nuclei. Many of these newly found sources are not listed in existing catalogs due to their low signal-to-noise ratio, which complicates their identification when observed individually. However, by combining data from multiple epochs, we are able to enhance the sensitivity of our survey enough to detect even the faintest signals.",
        "ori-fast-z-score": 1.3416407864998738,
        "water-fast-z-score": 9.03371462909915,
        "rewrite-fast-z-score": 2.0426487199475707
    },
    {
        "original_text": "We study the effect of removing nodes in scale-free networks with power-law degree distribution P(k) ~ k^{-γ}, where γ is between 2 and 3. We find that when we remove one node at random or according to its degree, the average path length increases logarithmically as the number of removed nodes grows linearly. \n \n This logarithmic increase can be explained by considering how the removal of a node affects the shortest paths through it. The probability for an edge to belong to such a path decreases exponentially with distance from the deleted node. As a result, most edges affected are those which were close to the deleted node before deletion. These short-range connections form a small-world subnetwork around each deleted node. Removing these local links leads to longer distances within this subnetwork but leaves the global connectivity unchanged. Thus, the overall effect of removing many nodes is similar to adding new long-range links into the system.",
        "watermark_text": "We explore the impact of eliminating nodes in scale - free networks with power - law degree function P ( h ) ~ k ^ { - γ } , where γ is between 2 and 3 . We see that when we eliminate one vertex at random or according to its degree , the average route length grows logarithmically as the quantity of expelled connections grows linearly .This logarithmic increase can be described by examining how the removal of a node impacts the shortest trails through it . The probability for an path to belong to such a path reduces exponentially with distance from the eliminated node .As a result , most paths affected are those which were close to the erased node before deletion . These short - range connections form a small - global subnetwork around each deleted node .Removing these local connections leads to longer distances within this subnetwork but leaves the global access unchanged . Thus , the overall impact of eliminating many nodes is analogous to introducing additional large - range connections into the system .",
        "rewrite_text": "We investigate the effects of node removal in scale-free networks characterized by a power-law degree distribution P(h) ~ k^{-γ}, where γ takes values between 2 and 3. Our analysis reveals that when a vertex is randomly removed or removed based on its degree, the average path length increases logarithmically as the number of removed connections rises linearly. This logarithmic growth can be understood by looking at how the removal of a node affects the shortest paths that traverse it. The likelihood of a given path being dependent on the removed node diminishes exponentially with distance from that node. Consequently, the paths that are most impacted are those that were located nearby prior to the deletion. These close-range connections create a small, localized subnetwork around each removed node. The loss of these local connections results in longer distances within this subnetwork, while the overall global connectivity remains unaffected. Therefore, the cumulative effect of removing multiple nodes resembles the introduction of additional long-range connections within the network.",
        "ori-fast-z-score": -1.8888888888888888,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": -1.193117518002609
    },
    {
        "original_text": "We present an analysis on the spatial and velocity distributions of the young (<10Myr) stars within 0.5pc of Sgr A*, based on our spectroscopic survey with Subaru/HDS. We find that these stars are distributed into two groups along the line-of-sight; one is located at ~0.1pc to the west side of Sgr A* and another is found at ~0.3pc to its east side. The former group has a mean radial velocity of -200km/sec while the latter shows +100km/sec. These results suggest that there exist two distinct populations of young stars around Sgr A*; one is associated with the clockwise disk-like structure seen in infrared images and the other may be related to the counter-clockwise rotating ring-like feature recently discovered by Genzel et al. (2003) . In addition we have identified several new candidate members for the clockwise disk population.",
        "watermark_text": "We present an assessment on the spatial and speed distributions of the young ( < 10Myr ) stars within 0 . 5pc of Sgr A * , using on our spectroscopic study with Subaru / HDS . We see that these stars are distributed into two groups along the line - of - view ; one is situated at ~ 0 . 1pc to the west end of Sgr A * and another is found at ~ 0 . 3pc to its east side .The first group has a mean radial speed of - 200km / sec while the former shows + 100km / sec . These data suggest that there exist two separate populations of young galaxies around Sgr A * ; one is associated with the clockwise disk - like structure seen in infrared images and the other may be connected to the counter - clockwise expanding box - like feature newly discovered by Genzel et al .( 2003 ) . In addition we have discovered numerous new likely groups for the clockwise disk population .",
        "rewrite_text": "We provide an analysis of the spatial and velocity distributions of young stars (less than 10 million years old) within 0.5 parsecs of Sgr A* based on our spectroscopic study conducted with Subaru/HDS. Our findings indicate that these stars are divided into two distinct groups along the line of sight: one group is located approximately 0.1 parsecs to the west of Sgr A*, while the other is situated about 0.3 parsecs to the east. The western group has a mean radial velocity of -200 km/s, whereas the eastern group shows a velocity of +100 km/s. This evidence suggests the presence of two separate populations of young stars around Sgr A*: one associated with the clockwise disk-like structure observed in infrared images, and the other possibly related to the counter-clockwise expanding box-like feature recently identified by Genzel et al. (2003). Additionally, we have identified several new potential groups within the clockwise disk population.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present millimeter (mm) and radio observations for four quasars at redshifts 6<z<7, including the highest-redshift quasar known to date. We detect all sources in our sample with high signal-to-noise ratio using the Atacama Large Millimeter/submillimeter Array (ALMA). The observed spectral energy distributions are well-fit by models that include both synchrotron emission from relativistic jets and thermal dust emission heated by star formation activity. Our results show that these high-redshift quasars have properties similar to those seen in lower-redshift counterparts. These findings suggest that massive black holes grow rapidly during this early epoch of cosmic time. This work is based on data obtained as part of ALMA program 2013.1.00010.S. Millimeter-wave and radio observations provide important insights into the physical processes occurring within distant galaxies. In particular, they can be used to study the growth history of supermassive black holes (SMBHs), which power active galactic nuclei (AGNs).\nIn recent years, several SMBH candidates have been discovered at redshifts greater than six through their strong rest-frame ultraviolet (UV) continua  1  . However, it remains unclear how such objects evolve over cosmological timescales because there has not yet been any direct detection of AGN signatures associated with them  2  .\nThe most promising method for detecting AGN signatures involves observing the mm-wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets  3  , or via the free-free emission from ionized gas surrounding the central engine  4  . Previous studies have shown that some high-redshift quasars exhibit bright mm-continuum fluxes  5, 6  ; however, only one source was detected in each case  7, 8  . Here we report new mm-and radio-continuum observations made toward four quasars at redhifts between 6 < z < 7, including the highest-redshi",
        "watermark_text": "We report millimeter ( mm ) and radio observations for four quasars at redshifts 6 < z < 7 , including the highest - redshift quasar known to date . We detect all sources in our sample with high signal - to - noise ratio utilizing the Atacama Large Millimeter / submillimeter Array ( ALMA ) .The observed spectral power distributions are best - fitting by methods that include both synchrotron emission from relativistic jets and thermal dust absorption warmed by galaxy formation activity . Our results show that these high - redshift quasars have properties similar to those shown in lower - redshift cousins .These studies propose that massive black holes occur dramatically during this early epoch of cosmic time . This work is based on data obtained as part of ALMA program 2013 . 1 . 00010 . S .Millimeter - wave and radio observations offer important knowledge into the natural processes arising within distant galaxies . In particular , they can be used to study the development period of supermassive black holes ( SMBHs ) , which fuel active galactic nuclei ( AGNs ) .In recent years , various SMBH candidates have been detected at redshifts greater than six through their powerful rest - frame ultraviolet ( UV ) continua 1 . However , it remains obscure how such objects evolve over cosmological timescales because there has not already been any direct detection of AGN signatures identified with them 2 .The most feasible method for detecting AGN signatures involves studying the mm - wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets 3 , or via the free - free emission from ionized gas surrounding the main engine 4 . Earlier investigations have shown that some high - redshift quasars exhibit bright mm - continuum fluxes 5 , 6 ; however , only one source was seen in each case 7 , 8 .Here we publish new mm - and radio - continuum measurements made toward four quasars at redhifts between 6 < z < 7 , including the highest - redshi",
        "rewrite_text": "We present millimeter (mm) and radio observations of four quasars at redshifts ranging from 6 to 7, including the highest-redshift quasar identified to date. Utilizing the Atacama Large Millimeter/submillimeter Array (ALMA), we successfully detected all sources in our sample with a high signal-to-noise ratio. The observed spectral power distributions are best explained by models that incorporate both synchrotron emission from relativistic jets and thermal dust absorption due to galaxy formation activities. Our findings indicate that these high-redshift quasars possess characteristics similar to their lower-redshift counterparts, suggesting that massive black holes were forming dramatically during this early epoch of cosmic history. This research is based on data from ALMA program 2013.1.00010.S. Millimeter-wave and radio observations provide crucial insights into the processes occurring within distant galaxies, particularly the developmental phase of supermassive black holes (SMBHs) that power active galactic nuclei (AGNs). Recently, several SMBH candidates have been detected at redshifts above six through their strong rest-frame ultraviolet (UV) emissions. However, the evolution of these objects over cosmological timescales remains unclear due to the lack of direct detection of AGN signatures associated with them. The most effective approach to identifying AGN signatures involves analyzing the mm-wavelength continuum produced by hot electrons accelerated along magnetic field lines in relativistic jets or through free-free emission from ionized gas surrounding the central engine. Previous studies have shown that some high-redshift quasars exhibit significant mm-continuum fluxes; however, only one source has been observed in each instance. In this paper, we present new mm and radio continuum measurements for four quasars at redshifts between 6 and 7, including the highest-redshift quasar known.",
        "ori-fast-z-score": 0.5144957554275265,
        "water-fast-z-score": 6.236810901332355,
        "rewrite-fast-z-score": 1.8209309360006518
    },
    {
        "original_text": "We present an analysis of the distribution and properties of open clusters in the Galactic anticenter region, using data from the Two Micron All Sky Survey (2MASS). We have compiled a list of all open clusters with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun. The total number is about 1000. This sample includes most known open clusters in this part of the Galaxy. Using photometric distances derived by fitting theoretical stellar evolutionary tracks to observed color-magnitude diagrams for each cluster we construct maps showing their spatial distributions as well as their luminosity functions. Our results show that there are two distinct populations of open clusters: one population located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr; another population located at galactocenric radii greater than 7 kpc which has ages older than 5 Gyr.",
        "watermark_text": "We publish an assessment of the distribution and features of open complexes in the Galactic anticenter region , using data from the Two Micron All Sky Survey ( 2MASS ) . We have published a list of all open complexes with angular diameters larger than 1 arcmin within a distance of 8 kpc from the Sun .The total number is about 1000 . This sample comprises most notable open objects in this area of the Galaxy .Using photometric distances derived by fitting experimental stellar evolutionary tracks to observed color - magnitude diagrams for each cluster we create maps showing their spatial distributions as well as their luminosity functions . Our results show that there are two different populations of open complexes : one community located at galactocentric radii between 4 and 6 kpc which has ages less than 3 Gyr ; another population located at galactocenric radii greater than 7 kpc which has ages younger than 5 Gyr .",
        "rewrite_text": "We present an analysis of the distribution and characteristics of open complexes in the Galactic anticenter region, utilizing data from the Two Micron All Sky Survey (2MASS). Our research includes a compilation of all open complexes with angular diameters exceeding 1 arcminute and located within 8 kpc from the Sun, totaling approximately 1,000 complexes. This dataset encompasses the most prominent open objects in this area of the Galaxy. By employing photometric distances derived from fitting observed color-magnitude diagrams to empirical stellar evolutionary tracks for each cluster, we generate maps displaying their spatial distributions and luminosity functions. Our findings reveal the existence of two distinct populations of open complexes: one group situated at galactocentric radii between 4 and 6 kpc, with ages under 3 Gyr, and another group at galactocentric radii greater than 7 kpc, characterized by ages younger than 5 Gyr.",
        "ori-fast-z-score": -0.6622661785325219,
        "water-fast-z-score": 3.9391929857916765,
        "rewrite-fast-z-score": 0.5252257314388902
    },
    {
        "original_text": "We study information diffusion on networks with multiple communities, where each community has its own set of nodes and edges. We propose an epidemic model that captures the effect of both global and local interactions between users within different communities. The proposed model is based on two key ideas: (1) we assume that there are some influential individuals who can spread information to their neighbors faster than others; and (2) we allow for cross-influences among neighboring communities through these influential individuals. Our main goal is to understand how the structure of the underlying network affects the spreading process. In particular, our contributions include:  1. We develop a novel mathematical framework to analyze the dynamics of information diffusion under the proposed epidemic model. 2. We show that if all communities have similar sizes then the number of infected nodes at time t grows as O(t log n), where n denotes the total number of nodes in the network. 3. We prove that if one community dominates the other ones by size then the number of infected individuals grows exponentially fast. 4. Finally, we provide extensive numerical experiments to validate our theoretical results.",
        "watermark_text": "We research information diffusion on networks with many communities , where each community has its own set of vertices and edges . We suggest an outbreak model that captures the impact of both regional and local interactions between users within various communities .The proposed theory is based on two fundamental ideas : ( 1 ) we suppose that there are some influential citizens who can distribute information to their relatives faster than others ; and ( 2 ) we allow for cross - impact among neighboring communities through these powerful individuals . Our main goal is to explain how the composition of the underlying community affects the spreading process .In particular , our achievements include : 1 . We develop a innovative computational framework to analyze the dynamics of information diffusion under the suggested disease model .2 . We see that if all communities have equal sizes then the quantity of infected sites at time t grows as O ( t log n ) , where n represents the total quantity of nodes in the organization .3 . We establish that if one community dominates the other ones by size then the quantity of infected individuals grows exponentially rapidly .4 . Finally , we provide extensive numerical studies to validate our theoretical results .",
        "rewrite_text": "We investigate information diffusion across networks with multiple communities, each defined by its own set of vertices and edges. Our proposed outbreak model accounts for both regional and local interactions among users within these diverse communities. The theory is founded on two key principles: (1) we assume the presence of influential individuals who can disseminate information to their networks more quickly than others, and (2) we acknowledge the cross-influence that can occur among adjacent communities through these influential members. Our primary objective is to understand how the structure of the underlying community influences the spread of information. Our contributions include: 1. The development of an innovative computational framework for analyzing the dynamics of information diffusion based on the proposed model. 2. Evidence that when all communities are of equal size, the number of infected nodes at time t increases as O(t log n), with n representing the total number of nodes in the network. 3. Establishing that if one community is significantly larger than others, the number of infected individuals increases at an exponential rate. 4. Comprehensive numerical studies to support our theoretical findings.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": -1.0314212462587933
    },
    {
        "original_text": "We show that the problem of approximating Nash equilibria in two-player zero-sum entangled games is NP-hard, even if one restricts attention to pure strategies and exact equilibria.  We also prove an analogous result for mixed-strategy equilibria. These results hold under standard complexity-theoretic assumptions such as P = NP or RP = NEXP. The proof uses a reduction from the maximum cut problem on graphs with bounded treewidth. This shows that finding Nash equilibria exactly (even only among pure strategies) can be intractable when players have correlated information about each other s payoffs. Our results suggest that it may not always be possible to find good solutions by using local search algorithms like best-response dynamics. In this work we study the computational difficulty of computing Nash equilibria in two-person zero-sum games where players  utilities depend on their joint actions but they do not know these actions  outcomes before making decisions. Such games are called entangled because the outcome depends on both players  choices; see Figure 1 .",
        "watermark_text": "We see that the question of approximating Nash equilibria in two - player zero - sum entangled players is NP - hard , even if one restricts focus to pure strategies and exact equilibria . We also prove an analogous result for mixed - strategy equilibria .These results hold under standard complexity - theoretic conditions such as P = NP or RP = NEXP . The proof uses a reduction from the maximum cut question on graphs with bounded treewidth .This shows that finding Nash equilibria precisely ( even only among true schemes ) can be intractable when participants have correlated information about each other s payoffs . Our results show that it could not always be possible to find good answers by using local search methods like first - response dynamics .In this research we study the theoretical problem of computing Nash equilibria in two - person zero - sum games where players utilities depend on their joint actions but they do not understand these actions outcomes before thinking decisions . Such matches are called entangled because the result relies on both participants decisions ; use Figure 1 .",
        "rewrite_text": "We demonstrate that approximating Nash equilibria in two-player zero-sum games involving entangled players is NP-hard, even when we limit our consideration to pure strategies and exact equilibria. Additionally, we establish a similar result for mixed-strategy equilibria. These findings are valid under standard complexity-theoretic assumptions, such as P = NP or RP = NEXP. Our proof employs a reduction from the maximum cut problem in graphs with bounded treewidth. This indicates that accurately determining Nash equilibria—even when restricted to true schemes—can be infeasible when players have correlated information regarding each other's payoffs. Our results suggest that local search techniques, such as first-response dynamics, may not always yield effective solutions. In this study, we explore the theoretical challenge of calculating Nash equilibria in two-person zero-sum games where players' utilities are contingent on their joint actions, but they lack understanding of the outcomes of these actions prior to making decisions. Such games are referred to as entangled, as the results depend on the decisions of both participants; refer to Figure 1 for clarification.",
        "ori-fast-z-score": 0.6708203932499369,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": -0.5423261445466404
    },
    {
        "original_text": "We present new structural parameters for 23 globular clusters (GCs) in M31, derived using HST/ACS images taken with the F606W filter. We use these data to test whether GCs follow the same scaling relations as elliptical galaxies do on the so-called  fundamental plane . The results show that most of our sample follows this relation within uncertainties, but we also find some outliers which are likely due to their different formation histories or dynamical states. In addition, we compare our results with those obtained by other authors who used ground-based observations. Our analysis shows that there is no significant difference between the two datasets when they are analyzed consistently. Finally, we discuss possible reasons why previous studies have found conflicting results about the existence of such a relationship among GC systems. This work was supported by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane",
        "watermark_text": "We report new structural values for 23 globular complexes ( GCs ) in M31 , obtained using HST / ACS images obtained with the F606W filter . We use these information to test whether GCs obey the same scaling relations as elliptical galaxies do on the so - called core plane .The results show that most of our sample follows this relation within uncertainties , but we also find some outliers which are likely due to their different formation histories or dynamical states . In addition , we compare our findings with those acquired by other researchers who used ground - based observations .Our study shows that there is no major difference between the two datasets when they are examined consistently . Finally , we explain possible reasons why previous research have discovered differing results about the existence of such a connection among GC systems .This project was supported by NASA grant NAG5 - 12140 . Keywords : Globular cluster ; Fundamental Plane",
        "rewrite_text": "We present new structural measurements for 23 globular clusters (GCs) in M31, obtained using HST/ACS images captured with the F606W filter. This data allows us to examine whether GCs adhere to the same scaling relations as elliptical galaxies on the so-called core plane. Our findings indicate that most of our sample aligns with this relationship within the margins of uncertainty; however, we also identify some outliers that likely result from distinct formation histories or dynamical conditions. Additionally, we compare our results with those from other studies based on ground-based observations and find no significant differences between the two datasets when analyzed consistently. Finally, we discuss potential reasons for the varying conclusions drawn by previous research regarding the existence of connections within GC systems. This project was funded by NASA grant NAG5-12140. Keywords: Globular cluster; Fundamental Plane.",
        "ori-fast-z-score": 1.1338934190276817,
        "water-fast-z-score": 5.842005842008763,
        "rewrite-fast-z-score": 1.016001016001524
    },
    {
        "original_text": "We present the results of our study on astrophysical accretion phenomena in black holes and neutron stars, which are considered to be analogues for gravitational wave sources such as binary black hole mergers or neutron star -black hole binaries. We have investigated how these systems can produce detectable signals at radio wavelengths by using numerical simulations with high spatial resolution. The main goal is to understand whether we could detect any signal associated with the merger process itself (i.e., before the final plunge) through observations of electromagnetic counterparts. In addition, we also studied the possibility that some of the observed gamma-ray bursts may originate from similar processes occurring in supermassive black holes. Our results show that there should exist observable signatures during the inspiral phase of the system evolution. These include quasi-periodic oscillations produced by the orbital motion of matter around the central compact object, and rapid changes in luminosity due to the interaction between the infalling gas stream and the surrounding disk.",
        "watermark_text": "We present the conclusion of our research on astrophysical accretion events in black holes and neutron galaxies , which are considered to be analogues for gravitational wave sources such as binary white hole mergers or neutron star - black hole binaries . We have researched how these systems can generate detectable frequencies at radio wavelengths by using numerical simulations with high visual resolution .The main goal is to study whether we may detect any sound identified with the merger process itself ( i . e . , before the last plunge ) through observations of electromagnetic counterparts . In addition , we also examined the prospect that some of the reported gamma - ray bursts would occur from identical processes resulting in supermassive black holes .Our results show that there should exist observable signatures during the inspiral phase of the system progression . These include quasi - periodic oscillations formed by the orbital movement of matter around the main compact body , and fast changes in luminosity due to the interaction between the infalling gas stream and the nearby disk .",
        "rewrite_text": "We summarize our findings on astrophysical accretion events related to black holes and neutron stars, which serve as analogs for potential gravitational wave sources, such as binary white hole mergers and neutron star-black hole binaries. Our research focused on how these systems can produce detectable radio frequency emissions, utilizing high-resolution numerical simulations. The primary aim was to determine whether we could identify sound linked to the merger process (i.e., before the final plunge) through observations of electromagnetic counterparts. Furthermore, we explored the possibility that some observed gamma-ray bursts could arise from similar processes leading to the formation of supermassive black holes. Our results indicate that observable signatures should emerge during the inspiral phase of these systems, including quasi-periodic oscillations caused by the orbital motion of matter around the central compact object, as well as rapid luminosity variations resulting from the interaction between the infalling gas stream and the surrounding disk.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.888888888888889,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "We present an approach to the analysis and design of stochastic gene regulatory networks based on deterministic models that are derived by averaging over all possible realizations of the underlying random process.  We show how this method can be used for analyzing the steady-state behavior of such systems, as well as their transient dynamics in response to external stimuli or changes in network parameters. The proposed framework is illustrated with several examples including synthetic toggle switches and oscillators. Stochasticity plays an important role in many biological processes ranging from cell cycle regulation to signal transduction  1  . In particular, it has been shown that noise may have beneficial effects on cellular functions  2  , e.g., by enhancing the sensitivity of cells to signals  3  .\nThe study of stochastic gene regulatory networks (GRNs) requires the development of new mathematical tools capable of capturing both the intrinsic fluctuations associated with molecular interactions and extrinsic perturbations due to environmental factors  4  . Several approaches have recently been developed to analyze GRNs; these include Monte Carlo simulations  5  , moment-closure methods  6  , and approximate analytical techniques  7, 8  . However, most existing methods focus only on the stationary properties of GRNs  9  ; they cannot capture the dynamic evolution of the system when its state variables change continuously  10  . Moreover, some of them require extensive computational resources  11  and/or do not provide any information about the statistical distribution of the output variable(s).\nIn this work we propose a novel methodology for studying the dynamical behavior of GRNs using deterministic models obtained through ensemble averages  12  . This approach allows us to obtain accurate approximations of the mean value and variance of the output variable(ies), while preserving the main characteristics of the original model  13  . Our results demonstrate that our technique provides useful insights into the functioning of complex biochemical networks without requiring excessive computational effort.",
        "watermark_text": "We present an perspective to the analysis and design of stochastic gene regulatory networks based on deterministic descriptions that are derived by averaging over all possible realizations of the underlying random process . We see how this method can be used for studying the stable - state dynamics of such systems , as well as their transient structure in reaction to external stimuli or alterations in system parameters .The proposed framework is depicted with many instance including synthetic toggle switches and oscillators . Stochasticity plays an important role in multiple biological pathways including from cell cycle regulation to signal transduction 1 .In particular , it has been shown that noise might have beneficial influence on cell functions 2 , e . g . , by increased the sensitivity of cells to stimuli 3 . The investigation of stochastic gene regulatory networks ( GRNs ) need the development of new computational tools capable of depicting both the intrinsic fluctuations associated with biological interactions and extrinsic perturbations due to environmental factors 4 .Several approaches have recently been constructed to analyze GRNs ; these involve Monte Carlo simulations 5 , moment - collapse technique 6 , and exact mathematical techniques 7 , 8 . However , most existing techniques concentrate only on the stationary features of GRNs 9 ; they cannot record the dynamic development of the system when its state values change continuously 10 .Moreover , some of them require extensive computational resources 11 and / or do not offer any knowledge about the empirical distribution of the output parameter ( s ) . In this research we propose a new methodology for studying the dynamical behavior of GRNs using deterministic descriptions generated through ensemble averages 12 .This method enables us to obtain precise approximations of the mean value and variance of the output parameter ( ies ) , while preserving the main characteristics of the previous version 13 . Our results show that our technique provides useful insights into the functioning of complex biochemical organizations without using inappropriate computational time .",
        "rewrite_text": "We present a new approach to the analysis and design of stochastic gene regulatory networks, focusing on deterministic descriptions derived from averaging all possible realizations of the underlying random processes. This method allows us to investigate the stable-state dynamics of these systems, as well as their transient responses to external stimuli or changes in system parameters. We illustrate this framework with various examples, including synthetic toggle switches and oscillators. Stochasticity is crucial in numerous biological pathways, ranging from cell cycle regulation to signal transduction. Notably, research has shown that noise can positively influence cell functions, such as enhancing sensitivity to stimuli. Analyzing stochastic gene regulatory networks (GRNs) necessitates the development of new computational tools capable of capturing both the intrinsic fluctuations from biological interactions and the extrinsic disturbances caused by environmental factors. Recently, several approaches have emerged for analyzing GRNs, including Monte Carlo simulations, moment-collapse techniques, and exact mathematical methods. However, most current techniques primarily focus on the stationary characteristics of GRNs and fail to account for the dynamic evolution of the system as its state values continuously change. Additionally, many existing methods require substantial computational resources and do not provide insights into the empirical distribution of output parameters. In this study, we propose a novel methodology for examining the dynamical behavior of GRNs through deterministic descriptions generated via ensemble averages. This approach allows us to precisely estimate the mean and variance of output parameters while retaining the key characteristics of previous models. Our findings demonstrate that this technique yields valuable insights into the functioning of complex biochemical networks without requiring excessive computational time.",
        "ori-fast-z-score": -0.9607689228305227,
        "water-fast-z-score": 7.58182540244241,
        "rewrite-fast-z-score": 1.5670935878004129
    },
    {
        "original_text": "We report an experimental realization of a novel type of 3D Brownian motors based on colloidal particles trapped in periodic potentials created by laser interference patterns. The potential landscape is engineered to have two different types of local minima separated by barriers with varying heights and widths. We show that this design allows for controlling both directional transport as well as its speed over several orders of magnitude. This work opens up new possibilities for designing active materials with tunable properties. A growing number of applications require systems capable of converting energy into directed motion at low Reynolds numbers  1  . In recent years, there has been significant progress towards realizing such devices known as  Brownian motors   2  , which are typically composed of many interacting particles moving through complex environments  3  .\nIn particular, it was shown theoretically  4  and experimentally  5  that one can create a unidirectional current of colloids using optical traps arranged in a 2D square or honeycomb pattern  6  . However, these designs cannot be easily extended to three dimensions (3D) due to technical limitations associated with creating stable trapping sites  7, 8  . Here we demonstrate how to overcome those challenges by engineering the shape of the potential wells and barriers in order to achieve robust 3D transport. Our approach relies on the use of holographic optical tweezers  9  to trap polystyrene microspheres suspended in water inside a glass capillary tube  10  . By changing the phase between the beams forming each individual trap  11  , we were able to generate a variety of potential landscapes  12  ranging from simple double-well structures  13  to more complicated ones containing multiple barriers  14  .",
        "watermark_text": "We report an research realization of a new kind of 3D Brownian motors based on colloidal particles caught in periodic potentials created by laser interference patterns . The potential landscape is designed to have two different kinds of local minima separated by barriers with varying heights and widths .We see that this design allows for controlling both directional travel as well as its velocity over several orders of magnitude . This project opens up new possibilities for designing active materials with tunable properties .A growing number of applications need devices capable of converting energy into guided motion at low Reynolds value 1 . In past decades , there has been significant progress towards realizing such machines called as Brownian cars 2 , which are typically consists of several interacting molecules moving through complex environments 3 .In particular , it was shown theoretically 4 and experimentally 5 that one can create a unidirectional current of colloids using optical trapping ordered in a 2D square or honeycomb shape 6 . However , these designs cannot be easily enlarged to three dimensions ( 3D ) due to technical restrictions associated with creating stable trap places 7 , 8 .Here we prove how to overcome those obstacles by designing the form of the potential reservoirs and barriers in order to achieve robust 3D transport . Our solution uses on the using of holographic optical tweezers 9 to capture polystyrene microspheres hanging in water inside a glass capillary frame 10 .By changing the phase between the beams making each individual trap 11 , we were could to produce a variety of potential landscapes 12 ranging from basic double - well complexes 13 to more complicated ones featuring multiple barriers 14 .",
        "rewrite_text": "We present the realization of a novel type of 3D Brownian motors utilizing colloidal particles trapped in periodic potentials generated by laser interference patterns. The designed potential landscape contains two distinct types of local minima, separated by barriers of varying heights and widths. This configuration enables precise control over both the direction of movement and the speed, allowing for changes across several orders of magnitude. This research opens up new avenues for the development of active materials with adjustable properties. An increasing number of applications require devices that can convert energy into controlled motion at low Reynolds numbers. In recent decades, significant advancements have been made in creating such machines, often referred to as Brownian cars, which typically consist of multiple interacting molecules navigating through complex environments. Theoretically and experimentally, it has been shown that a unidirectional current of colloids can be established using optical trapping arranged in 2D configurations like square or honeycomb patterns. However, these designs face challenges when scaling to three dimensions due to technical limitations in creating stable trapping sites. In our work, we demonstrate how to overcome these challenges by strategically shaping the potential reservoirs and barriers to facilitate robust 3D transport. Our approach employs holographic optical tweezers to capture polystyrene microspheres suspended in water within a glass capillary. By varying the phase between the beams forming each individual trap, we can create a range of potential landscapes, from simple double-well structures to more complex configurations with multiple barriers.",
        "ori-fast-z-score": -0.17025130615174972,
        "water-fast-z-score": 6.810052246069989,
        "rewrite-fast-z-score": 0.9684959969581862
    },
    {
        "original_text": "We present the light-cone distribution amplitudes (DAs) for axial vector mesons in terms of their helicity components, which are determined by solving the Bethe-Salpeter equation with an instantaneous interaction kernel and applying the method developed recently to calculate DAs.  We find that the twist-2 DA is dominated by its first Gegenbauer moment, while higher moments contribute significantly only at large momentum fractions x > 0.7. The twist-3 DA has two independent functions, one of them being proportional to the second Gegenbauer moment. Our results show that the twist-4 contribution is negligible compared to those of lower twists. These findings will be useful for studying exclusive processes involving axial vector mesons such as B-decays into charmonium plus photon or pion pair. \nI. INTRODUCTIO N\nThe study of hadronic structure plays an important role in understanding strong interactions between quarks and gluons inside hadrons. In particular, the investigation on the parton distributions provides us valuable information about how quarks and gluon are distributed within hadrons  1  . Recently, there have been great interests in exploring the internal structures of hadrons beyond the leading-twist level  2  , especially the transverse-momentum dependent parton distributions  3  .\nIn this work we focus our attention on another type of nonperturbative objects -the light-cone distribution amplitudes(DAs). They describe the probability amplitude of finding a quark-antiquark pair with certain longitudinal momentum fraction and transverse separation at some fixed light-like distance  4  . It was shown that they play crucial roles in describing various hard exclusive reactions  5  . For example, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs  6  ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the lowest-and next-to-lowest order DAs  7, 8  . Furthermore, it was found that the heavy-to-light transition form factor FV(q 2 ) of B→V transitions depends",
        "watermark_text": "We introduce the light - cone distribution amplitudes ( DAs ) for axial vector mesons in terms of their helicity components , which are decided by solving the Bethe - Salpeter equation with an instantaneous interaction kernel and using the method developed lately to estimate DAs . We see that the twist - 2 DA is dominated by its initial Gegenbauer moment , while greater moments contribute considerably only at large velocity fractions x > 0 . 7 .The twist - 3 DA has two independent functions , one of them being equal to the second Gegenbauer moment . Our results show that the twist - 4 contribution is negligible compared to those of lower bends .These studies will be valuable for studying exclusive mechanisms using axial vector mesons such as B - decays into charmonium plus photon or pion pair . I . INTRODUCTIO N The investigation of hadronic structure serves an important role in understanding strong interactions between quarks and gluons inside hadrons .In particular , the investigation on the parton distributions offers us valuable info about how quarks and gluon are distributed within hadrons 1 . Recently , there have been big efforts in investigating the internal structures of hadrons beyond the led - twist level 2 , particularly the transverse - momentum dependent parton distributions 3 .In this research we focus our focus on another type of nonperturbative objects - the light - cone distribution amplitudes ( DAs ) . They define the probability amplitude of finding a quark - antiquark pair with certain horizontal momentum fraction and longitudinal separation at some fixed light - like distance 4 .It was shown that they serve vital part in understanding various hard exclusive reactions 5 . For instance , the decay constants fBπ and fBs can be stated in terms of the lowest - order DAs 6 ; the form factors of semileptonic decays B→πlν l and B→Klν l depend on both the highest - and last - to - lowest order DAs 7 , 8 .Furthermore , it was shown that the heavy - to - light shift form parameter FV ( q 2 ) of B→V transitions depends",
        "rewrite_text": "We present the light-cone distribution amplitudes (DAs) for axial vector mesons, expressed in terms of their helicity components. This formulation arises from solving the Bethe-Salpeter equation using an instantaneous interaction kernel, alongside a recently developed method for estimating DAs. Our findings indicate that the twist-2 DA is predominantly influenced by its primary Gegenbauer moment, while higher moments only make significant contributions at large velocity fractions (x > 0.7). In contrast, the twist-3 DA consists of two independent functions, one of which corresponds to the second Gegenbauer moment. Notably, our analysis reveals that the twist-4 contribution is negligible when compared to the lower twists. These insights will prove beneficial for exploring exclusive processes involving axial vector mesons, such as B-decays into charmonium and photon or pion pairs.\n\nI. INTRODUCTION\n\nThe study of hadronic structure is crucial for understanding the strong interactions among quarks and gluons within hadrons. In particular, examining parton distributions provides key insights into the distribution of quarks and gluons within these particles. Recently, considerable efforts have been made to probe the internal structures of hadrons beyond the leading-twist level, with a specific focus on transverse momentum-dependent parton distributions. In this research, we turn our attention to another category of nonperturbative objects: light-cone distribution amplitudes (DAs). These functions define the probability amplitude for finding a quark-antiquark pair with specific transverse momentum fractions and longitudinal separations at fixed light-like distances. It has been demonstrated that DAs play a crucial role in elucidating various hard exclusive processes. For instance, the decay constants fBπ and fBs can be expressed in terms of the lowest-order DAs, while the form factors for semileptonic decays such as B→πlνl and B→Klνl are influenced by both the highest-order and the next-to-lowest-order DAs. Additionally, the heavy-to-light transition form parameter FV(q²) related to B→V transitions also depends on these DAs.",
        "ori-fast-z-score": 1.044465935734187,
        "water-fast-z-score": 8.181649829917799,
        "rewrite-fast-z-score": 1.3620104492139977
    },
    {
        "original_text": "The asteroid 144898 was discovered on September 24, 2004 by the Catalina Sky Survey at an apparent magnitude of 18.7 and is classified as potentially hazardous due to its large size.  The orbit has been determined using astrometric observations made with the US Naval Observatory s 1-meter telescope in Flagstaff Arizona between October 2005 and March 2007.   These data show that this object will not impact Earth during the next 100 years but may be a good candidate for future space mission targets. This work was supported by NASA under grant NNX07AG70G issued through the Planetary Defense Coordination Office. We report here our results of physical investigations carried out on the surface of the asteroid 144898 (2004VD17). Our analysis shows that it is a S-type asteroid with a diameter D = 2.5 ± 0.2 km. Its rotation period P = 3.6 ± 0.1 hours and pole position are also derived.",
        "watermark_text": "The asteroid 144898 was discovered on September 24 , 2004 by the Catalina Sky Survey at an apparent magnitude of 18 . 7 and is categorized as potentially dangerous due to its large size . The orbit has been determined using astrometric observations made with the US Naval Observatory s 1 - meter telescope in Flagstaff Arizona between October 2005 and March 2007 .These data demonstrate that this body will not hit Earth during the last 100 years but might be a better contender for future space flight targets . This effort was supported by NASA under grant NNX07AG70G sent through the Planetary Defense Coordination Office .We report here our findings of physical tests carried out on the surface of the asteroid 144898 ( 2004VD17 ) . Our study shows that it is a S - class asteroid with a diameter D = 2 . 5 ± 0 . 2 km .Its rotation period P = 3 . 6 ± 0 . 1 hours and pole position are also derived .",
        "rewrite_text": "Asteroid 144898 was identified on September 24, 2004, by the Catalina Sky Survey, exhibiting an apparent magnitude of 18.7. Due to its considerable size, it is classified as potentially hazardous. The asteroid's orbit has been established through astrometric observations conducted with the US Naval Observatory's 1-meter telescope in Flagstaff, Arizona, from October 2005 to March 2007. These observations indicate that the asteroid poses no risk of collision with Earth over the next century, positioning it as a potential target for future space exploration. This research was supported by NASA under grant NNX07AG70G via the Planetary Defense Coordination Office. In this report, we present our findings from physical tests performed on the surface of asteroid 144898 (2004 VD17). Our analysis reveals that it is an S-class asteroid with a diameter of D = 2.5 ± 0.2 km, and we have also determined its rotation period to be P = 3.6 ± 0.1 hours along with its pole position.",
        "ori-fast-z-score": -1.0690449676496976,
        "water-fast-z-score": 4.008918628686366,
        "rewrite-fast-z-score": 1.1920791213585393
    },
    {
        "original_text": "The PVLAS collaboration has recently reported results on light-by-light scattering in vacuum, which are inconsistent with Standard Model predictions.  In this note we discuss possible interpretations of these data within the framework of quantum field theory and string theory. We argue that the most natural interpretation is to assume that the observed effect arises due to new particles coupling to photons via an effective dimension-8 operator. The required mass scale for such particles can be as low as 10 GeV or even lower if one assumes that they couple only weakly to ordinary matter. If confirmed by further experiments, these observations would have profound implications both for particle physics phenomenology and cosmological models. The PVLAS collaboration has recently announced their measurement of light-by-light scattering in vacuo  1  . This process violates parity conservation at tree level and thus cannot occur in the Standard Model (SM)  2  , but it could arise through loop effects  3  .\nIn particular, the authors report observing a signal consistent with the SM prediction  4  \nwhere G F = 1.1663787(6) × 10−5GeV−2 is Fermi s constant  5  , θ W ≈ 0.23 is the weak mixing angle  6  , m e is the electron mass, and M Pl ≡ 1/ √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass  7, 8  . However, the measured value of the cross section exceeds the theoretical expectation by more than three standard deviations,\nThis discrepancy between experiment and theory may indicate the presence of new physics beyond the SM  9  .",
        "watermark_text": "The PVLAS collaboration has recently announced findings on light - by - light diffusion in vacuum , which are inconsistent with Standard Model expectations . In this note we explain possible interpretations of these information within the framework of quantum field theory and string theory .We argue that the most natural interpretation is to assume that the seen effect arises owing to new objects coupling to photons via an efficient dimension - 8 operator . The expected mass scale for such particles can be as low as 10 GeV or even smaller if one assumes that they couple only weakly to normal matter .If confirmed by further studies , these observations would have profound implications both for electron physics phenomenology and cosmological predictions . The PVLAS collaboration has recently announced their observation of light - by - light diffusion in vacuo 1 .This process violates parity conservation at forest level and therefore cannot occur in the Standard Model ( SM ) 2 , but it could occur through ring effects 3 . In particular , the writers report studying a signal compatible with the SM prediction 4 where G F = 1 . 1663787 ( 6 ) × 10−5GeV−2 is Fermi s constant 5 , θ W ≈ 0 . 23 is the strong mixing angle 6 , m e is the electron mass , and M Pl ≡ 1 / √ 8πG N ≈ 2×10 18 GeV is the reduced Planck mass 7 , 8 .However , the measured value of the cross area approaches the theoretical expectation by more than three standard deviations , This discrepancy between experiment and theory could indicate the presence of new science beyond the SM 9 .",
        "rewrite_text": "The PVLAS collaboration has recently revealed findings regarding light-by-light scattering in a vacuum that diverge from the predictions of the Standard Model. In this note, we explore various interpretations of these results within the contexts of quantum field theory and string theory. We propose that the most plausible explanation is the presence of new particles interacting with photons through a highly effective dimension-8 operator. The anticipated mass scale for these particles may be as low as 10 GeV or potentially even lower if they have only weak interactions with ordinary matter. Should these observations be validated through further research, they could have significant implications for the study of electron physics and cosmological models. The PVLAS collaboration has reported their detection of light-by-light scattering in vacuo, a process that violates parity conservation at the fundamental level, thus precluding its occurrence in the Standard Model. However, such events could arise through loop effects. Notably, the authors describe a signal that aligns with the Standard Model prediction, where G_F = 1.1663787(6) × 10^-5 GeV^-2 is Fermi's constant, θ_W ≈ 0.23 is the weak mixing angle, m_e represents the electron mass, and M_Pl ≡ 1/√(8πG_N) ≈ 2 × 10^18 GeV is the reduced Planck mass. Nonetheless, the observed value of the scattering area surpasses the theoretical expectation by more than three standard deviations, suggesting a potential frontier of new discoveries beyond the Standard Model.",
        "ori-fast-z-score": -0.9534625892455924,
        "water-fast-z-score": 4.461042580036746,
        "rewrite-fast-z-score": -0.39605901719066977
    },
    {
        "original_text": "We present an explicit expression for the irreducible form of the metric variation of the action term in sixth order gravity, which is valid to all orders in perturbation theory. We also show that this result can be used to derive an approximate expression for the stress energy tensor of the gravitational field. The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non-canonical kinetic term. In particular we find that the non-Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is exactly de Sitter space-time. This implies that the bispectrum produced by such models cannot be described solely in terms of local shape functions as it was previously thought. \nI. INTRODUCTORY REMARK\nIn recent years there has been renewed interest on higher derivative theories of gravity motivated mainly by their possible role in quantum gravity phenomenology (see e.g. ), but also because they provide interesting alternatives to standard General Relativity (GR) in the context of modified gravity scenarios . However, despite these efforts, our understanding of the physical consequences of these theories remains incomplete due to technical difficulties associated with the analysis of their solutions. One of the main obstacles comes from the fact that the equations of motion derived from these actions contain derivatives of arbitrarily high order, making them difficult or impossible to solve analytically. A way out of this problem consists in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms. Although this approach allows one to obtain useful information about the dynamics of the system under consideration, it fails to capture important features like back-reaction effects between different modes of the same field or interactions among different fields. For example, in the case of inflationary cosmologies based on higher derivative gravity, the truncated perturbative expansions do not reproduce correctly the observed level of primordial non-Gaussianities .\nA more systematic method to deal with these problems involves the use of covariant techniques developed originally within the framework of GR. These methods allow us to express the equations of motion in a manifestly gauge",
        "watermark_text": "We present an explicit expression for the irreducible form of the metric variation of the activity term in sixth order gravity , which is valid to all orders in perturbation theory . We additionally prove that this consequence can be used to derive an approximate representation for the strain energy tensor of the gravitational field .The results are applied to study the evolution of cosmological perturbations during inflation driven by a scalar field with non - canonical kinetic term . In particular we find that the non - Gaussianity generated at second order in perturbation theory does not vanish even if the background geometry is precisely de Sitter space - time .This implies that the bispectrum produced by such theories cannot be described solely in terms of local form variables as it was formerly thought . I .INTRODUCTORY REMARK In recent years there has been continued interest on higher derivative theories of gravitational motivated mainly by their possible role in quantum gravitational phenomenology ( saw e . g . ) , but also because they give exciting alternatives to standard General Relativity ( GR ) in the context of modified gravity scenarios .However , despite these attempts , our grasp of the physical effects of these theories appears incomplete due to technical problems related with the processing of their solutions . One of the main problems comes from the fact that the equations of movement obtained from these actions involve derivatives of arbitrarily high order , making them harder or impossible to solve analytically .A way out of this question involves in expanding the fields around some fixed background solution and truncating the resulting series expansion after a finite number of terms . Although this methodology allows one to obtain usable information about the dynamics of the process under consideration , it fails to capture important features like back - reaction effects between various modes of the same field or relationships among different fields .For instance , in the case of inflationary cosmologies based on higher derivative gravity , the truncated perturbative expansions do not reproduce correctly the seen level of primordial non - Gaussianities . A more thorough method to deal with these problems involves the using of covariant techniques established originally within the framework of GR .These methods provide us to express the equations of movement in a manifestly gauge",
        "rewrite_text": "We present a clear expression for the irreducible form of the metric variation associated with the activity term in sixth-order gravity, which remains valid for all orders in perturbation theory. Furthermore, we demonstrate that this result can be utilized to derive an approximate representation of the strain energy tensor for the gravitational field. Our findings are applied to examine the evolution of cosmological perturbations during inflation driven by a scalar field with a non-canonical kinetic term. Notably, we discover that the non-Gaussianity generated at second order in perturbation theory persists, even when the background geometry is precisely described by de Sitter space-time. This suggests that the bispectrum produced by such theories cannot solely be characterized using local form variables, as previously believed.\n\n**I. INTRODUCTORY REMARK**  \nIn recent years, there has been a growing interest in higher derivative theories of gravity, primarily due to their potential relevance in quantum gravitational phenomenology and their exciting alternatives to standard General Relativity (GR) in modified gravity scenarios. However, despite these efforts, our understanding of the physical implications of these theories remains incomplete, largely because of technical challenges associated with solving their equations. A significant issue arises from the equations of motion derived from these actions, which include derivatives of arbitrarily high order, complicating or even preventing analytical solutions. One approach to address this challenge involves expanding the fields around a fixed background solution and truncating the resulting series expansion after a finite number of terms. While this method provides useful insights into the dynamics being studied, it fails to capture critical features such as back-reaction effects between different modes of the same field or interactions among various fields. For example, in inflationary cosmologies that incorporate higher derivative gravity, truncated perturbation expansions do not accurately reproduce the observed levels of primordial non-Gaussianities. A more comprehensive method for tackling these issues involves employing covariant techniques originally developed in the context of GR, which enable us to express the equations of motion in a manifestly gauge-invariant manner.",
        "ori-fast-z-score": 0.5261522196019802,
        "water-fast-z-score": 7.29096647162744,
        "rewrite-fast-z-score": -0.914991421995628
    },
    {
        "original_text": "We present 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves for the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1. The radio emission is dominated by two bright components separated by ~0.5 arcsec at all epochs observed between 2005 January and 2007 December. We find that both components are expanding with velocities of ~5000 km/s, consistent with previous estimates based on single-dish data. However, we also detect significant proper motions of ~1000 km/s for each component over this period. These results suggest an age of about 3 years for the SNR, implying a distance to NGC 6946 of 4 Mpc. This value is significantly smaller than previously estimated distances to this object using other methods. Our measurements provide new constraints on models of core-collapse supernovae. \n \n Keywords: Supernova remnants",
        "watermark_text": "We create 8 . 4 GHz Very Long Baseline Interferometry ( VLBI ) images and light angles for the supernova remnant ( SNR ) associated with the Type IIb supernova SN2004et , which exploded in the nearby spiral galaxy NGC 6946 on 2004 September 24 UT1 . The radio emission is dominated by two bright components differentiated by ~ 0 . 5 arcsec at all epochs observed between 2005 January and 2007 December .We see that both components are growing with velocities of ~ 5000 kilometres / s , consistent with previous estimates based on single - dish data . However , we also observe significant normal motions of ~ 1000 kilometers / s for each component over this time .These data suggest an age of about 3 years for the SNR , suggests a proximity to NGC 6946 of 4 Mpc . This value is significantly less than previously estimated distances to this body using other methods .Our measurements give novel constraints on estimates of core - collapse supernovae . Keywords : Supernova remnants",
        "rewrite_text": "We have produced 8.4 GHz Very Long Baseline Interferometry (VLBI) images and light curves of the supernova remnant (SNR) associated with the Type IIb supernova SN2004et, which occurred in the nearby spiral galaxy NGC 6946 on September 24, 2004 UT. The radio emissions are primarily comprised of two prominent components, separated by approximately 0.5 arcseconds, consistently observed from January 2005 to December 2007. Both components exhibit growth at velocities of around 5000 kilometers per second, aligning with prior estimates derived from single-dish observations. Additionally, we detected significant motion of approximately 1000 kilometers per second for each component during this period. These findings imply an approximate age of 3 years for the SNR and indicate that it is located about 4 megaparsecs away from NGC 6946. This distance is considerably less than previously assessed distances for this galaxy using other techniques. Our measurements provide new insights into the characteristics of core-collapse supernovae. Keywords: Supernova remnants.",
        "ori-fast-z-score": -0.6509445549041194,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": -0.12403473458920847
    },
    {
        "original_text": "We study the gravitational field generated by a massive scalar point source moving on an equatorial circular geodesic around a Schwarzschild black hole, and we evolve it numerically in two spatial dimensions (2+1) with the puncture method. We find that the perturbation is dominated by a single mode which grows exponentially as time goes on. The growth rate agrees well with the prediction based on quasinormal modes for this system. This result suggests that the exponential growth may be related to the instability of the scalar field near the horizon. In addition, we also show that the amplitude of the growing mode decreases rapidly when the mass of the scalar field increases. Finally, we discuss possible applications of our results to astrophysical phenomena such as gamma-ray bursts. \n \n Introduction \n \n Black holes are among the most fascinating objects predicted by general relativity. They have been studied extensively both theoretically and observationally over many decades  1  . One important aspect of their physics concerns how particles move close to them  2  , especially those that can escape from the black hole s gravity  3  . It has recently become clear that there exist some interesting physical processes taking place very close to the event horizon  4  -  6  . For example, if one considers a charged particle falling into a Reissner-Nordström black hole, then its motion will be unstable due to the so-called  photon sphere effect   7, 8  . If the charge of the particle is sufficiently large, then the particle will eventually fall into the black hole after emitting photons  9  . Another interesting phenomenon occurs when a neutral particle falls into a Kerr black hole  10  . Here again, the motion becomes unstable because of the existence of the photon sphere  11  . However, unlike the case of a Reissner-Norström black hole, the emitted radiation now contains not only photons but also gravitons  12  . \n \n In recent years, much attention has been paid to studying the dynamics of fields outside black holes  13  -  17  . In particular, the problem of finding the spectrum of quasi-normal modes (QNMs), i.e., the characteristic frequencies at",
        "watermark_text": "We test the gravitational field produced by a huge scalar point source rotating on an equatorial circular geodesic around a Schwarzschild black hole , and we evolve it numerically in two spatial dimensions ( 2 + 1 ) with the puncture method . We see that the perturbation is dominated by a single mode which increases exponentially as time went on .The growth speed agrees well with the observation based on quasinormal modes for this scheme . This result suggests that the exponential growth could be connected to the instability of the scalar field near the horizon .In addition , we also demonstrate that the frequency of the increasing mode decreases quickly when the mass of the scalar field increases . Finally , we explain possible applied of our findings to astrophysical processes such as gamma - ray bursts .Introduction Black holes are among the most beautiful objects anticipated by general relativity . They have been studied frequently both theoretically and observationally over numerous centuries 1 .One important element of their physics matters how particles moving nearer to them 2 , particularly those that can escape from the dark hole s gravity 3 . It has recently become clear that there exist some interesting physical processes take place very close to the event horizon 4 - 6 .For instance , if one considers a charged particle falling into a Reissner - Nordström black hole , then its motion will be unstable due to the so - called photon sphere phenomenon 7 , 8 . If the charge of the particle is sufficiently huge , then the particle will eventually go into the dark hole after emitting photons 9 .Another important process occurs when a neutral element goes into a Kerr black hole 10 . Here again , the movement becomes unstable because of the existence of the photon sphere 11 .However , unlike the case of a Reissner - Norström black hole , the emitted radiation now contains not only photons but also gravitons 12 . In recent years , much attention has been paid to researching the dynamics of fields outside brown holes 13 - 17 .In particular , the question of finding the spectrum of quasi - normal frequencies ( QNMs ) , i . e . , the typical frequencies at",
        "rewrite_text": "We investigate the gravitational field produced by a massive scalar point source that is rotating in an equatorial circular geodesic around a Schwarzschild black hole. Using the puncture method, we numerically evolve this system in two spatial dimensions (2 + 1). Our findings indicate that the perturbation is predominantly characterized by a single mode that increases exponentially over time. The rate of this growth aligns well with observations derived from quasinormal modes associated with this configuration. This phenomenon suggests a potential link between the exponential growth and the instability of the scalar field near the event horizon. Additionally, we demonstrate that the frequency of the growing mode decreases rapidly as the mass of the scalar field increases. Finally, we discuss the possible applications of our findings to astrophysical events, including gamma-ray bursts.\n\n**Introduction**: Black holes are some of the most fascinating objects predicted by general relativity, and they have been the focus of extensive theoretical and observational studies for many centuries. A key aspect of black hole physics is understanding how particles behave when they approach them, particularly those that manage to escape the gravitational pull of these dark entities. Recent research has illuminated several intriguing physical processes occurring very close to the event horizon. For example, the motion of a charged particle falling into a Reissner-Nordström black hole becomes unstable due to the presence of the photon sphere. If the particle's charge is sufficiently large, it will ultimately be pulled into the black hole after emitting photons. Another significant process occurs when a neutral particle approaches a Kerr black hole, where again the trajectory is rendered unstable due to the photon sphere. However, in contrast to the Reissner-Nordström scenario, the radiation emitted in this case includes both photons and gravitons. In recent years, considerable attention has been directed towards studying the dynamics of fields surrounding black holes, particularly in terms of determining the spectrum of quasinormal frequencies (QNF), which represent the characteristic frequencies associated with these systems.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 7.526023228839096,
        "rewrite-fast-z-score": 1.0245435281108308
    },
    {
        "original_text": "We have analyzed Chandra observations for eight galaxy clusters with redshifts between 0.1 and 0.3 to determine their radial temperature, density, pressure, entropy, cooling time, and metallicity profiles. We find that all these quantities are well described by single-parameter scaling relations as functions of radius r normalized by the virial radius Rvir.  The best-fit values of the normalization parameters depend on redshift but not significantly so; we therefore adopt fixed values based on our results for the two most distant clusters (z = 0.2 and 0.3) which yield good fits to the other six clusters. Our main conclusions are:  1. All cluster properties show significant evolution out to z ~ 0.3; this is consistent with previous studies using XMM data. 2. The gas fraction fgas(r/Rvir), defined as the ratio of the total thermal energy within a sphere of radius r to its gravitational binding energy, decreases monotonically outwards; it also shows some evidence for evolution with redshift. 3. The electron number density ne(r) increases inwardly toward the center of each cluster until reaching a peak value near r ~ 0.1r200 where r200 denotes the radius enclosing an average overdensity of 200 times the critical density of the universe. Beyond this point, ne(r) declines slowly or remains roughly constant depending on the cluster. 4. The mean molecular weight µe(r) increases outwardly due to the increasing contribution of helium ions relative to hydrogen atoms. 5. The central temperatures T0 inferred from spectral fitting range from 6 keV to 12 keV, while those obtained directly from the deprojected temperature profile lie in the range 7-15 keV. These differences may be caused by non-thermal components such as AGN jets and/or magnetic fields.",
        "watermark_text": "We have analyzed Chandra measurements for eight galaxy galaxies with redshifts between 0 . 1 and 0 . 3 to estimate their radial temperature , density , pressure , entropy , cooling period , and metallicity profiles . We see that all these quantities are better represented by single - parameter scaling relations as functions of radius r normalized by the virial diameter Rvir .The best - fitting values of the normalization coefficients differ on redshift but not considerably so ; we thus choose fixed values based on our findings for the two most distant populations ( z = 0 . 2 and 0 . 3 ) which provide better fits to the other six regions . Our main results are : 1 .All cluster elements exhibit substantial development out to z ~ 0 . 3 ; this is consistent with previous research utilizing XMM data . 2 .The gas fraction fgas ( r / Rvir ) , defined as the proportion of the total heat power within a sphere of radius r to its gravitational binding energy , decreases monotonically outwards ; it also shows some evidence for evolution with redshift . 3 .The electron number density ne ( r ) rises inwardly toward the center of each cluster until reaching a peak value near r ~ 0 . 1r200 where r200 denotes the radius enclosing an mean overdensity of 200 times the critical density of the universe . Beyond this point , ne ( r ) declines slowly or remains fairly constant depending on the cluster .4 . The mean molecular weight µe ( r ) rises outwardly due to the increasing contribution of helium ions relative to hydrogen atoms .5 . The central temperatures T0 inferred from spectral fit range from 6 keV to 12 keV , while those generated directly from the deprojected temperature profile lie in the range 7 - 15 keV .These changes may be caused by non - electrical components such as AGN jets and / or magnetic waves .",
        "rewrite_text": "We analyzed Chandra measurements from eight galaxies with redshifts ranging from 0.1 to 0.3 to estimate their radial profiles for temperature, density, pressure, entropy, cooling time, and metallicity. Our findings suggest that these quantities are more accurately described by single-parameter scaling relations as functions of radius \\( r \\) normalized by the virial diameter \\( R_{vir} \\). Although the best-fitting normalization coefficients vary with redshift, the differences are not significant. Therefore, we selected fixed values based on our analysis of the two most distant populations (z = 0.2 and 0.3), which provide improved fits for the other six regions. Our main results include: (1) All cluster elements show significant development up to z ~ 0.3, consistent with earlier studies using XMM data. (2) The gas fraction \\( f_{gas}(r / R_{vir}) \\), defined as the ratio of total thermal energy within a sphere of radius \\( r \\) to its gravitational binding energy, decreases monotonically outward, with some indication of evolution with redshift. (3) The electron number density \\( n_e(r) \\) increases toward the cluster center, peaking near \\( r \\sim 0.1r_{200} \\), where \\( r_{200} \\) is the radius corresponding to a mean overdensity of 200 times the critical density of the universe. Beyond this point, \\( n_e(r) \\) either declines slowly or remains relatively constant, depending on the cluster. (4) The mean molecular weight \\( \\mu_e(r) \\) increases outward due to a larger relative contribution from helium ions compared to hydrogen atoms. (5) The central temperatures \\( T_0 \\) obtained from spectral fittings range from 6 keV to 12 keV, while those derived directly from the deprojected temperature profile fall between 7 and 15 keV. These variations may be influenced by non-thermal components such as AGN jets and/or magnetic waves.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 6.11070705734948,
        "rewrite-fast-z-score": 2.6106709553062086
    },
    {
        "original_text": "We study the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions by means of Monte Carlo (MC) simulations, using an off-lattice model with freely jointed chains. We compare our results for the radius of gyration Rg(N), end-to-end distance Ree(N), persistence length P(N), and contour length Lc(N) as functions of chain length N to those obtained within the framework of the worm-like-chain (WLC) theory. The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over several orders of magnitude in chain lengths. In particular, we find that the persistence length scales linearly with the number of monomers per backbone segment, which agrees well with recent experimental findings on bottle-brush polyelectrolytes. \n \n Keywords: Polymer brushes, Persistence length",
        "watermark_text": "We research the conformational characteristics of one - and two - component bottlebrush polymers in good solvent circumstances by means of Monte Carlo ( MC ) simulations , using an off - lattice model with freely jointed strands . We relate our findings for the radius of gyration Rg ( N ) , end - to - end distance Ree ( N ) , persistence length P ( N ) , and contour distance Lc ( N ) as functions of chain length N to those achieved within the framework of the worm - like - chain ( WLC ) theory .The WLC approach is found to be very accurate at predicting the scaling behavior of these quantities over numerous orders of magnitude in chain lengths . In particular , we find that the persistence length varies linearly with the quantity of monomers per backbone segment , which agrees well with recent experimental discoveries on bottle - brush polyelectrolytes .Keywords: Polymer brushes, Persistence length",
        "rewrite_text": "We investigate the conformational properties of one- and two-component bottlebrush polymers in good solvent conditions using Monte Carlo (MC) simulations, employing an off-lattice model with freely jointed segments. Our results for the radius of gyration \\( R_g(N) \\), end-to-end distance \\( R_{ee}(N) \\), persistence length \\( P(N) \\), and contour length \\( L_c(N) \\) as functions of chain length \\( N \\) are compared to predictions made by the worm-like chain (WLC) theory. The WLC model proves to be highly effective in capturing the scaling behavior of these properties across a wide range of chain lengths. Notably, we observe that the persistence length increases linearly with the number of monomers per backbone segment, which is consistent with recent experimental findings on bottlebrush polyelectrolytes. \nKeywords: Polymer brushes, Persistence length.",
        "ori-fast-z-score": 0.6868028197434451,
        "water-fast-z-score": 4.2581774824093594,
        "rewrite-fast-z-score": -0.9615239476408232
    },
    {
        "original_text": "The Variable Star One-Shot project is an open-source software package for the analysis of astronomical data.  It was developed by members of the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers at other institutions around the world.   The goal of this project is to provide a single tool that can be used to analyze all types of astronomical data sets, including photometric time series, spectroscopic observations, images, etc., using state-of-the-art techniques such as image subtraction, cross correlation, period finding algorithms, spectral line fitting, etc.    This software has been released under the GNU General Public License v3.0 and is available on GitHub at: https://github.com/VariableStar/one-shot-astro .\nOne Shot Astro includes several tools which are useful for analyzing large amounts of astronomical data quickly and efficiently.   These include one-shot-datacleaner , which performs automated quality control checks on raw data files;   one-shot-mosaic , which creates mosaicked images from multiple dithered exposures;  one-shot-astrometry , which determines astrometric solutions for individual frames or entire mosaic images;    one-shot-photometry , which measures fluxes and/or magnitudes for stars across a field-of-view;    one-shot-pipeline , which automates the process of running these various tasks together into a pipeline;    one-shot-wikimapia , which allows users to create custom sky maps based on their own catalogs of objects.     In addition to these core packages, there are also many additional modules available through the one-shot-astro repository which allow users to perform more specialized analyses, such as:    one-shot-catalog , which provides access to a variety of different astrophysical databases via SQL queries;    one-shot-fastphot , which uses machine learning methods to measure stellar",
        "watermark_text": "The Variable Star One - Shot initiative is an free - source software tool for the analysis of astronomical data . It was developed by participants of the Harvard - Smithsonian Center for Astrophysics ( CfA ) in partnership with researchers at other institutions around the world .The goal of this project is to provide a single method that can be used to analyze all types of astronomical data sets , including photometric period series , spectroscopic observations , photographs , etc . , using state - of - the - art methods such as image subtraction , inter correlation , period finding methods , spectral line fitting , etc . This program has been released under the GNU General Public License v3 . 0 and is accessible on GitHub at : https : / / github . com / VariableStar / one - shot - astro .One Shot Astro includes several methods which are helpful for studying huge amounts of astronomical data easily and smoothly . These include one - shot - datacleaner , which performs automated quality control checks on raw data data ; one - shot - mosaic , which forms mosaicked images from multiple dithered exposures ; one - shot - astrometry , which determines astrometric solutions for individual frames or entire mosaic images ; one - shot - photometry , which estimates fluxes and / or magnitudes for planets across a field - of - view ; one - shot - pipeline , which automates the process of running these numerous tasks combined into a pipeline ; one - shot - wikimapia , which allows users to create custom skies mapping based on their own catalogs of items .In addition to these core programs , there are also many extra modules available through the one - shot - astro repository which allow users to conduct more sophisticated analyses , such as : one - shot - collection , which offers connections to a variety of different astrophysical databases via SQL queries ; one - shot - fastphot , which uses computer learning techniques to measure stellar",
        "rewrite_text": "The Variable Star One-Shot initiative is an open-source software tool designed for the analysis of astronomical data. Developed by members of the Harvard-Smithsonian Center for Astrophysics (CfA) in collaboration with researchers from various global institutions, this project aims to provide a unified approach to analyze diverse astronomical data sets, such as photometric period series, spectroscopic observations, and photographs. It employs cutting-edge techniques including image subtraction, cross-correlation, period finding algorithms, and spectral line fitting. The software is released under the GNU General Public License v3.0 and can be accessed on GitHub at: https://github.com/VariableStar/one-shot-astro. \n\nOne-Shot Astro features several methods that facilitate the smooth analysis of large volumes of astronomical data. These include One-Shot Datacleaner for automated quality control of raw data; One-Shot Mosaic for creating mosaicked images from multiple dithered exposures; One-Shot Astrometry for determining astrometric solutions for individual frames or entire mosaicked images; and One-Shot Photometry for estimating fluxes and magnitudes of celestial bodies across a field of view. Additionally, One-Shot Pipeline automates the execution of these processes combined into a cohesive workflow, while One-Shot Wikimapia allows users to create custom sky maps based on their own item catalogs. \n\nBeyond these core functions, the One-Shot Astro repository also offers numerous additional modules for more advanced analyses, such as One-Shot Collection, which provides connections to various astrophysical databases through SQL queries, and One-Shot FastPhot, which utilizes machine learning techniques for measuring stellar brightness.",
        "ori-fast-z-score": 1.2094157958139042,
        "water-fast-z-score": 8.465910570697329,
        "rewrite-fast-z-score": 0.7302967433402214
    },
    {
        "original_text": "We present an algorithm to compute the Jones polynomial using three quantum wires and one ancilla qubit, which is based on the idea that the Jones polynomial can be computed by evaluating the Tutte polynomial at 1. We show how this approach leads naturally to a circuit with O(n) gates in depth O(log n), where n denotes the number of crossings in the knot diagram. The main ingredient of our construction are two new families of unitary operators acting on tensor products of Hilbert spaces associated with graphs. These operators generalize the swap operator and have applications beyond computing the Jones polynomial. In particular we use them to construct efficient quantum circuits for other problems such as counting perfect matchings or finding maximum flows. \nIntroduction\n\nThe Jones polynomial J K (t) ∈ Z t ±1/2   of a knot K is a Laurent polynomial invariant of knots and links defined by V. Jones  Jon87  . It has been shown that it distinguishes all prime knots up to 10 crossings  Kaw96  , but its complexity makes it hard to evaluate even for small values of t. For example, if K is the trefoil knot then J K (1) = −1/4 while J K (−1/2) = 1/2. This raises the question whether there exists a quantum algorithm to efficiently calculate the Jones polynomial.",
        "watermark_text": "We present an algorithm to compute the Jones function using three quantum wires and one ancilla qubit , which is based on the idea that the Jones function can be computed by evaluating the Tutte polynomial at 1 . We see how this approach leads naturally to a circuit with O ( n ) gates in depth O ( log k ) , where n represents the number of crossings in the knot diagram .The main ingredient of our build are two new families of unitary operators acting on tensor products of Hilbert spaces identified with graphs . These operators generalize the swap operator and have applications beyond computing the Jones function .In particular we using them to build efficient quantum devices for other difficulties such as counting perfect matchings or finding maximum flows . Introduction The Jones function J K ( t ) ∈ Z t ±1 / 2 of a knot K is a Laurent polynomial invariant of knots and links given by V . Jones Jon87 .It has been shown that it distinguishes all prime twists up to 10 crossings Kaw96 , but its complexity makes it difficult to estimate even for little values of t . For instance , if K is the trefoil knot then J K ( 1 ) = −1 / 4 while J K ( −1 / 2 ) = 1 / 2 . This opens the question whether there exists a quantum algorithm to easily calculate the Jones function .",
        "rewrite_text": "We introduce an algorithm designed to compute the Jones function using three quantum wires and one ancilla qubit. This method is grounded in the concept that the Jones function can be obtained by evaluating the Tutte polynomial at the value of 1. Our approach naturally results in a quantum circuit with O(n) gates and a depth of O(log k), where n denotes the number of crossings in the knot diagram. Central to our construction are two innovative families of unitary operators that operate on tensor products of Hilbert spaces associated with graphs. These operators extend the functionality of the traditional swap operator and have broader implications beyond the computation of the Jones function. Specifically, we utilize them to develop efficient quantum devices for tackling other problems, such as counting perfect matchings and determining maximum flows. \n\nThe Jones function, denoted as J_K(t) ∈ Z[t ±1/2], is a Laurent polynomial invariant representing knots and links, as introduced by V. Jones in 1987. It has been demonstrated that this function can differentiate all prime twists with up to 10 crossings (Kaw96), but its inherent complexity complicates calculations even for small values of t. For example, for the trefoil knot, we find J_K(1) = -1/4 and J_K(-1/2) = 1/2. This raises the question of whether a quantum algorithm exists that can effectively and efficiently compute the Jones function.",
        "ori-fast-z-score": -0.5360562674188973,
        "water-fast-z-score": 4.1812388858673994,
        "rewrite-fast-z-score": 0.6123724356957946
    },
    {
        "original_text": "We present an algorithm for checking consistency in quantified constraints, which is based on the concept of generalized quantifiers. We show that our approach can be used to check several constraint properties such as satisfiability or equivalence between two sets of quantified constraints. Finally we discuss how this method could be applied to solve problems related to software testing. In computer science, many problems are formulated using constraints. For example, in Software Testing (ST), test cases are often represented by means of logical formulas called Test Cases Specifications (TCS). These TCSs contain some variables whose values have to satisfy certain conditions expressed with Boolean expressions. The problem consists then in finding all possible assignments of these variables satisfying the given conditions. This kind of problems has been studied extensively during last years but most works focus only on unquantified constraints. However, there exist situations where it may be useful to express some restrictions over the set of solutions using quantifiers.",
        "watermark_text": "We present an algorithm for checking consistency in quantified constraints , which is based on the idea of generalized quantifiers . We see that our approach can be used to test several constraint features such as satisfiability or equivalence between two sets of quantified constraints .Finally we talk how this technology could be applied to solve difficulties related to programming testing . In computer science , many difficulties are formulated using restrictions .For instance , in Software Testing ( ST ) , test situations are often represented by means of logical formulas called Test Cases Specifications ( TCS ) . These TCSs comprise some parameters whose values have to meet particular conditions stated with Boolean expressions .The question involves then in seeking all possible assignments of these parameters satisfying the particular conditions . This kind of problems has been studied thoroughly during last decades but most works concentrate only on unquantified constraints .However , there exist situations where it could be beneficial to define some restrictions over the group of solutions use quantifiers .",
        "rewrite_text": "We introduce an algorithm for checking consistency in quantified constraints, grounded in the concept of generalized quantifiers. Our method can be utilized to evaluate various constraint properties, such as satisfiability and equivalence between different sets of quantified constraints. Additionally, we discuss potential applications of this technology in addressing challenges in software testing. In computer science, numerous problems are framed using restrictions. For example, in Software Testing (ST), test scenarios are frequently represented by logical formulas known as Test Cases Specifications (TCS). These TCSs include parameters with values that must satisfy specific conditions expressed through Boolean expressions. Consequently, the challenge lies in identifying all possible parameter assignments that fulfill these conditions. Although this type of problem has been extensively researched over the past few decades, most studies have focused solely on unquantified constraints. Nonetheless, there are cases where applying quantifiers to define restrictions on the solution set could be advantageous.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "The pressure-induced insulator-metal (IMT) phase transition is studied by means of the first-principles calculations based on density functional theory within local spin-density approximation and generalized gradient approximations. The calculated results show that the IMT occurs at about 20 GPa for both LSDA and PBE functionals, which are consistent with previous theoretical studies. However, it should be noted that there exists an obvious difference between these two methods when calculating the electronic structure near Fermi level. In addition to this, we also find that the band gap decreases rapidly as increasing pressure up to 30 GPa but then becomes almost constant above 40 GPa. Finally, our calculation shows that the volume collapse takes place around 50 GPa. \n \n Keywords: Pressure-induced insulator-metal transition; First-principles calculations; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap",
        "watermark_text": "The pressure - mediated insulator - metal ( IMT ) phase shift is studied by means of the first - principles experiments using on density functional theory within local spin - density approximation and generalized gradient approximations . The measured conclusions show that the IMT occurs at about 20 GPa for both LSDA and PBE functionals , which are compatible with previous conceptual research .However , it should be mentioned that there exists an evident gap between these two means when calculating the electronic stability near Fermi level . In addition to this , we also find that the band gap falls gradually as increasing temperature up to 30 GPa but then remains virtually constant above 40 GPa .Finally , our estimate reveals that the volume collapse took place around 50 GPa . Keywords : Pressure - caused insulator - iron transition ; First - principles measurements ; Local spin - density simulation ; Generalized gradient approximations ; Electronic structure ; Band gap",
        "rewrite_text": "The pressure-induced insulator-metal transition (IMT) has been investigated through first-principles experiments based on density functional theory, utilizing both local spin-density approximation (LSDA) and generalized gradient approximations (PBE). Our findings indicate that the IMT occurs at approximately 20 GPa for both the LSDA and PBE functionals, aligning with previous conceptual studies. However, it is important to note a significant discrepancy between these methods when assessing electronic stability near the Fermi level. Furthermore, we observe that the band gap decreases gradually with increasing temperature up to 30 GPa, after which it remains almost constant beyond 40 GPa. Lastly, our analysis suggests that a volume collapse occurs around 50 GPa. \n\nKeywords: Pressure-induced insulator-metal transition; First-principles measurements; Local spin-density approximation; Generalized gradient approximations; Electronic structure; Band gap.",
        "ori-fast-z-score": -0.24618298195866545,
        "water-fast-z-score": 5.820855000871991,
        "rewrite-fast-z-score": 1.270001270001905
    },
    {
        "original_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project to map the distribution and motion of galaxies, quasars, stars, and other celestial objects in space. The fourth data release was made public on September 30th 2003. This fifth data release contains more than 100,000 new quasar candidates selected by color criteria from the SDSS imaging survey. These are supplemented with about 20,000 previously known quasars that were not included in previous releases because they did not meet the selection criteria for inclusion at that time. \n \n In addition to these newly discovered quasars, this catalog also includes all quasars found during the first four years of the survey as well as those found since then but which have yet to be released publicly. A total of over 250,000 quasars are now available through this catalog. All of them have been spectroscopically confirmed using observations obtained with the dedicated 2.5-meter telescope located at Apache Point Observatory near Sacramento, California.",
        "watermark_text": "The Sloan Digital Sky Survey ( SDSS ) is an continuing work to map the distribution and motion of galaxies , quasars , stars , and other celestial entities in space . The fourth information publication was making public on September 30th 2003 .This fifth information update contains more than 100 , 000 new quasar finalists chosen by color categories from the SDSS imaging survey . These are supplemented with about 20 , 000 former recognized quasars that were not included in earlier versions because they did not meet the selection standards for inclusion at that point .In addition to these newly discovered quasars , this list also contains all quasars discovered during the first four seasons of the census as well as those identified since then but which have never to be available openly . A total of over 250 , 000 quasars are now released through this catalog .All of them have been spectroscopically confirmed using observations collected with the reserved 2 . 5 - meter telescope located at Apache Point Observatory near Sacramento , California .",
        "rewrite_text": "The Sloan Digital Sky Survey (SDSS) is an ongoing project dedicated to mapping the distribution and motion of galaxies, quasars, stars, and other celestial objects in the universe. The fourth data release occurred on September 30, 2003. The fifth data update features over 100,000 new quasar candidates selected based on color classifications from the SDSS imaging survey. This update also includes approximately 20,000 previously identified quasars that were not part of earlier releases due to not meeting the selection criteria at that time. In addition to the newly identified quasars, this catalog contains all quasars discovered during the first four years of the survey, as well as those identified thereafter that had not been publicly released before. In total, over 250,000 quasars are now available through this catalog, all of which have been spectroscopically confirmed using data gathered from the 2.5-meter telescope at Apache Point Observatory in New Mexico, near Sacramento, California.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 5.25,
        "rewrite-fast-z-score": -0.629940788348712
    },
    {
        "original_text": "The statistical mechanics of networks is an emerging field that has attracted much attention in recent years, especially after the discovery of scale-free and small-world properties in many real world systems such as social networks, biological networks, technological networks etc.. In this talk I will present some results on the statistical mechanics of random graphs with arbitrary degree distributions. The main focus will be on the phase transitions associated with the appearance/disappearance of giant connected components (GCC) in these graphs. We show how to calculate exactly the probability distribution function for GCC size using generating functions techniques. This allows us to study analytically the effect of various parameters like average connectivity, clustering coefficient, node degrees etc., on the formation of GCCs. Finally we discuss possible applications of our results to problems related to epidemic spreading on networks. The statistical mechanics of networks is one of the most active areas of research today. It was pioneered by Barabasi-Albert who discovered that many real world networks have power law degree distributions  1  . Since then there has been considerable interest in understanding the statistical mechanical behavior of different classes of networks  2  .\nIn particular it has been shown recently  3  , that the presence or absence of a giant component (GC), which contains a finite fraction of all nodes in the graph, can lead to interesting phase transitions. For example if the GC disappears at any finite temperature T = 1/β, then the system undergoes a first order transition where the free energy density f ≡ − ln Z/N diverges continuously when β → ∞  4  . Here N denotes the number of nodes in the network while Z is the partition sum defined as:",
        "watermark_text": "The mathematical mechanics of networks is an developing field that has garnered considerable scrutiny in recent years , particularly after the discovery of scale - free and tiny - world properties in many actual world systems such as communication groups , biological organizations , technological networks etc . . In this talk I will present some papers on the statistical mechanics of random graphs with arbitrary degree distributions .The main attention will be on the phase transitions associated with the appearance / extinction of giant connected parts ( GCC ) in these graphs . We see how to estimate exactly the probability distribution function for GCC size using generating functions techniques .This enables us to study analytically the impact of several variables like average connectivity , clustering density , node degrees etc . , on the formation of GCCs . Finally we explain possible applied of our findings to problems related to epidemic spreading on networks .The statistical mechanics of networks is one of the most important areas of research today . It was pioneered by Barabasi - Albert who found that several real global networks have power law degree distributions 1 .Since then there has been substantial interest in understanding the statistical mechanical behavior of different kinds of networks 2 . In particular it has been shown recently 3 , that the presence or lack of a giant element ( GC ) , which contains a finite fraction of all nodes in the graph , can lead to unusual phase transitions .For instance if the GC disappears at any finite temperature T = 1 / beta , then the system undergoes a first order process where the free energy density f ≡ − ln Z / N diverges steadily when β → ∞ 4 . Here N refers the number of vertices in the network while Z is the partition sum defined as :",
        "rewrite_text": "The mathematical mechanics of networks is an emerging field that has attracted significant attention in recent years, especially following the discovery of scale-free and small-world properties in various real-world systems, including communication networks, biological systems, and technological infrastructures. In this talk, I will discuss several papers on the statistical mechanics of random graphs with arbitrary degree distributions. The focus will be on the phase transitions related to the emergence and disappearance of giant connected components (GCCs) within these graphs. We will explore how to accurately estimate the probability distribution function for the size of GCCs using generating function techniques, which allows us to analytically investigate the effects of various factors such as average connectivity, clustering density, and node degree on GCC formation. Lastly, we will discuss potential applications of our findings to issues concerning epidemic spread in networks. The study of the statistical mechanics of networks is one of the most vital research areas today. It was initiated by Barabási-Albert, who discovered that many large-scale networks exhibit power-law degree distributions. Since their breakthrough, there has been a growing interest in understanding the statistical mechanical properties of different network types. Recent research has revealed that the presence or absence of a giant component (GC)—which encompasses a significant fraction of the nodes in the graph—can cause notable phase transitions. For example, if the GC vanishes at a finite temperature \\( T = \\frac{1}{\\beta} \\), the system experiences a first-order transition, where the free energy density \\( f \\equiv -\\frac{\\ln Z}{N} \\) diverges as \\( \\beta \\) approaches infinity. Here, \\( N \\) indicates the number of vertices in the network, while \\( Z \\) is the partition sum defined as follows:",
        "ori-fast-z-score": -0.2491364395612199,
        "water-fast-z-score": 5.564047150200578,
        "rewrite-fast-z-score": -0.9011551125709446
    },
    {
        "original_text": "The low energy effective theories for superstrings are supergravity and supersymmetric gauge theories in four dimensions, which can be obtained by compactifying the extra six spatial dimensions on a Calabi-Yau manifold.  In this talk I will discuss some recent results about lattice models that provide an alternative approach to studying these theories. The basic idea is to use Monte Carlo simulations to study supersymmetric field theories defined on a finite number of points (the sites) of a regular d-dimensional hypercubic lattice with periodic boundary conditions. These models have been studied extensively over the past few years using numerical techniques such as exact diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently we developed new Monte Carlo simulation techniques based on the worm algorithm that allow us to simulate large systems at very high temperatures where conventional Monte Carlo methods fail because they suffer from critical slowing down. We used our new method to calculate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang-Mills theory and the N = 1 supersymmetric U(1) gauge theory coupled to matter fields in various representations.",
        "watermark_text": "The lowest energy effective models for superstrings are supergravity and supersymmetric gauge fields in four dimensions , which can be obtained by compactifying the extra six spatial dimensions on a Calabi - Yau manifold . In this talk I will explore some latest findings about lattice models that provide an different approach to researching these theories .The basic idea is to use Monte Carlo simulations to study supersymmetric field theories specified on a finite number of points ( the sites ) of a regular d - dimensional hypercubic structure with periodic border conditions . These systems have been studied frequently over the previous few years employing mathematical techniques such as approximate diagonalization , quantum Monte Carlo methods , and density matrix renormalization group algorithms .Recently we developed novel Monte Carlo simulation method based on the worm algorithm that enable us to simulate large systems at very high altitudes where conventional Monte Carlo methods fail because they suffer from critical speed down . We utilized our new method to estimate the free energies of several different supersymmetric lattice models including the N = 4 supersymmetric Yang - Mills theory and the N = 1 supersymmetric U ( 1 ) gauge theory connected to matter groups in different representations .",
        "rewrite_text": "The simplest effective models for superstrings are supergravity and supersymmetric gauge fields in four dimensions, which arise from compactifying the additional six spatial dimensions on a Calabi-Yau manifold. In this talk, I will discuss recent discoveries related to lattice models that offer an alternative approach to studying these theories. The fundamental concept involves employing Monte Carlo simulations to examine supersymmetric field theories defined on a finite set of points (the sites) within a regular d-dimensional hypercubic framework under periodic boundary conditions. Over the past few years, these systems have been extensively analyzed using various mathematical techniques, including approximate diagonalization, quantum Monte Carlo methods, and density matrix renormalization group algorithms. Recently, we introduced an innovative Monte Carlo simulation technique based on the worm algorithm, which allows us to simulate large systems at significantly high altitudes where traditional Monte Carlo methods struggle due to critical slowdown. We applied this new method to estimate the free energies of several different supersymmetric lattice models, including N=4 supersymmetric Yang-Mills theory and N=1 supersymmetric U(1) gauge theory related to matter groups in various representations.",
        "ori-fast-z-score": 0.6255432421712244,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": 0.7181848464596079
    },
    {
        "original_text": "Social networks are ubiquitous in our daily life, and have been studied extensively by researchers across many disciplines. However, the analysis of social network data is often challenging due to its complexity. In this work we propose NodeTrix, an efficient hybrid representation that can be used to analyze large-scale social networks efficiently. We show how NodeTrix can be applied to solve several important problems including community detection, link prediction, node classification, and influence maximization. Our experiments on real-world datasets demonstrate that NodeTrix outperforms state-of-the-art approaches significantly both in terms of efficiency and effectiveness. 1 Introduction Social networks play an increasingly important role in people s lives. They provide us with new ways to communicate with each other, share information, collaborate, or even make friends. As such, they have attracted much attention from researchers across various fields ranging from sociology  1  , psychology  2  , biology  3  , computer science  4  , engineering  5  , etc.. The rapid development of online social media has led to unprecedented growth in the amount of available social network data  6  . For example, Facebook alone now contains more than one billion active users  7  .\nHowever, analyzing large volumes of social network data remains a challenge because it usually involves complex relationships among nodes  8  . To tackle these challenges, recent research efforts focus on developing effective representations for social networks  9  -  11  . These representations aim at capturing different aspects of social networks while being able to scale up well when dealing with massive amounts of data  12  . Among them, matrix factorization techniques  13  -  15  have shown great promise as they allow us to represent social networks using low-rank matrices  16  . Matrix factorization methods decompose a given adjacency matrix into two smaller matrices (i.e., latent factors) which capture structural properties of the original graph  17  .",
        "watermark_text": "Social networks are ubiquitous in our daily living , and have been studied frequently by researchers across many disciplines . However , the examination of social group information is often challenging due to its complexity .In this research we develop NodeTrix , an efficient hybrid representation that can be used to analyze large - scale social systems efficiently . We see how NodeTrix can be applied to solve many important problems namely community detection , link discovery , node classification , and influence maximization .Our experiments on real - global datasets prove that NodeTrix outperforms state - of - the - art methods significantly both in terms of efficiency and effectiveness . 1 Introduction Social networks take an increasingly important role in everyone s lives .They offer us with innovative ways to interact with each other , transfer material , collaborate , or even keep friends . As such , they have garnered many scrutiny from researchers across numerous topics including from geography 1 , psychology 2 , chemistry 3 , computer science 4 , engineering 5 , etc . .The rapid progress of internet social marketing has led to unprecedented growth in the quantity of available social platform data 6 . For instance , Facebook alone now contains more than one billion active people 7 .However , examining high quantities of social group information remains a problem because it often includes difficult connections among nodes 8 . To tackle these problems , recent study efforts focus on developing accurate representations for social organizations 9 - 11 .These representations aim at representing different components of social organizations while being able to level up well when dealing with massive amounts of statistics 12 . Among them , matrix factorization techniques 13 - 15 have shown great hope as they allow us to depict public networks using reduced - class matrices 16 .Matrix factorization techniques decompose a given adjacency vector into two smaller matrices ( i . e . , latent factors ) which capture structural aspects of the original graph 17 .",
        "rewrite_text": "Social networks have become an integral part of our daily lives and have been the focus of extensive study across various academic disciplines. However, analyzing social group information presents challenges due to its inherent complexity. In this research, we introduce NodeTrix, an efficient hybrid representation designed to facilitate the analysis of large-scale social systems. We demonstrate how NodeTrix can address several critical issues, including community detection, link discovery, node classification, and influence maximization. Our experiments conducted on real global datasets show that NodeTrix significantly outperforms state-of-the-art methods in both efficiency and effectiveness.\n\nSocial networks play an increasingly vital role in our lives, providing innovative ways to interact, share resources, collaborate, and maintain friendships. This prominence has drawn considerable attention from researchers in diverse fields such as geography, psychology, chemistry, computer science, and engineering. The rapid advancement of internet social marketing has resulted in an unprecedented increase in the volume of data available from social platforms. For example, Facebook alone has over one billion active users. Yet, the challenge of analyzing vast amounts of social group information persists, often due to complex relationships between nodes. Recent research efforts have focused on creating accurate representations of social networks, aimed at effectively modeling various components while managing large datasets. Among these efforts, matrix factorization techniques have shown great promise, as they allow for the representation of social networks using reduced-dimensional matrices. These techniques work by decomposing an adjacency matrix into two smaller matrices (latent factors) that capture the structural characteristics of the original graph.",
        "ori-fast-z-score": 0.15617376188860607,
        "water-fast-z-score": 9.419837224354428,
        "rewrite-fast-z-score": 1.9339751136609127
    },
    {
        "original_text": "We study the holographic entanglement entropy for three-dimensional de Sitter space with gravitational Chern-Simons term by using the replica trick and the covariant phase-space method. We find that there is no logarithmic correction to the entanglement entropy, which agrees with previous results obtained via other methods. In addition, we show that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Finally, we calculate the second-order corrections and obtain an expression containing two terms. One of them has been previously found in Ref.  Phys. Rev. D 98 (2018) 084011  while another one is new. The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points. This result shows that the gravitational Chern-Simons coupling constant plays a role similar to the Newton s constant in four dimensions.",
        "watermark_text": "We explore the holographic entanglement entropy for three - dimensional de Sitter space with gravitational Chern - Simons term by using the replica trick and the covariant phase - space method . We see that there is no logarithmic correction to the entanglement entropy , which agrees with previous findings obtained via other methods .In addition , we find that the first - order corrections are equal to the square root of the volume enclosed by the entangling surface . Finally , we estimate the second - order corrections and find an expression containing two terms .One of them has been previously found in Ref.Phys.Rev.D 98 (2018) 084011  while another one is new.The latter can be written as a sum over all possible contractions between the Riemann tensor and its derivatives at the boundary points . This result suggests that the gravitational Chern - Simons correlation function plays a role similar to the Newton s constant in four dimensions .",
        "rewrite_text": "We investigate holographic entanglement entropy in three-dimensional de Sitter space with a gravitational Chern-Simons term, utilizing the replica trick and the covariant phase-space method. Our findings indicate that there are no logarithmic corrections to the entanglement entropy, consistent with previous results achieved through other techniques. Furthermore, we discover that the first-order corrections are proportional to the square root of the volume enclosed by the entangling surface. Lastly, we estimate the second-order corrections, yielding an expression with two components. One of these components has been reported in Phys. Rev. D 98 (2018) 084011, while the other is novel. The latter can be expressed as a sum of all possible contractions involving the Riemann tensor and its derivatives at the boundary points. This outcome suggests that the gravitational Chern-Simons correlation function may play a role analogous to Newton's constant in four-dimensional space.",
        "ori-fast-z-score": 1.1920791213585393,
        "water-fast-z-score": 3.474396144861517,
        "rewrite-fast-z-score": 0.6622661785325219
    },
    {
        "original_text": "We present the first results for atmospheric dynamics in short period extra-solar gas giant planets (EGPs) using 3D general circulation models with radiative transfer and realistic opacities. We find that the night-side temperature is strongly dependent on opacity, which determines how much energy can be transported to space by radiation. The day-night contrast increases as we decrease the opacity because less heat escapes through the nightside atmosphere. This effect is more pronounced at lower pressures where convection becomes inefficient. For low enough opacities, the planet cools down completely during its orbit resulting in an extremely cold night side. Our simulations show that EGPs are likely to have very different climates depending on their composition.  Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "watermark_text": "We present the first findings for atmospheric mechanics in small history additional - solar gas giant galaxies ( EGPs ) using 3D general circulation estimates with radiative transfer and realistic opacities . We see that the night - side temperature is strongly dependent on opacity , which determines how many heat can be transported to space by radiation .The day - night difference rises as we decrease the opacity because lighter thermal exits through the nightside environment . This phenomenon is more pronounced at lower pressures where circulation becomes inefficient .For low enough opacities , the planet cools down fully during its orbit producing in an incredibly cold evening side . Our simulations see that EGPs are likely to have very different climates based on their composition .Keywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast",
        "rewrite_text": "We present the initial findings on atmospheric dynamics in small, solar gas giant galaxies (EGPs) through 3D general circulation modeling that incorporates radiative transfer and realistic opacities. Our results indicate that the temperature on the night side is significantly influenced by opacity, which affects the amount of heat that can escape into space via radiation. As opacity decreases, the day-night temperature disparity increases, as lighter thermal energy is allowed to dissipate through the night side. This effect is particularly pronounced at lower pressures, where atmospheric circulation becomes less efficient. In cases of sufficiently low opacity, the planet can cool completely throughout its orbit, resulting in an extremely cold night side. Our simulations suggest that EGPs may exhibit markedly different climates depending on their compositions. \n\nKeywords: General Circulation Modeling, Extrasolar Planetary Systems, Radiation Transfer, Climate, Atmosphere, Energy Transport, Convection, Cooling Rates, Day-Night Contrast.",
        "ori-fast-z-score": -0.2581988897471611,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results for y-scaling analysis performed on data collected at Jefferson Lab Hall A experiment E06-010 using an unpolarized electron beam with energies ranging between 1 and 5 GeV incident upon a liquid hydrogen target. The experimental data are compared to theoretical calculations based on relativistic impulse approximation (RIA) model, which is used as a baseline reference calculation. We also compare our results to those obtained by applying the same analysis technique to previously published data sets taken at lower energy ranges. \n \n In this work we have studied scaling behavior of quasi-elastic scattering cross sections from nuclei over wide range of kinematics covering both longitudinal and transverse response functions. Our study has been carried out within the framework of relativistic impulse approximation (RIA), where nuclear responses are calculated using realistic nucleonnucleon interactions. For comparison purposes, we have applied similar analysis techniques to two different data sets: one measured recently at Jefferson Lab Hall A  1  , and another one taken earlier  2  .",
        "watermark_text": "We present the conclusion for y - scaling investigation performed on evidence generated at Jefferson Lab Hall A project E06 - 010 using an unpolarized electron beam with energies ranging between 1 and 5 GeV incident upon a liquid hydrogen source . The empirical data are compared to theoretical calculations based on relativistic impulse approximation ( RIA ) model , which is utilized as a baseline reference calculation .We additionally compare our findings to those achieved by using the same evaluation technique to formerly reported information sets taken at lower energy ranges . In this research we have researched scaling behavior of quasi - elastic scattering cross sections from nuclei over broad spectrum of kinematics encompassing both longitudinal and longitudinal response properties .Our study has been carried out within the framework of relativistic impulse approximation ( RIA ) , where nuclear signals are measured using accurate nucleonnucleon interactions . For comparison purposes , we have applied related analysis methods to two different data sets : one measured recently at Jefferson Lab Hall A 1 , and another one taken earlier 2 .",
        "rewrite_text": "We present our conclusions from the y-scaling investigation conducted using data from the Jefferson Lab Hall A project E06-010. This study utilized an unpolarized electron beam with energies between 1 and 5 GeV, targeting a liquid hydrogen source. The empirical results were compared to theoretical predictions based on the relativistic impulse approximation (RIA) model, which served as our baseline reference. Additionally, we contrasted our findings with those obtained from the same analytical approach applied to previously reported datasets collected at lower energy ranges. Our research focused on examining the scaling behavior of quasi-elastic scattering cross sections from nuclei across a broad kinematic spectrum, addressing both longitudinal and transverse response characteristics. This study was executed within the RIA framework, utilizing precise nucleon-nucleon interactions to measure nuclear signals. For comparative analysis, we employed similar methods on two distinct datasets: one collected recently at Jefferson Lab Hall A, and another from an earlier study.",
        "ori-fast-z-score": -0.7683498199278324,
        "water-fast-z-score": 6.037034299432969,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "We present the first analysis of water vapor in irradiated planets using infrared (IR) spectroscopy with the Infrared Spectrograph on board the Spitzer Space Telescope. We have analyzed four transiting exoplanet systems, HD 189733b, HD 209458b, WASP-12b and XO-1b, which are known to be strongly irradiated by their host stars. The IR spectra were obtained during secondary eclipse events when the planet passes behind its star as seen from Earth. Our results show that all these planets exhibit strong absorption features at wavelengths longer than 5 microns due to water vapor in their atmospheres. These observations provide direct evidence for the presence of water vapor in highly-irradiated planetary atmospheres.  Keywords: Exoplanet, Transmission spectrum, Secondary eclipse, Water vapor, Infrared spectrophotometry, Spitzer Space Telescope. 1 Introduction   Water is one of the most important molecules in our Solar System because it plays an essential role in life processes. It has been detected in many different environments ranging from comets to icy satellites such as Europa or Enceladus. However, despite numerous efforts over several decades, no unambiguous detection of water had yet been reported outside our Solar System until recently. This situation changed dramatically thanks to space-based observatories like Hubble Space Telescope (HST), Chandra X-ray Observatory, and especially Spitzer Space Telescope (Werner et al., 2004) .  Since its launch in 2003, Spitzer has observed thousands of targets including hundreds of extrasolar planets. Among them, there are some very interesting cases where the planet orbits close to its parent star so that the intense stellar radiation heats up the atmosphere of the planet significantly. As a result, the atmospheric composition can change drastically compared to what we know about terrestrial planets in our Solar System. For example, if the temperature becomes high enough, hydrogen could escape from the planet s upper atmosphere into space leaving only helium behind (Lammer et al., 2003; Baraffe et al., 2004; Yelle et al., 2006) , while other species may condense out onto",
        "watermark_text": "We present the first assessment of water vapor in irradiated planets using infrared ( IR ) spectroscopy with the Infrared Spectrograph on board the Spitzer Space Telescope . We have analyzed four transiting exoplanet systems , HD 189733b , HD 209458b , WASP - 12b and XO - 1b , which are known to be highly irradiated by their host stars .The IR spectra were obtained during secondary eclipse events when the planet walks behind its sun as watched from Earth . Our results show that all these planets exhibit strong absorption elements at wavelengths greater than 5 microns due to water vapor in their atmospheres .These measurements give substantial proof for the activity of liquid vapor in highly - irradiated planetary atmospheres . Keywords : Exoplanet , Transmission spectrum , Secondary eclipse , Water dust , Infrared spectrophotometry , Spitzer Space Telescope .1 Introduction Water is one of the most important molecules in our Solar System because it serves an essential part in living systems . It has been detected in multiple diverse settings ranging from comets to icy spacecraft such as Europa or Enceladus .However , despite several efforts over numerous years , no unambiguous detection of water had yet been reported outside our Solar System until recently . This condition changed dramatically due to space - based observatories like Hubble Space Telescope ( HST ) , Chandra X - ray Observatory , and particularly Spitzer Space Telescope ( Werner et al . , 2004 ) .Since its launch in 2003 , Spitzer has observed thousands of targets covering hundreds of extrasolar stars . Among them , there are some very interesting cases where the planet orbits low to its father planet so that the powerful stellar radiation heats up the atmosphere of the planet significantly .As a result , the atmospheric composition can shift drastically compared to what we know about terrestrial worlds in our Solar System . For instance , if the temperature gets high enough , hydrogen could exit from the planet s upper atmosphere into space leaving only helium behind ( Lammer et al . , 2003 ; Baraffe et al . , 2004 ; Yelle et al . , 2006 ) , while other species may condense out onto",
        "rewrite_text": "We present the inaugural evaluation of water vapor on irradiated exoplanets using infrared (IR) spectroscopy conducted with the Infrared Spectrograph aboard the Spitzer Space Telescope. Our analysis includes four transiting exoplanet systems: HD 189733b, HD 209458b, WASP-12b, and XO-1b, all of which experience significant irradiation from their host stars. The IR spectra were collected during secondary eclipse events, when the planets transit behind their stars as viewed from Earth. Our findings indicate that all these exoplanets show strong absorption features at wavelengths exceeding 5 microns, attributable to water vapor in their atmospheres. These observations provide compelling evidence for the presence of liquid vapor in highly-irradiated planetary atmospheres. \n\nKeywords: Exoplanet, Transmission spectrum, Secondary eclipse, Water vapor, Infrared spectrophotometry, Spitzer Space Telescope.\n\n1. Introduction \nWater is a crucial molecule within our Solar System due to its vital role in biological systems. It has been observed in various environments, from comets to icy celestial bodies like Europa and Enceladus. Nevertheless, despite extensive efforts over many years, unambiguous detections of water beyond our Solar System had not been achieved until recently. This situation improved significantly with the advent of space-based observatories such as the Hubble Space Telescope (HST), Chandra X-ray Observatory, and especially the Spitzer Space Telescope (Werner et al., 2004). Since its launch in 2003, Spitzer has examined thousands of targets, including numerous extrasolar stars. Among these are particularly intriguing cases of planets that orbit close to their host stars, resulting in intense stellar radiation that markedly heats their atmospheres. Consequently, the atmospheric compositions of these planets can differ significantly from the terrestrial environments we know in our Solar System. For instance, at sufficiently high temperatures, hydrogen may escape from the upper atmosphere into space, leaving behind only helium (Lammer et al., 2003; Baraffe et al., 2004; Yelle et al., 2006), while other species might condense.",
        "ori-fast-z-score": -1.044465935734187,
        "water-fast-z-score": 5.63621480190678,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "We present an ab initio study on spin relaxation mechanisms in bulk, single- and double-quantum-well (DQW) structures based on zinc-blende semiconductors such as GaAs or InP. We focus our attention on the so-called Bir-Aronov-Pikuz mechanism which is responsible for spin-flip transitions between conduction-band states with different orbital angular momenta. The main results are summarized below.  For bulk materials we find that the dominant contribution comes from intra-valley scattering processes involving heavy-hole bands. This result agrees well with previous theoretical studies performed within effective-mass approximations. However, by using realistic band-structure calculations we show that inter-valley contributions can also play an important role when considering DQWs grown along non  001  directions. Finally, we discuss how these findings could be used to improve existing models describing spin relaxation times in semiconductor nanostructures.",
        "watermark_text": "We present an ab initio investigation on spin relaxation mechanisms in bulk , double - and double - quantum - well ( DQW ) complexes based on aluminium - blende semiconductors such as GaAs or InP . We focus our focus on the so - called Bir - Aronov - Pikuz process which is responsible for spin - flip transitions between conduction - band elements with various orbital angular momenta .The main results are presented below . For bulk materials we find that the dominant contribution comes from intra - valley reflection processes involving heavy - hole lines .This result agrees well with previous conceptual research performed within efficient - mass approximations . However , by using accurate band - structure estimates we find that cross - valley contributions can also play an important role when assessing DQWs grown along non 001 directions .Finally , we explain how these results could be used to improve previous descriptions describing spin relaxation times in semiconductor nanostructures .",
        "rewrite_text": "We present an ab initio study of spin relaxation mechanisms in bulk materials, as well as double and double quantum well (DQW) complexes based on aluminum-blende semiconductors like GaAs and InP. Our primary focus is on the Bir-Aronov-Pikus process, which facilitates spin-flip transitions among conduction-band states with different orbital angular momenta. The key findings are outlined below. For bulk materials, we determine that the primary contribution arises from intra-valley reflection processes involving heavy-hole bands. This finding aligns well with earlier conceptual studies conducted using effective mass approximations. However, through precise band-structure calculations, we discover that cross-valley contributions also significantly influence spin relaxation when examining DQWs oriented in non-001 directions. Finally, we discuss how these insights could enhance existing models of spin relaxation times in semiconductor nanostructures.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 1.937329799813845
    },
    {
        "original_text": "We study the fractal dimension of domain walls (DWs) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures. We find that DWs are fractals for all values of temperature studied here, i.e., T = 0.5J/kB to 1.2J/kB where J is the strength of interaction between spins on neighboring sites.  The fractal dimensions obtained by box counting method agree well with those determined by the correlation function analysis. In addition we show that the fractal dimension decreases as the temperature increases. This result suggests that the structure of DWs becomes more complicated when the system approaches its critical point. Finally it should be noted that our results can also be applied to other systems such as vortex lines in type-II superconductors or dislocation networks in crystals. Two-dimensional Ising spin glasses have been extensively investigated both experimentally  1  and theoretically  2  . It has been shown that these models exhibit many interesting phenomena including phase transitions  3  , spin-glass states  4  , and glassy dynamics  5  .\nIn this work we focus on one particular aspect of the model which is the fractal nature of domain walls  6  . Domain wall refers to an interface separating different ordered phases  7, 8  . For example, in ferromagnetic materials there exist two types of domains; up and down magnetization  9  . These domains are separated by interfaces called domain walls  10  . Similarly, in antiferromagnets  11  , there exists four possible orientations of magnetic moments  12  ; three of them form triangular sublattices while the fourth forms a square lattice  13  . Therefore, there will be six types of domain walls  14  .",
        "watermark_text": "We research the fractal dimension of domain walls ( DWs ) in two dimensional Ising spin glasses with nearest neighbor interactions and random bonds using Monte Carlo simulations at finite temperatures . We see that DWs are fractals for all values of temperature described here , i . e . , T = 0 . 5J / kB to 1 . 2J / kB where J is the strength of coupling between spins on nearby locations .The fractal sizes obtained by box counting method comply better with those determined by the correlation function analysis . In addition we prove that the fractal dimension decreases as the temperature increases .This result suggests that the composition of DWs changes more complicated when the process approaches its critical position . Finally it should be mentioned that our findings can also be applied to other structures such as vortex lines in type - II superconductors or dislocation networks in crystals .Two - dimensional Ising spin glasses have been heavily examined both experimentally 1 and theoretically 2 . It has been shown that these models exhibit several interesting phenomena including phase transitions 3 , spin - glass states 4 , and glassy dynamics 5 .In this research we focus on one special aspect of the model which is the fractal nature of domain barriers 6 . Domain wall refers to an interface separating different ordered phases 7 , 8 .For instance , in ferromagnetic metal there exist two forms of residues ; up and down magnetization 9 . These residues are separated by interfaces called domain barriers 10 .Similarly , in antiferromagnets 11 , there exists four possible orientations of magnetic moments 12 ; three of them form square sublattices while the fifth forms a square lattice 13 . Therefore , there will be six kinds of domain walls 14 .",
        "rewrite_text": "We investigate the fractal dimension of domain walls (DWs) in two-dimensional Ising spin glasses characterized by nearest neighbor interactions and random bonds through Monte Carlo simulations at finite temperatures. Our findings indicate that DWs exhibit fractal properties across the entire temperature range studied, specifically from T = 0.5J/kB to 1.2J/kB, where J represents the coupling strength between adjacent spins. The fractal dimensions computed using the box counting method align more closely with those derived from correlation function analyses. Furthermore, we demonstrate that the fractal dimension decreases with increasing temperature, suggesting that the structure of DWs becomes more complex as the system approaches its critical point. Notably, our results may also have implications for other configurations, such as vortex lines in type-II superconductors and dislocation networks in crystals. Two-dimensional Ising spin glasses have been extensively studied both experimentally and theoretically, revealing various intriguing phenomena, including phase transitions, spin-glass states, and glassy dynamics. Our research specifically highlights the fractal characteristics of domain walls, which act as interfaces separating distinct ordered phases. For example, in a ferromagnetic metal, there are two forms of magnetization—up and down—that are divided by domain walls. Similarly, in antiferromagnets, four orientations of magnetic moments exist, with three forming square sublattices and the fourth forming a square lattice, resulting in six types of domain walls.",
        "ori-fast-z-score": 0.9205746178983234,
        "water-fast-z-score": 6.812252172447593,
        "rewrite-fast-z-score": -1.0536089137432665
    },
    {
        "original_text": "The shock temperatures for molybdenum were determined by measuring the electrical resistance of samples shocked to pressures up to 1,000 kilobars (1 Mbar). The release temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at various levels of pressure. \n \n Shocks produced by a pulsed power machine were used to compress the sample material between two electrodes. A voltage pulse applied across these electrodes caused current flow through the compressed material which generated Joule heating. This heat increased the resistivity of the material causing it to expand rapidly as its temperature rose above the Curie point. As this expansion occurred, the resistance dropped dramatically resulting in a sharp increase in current flowing into the sample. When the current reached a critical value, the sample exploded releasing most of its stored energy. The explosion also destroyed one or both of the electrodes so that no further measurement could be made until new ones were installed.",
        "watermark_text": "The jolt pressures for molybdenum were determined by monitoring the electrical resistance of samples shocked to pressures up to 1 , 000 kilobars ( 1 Mbar ) . The return temperature was measured using an optical pyrometer on samples that had been heated with laser light after being shocked at several degrees of pressure .Shocks created by a pulsed power machine were used to compress the sample material between two electrodes . A voltage beam applied across these electrodes induced charge flow through the compressed material which generated Joule heating .This temperature improved the resistivity of the material creating it to expand rapidly as its temperature climbed above the Curie point . As this growth occurred , the tolerance fell significantly causing in a sharp increase in current flowing into the sample .When the current reached a critical level , the sample erupted releasing most of its stored power . The explosion also damaged one or both of the electrodes so that no further measurement could be made until fresh ones were introduced .",
        "rewrite_text": "The jolt pressures for molybdenum were assessed by monitoring the electrical resistance of samples subjected to pressures of up to 1,000 kilobars (1 Mbar). The return temperature was recorded using an optical pyrometer on samples that had been heated with laser light after being subjected to various pressure levels. Pulsed power machines generated shocks that compressed the sample material between two electrodes. A voltage applied across these electrodes initiated a charge flow through the compressed material, resulting in Joule heating. This increase in temperature reduced the material's resistivity, causing it to expand rapidly as its temperature surpassed the Curie point. As this expansion took place, the material's tolerance decreased significantly, leading to a sharp rise in the current passing through the sample. Once the current reached a critical threshold, the sample erupted, releasing most of its stored energy. This explosion often damaged one or both electrodes, necessitating the introduction of new electrodes for further measurements.",
        "ori-fast-z-score": 1.0256451881367414,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 3.348631561299829
    },
    {
        "original_text": "We present new near-infrared integral field spectroscopy observations of two nearby active galactic nuclei (AGNs) with compact nuclear star formation, NGC 1068 and Mrk 3. The data were obtained using SINFONI at VLT/UT4. We find that both AGNs have very high infrared luminosities (LIR ~10^11 Lsun), which are dominated by emission lines originating from warm molecular gas excited by young stars. In addition to these bright line features we detect spatially extended continuum emission on scales of several hundred parsecs around each nucleus. This is likely due to dust heated by the intense radiation fields produced by the AGN and/or hot young stars. Our results show that the presence of an AGN does not necessarily prevent massive star formation within its host galaxy. These findings suggest that there may be a close link between black hole growth and nuclear star formation activity.  Keywords: Active Galactic Nuclei",
        "watermark_text": "We report new near - infrared integral field spectroscopy observations of two nearby active galactic nuclei ( AGNs ) with compact radioactive star formation , NGC 1068 and Mrk 3 . The data were obtained using SINFONI at VLT / UT4 .We see that both AGNs have very high infrared luminosities ( LIR ~ 10 ^ 11 Lsun ) , which are dominated by absorption patterns derived from hot molecular gas excited by young stars . In addition to these faint line characteristics we perceive spatially extended continuum emission on scales of several hundred parsecs around each core .This is probably due to dust warmed by the energetic radiation fields produced by the AGN and / or hotter young galaxies . Our results show that the presence of an AGN does not necessarily cause heavy star formation within its host universe .These studies propose that there may be a close link between black hole growth and nuclear star formation activity . Keywords : Active Galactic Nuclei",
        "rewrite_text": "We present new near-infrared integral field spectroscopy observations of two nearby active galactic nuclei (AGNs) exhibiting compact regions of star formation: NGC 1068 and Mrk 3. The data were collected using SINFONI at the VLT/UT4. Our findings indicate that both AGNs possess significantly high infrared luminosities (LIR ~ 10^11 Lsun), primarily characterized by absorption patterns from hot molecular gas excited by young stars. Alongside these subtle line features, we also observe spatially extended continuum emission over scales of several hundred parsecs around each nucleus. This emission is likely the result of dust being heated by the intense radiation fields generated by the AGN and/or younger, hotter galaxies. Our results suggest that the presence of an AGN does not necessarily lead to intense star formation in its host galaxy. These studies indicate a potential connection between black hole growth and nuclear star formation activity. Keywords: Active Galactic Nuclei",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.09644015186457,
        "rewrite-fast-z-score": 1.709408646894569
    },
    {
        "original_text": "We study the gauge structure of quantum spin systems with local interactions, which are described by the so-called Kitaev model. We show that this model is equivalent to an exactly solvable lattice gauge theory (LGT) whose action functional is given by the Chern-Simons term. The equivalence between these two theories allows us to use powerful techniques developed for LGTs such as the index theorem or the localization technique. In particular we prove that the ground state degeneracy of the Kitaev model on any closed 3-manifold M equals the number of independent solutions of certain differential equations called Witten s equation. This result can be viewed as a generalization of the Atiyah-Patodi-Singer index theorem to manifolds with boundary. Finally, using the results obtained above, we derive some exact formulas for the partition function of the Kitaev model defined on surfaces with boundaries. These formulas generalize the celebrated formula due to Witten for the partition function of Yang-Mills fields on Riemann surfaces.",
        "watermark_text": "We research the gauge formation of quantum spin systems with local interactions , which are explained by the so - called Kitaev description . We see that this description is analogous to an exactly solvable lattice gauge theory ( LGT ) whose action functional is given by the Chern - Simons term .The equivalence between these two models provides us to use powerful tools developed for LGTs such as the index theorem or the localization technique . In particular we prove that the ground state degeneracy of the Kitaev theory on any closed 3 - sphere M equals the quantity of independent solutions of certain differential equations called Witten s equation .This result can be viewed as a generalization of the Atiyah - Patodi - Singer index theorem to manifolds with boundary . Finally , using the results derived above , we derive some exact formulas for the splitting function of the Kitaev theory defined on surfaces with borders .These formulas generalize the celebrated formula due to Witten for the splitting function of Yang - Mills fields on Riemann surfaces .",
        "rewrite_text": "We investigate the gauge formation in quantum spin systems characterized by local interactions, which are described by the Kitaev framework. This framework is analogous to an exactly solvable lattice gauge theory (LGT), where the action functional is represented by the Chern-Simons term. The correspondence between these two models allows us to apply powerful techniques developed for LGTs, including the index theorem and localization methods. Notably, we demonstrate that the ground state degeneracy of the Kitaev model on any closed 3-sphere \\( M \\) corresponds to the number of independent solutions to certain differential equations known as Witten's equation. This finding can be regarded as a generalization of the Atiyah-Patodi-Singer index theorem for manifolds with boundaries. Furthermore, by leveraging the aforementioned results, we derive exact formulas for the splitting function of the Kitaev model defined on surfaces with boundaries. These formulas extend the well-known result by Witten concerning the splitting function of Yang-Mills fields on Riemann surfaces.",
        "ori-fast-z-score": 1.7801724872907798,
        "water-fast-z-score": 6.289942788427422,
        "rewrite-fast-z-score": -0.47140452079103173
    },
    {
        "original_text": "We report on radio observations made with the Australia Telescope Compact Array (ATCA) and Parkes Observatory toward the magnetar candidate X-ray transient source XTE J1810−197, which was discovered in outburst by RXTE/ASM during March 2009. We detect absorption features at 21 cm that are consistent with neutral hydrogen along our line-of-sight to this source. Using these data we derive an upper limit for its distance of <5 kpc. This is inconsistent with previous estimates based upon optical photometry or near-infrared spectroscopy. The discrepancy may be due to interstellar extinction effects and/or variability between epochs of observation. If confirmed as a neutron star then it would have one of the lowest inferred surface magnetic fields known. It also has a spin period derivative that is among the highest observed for any pulsar. These properties make it unique amongst currently-known neutron stars. Keywords: Neutron Star -Magnetic Field Strength, Pulsar -Distance Measurement",
        "watermark_text": "We report on radio observations made with the Australia Telescope Compact Array ( ATCA ) and Parkes Observatory toward the magnetar candidate X - ray transient source XTE J1810−197 , which was discovered in outburst by RXTE / ASM during March 2009 . We detect reflection features at 21 cm that are compatible with neutral hydrogen along our line - of - view to this source .Using these information we derive an upper maximum for its radius of < 5 kpc . This is conflicting with previous accounts based upon infrared photometry or near - infrared spectroscopy .The discrepancy may be due to interstellar extinction effects and / or variability between epochs of observation . If confirmed as a neutron star then it would have one of the smallest inferred surface magnetic fields known .It additionally has a spin time derivative that is among the highest studied for any pulsar . These properties make it distinctive amongst currently - recorded neutron stars .Keywords: Neutron Star -Magnetic Field Strength, Pulsar -Distance Measurement",
        "rewrite_text": "We present radio observations conducted with the Australia Telescope Compact Array (ATCA) and Parkes Observatory focused on the magnetar candidate X-ray transient source XTE J1810−197, which was identified during its outburst by RXTE/ASM in March 2009. Our observations reveal reflection features at 21 cm that correspond to neutral hydrogen along the line of sight to this source. Based on this data, we estimate an upper limit for its radius of less than 5 kpc. This finding contradicts earlier conclusions derived from infrared photometry and near-infrared spectroscopy. The discrepancy might be attributed to interstellar extinction effects or variations between different observational epochs. If confirmed as a neutron star, it could possess one of the smallest inferred surface magnetic fields known. Additionally, it has one of the highest spin time derivatives recorded for any pulsar. These characteristics make it unique among currently documented neutron stars. Keywords: Neutron Star - Magnetic Field Strength, Pulsar - Distance Measurement.",
        "ori-fast-z-score": 0.2626128657194451,
        "water-fast-z-score": 5.337745350213779,
        "rewrite-fast-z-score": 1.1523319193960637
    },
    {
        "original_text": "We study the collective modes in two-band superconductors with different gaps and masses, using the random phase approximation (RPA). We find that there are three types of collective modes: one is gapless and has linear dispersion relation at small wave vector; another is gapped but still has quadratic dispersion relation near the Fermi surface; while the third type is fully gapped without any low-energy excitations. The latter two types can be regarded as phonon-like collective modes. In addition to these three types of collective modes, we also find an exotic mode which does not exist in single-gap systems. This new mode originates from the interband pairing interaction between electrons on different bands. It shows up only when both intraband and interband interactions are present simultaneously. Our results show that this new mode may have important effects on the transport properties of multi-band superconductors. \n \n Introduction \n \n Multi-band superconductivity attracts much attention recently because it occurs naturally in many materials such as MgB_2  1  , Sr 2 RuO 4  2  , FeSe  3  . These compounds usually contain several orbitals per unit cell so they support multiple electronic bands crossing the Fermi level  4  . Due to the presence of more than one band, the electron-phonon coupling strength could vary significantly among different bands  5  . Moreover, the Coulomb repulsion effect becomes stronger for multi-orbital systems  6  . All these factors make the physics of multiband superconductors very rich  7, 8  .\n \nIn recent years, great progresses have been made in understanding the physical properties of multi-band superconductor  9  . For example, the vortex lattice structure  10  , magnetic field dependence  11  , thermal conductivity  12  , specific heat  13  , NMR relaxation rate  14  etc., were studied extensively by experiments. On the theoretical side, various methods including mean-field theory  15  , Eliashberg formalism  16  , functional renormalization group  17  , variational Monte Carlo  18  , exact diagonalization  19  , density matrix renormalization group  20  , and quantum Monte Carlo  21  were used to investigate the ground state properties  22  , thermodynamic quantities  23  ,",
        "watermark_text": "We research the collective modes in two - band superconductors with various gaps and masses , using the random phase approximation ( RPA ) . We see that there are three categories of collective modes : one is gapless and has continuous dispersion relation at small wave vector ; another is gapped but still has quadratic dispersion relation near the Fermi surface ; while the third type is fully gapped without any low - energy excitations .The last two forms can be regarded as phonon - like collective modes . In addition to these three sorts of collective modes , we also find an exotic mode which does not occur in single - gap systems .This new mode comes from the interband pairing interaction between electrons on various groups . It gets up only when both intraband and interband interactions are present concurrently .Our results show that this new mode may have important effects on the travel properties of multi - band superconductors . Introduction Multi - band superconductivity attracts great concern lately because it appears naturally in many materials such as MgB _ 2 1 , Sr 2 RuO 4 2 , FeSe 3 .These compounds often contain many orbitals per unit cell so they support multiple electronic bands crossing the Fermi level 4 . Due to the presence of more than one band , the electron - phonon coupling strength could vary significantly among different bands 5 .Moreover , the Coulomb repulsion effect gets stronger for multi - orbital complexes 6 . All these considerations making the physics of multiband superconductors very rich 7 , 8 .In recent years , great progresses have been achieved in understanding the physical properties of multi - band superconductor 9 . For instance , the vortex lattice structure 10 , magnetic field dependence 11 , thermal conductivity 12 , basic heat 13 , NMR relaxation speed 14 etc . , were studied thoroughly by research .On the theoretical front , various methods notably mean - field model 15 , Eliashberg formalism 16 , functional renormalization group 17 , variational Monte Carlo 18 , exact diagonalization 19 , density matrix renormalization group 20 , and quantum Monte Carlo 21 were used to examine the ground state properties 22 , thermodynamic quantities 23 ,",
        "rewrite_text": "We investigate the collective modes in two-band superconductors characterized by diverse gap energies and effective masses through the random phase approximation (RPA). Our findings reveal three main categories of collective modes: the first category is gapless, exhibiting a continuous dispersion relationship at small wave vectors; the second category is gapped, yet it displays a quadratic dispersion near the Fermi surface; and the third category is fully gapped with no low-energy excitations. The second and third categories can be interpreted as phonon-like collective modes. Additionally, we identify an exotic mode that is not present in single-gap systems. This new mode arises from the interband pairing interactions among electrons in different bands and only manifests when both intraband and interband interactions coexist. Our results suggest that this new mode could significantly influence the transport properties of multi-band superconductors.\n\n**Introduction**  \nRecently, multi-band superconductivity has garnered significant attention due to its natural occurrence in various materials such as MgB₂, Sr₂RuO₄, and FeSe. These compounds often feature multiple orbitals per unit cell, enabling several electronic bands to cross the Fermi level. The existence of multiple bands leads to considerable variability in electron-phonon coupling strength across different bands. Furthermore, the Coulomb repulsion effect becomes more pronounced in multi-orbital systems. These factors contribute to the intricate physics underlying multi-band superconductors. In recent years, substantial advancements have been made in understanding the physical characteristics of these materials. For example, thorough investigations have been conducted on the vortex lattice structure, magnetic field dependence, thermal conductivity, basic heat properties, and NMR relaxation rates. On the theoretical side, a variety of methods—including the mean-field model, Eliashberg formalism, functional renormalization group, variational Monte Carlo, exact diagonalization, density matrix renormalization group, and quantum Monte Carlo—have been employed to explore ground state properties and thermodynamic quantities.",
        "ori-fast-z-score": 0.9607689228305227,
        "water-fast-z-score": 6.943355894868313,
        "rewrite-fast-z-score": 1.750226025186606
    },
    {
        "original_text": "We study the phase behavior of a system of N identical hard rods confined to a square box with periodic boundary conditions, using Monte Carlo simulations at constant pressure P . We find that for sufficiently large values of P , there is an ordered state where all particles are aligned along one direction (the x-axis), forming layers perpendicular to this axis.  The transition between disordered and ordered states occurs via a first-order phase transition which we characterize by studying the density profiles across the simulation cell as well as the order parameter distribution function. For small values of P , however, no such ordered state exists. Instead, the system exhibits a glassy dynamics characterized by slow relaxation timescales. Finally, we show how our results can be used to explain recent experiments on colloidal suspensions under shear flow. In many physical systems, it has been observed that particles tend to align themselves into regular patterns when they interact strongly enough. This phenomenon is known as capillarity  1  or self-assembly  2  .\nIn particular, in two dimensions, particles may form stripes  3  -  5  , squares  6  , hexagons  7  , or even more complex structures  8  depending on their shape  9  , size  10  , interactions  11  , and external fields  12  . These phenomena have attracted considerable attention over the past few years due to both fundamental interest  13  and potential applications  14  -  16  . A particularly interesting example is provided by colloidal suspensions  17  -  20  : When these are subjected to strong shear flows  21  , they often exhibit striped phases  22  -  24  whose formation mechanism remains poorly understood  25  .",
        "watermark_text": "We explore the phase response of a system of N identical hard rods confined to a square box with periodic border conditions , using Monte Carlo simulations at steady pressure P . We see that for enough large values of P , there is an ordered state where all atoms are aligned along one orientation ( the x - axis ) , forming sheets parallel to this axis .The shift between disordered and ordered states happens via a first - order phase shift which we characterize by examining the density patterns across the model cell as well as the order parameter distribution relation . For small values of P , however , no such ordered state exists .Instead , the system displays a glassy dynamics defined by small relaxation timescales . Finally , we tell how our findings can be used to explain latest studies on colloidal suspensions under shear flow .In many physical structures , it has been observed that particles tend to align themselves into normal patterns when they interact strongly sufficiently . This phenomenon is known as capillarity 1 or self - assembly 2 .In particular , in two dimensions , particles may form colors 3 - 5 , circles 6 , hexagons 7 , or especially more complex shapes 8 depending on their shape 9 , size 10 , interactions 11 , and external fields 12 . These phenomena have garnered considerable focus over the previous few years owing to both basic attention 13 and possible use 14 - 16 .A notably important example is provided by colloidal suspensions 17 - 20 : When these are subjected to powerful shear flows 21 , they frequently exhibit striped cycles 22 - 24 whose formation system stays little explained 25 .",
        "rewrite_text": "We investigate the phase response of a system composed of N identical hard rods confined within a square box featuring periodic boundary conditions, utilizing Monte Carlo simulations under steady pressure P. Our results indicate that at sufficiently high pressure values, the system transitions into an ordered state where all atoms align along a single orientation (the x-axis), resulting in sheets that are parallel to this axis. The transition from a disordered to an ordered state occurs through a first-order phase shift, which we analyze by observing the density patterns within the model cell and the relationship of the order parameter distribution. In contrast, at low pressure values, no ordered state is present; instead, the system exhibits glassy dynamics characterized by short relaxation timescales. Furthermore, we discuss how our findings contribute to the understanding of recent studies on colloidal suspensions subjected to shear flow. Observations in various physical structures have demonstrated that particles often arrange themselves into regular patterns when they exert sufficiently strong interactions. This phenomenon, referred to as capillarity or self-assembly, manifests in two dimensions where particles can form various shapes such as colors, circles, hexagons, or more complex configurations, depending on factors like their shape, size, interactions, and external fields. These phenomena have attracted significant attention in recent years, driven by both fundamental interest and potential applications. A particularly noteworthy example is seen in colloidal suspensions; under strong shear flows, these systems often develop striped patterns, the formation mechanism of which remains inadequately explained.",
        "ori-fast-z-score": -1.863448669773839,
        "water-fast-z-score": 6.894291116568838,
        "rewrite-fast-z-score": 1.3719886811400706
    },
    {
        "original_text": "We present an analytical model to study decoherence and relaxation processes in a few coupled superconducting quantum interference devices (SQUID) immersed into a phonon bath at low temperatures. The system is described by a chain-boson Hamiltonian with nearest-neighbor interactions, which can be diagonalized exactly using the Bethe ansatz method. We show that this approach allows us to obtain exact results for the dynamics of the reduced density matrix describing the SQUID subsystem as well as its entanglement entropy. In particular we find that the decay rate of the off-diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy grows logarithmically with time t. These findings are consistent with previous numerical studies on similar systems. \n \n Introduction \n \n Superconducting circuits have been proposed recently as promising candidates for realizations of quantum information processing  1  . One important issue in these proposals concerns how to protect qubits against environmental noise  2  . It has been shown theoretically  3  -  6  and experimentally  7  -  9  that coupling between different parts of a circuit may lead to unwanted effects such as dephasing or relaxation. This problem becomes particularly severe when considering large networks of interacting qubits  10  . \n \n Here we consider a simple model consisting of two weakly-coupled SQUIDs  11  immersed into a phonon environment  12  . Our aim is to investigate the effect of the interaction term on the evolution of the reduced density matrix of each SQUID separately. To do so, we use the Bethe ansatz  13  to solve analytically the Schrödinger equation corresponding to our model. As expected, we observe that the presence of the interaction leads to decoherence and dissipation phenomena. Moreover, we find that the decay rates of the off-diagonals of the reduced density matrices scale linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t. \nModel\n\nThe total Hamiltonian H = H0 + V describes the system composed of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature.",
        "watermark_text": "We present an analytical theory to study decoherence and relaxation processes in a few coupled superconducting quantum interference machines ( SQUID ) immersed into a phonon bath at low temperatures . The system is characterized by a chain - boson Hamiltonian with nearest - neighbor interactions , which can be diagonalized precisely using the Bethe ansatz technique .We see that this methodology allows us to obtain exact findings for the dynamics of the reduced density matrix describing the SQUID subsystem as also as its entanglement entropy . In particular we find that the decay rate of the off - diagonal elements of the reduced density matrix scales linearly with temperature T , while the von Neumann entropy increases logarithmically with time t . These conclusions are compatible with previous quantitative studies on similar systems .Introduction Superconducting circuits have been proposed lately as hopeful candidates for realizations of quantum information processing 1 . One important concern in these proposals issues how to shield qubits against environmental interference 2 .It has been shown theoretically 3 - 6 and experimentally 7 - 9 that coupling between multiple parts of a circuit could lead to unexpected problems such as dephasing or relaxation . This problem arises particularly severe when examining massive networks of interacting qubits 10 .Here we study a simple study composed of two weakly - coupled SQUIDs 11 immersed into a phonon system 12 . Our aim is to examine the impact of the interaction term on the evolution of the reduced density matrix of each SQUID separately .To do so , we utilize the Bethe ansatz 13 to correct analytically the Schrödinger equation relating to our model . As expected , we determine that the presence of the interaction results to decoherence and dissipation phenomena .Moreover , we find that the decay rates of the off - diagonals of the reduced density matrices increase linearly with temperature T , whereas their von Neumann entropies grow logarithmically with time t . Model The total Hamiltonian H = H0 + V describes the scheme consists of N = 2 SQUIDs coupled via a weak tunneling amplitude J immersed into a phonon reservoir at zero temperature .",
        "rewrite_text": "We develop an analytical framework to investigate decoherence and relaxation processes in a pair of coupled superconducting quantum interference devices (SQUIDs) immersed in a low-temperature phonon bath. The system is described by a chain-boson Hamiltonian that incorporates nearest-neighbor interactions, and we apply the Bethe ansatz technique for precise diagonalization. This approach enables us to derive exact results for the dynamics of the reduced density matrix corresponding to the SQUID subsystem, along with its entanglement entropy. Notably, we find that the decay rate of the off-diagonal elements of the reduced density matrix has a linear dependence on temperature \\( T \\), while the von Neumann entropy increases logarithmically with time \\( t \\). These results align with prior quantitative analyses of similar systems. \n\nSuperconducting circuits are currently regarded as promising candidates for quantum information processing applications. A critical issue in these proposals is the protection of qubits from environmental interference. Theoretical and experimental studies have demonstrated that coupling between different components of a circuit can lead to challenges such as dephasing and relaxation, particularly in extensive networks of interacting qubits. In this paper, we focus on a simplified model comprising two weakly coupled SQUIDs embedded in a phonon environment. Our goal is to investigate how the interaction term affects the evolution of each SQUID's reduced density matrix. To achieve this, we analytically solve the Schrödinger equation associated with our model using the Bethe ansatz. Our findings indicate that interactions lead to decoherence and dissipation phenomena. Furthermore, we observe that the decay rates of the off-diagonal components of the reduced density matrices increase linearly with temperature \\( T \\), while their von Neumann entropies grow logarithmically with time \\( t \\). \n\nThe model is defined by the total Hamiltonian \\( H = H_0 + V \\), where \\( H_0 \\) represents the individual components of the two SQUIDs, coupled by a weak tunneling amplitude \\( J \\), and \\( V \\) accounts for the interaction with a phonon reservoir at zero temperature.",
        "ori-fast-z-score": 0.5980503604017327,
        "water-fast-z-score": 7.379243688215747,
        "rewrite-fast-z-score": 1.9650226127485502
    },
    {
        "original_text": "We present new observations of the nuclear region in the nearby radio galaxy NGC315, made using the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy that is surrounded by diffuse emission extending to about 1 arcmin (3 kpc), which we identify as thermal gas heated by the central AGN. We detect two bright knots embedded within the extended emission; these are likely associated with shocks driven into the surrounding medium by the expanding radio jets. Using high-resolution VLA images obtained simultaneously with the CXO observation, we find evidence for a one-sided parsec-scale radio jet emerging from the nucleus along position angle PA = -45 degrees. This jet has been previously detected on larger scales out to several kiloparsecs. In addition, there appears to be another fainter component of the radio jet located further south-west than the main knot.",
        "watermark_text": "We present new images of the atomic region in the nearby radio star NGC315 , made using the Chandra X - ray Observatory ( CXO ) and the Very Large Array ( VLA ) . The CXO data reveal an unresolved point origin at the center of this elliptical galaxy that is flanked by diffuse emission stretching to about 1 arcmin ( 3 kpc ) , which we identify as heat gas cooled by the main AGN .We detect two faint knots embedded within the extended emission ; these are likely correlated with shocks driven into the nearby medium by the increasing radio jets . Using long - resolution VLA images obtained simultaneously with the CXO observation , we find proof for a one - sided parsec - scale radio jet developing from the nucleus along position angle PA = - 45 degrees .This jet has been previously observed on larger scales out to several kiloparsecs . In addition , there seems to be another fainter component of the radio jet located further west - west than the main knot .",
        "rewrite_text": "We present new images of the atomic region in the nearby radio star NGC 315, captured using data from the Chandra X-ray Observatory (CXO) and the Very Large Array (VLA). The CXO data reveal an unresolved point source at the center of this elliptical galaxy, surrounded by diffuse emission extending approximately 1 arcminute (3 kpc), which we identify as heated gas that has cooled due to the influence of the primary active galactic nucleus (AGN). We also detect two faint knots within the extended emission, likely correlated with shocks created by the expanding radio jets. Utilizing high-resolution VLA images obtained concurrently with the CXO observation, we provide evidence for a one-sided, parsec-scale radio jet emerging from the nucleus at a position angle of PA = -45 degrees. This jet has been previously observed on much larger scales, extending several kiloparsecs. Additionally, we note the presence of a fainter component of the radio jet located further to the west of the main knot.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 4.391092135317257,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "We present an improved semi-analytical method (SAM) that includes gravitational heating by dark matter halos and gas cooling in galaxy formation, which is essential to reproduce observed properties of galaxies such as luminosity functions at different redshifts.  We show that our SAM can successfully explain the evolution of the stellar mass function over cosmic time with reasonable parameters. In addition, we find that the inclusion of gravitational heating leads to more realistic predictions on the star formation rate density history than previous models without this effect. Finally, we discuss how the model could be further improved by including other physical processes like supernova feedback or AGN activity. The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature. Red circles represent the predicted number densities using our new SAM code while blue squares indicate those obtained with the original SAM code developed by Nagashima & Yoshii (2004) .",
        "watermark_text": "We introduce an excellent semi - modelling techniques ( SAM ) that encompasses gravitational heating by black material halos and gas warming in universe formation , which is crucial to reproduce observed properties of stars such as luminosity functions at different redshifts . We suggest that our SAM can effectively predict the evolution of the stellar mass function over cosmic time with satisfactory parameters .In addition , we find that the introduction of gravitational heating results to more realistic predictions on the star formation rate density history than prior models without this effect . Finally , we explain how the model could be further better by including other physical processes like supernova feedback or AGN activity .The results presented here are based on observations made with ESO Telescopes at Paranal Observatory under programme ID 085 . A - 0488 ( A ) . This project was supported by JSPS KAKENHI Grant Number JP15K05481 .Figure 1 . Predicted number densities of galaxies as a function of their total stellar masses compared with observational data taken from the literature .Red rings represent the expected number densities using our new SAM code while blue squares correspond those acquired with the previous SAM code developed by Nagashima & Yoshii ( 2004 ) .",
        "rewrite_text": "We present a novel semi-analytical modeling technique (SAM) that incorporates gravitational heating from dark matter halos and gas warming during the formation of the universe. This approach is essential for accurately reproducing the observed properties of stars, including luminosity functions at various redshifts. Our SAM effectively predicts the evolution of the stellar mass function over cosmic time with satisfactory parameters. Furthermore, we demonstrate that adding gravitational heating leads to more accurate predictions of the star formation rate density history compared to earlier models that did not account for this effect. We also discuss how incorporating additional physical processes, such as supernova feedback and AGN activity, could enhance the model further. The results presented here are based on observations obtained with ESO Telescopes at the Paranal Observatory under program ID 085.A-0488(A). This work was supported by JSPS KAKENHI Grant Number JP15K05481. Figure 1 illustrates the predicted number densities of galaxies as a function of their total stellar masses, compared with observational data from the literature. The red rings indicate the expected number densities using our new SAM code, while the blue squares represent those derived from the previous SAM code developed by Nagashima & Yoshii (2004).",
        "ori-fast-z-score": -0.21081851067789195,
        "water-fast-z-score": 5.346252667281783,
        "rewrite-fast-z-score": 0.30779350562554625
    },
    {
        "original_text": "We study the evaporation process of black holes (BHs) in an expanding universe by using the tunneling method and the WKB approximation. We find that, for large BH masses M ≫ Mc2 = 3 × 10^8M⊙, where Mc is the critical mass at which the Hawking temperature vanishes, the lifetime of the BH decreases with increasing M as t ~ M-1/2. For small BH masses M < Mc2 , we show that the lifetime increases exponentially with decreasing M . The results are compared to those obtained within the framework of quantum field theory on curved space-time. It turns out that our predictions agree well with these results when one takes into account the effect of back reaction due to particle creation during the evaporation process. \nPACS numbers: 04.20.-q; 98.80.Cq \nI. INTRODUCTORY REMARK\nThe discovery of Hawking radiation  1  has led to renewed interest in the problem of black hole (BH) evaporation  2  -  4  . In this work, we will use the tunneling method  5  -  8  to calculate the decay rate of massive BHs in an expanding universe  9  .\nII. BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE\n\nA. Tunneling Method\nIn order to apply the tunneling method to the case of evaporating BHs, it is convenient to introduce new coordinates (t′, r′), related to the original ones (t,r) through the following transformations  10  :",
        "watermark_text": "We explore the evaporation process of brown holes ( BHs ) in an evolving galaxy by using the tunneling procedure and the WKB approximation . We see that , for large BH masses M [UNK] Mc2 = 3 × 10 ^ [UNK] , where Mc is the critical mass at which the Hawking temperature vanishes , the life of the BH decreases with expanding M as t ~ M - 1 / 2 .For small BH masses M < Mc2 , we find that the life grows exponentially with varying M . The results are compared to those achieved within the framework of quantum field theory on curved space - time .It turns out that our predictions agree well with these results when one takes into consideration the impact of back response due to particle creation during the evaporation process . PACS codes : 04 . 20 . - q ; 98 . 80 . Cq I .INTRODUCTORY REMARK The observation of Hawking radiation 1 has led to renewed concern in the issue of grey hole ( BH ) evaporation 2 - 4 . In this research , we will use the tunneling method 5 - 8 to estimate the decay rate of large BHs in an evolving galaxy 9 .II . BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE A . Tunneling Method In order to apply the tunneling procedure to the case of evaporating BHs , it is convenient to introduce different coordinates ( t ′ , r ′ ) , related to the previous ones ( t , r ) through the following transformations 10 :",
        "rewrite_text": "We investigate the evaporation of black holes (BHs) within an evolving galaxy using the tunneling method and the WKB approximation. Our findings indicate that for large BH masses \\( M > M_c^2 = 3 \\times 10^{...} \\), where \\( M_c \\) is the critical mass at which the Hawking temperature approaches zero, the lifespan of the BH decreases with increasing mass, following the relationship \\( t \\sim M^{-1/2} \\). Conversely, for small BH masses \\( M < M_c^2 \\), we observe that the lifespan increases exponentially with changes in mass. We compare our results with predictions derived from quantum field theory in curved spacetime and find a strong agreement when accounting for the effects of back-reaction from particle creation during the evaporation process. \n\n**PACS codes**: 04.20.-q; 98.80.Cq \n\n**I. INTRODUCTION**  \nThe discovery of Hawking radiation has reignited interest in the phenomenon of black hole evaporation. In this study, we utilize the tunneling method to estimate the decay rate of large black holes in an expanding universe. \n\n**II. BLACK HOLE EVAPORATION IN AN EXPANDING UNIVERSE**  \n**A. Tunneling Method**  \nTo apply the tunneling method to the evaporation of black holes, we introduce a new set of coordinates \\( (t', r') \\), which are related to the original coordinates \\( (t, r) \\) through specific transformations.",
        "ori-fast-z-score": 0.6546536707079772,
        "water-fast-z-score": 6.546536707079771,
        "rewrite-fast-z-score": -0.10976425998969035
    },
    {
        "original_text": "We present the results for the one-loop amplitudes in N=8 supergravity, which are obtained by using the spinor helicity formalism with dimensional reduction to four dimensions.  We show that our result agrees with the previous calculation based on Feynman diagrams at high energy limit. In addition we find that the amplitude vanishes when all external particles have negative helicities. This is consistent with the fact that there exists no graviton state with negative helicity in the massless spectrum of N=8 supergravity. The vanishing property also holds true even if some gravitons carry positive helicities but others do not. \nThe organization of this article is as follows. In section 2, we briefly review the basic properties of N=8 supergravity theory. In section 3, we explain how to calculate the scattering amplitudes in N=8 supergravitvity. In section 4, we give explicit expressions for the tree-level amplitudes. In sections 5-8, we present the results for the 1-loop amplitudes. Section 9 contains discussions about the physical meaning of these results.",
        "watermark_text": "We present the results for the one - loop amplitudes in N = 8 supergravity , which are derived by using the spinor helicity formalism with dimensional reduction to four dimensions . We see that our conclusion agrees with the previous measurement based on Feynman diagrams at high energy limit .In addition we find that the amplitude vanishes when all external particles have negative helicities . This is compatible with the fact that there exists no graviton state with negative helicity in the massless spectrum of N = 8 supergravity .The vanishing property also stands true even if some gravitons carry positive helicities but others do not . The organization of this page is as follows .In section 2 , we briefly investigate the fundamental properties of N = 8 supergravity physics . In section 3 , we explain how to estimate the scattering amplitudes in N = 8 supergravitvity .In section 4 , we give explicit expressions for the tree - level amplitudes . In sections 5 - 8 , we present the results for the 1 - loop amplitudes .Section 9 includes talks about the physical context of these results .",
        "rewrite_text": "We report the results of one-loop amplitudes in N = 8 supergravity, derived using the spinor helicity formalism along with dimensional reduction to four dimensions. Our findings are consistent with previous calculations based on Feynman diagrams in the high-energy limit. Notably, we observe that the amplitude disappears when all external particles exhibit negative helicities. This observation aligns with the fact that there is no graviton state with negative helicity in the massless spectrum of N = 8 supergravity. Furthermore, the vanishing property holds even if some gravitons possess positive helicities while others do not. The structure of this document is as follows: Section 2 provides a brief overview of the fundamental properties of N = 8 supergravity physics. In Section 3, we detail the estimation of scattering amplitudes within N = 8 supergravity. Section 4 presents explicit expressions for the tree-level amplitudes. Sections 5 through 8 offer the results for the one-loop amplitudes, while Section 9 discusses the physical implications of these findings.",
        "ori-fast-z-score": 0.6201736729460423,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": 0.2581988897471611
    },
    {
        "original_text": "We present new photometry in the optical and near-infrared bands for the open cluster Blanco 1 (M35). The data were obtained with the 2 m telescope at the Observatorio Astronómico Nacional de San Pedro Mártir, Baja California Sur, Mexico; the 0.9 m telescope at the Cerro Tololo Inter-American Observatory, Chile; and the Nordic Optical Telescope, La Palma, Canary Islands, Spain. We have used these observations together with archival data from the Hubble Space Telescope archive to derive an improved age estimate for this open cluster. Our results show that M35 is about 300 Myr old, which makes it one of the youngest open clusters known within 100 pc of the Sun. Using our derived age we have determined the initial mass function (IMF) of M35 down to masses as low as 3 MJUP . This IMF shows no evidence for any significant deviation from the Salpeter power law index over more than three orders of magnitude in mass.",
        "watermark_text": "We report new photometry in the optical and far - infrared bands for the open cluster Blanco 1 ( M35 ) . The data were obtained with the 2 m observatory at the Observatorio Astronómico Nacional de San Pedro Mártir , Baja California Sur , Mexico ; the 0 . 9 m observatory at the Cerro Tololo Inter - American Observatory , Chile ; and the Nordic Optical Telescope , La Palma , Canary Islands , Spain .We have utilized these observations together with archival data from the Hubble Space Telescope archive to derive an better age forecast for this open cluster . Our results show that M35 is about 300 Myr aged , which makes it one of the youngest open complexes discovered within 100 pc of the Sun .Using our derived age we have decided the initial mass function ( IMF ) of M35 down to masses as low as 3 MJUP . This IMF indicates no evidence for any large deviation from the Salpeter power law index over more than three orders of magnitude in mass .",
        "rewrite_text": "We present new photometric data in the optical and far-infrared bands for the open cluster Blanco 1 (M35). The observations were conducted at three locations: the 2 m telescope at the Observatorio Astronómico Nacional de San Pedro Mártir in Baja California Sur, Mexico; the 0.9 m telescope at the Cerro Tololo Inter-American Observatory in Chile; and the Nordic Optical Telescope in La Palma, Canary Islands, Spain. By integrating these findings with archival data from the Hubble Space Telescope, we have refined the age estimate for this open cluster. Our analysis indicates that M35 is approximately 300 million years old, making it one of the youngest open clusters identified within 100 parsecs of the Sun. Using this age estimate, we have determined the initial mass function (IMF) of M35 down to masses as small as 3 MJUP. Our findings suggest that there is no significant deviation from the Salpeter power law index across more than three orders of magnitude in mass.",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 3.959797974644666,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present the results for dark matter annihilations into gamma rays and neutrinos using an improved treatment of subhalos within galaxy clusters, including their internal structure as well as tidal stripping effects on their outer parts. We find that this leads to a significant increase (by up to one order of magnitude) in the predicted fluxes at energies above 1 GeV compared with previous studies. The effect is particularly strong when considering nearby galaxy clusters such as Virgo or Coma. This has important consequences for current and future experiments searching for signals from dark matter particles. In particular, we show how our predictions can be used to derive constraints on the properties of dark matter candidates by comparing them with existing data from Fermi/LAT and IceCube/DeepCore. Introduction: Dark matter (DM), if it exists, may interact weakly with ordinary matter through its self-annihilation products  1  . If DM consists of new light particles, then these interactions would produce detectable signatures in cosmic ray spectra  2  , gamma-ray emission  3  , and high-energy neutrino production  4  .\nIn recent years there have been many attempts to detect DM indirectly via observations of astrophysical objects which are expected to contain large amounts of DM  5  . These include dwarf galaxies  6  , galaxy clusters  7, 8  , and galactic haloes  9  . However, no convincing evidence for DM annihilation has yet been found  10  . One possible explanation for this lack of detection could be that most of the DM mass resides in small-scale structures  11  , which are not resolved observationally  12  . Another possibility is that the DM density profiles inferred from gravitational lensing measurements  13  do not accurately reflect the true distribution of DM  14  . Finally, it should also be noted that some models predict very low rates of DM annihilation  15  .\nThe aim of this work is to investigate whether the inclusion of substructure information improves the prospects for detecting DM annihilation products. To achieve this goal, we use high-resolution N-body simulations  16  to study the impact of subhalo populations on the resulting gamma-ray  17  and neutrino  18  fluxes produced by",
        "watermark_text": "We present the results for black material annihilations into beta rays and neutrinos using an better treatment of subhalos within star clusters , notably their internal structure as well as tidal stripping impacts on their exterior portions . We see that this contributes to a substantial rise ( by up to one order of magnitude ) in the expected fluxes at energies above 1 GeV compared with previous research .The impact is especially powerful when examining nearby galaxy galaxies such as Virgo or Coma . This has significant implications for recent and future research exploring for messages from dark matter particles .In particular , we show how our predictions can be used to derive restrictions on the properties of dark matter candidates by using them with existing information from Fermi / LAT and IceCube / DeepCore . Introduction : Dark matter ( DM ) , if it exists , might interact weakly with everyday matter through its self - annihilation products 1 .If DM consists of new light particles , then these interactions would create detectable signatures in cosmic ray spectra 2 , gamma - ray radiation 3 , and large - energy neutrino production 4 . In recent history there have been many efforts to observe DM indirectly via surveys of astrophysical objects which are expected to contain significant amounts of DM 5 .These include dwarf stars 6 , galaxy regions 7 , 8 , and galactic haloes 9 . However , no convincing evidence for DM annihilation has yet been detected 10 .One potential explanation for this lack of study could be that most of the DM mass resides in small - scale structures 11 , which are not resolved observationally 12 . Another possibility is that the DM density patterns inferred from gravitational lensing observations 13 do not correctly reflect the true distribution of DM 14 .Finally , it should additionally be mentioned that some models predict very low rates of DM annihilation 15 . The goal of this research is to examine whether the integration of substructure knowledge improves the possibilities for detecting DM annihilation products .To achieve this goal , we using high - resolution N - bodies simulations 16 to study the impact of subhalo environments on the resulting γ - ray 17 and neutrino 18 fluxes produced by",
        "rewrite_text": "We present findings on the annihilation of dark matter into beta rays and neutrinos, employing an enhanced treatment of subhalos within star clusters, particularly focusing on their internal structures and the effects of tidal stripping on their outer regions. Our analysis reveals a significant increase—up to an order of magnitude—in the anticipated fluxes at energies exceeding 1 GeV compared to previous studies. This effect is particularly pronounced when observing nearby galaxy clusters such as Virgo and Coma, which carries important implications for ongoing and future investigations aimed at detecting signals from dark matter particles. Specifically, we demonstrate how our predictions can impose constraints on the characteristics of potential dark matter candidates, integrating our results with extant data from Fermi/LAT and IceCube/DeepCore.\n\nIntroduction: If dark matter (DM) exists, it may interact weakly with ordinary matter through its annihilation products. If DM consists of new light particles, such interactions could produce detectable signatures in cosmic ray spectra, gamma-ray emissions, and high-energy neutrinos. Numerous attempts have been made to indirectly observe DM through surveys of astrophysical objects that are anticipated to harbor substantial DM, including dwarf galaxies, regions within galaxies, and galactic halos. Yet, no convincing evidence of DM annihilation has been found so far. One possible reason for this absence of detection is that a significant portion of DM is concentrated in small-scale structures that elude observational resolution. Another consideration is that the DM density profiles inferred from gravitational lensing studies may not accurately depict the true distribution of DM. Additionally, some models suggest very low rates of DM annihilation. This research aims to investigate whether incorporating knowledge of substructure enhances the prospects for detecting DM annihilation products. To this end, we utilize high-resolution N-body simulations to analyze the influence of subhalo environments on the resulting gamma-ray and neutrino fluxes produced by dark matter annihilations.",
        "ori-fast-z-score": -1.7728105208558367,
        "water-fast-z-score": 7.897065047448726,
        "rewrite-fast-z-score": 0.9733285267845753
    },
    {
        "original_text": "We study mesoscopic fluctuations of the supercurrents flowing through two weakly coupled superconductors with different transparencies and temperatures, using the Usadel equations for quasiclassical Green s functions. We show that the current noise is suppressed by increasing transparency between the leads or decreasing temperature. The suppression can be explained as due to an increase of the effective junction length caused by Andreev reflection at the interface. In addition we find that the shot-noise power decreases when the phase difference across the junction increases. This effect originates from the dependence of the density of states on the phase difference. Finally, we discuss how our results are related to recent experiments performed on diffusive SNS junctions. \nI. INTRODUCTORY REMARK\nThe Josephson effect describes macroscopically coherent transport of Cooper pairs across weak links connecting two superconducting electrodes  1  . It has been observed experimentally over many decades  2  , but only recently have researchers begun to explore its microscopic origins  3  .\nIn this work we consider a system consisting of two weakly-coupled superconductors (S) connected via a normal metal region (N). Such systems are known as diffusive SNS junctures  4  . They exhibit interesting phenomena such as the proximity effect  5  , which causes the formation of a minigap inside the N region  6  . Another important feature of these devices is their ability to carry both charge and spin currents  7, 8  . These properties make them promising candidates for applications ranging from quantum information processing  9  to magnetic field sensing  10  .\nRecently there has been renewed interest in studying the physics of diffusive SNS juncture  11  -  16  . For example, it was shown theoretically that the critical current I c depends strongly on the transparency T = R Q /R N of the NS interfaces  17  where R Q and R N are the resistance quantum and the resistance of the N region respectively. Experimentally, this prediction could not yet be confirmed because of difficulties associated with fabricating clean NS interfaces  18  . However, several groups managed to observe similar effects indirectly  19, 20  .",
        "watermark_text": "We test mesoscopic fluctuations of the supercurrents rushing through two tightly correlated superconductors with varying transparencies and temperatures , using the Usadel equations for quasiclassical Green s functions . We see that the current noise is suppressed by expanding transparency between the leads or decreasing temperature .The suppression can be understood as owing to an increase of the effective junction size caused by Andreev reflection at the interface . In addition we find that the shot - noise power decreases when the phase change across the junction increases .This phenomenon originates from the dependence of the density of states on the phase change . Finally , we talk how our findings are related to recent experiments conducted on diffusive SNS junctions .I . INTRODUCTORY REMARK The Josephson effect explains macroscopically consistent transport of Cooper pairs across weak links connecting two superconducting electrodes 1 .It has been observed experimentally over numerous years 2 , but only lately have researchers begun to examine its microscopic origins 3 . In this research we study a system consisting of two weakly - coupled superconductors ( S ) connected via a normal metal area ( N ) .Such structures are known as diffusive SNS junctures 4 . They show exciting phenomena such as the contact influence 5 , which forms the formation of a minigap inside the N region 6 .Another important feature of these systems is their potential to carry both charge and spin currents 7 , 8 . These properties make them promising candidates for applications extending from molecular data processing 9 to magnetic field monitoring 10 .Recently there has been continued interest in investigating the physics of diffusive SNS juncture 11 - 16 . For instance , it was shown theoretically that the critical potential I c depends strongly on the transparency T = R Q / R N of the NS interfaces 17 where R Q and R N are the tolerance quantum and the resistance of the N region respectively .Experimentally , this measurement came not already be verified because of troubles associated with fabricating safe NS interfaces 18 . However , various groups helped to observe comparable effects indirectly 19 , 20 .",
        "rewrite_text": "We investigate the mesoscopic fluctuations of supercurrents flowing through two closely correlated superconductors with varying transparencies and temperatures, utilizing the Usadel equations for quasiclassical Green’s functions. Our results indicate that the current noise is diminished by either increasing the transparency between the leads or lowering the temperature. This reduction can be attributed to an enlargement of the effective junction size, which occurs due to Andreev reflection at the interface. Additionally, we observe that the shot noise power decreases as the phase difference across the junction increases, a phenomenon that is linked to the phase-dependent density of states. We also discuss how our findings relate to recent experiments on diffusive SNS junctions.\n\n**I. Introductory Remarks**  \nThe Josephson effect elucidates the macroscopically coherent transport of Cooper pairs across weak links that connect two superconducting electrodes. This effect has been experimentally demonstrated over many years, but only recently have researchers started to delve into its microscopic underpinnings. In this study, we focus on a system comprised of two weakly coupled superconductors (S) connected by a normal metal region (N), commonly referred to as diffusive SNS junctions. These systems exhibit intriguing phenomena, such as contact influences that lead to the formation of a minigap in the N region. Another significant characteristic of these junctions is their ability to facilitate the flow of both charge and spin currents, making them promising candidates for applications ranging from molecular data processing to magnetic field detection. Recently, there has been a growing interest in exploring the physics of diffusive SNS junctions. For example, theoretical work has shown that the critical current I_c significantly depends on the transparency T = R_Q / R_N of the NS interfaces, where R_Q is the quantum resistance and R_N is the resistance of the N region. However, experimental verification of these theoretical predictions has faced challenges due to difficulties in fabricating reliable NS interfaces. Nevertheless, multiple research groups have managed to observe related effects indirectly.\n",
        "ori-fast-z-score": -0.5895063447465633,
        "water-fast-z-score": 8.446044301071616,
        "rewrite-fast-z-score": 1.3522468075656264
    },
    {
        "original_text": "We present an alternative formulation for the Kuramoto model that is based on a linearization of the original nonlinear system and leads to a more efficient numerical solution method than existing approaches.  The new approach can be used in combination with any standard solver, such as Newton s or fixed-point iteration methods. We demonstrate its performance by applying it to several examples including networks of coupled phase oscillators and chaotic systems. Synchronized behavior has been observed across many different fields ranging from physics  1  , biology  2  , chemistry  3  , engineering  4  , and social sciences  5  . In particular, synchronization phenomena are often studied using models of interacting dynamical systems  6  .\nThe most commonly used mathematical description of synchronized dynamics is given by the Kuramoto model  7, 8  which describes how N identical oscillators evolve over time t according to: \nwhere θi(t) ∈  0, 2π  denotes the phase angle of oscillator i at time t, ωi > 0 represents the natural frequency of each individual oscillator, and Kij ≥ 0 quantifies the strength of interaction between oscillators i and j. For simplicity we assume here that all interactions have equal weight (Kij = 1). This assumption does not affect our results but simplifies notation considerably.",
        "watermark_text": "We introduce an additional formulation for the Kuramoto model that is based on a linearization of the previous nonlinear system and results to a more efficient numerical solving approach than existing techniques . The modern formulation can be used in combination with any typical solver , such as Newton s or fixed - point iteration algorithms .We test its effectiveness by application it to several examples namely groups of coupled phase oscillators and chaotic networks . Synchronized structure has been observed across many various fields ranging from science 1 , chemistry 2 , chemistry 3 , engineering 4 , and social sciences 5 .In particular , synchronization processes are often investigated using models of interacting dynamical systems 6 . The most commonly used numerical model of synchronized mechanics is given by the Kuramoto model 7 , 8 which explains how N identical oscillators evolve over time t according to : where θi ( t ) ∈ 0 , 2π denotes the phase angle of oscillator i at time t , ωi > 0 represents the natural frequency of each individual oscillator , and Kij ≥ 0 quantifies the strength of coupling between oscillators i and j .For simplicity we suppose here that all interactions have equal strength ( Kij = 1 ) . This assumption does not alter our findings but simplifies notation significantly .",
        "rewrite_text": "We present a revised formulation of the Kuramoto model, which arises from a linearization of the previous nonlinear system, yielding a more efficient numerical solution method compared to existing approaches. This modern formulation can be integrated with conventional solvers, such as Newton's method or fixed-point iteration algorithms. To evaluate its effectiveness, we apply it to a variety of cases, including groups of coupled phase oscillators and chaotic networks. Synchronized structures have been observed across a wide range of fields, including science, chemistry, engineering, and social sciences. Synchronization processes are frequently studied through models of interacting dynamical systems. The Kuramoto model is the most widely used numerical representation for synchronized mechanics, describing how N identical oscillators evolve over time, where θ_i(t) ∈ [0, 2π] indicates the phase angle of oscillator i at time t, ω_i > 0 is the natural frequency of each oscillator, and K_ij ≥ 0 signifies the coupling strength between oscillators i and j. For simplicity, we assume that all interactions have equal strength (K_ij = 1), which does not affect our results but significantly simplifies the notation.",
        "ori-fast-z-score": 0.21320071635561041,
        "water-fast-z-score": 7.248824356090754,
        "rewrite-fast-z-score": 0.7777777777777778
    },
    {
        "original_text": "We present an analysis of the neutral hydrogen (HI) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100-m telescope to study the dark matter content of our Galaxy. We use the rotation curve derived by Clemens (1985) , which is based on 21-cm line observations of nearby spiral galaxies. The total mass enclosed within a radius R can be written as: M(R) = Vrot2πGRL + MDW(R), where Vrot is the circular velocity at galactocentric distance R, G is Newton s constant, L is the luminosity density, and MDW(R) is the contribution due to the dark matter halo. In this work we assume that the dark matter follows a Navarro-Frenk-White profile.  Using the rotation curve for the solar neighbourhood given by Clemens (1985) (V⊙ = 220 km/sec), we find that the best-fit parameters are L0 = 0.0013 Msun/pc3 and r0 = 1 kpc. This implies that the local surface brightness ΣL = L/L0 = 3.6 × 10^−26 W/m2/Hz/sr. For comparison, the average value found by Dickey & Lockman (1990)  is ΣL = 2×10^−25 W/m2/Hz/sr; their sample includes only high latitude regions outside the Galactic plane.",
        "watermark_text": "We present an analysis of the neutral hydrogen ( HI ) emission observed with the Westerbork Synthesis Radio Telescope and the Effelsberg 100 - m observatory to study the dark matter content of our Galaxy . We use the rotation curve obtained by Clemens ( 1985 ) , which is based on 21 - cm line surveys of distant spiral galaxies .The total mass surrounded within a diameter R can be written as : M ( R ) = Vrot2πGRL + MDW ( R ) , where Vrot is the spherical momentum at galactocentric distance R , G is Newton s constant , L is the luminosity density , and MDW ( R ) is the contribution owing to the dark matter halo . In this work we suppose that the dark matter follows a Navarro - Frenk - White model .Using the rotation curve for the solar neighbourhood given by Clemens ( 1985 ) ( [UNK] = 220 km / sec ) , we find that the best - fitting coefficients are L0 = 0 . 0013 Msun / pc3 and r0 = 1 kpc . This implies that the local surface brightness ΣL = L / L0 = 3 . 6 × 10 ^ −26 W / m2 / Hz / sr .For comparison , the average value found by Dickey & Lockman ( 1990 ) is ΣL = 2×10 ^ −25 W / m2 / Hz / sr ; their sample comprises only high elevation regions outside the Galactic jet .",
        "rewrite_text": "We provide an analysis of the neutral hydrogen (HI) emission detected by the Westerbork Synthesis Radio Telescope and the Effelsberg 100-m Radio Telescope to investigate the dark matter content of our Galaxy. Our study utilizes the rotation curve from Clemens (1985), derived from 21-cm line surveys of distant spiral galaxies. The total mass enclosed within a radius \\( R \\) can be expressed as \\( M(R) = \\frac{V_{\\text{rot}}^2}{2\\pi GR} + M_{\\text{DW}}(R) \\), where \\( V_{\\text{rot}} \\) is the rotational velocity at a galactocentric distance \\( R \\), \\( G \\) is Newton's gravitational constant, \\( L \\) represents the luminosity density, and \\( M_{\\text{DW}}(R) \\) accounts for the dark matter halo contribution. In our analysis, we assume that dark matter follows a Navarro-Frenk-White profile. Utilizing Clemens's (1985) rotation curve for the solar neighborhood, with \\( V_{\\text{rot}} = 220 \\, \\text{km/s} \\), we determine the best-fitting parameters to be \\( L_0 = 0.0013 \\, M_{\\odot}/\\text{pc}^3 \\) and \\( r_0 = 1 \\, \\text{kpc} \\). This yields a local surface brightness of \\( \\Sigma_L = \\frac{L}{L_0} = 3.6 \\times 10^{-26} \\, \\text{W/m}^2/\\text{Hz}/\\text{sr} \\). For comparison, Dickey & Lockman (1990) report an average value of \\( \\Sigma_L = 2 \\times 10^{-25} \\, \\text{W/m}^2/\\text{Hz}/\\text{sr} \\) based on their sample, which includes only high-elevation regions outside the Galactic plane.",
        "ori-fast-z-score": 0.7276068751089989,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 3.2349831961031525
    },
    {
        "original_text": "The focusing effect on relativistic electrons by an electrostatic lens is investigated theoretically for different values of the charge density, radius and potential difference between two spherical electrodes. The results are compared with those obtained using the so-called  effective momentum  approximation (EMA). It has been found that EMA gives good agreement only when the electron energy is much higher than its rest mass energy. For lower energies it overestimates the focal length as well as the maximum value of the electric field strength at the focus point. This discrepancy can be explained by taking into account the contribution to the total force acting upon the particle due to the magnetic component of the Lorentz force. In this case we have shown that the focal length decreases while the maximum electric field increases. \n \n We also show how the focal properties depend on the initial direction of motion of the incident beam. Finally, we discuss possible applications of such lenses in accelerator physics.",
        "watermark_text": "The focusing influence on relativistic electrons by an electrostatic lens is investigated theoretically for different values of the charge density , diameter and potential change between two spherical electrodes . The results are compared with those achieved using the so - called effective momentum approximation ( EMA ) .It has been shown that EMA gives excellent agreement only when the electron mass is much higher than its rest mass mass . For lower energies it overestimates the lens width as well as the maximum value of the electric field intensity at the focus position .This discrepancy can be described by take into consideration the contribution to the total stress acted upon the particle attributed to the magnetic element of the Lorentz force . In this situation we have shown that the lens width decreases while the maximum magnetic force increases .We also demonstrate how the lens qualities rely on the first direction of movement of the incident beam . Finally , we explain possible use of such lenses in accelerator physics .",
        "rewrite_text": "The theoretical investigation examines the focusing effect of an electrostatic lens on relativistic electrons, varying parameters such as charge density, diameter, and potential difference between two spherical electrodes. The findings are compared to those derived from the effective momentum approximation (EMA). It has been demonstrated that EMA yields excellent results only when the electron mass is significantly greater than its rest mass. At lower energies, however, it tends to overestimate both the lens width and the peak electric field intensity at the focus. This discrepancy can be explained by considering the contribution of the magnetic component of the Lorentz force to the total stress acting on the particle. In this scenario, our analysis shows that the lens width decreases while the maximum magnetic force increases. We also illustrate how the lens characteristics depend on the initial direction of the incident beam. Lastly, we discuss the potential applications of this type of lens in accelerator physics.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 1.0256451881367414
    },
    {
        "original_text": "We present the first results on infrared high-resolution spectroscopy (HRS) of post-AGB disks, obtained with CRIRES/VLT and NIRSPEC/Keck II. We find that the disk around HR 4049 is dominated by emission lines originating in an extended region at temperatures between 1000-2000 K. This temperature range corresponds to the expected location of dust grains which are being evaporated due to stellar radiation pressure. In addition we detect several absorption features which can be attributed to gas-phase molecules such as CO, H2O, OH, NH3, CH4, C2H2, SiO, HCN, CCH, CN, H2S, SO2, CS, and FeH. These observations provide new insights into the physical conditions within these objects. They also show how important it will be for future studies to combine spatially resolved information about the distribution of molecular species with detailed spectroscopic data. \n \n Keywords: circumstellar disk",
        "watermark_text": "We present the first findings on infrared high - resolution spectroscopy ( HRS ) of post - AGB disks , obtained with CRIRES / VLT and NIRSPEC / Keck II . We see that the disk around HR 4049 is dominated by absorption tracks originating in an extended region at temperatures between 1000 - 2000 K . This temperature spectrum refers to the expected location of dust grains which are being evaporated due to stellar radiation stress .In addition we locate many absorption properties which can be due to gas - phase molecules such as CO , H2O , OH , NH3 , CH4 , C2H2 , SiO , HCN , CCH , CN , H2S , SO2 , CS , and FeH . These measurements give novel knowledge into the physical conditions within these objects .They also demonstrate how important it will be for future research to mix spatially resolved data about the distribution of molecular species with comprehensive spectroscopic data . Keywords : circumstellar disk",
        "rewrite_text": "We present our groundbreaking findings on infrared high-resolution spectroscopy (HRS) of post-AGB disks, obtained using CRIRES at the VLT and NIRSPEC at Keck II. Our observations reveal that the disk surrounding HR 4049 is primarily characterized by absorption features resulting from an extended region with temperatures ranging from 1000 to 2000 K. This temperature profile corresponds to the expected location of dust grains that are evaporating under the influence of stellar radiation. Furthermore, we identify numerous absorption characteristics that may be attributed to gas-phase molecules such as CO, H2O, OH, NH3, CH4, C2H2, SiO, HCN, CCH, CN, H2S, SO2, CS, and FeH. These measurements provide new insights into the physical conditions within these objects and highlight the necessity for future studies to integrate spatially resolved data on the distribution of molecular species with detailed spectroscopic information. Keywords: circumstellar disk",
        "ori-fast-z-score": -0.5252257314388902,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.8962581595302719
    },
    {
        "original_text": "We study the global structure and causal properties of (2 + 1)-dimensional Einstein spaces with nonnegative cosmological constant, which are locally conformally flat but not globally conformal to Minkowski space-time.  We show that such spaces can be classified into three types according to their global structures as follows:  Type I : The universal covering is homeomorphic to R × S 2 . Type II : The universal covering has infinitely many ends. Type III : The universal covering is compact. In particular,  we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine parameter along them.   As applications,   we give examples of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic plane H 2 , the complex projective line CP1 , and the 3-sphere S3 respectively.    Keywords: Global structure, Causality, Closed timelike curve",
        "watermark_text": "We research the global structure and causal properties of ( 2 + 1 ) - dimensional Einstein spaces with nonnegative cosmological constant , which are locally conformally flat but not nationally conformal to Minkowski space - time . We see that such spaces can be categorized into three sorts according to their global structures as follows : Type I : The universal covering is homeomorphic to R × S 2 .Type II : The universal covering has infinitely many ends . Type III : The universal covering is compact .In particular , we prove that any spacetime in Types I or II admits closed timelike curves if it contains an incomplete null geodesic segment whose endpoints have different values of the affine vector along them . As applications , we give instance of complete solutions for each type by constructing explicit metrics on open subsets of the hyperbolic space H 2 , the complex projective line CP1 , and the 3 - sphere S3 respectively .Keywords : Global structure , Causality , Closed timelike curve",
        "rewrite_text": "We investigate the global structure and causal characteristics of (2 + 1)-dimensional Einstein spaces with a nonnegative cosmological constant, which are locally conformally flat but not globally conformal to Minkowski spacetime. Our classification of these spaces reveals three categories based on their global structures: Type I, where the universal cover is homeomorphic to R × S²; Type II, where the universal cover has infinitely many ends; and Type III, where the universal cover is compact. Notably, we demonstrate that any spacetime of Type I or II allows for closed timelike curves if it contains an incomplete null geodesic segment whose endpoints differ in their affine parameter values. Additionally, we provide concrete examples of complete solutions for each type by constructing explicit metrics on open subsets of hyperbolic space H², the complex projective line CP¹, and the 3-sphere S³, respectively. \n\nKeywords: Global structure, Causality, Closed timelike curve",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": 1.2135597524338357
    },
    {
        "original_text": "We present an analysis of the basis-set dependence of post-Hartree-Fock (HF) corrections to atomization energies for small molecules, using explicitly correlated Gaussian functions and extrapolation techniques.  We show that the correlation energy contribution is more sensitive than the HF energy to the choice of basis sets used in calculations. The results are compared with those obtained by other authors who have studied this problem previously. Finally we discuss how these findings can be applied to improve the accuracy of thermochemical data calculated at the CCSD(T) level. In recent years there has been considerable interest in improving the accuracy of theoretical predictions of thermochemical properties such as enthalpies of formation or heats of reaction. These quantities are often determined experimentally but it would clearly be useful if they could also be predicted theoretically. One approach which has proved successful involves calculating the total electronic energy E tot  n  of a molecule within some chosen approximation n to quantum mechanics, where n = 1 corresponds to Hartree-Fock theory and n = 2 to second-order Møller-Plesset perturbation theory (MP2). Corrections beyond MP2 may then be estimated either by performing higher-level ab initio calculations on smaller subsets of atoms  1  , or alternatively by fitting empirical parameters to experimental data  2  .",
        "watermark_text": "We present an assessment of the basis - set dependence of post - Hartree - Fock ( HF ) corrections to atomization energies for little molecules , using explicitly coupled Gaussian functions and extrapolation methods . We see that the correlation power contribution is more sensitive than the HF power to the selection of basis sets involved in calculations .The results are compared with those acquired by other researchers who have researched this question previously . Finally we talk how these results can be applied to improve the accuracy of thermochemical data calculated at the CCSD ( T ) level .In recent years there has been substantial interest in improving the accuracy of theoretical estimates of thermochemical properties such as enthalpies of formation or heats of reaction . These quantities are often determined experimentally but it would clearly be valuable if they could also be determined theoretically .One approach which has found successful means measuring the total electronic energy E tot n of a molecule within some selected approximation n to quantum mechanics , where n = 1 corresponds to Hartree - Fock physics and n = 2 to second - order Møller - Plesset perturbation theory ( MP2 ) . Corrections beyond MP2 might then be estimated either by performing higher - grade ab initio analyses on smaller subsets of atoms 1 , or alternatively by fitting experimental parameters to experimental data 2 .",
        "rewrite_text": "We evaluate the dependence of post-Hartree-Fock (HF) corrections on the choice of basis sets for small molecules, utilizing explicitly coupled Gaussian functions and extrapolation techniques. Our findings indicate that the contribution from correlation energy is more sensitive to the selected basis sets than the HF contribution itself. We compare our results with previously published work by other researchers who have explored this issue. Additionally, we discuss how these insights can be leveraged to enhance the accuracy of thermochemical data calculated at the CCSD(T) level. Recently, there has been significant interest in improving the precision of theoretical estimates for thermochemical properties, such as enthalpies of formation and heats of reaction. While these values are frequently obtained through experimental means, it would be advantageous to also derive them theoretically. One effective approach involves calculating the total electronic energy \\(E^{\\text{tot}}_n\\) of a molecule using a selected approximation \\(n\\) to quantum mechanics, where \\(n = 1\\) corresponds to Hartree-Fock theory and \\(n = 2\\) corresponds to second-order Møller-Plesset perturbation theory (MP2). Further corrections beyond MP2 can be estimated by conducting higher-level ab initio calculations on smaller atomic subsets or by fitting experimental data to theoretical parameters.",
        "ori-fast-z-score": -0.7035264706814485,
        "water-fast-z-score": 5.050762722761053,
        "rewrite-fast-z-score": -1.0314212462587933
    },
    {
        "original_text": "We study how popularity emerges in collective choice dynamics, where individuals sequentially choose between two options and are influenced by their peers. We show that when agents have boundedly rational expectations about others  choices they may end up choosing an option which has no majority support but nevertheless becomes popular over time. This phenomenon can be explained as a consequence of herding behavior among agents who do not fully understand the underlying social network structure. Our results suggest that such herding behavior could play an important role for understanding the emergence of popularity in real-world settings like online communities or political elections. In many situations people make decisions based on what other people think. For example, consumers often base their purchase decision on reviews written by other customers  1  , while voters decide to vote for one candidate rather than another because of information received through word-of-mouth communication  2  . Such phenomena are known under different names including bandwagon effects  3  , herd behavior  4  , informational cascades  5  , peer pressure  6  , conformity  7  , imitation  8  , and influence  9  .\nIn this work we focus on the case where individuals  decisions are made collectively via voting  10  . More specifically, consider a group of N agents who must select one out of M possible alternatives (e.g., candidates) at each round t = 1, ..., T . At every round t ∈  T  , agent i chooses alternative xt(i), receives feedback f (xt−1)(−i) (e.g., votes casted against his/her choice), and updates his/her belief about the state of nature θ according to Bayes  rule  11  :",
        "watermark_text": "We research how popularity emerges in group choosing dynamics , where adults sequentially choose between two choices and are influenced by their colleagues . We suggest that when agents have boundedly rational expectations about others choices they may end up picking an option which has no majority support but still gets influential over time .This phenomenon can be understood as a outcome of herding behavior among agents who do not truly understand the intrinsic social group structure . Our results propose that such herding behavior could play an important role for explaining the emergence of popularity in real - time environments like online communities or political elections .In many situations people form decisions depending on what other people thought . For instance , consumers commonly base their purchase decision on ratings written by other customers 1 , while voters choose to voting for one nominee rather than another because of information received through word - of - mouth communication 2 .Such effects are known under various names including bandwagon effects 3 , herd behavior 4 , informational cascades 5 , peer stress 6 , conformity 7 , imitation 8 , and influence 9 . In this research we focus on the case where groups decisions are making collectively via voting 10 .More specifically , consider a group of N agents who must select one out of M possible options ( e . g . , candidates ) at each round t = 1 , . . . , T . At every round t ∈ T , agent i picks alternative xt ( i ) , receives feedback f ( xt−1 ) ( −i ) ( e . g . , votes casted against his / her selection ) , and confirms his / her belief about the state of nature θ according to Bayes rule 11 :",
        "rewrite_text": "We investigate how popularity arises within group decision-making dynamics, where adults make sequential choices between two options while being influenced by their peers. Our findings indicate that when individuals hold limited rational expectations about the choices of others, they may ultimately select an option that lacks majority support, yet gains influence over time. This phenomenon can be seen as a result of herding behavior among agents who do not fully grasp the underlying social group structure. Our results suggest that such herding behavior may be crucial in explaining how popularity develops in real-time contexts, such as online communities or political elections. In many instances, people's decisions are swayed by the opinions of others. For example, consumers often rely on ratings from fellow shoppers when making purchasing decisions, while voters may opt for one candidate over another based on information shared through word-of-mouth. These influences have been labeled in various ways, including bandwagon effects, herd behavior, informational cascades, peer pressure, conformity, imitation, and influence. This research specifically examines scenarios where group decisions are made collectively through voting. We consider a group of N agents who must choose one option from M possible alternatives (e.g., candidates) in each round, t = 1, ..., T. In every round t ∈ T, agent i selects an alternative x_t(i), receives feedback f(x_{t-1})(-i) (such as votes against their selection), and updates their belief about the true state of nature θ according to Bayes' rule.",
        "ori-fast-z-score": 0.8703882797784892,
        "water-fast-z-score": 7.7748158302322405,
        "rewrite-fast-z-score": 2.0732842213952645
    },
    {
        "original_text": "We present photometric observations of the SW Sex type nova-like binary system BH Lyn during its recent outburst, which was discovered on September 24th 2010 by amateur astronomers. The light curve shows that this is an intermediate polar (IP) with a very short orbital period and strong magnetic field. We also report spectroscopic data obtained at the end of October when the object returned to quiescence. \n \n Keywords: Novae, Intermediate polars, Photometry, Spectroscopy, Outbursts \n \n 1. Introduction \n \n In 2009 we reported the discovery of a new SW Sex type nova-like variable BH Lyn (Kato et al., 2009) . It showed a large amplitude variation of about 3 mag between maximum brightness and minimum one. This object has been classified as a member of the SW Sex class because it exhibits all typical characteristics such as: double-humped emission-line profiles; prominent phase-dependent absorption lines; rapid flickering superposed on smooth sinusoidal variations; and deep eclipses lasting for several hours . However, there are some differences compared to other members of the class: BH Lyn has a shorter orbital period (P orb = 0.084 d), stronger magnetic field strength (B > 10 MG), and higher mass transfer rate (Ṁ ~10−7 M⊙ yr−1). These properties suggest that BH Lyn may be a progenitor candidate of AM CVn systems .\n \n2. Observations\n\nObservations were carried out using two telescopes equipped with CCD cameras attached: the 60 cm telescope at Okayama Astrophysical Observatory (OAO) and the 50/70 cm Schmidt-Cassegrain telescope at Mt. Lemmon Optical Astronomy Observatory (LOAO). \n \n 2.1 OAO60cm telescope \n \n \n \n The first part of our observation campaign started on September 25th 2010, just after the detection of the outburst. During the following three weeks, we performed time-series photometry every night except for bad weather conditions or technical problems. A total number of 56 nights were observed until November 8th 2010. All images were taken through Johnson V filter with",
        "watermark_text": "We present photometric images of the SW Sex type nova - like binary system BH Lyn during its recent outburst , which was discovered on September 24th 2010 by amateur astronomers . The light curve shows that this is an intermediate polar ( IP ) with a very small orbital period and strong magnetic force .We additionally report spectroscopic data received at the end of October when the object recovered to quiescence . Keywords : Novae , Intermediate polars , Photometry , Spectroscopy , Outbursts 1 .Introduction In 2009 we reported the discovery of a new SW Sex type nova - like variable BH Lyn ( Kato et al . , 2009 ) . It showed a large intensity variation of about 3 mag between maximum brightness and minimum one .This object has been classified as a member of the SW Sex class because it displays all characteristic characteristics such as : triple - humped radiation - line profiles ; prominent phase - based absorption patterns ; quick burning superposed on soft sinusoidal variations ; and dark eclipses lasting for multiple weeks . However , there are some variations compared to other members of the class : BH Lyn has a shorter orbital period ( P orb = 0 . 084 d ) , greater magnetic force speed ( B > 10 MG ) , and larger mass transfer time ( [UNK] ~ 10−7 [UNK] yr−1 ) .These properties suggest that BH Lyn may be a progenitor candidate of AM CVn systems . 2 .Observations Observations were carried out use two telescopes equipped with CCD cameras connected : the 60 centimetres observatory at Okayama Astrophysical Observatory ( OAO ) and the 50 / 70 meter Schmidt - Cassegrain observatory at Mt . Lemmon Optical Astronomy Observatory ( LOAO ) .2 . 1 OAO60cm telescope The first part of our observation project started on September 25th 2010 , just after the discovery of the outburst . During the subsequent three weeks , we performed time - series photometry every night except for good storms circumstances or structural failures .A total number of 56 nights were detected until November 8th 2010 . All pictures were took through Johnson V filter with",
        "rewrite_text": "We provide photometric images of the SW Sex-type nova-like binary system BH Lyn, which was observed during a recent outburst that amateur astronomers discovered on September 24, 2010. The light curve indicates that BH Lyn is an intermediate polar (IP) system with a very short orbital period and a strong magnetic field. Additionally, we present spectroscopic data obtained at the end of October when the system returned to its quiescent state. \n\n**Keywords**: Novae, Intermediate polars, Photometry, Spectroscopy, Outbursts \n\n**1. Introduction**  \nIn 2009, we reported the identification of a new SW Sex-type nova-like variable, BH Lyn (Kato et al., 2009). This system exhibited significant intensity variations of approximately 3 magnitudes between peak and minimum brightness. BH Lyn has been classified as a member of the SW Sex class due to its characteristic features, including triple-humped emission line profiles, notable phase-dependent absorption patterns, rapid variations superimposed on soft sinusoidal light curves, and prolonged dark eclipses lasting several weeks. Nonetheless, compared to other systems in the class, BH Lyn displays some distinct characteristics: it has a shorter orbital period (P_orb = 0.084 d), a stronger magnetic field (B > 10 MG), and a higher mass transfer rate (Ṁ ~ 10−7 M☉ yr−1). These attributes suggest that BH Lyn may serve as a progenitor candidate for AM CVn systems. \n\n**2. Observations**  \nOur observations were conducted using two telescopes equipped with CCD cameras: the 60-cm telescope at the Okayama Astrophysical Observatory (OAO) and the 50/70 cm Schmidt-Cassegrain telescope at the Mt. Lemmon Optical Astronomy Observatory (LOAO). \n\n**2.1 OAO 60-cm Telescope**  \nThe initial phase of our observation campaign began on September 25, 2010, immediately following the discovery of the outburst. Over the next three weeks, we conducted time-series photometry nightly, excluding nights hindered by severe weather or technical issues. A total of 56 nights of observations were recorded until November 8, 2010. All images were captured through the Johnson V filter.",
        "ori-fast-z-score": -0.1841149235796647,
        "water-fast-z-score": 6.818181818181818,
        "rewrite-fast-z-score": 0.5303300858899106
    },
    {
        "original_text": "We study non-Abelian hydrodynamic equations for fluids with spin-orbit coupling, which are derived by applying Noether s theorem to an action functional describing the dynamics of such systems. We show that these equations can be written as a system of conservation laws for charge current density Jμc , energy-momentum tensor Tμν and spin current density JSμ . The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e, μ, τ .\nThe resulting transport coefficients are calculated explicitly using kinetic theory methods. In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species (e.g., electrons) or if the fluid contains only neutral bosons like photons. This result holds both for relativistic and nonrelativistic fluids. Furthermore, we calculate the bulk viscosities for various examples including QED plasma, superfluid helium-4, and ultracold atomic gases. Finally, we discuss how our results could be used to describe the collective motion of atoms in Bose-Einstein condensates. \nI. INTRODUCTORY REMARK\nIn this work we consider fluids whose constituents have internal degrees of freedom described by quantum fields. Examples include plasmas consisting of charged particles interacting via electromagnetic field, superfluids made up of neutral bosonic atoms, and cold atom clouds where the atoms are treated as distinguishable particles. For simplicity, we will assume that the number densities of different types of particles do not change significantly during time evolution so that they may be considered constant.",
        "watermark_text": "We research non - Abelian hydrodynamic equations for fluids with spin - orbit coupling , which are derived by using Noether s theorem to an action functional describing the dynamics of such systems . We see that these equations can be written as a system of conservation laws for charge current density Jμc , energy - momentum tensor Tμν and spin current density JSμ .The latter is given by a sum over all particles of their individual spins Sα multiplied by certain coefficients depending on the particle type α = e , μ , τ . The resulting transport values are measured precisely use kinetic theory techniques .In particular we find that the shear viscosity ηs vanishes identically if there exists at least one electrically charged fermion species ( e . g . , electrons ) or if the liquid includes only neutral bosons like photons . This result holds both for relativistic and nonrelativistic fluids .Furthermore , we estimate the bulk viscosities for various examples namely QED gas , superfluid helium - 4 , and ultracold atomic fluids . Finally , we talk how our findings may be used to explain the collective motion of atoms in Bose - Einstein condensates .I . INTRODUCTORY REMARK In this research we study fluids whose members have internal degrees of autonomy explained by quantum fields .Examples example plasmas consisting of charged particles communicating via electromagnetic field , superfluids composed up of neutral bosonic atoms , and cold particle clouds where the atoms are treated as distinguishable molecules . For simplicity , we will assume that the number densities of different kinds of atoms do not change considerably during time progression so that they may be regarded continuous .",
        "rewrite_text": "We investigate non-Abelian hydrodynamic equations for fluids exhibiting spin-orbit coupling, derived using Noether's theorem from an action functional that describes the dynamics of such systems. Our analysis reveals that these equations can be framed as a set of conservation laws for charge current density \\( J^\\mu_c \\), energy-momentum tensor \\( T^{\\mu\\nu} \\), and spin current density \\( J^S_\\mu \\). The spin current density is defined as a sum over all particles, where each particle's individual spin \\( S^\\alpha \\) is multiplied by specific coefficients depending on the particle type \\( \\alpha = e, \\mu, \\tau \\). We leverage kinetic theory techniques to accurately measure the resulting transport properties. Notably, we discover that the shear viscosity \\( \\eta_s \\) becomes zero if there is at least one electrically charged fermion species (e.g., electrons) or if the fluid consists solely of neutral bosons like photons. This finding applies to both relativistic and nonrelativistic fluids. Additionally, we estimate the bulk viscosities for various cases, including QED gas, superfluid helium-4, and ultracold atomic fluids. Finally, we discuss how our results may help explain the collective behavior of atoms in Bose-Einstein condensates. \n\n**I. INTRODUCTORY REMARKS** \n\nIn this research, we focus on fluids composed of constituents that possess internal degrees of freedom explained by quantum fields. Examples include plasmas made up of charged particles interacting via electromagnetic fields, superfluids composed of neutral bosonic atoms, and cold atomic clouds where the atoms are treated as distinguishable particles. For simplicity, we assume that the number densities of different types of atoms remain relatively constant over time, allowing us to treat them as continuous.",
        "ori-fast-z-score": 0.4508348173337161,
        "water-fast-z-score": 6.041186552271796,
        "rewrite-fast-z-score": 1.2185435916898848
    },
    {
        "original_text": "We study the gravitational self-force acting on a wave packet in curved spacetime, and show that it causes its distortion.  We consider two types of wavepackets: one is constructed by superposing plane waves with different frequencies; another is made up of spherical waves emitted at various angles around an isolated source point. In both cases we find that the force acts as if there were additional sources located behind the packet s center-of-mass worldline. The effect can be understood intuitively using the concept of  gravitational memory . Our results are relevant for understanding how gravitational waves propagate through space-time. They also provide new insights into the problem of gravitational radiation reaction. Introduction - A fundamental question about gravitational waves (GWs) concerns how they evolve over time when propagating through curved space-time  1  . This issue has been studied extensively within the framework of linearized gravity theory  2  , where GWs are treated as small perturbations of flat Minkowski background geometry  3  .\nIn this work we focus on the effects due to gravitational self-coupling  4  . These arise because each part of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity  5  . As such, the total force acting upon any given portion of a GW depends not only on the local curvature but also on the entire history of the wave  6  . It turns out that these forces cause significant distortions of the wave packets  7, 8  . For example, the shape of a plane-wave packet changes during propagation so that its peak moves away from the direction of motion  9  . Similar behavior was found for spherical wave packets  10  .",
        "watermark_text": "We consider the gravitational self - force acting on a wave packet in curved spacetime , and find that it creates its degradation . We consider two forms of wavepackets : one is built by superposing plane waves with various frequencies ; another is made up of circular signals emitted at several angles around an exposed source point .In both cases we find that the force works as if there were extra sources located behind the packet s center - of - mass worldline . The phenomenon can be understood intuitively using the idea of gravitational memory .Our results are important for studying how gravity signals propagate through space - time . They also bring fresh insights into the question of gravitational waves reaction .Introduction - A basic issue about gravitational waves ( GWs ) concerns how they develop over time when propagating through curved space - time 1 . This problem has been studied thoroughly within the framework of linearized gravity physics 2 , where GWs are treated as low perturbations of flattened Minkowski background geometry 3 .In this research we focus on the effects due to gravitational self - interaction 4 . These occur because each portion of a GW carries energy density which exerts pressure back onto itself via Newtonian gravity 5 .As such , the total stress acted upon any certain parts of a GW relies not only on the local curvature but also on the entire history of the signal 6 . It turns out that these forces cause significant distortions of the signal packets 7 , 8 .For instance , the morphology of a plane - wave packet shifts during propagation so that its peak changes away from the direction of movement 9 . Similar behavior was seen for spherical wave packets 10 .",
        "rewrite_text": "We examine the gravitational self-force affecting a wave packet in curved spacetime and discover that it contributes to the packet's degradation. We analyze two types of wave packets: one constructed by superposing plane waves of varying frequencies, and the other composed of circular signals emitted from different angles around a specific source point. In both scenarios, we observe that the self-force behaves as though additional sources are positioned behind the center-of-mass worldline of the packet. This phenomenon can be intuitively explained through the concept of gravitational memory. Our findings are crucial for understanding how gravitational signals propagate through spacetime and offer new perspectives on the interaction of gravitational waves. \n\nIntroduction: A key aspect of gravitational waves (GWs) is how they evolve over time as they travel through curved spacetime. This issue has been extensively explored within the framework of linearized gravity, treating GWs as small perturbations of a flat Minkowski background. In our research, we concentrate on the effects of gravitational self-interaction, which arise because each segment of a GW carries energy density that exerts pressure on itself through Newtonian gravity. Consequently, the total stress affecting any part of a GW depends not only on the local curvature but also on the entire history of the signal. These forces lead to notable distortions in the signal packets; for instance, the shape of a plane-wave packet shifts during propagation, causing its peak to deviate from the direction of travel. A similar alteration has been observed in spherical wave packets.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 8.131727983645296,
        "rewrite-fast-z-score": 1.2888044650576527
    },
    {
        "original_text": "We study the statistical properties of surface fluctuations for different values of the lateral size L and height H of the system, using numerical simulations on square lattices with periodic boundary conditions. We find that the probability density function (PDF) of the local slope angle θ is well described by an exponential decay at large angles, while it displays power-law tails at small slopes. The crossover between these two regimes occurs around θ = 0.5π. In addition to this behavior, we observe that the PDFs are strongly dependent upon both L and H. This dependence can be explained as finite-size effects: when L or H becomes smaller than some characteristic length scale, the PDFs become broader due to the presence of more rare events. Finally, we show how our results compare favorably with experimental data obtained from scanning tunneling microscopy measurements performed on Si(111). PACS numbers: 68.35.Bs",
        "watermark_text": "We research the statistical characteristics of fluid fluctuations for different values of the longitudinal length L and elevation H of the system , using numerical simulations on square lattices with periodic border conditions . We see that the probability density function ( PDF ) of the local slope angle θ is well described by an exponential decay at large angles , while it displays power - law tails at small elevations .The crossover between these two regimes occurs around θ = 0 . 5π . In addition to this behavior , we find that the PDFs are strongly dependent upon both L and H . This reliance can be described as finite - length effects : when L or H becomes lower than some characteristic length range , the PDFs get broader owing to the presence of more unusual events .Finally , we prove how our findings compare favorably with experimental evidence derived from scanning tunneling microscopy observations performed on Si ( 111 ) . PACS codes : 68 . 35 . Bs",
        "rewrite_text": "We investigate the statistical properties of fluid fluctuations across various longitudinal lengths (L) and elevations (H) in our system, utilizing numerical simulations on square lattices with periodic boundary conditions. Our analysis reveals that the probability density function (PDF) of the local slope angle (θ) follows an exponential decay for larger angles, while exhibiting power-law tails for smaller elevations. The transition between these two behaviors occurs at approximately θ = 0.5π. Additionally, we observe that the PDFs are significantly influenced by both L and H. This relationship can be attributed to finite-length effects; specifically, as L or H decreases below a certain characteristic length scale, the PDFs broaden due to an increase in the occurrence of atypical events. Ultimately, we demonstrate how our results align well with experimental data obtained from scanning tunneling microscopy observations on Si (111). PACS codes: 68.35.Bs.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 5.658135095031152,
        "rewrite-fast-z-score": -0.7385489458759964
    },
    {
        "original_text": "We study the stability properties of circumnuclear disks (CNDs) embedded within elliptical galaxies, using N-body simulations with live dark matter halos and stellar components. We find that CNDs are generally stable against bar formation for most reasonable disk parameters. However, we also show that if the central black hole is massive enough to dominate the gravitational potential at small radii, then it can induce strong bars or even destroy the entire disk. This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear bars in nearby elliptical galaxies. \n \n Keywords: Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy evolution; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology \n \n 1 Introduction \n \n The existence of nuclear bars has been inferred observationally by several authors based on photometric data (e.g., Laine et al. 2002; Erwin 2004) . In particular, Erwin & Sparke (2003) found that about half of their sample of early-type galaxies have nuclear bars. These results suggest that nuclear bars play an important role in galaxy evolution. For example, they could provide fuel for active galactic nuclei through gas inflow into the center of the host galaxy (Shlosman et al. 1990 ). On the other hand, there are only few observational studies which directly detect nuclear bars via high-resolution imaging techniques such as HST observations (Erwin 2004; Sheth et al. 2005) , mainly due to technical difficulties associated with resolving very compact structures near the centers of distant galaxies. Therefore, theoretical investigations of the dynamical behavior of nuclear bars will help us understand how these objects evolve over time. \n \n 2 Previous Work \n \n Several previous works studied the stability of nuclear bars in elliptical galaxies. Athanassoula et al. (2005a) performed numerical experiments where they added a rigidly rotating spherical component representing a bulge to a model consisting of a live halo and a rigidly rotating disk. They showed that this system becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "watermark_text": "We research the stability properties of circumnuclear drives ( CNDs ) lodged within elliptical galaxies , using N - bodies simulations with live dark matter halos and stellar parts . We see that CNDs are typically consistent against bar structure for most reasonable disk characteristics .However , we also demonstrate that if the main white hole is massive enough to dominate the gravitational potential at small radii , then it can induce strong bars or even kill the entire disk . This result suggests that the presence of a supermassive black hole may be responsible for some observed nuclear spots in nearby elliptical galaxies .Keywords : Gravitational instability ; Black holes ; Bars ; Nuclear activity ; Galaxy growth ; Disk galaxies ; Dark matter halos ; Stellar dynamics ; Cosmology 1 Introduction The nature of nuclear bars has been inferred observationally by many authors based on photometric data ( e . g . , Laine et al . 2002 ; Erwin 2004 ) .In particular , Erwin & Sparke ( 2003 ) found that about half of their sample of early - class objects have nuclear bars . These data suggest that atomic bars serve an important role in universe growth .For instance , they may provide energy for active galactic nuclei through gas inflow into the center of the host universe ( Shlosman et al . 1990 ) .On the other hand , there are only few observational investigations which directly identify atomic bars via high - resolution detection methods such as HST observations ( Erwin 2004 ; Sheth et al . 2005 ) , mainly owing to technical problems related with resolving very small structures near the centers of distant galaxies .Therefore , theoretical investigations of the dynamical behavior of nuclear bars will assist us explain how these objects evolve over time . 2 Earlier Work Several earlier works studied the stability of nuclear bars in elliptical galaxies .Athanassoula et al . ( 2005a ) made numerical studies where they added a rigidly rotating spherical component describing a bulge to a simulation consisting of a living halo and a rigidly rotating disk .They showed that this body becomes unstable when the mass ratio between the bulge and the disk exceeds a critical value",
        "rewrite_text": "We investigate the stability characteristics of circumnuclear disks (CNDs) situated within elliptical galaxies through N-body simulations incorporating dynamic dark matter halos and stellar components. Our findings indicate that CNDs generally maintain stability against bar formations across a wide range of disk properties. However, we also reveal that if the central white hole possesses sufficient mass to dominate the gravitational potential at small radii, it can lead to the formation of strong bars or potentially disrupt the entire disk. This outcome implies that the existence of a supermassive black hole could be a contributing factor to certain observed nuclear features in nearby elliptical galaxies. \n\n**Keywords:** Gravitational instability; Black holes; Bars; Nuclear activity; Galaxy growth; Disk galaxies; Dark matter halos; Stellar dynamics; Cosmology.  \n\n**1 Introduction** The nature of nuclear bars has been inferred through various observational studies based on photometric data (e.g., Laine et al. 2002; Erwin 2004). Notably, Erwin & Sparke (2003) discovered that roughly half of the early-type galaxies examined exhibited nuclear bars. Such findings indicate that nuclear bars may play a crucial role in the evolution of the universe, as they could channel energy into active galactic nuclei via gas inflow toward the center of their host galaxies (Shlosman et al. 1990). Conversely, few observational studies have successfully identified nuclear bars using high-resolution techniques like Hubble Space Telescope (HST) observations (Erwin 2004; Sheth et al. 2005), primarily due to the technical challenges associated with resolving small structures in the centers of distant galaxies. Hence, theoretical analyses of the dynamics of nuclear bars will enhance our understanding of their evolution over time. \n\n**2 Earlier Work** Previous research has examined the stability of nuclear bars within elliptical galaxies. Athanassoula et al. (2005a) conducted numerical simulations incorporating a rigidly rotating spherical bulge component alongside a live halo and a rigid disk. Their results indicated that the system becomes unstable when the mass ratio of the bulge to the disk surpasses a critical threshold.",
        "ori-fast-z-score": 0.5482823149915702,
        "water-fast-z-score": 7.96486185631891,
        "rewrite-fast-z-score": 1.4791479939068937
    },
    {
        "original_text": "We study the problem of finding an optimal schedule for data transmission over multiple channels with interference constraints, where each channel is assigned to one transmitter-receiver pair and transmissions on different pairs are subject to mutual interference. We consider two models: (i) The first model assumes that all transmitters have fixed power levels. (ii) In the second model we assume that transmitters can adjust their powers dynamically. For both cases, we show how to find an optimal schedule by solving a sequence of linear programs. Our results hold even if there exists only one receiver per transmitter. This work was supported by NSF grant CCF-0430018. 1 Introduction Wireless networks consist of many nodes communicating via radio signals. Each node has limited range and thus cannot communicate directly with every other node. Instead, it communicates indirectly through intermediate nodes called relays or routers. A fundamental question in this context is: How should these relays be placed?",
        "watermark_text": "We explore the question of finding an appropriate schedule for information transmission over multiple channels with interference constraints , where each channel is allotted to one transmitter - receiver set and signals on separate pairs are subject to mutual interference . We consider two models : ( i ) The first theory requires that all transmitters have fixed power rates .( ii ) In the second theory we suppose that transmitters can adjust their powers dynamically . For both cases , we show how to find an appropriate schedule by solving a sequence of linear programs .Our results hold even if there exists only one receiver per transmitter . This work was supported by NSF grant CCF - 0430018 .1 Introduction Wireless networks consist of several nodes communicating via radio signals . Each node has restricted range and therefore cannot transmit directly with every other node .Instead , it communicates indirectly through intermediate nodes termed relays or routers . A basic issue in this situation is : How should these relays be placed ?",
        "rewrite_text": "We investigate the problem of establishing an effective schedule for information transmission across multiple channels while considering interference constraints. In this scenario, each channel is designated for a specific transmitter-receiver pair, and signals from different pairs can interfere with one another. We evaluate two models: (i) In the first model, all transmitters operate with fixed power levels; (ii) in the second model, we assume that transmitters can dynamically adjust their power. For both approaches, we demonstrate a method for determining an appropriate schedule by solving a series of linear programming problems. Our findings remain valid even when there is only one receiver assigned to each transmitter. This research was funded by NSF Grant CCF-0430018. \n\n1 Introduction: Wireless networks comprise various nodes that communicate using radio signals. Due to limited transmission ranges, nodes are unable to communicate directly with every other node. Instead, they rely on intermediate nodes, known as relays or routers, for indirect communication. A fundamental challenge in this context is determining the optimal placement of these relays.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 3.623286509262706,
        "rewrite-fast-z-score": 0.7504787743864564
    },
    {
        "original_text": "We consider the problem of scheduling jobs on parallel machines with preemption and failure, where each job consists of several tasks that must be processed in sequence by different machines. We assume that if any task fails to complete processing before its deadline then all remaining tasks for this job are lost. In addition we allow preemptions at no cost within each machine but not across machines. Our objective is to minimize total completion time subject to deadlines associated with individual tasks.  We show how our results can be used to obtain an O(log n)-approximation algorithm for minimizing makespan when there may be failures during execution. Finally, we present some open problems related to our work. The research described here was supported by NSF grant CCF-0635035. This document contains supplemental material to  1  . \nContents\n\nA Proofs of Lemmas 1-7\nB Proof of Theorem 8 B",
        "watermark_text": "We consider the issue of scheduling jobs on parallel machines with preemption and failure , where each work consists of several tasks that must be processed in sequence by various machines . We assume that if any task fails to complete processing before its deadline then all surviving tasks for this job are losing .In addition we allow preemptions at no price within each machine but not across machines . Our objective is to minimize maximum finish time according to deadlines associated with particular processes .We see how our findings can be used to obtain an O ( log n ) - approximation algorithm for minimizing makespan when there may be failures during execution . Finally , we present some open problems related to our work .The research described here was supported by NSF grant CCF - 0635035 . This document contains supplemental material to 1 .Contents\n\nA Proofs of Lemmas 1-7\nB Proof of Theorem 8 B",
        "rewrite_text": "We address the challenge of scheduling jobs on parallel machines while accounting for preemption and failures. Each job consists of multiple tasks that must be executed sequentially on different machines. If any task fails to finish before its deadline, all remaining tasks for that job also fail. We permit preemption without cost on each machine, but not between different machines. Our goal is to minimize the maximum finish time based on deadlines tied to specific processes. We demonstrate how our results can lead to an O(log n)-approximation algorithm for minimizing makespan in the presence of potential execution failures. Additionally, we outline several open problems stemming from our research. This study was funded by NSF grant CCF-0635035. The document includes supplementary material as follows: \n\n1. Proofs of Lemmas 1-7\n2. Proof of Theorem 8",
        "ori-fast-z-score": 3.25,
        "water-fast-z-score": 6.5,
        "rewrite-fast-z-score": 2.54000254000381
    },
    {
        "original_text": "We present an improved method for solving radiative transfer (RT) problems using smoothed particle hydrodynamics (SPH). The new scheme is based on the Monte Carlo approach, which has been shown to be accurate and efficient when applied to RT problems with optically thick media. We have implemented this algorithm into our existing SPH code, Gadget-2, and tested it by simulating the collapse of a molecular cloud core under the influence of radiation pressure. Our results show that the inclusion of radiation feedback leads to more realistic models of protostellar accretion disks than those obtained without including any form of radiation physics. In addition, we find that the disk mass increases as the strength of the radiation field decreases. This suggests that the effect of photoevaporation may play an important role during the early stages of disk evolution. \n \n Keywords: Radiative transfer, Smoothed particle hydrodynamics",
        "watermark_text": "We introduce an updated approach for solving radiative transfer ( RT ) difficulties using smoothed particle hydrodynamics ( SPH ) . The revised scheme is based on the Monte Carlo approach , which has been shown to be precise and efficient when applied to RT problems with optically thick films .We have integrated this algorithm into our existing SPH software , Gadget - 2 , and demonstrated it by simulating the collapse of a chemical cloud core under the impact of radiation pressure . Our results show that the introduction of radiation feedback contributes to more realistic theories of protostellar accretion disks than those achieved without including any form of radiation physics .In addition , we find that the disk mass rises as the strength of the radiation field decreases . This implies that the impact of photoevaporation possibly play an important role during the early stages of disk evolution .Keywords: Radiative transfer, Smoothed particle hydrodynamics",
        "rewrite_text": "We present a refined method for addressing challenges in radiative transfer (RT) using smoothed particle hydrodynamics (SPH). Our updated scheme employs a Monte Carlo approach, which has proven to be both accurate and efficient for RT issues involving optically thick media. This algorithm has been incorporated into our existing SPH software, Gadget-2, and we have validated it through simulations of a chemical cloud core's collapse under radiation pressure. Our findings indicate that integrating radiation feedback yields a more realistic understanding of protostellar accretion disks compared to models that exclude radiation effects. Furthermore, we observe that as the intensity of the radiation field diminishes, the disk mass increases, suggesting that photoevaporation may play a significant role in the early stages of disk evolution.  \nKeywords: Radiative transfer, Smoothed particle hydrodynamics",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 5.986302773458956,
        "rewrite-fast-z-score": 1.524001524002286
    },
    {
        "original_text": "We compare the magnetic flux distribution in coronal holes (CHs) with that in quiet regions using vector magnetograms observed by Hinode/SOT/SP. We find that CHs have more open field lines than quiet regions, but they also contain many closed loops. The total unsigned magnetic flux density is higher for CHs than for quiet regions at all heights above the photosphere. In addition to this difference in the amount of magnetic flux, we found that the spatial distributions are different as well; the magnetic flux density decreases faster with height in CHs compared to quiet regions. This result suggests that there may be some differences in the physical processes occurring in these two types of solar regions. Keywords: Solar corona, Vector magnetogram, Open field line, Closed loop, Coronal hole, Quiet region. 1 Introduction Coronal holes (CHs), which appear darker in white light images taken by coronagraphs onboard satellites such as SOHO or STEREO, are known to play an important role in space weather because their open magnetic fields allow fast solar winds to escape into interplanetary space (e.g., Wang et al. (1998) , Cranmer & van Ballegooijen (2005) ).\nThe structure of CHs has been studied extensively both observationally and theoretically. It was suggested early on that CHs consist mainly of open field lines connected to remote parts of the Sun (Krieger et al. (1971) ), while closed loops were rarely seen inside them (Wiegelmann et al. (2010a) ). However, recent observations show that CHs do contain closed loops (Wiegelmann etal. (2010b) , Parnell et al. (2011 ), DeForest et al. (2013 , Brooks et al. (2014) ). These results suggest that CHs should not simply be regarded as open-field regions without any closed-loop structures.",
        "watermark_text": "We relate the magnetic flux spread in coronal holes ( CHs ) with that in quiet regions using vector magnetograms observed by Hinode / SOT / SP . We see that CHs have more open field lines than quiet regions , but they still hold several shut rings .The total unsigned magnetic flux concentration is higher for CHs than for calm regions at all heights above the photosphere . In addition to this contrast in the quantity of magnetic flux , we reported that the spatial distributions are distinct as well ; the magnetic flux concentration drops quicker with depth in CHs compared to quiet regions .This result suggests that there may be some variations in the physical processes arising in these two kind of sun areas . Keywords : Solar corona , Vector magnetogram , Open field system , Closed loop , Coronal hole , Quiet region .1 Introduction Coronal holes ( CHs ) , which appear darker in white light pictures taken by coronagraphs onboard satellites such as SOHO or STEREO , are known to play an important role in space weather because their open magnetic fields allow strong sun winds to escape into interplanetary space ( e . g . , Wang et al . ( 1998 ) , Cranmer & van Ballegooijen ( 2005 ) ) .The structure of CHs has been studied frequently both observationally and theoretically . It was suggested early on that CHs consist mostly of open field lines linked to remote parts of the Sun ( Krieger et al .( 1971 ) ) , while closed loops were seldom visible inside them ( Wiegelmann et al . ( 2010a ) ) .However , recent observations show that CHs do contain shut loops ( Wiegelmann etal . ( 2010b ) , Parnell et al .( 2011 ) , DeForest et al . ( 2013 , Brooks et al .( 2014 ) ) . These conclusions show that CHs should not simply be regarded as open - field regions without any closed - loop fields .",
        "rewrite_text": "We examine the distribution of magnetic flux in coronal holes (CHs) compared to tranquil regions using vector magnetograms from the Hinode/SOT/SP. Our observations indicate that CHs exhibit a greater number of open field lines than quiet regions, yet they still contain several closed-loop structures. Throughout all heights above the photosphere, the total unsigned magnetic flux concentration is notably higher in CHs than in calm areas. Additionally, we found that the spatial distributions of the magnetic flux differ; in CHs, the concentration diminishes more rapidly with depth than in quiet regions. This finding implies potential differences in the physical processes occurring in these two types of solar regions. \n\nKeywords: Solar corona, Vector magnetogram, Open field system, Closed loop, Coronal hole, Quiet region.\n\n1 Introduction\nCoronal holes (CHs), which appear darker in white light imagery captured by coronagraphs on satellites like SOHO or STEREO, are significant for space weather because their open magnetic fields facilitate the escape of strong solar winds into interplanetary space (e.g., Wang et al. 1998; Cranmer & van Ballegooijen 2005). The structure of CHs has been extensively studied through both observational and theoretical methods. Early suggestions indicated that CHs are primarily composed of open field lines linked to distant regions of the Sun (Krieger et al. 1971), with closed loops rarely observed within them (Wiegelmann et al. 2010a). However, more recent observations have confirmed the presence of closed loops within CHs (Wiegelmann et al. 2010b; Parnell et al. 2011; DeForest et al. 2013; Brooks et al. 2014), indicating that CHs should not simply be categorized as open-field regions devoid of closed-loop structures.",
        "ori-fast-z-score": -0.44367825470805694,
        "water-fast-z-score": 5.833630944789017,
        "rewrite-fast-z-score": -0.35355339059327373
    },
    {
        "original_text": "We consider the energy conditions in cosmology, which are necessary for the existence of matter with positive pressure. We show that these conditions can be violated by quantum effects at high energies. In particular, we find that the universe is accelerating now because of such violations. The results obtained agree well with observations. \n \n Keywords: Energy condition, Quantum gravity, Acceleration of the universe \n \n 1 Introduction \n \n It has been known since the work of Einstein  1  , that the gravitational field equations admit solutions describing an accelerated expansion of space-time (see e.g.,  2  ). This fact was confirmed later on by numerous observational data  3  . However, it remains unclear what causes this phenomenon. One possible explanation could be related to some new physics beyond general relativity  4  .\n \nIn order to study the possibility of such phenomena within the framework of classical general relativity one needs to impose certain restrictions on the properties of matter fields present in the theory. These restrictions are usually called  energy conditions   5  . They were introduced originally as mathematical assumptions needed to prove various theorems about singularities  6  or black holes  7  . Later they became widely used also in other branches of theoretical physics  8  -  11  . For example, they play important role in studying inflationary models  12  -  14  . \n \n 2 Energy Conditions in Cosmology \n \n Let us start our discussion with the following set of four basic energy conditions  15  :",
        "watermark_text": "We consider the energy conditions in cosmology , which are necessary for the existence of matter with positive pressure . We see that these conditions can be violated by quantum effects at high energies .In particular , we find that the universe is accelerating now because of such defects . The results collected accord well with observations .Keywords : Energy condition , Quantum gravity , Acceleration of the universe 1 Introduction It has been known since the work of Einstein 1 , that the gravitational field equations imply solutions describing an accelerated expansion of space - time ( saw e . g . , 2 ) . This fact was confirmed later on by various observational data 3 .However , it remains unsure what causes this phenomenon . One potential explanation could be connected to some new science beyond general relativity 4 .In order to study the possibility of such interactions within the framework of classical general relativity one needs to introduce certain constraints on the properties of matter fields encountered in the theory . These restrictions are typically called energy conditions 5 .They were introduced originally as mathematical statements needed to prove several theorems about singularities 6 or black holes 7 . Afterwards they becoming widely useful also in other branches of theoretical physics 8 - 11 .For instance , they serve useful character in understanding inflationary theories 12 - 14 . 2 Energy Conditions in Cosmology Let us begin our debate with the following setting of four fundamental energy conditions 15 :",
        "rewrite_text": "We examine the energy conditions in cosmology, which are essential for the existence of matter that exerts positive pressure. It appears that these conditions may be breached by quantum effects at high energy levels. Specifically, we determine that the universe is currently experiencing acceleration due to these anomalies, and our findings align well with observational data. **Keywords:** Energy condition, Quantum gravity, Acceleration of the universe. \n\n**1 Introduction** Since Einstein's work, it has been established that the equations governing gravitational fields yield solutions that depict an accelerated expansion of spacetime. This phenomenon was subsequently corroborated by various observational findings. However, the underlying cause of this effect remains unclear. One possible explanation may involve new theories that extend beyond general relativity. To explore such interactions within the classical framework of general relativity, we must impose certain constraints on the properties of matter fields involved in the theory. These constraints, known as energy conditions, were initially introduced as mathematical requirements to demonstrate several theorems concerning singularities and black holes. Over time, they have proven valuable in various areas of theoretical physics, including their role in understanding inflationary theories. \n\n**2 Energy Conditions in Cosmology** We begin our discussion with the presentation of four fundamental energy conditions.",
        "ori-fast-z-score": 0.944911182523068,
        "water-fast-z-score": 6.614378277661476,
        "rewrite-fast-z-score": 2.1783245945486835
    },
    {
        "original_text": "We study the effect of measurement noise on the Markov property for stochastic processes with continuous state spaces and discrete time steps. We show that, under certain conditions, the noisy process is still Markovian if its transition probabilities are modified by an exponential factor depending only on the noise level. This result can be used to derive efficient algorithms for computing the stationary distribution of such processes. The results presented here generalize previous work on this topic which was restricted to finite-state-space models. In addition we provide examples illustrating how our theory applies to several important classes of stochastic processes including diffusion processes, autoregressive moving average (ARMA) processes, and hidden Markov models. \nI. INTRODUCTORY REMARK\nThe main goal of this article is to present some new theoretical results about the effects of measurement noise on the statistical behavior of stochastic processes. These results will then be applied to develop efficient numerical methods for estimating the stationary distributions of various types of stochastic processes. Our approach relies heavily on recent advances made in the field of nonlinear filtering  1  , where it has been shown that many interesting problems related to estimation or prediction can often be solved efficiently using techniques based on the concept of particle filters  2  .",
        "watermark_text": "We test the impact of monitoring interference on the Markov property for stochastic systems with continuous state spaces and finite period steps . We see that , under certain conditions , the noisy process is nevertheless Markovian if its transition probabilities are modified by an exponential factor depending only on the noise rate .This result can be used to derive elegant techniques for modeling the stationary distribution of such processes . The results presented here generalize past work on this topic which was confined to finite - state - space models .In addition we provide examples illustrating how our theory applies to several important classes of stochastic systems including diffusion processes , autoregressive moving average ( ARMA ) events , and hidden Markov models . I .INTRODUCTORY REMARK The main goal of this page is to provide some fresh theoretical results about the effects of measurement noise on the statistical behavior of stochastic systems . These conclusions will then be applied to develop able numerical models for estimating the stationary distributions of several kinds of stochastic systems .Our solution draws highly on current developments made in the field of nonlinear routing 1 , where it has been shown that several interesting problems related to estimation or prediction can often be answered easily using techniques based on the idea of particle filters 2 .",
        "rewrite_text": "We examine how monitoring interference affects the Markov property in stochastic systems characterized by continuous state spaces and finite time steps. Our findings indicate that, under specific conditions, the process remains Markovian even when its transition probabilities are adjusted by an exponential factor that solely depends on the noise rate. This insight paves the way for developing refined techniques to model the stationary distribution of these processes. Our results extend previous research, which was limited to models with finite state spaces. Additionally, we present examples that demonstrate the applicability of our theory to several noteworthy categories of stochastic systems, including diffusion processes, autoregressive moving average (ARMA) events, and hidden Markov models. \n\n**I. INTRODUCTORY REMARK**  \nThis section aims to unveil new theoretical insights into the influence of measurement noise on the statistical behavior of stochastic systems. These findings will then be employed to create effective numerical models for estimating the stationary distributions of various types of stochastic systems. Our approach heavily utilizes recent advancements in the field of nonlinear routing, where it has been shown that many intriguing problems concerning estimation or prediction can often be readily addressed using techniques inspired by particle filters.",
        "ori-fast-z-score": 0.29002094671369905,
        "water-fast-z-score": 7.057176370033344,
        "rewrite-fast-z-score": 2.463323195410733
    },
    {
        "original_text": "We present new abundance determinations for the CNO elements, Na, Mg, Al, Si, S, Ar, Ca, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, Hf, Ta, W, Re, Os, Ir, Pt, Au, Hg, Tl, Pb, Bi, Th, U, and Np in two Galactic globular clusters (NGC 6752 and M4) based on high-resolution optical spectroscopy obtained with UVES at the Very Large Telescope Observatory. We find that these abundances are consistent with those predicted by standard stellar evolution theory when we take into account the effects of nuclear burning during the thermally pulsing asymptotic giant branch phase.  The observed chemical composition is also compatible with predictions made using theoretical yields calculated with state-of-the-art nucleosynthesis models including both convective overshoot mixing and rotation-induced mixing processes.",
        "watermark_text": "We report new abundance determinations for the CNO objects , Na , Mg , Al , Si , S , Ar , Ca , Sc , Ti , V , Cr , Mn , Fe , Co , Ni , Cu , Zn , Y , Zr , Ba , La , Ce , Nd , Sm , Eu , Gd , Dy , Yb , Hf , Ta , W , Re , Os , Ir , Pt , Au , Hg , Tl , Pb , Bi , Th , U , and Np in two Galactic globular complexes ( NGC 6752 and M4 ) based on wide - resolution optical spectroscopy acquired with UVES at the Very Large Telescope Observatory . We see that these abundances are compatible with those predicted by typical stellar evolution theory when we give into consideration the effects of nuclear combustion during the thermally pulsing asymptotic giant branch period .The observed chemical composition is also consistent with predictions produced utilizing theoretical yields measured with state - of - the - art nucleosynthesis models featuring both convective overshoot mixing and rotation - caused mixed mechanisms .",
        "rewrite_text": "We present new abundance measurements for a range of elements including CNO objects, Na, Mg, Al, Si, S, Ar, Ca, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, Hf, Ta, W, Re, Os, Ir, Pt, Au, Hg, Tl, Pb, Bi, Th, U, and Np in two Galactic globular clusters (NGC 6752 and M4). These measurements were obtained through high-resolution optical spectroscopy using UVES at the Very Large Telescope Observatory. The resulting abundances align with those predicted by standard stellar evolution theory, particularly when accounting for nuclear burning during the thermally pulsing asymptotic giant branch phase. Moreover, the observed chemical composition is in agreement with predictions derived from advanced nucleosynthesis models that incorporate both convective overshooting and mixing due to rotation.",
        "ori-fast-z-score": -0.8846517369293828,
        "water-fast-z-score": 4.718142596956708,
        "rewrite-fast-z-score": 1.543033499620919
    },
    {
        "original_text": "The measurement calculus is an extension to the standard quantum mechanics formalism that allows for measurements on composite systems, and it has been used in several recent works as well as in this thesis.  The main idea behind the measurement calculus is to consider all possible outcomes of a measurement process as separate states of the system being measured instead of just one outcome (as done by von Neumann). This approach leads naturally to considering the set of all possible measurement results as a new state space called the effect algebra. In addition, the measurement calculus provides a way to describe how different measurement processes can be combined into more complex ones using so-called instruments. Finally, the measurement calculus also includes a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty. The measurement calculus was first introduced by Aharonov et al., and since then there have been many papers written about its properties and applications.",
        "watermark_text": "The measurement calculus is an addition to the standard quantum mechanics formalism that enables for measurements on composite systems , and it has been used in multiple recent works as well as in this dissertation . The main idea behind the observation calculus is to consider all possible outcomes of a measurement process as separate states of the process being measured instead of just one outcome ( as done by von Neumann ) .This method results naturally to defining the group of all possible measurement results as a new state space termed the effect algebra . In addition , the observation calculus provides a way to explain how various measurement processes can be merged into more sophisticated ones utilizing so - called instruments .Finally , the observation calculus also contains a description of what happens when we perform a measurement on a system whose state is not known exactly but only up to some uncertainty . The measurement calculus was first developed by Aharonov et al . , and since then there have been many books written about its properties and use .",
        "rewrite_text": "The measurement calculus expands upon the standard formalism of quantum mechanics, allowing for measurements on composite systems. It has been referenced in various recent studies, including this dissertation. The core concept of the observation calculus is to treat all possible outcomes of a measurement as distinct states of the system being observed, rather than focusing solely on a single outcome, as proposed by von Neumann. This approach leads to the definition of a new state space called the effect algebra, which encompasses all potential measurement results. Furthermore, the observation calculus illustrates how different measurement processes can be integrated into more complex ones through the use of instruments. It also provides a framework for understanding measurements performed on systems with uncertain states. The measurement calculus was initially introduced by Aharonov and colleagues, and since then, numerous books have been published discussing its properties and applications.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We study the slow wave resonance (SWR) effect for periodically layered media with an arbitrary number N of anisotropic layers, each characterized by its own permittivity tensor and thickness. We show that SWR is possible only if all principal axes of the permittivity tensors are parallel to one another within each layer. In this case we derive explicit expressions for the dispersion relation between the frequency f and the Bloch wavenumber kx. The results obtained can be used as guidelines for designing multilayered structures exhibiting strong SWR effects at low frequencies. \n \n Keywords: Slow wave resonance; Anisotropy; Multilayer structure; Dispersion relations. 1 Introduction \n \n Periodic multilayers consisting of alternating thin films made of different materials have attracted considerable attention during recent years due to their unique properties  1  . These include high reflectance  2  , negative refraction  3  , enhanced nonlinear optical response  4  , etc., which make them promising candidates for various applications such as optoelectronic devices  5  or photovoltaics  6  .\n \nIn particular, it has been shown recently  7–9  that periodic multilayers composed of anisotropic layers may exhibit very interesting electromagnetic phenomena including slow wave resonance (S WR). This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium  10  . It leads to extremely large values of the effective refractive index n eff = c / v ph  11  where c is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode  12  . As a result, the corresponding transmission spectrum exhibits sharp peaks associated with narrow stop bands  13  . Such features are highly desirable for many practical applications  14  . \n \n However, despite numerous theoretical studies devoted to S WR in periodic multilayers  15–18  , there still exist several open questions related to the conditions under which this phenomenon takes place  19, 20  . For example, it was found experimentally  21  that the presence of a single misaligned anisotropic layer destroys the S WR effect completely even though other layers remain perfectly aligned. On the other hand, numerical simulations  22  suggest that",
        "watermark_text": "We explore the slow frequency resonance ( SWR ) effect for regularly layered media with an arbitrary number N of anisotropic layers , each described by its own permittivity matrix and thickness . We see that SWR is possible only if all primary directions of the permittivity tensors are connected to one another within each surface .In this situation we derive explicit expressions for the dispersion constant between the frequency f and the Bloch wavenumber kx . The results derived can be used as guidelines for constructing multilayered buildings presenting strong SWR effects at low frequencies .Keywords : Slow wave vibration ; Anisotropy ; Multilayer structure ; Dispersion relations . 1 Introduction Periodic multilayers consisting of alternating thin sheets formed of different materials have garnered considerable scrutiny during recent seasons due to their distinct characteristics 1 .These include high reflectance 2 , negative refraction 3 , enhanced nonlinear optical reaction 4 , etc . , which make them promising candidates for various uses such as optoelectronic technologies 5 or photovoltaics 6 . In particular , it has been shown ago 7 – 9 that periodic multilayers consisting of anisotropic surfaces may exhibit very interesting electrical processes including slow frequency resonance ( S WR ) .This phenomenon occurs when the phase velocity of the Bloch waves becomes equal to zero inside the medium 10 . It results to incredibly large values of the effective refractive index n eff = c / u ph 11 where p is the speed of light in vacuum and v ph is the phase velocity of the propagating Bloch mode 12 .As a result , the associated transmission spectrum exhibits severe spikes identified with narrow stop rings 13 . Such characteristics are extremely practical for numerous practical applications 14 .However , despite several theoretical experiments devoted to S WR in periodic multilayers 15 – 18 , there still appear several open questions related to the conditions under which this phenomenon happens place 19 , 20 . For instance , it was shown experimentally 21 that the presence of a single misaligned anisotropic surface destroys the S WR effect totally even though other layers remain perfectly aligned .On the other hand , numerical simulations 22 suggest that",
        "rewrite_text": "We investigate the slow frequency resonance (SWR) effect in periodically layered media composed of N anisotropic layers, each characterized by its own permittivity matrix and thickness. Our findings indicate that SWR can only occur when all primary directions of the permittivity tensors are interconnected across each surface. In this context, we derive explicit expressions that relate the dispersion constant to the frequency (f) and the Bloch wave vector (kx). These results serve as valuable guidelines for designing multilayered structures that exhibit significant SWR effects at low frequencies. \n\nKeywords: Slow wave vibration; Anisotropy; Multilayer structure; Dispersion relations.\n\n1. Introduction \nRecently, periodic multilayers made up of alternating thin sheets of different materials have attracted significant attention due to their unique properties. These properties include high reflectance, negative refraction, and enhanced nonlinear optical responses, making them promising candidates for applications in optoelectronic technologies and photovoltaics. Notably, previous research has demonstrated that periodic multilayers incorporating anisotropic surfaces can exhibit intriguing electrical phenomena, such as slow frequency resonance (SWR). This phenomenon arises when the phase velocity of Bloch waves becomes zero within the medium, resulting in remarkably high values of the effective refractive index (n_eff = c / v_ph), where c represents the speed of light in a vacuum and v_ph is the phase velocity of the propagating Bloch mode. Consequently, the transmission spectrum displays pronounced spikes associated with narrow stop bands, which are highly beneficial for numerous applications. However, despite various theoretical studies investigating SWR in periodic multilayers, several questions remain regarding the conditions that facilitate this phenomenon. For example, experimental findings indicate that the introduction of even a single misaligned anisotropic surface can completely negate the SWR effect, whereas other layers remain perfectly aligned. Conversely, numerical simulations suggest that...",
        "ori-fast-z-score": -0.15811388300841897,
        "water-fast-z-score": 8.432781346758377,
        "rewrite-fast-z-score": 3.265986323710904
    },
    {
        "original_text": "We present the results of cosmological simulations aimed at studying the formation and evolution of tidally stripped satellite galaxies in clusters, which we refer to as  tidal dwarfs  (TDs). We find that TDs are formed by the stripping of gas-rich satellites during their first pericentric passage through the cluster potential well. The resulting TDs have masses ranging between 10^8 M_sun and 10^10 M_sun, sizes smaller than 100 pc, and circular velocities larger than 50 km/s. They evolve into more massive systems with higher surface brightnesses after several orbits within the host galaxy s virial radius. Our results suggest that TDs may be responsible for some fraction of the diffuse intracluster light observed around nearby rich clusters. Tidal dwarf galaxies (TDGs) are small star forming objects found near interacting or merging galaxies. Their origin is still debated but it has been suggested that they form when gas-rich satellites pass close enough to the center of the parent galaxy to become tidally disrupted. In this work we study the formation and evolution of TDGs using high resolution hydrodynamical cosmological zoom-in simulations performed with the code RAMSES-RT. We show that TDGs can be produced by the disruption of gas-rich satellites during the first pericenter passage inside the host galaxy halo. These TDGs typically have masses between 108M⊙ and 1011M⊙, sizes below 100pc, and circular velocities above 50km/s. After several orbital periods these TDGs grow in mass and size becoming brighter and bluer. Finally, our results indicate that TDGs could contribute up to 50% of the total amount of diffuse intra-cluster light observed around nearby rich galaxy clusters.",
        "watermark_text": "We present the conclusion of cosmological simulations aiming at studying the formation and evolution of tidally stripped satellite galaxies in clusters , which we termed to as tidal dwarfs ( TDs ) . We see that TDs are created by the stripping of gas - rich satellites during their early pericentric passage through the cluster potential well .The produced TDs have masses ranging between 10 ^ 8 M _ sun and 10 ^ 10 M _ sun , sizes less than 100 pc , and spherical velocities greater than 50 km / s . They evolve into more massive components with higher surface brightnesses after many orbits within the recipient universe s virial diameter .Our results propose that TDs might be responsible for some fraction of the diffuse intracluster light observed around nearby dense clusters . Tidal dwarf galaxies ( TDGs ) are small galaxy producing objects found near interacting or merging galaxies .Their origin is still unclear but it has been proposed that they shape when gas - rich satellites come close enough to the center of the parent galaxy to become tidally disrupted . In this research we study the formation and evolution of TDGs using high resolution hydrodynamical cosmological zoom - in simulations conducted with the code RAMSES - RT .We see that TDGs can be formed by the disruption of gas - containing satellites during the first pericenter passage inside the host universe halo . These TDGs typically have masses between [UNK] and [UNK] , sizes below 100pc , and circular velocities above 50km / s .After many orbital periods these TDGs grow in mass and size getting brighter and bluer . Finally , our findings confirm that TDGs might contribute up to 50 % of the total amount of diffuse intra - cluster energy seen around nearby dense star clusters .",
        "rewrite_text": "We present the findings from cosmological simulations focused on the formation and evolution of tidally stripped satellite galaxies within clusters, which we refer to as tidal dwarfs (TDs). Our analysis indicates that TDs are formed when gas-rich satellites experience significant mass loss during their initial close passage through the gravitational well of the cluster. The resulting TDs possess masses ranging from \\(10^8\\) to \\(10^{10}\\) solar masses, dimensions of less than 100 parsecs, and circular velocities exceeding 50 km/s. As they orbit within the cluster's virial radius, these TDs evolve into more massive entities with enhanced surface brightness. Our results suggest that TDs could account for a portion of the diffuse intracluster light observed around nearby dense clusters.\n\nTidal dwarf galaxies (TDGs) are small galaxy-like structures that arise near interacting or merging galaxies. Their formation mechanisms remain somewhat ambiguous; however, it has been proposed that they originate from the tidal disruption of gas-rich satellites that venture close to the center of a parent galaxy. In this study, we investigate the formation and evolution of TDGs through high-resolution hydrodynamical cosmological zoom-in simulations conducted using the RAMSES-RT code. We find that TDGs can form from the disruption of gas-bearing satellites during their first pericentric passage through the halo of the host galaxy. Typically, these TDGs have masses within a specific range, sizes below 100 parsecs, and circular velocities above 50 km/s. Over time, they grow in both mass and size, becoming increasingly luminous and blue. Ultimately, our findings indicate that TDGs may contribute as much as 50% of the total diffuse intra-cluster light observed around dense star clusters.",
        "ori-fast-z-score": 0.09016696346674323,
        "water-fast-z-score": 7.061788191316445,
        "rewrite-fast-z-score": 1.227881227029841
    },
    {
        "original_text": "We present results for multidimensional, multi-group flux-limited diffusion (MGFLD) calculations of convection and acoustic oscillations in the postbounce phase of corecollapse supernovae using an approximate treatment of neutrino transport. We find that our MGFLD models are able to reproduce many features observed in recent two-dimensional hydrodynamic simulations including the growth rate of the SASI as well as its characteristic spiral mode structure. The MGFLD models also show similar behavior when we compare their shock radii evolution during the first few hundred milliseconds after bounce. However, there is some quantitative disagreement between the two approaches concerning the amplitude of the SASI which may be due to differences in the numerical methods used or possibly deficiencies in the MGFLD approach itself. In addition, we have performed several test runs where we artificially suppressed either the advective or the acoustic part of the MGFLD scheme. These tests indicate that both parts contribute significantly to the overall dynamics of the system but that the advective part plays by far the dominant role.",
        "watermark_text": "We publish results for multidimensional , multi - group flux - limited diffusion ( MGFLD ) observations of convection and sound oscillations in the postbounce mode of corecollapse supernovae using an approximate formulation of neutrino travel . We see that our MGFLD predictions are able to capture several characteristics found in recent two - dimensional hydrodynamic simulations notably the development frequency of the SASI as well as its typical spiral mode shape .The MGFLD theories also demonstrate identical dynamics when we compare their shock radii evolution during the first few hundred milliseconds after bounce . However , there is some numerical dispute between the two approaches involving the frequency of the SASI which may be due to differences in the numerical methods used or possibly deficiencies in the MGFLD method itself .In addition , we have done several test runs where we artificially suppressed either the advective or the acoustic portion of the MGFLD scheme . These analyses suggest that both parts play significantly to the overall structure of the system but that the advective portion plays by far the dominant role .",
        "rewrite_text": "We present findings from our multidimensional, multi-group flux-limited diffusion (MGFLD) observations of convection and sound oscillations in the post-bounce phase of core-collapse supernovae, utilizing an approximate formulation of neutrino travel. Our MGFLD predictions successfully replicate several features observed in recent two-dimensional hydrodynamic simulations, particularly the growth frequency of the SASI and its characteristic spiral mode shape. The MGFLD models exhibit comparable dynamics when we analyze the evolution of shock radii during the initial few hundred milliseconds following the bounce. However, there is some inconsistency between the two approaches regarding the frequency of the SASI, which may arise from variations in numerical methods or potential limitations of the MGFLD technique itself. Furthermore, we conducted several test runs in which we artificially suppressed either the advective or acoustic aspects of the MGFLD method. These investigations indicate that both components significantly contribute to the overall system structure, although the advective part is by far the most influential.",
        "ori-fast-z-score": -0.5698028822981898,
        "water-fast-z-score": 7.111887749987414,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "We study the nonlinear dynamics of semiconductor superlattices (SLs) under an external magnetic field, which is applied at an angle to SL layers. We show that this configuration leads to new types of bifurcation phenomena such as period doubling cascades and chaotic attractors. The results are obtained by solving numerically the coupled Schrödinger-Poisson equations for the electron wave function and electrostatic potential using the split-step Fourier method. \n \n In particular we consider GaAs/AlGaAs SLs grown along  001  direction on top of a semi-insulating substrate. This system has been studied experimentally in Refs.  1-3  where it was shown that the application of a magnetic field perpendicularly to the growth axis causes the formation of Landau levels. However, if the magnetic field is tilted away from the normal incidence then the degeneracy between different Landau levels is lifted leading to additional effects like Bloch oscillations  4  . \n \n For example, in Ref.  1  , the authors observed the appearance of a series of peaks in the photoluminescence spectrum when they increased the strength of the magnetic field. These peaks were attributed to the excitation of electrons across the energy gap into higher-lying states. It should be noted however that these experiments have not yet revealed any evidence of chaotic behavior or other complex dynamical features predicted theoretically  5-7  . \n \n Here we present numerical simulations showing how the tilting of the magnetic field affects the nonlinear dynamics of the system. Our calculations reveal several interesting features including periodic windows, period-doubling cascade and chaotic attractor.",
        "watermark_text": "We research the nonlinear dynamics of semiconductor superlattices ( SLs ) under an external magnetic force , which is applied at an angle to SL layers . We see that this configuration leads to novel sorts of bifurcation phenomena such as period doubling cascades and chaotic attractors .The results are derived by solving numerically the coupled Schrödinger - Poisson equations for the electron wave function and electrostatic potential using the split - step Fourier algorithm . In particular we study GaAs / AlGaAs SLs grown along 001 position on top of a semi - insulating substrate .This system has been studied experimentally in Refs . 1 - 3 where it was shown that the application of a magnetic force perpendicularly to the development axis causes the formation of Landau concentrations .However , if the magnetic force is tilted away from the usual incidence then the degeneracy between various Landau concentrations is lifted contributing to extra effects like Bloch oscillations 4 . For instance , in Ref .1 , the published noticed the appearance of a sequence of peaks in the photoluminescence spectrum when they increased the strength of the magnetic force . These peaks were attributed to the excitation of electrons across the power gap into greater - lying states .It should be mentioned however that these experiments have not already revealed any evidence of turbulent activity or other difficult dynamical characteristics anticipated theoretically 5 - 7 . Here we present numerical simulations demonstrating how the tilting of the magnetic force influenced the nonlinear dynamics of the system .Our calculations reveal numerous interesting features including periodic windows , interval - doubling cascade and chaotic attractor .",
        "rewrite_text": "We investigate the nonlinear dynamics of semiconductor superlattices (SLs) subjected to an external magnetic field applied at an angle to the SL layers. This configuration results in unique bifurcation phenomena, including period-doubling cascades and chaotic attractors. Our results are obtained by numerically solving the coupled Schrödinger-Poisson equations for the electron wave function and electrostatic potential using the split-step Fourier method. Specifically, we focus on GaAs/AlGaAs SLs aligned along the [001] direction, which are grown on a semi-insulating substrate. Previous experimental studies (Refs. 1-3) have shown that applying a magnetic field perpendicular to the growth axis leads to the formation of Landau levels. However, when the magnetic field is angled away from this conventional orientation, the degeneracy among Landau levels is broken, resulting in additional phenomena such as Bloch oscillations. For instance, in Ref. 1, researchers observed a series of peaks in the photoluminescence spectrum as the magnetic field strength was increased, which were linked to the excitation of electrons across the energy gap into higher-lying states. It is important to note, however, that these experiments did not provide any evidence of turbulent behavior or other complex dynamical characteristics predicted by theory (Refs. 5-7). In this study, we present numerical simulations that illustrate how tilting the magnetic field affects the nonlinear dynamics of the system. Our calculations reveal a wealth of intriguing features, including periodic windows, interval-doubling cascades, and chaotic attractors.",
        "ori-fast-z-score": 0.5669467095138409,
        "water-fast-z-score": 7.559289460184544,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new results on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures, using Monte Carlo radiative transfer simulations. We find that for stars with high mass-loss rates (Ṁ > 10-7 M⊙ yr-1), the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering processes within the wind. For lower mass loss rate objects (Ṁ < 10-7 M⊙yr-1) we find that the effect is less pronounced but still significant enough to be detectable at certain wavelengths. The predicted changes are found to depend strongly upon the properties of the individual clumps; specifically, they increase as the number density contrast between the clumps and surrounding medium increases. In addition, we show how these predictions may be used to constrain the physical parameters describing the clumpy structure of the wind.  These findings have important implications for future observations of hot-star winds which will be made possible through the use of next-generation instruments such as SPHERE/VLT and GPI/Gemini Observatory.",
        "watermark_text": "We report new data on modelling the effects of clumps in stellar winds on their observed linear and circular polarization signatures , using Monte Carlo radiative transfer simulations . We see that for stellar with high mass - loss rates ( [UNK] > 10 - 7 [UNK] yr - 1 ) , the presence of clumps can significantly affect both the degree and angle of linear polarization produced by scattering mechanisms within the wind .For lower mass loss temperature objects ( [UNK] < 10 - 7 [UNK] - 1 ) we find that the impact is less noticeable but still significant enough to be detectable at certain wavelengths . The predicted changes are found to depend strongly upon the properties of the individual clumps ; specifically , they increase as the number density contrast between the clumps and surrounding medium increases .In addition , we show how these predictions may be used to constrain the physical factors describing the clumpy composition of the wind . These insights have important implications for future discoveries of bright - star winds which will be made possible through the using of next - generation satellites such as SPHERE / VLT and GPI / Gemini Observatory .",
        "rewrite_text": "We present new findings on the influence of clumps in stellar winds on their observed linear and circular polarization signatures, utilizing Monte Carlo radiative transfer simulations. Our results indicate that for stars with high mass-loss rates (greater than \\(10^{-7}\\) \\(M_{\\odot}\\) yr\\(^{-1}\\)), the presence of clumps can significantly impact both the degree and angle of linear polarization resulting from scattering mechanisms within the wind. In contrast, for stars with lower mass-loss rates (less than \\(10^{-7}\\) \\(M_{\\odot}\\) yr\\(^{-1}\\)), the effects are less pronounced but still detectable at certain wavelengths. The anticipated changes are found to be highly dependent on the characteristics of the individual clumps; specifically, they become more pronounced as the density contrast between the clumps and the surrounding medium increases. Furthermore, we discuss how these insights can help constrain the physical factors that describe the clumpy nature of the wind. This research holds significant implications for the forthcoming discoveries of stellar winds from bright stars, facilitated by next-generation satellites such as SPHERE/VLT and GPI/Gemini Observatory.",
        "ori-fast-z-score": 0.6546536707079772,
        "water-fast-z-score": 3.709704134011871,
        "rewrite-fast-z-score": -0.7875615306482168
    },
    {
        "original_text": "In this paper, we propose an underwater acoustic communication system using orthogonal frequency division multiplexing (OFDM) combined with direct sequence code division multiple access (DS-CDMA). The proposed scheme is based on the combination of two techniques that are widely used in terrestrial wireless communications to achieve higher data rates. In order to reduce the effect of multipath fading caused by channel dispersion, cyclic prefixes are inserted into each symbol block before transmission. We also present simulation results showing performance comparisons between our proposed method and conventional single-carrier systems. Finally, we show that the proposed technique can be applied to high-speed underwater acoustic channels. Keywords: Underwater acoustic communication; Orthogonal frequency division multiplexing; Direct sequence code division multiple access; Channel dispersion; Multipath fading: Cyclic prefixes; High-speed underwater acoustic channels. 1 Introduction Acoustic waves have been extensively studied as a means of transmitting information over water due to their low cost and ease of deployment  1  . However, the limited bandwidth available at frequencies below 10 kHz has restricted the achievable data rates  2  .\nRecently, there has been growing interest in developing broadband underwater acoustic communication systems capable of supporting high data rates  3  -  6  . One promising approach involves combining orthogonal frequency division multiplexing(OFDM), which was originally developed for use in wired and wireless radio-frequency applications  7  , with direct-sequence code-division multiple-access (DS-CDMA), which is commonly employed in cellular mobile networks  8  . This hybrid scheme combines the advantages of both technologies while mitigating some of their disadvantages  9  . For example, it allows us to exploit the large number of subcarriers offered by OFDM to combat inter-symbol interference (ISI) resulting from dispersive channels  10  . It also enables DS-CDMA to provide robustness against narrowband interferers  11  .",
        "watermark_text": "In this paper , we propose an underwater sound communication network utilizing orthogonal frequency division multiplexing ( OFDM ) coupled with direct sequence code unit multiple entry ( DS - CDMA ) . The proposed system is based on the combination of two strategies that are widely using in terrestrial wireless communications to achieve greater signal levels .In order to reduce the impact of multipath fading induced by block dispersion , cyclic prefixes are introduced into each symbol block before broadcast . We additionally offer simulation data demonstrating performance similarities between our proposed method and conventional single - carrier systems .Finally , we find that the suggested method can be applied to large - speed underwater sound networks . Keywords : Underwater sound transmissions ; Orthogonal frequency division multiplexing ; Direct sequence code unit multiple entry ; Channel dispersion ; Multipath fading : Cyclic prefixes ; High - speed underwater sound networks .1 Introduction Acoustic waves have been heavily studied as a means of transmitting information over water owing to their low cost and ease of deployment 1 . However , the limited bandwidth available at speeds below 10 kHz has restricted the achievable data levels 2 .Recently , there has been growing interest in building broadband underwater sound communication devices suitable of supporting high data levels 3 - 6 . One promising solution involves merging orthogonal frequency division multiplexing ( OFDM ) , which was originally developed for use in wired and wireless radio - frequency users 7 , with direct - sequence code - division multiple - access ( DS - CDMA ) , which is often employed in cell mobile services 8 .This hybrid scheme mixes the advantages of both technologies while mitigating some of their disadvantages 9 . For instance , it allows us to harness the huge amount of subcarriers supplied by OFDM to counter inter - sign noise ( ISI ) resulting from dispersive channels 10 .It additionally permits DS - CDMA to provide robustness against narrowband interferers 11 .",
        "rewrite_text": "In this paper, we present a novel underwater sound communication network that integrates orthogonal frequency division multiplexing (OFDM) with direct sequence code division multiple access (DS-CDMA). This system combines two widely used strategies from terrestrial wireless communications to enhance signal levels. To mitigate the effects of multipath fading caused by channel dispersion, we incorporate cyclic prefixes into each symbol block prior to transmission. Furthermore, we provide simulation results that illustrate performance parallels between our proposed approach and traditional single-carrier systems. Ultimately, our findings suggest that this method is well-suited for high-speed underwater sound networks. \n\n**Keywords**: Underwater sound transmissions; Orthogonal frequency division multiplexing; Direct sequence code division multiple access; Channel dispersion; Multipath fading; Cyclic prefixes; High-speed underwater sound networks. \n\n**1 Introduction**: Acoustic waves have been extensively researched as a cost-effective and easily deployable medium for transmitting information over water. However, the limited bandwidth at frequencies below 10 kHz has constrained achievable data rates. Recently, there has been a surge of interest in developing broadband underwater sound communication devices capable of supporting higher data rates. A promising approach involves the combination of OFDM—originally designed for wired and wireless radio-frequency applications—with DS-CDMA, which is commonly used in mobile cellular services. This hybrid strategy leverages the strengths of both technologies while addressing some of their limitations. For instance, it utilizes the extensive number of subcarriers offered by OFDM to combat inter-symbol interference (ISI) caused by dispersive channels, and harnesses the robustness of DS-CDMA to resist narrowband interference.",
        "ori-fast-z-score": 0.08032193289024989,
        "water-fast-z-score": 9.447561074500141,
        "rewrite-fast-z-score": 1.4419211804559506
    },
    {
        "original_text": "Fullerenes are spherical molecules composed entirely of sp2-hybridized carbons, which can be arranged in different ways to form various allotropes with unique properties. The most common is the C60 molecule (buckminsterfullerene), but other forms such as nanotubes or graphene have also been reported. Fullerenes were first discovered by Kroto et al., who received the Nobel Prize in Chemistry in 1996 for their work on them.  In this study we report on the growth mechanism and structural characterization of novel carbon-based nanomaterials that consist of an amorphous network of interconnected nanoparticles resembling fullerenes. These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at temperatures between 600-700 °C under ultrahigh vacuum conditions. We show how these new carbon nanostructures exhibit remarkable mechanical properties including high elasticity and hardness combined with low friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion.",
        "watermark_text": "Fullerenes are spherical compounds composed exclusively of sp2 - hybridized carbons , which can be grouped in different ways to form several allotropes with peculiar characteristics . The most common is the C60 atom ( buckminsterfullerene ) , but other types such as nanotubes or graphene have also been reported .Fullerenes were first discovered by Kroto et al . , who received the Nobel Prize in Chemistry in 1996 for their studies on them . In this study we publish on the formation pathway and structural characterization of new carbon - based nanomaterials that consist of an amorphous system of interconnected nanoparticles resembling fullerenes .These structures grow spontaneously when evaporating carbon atoms onto silicon substrates at conditions between 600 - 700 °C under ultrahigh pressure circumstances . We see how these new carbon nanostructures exhibit remarkable thermal properties including high elasticity and hardness coupled with poor friction coefficients making them ideal candidates for use as protective coatings against wear and corrosion .",
        "rewrite_text": "Fullerenes are spherical molecules made entirely of sp²-hybridized carbon atoms, which can organize in various configurations to create different allotropes, each with unique properties. The most well-known fullerene is C60, also known as buckminsterfullerene, though other forms like nanotubes and graphene have also been identified. The initial discovery of fullerenes was made by Kroto and colleagues, who were awarded the Nobel Prize in Chemistry in 1996 for their research. In this study, we present findings on the formation and structural characterization of new carbon-based nanomaterials that resemble fullerenes through an amorphous arrangement of interconnected nanoparticles. These structures develop spontaneously when carbon atoms are evaporated onto silicon substrates at temperatures ranging from 600 to 700 °C under ultrahigh pressure. Our results indicate that these new carbon nanostructures possess exceptional thermal properties, including high elasticity and hardness, along with low friction coefficients, making them promising candidates for protective coatings against wear and corrosion.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": 0.9847319278346618
    },
    {
        "original_text": "We present chemical abundances for a sample of 16 Planetary Nebulae (PNe) in the Small Magellanic cloud, obtained with high resolution spectroscopy at the VLT and Keck telescopes. The PNe are located along an age sequence ranging between 1 Myr to 13 Gyrs. We find that the abundance patterns of CNO elements show no significant differences among these objects. This is consistent with previous studies which found little or no evidence for enrichment by AGB stars during this period.  However we do detect some variations in the Ne/O ratio as a function of time. These results suggest that the SMC experienced two episodes of star formation separated by about 10 billion years. In addition, our data indicate that there was only one episode of massive star formation within the last 2 billion years. Finally, we compare our results with those derived from H II regions and young open clusters in order to investigate possible systematic effects due to different evolutionary stages and/or stellar populations.",
        "watermark_text": "We present molecular abundances for a sample of 16 Planetary Nebulae ( PNe ) in the Small Magellanic cloud , obtained with high resolution spectroscopy at the VLT and Keck telescopes . The PNe are situated along an age sequence ranging between 1 Myr to 13 Gyrs .We see that the density patterns of CNO objects show no considerable variations among these objects . This is consistent with previous research which revealed nothing or no evidence for enrichment by AGB stars during this era .However we do predict some variations in the Ne / O ratio as a function of time . These data suggest that the SMC experienced two episodes of star formation divided by about 10 billion decades .In addition , our statistics indicate that there was only one episode of large galaxy formation within the last 2 billion years . Finally , we compare our findings with those generated from H II regions and older close galaxies in trying to examine possible widespread impacts owing to different evolutionary stages and / or stellar groups .",
        "rewrite_text": "We present molecular abundances for a sample of 16 planetary nebulae (PNe) in the Small Magellanic Cloud, determined through high-resolution spectroscopy at the VLT and Keck telescopes. These PNe span an age range from 1 million to 13 billion years. Our analysis indicates that the density patterns of CNO elements exhibit minimal variation across these objects, aligning with prior research that found little evidence of enrichment from AGB stars during this time frame. However, we anticipate some changes in the Ne/O ratio over time. Our findings suggest that the SMC underwent two significant episodes of star formation spaced approximately 10 billion years apart. Furthermore, our data indicates only one major event of galaxy formation in the last 2 billion years. Lastly, we compare our results with data from H II regions and older nearby galaxies to explore potential widespread effects related to different evolutionary stages and stellar groups.",
        "ori-fast-z-score": -1.697749375254331,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 1.0834726777719228
    },
    {
        "original_text": "The Generator Coordinate Method (GCM) is an efficient approach to calculate the electronic structure and properties of materials with strong electron-phonon coupling, such as polar semiconductors or insulators. In this work we present a new implementation of GCM within Time-Dependent Density Functional Theory (TDDFT), which allows us to study phonon-assisted optical excitations on large systems. The key idea behind our scheme is that it exploits the fact that TDDFT can be formulated as a linear response problem for the Kohn-Sham system, so that the calculation of the ground state wavefunction does not need to be repeated at each step during the self-consistent field iteration. We demonstrate the efficiency of our algorithm by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa. Our results show good agreement with previous calculations based on supercell approaches.",
        "watermark_text": "The Generator Coordinate Method ( GCM ) is an efficient approach to estimate the electronic configuration and structures of substances with powerful ion - phonon coupling , such as polar semiconductors or insulators . In this project we present a new implementation of GCM within Time - Dependent Density Functional Theory ( TDDFT ) , which allows us to study phonon - aided optical excitations on big systems .The main idea behind our scheme is that it utilizes the fact that TDDFT can be derived as a linear response problem for the Kohn - Sham system , so that the determination of the ground state wavefunction does not require to be repeated at each step during the self - consistent field iteration . We test the performance of our technique by calculating the absorption spectrum of bulk silicon under hydrostatic pressure up to 100 GPa .Our results show good agreement with previous calculations based on supercell methods .",
        "rewrite_text": "The Generator Coordinate Method (GCM) offers a highly effective way to evaluate the electronic configurations and structures of materials characterized by strong ion-phonon coupling, such as polar semiconductors and insulators. In this project, we introduce a novel application of GCM within the framework of Time-Dependent Density Functional Theory (TDDFT), enabling the exploration of phonon-assisted optical excitations in large systems. The core principle of our approach is grounded in the fact that TDDFT can be formulated as a linear response problem for the Kohn-Sham system, which allows us to avoid repetitive calculations of the ground state wavefunction during each iteration of the self-consistent field process. We assess the effectiveness of our method by computing the absorption spectrum of bulk silicon subjected to hydrostatic pressure up to 100 GPa. Our findings demonstrate strong agreement with earlier results obtained through supercell methods.",
        "ori-fast-z-score": -1.1920791213585393,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": -1.1338934190276817
    },
    {
        "original_text": "We report the observation of magnetic fluctuations at low temperatures and high fields in single crystals of YBa2Cu3O6+x (YBCO) with x=0.4, 0.45, and 0.5 using muon spin relaxation measurements. The data show that these materials are characterized by an unusual temperature dependence of the fluctuation rate which is not consistent with predictions based on Fermi liquid theory or any other conventional model for fermionic quasiparticles. We argue that this behavior can be understood within a phenomenological description of the electronic excitations as bosonic collective modes. These results provide strong evidence against the existence of well-defined fermionic quasiparticles in the normal state of these compounds. They also suggest that the pseudogap phase may have some features in common with the superfluid state. \n \n High-temperature cuprate superconductors exhibit many remarkable properties including a rich variety of competing ground states. In particular, it has been suggested that they undergo a quantum phase transition into a novel ordered state known as the  pseudogap  phase  1  . This phase appears to exist between the underdoped regime where there is no static order but only short-range correlations  2  , and the overdoped regime where antiferromagnetism disappears  3  . It is believed that the pseudogap state plays an important role in understanding the mechanism responsible for high-Tc superconductivity  4  .\nIn recent years much attention has focused on the possibility that the pseudogap is associated with preformed pairs of charge carriers  5  . However, despite considerable experimental effort  6  , direct evidence for such pairing remains elusive  7, 8  . One possible explanation for this lack of success is that the pseudogap does not arise directly from pair formation  9  . Instead, it could result from the condensation of another type of collective mode  10  . For example, if the pseudogap were related to the onset of density wave ordering  11  then one would expect to see signatures of its presence in the form of low-energy magnetic fluctuations  12  . Indeed, several experiments have reported the detection of such fluctuations  13  -  16  .",
        "watermark_text": "We report the observation of magnetic fluctuations at low temperatures and low fields in single crystals of YBa2Cu3O6 + x ( YBCO ) with x = 0 . 4 , 0 . 45 , and 0 . 5 utilizing muon spin relaxation measurements . The data reveal that these structures are marked by an peculiar temperature dependence of the fluctuation speed which is not consistent with predictions based on Fermi solid physics or any other usual description for fermionic quasiparticles .We argue that this behavior can be understood within a phenomenological representation of the electronic excitations as bosonic collective modes . These conclusions provide strong evidence against the existence of well - defined fermionic quasiparticles in the usual state of these reactions .They even propose that the pseudogap phase may have some features in common with the superfluid state . High - temperature cuprate superconductors exhibit several notable properties including a rich range of competing ground states .In particular , it has been proposed that they undergo a quantum phase shift into a novel organized state known as the pseudogap phase 1 . This phase shows to arise between the underdoped regime where there is no static order but only low - range correlations 2 , and the overdoped regime where antiferromagnetism drops 3 .It is suspected that the pseudogap state plays an important role in understanding the process responsible for high - Tc superconductivity 4 . In recent years much attention has concentrated on the idea that the pseudogap is associated with preformed pairs of charge carriers 5 .However , despite considerable experimental effort 6 , direct data for such matching remains elusive 7 , 8 . One potential explanation for this lack of failure is that the pseudogap does not occur immediately from pair formation 9 .Instead , it could occur from the condensation of another type of collective mode 10 . For instance , if the pseudogap were linked to the emergence of density wave ordering 11 then one would expect to see signatures of its presence in the form of low - energy magnetic fluctuations 12 .Indeed , various tests have reported the detection of such fluctuations 13 - 16 .",
        "rewrite_text": "We present our findings on magnetic fluctuations observed at low temperatures and low magnetic fields in single crystals of YBa2Cu3O6+x (YBCO) for x values of 0.4, 0.45, and 0.5, using muon spin relaxation measurements. The results indicate a unique temperature dependence of the fluctuation speed that contradicts expectations based on Fermi solid physics or conventional descriptions of fermionic quasiparticles. We propose that this behavior can be interpreted through a phenomenological framework where the electronic excitations are treated as bosonic collective modes. This interpretation offers compelling evidence against the presence of well-defined fermionic quasiparticles in the typical states of these materials and suggests that the pseudogap phase may share characteristics with the superfluid state. High-temperature cuprate superconductors are known for their diverse and complex ground states. Notably, it has been suggested that these materials undergo a quantum phase transition into a distinct organized state referred to as the pseudogap phase. This phase emerges between the underdoped region, characterized by the absence of static order and only short-range correlations, and the overdoped region, where antiferromagnetism diminishes. The pseudogap state is thought to be crucial for understanding the mechanisms underlying high-temperature superconductivity. Recently, significant attention has been directed toward the idea that the pseudogap is associated with preformed pairs of charge carriers. However, despite extensive experimental research, direct evidence supporting this notion has remained elusive. One possible reason for this lack of clarity is that the pseudogap may not stem directly from pair formation; rather, it could arise from the condensation of a different type of collective mode. For example, if the pseudogap is linked to the formation of density wave order, one would expect to observe evidence of this in the form of low-energy magnetic fluctuations. Indeed, various studies have reported the detection of such fluctuations.",
        "ori-fast-z-score": 0.8391813582966892,
        "water-fast-z-score": 7.889320586105296,
        "rewrite-fast-z-score": 2.459747896071916
    },
    {
        "original_text": "We present the results of our investigation on semiclassical scalar propagator in curved space-time, which is based on the WKB approximation to the wave function. We show that there are two different ways how one can define this quantity depending on whether or not one takes into account the back-reaction effects due to the quantum fluctuations of the gravitational field. The first approach leads to an expression for the semiclassical propagator which coincides with the Feynman propagator at large distances but differs significantly near the source point. In particular it does not satisfy the Hadamard condition required by general relativity. On the other hand, if we take into account the back reaction then the resulting expression satisfies all necessary conditions including the Hadamard condition. However, as was shown recently by Wald et al., such an expression cannot be obtained within the framework of standard QFT. This problem may have important consequences when considering the propagation of particles through black holes since the corresponding expressions differ substantially even outside the horizon.",
        "watermark_text": "We present the conclusion of our analysis on semiclassical scalar propagator in curved space - time , which is based on the WKB approximation to the wave function . We see that there are two different ways how one can define this quantity based on whether or not one takes into consideration the back - reaction effects due to the quantum fluctuations of the gravitational field .The first method results to an definition for the semiclassical propagator which coincides with the Feynman propagator at large distances but varies dramatically near the origin point . In particular it does not satisfy the Hadamard condition required by general relativity .On the other hand , if we took into consideration the back response then the resulting expression satisfies all necessary circumstances including the Hadamard condition . However , as was shown lately by Wald et al . , such an form cannot be obtained within the framework of standard QFT .This problem could have important implications when examining the propagation of particles through black holes since the equivalent definitions differ substantially even outside the horizon .",
        "rewrite_text": "We provide the conclusion of our analysis on the semiclassical scalar propagator in curved spacetime, which utilizes the WKB approximation to the wave function. Our findings reveal two distinct methods of defining this quantity, depending on whether or not the back-reaction effects from quantum fluctuations of the gravitational field are taken into account. The first method yields a definition for the semiclassical propagator that aligns with the Feynman propagator at large distances but exhibits significant variation near the origin. Notably, this definition does not meet the Hadamard condition required by general relativity. Conversely, when accounting for back-reaction effects, the derived expression adheres to all necessary criteria, including the Hadamard condition. However, as recently demonstrated by Wald et al., such a formulation cannot be derived within the framework of standard quantum field theory (QFT). This issue could have critical implications for studying particle propagation through black holes, as the two definitions differ markedly even outside the event horizon.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 2.2941573387056176
    },
    {
        "original_text": "We present new results on diffuse gamma-ray emission produced by cosmic rays interacting with interstellar gas, based on data collected during the first year of operation of the Large Area Telescope (LAT) aboard Fermi satellite. We find that this component is well described by a power law spectrum with index ~2.3 extending up to 100 GeV. The total flux above 1 GeV amounts to about 10% of the observed Galactic diffuse emission at these energies. This result confirms previous estimates obtained using EGRET data. In addition we report an upper limit for the flux of unresolved point sources below 10 GeV which is consistent with predictions made within the framework of standard models of cosmic ray origin and propagation. Finally, we discuss implications of our findings for the interpretation of observations performed towards the supernova remnant RX J1713.7--3946. PACS numbers: 98.70.Sa, 95.55.Ym",
        "watermark_text": "We report new data on diffuse γ - ray radiation generated by cosmic rays interacting with interstellar gas , based on evidence generated during the first year of operation of the Large Area Telescope ( LAT ) aboard Fermi satellite . We see that this product is well described by a power law spectrum with index ~ 2 . 3 stretching up to 100 GeV .The total flux above 1 GeV corresponds to about 10 % of the seen Galactic diffuse emission at these frequencies . This result confirms previous estimates obtained using EGRET data .In addition we publish an upper maximum for the flux of unresolved point bodies below 10 GeV which is compatible with predictions taken within the framework of standard models of cosmic ray origin and propagation . Finally , we explain significance of our findings for the interpretation of measurements accomplished towards the supernova remnant RX J1713 . 7 - - 3946 .PACS codes : 98 . 70 . Sa , 95 . 55 . Ym",
        "rewrite_text": "We present new data on diffuse γ-ray radiation produced by the interaction of cosmic rays with interstellar gas, based on observations made during the first year of the Large Area Telescope (LAT) on the Fermi satellite. Our findings indicate that this radiation is well-represented by a power law spectrum with an index of approximately 2.3, extending up to 100 GeV. The total flux above 1 GeV accounts for about 10% of the observed Galactic diffuse emission at these energies, which supports previous estimates derived from EGRET data. Furthermore, we provide an upper limit for the flux of unresolved point sources below 10 GeV that aligns with predictions based on standard models of cosmic ray origin and propagation. Finally, we discuss the implications of our results for interpreting measurements related to the supernova remnant RX J1713.7-3946. PACS codes: 98.70.Sa, 95.55.Ym.",
        "ori-fast-z-score": 0.24618298195866545,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 1.2909944487358056
    },
    {
        "original_text": "We present new models for non-abelian string networks, which are based on the idea that the network is composed by many entangled strings with different velocities and orientations. We show how to construct such velocity-dependent models in terms of Feynman diagrams. In particular we consider two types of diagrams: (i) ladder-like diagrams corresponding to the exchange of gluons between pairs of strings, (ii) cross-ladder like-diagrams describing interactions among three or more strings. The latter type of diagrams can be interpreted as representing junctions where several strings meet at one point. We discuss some properties of these models and compare them with previous results obtained within the framework of Abelian-Higgs model. Finally, we study numerically the evolution of an initial configuration consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations. This work was supported by the DFG under contract SFB-TR9  Gravitational Physics ",
        "watermark_text": "We introduce novel theories for non - abelian string systems , which are based on the idea that the network is composed by many entangled strings with various velocities and orientations . We see how to build such velocity - dependent models in terms of Feynman diagrams .In particular we define two forms of diagrams : ( i ) ladder - like diagrams corresponding to the transfer of gluons between pairs of sequences , ( ii ) cross - ladder like - diagrams describing relationships among three or more strings . The last sort of diagrams can be interpreted as representing junctions where many strings join at one point .We discuss some properties of these models and link them with previous findings obtained within the framework of Abelian - Higgs system . Finally , we study numerically the evolution of an initial structure consisting of a single straight string into a complicated tangle of interacting strings using Monte Carlo simulations .This project was supported by the DFG under contract SFB - TR9 Gravitational Physics",
        "rewrite_text": "We present new theories for non-abelian string systems, grounded in the concept of a network made up of numerous entangled strings, each exhibiting different velocities and orientations. We illustrate how to construct velocity-dependent models using Feynman diagrams. Specifically, we define two types of diagrams: (i) ladder-like diagrams that represent the transfer of gluons between pairs of sequences, and (ii) cross-ladder-like diagrams that depict interactions among three or more strings. The latter type of diagrams can be understood as representing junctions where multiple strings converge at a single point. We discuss various properties of these models and connect them to earlier results obtained within the Abelian-Higgs framework. Lastly, we numerically explore the evolution of an initial configuration consisting of a single straight string into a complex tangle of interacting strings through Monte Carlo simulations. This research was supported by the DFG under contract SFB-TR9 Gravitational Physics.",
        "ori-fast-z-score": 0.9847319278346618,
        "water-fast-z-score": 5.41602560309064,
        "rewrite-fast-z-score": 2.25
    },
    {
        "original_text": "We propose that the most energetic cosmic rays are accelerated in supernova remnants by relativistic jets powered by hypernova explosions, which may be associated with gamma-ray bursts (GRBs). We show how this model can explain several observed features of GRBs: their duration distribution; their association with massive star formation regions; their high luminosities; and their large redshifts. The proposed mechanism is also able to accelerate protons up to energies beyond 10^20 eV without violating current observational constraints on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario provides an explanation for the origin of ultra-high energy cosmic rays as well as for the production of the highest energy neutrinos detected so far. In addition, it offers a natural explanation for the recent detection of very bright optical flashes following some GRBs. \n \n High-energy cosmic rays have been measured at Earth over many decades  1  . Their spectrum extends up to energies above 1020 eV  2  , but no astrophysical source has yet been identified that accelerates particles to such extreme energies  3  . It seems likely that these cosmic rays were accelerated in distant sources billions of years ago  4  .\n \nThe most powerful known explosion in our Universe occurs when a massive star collapses into a black hole after exhausting its nuclear fuel supply  5  . Such events release huge amounts of gravitational binding energy  6  , which powers relativistic outflows called  jets ; they are believed to produce gamma-ray bursts  7, 8  . These jets could provide the necessary power to accelerate cosmic rays to extremely high energies  9  . \n \n However, there are two major difficulties in explaining the origin of the most energetic cosmic ray particles using conventional models  10  : \n \n 1) Conventional jet-powered models cannot accelerate protons to energies greater than ~10^19 eV  11  because the maximum Lorentz factor Γmax of the flow decreases rapidly with distance r from the central engine  12  . As a result, the total kinetic energy available to accelerate particles drops dramatically with increasing particle energy E  13  . For example, if we assume that the bulk Lorentz factor of the",
        "watermark_text": "We suggest that the most intense cosmic rays are accelerated in supernova remnants by relativistic jets driven by hypernova bursts , which may be involved with gamma - ray clusters ( GRBs ) . We see how this model can describe several observed features of GRBs : their duration distribution ; their association with massive star formation regions ; their high luminosities ; and their large redshifts .The proposed process is also could to accelerate protons up to energies beyond 10 ^ 20 eV without violating present observational restrictions on the diffuse fluxes of high - energy neutrinos or photons generated during the acceleration cycle . This scenario offers an reason for the origin of ultra - large energy cosmic rays as well as for the production of the highest power neutrinos detected so far .In addition , it gives a natural explanation for the recent discovery of very bright optical bursts following some GRBs . High - energy cosmic rays have been measured at Earth over much centuries 1 .Their range extends up to frequencies above 1020 eV 2 , but no astrophysical source has already been determined that accelerates particles to such extreme energies 3 . It seems likely that these cosmic rays were accelerated in nearby sources billions of years early 4 .The most intense reported blast in our Universe comes when a huge star collapses into a black hole after exhausting its radioactive fuel supply 5 . Such episodes release massive amounts of gravitational binding energy 6 , which powers relativistic outflows called rockets ; they are said to produce gamma - ray waves 7 , 8 .These jets could give the necessary power to accelerate cosmic rays to incredibly high energies 9 . However , there are two major obstacles in understanding the origin of the most intense cosmic ray ions using conventional versions 10 : 1 ) Conventional jet - powered designs cannot accelerate protons to energies higher than ~ 10 ^ 19 eV 11 because the maximum Lorentz factor Γmax of the flow varies dramatically with distance r from the main engine 12 .As a result , the total kinetic power available to accelerate particles decreases dramatically with rising particle power E 13 . For instance , if we suppose that the bulk Lorentz factor of the",
        "rewrite_text": "We propose that the most powerful cosmic rays are accelerated in supernova remnants through relativistic jets generated by hypernova explosions, which may also be linked to gamma-ray bursts (GRBs). This model effectively accounts for several observed characteristics of GRBs, including their duration distribution, their connection to regions of massive star formation, their high luminosities, and their significant redshifts. The suggested mechanism could accelerate protons to energies exceeding \\(10^{20}\\) eV without breaching current observational limits on the diffuse fluxes of high-energy neutrinos or photons produced during the acceleration process. This scenario not only provides a possible explanation for the origin of ultra-high-energy cosmic rays but also accounts for the production of the most powerful neutrinos detected to date. Additionally, it offers a natural interpretation for the recent observation of exceptionally bright optical flashes following certain GRBs. High-energy cosmic rays have been detected on Earth for centuries, with their energies reaching above \\(10^{20}\\) eV. However, no known astrophysical sources have been identified that can propel particles to such extreme energies. It is likely that these cosmic rays were generated in nearby sources billions of years ago. The most intense explosion recorded in our universe occurs when a massive star collapses into a black hole after depleting its radioactive fuel supply. These events release vast amounts of gravitational binding energy, which drives relativistic outflows known as jets, believed to produce gamma-ray emissions. These jets have the potential to provide the necessary energy for accelerating cosmic rays to extraordinarily high energies. Nevertheless, there are two significant challenges in explaining the origin of the most intense cosmic ray ions using conventional models: first, traditional jet-powered systems struggle to accelerate protons beyond approximately \\(10^{19}\\) eV because the maximum Lorentz factor \\(\\Gamma_{\\text{max}}\\) of the flow varies significantly with the distance \\(r\\) from the central engine. Consequently, the total kinetic power available for particle acceleration diminishes sharply as the particle energy \\(E\\) increases. For example, if we assume a bulk Lorentz factor for the...",
        "ori-fast-z-score": -0.22677868380553634,
        "water-fast-z-score": 8.187458870652156,
        "rewrite-fast-z-score": 1.1404288819045583
    },
    {
        "original_text": "We report on the X-ray properties of the young, nearby (d = 11 pc), low-mass binary system 2MASS J1101-2677AB discovered by Burgasser et al. (2007) . The primary component is an M8 dwarf with T eff ~ 2600 K and log g ~ 5.0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log g ~ 4.2. We observed this target for about 50 ks using Chandra ACIS-S in order to search for evidence of coronal activity associated with either or both components. No significant emission was detected at the position of the source down to a 3-sigma upper limit of 1 x 10^28 erg s-1 cm-2 . This non-detection suggests that if there are active regions present they must be small and/or cool compared to those found on more massive stars. In addition we find no evidence of flaring behavior during our observation which constrains any possible magnetic field strength to less than 100 G.",
        "watermark_text": "We report on the X - ray characteristics of the early , neighboring ( d = 11 pc ) , low - weight binary system 2MASS J1101 - 2677AB discovered by Burgasser et al . ( 2007 ) .The main component is an M8 dwarf with T eff ~ 2600 K and log f ~ 5 . 0 while its companion has been classified as an L5 brown dwarf with T eff ~ 1400K and log f ~ 4 . 2 . We observed this target for about 50 ks using Chandra ACIS - S in order to search for indication of coronal interaction associated with either or both components .No much emitted was seen at the position of the source down to a 3 - sigma upper maximum of 1 x 10 ^ 28 erg s - 1 cm - 2 . This non - measurement indicates that if there are active regions present they must be small and / or cold relative to those observed on more massive stars .In addition we find no evidence of flaring behavior during our experiment which constrains any proposed magnetic force power to fewer than 100 G .",
        "rewrite_text": "We present the X-ray characteristics of the nearby, low-mass binary system 2MASS J1101-2677AB, located 11 parsecs away and first discovered by Burgasser et al. (2007). The primary component is an M8 dwarf with an effective temperature of approximately 2600 K and a logarithmic flux of around 5.0, while its companion is classified as an L5 brown dwarf, with an effective temperature of about 1400 K and a logarithmic flux of around 4.2. We conducted observations of this target for approximately 50 ks using the Chandra ACIS-S, aiming to detect any coronal interactions associated with either component. However, we did not detect significant emission at the source's position, with a 3-sigma upper limit of 1 x 10^28 erg s^-1 cm^-2. This lack of detection suggests that if active regions exist, they must be small and/or cooler compared to those seen in more massive stars. Furthermore, our observations showed no signs of flaring activity, which limits any potential magnetic field strength to less than 100 G.",
        "ori-fast-z-score": -1.1338934190276817,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -0.601929265428846
    },
    {
        "original_text": "We consider an insurance company that controls its exposure to risk by dynamically adjusting its premiums, reserves and investments in financial markets. We assume that the insurer s surplus process is given by a diffusion with jumps driven by Brownian motion and Poisson random measure. The objective of this work is to study how the insurer can control ruin probability using dynamic investment strategies under proportional transaction costs on both assets and liabilities. In particular we prove regularity properties for the minimal probability of ruin as well as optimality results for some specific investment policies. Our main contributions are summarized below:  1) We show that the minimal probability of ruin has at most countably many discontinuities (Proposition 3). 2) We provide sufficient conditions ensuring continuity of the minimal probability of ruin (Theorem 4). 3) We establish necessary and sufficient conditions for the existence of a unique minimizer for the minimal probability of ruin: if there exists one then it coincides with the value function associated with the problem of maximizing expected discounted utility over all admissible investment strategies (Theorems 5 and 6).",
        "watermark_text": "We consider an insurance company that monitors its exposure to risk by dynamically adjusting its premiums , reserves and assets in financial markets . We assume that the insurer s surplus cycle is given by a diffusion with loops accompanied by Brownian movement and Poisson random measure .The goal of this study is to study how the insurer can manage collapse probability utilizing dynamic financial strategies under proportional transaction losses on both assets and liabilities . In particular we prove regularity values for the reduced likelihood of ruin as well as optimality results for some specific investment policies .Our main contributions are presented below : 1 ) We see that the reduced likelihood of ruin has at most countably several discontinuities ( Proposition 3 ) . 2 ) We derive sufficient properties ensuring continuity of the reduced likelihood of ruin ( Theorem 4 ) .3 ) We establish appropriate and sufficient requirements for the existence of a unique minimizer for the minimal probability of ruin : if there exists one then it coincides with the value function related with the question of maximizing expected discounted utility over all admissible investment policies ( Theorems 5 and 6 ) .",
        "rewrite_text": "We examine an insurance company that actively manages its risk exposure by adjusting its premiums, reserves, and investment assets in financial markets. We model the insurer's surplus cycle as a diffusion process with feedback loops, influenced by Brownian motion and a Poisson random measure. This study aims to analyze how the insurer can mitigate the probability of insolvency through dynamic financial strategies, considering proportional transaction costs on both assets and liabilities. Specifically, we demonstrate regularity properties for the reduced likelihood of ruin and establish optimality conditions for certain investment strategies. Our key contributions are as follows: 1) We show that the reduced likelihood of ruin exhibits at most countably many discontinuities (Proposition 3). 2) We provide sufficient conditions for ensuring the continuity of the reduced likelihood of ruin (Theorem 4). 3) We outline necessary and sufficient criteria for the existence of a unique minimizer of the minimal probability of ruin; when such a minimizer exists, it aligns with the value function associated with the maximization of expected discounted utility across all permissible investment policies (Theorems 5 and 6).",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "Mutations in proteins can lead to diseases, such as cancer and cystic fibrosis.  In this study we present a new method for predicting the effect that single-point mutations have on the thermodynamic stability (ΔG) of proteins using only sequence information. The proposed approach is based on an ensemble learning technique called Random Forests which combines multiple decision trees into one model. We show how our method outperforms state-of-the-art methods by achieving higher accuracy with lower computational cost. Our results suggest that the proposed method could be used to predict ΔG values for thousands of mutants at once. Mutations in proteins are known to cause many human diseases including cancer1 and cystic fibross2. Therefore it is important to understand what effects these mutations have on the structure and function of proteins3-6. One way to investigate the impact of mutations is through their influence on the thermodynamic stability7-9 of proteins10-12. Thermodynamic stability refers to the free energy difference between folded and unfolded states13-15. A negative value indicates that the folded state has more favorable interactions than the unfolded state16-18 while positive values indicate unfavorable interactions19-21. It was shown22-24 that the change in Gibbs free energy upon folding (ΔGf) correlates well with experimental measurements25-27. Thus, accurate predictions of ΔGf may help us better understand the relationship between mutations and disease28-30.",
        "watermark_text": "Mutations in proteins can lead to diseases , such as tumors and cystic fibrosis . In this study we present a new method for predicting the impact that single - point mutations have on the thermodynamic stability ( ΔG ) of proteins using only sequence data .The proposed approach is based on an ensemble learning methodology called Random Forests which mixes several judgment forests into one model . We see how our technique outperforms state - of - the - art methods by achieving larger precision with lower mathematical price .Our results propose that the suggested method could be used to predict ΔG rates for thousands of mutants at once . Mutations in proteins are known to produce many human diseases specifically cancer1 and cystic fibross2 .Therefore it is important to consider what effects these mutations have on the structure and activity of proteins3 - 6 . One chance to examine the impact of mutations is through their influence on the thermodynamic stability7 - 9 of proteins10 - 12 .Thermodynamic stability refers to the free energy distinction between folded and unfolded states13 - 15 . A negative value indicates that the folded state has more favorable interactions than the unfolded state16 - 18 while positive measures indicate unfavorable interactions19 - 21 .It was shown22 - 24 that the shift in Gibbs free energy upon folding ( ΔGf ) correlates well with experimental measurements25 - 27 . Thus , able predictions of ΔGf may assist us better understand the relationship between mutations and disease28 - 30 .",
        "rewrite_text": "Mutations in proteins can result in various diseases, including tumors and cystic fibrosis. This study introduces a novel method for predicting the impact of single-point mutations on the thermodynamic stability (ΔG) of proteins using only sequence data. Our approach employs an ensemble learning technique known as Random Forests, which integrates multiple decision trees into a single model. We demonstrate that our method surpasses current state-of-the-art techniques, achieving higher precision with a lower computational cost. Our findings suggest that this method could be applied to predict ΔG values for thousands of mutants simultaneously. Protein mutations are recognized as key contributors to numerous human diseases, particularly cancer and cystic fibrosis. Therefore, it is crucial to understand how these mutations affect protein structure and function. One way to evaluate mutational impact is by examining their influence on protein thermodynamic stability. This stability refers to the free energy difference between the folded and unfolded states. A negative ΔG indicates that the folded conformation is more energetically favorable than the unfolded state, whereas a positive ΔG suggests the opposite. Previous studies have demonstrated a strong correlation between the change in Gibbs free energy upon folding (ΔGf) and experimental measurements. Thus, accurate predictions of ΔGf could enhance our understanding of the link between mutations and disease.",
        "ori-fast-z-score": 1.2,
        "water-fast-z-score": 7.0,
        "rewrite-fast-z-score": -0.0949157995752499
    },
    {
        "original_text": "We present the results of an investigation into the physical nature of polar broad absorption line quasars (BALQSOs). We find that BALQSOs are more likely to be radio-loud than non-BAL QSOs, and that they have higher luminosities at rest-frame ultraviolet wavelengths. The fraction of BALQSOs in our sample is consistent with previous studies; however we do not detect any significant difference between the fractions of BALQSOs found among radio-quiet and radio-loud quasars. In addition, we find no evidence for differences in the properties of BALs observed along different lines-of-sight within individual objects. Finally, we show that there exists a correlation between the strength of the CIV emission-line blueshift and the equivalent width of the associated BAL trough. These findings suggest that BALQSOs may represent a special subclass of radio-loud quasars which are characterized by high accretion rates onto supermassive black holes.",
        "watermark_text": "We present the conclusion of an research into the physical nature of polar wide absorption line quasars ( BALQSOs ) . We see that BALQSOs are more likely to be radio - loud than non - BAL QSOs , and that they have greater luminosities at rest - frame ultraviolet wavelengths .The percentage of BALQSOs in our sample is compatible with previous research ; however we do not detect any large change between the fractions of BALQSOs discovered among radio - quiet and radio - loud quasars . In addition , we find no evidence for variations in the properties of BALs observed along various lines - of - view within particular objects .Finally , we prove that there exists a correlation between the strength of the CIV radiation - line blueshift and the equivalent size of the associated BAL trough . These conclusions propose that BALQSOs might represent a unique subclass of radio - loud quasars which are marked by high accretion speeds onto supermassive black holes .",
        "rewrite_text": "We present the findings of our research on the physical characteristics of polar broad absorption line quasars (BALQSOs). Our analysis indicates that BALQSOs are more likely to be radio-loud compared to non-BAL QSOs and exhibit higher luminosities at rest-frame ultraviolet wavelengths. The proportion of BALQSOs in our sample aligns with prior studies; however, we observe no significant differences in the prevalence of BALQSOs between radio-quiet and radio-loud quasars. Additionally, we find no evidence of variations in the properties of BALs when observed from different lines of sight within the same object. Lastly, we establish a correlation between the strength of the CIV emission-line blueshift and the relative size of the associated BAL trough. These findings suggest that BALQSOs may represent a distinct subclass of radio-loud quasars characterized by high accretion rates onto supermassive black holes.",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 6.573840933228048,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "We have performed neutron powder diffraction experiments on the intermetallic compound Sm2IrIn8 in order to determine its magnetic structure and compare it with that proposed for YbMgGaO4, another member of this family of compounds. The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3.5 K with moments aligned along the c-axis. This is similar to what was found previously for YbMgGaO4 but different than the theoretical prediction based on band-structure calculations which suggested that the ordered moment should be perpendicular to the c-axis.  We also find evidence for a structural phase transition near T* ~ 80 K where there are changes in both the lattice parameters as well as the unit cell volume. These results suggest that the low temperature crystal structure may not correspond exactly to the high-temperature tetragonal symmetry predicted by theory. Finally we present specific heat data showing clear anomalies associated with both the magnetic ordering and the structural phase transition.",
        "watermark_text": "We have done neutron dust diffraction experiments on the intermetallic complex Sm2IrIn8 in order to study its magnetic shape and compare it with that suggested for YbMgGaO4 , another member of this class of compounds . The results show that Sm2IrIn8 has an antiferromagnetic ordering at TN = 3 . 5 K with moments aligned along the c - axis .This is related to what was seen previously for YbMgGaO4 but different than the theoretical forecast based on band - structure estimates which suggested that the ordered moment should be perpendicular to the c - axis . We additionally find proof for a structural phase shift near T * ~ 80 K where there are variations in both the crystal parameters as well as the unit cell size .These data suggest that the reduced temperature crystal composition may not correspond exactly to the high - temperature tetragonal lattice anticipated by theory . Finally we present specific heat statistics suggesting obvious anomalies associated with both the magnetic ordering and the structural phase shift .",
        "rewrite_text": "We conducted neutron dust diffraction experiments on the intermetallic complex Sm2IrIn8 to investigate its magnetic properties and compare them with those proposed for YbMgGaO4, another compound in this category. Our findings indicate that Sm2IrIn8 exhibits antiferromagnetic ordering at a temperature of TN = 3.5 K, with magnetic moments oriented along the c-axis. This aligns with previous observations for YbMgGaO4; however, it deviates from theoretical predictions based on band-structure calculations, which suggested that the ordered moments would be perpendicular to the c-axis. Additionally, we observed evidence of a structural phase transition around T* ~ 80 K, characterized by changes in both the crystal parameters and the unit cell dimensions. These results imply that the crystal structure at reduced temperatures may not perfectly match the high-temperature tetragonal lattice expected from theoretical models. Lastly, we present specific heat measurements indicating significant anomalies linked to both the magnetic ordering and the structural phase transition.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 1.811643254631353
    },
    {
        "original_text": "We present the results for the equation of state (EoS) in hot Quantum Chromodynamics (QCD). We use two different approaches to solve numerically the lattice QCD EoS at finite temperature, namely the Taylor expansion method and the integral method. The latter is based on an exact representation of the pressure as a function of energy density using Padé approximants. In addition we also study the dependence of the EoS on the number of flavors Nf . Finally, we compare our numerical results with those obtained by other authors within various theoretical frameworks. Our main conclusions are that both methods give consistent results which agree well with previous calculations performed in the literature. Moreover, it turns out that the inclusion of strange quarks has only minor effects on the thermodynamic quantities considered here. \n \n Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics",
        "watermark_text": "We present the results for the equation of state ( EoS ) in hot Quantum Chromodynamics ( QCD ) . We use two different methods to solve numerically the lattice QCD EoS at finite temperature , namely the Taylor expansion method and the integral method .The latter is based on an precise representation of the pressure as a function of energy density using Padé approximants . In addition we also study the dependence of the EoS on the quantity of flavors Nf .Finally , we compare our numerical findings with those achieved by other researchers within various theoretical frameworks . Our main results are that both approaches give consistent conclusions which agree well with previous analyses performed in the literature .Moreover , it turns out that the introduction of odd quarks has only minor impacts on the thermodynamic quantities considered here . Keywords : Equation of State ; Heavy Ion Collisions ; Lattice QCD ; Relativistic Hydrodynamics",
        "rewrite_text": "We present our findings on the equation of state (EoS) in hot Quantum Chromodynamics (QCD). To numerically determine the lattice QCD EoS at finite temperature, we employ two distinct approaches: the Taylor expansion method and the integral method. The latter involves an accurate representation of pressure as a function of energy density, utilizing Padé approximants. Additionally, we investigate how the EoS depends on the number of flavors, Nf. We also compare our results with those from other researchers using various theoretical frameworks. Our primary conclusions indicate that both methods yield consistent results that align well with previous literature. Furthermore, we find that the inclusion of odd quarks has a negligible effect on the thermodynamic quantities under consideration. Keywords: Equation of State; Heavy Ion Collisions; Lattice QCD; Relativistic Hydrodynamics.",
        "ori-fast-z-score": 0.13245323570650439,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": 0.2773500981126146
    },
    {
        "original_text": "We study the random spatial growth in two dimensions, where new sites are added to an initially empty square lattice at randomly chosen locations and grow into circular clusters if they do not hit any existing cluster or obstacle site.  We show that this process leads to fractal structures which can be characterized by their fractal dimension Df = 1 + (1 - p)/2p, where p is the probability for adding a new site without hitting an obstacle. The results agree well with numerical simulations. \nPACS numbers: 05.40.+j, 64.60.Cn, 68.35.-k \nI. INTRODUCTORY REMARK\nIn recent years there has been considerable interest in studying various aspects of the so-called Eden model  1  . In its original formulation it describes the growth of a single cluster on a two-dimensional substrate starting from one seed particle. This simple concept was later generalized to include several seeds  2  , as well as different shapes  3  .\nThe present work deals with another generalization of the Eden model: Instead of growing only one cluster we consider the simultaneous growth of many clusters competing for space  4  . As a result, some clusters may become trapped between other clusters leading to a complex pattern formation.",
        "watermark_text": "We research the random spatial growth in two dimensions , where new sites are adding to an initially vacant rectangular matrix at randomly picked areas and grow into circular groups if they do not hit any original cluster or obstacle site . We see that this process results to fractal structures which can be described by their fractal dimension Df = 1 + ( 1 - p ) / 2p , where p is the probability for adding a new site without hits an barrier .The results agree well with numerical simulations . PACS scores : 05 . 40 . + j , 64 . 60 . Cn , 68 . 35 . - k I .INTRODUCTORY REMARK In recent years there has been substantial interest in studying several elements of the so - called Eden model 1 . In its initial formulation it assumes the development of a single cluster on a two - dimensional substrate beginning from one seed particle .This basic concept was afterwards generalized to consider many seeds 2 , as well as varying shapes 3 . The present work deals with another generalization of the Eden model : Instead of growing only one cluster we define the concurrent development of several clusters battling for space 4 .As a result , some clusters might form trapped between other communities leading to a complex pattern formation .",
        "rewrite_text": "We investigate random spatial growth in two dimensions, where new sites are added to an initially empty rectangular matrix at randomly selected locations. These sites expand into circular clusters, provided they do not intersect with existing clusters or obstacles. This process generates fractal structures characterized by the fractal dimension given by Df = 1 + (1 - p) / 2p, where p represents the probability of adding a new site without encountering a barrier. Our findings are consistent with numerical simulations. \n\n**PACS scores:** 05.40.+j, 64.60.Cn, 68.35.-k \n\n**I. INTRODUCTORY REMARK** \n\nIn recent years, there has been significant interest in exploring various aspects of the Eden model. Originally, the model described the growth of a single cluster on a two-dimensional substrate starting from one seed particle. This foundational idea was later expanded to consider multiple seed particles and varied shapes. The current study presents another extension of the Eden model: instead of growing a single cluster, we define the simultaneous development of several clusters competing for space. Consequently, some clusters may become trapped between other communities, resulting in intricate pattern formation.",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 6.464976285134148,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "We study the statistical mechanics of nonlinear on-line learning in ensemble teachers, where each teacher is trained by an independent student population and all students are drawn from the same distribution. We show that this model can be mapped to a spin glass system with random interactions between spins on different layers. Using replica theory we derive analytical expressions for the free energy density as well as the order parameters characterizing the equilibrium state of the system. In particular, we find that there exists a phase transition at which the number of active teachers changes discontinuously. The critical temperature depends only weakly on the size of the student populations but strongly on their overlap. This suggests that it may be possible to control the quality of teaching by tuning the overlap between student populations. Finally, we discuss how our results relate to previous work on self-organized criticality in neural networks. PACS numbers: 05.45.Mt, 89.70.+c",
        "watermark_text": "We research the statistical mechanics of nonlinear on - line learning in ensemble educators , where each teacher is trained by an independent teacher population and all students are drawn from the same distribution . We see that this simulation can be mapped to a spinning glass network with random interactions between spins on various sheets .Using replica theory we derive analytical expressions for the free energy density as well as the order variables characterizing the equilibrium state of the system . In particular , we find that there exists a phase shift at which the number of active teachers shifts discontinuously .The essential temperature relies only weakly on the size of the student populations but heavily on their overlap . This implies that it could be possible to affect the performance of teaching by tuning the overlap between pupil populations .Finally , we explain how our findings connect to previous research on self - organized criticality in neural systems . PACS scores : 05 . 45 . Mt , 89 . 70 . + c",
        "rewrite_text": "We investigate the statistical mechanics of nonlinear online learning within an ensemble of educators, where each teacher is trained independently and all students come from the same distribution. Our simulations can be related to a spin glass model, featuring random interactions among spins across multiple layers. Utilizing replica theory, we derive analytical expressions for both the free energy density and the order parameters that characterize the system's equilibrium state. Notably, we discover a phase transition where the number of active teachers changes abruptly. The critical temperature is found to be only weakly dependent on the size of the student populations, but strongly influenced by their overlap. This suggests that the performance of teaching could potentially be optimized by adjusting the overlap among student groups. Lastly, we discuss how our results relate to prior research on self-organized criticality in neural systems. PACS numbers: 05.45.Mt, 89.70.+c.",
        "ori-fast-z-score": -1.0256451881367414,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 1.162476387438193
    },
    {
        "original_text": "We present the results for neutrino mixing angles, CP violating phases and mass squared differences obtained by using the Fritzsch ansatz to generate the charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). We find that this model can accommodate all experimental data with only one free parameter which is related to the ratio between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati-Salam gauge symmetry into the Standard Model gauge group. The CKM matrix elements are predicted as well as the Majorana phase associated with leptonic CP violation. In addition we show how the tri-bimaximal pattern observed experimentally in the lepton sector emerges naturally when the quark-lepton unification hypothesis is imposed at high energies. Finally we discuss briefly some phenomenological consequences of our scenario such as neutrinoless double beta decay and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "watermark_text": "We present the results for neutrino mixing angles , CP violating stages and mass squared variations obtained by using the Fritzsch ansatz to produce the charged fermion masses within an extended supersymmetric grand unified theory based on SO ( 10 ) . We see that this description can handle all theoretical data with only one free parameter which is related to the proportion between the two vacuum expectation values of the Higgs fields responsible for breaking down the Pati - Salam gauge symmetry into the Standard Model gauge group .The CKM matrix elements are expected as well as the Majorana process associated with leptonic CP violation . In addition we show how the tri - bimaximal pattern found experimentally in the lepton region arises readily when the quark - lepton unification theory is imposed at high energies .Finally we talk briefly some phenomenological consequences of our scenario such as neutrinoless double alpha emission and proton decay . PACS scores : 11 . 30 . Pb , 12 . 60 . Cn",
        "rewrite_text": "We present findings on neutrino mixing angles, stages of CP violation, and variations in mass squared, derived from the Fritzsch ansatz which generates charged fermion masses within an extended supersymmetric grand unified theory based on SO(10). Our analysis shows that this framework can accommodate all theoretical data with just one free parameter, linked to the ratio of the two vacuum expectation values of the Higgs fields that break the Pati-Salam gauge symmetry down to the Standard Model gauge group. We predict the elements of the CKM matrix and discuss the associated Majorana processes linked to leptonic CP violation. Furthermore, we demonstrate how the tri-bimaximal pattern observed experimentally in the lepton sector naturally emerges when we apply the quark-lepton unification theory at high energies. Finally, we briefly address some phenomenological implications of our model, including neutrinoless double-alpha emissions and proton decay. PACS numbers: 11.30.Pb, 12.60.Cn",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "We present new high-resolution, near-infrared (NIR) spectra for the coolest known members of open clusters M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory. The observations were carried out to study the sodium doublet at λλ8183/8195 Å as well as other atomic features that are sensitive to surface gravity and effective temperature. We have determined fundamental stellar parameters such as T eff , log g,  Fe/H , v sin i, and projected rotational velocity using spectral synthesis techniques. Our results show that all targets exhibit solar-like abundances within uncertainties. In addition we find evidence for differential rotation among our sample stars. Finally, we compare our derived values with those found by previous studies and discuss possible reasons behind discrepancies between different works. \n \n Keywords: Near-infrared spectroscopy, Open cluster, Surface gravity, Differential rotation, Fundamental parameters",
        "watermark_text": "We report new high - resolution , near - infrared ( NIR ) spectra for the coolest known members of open complexes M67 and NGC 2516 obtained with the Phoenix spectrograph on Gemini South Observatory . The images were carried out to study the sodium doublet at λλ8183 / 8195 Å as well as other molecular properties that are subject to surface gravity and effective heat .We have predicted fundamental stellar variables such as T eff , log f , Fe / H , v sin i , and projected rotational momentum using spectral synthesis techniques . Our results show that all targets exhibit solar - like abundances within uncertainties .In addition we find data for differential rotation among our sample stars . Finally , we compare our derived values with those detected by earlier surveys and consider alternative causes behind discrepancies between various works .Keywords : Near - infrared spectroscopy , Open cluster , Surface gravity , Differential rotation , Fundamental parameters",
        "rewrite_text": "We present new high-resolution near-infrared (NIR) spectra of the coolest known stars in the open clusters M67 and NGC 2516, obtained using the Phoenix spectrograph at the Gemini South Observatory. These observations were conducted to analyze the sodium doublet at λλ8183/8195 Å, as well as other molecular characteristics influenced by surface gravity and effective temperature. Using spectral synthesis techniques, we have estimated fundamental stellar parameters such as effective temperature (T_eff), surface gravity (log g), metallicity ([Fe/H]), rotational velocity (v sin i), and projected angular momentum. Our findings indicate that all targets display solar-like abundances within the measurement uncertainties. Additionally, we observe evidence of differential rotation among the stars in our sample. Finally, we compare our results with those from previous studies and explore possible explanations for discrepancies found in the literature. \n\nKeywords: Near-infrared spectroscopy, Open cluster, Surface gravity, Differential rotation, Fundamental parameters",
        "ori-fast-z-score": 0.5,
        "water-fast-z-score": 4.5,
        "rewrite-fast-z-score": -0.48507125007266594
    },
    {
        "original_text": "We demonstrate that it is possible to generate all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths, which are generated via second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO consists of a periodically poled lithium niobate (PPLN) crystal as nonlinear medium and a concave mirror for cavity feedback. We show experimentally that this approach allows us to obtain high-visibility quantum interference between photons emitted at degenerate wavelength pairs across the entire PPLN acceptance bandwidth. This method can be used to simplify future experiments on continuous-variable entanglement distribution over large distances. \n \n Quantum information processing requires the ability to create and manipulate entangled states of light. In particular, the Bell state measurement plays a key role in many applications such as teleportation or quantum repeaters  1  . However, generating these highly nonclassical states is challenging because they require indistinguishable photon pairs  2  , which cannot be produced deterministically  3  .\nIn recent years, several approaches have been developed to overcome this problem  4  . One possibility is based on spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons  5  . By adjusting the relative phases of the pump fields  6  , it has become possible to produce any desired superposition of the four Bell states  7, 8  . Another option uses squeezed vacuum states  9  or displaced number states  10  instead of coherent laser pulses  11  . These methods allow for efficient generation of entangled states but usually suffer from low visibility due to imperfections  12  .",
        "watermark_text": "We suggest that it is easy to create all four Bell states in one single nonlinear crystal by using two pump beams with orthogonal polarizations and slightly different wavelengths , which are produced via second - harmonic production ( SHG ) inside an optical parametric oscillator ( OPO ) . The OPO consists of a periodically poled lithium niobate ( PPLN ) crystal as nonlinear material and a concave frame for cavity feedback .We suggest experimentally that this methodology allows us to obtain high - visibility particle interference between photons generated at degenerate intensity pairs across the entire PPLN acceptance bandwidth . This method can be used to simplify future research on continuous - variable entanglement flow over large distances .Quantum knowledge processing requires the ability to create and manipulate entangled states of light . In particular , the Bell state measurement plays a key importance in many applications such as teleportation or quantum repeaters 1 .However , constructing these strongly nonclassical states is problematic because they use indistinguishable photon pairs 2 , which cannot be made deterministically 3 . In recent years , various approaches have been formulated to overcome this situation 4 .One possibility is based on spontaneous parametric down - transfer ( SPDC ) , where a pump beam creates correlated pairs of signal and idler photons 5 . By adjusting the relative modes of the pump fields 6 , it has become able to produce any desired superposition of the four Bell states 7 , 8 .Another option uses squeezed vacuum states 9 or displaced number states 10 instead of coherent beam waves 11 . These methods provide for efficient production of entangled states but typically suffer from small brightness due to imperfections 12 .",
        "rewrite_text": "We propose that all four Bell states can be easily generated within a single nonlinear crystal using two pump beams with orthogonal polarizations and slightly varying wavelengths, which are created through second-harmonic generation (SHG) inside an optical parametric oscillator (OPO). The OPO features a periodically poled lithium niobate (PPLN) crystal as its nonlinear medium, along with a concave frame for cavity feedback. Experimental evidence suggests that this approach enables high-visibility particle interference between photons produced at degenerate intensity pairs throughout the entire PPLN acceptance bandwidth. This technique holds potential for simplifying future studies on continuous-variable entanglement over extended distances. Quantum information processing hinges on the capability to create and manipulate entangled light states, with Bell state measurements playing a crucial role in various applications, such as teleportation and quantum repeaters. However, producing these highly nonclassical states poses challenges because they rely on indistinguishable photon pairs, which cannot be generated deterministically. In recent years, several strategies have emerged to address this issue. One such strategy involves spontaneous parametric down-conversion (SPDC), where a pump beam creates correlated pairs of signal and idler photons. By fine-tuning the relative modes of the pump fields, it becomes feasible to produce any desired superposition of the four Bell states. Alternatively, some approaches employ squeezed vacuum states or displaced number states instead of coherent beams. While these methods facilitate the efficient production of entangled states, they often suffer from limited brightness due to inherent imperfections.",
        "ori-fast-z-score": 0.7302967433402214,
        "water-fast-z-score": 6.818181818181818,
        "rewrite-fast-z-score": -0.9053574604251853
    },
    {
        "original_text": "We present new results on the outer jets of the symbiotic star, R Aqr (=V1016 Cyg). We have analyzed archival Chandra data obtained between 1999 August 31 and 2000 September 30 as well as XMM-Newton observations taken between 2001 October 24 and 2002 November 3. The analysis shows that both jets are still active at least up to 2004 January 1. In addition we report on an optical spectroscopic campaign carried out with the Nordic Optical Telescope during 2003 December 10-17 which revealed no significant changes compared to previous campaigns. Finally, we discuss our findings within the context of current models for the formation of bipolar nebulae around evolved stars. Keywords: Symbiosis, Jets, Bipolar Nebulae, Stellar Winds, Mass Ejection, Binary Star Systems, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg",
        "watermark_text": "We report new data on the exterior jets of the symbiotic star , R Aqr ( = V1016 Cyg ) . We have analyzed archival Chandra data acquired between 1999 August 31 and 2000 September 30 as well as XMM - Newton images took between 2001 October 24 and 2002 November 3 .The evaluation indicates that both jets are still active at least up to 2004 January 1 . In addition we publish on an optical spectroscopic campaign conducted out with the Nordic Optical Telescope during 2003 December 10 - 17 which revealed no major changes compared to previous efforts .Finally , we explain our findings within the context of recent models for the formation of bipolar nebulae around evolved stars . Keywords : Symbiosis , Jets , Bipolar Nebulae , Stellar Winds , Mass Ejection , Binary Star Systems , Chandra Observatory , XMM - Newton Observatory , R Aquarius , V1016 Cyg",
        "rewrite_text": "We present new findings on the external jets of the symbiotic star R Aqr (also known as V1016 Cyg). Our analysis includes archival Chandra data collected from August 31, 1999, to September 30, 2000, as well as XMM-Newton images obtained between October 24, 2001, and November 3, 2002. The assessment demonstrates that both jets remained active at least until January 1, 2004. Additionally, we report results from an optical spectroscopic campaign conducted with the Nordic Optical Telescope from December 10 to 17, 2003, which showed no significant changes compared to previous studies. Finally, we discuss our results in the context of recent models related to the formation of bipolar nebulae around evolved stars. Keywords: Symbiosis, Jets, Bipolar Nebulae, Stellar Winds, Mass Ejection, Binary Star Systems, Chandra Observatory, XMM-Newton Observatory, R Aquarius, V1016 Cyg.",
        "ori-fast-z-score": 0.1643989873053573,
        "water-fast-z-score": 4.8666426339228765,
        "rewrite-fast-z-score": -0.3086066999241838
    },
    {
        "original_text": "We present an algorithm to find galaxy groups using photometric redshifts, which is based on the Voronoi tessellation technique (VT). The VT method has been widely used for identifying clusters of galaxies with spectroscopic redshifts but it had not yet been applied to identify galaxy groups with photometric redshifts. We use the Sloan Digital Sky Survey data release five (SDSS DR5) as our input sample. Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available. In this work we have identified more than 12000 galaxy groups at 0 < z < 0.3. These groups contain about 30000 member galaxies. We also provide a catalog containing basic information such as positions, magnitudes, colors, and photometric redshifts for all these groups. \n \n Keywords: Galaxy Group, Photometric Redshift",
        "watermark_text": "We present an algorithm to find galaxy groups using photometric redshifts , which is based on the Voronoi tessellation method ( VT ) . The VT method has been widely using for finding clusters of stars with spectroscopic redshifts but it had not already been used to identify galaxy groups with photometric redshifts .We use the Sloan Digital Sky Survey data release five ( SDSS DR5 ) as our input data . Our results show that the VT method can be successfully applied to identify galaxy groups even when only photometric redshifts are available .In this research we have discovered more than 12000 star groups at 0 < z < 0 . 3 . These groups contain about 30000 member galaxies .We additionally offer a list containing essential information such as positions , magnitudes , colors , and photometric redshifts for all these groups . Keywords : Galaxy Group , Photometric Redshift",
        "rewrite_text": "We introduce an algorithm for identifying galaxy groups utilizing photometric redshifts, employing the Voronoi tessellation method (VT). Although the VT method has been extensively used for detecting star clusters based on spectroscopic redshifts, it has not previously been applied to locate galaxy groups using photometric redshifts. Our analysis utilizes data from the Sloan Digital Sky Survey, specifically data release five (SDSS DR5). Our findings indicate that the VT method is effective for identifying galaxy groups, even in the absence of spectroscopic redshifts. In this study, we have identified over 12,000 star groups within the redshift range of 0 < z < 0.3, which include approximately 30,000 member galaxies. Furthermore, we provide a comprehensive list containing key details such as positions, magnitudes, colors, and photometric redshifts for all identified groups. Keywords: Galaxy Group, Photometric Redshift.",
        "ori-fast-z-score": -1.2602520756252087,
        "water-fast-z-score": 3.2206441932644223,
        "rewrite-fast-z-score": -1.2135597524338357
    },
    {
        "original_text": "We present new results on interstellar absorption lines toward early type stars observed with UVES at VLT, as part of the ESO-POP project (ESO program 085.D-0571). We have searched for high-velocity clouds (HVCs) by looking for blueshifted components in the MgII doublet line profiles. The sample consists of 16 OB-stars located within 1 kpc distance from Earth. In addition to previously known HVCs we find several new ones. Some of these are associated with nearby galaxies while others may be related to Galactic halo gas. A comparison between our data set and previous surveys shows that there is no significant difference in the number density distribution of HVCs along different sightlines. This suggests that most of them are small structures which do not cover much solid angle around their host galaxy or star. \n \n Keywords: Interstellar medium",
        "watermark_text": "We release new results on interstellar absorption lines toward early type stars observed with UVES at VLT , as part of the ESO - POP project ( ESO program 085 . D - 0571 ) . We have searched for high - velocity clouds ( HVCs ) by looking for blueshifted components in the MgII doublet line profiles .The sample consists of 16 OB - stars situated within 1 kpc radius from Earth . In addition to formerly notable HVCs we find several new ones .Some of these are identified with nearby galaxies while several might be connected to Galactic halo gas . A comparison between our information set and previous searches reveals that there is no considerable difference in the number density spread of HVCs along various sightlines .This implies that most of them are small structures which do not cover many solid angle around their target galaxy or star . Keywords : Interstellar medium",
        "rewrite_text": "We present new findings on interstellar absorption lines toward early-type stars observed with UVES at the VLT, as part of the ESO-POP project (ESO program 085.D-0571). Our investigation focused on identifying high-velocity clouds (HVCs) by detecting blueshifted components in the MgII doublet line profiles. The sample includes 16 OB stars located within 1 kpc of Earth. Alongside previously documented HVCs, we have discovered several new ones, some of which are associated with nearby galaxies, while others may be linked to gas in the Galactic halo. A comparison of our dataset with earlier studies shows no significant difference in the density distribution of HVCs along various sightlines, suggesting that most of these clouds are small structures that do not cover a large area around their respective target galaxy or star. Keywords: Interstellar medium.",
        "ori-fast-z-score": -1.6644794391276478,
        "water-fast-z-score": 2.4327007187250236,
        "rewrite-fast-z-score": -1.4320780207890627
    },
    {
        "original_text": "We study the second-harmonic (SH) and third-harmonic (TH) generations in an inhomogeneously broadened cavity with two different resonance frequencies for fundamental wave (FW). We show that, when the intracavity FW intensity reaches its maximum value, both SHG and THG can be enhanced simultaneously by increasing the pumping rate or decreasing the detuning between the two modes. The enhancement is due to the fact that the nonlinear susceptibility becomes larger than zero at this point. This effect has been observed experimentally recently. \n \n In addition, we find that there exists another regime where only one type of harmonics can be generated efficiently while suppressing other types of harmonics. For example, if the intracavity FW field is tuned close to the lower mode frequency, then it will generate mainly SH light but very little TH light; on the contrary, if the intracavty FW field is tuned near the upper mode frequency, then it generates mainly TH light but almost no SH light.",
        "watermark_text": "We test the second - harmonic ( SH ) and fifth - harmonic ( TH ) generations in an inhomogeneously broadened cavity with two different resonance energies for fundamental wave ( FW ) . We suggest that , when the intracavity FW frequency exceeds its highest value , both SHG and THG can be enhanced simultaneously by expanding the pumping rate or decreasing the detuning between the two modes .The enhancement is due to the fact that the nonlinear susceptibility grows larger than zero at this time . This phenomenon has been observed experimentally recently .In addition , we find that there exists another regime where only one kinds of harmonics can be generated smoothly while suppressing other types of harmonics . For instance , if the intracavity FW field is tuned close to the higher mode wavelength , then it will generate mostly SH light but very less TH radiation ; on the contrary , if the intracavty FW field is tuned near the higher mode wavelength , then it generates largely TH radiation but almost no SH light .",
        "rewrite_text": "We investigate the second-harmonic generation (SHG) and fifth-harmonic generation (THG) within an inhomogeneously broadened cavity that features two distinct resonance energies for the fundamental wave (FW). Our analysis suggests that when the intracavity FW frequency surpasses its maximum value, both SHG and THG can be simultaneously enhanced by increasing the pumping rate or reducing the detuning between the two modes. This enhancement occurs because the nonlinear susceptibility becomes greater than zero during this period, and this phenomenon has been recently observed in experiments. Furthermore, we identify a different regime in which only one type of harmonic can be effectively generated while simultaneously suppressing the other types. For example, if the intracavity FW field is adjusted to be near the wavelength of the higher mode, it predominantly produces SH light with minimal TH radiation; conversely, if the intracavity FW field is tuned close to the lower mode wavelength, it primarily generates TH radiation while producing very little SH light.",
        "ori-fast-z-score": -1.889822365046136,
        "water-fast-z-score": 3.9056328877620143,
        "rewrite-fast-z-score": -1.5882027766319677
    },
    {
        "original_text": "We present an atlas for the phase space of all possible motions in two dimensions of a class of conservative Lagrangian mechanical systems, which are characterized by having one first and one second quadratic integral of motion. The construction is based on the existence of a special type of canonical transformation that maps each system into another one whose trajectories can be obtained explicitly as solutions to quadratures. We show how this method allows us to obtain explicit expressions for the general solution of some particular problems such as those corresponding to the harmonic oscillator or the Kepler problem. \nThe results presented here constitute a generalization of previous works dealing only with integrable cases. They also provide new insights about the structure of these types of systems. In addition we discuss several examples illustrating our approach. This article is part of a series of papers devoted to the study of the dynamics of certain classes of physical systems described by means of Hamiltonians depending on time through periodic functions (see  1  , 2 ).",
        "watermark_text": "We create an atlas for the phase space of all possible motions in two dimensions of a class of conservative Lagrangian mechanical models , which are characterized by having one second and one second quadratic integral of movement . The construction is based on the existence of a unique kind of canonical conversion that connects each system into another one whose trajectories can be obtained explicitly as solutions to quadratures .We see how this process allows us to obtain precise expressions for the general solution of some particular problems such as those corresponding to the harmonic oscillator or the Kepler problem . The results presented here represent a generalization of previous works dealing only with integrable cases .They addition offer additional knowledge about the organization of these kinds of structures . In addition we explain several examples illustrating our approach .This section is part of a sequence of papers focused to the study of the dynamics of certain classes of physical processes characterized by means of Hamiltonians varying on time through periodic functions ( saw 1 , 2 ) .",
        "rewrite_text": "We develop an atlas for the phase space of all possible motions in two-dimensional conservative Lagrangian mechanical models, which are distinguished by possessing one quadratic integral of movement. Our construction is grounded in the existence of a unique type of canonical transformation that links each system to another, for which the trajectories can be explicitly obtained as solutions to quadratures. This process enables us to derive precise expressions for the general solutions to specific problems, such as those related to the harmonic oscillator and the Kepler problem. The findings presented here expand upon previous research that focused solely on integrable cases, providing deeper insights into the organization of these structures. Additionally, we illustrate our approach through several examples. This work is part of a series of papers dedicated to exploring the dynamics of certain classes of physical processes characterized by Hamiltonians that vary over time according to periodic functions (see Papers 1, 2).",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 2.092457497388747
    },
    {
        "original_text": "We study the simple shear flow for two different types of inelastic Maxwell models, one with constant restitution coefficient and another with velocity dependent restitution coefficient. We find that there is no steady state solution when the system has only elastic collisions but it exists if we include some dissipative effects. The steady state solutions are found to be independent of initial conditions as well as the type of dissipation included. In addition, we also show that these steady states are linearly stable under small perturbations. Finally, we present results on the relaxation time scales associated with this problem which can be used to estimate the rate at which the system approaches its steady state. \n \n Introduction:- \n \n Simple shear flows have been studied extensively both theoretically  1 - 3  and experimentally  4 - 6  . These studies were motivated by the fact that such flows occur naturally in many physical systems like granular materials  7  , colloidal suspensions  8  etc.. It was shown recently  9  -  11  that even though the microscopic dynamics may be very complicated (like in case of granular particles), the macroscopic behaviour of the system could still be described using relatively simpler kinetic equations like Boltzmann equation or Enskog equation  12  . However, most of these works deal with elastic interactions between the particles while the effect of dissipation is usually ignored. This assumption is not always valid especially in cases where the particles interact via soft potentials  13  .\n \nIn recent years, several authors  14  -  16  have considered the effect of dissipation on various properties of dilute gases. For example, in Ref.  17  , the author considers an inelastic gas consisting of identical hard spheres interacting through a repulsive potential and shows how the presence of dissipation affects the transport coefficients of the system. On the other hand, in Refs.  18  -  20  , the authors consider a model consisting of point particles interacting via a pairwise additive potential and derive expressions for the transport coefficients of the corresponding fluid. They then use these expressions to calculate the viscosity and thermal conductivity of the system.",
        "watermark_text": "We explore the simple shear flow for two different kinds of inelastic Maxwell systems , one with constant restitution coefficient and another with velocity dependent restitution coefficient . We see that there is no steady state solution when the system has only elastic collisions but it exists if we involve some dissipative effects .The stable state solutions are found to be independent of initial conditions as well as the kind of dissipation included . In addition , we also prove that these steady states are linearly stable under small perturbations .Finally , we present results on the relaxation time ranges associated with this situation which can be used to estimate the speed at which the system approaches its steady state . Introduction : - Simple shear flows have been studied thoroughly both theoretically 1 - 3 and experimentally 4 - 6 .These studies were driven by the fact that such flows act naturally in many mechanical environments like granular materials 7 , colloidal suspensions 8 etc . . It was shown recently 9 - 11 that even though the microscopic behavior may be very complicated ( like in case of granular particles ) , the macroscopic nature of the system could still be described using relatively simpler kinetic equations like Boltzmann equation or Enskog function 12 .However , most of these works treat with elastic interactions between the molecules while the impact of dissipation is usually neglected . This assumption is not always legitimate especially in cases where the atoms interact via soft potentials 13 .In recent years , various literature 14 - 16 have explored the impact of dissipation on various properties of dilute gases . For instance , in Ref .17 , the writer describes an inelastic gas consisting of corresponding hard particles interacting through a repulsive potential and shows how the presence of dissipation influences the travel coefficients of the system . On the other hand , in Refs .18 - 20 , the authors take a theory consisting of point particles interacting via a pairwise additive potential and derive expressions for the travel coefficients of the associated fluid . They then use these expressions to estimate the viscosity and thermal conductivity of the system .",
        "rewrite_text": "We investigate simple shear flow in two types of inelastic Maxwell systems: one characterized by a constant restitution coefficient and the other by a restitution coefficient that depends on velocity. Our findings indicate that no steady-state solution exists when the system undergoes purely elastic collisions; however, introducing some dissipative effects allows for steady-state solutions to emerge. These stable states are shown to be independent of initial conditions and the specific form of dissipation included. Additionally, we demonstrate that these steady states exhibit linear stability in response to small perturbations. Lastly, we present data on the ranges of relaxation times pertinent to this scenario, which can be utilized to estimate the rate at which the system approaches its steady state.\n\nIntroduction: Simple shear flows have been extensively studied both theoretically and experimentally due to their natural occurrence in various mechanical environments, such as granular materials and colloidal suspensions. Recent research has revealed that, despite the complex microscopic behavior, such as that seen in granular particles, the macroscopic characteristics of the system can still be effectively described using simpler kinetic equations like the Boltzmann equation or the Enskog function. However, much of this research has focused on elastic interactions between molecules while often overlooking the effects of dissipation, an assumption that may not hold in cases where atoms interact through soft potentials. Recent literature has begun to address the role of dissipation in the properties of dilute gases. For example, one study examines an inelastic gas composed of hard particles interacting via a repulsive potential, illustrating how dissipation affects the system's transport coefficients. Conversely, other works derive expressions for the transport coefficients of fluids composed of point particles interacting through a pairwise additive potential, subsequently using these expressions to estimate the system's viscosity and thermal conductivity.",
        "ori-fast-z-score": -0.32659863237109044,
        "water-fast-z-score": 6.651078266361265,
        "rewrite-fast-z-score": 1.2543630150106362
    },
    {
        "original_text": "We study the ground-state properties of two-flavor color superconducting (2SC) quark matter at finite density and temperature by using an effective chiral model with vector interaction, which is derived from QCD under the mean-field approximation. We find that there exists a new type of 2SC phase where quarks are paired into diquark condensates with different colors but same flavor. This novel phase has been named as Larkin-Ovchinnikova-Fulde-Ferreell (LOFF) state because it was first proposed to describe superfluidity in nuclear systems. In this LOFF state, we show that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle. The magnitude of the gap decreases rapidly when they move away from each other along the Fermi surface. As a result, the energy gap vanishes completely near the boundary of the Brillouin zone.",
        "watermark_text": "We research the ground - state properties of two - flavor color superconducting ( 2SC ) quark matter at finite density and heat by using an efficient chiral description with vector coupling , which is generated from QCD under the mean - field approximation . We see that there exists a new kind of 2SC phase where quarks are paired into diquark condensates with various shades but same flavor .This novel mode has been called as Larkin - Ovchinnikova - Fulde - Ferreell ( LOFF ) state because it was first suggested to explain superfluidity in nuclear systems . In this LOFF state , we find that the gap parameter for pairing between quarks with opposite momenta depends on their relative angle .The magnitude of the gap falls strongly when they go away from each other along the Fermi surface . As a result , the power gap vanishes totally near the boundary of the Brillouin zone .",
        "rewrite_text": "We investigate the ground-state properties of two-flavor color superconducting (2SC) quark matter at finite density and temperature using a robust chiral framework with vector coupling, derived from QCD under the mean-field approximation. Our findings reveal a novel type of 2SC phase in which quarks pair into diquark condensates that share the same flavor but differ in orientation. This distinctive mode is referred to as the Larkin-Ovchinnikov-Fulde-Ferrell (LOFF) state, originally proposed to account for superfluidity in nuclear systems. Within the LOFF state, we observe that the pairing gap between quarks with opposite momenta is influenced by their relative angles. Specifically, the gap magnitude decreases significantly as the quarks move apart along the Fermi surface, ultimately disappearing entirely near the boundary of the Brillouin zone.",
        "ori-fast-z-score": -1.6924558427507104,
        "water-fast-z-score": 3.5151005964822444,
        "rewrite-fast-z-score": 1.0504514628777804
    },
    {
        "original_text": "We report on the discovery of an intervening galaxy with a mass M = 1011.5 ± 0.3M⊙ and size R = 1.7 ± 0.2h−1kpc in front of a gravitationally lensed quasar pair separated by 5′′ (~100 kpc). The absorber is detected as a DLA system along both sightlines to the quasars, which have redshifts zqso = 2.962 and zqso = 2. . We use this object to constrain the typical sizes of high-z galaxies. Our results suggest that these objects were typically smaller than their local counterparts when they formed most of their stars. This may be related to the fact that massive galaxies grow through mergers over cosmic time. \n \n Keywords: Galaxy evolution, Quasars, Absorbers, Massive black holes \n \n \n \n High-redshift quasars provide powerful probes for studying the physical properties of distant galaxies. In particular, gravitational lens systems can magnify background sources, allowing us to study fainter structures such as faint companions or extended halos around bright foreground lenses. Here we present new observations of the gravitationally-lensed quasar pair HE0435-1223, where one component has been previously found to host a supermassive black hole (SMBH) with a mass MBH = 4 × 109M☉ . Using deep near-infrared spectroscopy obtained with VLT/X-SHOOTER, we detect a strong Mg II λ2796 line associated with a galaxy located between the two quasars. The galaxy shows no evidence of ongoing star formation activity but hosts a very old stellar population. Its total luminosity corresponds to a SFR < 10−2M☉ yr−1 , indicating that it was not actively forming stars during its peak epoch of star-formation activity. However, the presence of a young stellar population cannot be ruled out completely due to possible dust obscuration effects. From our analysis, we find that the galaxy has a mass M = 1011+0.3−0.4M☉ and radius R =",
        "watermark_text": "We report on the discovery of an intervening galaxy with a mass M = 1011 . 5 ± 0 . [UNK] and size R = 1 . 7 ± 0 . 2h−1kpc in front of a gravitationally lensed quasar pair divided by 5 ′ ′ ( ~ 100 kpc ) . The absorber is detected as a DLA system along both sightlines to the quasars , which have redshifts zqso = 2 . 962 and zqso = 2 . .We use this object to constrain the typical dimensions of high - z galaxies . Our results show that these objects were generally tiny than their nearby rivals when they formed most of their stars .This might be connected to the fact that powerful nuclei grow through mergers over universe time . Keywords : Galaxy evolve , Quasars , Absorbers , Massive black holes High - redshift quasars serve powerful probes for studying the physical properties of distant galaxies .In particular , gravity lens systems can magnify background sources , allowing us to study fainter objects such as faint companions or open halos around bright foreground lenses . Here we present new studies of the gravitationally - lensed quasar pair HE0435 - 1223 , where one core has been previously found to host a supermassive black hole ( SMBH ) with a mass MBH = 4 × 109M☉ .Using deep near - infrared spectroscopy acquired with VLT / X - SHOOTER , we locate a powerful Mg II λ2796 point linked with a galaxy located between the two quasars . The galaxy displays no evidence of ongoing galaxy formation activity but hosts a very ancient stellar community .Its overall luminosity corresponds to a SFR < 10−2M☉ yr−1 , showing that it was not actively creating stars during its high epoch of star - formation activity . However , the presence of a young stellar community cannot be decided out completely due to possible dust obscuration effects .From our analysis , we find that the universe has a mass M = 1011 + 0 . 3−0 . 4M☉ and radius R =",
        "rewrite_text": "We report the discovery of an intervening galaxy with a mass of \\( M = 10^{11.5 \\pm 0.1} \\) and a size of \\( R = 1.7 \\pm 0.2 h^{-1} \\) kpc, located in front of a gravitationally lensed quasar pair separated by approximately 5 arcminutes (~100 kpc). The galaxy is identified as a Damped Lyman Alpha (DLA) system along both sightlines to the quasars, which have redshifts of \\( z_{qso} = 2.962 \\) and \\( z_{qso} = 2.5 \\). We use this discovery to refine our understanding of the typical dimensions of high-redshift galaxies. Our findings indicate that these early galaxies were generally smaller than their present-day counterparts when they formed the majority of their stars. This may be related to the growth of powerful nuclei through mergers over cosmic time. The keywords include galaxy evolution, quasars, absorbers, and massive black holes. High-redshift quasars serve as valuable tools for probing the physical properties of distant galaxies. In particular, gravitational lensing systems can amplify background sources, enabling the study of fainter objects, such as dim companions or diffuse halos surrounding bright foreground lenses. In this context, we present new observations of the gravitationally lensed quasar pair HE0435-1223, where one core has previously been identified to harbor a supermassive black hole (SMBH) with a mass \\( M_{BH} = 4 \\times 10^9 M_{\\odot} \\). Utilizing deep near-infrared spectroscopy from VLT/X-SHOOTER, we detected a strong Mg II \\( \\lambda2796 \\) emission associated with a galaxy situated between the two quasars. While the galaxy shows no signs of active star formation, it possesses an ancient stellar population. Its overall luminosity corresponds to a star formation rate (SFR) of less than \\( 10^{-2} M_{\\odot} \\) yr\\(^{-1}\\), indicating it was not actively forming stars during its peak star formation period. However, we cannot entirely rule out the presence of a young stellar population due to potential dust obscuration effects. Our analysis yields a total mass for the universe of \\( M = 10^{11 +0.3 -0.4} M_{\\odot} \\) and a radius of \\( R = \\).",
        "ori-fast-z-score": 1.9917864129354077,
        "water-fast-z-score": 8.215838362577491,
        "rewrite-fast-z-score": 0.7016464154456235
    },
    {
        "original_text": "We have analyzed the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and found evidence for intrinsic redshift components in both cases.  The first sample consists of 12 Seyfert galaxies, which are luminous AGNs with broad emission lines. We find that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose amplitude is correlated with the widths of the broad emission lines. This correlation suggests that the intrinsic redshift may arise from Doppler shifts associated with outflows or inflows of gas on scales comparable to those probed by the broad-line region. The second sample contains low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars show no obvious signs of being gravitationally lensed but do exhibit significant intrinsic redshift components. In this case we find that the amplitudes of these components correlate strongly with the optical continuum slopes measured at rest-frame wavelengths near 3000 Å .",
        "watermark_text": "We have analyzed the redshifts of two specimens of active galactic nuclei ( AGNs ) with varying luminosities and found proof for intrinsic redshift components in both cases . The first specimen consists of 12 Seyfert galaxies , which are luminous AGNs with broad absorption paths .We see that their observed redshifts can be decomposed into an extrinsic component due to gravitational lensing by foreground objects and an intrinsic component whose intensity is associated with the widths of the broad emission lines . This correlation suggests that the intrinsic redshift may arise from Doppler movements related with outflows or inflows of gas on scales similar to those probed by the broad - line region .The second survey features small - luminosity quasars chosen from the Sloan Digital Sky Survey Data Release 4 quasar catalog . These quasars exhibit no evident indication of being gravitationally lensed but do exhibit substantial intrinsic redshift components .In this situation we find that the amplitudes of these systems correlate strongly with the optical continuum curves observed at rest - mirror wavelengths near 3000 Å .",
        "rewrite_text": "We have studied the redshifts of two samples of active galactic nuclei (AGNs) with different luminosities and have identified evidence of intrinsic redshift components in both cases. The first sample includes 12 Seyfert galaxies, which are bright AGNs characterized by broad absorption lines. Our analysis shows that their observed redshifts can be separated into an extrinsic part caused by gravitational lensing from foreground objects and an intrinsic part related to the widths of the broad emission lines. This relationship indicates that the intrinsic redshift could be linked to Doppler motions associated with gas outflows or inflows at scales comparable to those of the broad-line region. The second study focuses on low-luminosity quasars selected from the Sloan Digital Sky Survey Data Release 4 quasar catalog. These quasars do not show clear signs of gravitational lensing but exhibit significant intrinsic redshift components. In this case, we find a strong correlation between the amplitudes of these systems and the optical continuum profiles observed at rest-frame wavelengths around 3000 Å.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": -1.0681034923744679
    },
    {
        "original_text": "We present an approach to validating the performance of machine-learning algorithms for identifying modules in networks, based on synthetic datasets generated by simulating random walks through known modular structures. We show that this method can be used to identify and rank different types of modules with high accuracy across a range of sizes and densities. The results are robust against noise and missing links. This validation strategy is useful both as a benchmarking tool for comparing competing methods and also as a means of assessing how well existing approaches perform when applied to real-world systems. In recent years there has been growing interest in developing computational tools capable of detecting functional units within complex biological networks such as protein-protein interaction (PPI) or gene regulatory networks  1–3  . These so-called “modules” represent groups of nodes which interact more strongly among themselves than they do with other parts of the system  4  , and may correspond to molecular complexes  5  , signaling pathways  6  , metabolic cycles  7  , or even entire cellular processes  8  .\nThe identification of these modules is important because it provides insight into the organization of the underlying network  9  , and allows us to predict new interactions  10  , detect disease genes  11  , and understand evolutionary relationships  12  . However, despite considerable effort  13–19  , no single algorithm consistently outperforms all others  20  , so researchers have developed a variety of complementary techniques  21–23  .",
        "watermark_text": "We present an way to validating the performance of machine - computing methods for finding modules in networks , using on synthetic datasets generated by simulating random runs through known modular structures . We see that this algorithm can be used to identify and rank different kinds of modules with high sensitivity across a range of sizes and densities .The results are robust against noise and lost connections . This validation methodology is important both as a benchmarking tool for evaluating rival techniques and also as a means of assessing how well existing techniques perform when applied to real - time systems .In past decades there has been growing interest in establishing computational tools capable of detecting functional units within complex biological groups such as protein - gene interaction ( PPI ) or protein regulatory circuits 1 – 3 . These so - called “ modules ” denote groups of nodes which interact more strongly among themselves than they do with other parts of the system 4 , and may correspond to chemical complexes 5 , signaling pathways 6 , metabolic processes 7 , or possibly entire cellular processes 8 .The identity of these modules is important because it gives knowledge into the organization of the underlying network 9 , and allows us to predict new interactions 10 , predict disease genes 11 , and understand evolutionary relationships 12 . However , despite considerable work 13 – 19 , no single method consistently outperforms all others 20 , so researchers have developed a variety of complementary techniques 21 – 23 .",
        "rewrite_text": "We introduce a method for validating the performance of machine computing techniques aimed at identifying modules within networks. This approach utilizes synthetic datasets created by simulating random pathways through established modular structures. Our findings indicate that this algorithm effectively identifies and ranks various types of modules with significant sensitivity over a diverse range of sizes and densities. The results demonstrate resilience against noise and lost connections. This validation strategy is crucial both as a benchmarking tool for assessing competing methods and for evaluating the effectiveness of existing techniques when applied to real-time systems. In recent decades, there has been increasing interest in developing computational tools to detect functional units within intricate biological networks, such as protein-gene interactions (PPI) and protein regulatory circuits. These \"modules\" refer to groups of nodes that exhibit stronger interactions among themselves compared to their interactions with other parts of the network, potentially representing chemical complexes, signaling pathways, metabolic processes, or entire cellular functions. Understanding these modules is vital as it provides insight into the organization of the underlying network and enables the prediction of new interactions, identification of disease-related genes, and comprehension of evolutionary relationships. However, despite significant advancements, no single method has consistently outperformed all others, leading researchers to create a variety of complementary techniques.",
        "ori-fast-z-score": 0.8908708063747479,
        "water-fast-z-score": 8.139287438099235,
        "rewrite-fast-z-score": 0.8181818181818182
    },
    {
        "original_text": "We present the results of our study on chemical composition, molecular line emission, dust properties, and thermal balance in dense cores with metallicities ranging between 1/100 solar to 1/10 000 solar. We find that the gas temperature decreases by about 10 K as the core density increases for all metallicities studied here (1/100-1/10 000 solar). The decrease is more rapid than predicted by current models which assume constant temperatures throughout the cloud evolution. This may be due to an increase in the importance of grain-surface chemistry relative to gas-phase reactions at higher densities. In addition we find evidence for significant depletion of carbon onto grains even at high metallicities such as Z = 1/10 000 solar. Our observations suggest that the critical density above which CO becomes optically thick depends strongly on metallicity. At lower metallicities this occurs at higher densities compared to higher metallicities. Finally, we show that the observed abundance ratios are consistent with those expected if the clouds were initially chemically enriched by supernovae type II explosions.",
        "watermark_text": "We present the conclusion of our research on chemical composition , molecular line emission , dust characteristics , and thermal balance in dense cores with metallicities ranging between 1 / 100 solar to 1 / 10 000 solar . We see that the gas temperature reduces by about 10 K as the core size grows for all metallicities researched here ( 1 / 100 - 1 / 10 000 solar ) .The reduction is more rapid than forecast by current scenarios which predict constant temperatures throughout the cloud evolution . This might be due to an increase in the importance of grain - boundary dynamics compared to liquid - phase processes at higher densities .In addition we find proof for significant depletion of carbon onto grains even at high metallicities such as Z = 1 / 10 000 solar . Our observations suggest that the critical mass above which CO becomes optically dense relies highly on metallicity .At lower metallicities this appears at higher densities compared to higher metallicities . Finally , we find that the seen concentrations proportions are compatible with those expected if the clouds were initially chemically enriched by supernovae class II explosions .",
        "rewrite_text": "We present the findings of our research on the chemical composition, molecular line emission, dust properties, and thermal balance in dense cores with metallicities ranging from 1/100 to 1/10,000 solar. Our results indicate that the gas temperature decreases by approximately 10 K as the size of the core increases across all metallicity ranges examined (1/100 - 1/10,000 solar). This decline occurs more rapidly than predicted by current models, which suggest that temperatures remain constant throughout the evolution of the cloud. This phenomenon may be attributed to the growing significance of grain-boundary dynamics relative to liquid-phase processes at higher densities. Additionally, we provide evidence of considerable carbon depletion onto grains, even at elevated metallicities such as Z = 1/10,000 solar. Our observations indicate that the critical mass at which CO becomes optically dense is highly dependent on metallicity, with lower metallicities exhibiting this threshold at higher densities compared to their higher metallicity counterparts. Finally, we find that the observed concentration ratios align with expectations if the clouds were initially enriched chemically by type II supernova explosions.",
        "ori-fast-z-score": 1.5652475842498528,
        "water-fast-z-score": 7.313071356019155,
        "rewrite-fast-z-score": 1.9639610121239315
    },
    {
        "original_text": "We present spatially-resolved spectroscopic observations for the central regions (r < 1 kpc) of six nearby, massive early-type galaxies in clusters or groups with Mvir > 1013M⊙. The data were obtained using the Gemini Multi-Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems. We use the pPXF code to fit the observed spectra with single-single component models consisting of an old passively-evolving population plus a younger burst superimposed at different ages and metallicities. Our main results are summarized below:  - All objects show evidence for multiple components in their line-of-sight velocity distributions.  - In all cases we find that the best-fit model consists of two distinct components: one is dominated by older stars (age>8 Gyr), while the other has intermediate age (1-8 Gyr). - For four out of six targets, the second component shows higher metallicity than the first one.",
        "watermark_text": "We generate spatially - resolved spectroscopic observations for the central regions ( r < 1 kpc ) of six nearby , massive early - type galaxies in clusters or groups with Mvir > [UNK] . The data were obtained using the Gemini Multi - Object Spectrograph on Gemini North telescope as part of our ongoing program to study the formation histories of these systems .We use the pPXF code to pack the known spectra with single - single component versions comprised of an old passively - expanding population plus a later burst superimposed at different ages and metallicities . Our main results are presented below : - All bodies exhibit data for multiple components in their line - of - seeing velocity distributions .- In all situations we find that the best - fitting model consists of two separate phases : one is dominated by older stars ( age > 8 Gyr ) , while the other has intermediate older ( 1 - 8 Gyr ) . - For four out of six targets , the second component displays higher metallicity than the first one .",
        "rewrite_text": "We conducted spatially-resolved spectroscopic observations of the central regions (r < 1 kpc) of six nearby, massive early-type galaxies located in clusters or groups with Mvir > [UNK]. The data were collected using the Gemini Multi-Object Spectrograph on the Gemini North telescope as part of our ongoing research into the formation histories of these galaxies. Utilizing the pPXF code, we analyzed the known spectra by fitting single-component models that combine an old, passively-expanding population with a later starburst at varying ages and metallicities. Our key findings are summarized as follows: - Each galaxy shows evidence of multiple components in their line-of-sight velocity distributions. - In every case, the optimal model comprises two distinct phases: one with older stars (age > 8 Gyr) and another with stars of intermediate age (1 - 8 Gyr). - For four out of the six galaxies, the second component exhibits a higher metallicity compared to the first.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 1.6733200530681511
    },
    {
        "original_text": "The BFKL equation is an effective theory for describing high-energy scattering processes at small Bjorken-x, where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. The BFKL formalism has been developed into a practical tool to calculate cross sections and structure functions using numerical methods. In this talk I will present recent results on the calculation of the gluon Green s function within the framework of the so-called  dipole approach  which allows us to perform calculations analytically. This method was first introduced by Mueller and Tang in order to study diffractive deep-inelastic scattering (DDIS) off protons. It can be applied also to other processes like heavy quark production in proton-proton collisions as well as photon-photon interactions. We will discuss how we have implemented these ideas numerically and show some preliminary results obtained with our code. Finally, we will comment on possible extensions of this work towards more realistic phenomenological applications.",
        "watermark_text": "The BFKL equation is an efficient model for describing long - energy scattering phenomena at small Bjorken - x , where x denotes the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei . The BFKL formalism has been built into a practical tool to estimate cross sections and structure functions using numerical models .In this talk I will present recent results on the determination of the gluon Green s function within the framework of the so - called dipole approach which allows us to conduct measurements analytically . This method was first developed by Mueller and Tang in order to study diffractive deep - inelastic reflection ( DDIS ) off protons .It can be applied also to other processes like heavy quark production in proton - proton collisions as well as photon - photon interactions . We will explore how we have formulated these ideas numerically and get some preliminary outcomes received with our code .Finally , we will mention on potential extensions of this research towards more realistic phenomenological applications .",
        "rewrite_text": "The BFKL equation serves as an effective model for analyzing high-energy scattering phenomena at small Bjorken-x, where x represents the fraction of longitudinal momentum carried by one of the colliding hadrons or nuclei. This formalism has been developed into a practical tool for estimating cross sections and structure functions through numerical models. In this presentation, I will share recent findings on the computation of the gluon Green's function using the dipole approach, which enables us to perform analytical measurements. This technique was initially introduced by Mueller and Tang to investigate diffractive deep inelastic scattering (DDIS) off protons. It is also applicable to other processes, such as heavy quark production in proton-proton collisions and photon-photon interactions. We will discuss how we have numerically formulated these concepts and present some preliminary results obtained with our code. Lastly, we will consider potential avenues for extending this research to more realistic phenomenological applications.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.259005881071332,
        "rewrite-fast-z-score": 0.25
    },
    {
        "original_text": "We report on five new planets discovered by the NASA K2 mission, which were found in the sample of targets observed during Campaigns 1 and 2 (C1/K2). The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 years. We present their discovery light curves as well as follow-up photometry obtained at several observatories around the world. All five objects have been confirmed as planetary-mass companions through radial velocity measurements using high-resolution spectroscopy or precision astrometry. \n \n Keywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby stars - TESS - PLATO - HARPS-N - SPECULOOS \n \n \n \n Five intermediate-period planets from the N2K sample \nThe NASA Kepler space telescope has revolutionized our understanding of extrasolar planets over its primary mission that lasted for four years . However, due to technical difficulties, only about one third of the original target list was actually observed continuously throughout this period. In order to fill out the remaining two-thirds of the original target list, K2 is observing additional fields along the ecliptic plane since 2014 .\nIn this work we report on five new planets detected by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1/K2 ) . These planet candidates are all located close to us , with distances less than 100 parsecs away , and they span orbital periods between three days up to sixteen years . Their masses range from 0 . 5 to 4 times Jupiter  s mass .  \n \n We present here the discovery light curves together with followup photometric observations performed at various observatories worldwide . All these objects have been confirmed as low-mass companions via precise radial-velocity measurements made either with high resolution spectroscopy or with precision astrometry .",
        "watermark_text": "We report on five new planets discovered by the NASA K2 flight , which were found in the sample of targets observed during Campaigns 1 and 2 ( C1 / K2 ) . The planet candidates are all located within 100 pc of Earth with periods ranging between 3 days to 16 days .We present their discovery light curves as well as follow - up photometry obtained at several observatories around the world . All five objects have been confirmed as planetary - mass companions through radial speed measurements involving high - resolution spectroscopy or precision astrometry .Keywords : Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanet - K2 Mission - Nearby galaxies - TESS - PLATO - HARPS - N - SPECULOOS Five intermediate - time planets from the N2K survey The NASA Kepler space telescope has revolutionized our knowing of extrasolar stars over its primary mission that lasted for four seasons . However , owing to technical problems , only about one third of the actual target list was actually seen continuously throughout this time .In order to fill out the remaining two - half of the original target roster , K2 is monitoring extra fields along the ecliptic plane since 2014 . In this research we publish on five new objects discovered by K2 , which were found among the sample of targets observed in campaigns 1 and 2 ( C1 / K2 ) .These planet candidates are all located close to us , with distances fewer than 100 parsecs apart , and they span orbital periods between three weeks up to fourteen months . Their masses range from 0 .5 to 4 times Jupiter s mass . We present here the discovery light curves combined with followup photometric surveys performed at numerous observatories worldwide .All these objects have been confirmed as low - weight companions via accurate radial - speed measurements made either with high resolution spectroscopy or with accuracy astrometry .",
        "rewrite_text": "We present the discovery of five new planets identified by the NASA K2 mission, which were detected among the targets observed during Campaigns 1 and 2 (C1/K2). These planet candidates are all located within 100 parsecs of Earth, with orbital periods ranging from 3 to 16 days. We include their discovery light curves along with follow-up photometry conducted at various observatories globally. All five candidates have been confirmed as planetary-mass companions through precise measurements of radial velocities using high-resolution spectroscopy or precision astrometry. \n\nKeywords: Planetary systems - Discovery methods - Radial velocities - Astrometry - Transits - Exoplanets - K2 Mission - Nearby galaxies - TESS - PLATO - HARPS - SPECULOOS\n\nFive intermediate-time planets from the N2K survey\n\nThe NASA Kepler space telescope has transformed our understanding of extrasolar planets during its primary mission, which spanned four years. However, due to technical difficulties, only about a third of the original target list was observed continuously during this period. To address this gap, the K2 mission has been monitoring additional fields along the ecliptic plane since 2014. In this study, we report on five new planets discovered by K2 from the targets observed in Campaigns 1 and 2 (C1/K2). These candidates are located relatively close to Earth, within 100 parsecs, with orbital periods ranging from three days to fourteen months. Their masses range from 0.5 to 4 times that of Jupiter. We present the light curves from their discovery along with follow-up photometric surveys conducted at numerous observatories worldwide. All of these objects have been confirmed as low-mass companions through accurate radial velocity measurements obtained using high-resolution spectroscopy or high-precision astrometry.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 7.423745685299301,
        "rewrite-fast-z-score": 0.7492686492653552
    },
    {
        "original_text": "We give an explicit description of the higher algebraic K-groups in terms of certain universal cohomology classes, which are defined by using only the ring structure and the unit element of the underlying commutative ring. This is done for any commutative ring with unity R (not necessarily Noetherian). The main result can be formulated as follows: Let M be a module over R. Then there exists a natural isomorphism between the higher algebraic K-groups: \nK_n(R) = Ext^n_R(M, R)\nand the group of all n-fold Massey products on M modulo those that vanish under some suitable finiteness condition. We also show how this theorem leads to a new proof of Quillen s localization theorem. Finally we discuss applications to the study of equivariant K-theory. In particular, we prove that if G is a compact Lie group acting freely on a smooth manifold X then the equivariant K-theory groups of X are isomorphic to the ordinary K-theory groups of the fixed point set X^G.",
        "watermark_text": "We get an explicit description of the higher algebraic K - fields in terms of certain universal cohomology groups , which are established by using only the ring composition and the unit element of the underlying commutative field . This is accomplished for any commutative field with unity R ( not necessarily Noetherian ) .The main consequence can be stated as follows : Let M be a module over R . Then there exists a natural isomorphism between the higher geometric K - families : K _ k ( R ) = Ext ^ n _ R ( M , R ) and the group of all k - fold Massey products on M modulo those that vanish under some suitable finiteness requirement . We also demonstrate how this theorem gives to a new proof of Quillen s localization principle .Finally we explain applications to the study of equivariant K - theory . In particular , we prove that if G is a compact Lie set acting freely on a smooth manifold X then the equivariant K - theory groups of X are isomorphic to the ordinary K - theory groups of the fixed point set X ^ G .",
        "rewrite_text": "We provide a clear characterization of higher algebraic K-fields through certain universal cohomology groups, established solely using the ring operations and the identity element of the underlying commutative field. This framework applies to any commutative field with unity \\( R \\), regardless of whether it is Noetherian. The key result can be summarized as follows: for a module \\( M \\) over \\( R \\), there exists a natural isomorphism between the higher geometric K-families: \\( K_k(R) \\cong \\text{Ext}^n_R(M, R) \\), and the group of k-fold Massey products on \\( M \\), modulo those that vanish under appropriate finiteness constraints. We also show how this theorem provides a new proof of Quillen’s localization principle. Lastly, we outline its applications to equivariant K-theory, demonstrating that if \\( G \\) is a compact Lie group acting freely on a smooth manifold \\( X \\), then the equivariant K-theory groups of \\( X \\) are isomorphic to the ordinary K-theory groups of the fixed point set \\( X^G \\).",
        "ori-fast-z-score": 0.23904572186687872,
        "water-fast-z-score": 5.176591682688076,
        "rewrite-fast-z-score": 2.80989722019502
    },
    {
        "original_text": "In this work, we propose an efficient direct-voting scheme to provide data fusion assurance (DFA) in wireless sensor networks (WSNs). The proposed DFA scheme is based on the concept that each node can directly vote its local decision with other nodes  decisions and then obtain final global decision by majority voting rule. In order to reduce communication overheads caused by direct voting process, our scheme adopts two novel techniques: 1) only one-hop neighbors are allowed to cast their votes at any time instant; 2) all nodes use different time slots to transmit their votes simultaneously. We also present theoretical analysis to show how much energy consumption will be saved compared with existing schemes. Finally, extensive simulation results demonstrate that our scheme outperforms state-of-the-art approaches in terms of both accuracy and power efficiency. Keywords: Data fusion assurance, Wireless sensor network, Power-efficient direct-voting scheme. This research was supported by National Natural Science Foundation of China under Grant No. 61371040",
        "watermark_text": "In this research , we develop an efficient direct - voted scheme to provide resource fusion assurance ( DFA ) in wireless sensor networks ( WSNs ) . The proposed DFA system is based on the idea that each node can personally voting its local decision with other nodes decisions and then obtain final global ruling by majority voting control .In order to reduce communication overheads caused by direct voting process , our scheme adopts two novel techniques : 1 ) only one - hop neighbors are allowed to cast their votes at any time instant ; 2 ) all nodes use different time slots to transmit their votes concurrently . We additionally offer experimental analysis to see how many electricity usage will be saved compared with existing plans .Finally , extensive modeling results show that our scheme outperforms state - of - the - art methods in terms of both precision and power performance . Keywords : Data fusion assurance , Wireless sensor network , Power - efficient direct - voting system .This research was supported by National Natural Science Foundation of China under Grant No.61371040",
        "rewrite_text": "In this study, we introduce an efficient direct voting scheme designed to enhance data fusion assurance (DFA) in wireless sensor networks (WSNs). The proposed DFA framework allows each node to independently vote on its local decisions and achieve a final global decision through majority voting. To minimize the communication overhead associated with the direct voting process, our approach employs two innovative techniques: (1) votes can only be cast by one-hop neighbors at any given moment; and (2) all nodes utilize distinct time slots for simultaneous vote transmission. We also present experimental analyses to evaluate energy savings compared to existing methods. Ultimately, our extensive modeling results demonstrate that our scheme surpasses state-of-the-art techniques in terms of both accuracy and energy efficiency. Keywords: Data fusion assurance, Wireless sensor network, Power-efficient direct voting system. This research was supported by the National Natural Science Foundation of China under Grant No. 61371040.",
        "ori-fast-z-score": 0.6324555320336759,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 0.10846522890932808
    },
    {
        "original_text": "We present the first post-newtonian expansion for Chern-Simons gravity, which is an alternative to general relativity that has been proposed as a candidate theory of quantum gravity.  We show how this can be done by using the method developed in Ref.  1  . The resulting expression agrees with previous results obtained within the framework of effective field theories  2  , and it also reproduces the leading-order corrections to Newton s law predicted by Einstein-Cartan gravity  3  .\nThe work presented here was supported by the National Science Foundation under Grant No. PHY-0555310. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. This document is available at http://arxiv.org/abs/1106.5389v1. In recent years there have been several attempts to develop new gravitational theories beyond general relativity (GR)  4  -  8  . One such proposal is ChernSimons gravity  9  , where the action contains higher order curvature terms but no matter fields  10  . It has been shown  11  that Chern-Simons gravity reduces to GR when expanded around flat space-time, while it gives rise to novel effects on cosmological scales  12  . Moreover, Chern-Simons gravity appears naturally in string theory  13  , and it may provide a description of black hole entropy  14  .",
        "watermark_text": "We introduce the first post - newtonian expansion for Chern - Simons gravity , which is an addition to general relativity that has been proposed as a candidate theory of quantum gravitational . We see how this can be performed by using the method developed in Ref .1 . The resulting expression agrees with previous findings obtained within the framework of effective field theories 2 , and it also reproduces the leading - order corrections to Newton s law predicted by Einstein - Cartan relativity 3 .The work presented here was supported by the National Science Foundation under Grant No . PHY - 0555310 .Any views , findings , findings or recommendations expressed in this material are those of the writers and do not necessarily reflect the views of the National Science Foundation . This publication is accessible at http : / / arxiv . org / abs / 1106 . 5389v1 .In past decades there have been numerous attempts to develop new gravity explanations beyond regular relativity ( GR ) 4 - 8 . One such proposal is ChernSimons gravity 9 , where the activity contains higher order curvature terms but no matter quantities 10 .It has been shown 11 that Chern - Simons gravity decrease to GR when accelerated around flat space - time , while it gives rise to novel influences on cosmological scales 12 . Moreover , Chern - Simons gravity appears naturally in string theory 13 , and it could give a description of white hole entropy 14 .",
        "rewrite_text": "We present the inaugural post-Newtonian expansion for Chern-Simons gravity, a modification of general relativity that has been put forward as a potential theory of quantum gravity. This expansion is achieved using the methodology outlined in Reference 1. Our resulting expression aligns with previous results derived from effective field theories (Reference 2) and accurately reflects the leading order corrections to Newton's law anticipated by Einstein-Cartan relativity (Reference 3). This research was funded by the National Science Foundation under Grant No. PHY-0555310. Any opinions, findings, conclusions, or recommendations expressed here are solely those of the authors and do not necessarily represent the views of the National Science Foundation. The publication can be accessed at http://arxiv.org/abs/1106.5389v1. In recent decades, there have been multiple efforts to explore new gravitational theories beyond standard general relativity (References 4-8). One noteworthy proposal is Chern-Simons gravity (Reference 9), which incorporates higher-order curvature terms while excluding matter variables (Reference 10). It has been demonstrated (Reference 11) that Chern-Simons gravity reduces to general relativity in the context of small perturbations around flat spacetimes while exhibiting unique effects on cosmological scales (Reference 12). Furthermore, Chern-Simons gravity naturally emerges in string theory (Reference 13) and may provide insights into the entropy of white holes (Reference 14).",
        "ori-fast-z-score": 0.10369516947304253,
        "water-fast-z-score": 5.629889179541019,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "The Standard Model (SM) is an extremely successful theory, but it has some problems that need to be addressed before we can claim to have found  theory of everything .  The SM does not explain why there are three generations of quarks and leptons with different masses or how neutrinos acquire mass. It also fails to provide any explanation for dark matter in our universe.   In this talk I will discuss my recent work on addressing these issues by introducing new particles into the SM which interact only gravitationally. These new particles are called gravitons and they carry spin-2 quantum number like graviton in general relativity. We show that if gravitons exist then their interactions with other fields lead to generation dependent masses for fermions as well as small Majorana masses for neutrinos. Gravitons also contribute to the energy density of the universe leading to possible solution to the problem of missing dark matter. Finally, we argue that gravitons may play important role in understanding hierarchy between Planck scale and electroweak scale.",
        "watermark_text": "The Standard Model ( SM ) is an incredibly successful theory , but it has some problems that require to be addressed before we can claim to have found explanation of things . The SM does not understand why there are three generations of quarks and leptons with various masses or how neutrinos gain mass .It even fails to provide any theory for black material in our universe . In this talk I will explore my current work on addressing these problems by introducing additional particles into the SM which interact only gravitationally .These new ions are called gravitons and they carry spin - 2 quantum number like graviton in general relativity . We see that if gravitons exist then their interactions with other fields give to generation dependent masses for fermions as also as low Majorana masses for neutrinos .Gravitons thus help to the power concentration of the universe contributing to possible solution to the issue of missing dark matter . Finally , we claim that gravitons may play crucial role in understanding hierarchy between Planck scale and electroweak scale .",
        "rewrite_text": "The Standard Model (SM) is a remarkably successful theory, yet it presents several challenges that need to be resolved before we can claim to fully understand the universe. For instance, the SM cannot explain why there are three generations of quarks and leptons with different masses, nor does it clarify how neutrinos acquire their mass. Additionally, it lacks a framework for understanding dark matter in our universe. In this presentation, I will discuss my ongoing research aimed at addressing these issues by introducing new particles within the SM that interact solely through gravity. These new particles, called gravitons, possess a spin-2 quantum number similar to that of gravitons in general relativity. If gravitons exist, their interactions with other fields could result in generation-dependent masses for fermions and small Majorana masses for neutrinos. Thus, gravitons could contribute to our understanding of the matter composition of the universe and potentially help explain the phenomenon of missing dark matter. Finally, we propose that gravitons may be essential for understanding the hierarchy between the Planck scale and the electroweak scale.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": -0.22360679774997896
    },
    {
        "original_text": "We study the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT) calculations, which show that this material is close to an insulator-metal transition driven by charge transfer between layers. We find that the Fermi surface topology changes dramatically across the metal-insulator boundary, with the appearance of new hole pockets at the Brillouin zone center. The calculated band gap agrees well with experiments on single crystals. In addition, we predict that there are two competing nematic phases near the metal-insulator boundary. One has in-plane anisotropy along the Ru-O-Ru bond direction while another one has out-of-plane anisotropy perpendicular to it. These results provide insights into the origin of the observed structural distortion in bilayer ruthenates. Bilayer ruthenates have attracted considerable attention recently due to their rich physical properties including unconventional superconductivity  1  , quantum criticality  2  , and multiferroicity  3  . Among these materials, Sr3Ru2O7 shows particularly interesting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying pressure  4  .\nIn recent years, several experimental studies have been performed to investigate the nature of the metal-insulator transition (MIT). For example, angle resolved photoemission spectroscopy measurements  5  found that the Fermi surface topology changed significantly when crossing the MIT line. X-ray scattering  6  showed that the crystal symmetry was lowered from tetragonal to orthorhombic below TMI = 160 K. Neutron scattering  7  revealed that the lattice parameters were different for the ab plane and c axis below TMIT ~ 150 K. However, despite extensive investigations, the microscopic mechanism behind the MIT remains unclear  8  .",
        "watermark_text": "We research the phase diagram and electronic structure of bilayer ruthenate Sr3Ru2O7 using density functional theory ( DFT ) observations , which show that this metal is close to an insulator - iron transition accelerated by charge transfer between layers . We see that the Fermi boundary topology changes dramatically across the metal - insulator boundary , with the emergence of new hole pockets at the Brillouin zone center .The measured band gap agrees well with experiments on single crystals . In addition , we estimate that there are two rival nematic phases near the metal - insulator boundary .One has in - plane anisotropy along the Ru - O - Ru bond direction while another one has out - of - plane anisotropy diagonal to it . These conclusions provide insights into the origin of the reported structural degradation in bilayer ruthenates .Bilayer ruthenates have garnered considerable scrutiny lately owing to their valuable physical properties including unconventional superconductivity 1 , quantum criticality 2 , and multiferroicity 3 . Among these structures , Sr3Ru2O7 shows particularly exciting behavior because its ground state can be tuned continuously from metallic to insulating states through chemical doping or applying tension 4 .In recent years , various experimental studies have been performed to examine the nature of the metal - insulator transition ( MIT ) . For instance , angle resolution photoemission spectroscopy measurements 5 found that the Fermi surface topology changed significantly when crossing the MIT line .X - ray scattering 6 revealed that the crystal symmetry was changed from tetragonal to orthorhombic below TMI = 160 K . Neutron propagation 7 revealed that the crystal conditions were different for the ab plane and c axis below TMIT ~ 150 K . However , despite extensive investigations , the microscopic process behind the MIT remains unsure 8 .",
        "rewrite_text": "We investigate the phase diagram and electronic structure of the bilayer ruthenate Sr3Ru2O7 using density functional theory (DFT). Our findings indicate that this metallic compound is on the verge of a metal-insulator transition, driven by charge transfer between its layers. Notably, we observe a significant transformation in the Fermi surface topology as we cross the metal-insulator boundary, marked by the formation of new hole pockets at the center of the Brillouin zone. The calculated band gap aligns well with experimental observations from single crystals. Furthermore, we propose the existence of two competing nematic phases near the metal-insulator boundary: one exhibiting in-plane anisotropy along the Ru-O-Ru bond direction and the other displaying out-of-plane anisotropy in a diagonal orientation. These insights shed light on the origins of the observed structural degradation in bilayer ruthenates, which have recently attracted considerable attention due to their remarkable physical properties, such as unconventional superconductivity, quantum criticality, and multiferroicity. Among these materials, Sr3Ru2O7 is particularly intriguing because its ground state can be continuously adjusted from metallic to insulating through chemical doping or applied strain. Recently, various experimental studies have been conducted to explore the nature of the metal-insulator transition (MIT). For instance, angle-resolved photoemission spectroscopy revealed significant changes in the Fermi surface topology at the MIT line. X-ray scattering investigations indicated a transition in crystal symmetry from tetragonal to orthorhombic below TMI = 160 K, while neutron scattering indicated differing crystal conditions for the ab plane and c axis below TMIT ~ 150 K. Despite extensive research, the microscopic mechanism underlying the MIT remains unclear.",
        "ori-fast-z-score": 0.9838699100999074,
        "water-fast-z-score": 7.244860247099318,
        "rewrite-fast-z-score": 1.4368424162141993
    },
    {
        "original_text": "We present results on the statistical properties of earthquake sequences generated by the 2D Burridge-Knopff (BK) model with random initial conditions and periodic boundary conditions, using numerical simulations. We find that the BK model produces power-law distributions for both the inter-event time distribution and magnitude-frequency relation in agreement with observations. The exponent values are found to be dependent upon the system size N . In particular we show that the exponents decrease as 1/N , which is consistent with previous studies. Finally, we discuss possible reasons behind this dependence. Keywords: Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical simulation; Burridge-Knopf model. 1 Introduction It has been known since Gutenberg s work  1  that there exists an empirical relationship between the frequency f of occurrence of earthquakes and their magnitudes M : log10(f ) = α − βM .\n(\nThe constants α and β depend on the region under consideration  2  . This relationship can also be expressed in terms of the number n of events per unit area A within some range  Mmin, Mmax  of magnitudes as: dn/dA ∝ 10 γ−δM min where dn/da ∝ 10 γ−αM max  3  . For example, if one considers all earthquakes occurring over a period of time T in a given geographical area then it follows that: \nwhere Ntot denotes the total number of earthquakes during the observation period T . If one instead counts only those earthquakes whose magnitude lies in the interval  Mmin, Mmax :",
        "watermark_text": "We report findings on the statistical characteristics of earthquake patterns formed by the 2D Burridge - Knopff ( BK ) model with random initial conditions and regular boundary conditions , using numerical simulations . We see that the BK theory creates power - law functions for both the inter - event time distribution and magnitude - frequency relation in agreement with observations .The exponent values are found to be dependent upon the scheme size N . In particular we find that the exponents decline as 1 / N , which is compatible with previous research .Finally , we explain possible reasons behind this dependence . Keywords : Earthquake statistics ; Power laws ; Random initial conditions ; Periodic border conditions ; Statistical mechanics ; Numerical modeling ; Burridge - Knopf equation .1 Introduction It has been known since Gutenberg s work 1 that there exists an empirical relationship between the frequency h of occurrence of earthquakes and their magnitudes M : log10 ( f ) = α − βM . ( The constants α and beta depend on the region under consideration 2 .This relationship can also be shown in terms of the number n of episodes per unit area A within some range Mmin , Mmax of magnitudes as : dn / dA [UNK] 10 γ−δM min where dn / da [UNK] 10 γ−αM max 3 . For instance , if one looks all earthquakes happening over a period of time T in a given geographical region then it follows that : where Ntot refers the total quantity of disasters during the observation period T .If one rather includes only those earthquakes whose magnitude rests in the interval Mmin , Mmax :",
        "rewrite_text": "We present findings on the statistical properties of earthquake patterns generated by the 2D Burridge-Knopff (BK) model under random initial conditions and regular boundary conditions, as analyzed through numerical simulations. Our results indicate that the BK theory produces power-law distributions for both the inter-event times and the magnitude-frequency relationship, consistent with empirical observations. Notably, we discover that the exponent values are influenced by the size of the simulation scheme, denoted as N. Specifically, we observe that the exponents decrease as 1/N, which aligns with prior studies. Furthermore, we discuss potential explanations for this observed dependence. \n\n**Keywords:** Earthquake statistics; Power laws; Random initial conditions; Periodic boundary conditions; Statistical mechanics; Numerical modeling; Burridge-Knopff equation.\n\n**1 Introduction**  \nSince Gutenberg's research, it has been recognized that a specific empirical relationship exists between the frequency (h) of earthquake occurrences and their magnitudes (M), given by the equation log10(f) = α - βM, where the constants α and β vary depending on the region. This relationship can also be expressed in terms of the number of events (n) per unit area (A) within a range of magnitudes (Mmin, Mmax) as follows: dn/dA ∝ 10^(γ - δMmin) and dn/dA ∝ 10^(γ - αMmax). For instance, considering all earthquakes occurring over a time period (T) in a specific geographical region, we can express this quantitatively, where Ntot represents the total number of earthquakes recorded during the observation period T. If we focus exclusively on earthquakes whose magnitudes fall within the interval [Mmin, Mmax]:",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.609001368025944,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "We present the results of an analysis of the 2 Micron All Sky Survey (2MASS) data for quasars with broad absorption lines (BALQSOs). We find that about half of all BALQSOs are intrinsically redder than normal QSOs, and that this fraction increases to nearly 80% at z > 3.5. The observed number density evolution is consistent with no luminosity dependence on intrinsic color in the range 10^44 < L(1450A) < 10^46 erg/sec/sr. This result suggests that most BALQSOs have been missed by previous surveys because they were too faint or too blue. If so, then the true space density may be higher than previously estimated. \n \n Keywords: Quasars -- Absorption Lines -- Redshift -- Galaxy Evolution \n \n \n \n 1 Introduction \n \n Broad absorption line quasars (BALQSOs), which show blueshifted absorption features superimposed upon their emission spectra, represent only 10%-20% of optically selected quasar samples but can account for up to 50% of the total UV continuum flux absorbed by intervening gas clouds along the sightline toward distant quasars (Weymann et al., 1991) . In addition to being important probes of the physical conditions within the absorbing gas itself, BALQSOs also provide information regarding the properties of the surrounding intergalactic medium through studies of the associated metal-line systems (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage 1988; Hamann 1998a ,b, 1999 . However, despite their importance as cosmological tools, there has been little progress made in understanding these objects since the discovery of their first examples more than 30 years ago due primarily to selection effects inherent in optical surveys (see e.g., Hewett & Foltz 2003 ) . \n \n Recently, several authors have suggested that many BALQSOs could be found among infrared-selected sources using large-area near-infrared sky surveys such as the Two-Micron All-Sky Survey (2MASS) (Cutri et",
        "watermark_text": "We present the conclusion of an assessment of the 2 Micron All Sky Survey ( 2MASS ) statistics for quasars with broad absorption patterns ( BALQSOs ) . We see that about half of all BALQSOs are intrinsically redder than usual QSOs , and that this amount rises to approximately 80 % at z > 3 . 5 .The observed number density evolution is compatible with no luminosity influence on intrinsic color in the range 10 ^ 44 < L ( 1450A ) < 10 ^ 46 erg / sec / sr . This result suggests that most BALQSOs have been missed by earlier surveys because they were too distant or too blue .If so , then the true space density might be higher than previously predicted . Keywords : Quasars - - Absorption Lines - - Redshift - - Galaxy Evolution 1 Introduction Broad absorption line quasars ( BALQSOs ) , which show blueshifted emission elements superimposed upon their absorption spectra , constitute only 10 % - 20 % of optically selected quasar samples but can provide for up to 50 % of the total UV continuum flux reflected by intervening gas clouds along the sightline toward distant quasars ( Weymann et al . , 1991 ) .In addition to being important probes of the physical conditions within the absorbing gas itself , BALQSOs additionally offer information regarding the properties of the nearby intergalactic medium through analyses of the associated metal - line systems ( e . g . , Weymann et al . , 1979 ; Foltz et al . , 1986 ; Turnshek & Savage 1988 ; Hamann 1998a , b , 1999 . However , despite their importance as cosmological tools , there has been limited progress made in understanding these objects since the discovery of their early instance more than 30 weeks ago due primarily to selection effects inherent in optical searches ( saw e . g . , Hewett & Foltz 2003 ) .Recently , various published have suggested that several BALQSOs might be found among infrared - selected sources using big - area near - infrared sky observations such as the Two - Micron All - Sky Survey ( 2MASS ) ( Cutri et",
        "rewrite_text": "We summarize the findings of an assessment of the 2 Micron All Sky Survey (2MASS) concerning quasars with broad absorption lines (BALQSOs). Our analysis reveals that nearly half of all BALQSOs are intrinsically redder than typical quasars, with this figure increasing to about 80% for redshifts greater than 3.5. The observed evolution in number density aligns with the notion that intrinsic color is not influenced by luminosity within the range of \\(10^{44} < L (1450Å) < 10^{46} \\text{ erg/s/sr}\\). This indicates that a significant number of BALQSOs may have been overlooked in previous surveys due to their substantial distance or bluer appearance, suggesting that the actual space density may be greater than earlier estimates. \n\nKeywords: Quasars, Absorption Lines, Redshift, Galaxy Evolution\n\n1. Introduction: Broad absorption line quasars (BALQSOs) exhibit blueshifted emission lines overlaid on their absorption spectra and account for only 10% to 20% of optically selected quasar samples. However, they can contribute up to 50% of the total UV continuum flux that is reflected by intervening gas clouds in the line of sight to distant quasars (Weymann et al., 1991). Besides serving as critical indicators of the physical conditions within the absorbing gas, BALQSOs also provide insights into the properties of the surrounding intergalactic medium through metal-line analysis (e.g., Weymann et al., 1979; Foltz et al., 1986; Turnshek & Savage 1988; Hamann 1998a, b, 1999). Despite their significance as cosmological probes, our understanding of these objects has progressed slowly since their initial discovery more than 30 years ago, largely due to selection biases in optical surveys (see, e.g., Hewett & Foltz 2003). Recently, various publications have indicated the potential for identifying additional BALQSOs through infrared-selected sources using extensive near-infrared sky observations, such as the Two-Micron All-Sky Survey (2MASS) (Cutri et al.).",
        "ori-fast-z-score": -0.3713906763541037,
        "water-fast-z-score": 6.247782215210102,
        "rewrite-fast-z-score": -1.4729193886373175
    },
    {
        "original_text": "We present Gemini GMOS-S spectroscopy for two young star clusters (ages ~10 Myr) in the interacting galaxy pair NGC 3256, which are located at projected distances of 1 kpc and 2 kpc from their respective nuclei. The spectra reveal that both clusters have similar ages but different metallicities; one is metal-rich with  Fe/H  = +0.2 dex while the other has solar metallicity. We find no evidence for multiple populations within either cluster. Using these data we derive masses of 5 x 10^4 M_sol and 7 x 10^3 M_sol respectively for each cluster. These values agree well with those derived using HST photometry. Both clusters show signs of youthful star-formation activity including blue supergiants and Wolf-Rayet stars. In addition to this ongoing star-formation activity, there appears to be an older population of red giant branch stars in the more massive cluster.",
        "watermark_text": "We present Gemini GMOS - S spectroscopy for two young galaxy regions ( ages ~ 10 Myr ) in the interacting galaxy pair NGC 3256 , which are situated at projected speeds of 1 kpc and 2 kpc from their respective nuclei . The spectra indicate that both clusters have related ages but different metallicities ; one is metal - rich with Fe / H = + 0 . 2 dex while the other has solar metallicity .We see no evidence for multiple colonies within either cluster . Using these information we derive masses of 5 x 10 ^ 4 M _ sol and 7 x 10 ^ 3 M _ sol respectively for each cluster .These values coincide well with those generated utilizing HST photometry . Both clusters show signs of young galaxy - formation activity including blue supergiants and Wolf - Rayet stars .In addition to this continued star - formation activity , there seems to be an larger growth of red giant line stars in the more massive cluster .",
        "rewrite_text": "We present Gemini GMOS-S spectroscopy for two young galaxy regions (approximately 10 Myr old) within the interacting galaxy pair NGC 3256, located at projected distances of 1 kpc and 2 kpc from their respective nuclei. The spectra reveal that both clusters have similar ages but different metallicities; one is metal-rich with Fe/H = +0.2 dex, while the other exhibits solar metallicity. No evidence of multiple colonies is found in either cluster. From this data, we estimate the masses of the clusters to be 5 x 10^4 M⊙ and 7 x 10^3 M⊙, respectively. These estimates align well with those obtained through HST photometry. Both clusters exhibit signs of ongoing galaxy formation activity, including the presence of blue supergiants and Wolf-Rayet stars. Additionally, there appears to be a more significant population of red giant stars in the more massive cluster.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 4.064004064006096,
        "rewrite-fast-z-score": 1.4320780207890627
    },
    {
        "original_text": "We present new near-infrared (NIR) spectroscopic observations with Keck II/DEIMOS, which cover the entire optical extent of the nearby spiral galaxy M33 out to its last measured isophote at 25 mag arcsec-2 in B-band. We also use archival data obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our study. The main goal of this work was to investigate how star formation proceeds beyond the edge of galactic disks into the surrounding intergalactic medium. Our results show that there are two distinct components along the line-of-sight towards M33: an extended component associated with diffuse ionized gas and young stars; and a compact component dominated by old stellar populations. Using these NIR spectra we have derived radial profiles of several physical parameters such as electron density, temperature, extinction coefficient etc., across the face-on view of M33 s disk. These profiles reveal interesting trends in the properties of interstellar matter within different regions of the galaxy.",
        "watermark_text": "We use new near - infrared ( NIR ) spectroscopic observations with Keck II / DEIMOS , which cover the entire optical extent of the nearby spiral galaxy M33 out to its last detected isophote at 25 mag arcsec - 2 in B - band . We additionally using archival measurements obtained by the Infrared Array Camera onboard the Spitzer Space Telescope for our research .The main goal of this research was to examine how star formation flows beyond the boundary of galactic disks into the nearby intergalactic medium . Our results show that there are two separate constituents along the line - of - seeing towards M33 : an extended component involved with diffuse ionized gas and older stars ; and a compact component dominated by aged stellar regions .Using these NIR spectra we have derived radial profiles of several physical factors such as electron concentration , temperature , extinction factor etc . , across the face - on vision of M33 s disk . These profiles indicate unusual trends in the properties of interstellar matter within various regions of the universe .",
        "rewrite_text": "We conducted new near-infrared (NIR) spectroscopic observations with the Keck II/DEIMOS, which encompass the entire optical extent of the nearby spiral galaxy M33, reaching its faintest detected isophote at 25 mag arcsec⁻² in the B band. Additionally, we incorporated archival data from the Infrared Array Camera on board the Spitzer Space Telescope into our analysis. The primary aim of this research was to investigate how star formation extends beyond the boundaries of galactic disks into the nearby intergalactic medium. Our findings reveal two distinct components along the line of sight towards M33: an extended region associated with diffuse ionized gas and older stars, and a compact area dominated by older stellar regions. From these NIR spectra, we have derived radial profiles of various physical parameters, including electron concentration, temperature, and extinction factors, across the face-on view of M33's disk. These profiles reveal unusual trends in the characteristics of interstellar matter in different regions of the universe.",
        "ori-fast-z-score": 0.3511234415883917,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "We present the first public release of an archive containing all available X-ray Telescope (XRT) data for Gamma-Ray Bursts observed by the Swift satellite since its launch in November 2004. The XRT is sensitive to soft X-ray photons with energies between 0.3 and 10 keV, and has a field-of-view of 23 x 23 arcminutes. It operates in two modes: Windowed Timing mode which provides high time resolution but low sensitivity; Photon Counting mode which gives higher sensitivity at the expense of temporal information. We have processed all publicly released XRT data into one homogeneous database using standard procedures. This includes both pre-launch calibration observations as well as in-orbit calibrations performed after each observation. For each burst we provide a table listing the basic properties of the event along with the raw count rates measured during the entire exposure. In addition, we also provide a set of FITS files that contain the background subtracted light curve binned on different timescales ranging from 1 second up to several hundred seconds.",
        "watermark_text": "We present the first public update of an archive encompassing all available X - ray Telescope ( XRT ) statistics for Gamma - Ray Bursts seen by the Swift satellite since its launch in November 2004 . The XRT is sensitive to soft X - ray photons with energies between 0 . 3 and 10 keV , and has a field - of - view of 23 x 23 arcminutes .It operates in two modes : Windowed Timing mode which offers high time resolution but little tolerance ; Photon Counting mode which gives higher sensitivity at the cost of temporal info . We have analyzed all officially released XRT data into one homogeneous database utilizing typical techniques .This contains both pre - launch calibration measurements as well as in - orbit calibrations performed after each measurement . For each burst we provide a list listing the fundamental properties of the event along with the raw count levels measured during the entire exposure .In addition , we also provide a setting of FITS files that include the background subtracted light curve binned on various timescales varied from 1 second up to several hundred moments .",
        "rewrite_text": "We are excited to announce the first public update of an archive that includes all available X-ray Telescope (XRT) statistics for Gamma-Ray Bursts observed by the Swift satellite since its launch in November 2004. The XRT is capable of detecting soft X-ray photons with energies ranging from 0.3 to 10 keV and has a field of view measuring 23 x 23 arcminutes. It operates in two modes: Windowed Timing mode, which provides high time resolution but limited tolerance, and Photon Counting mode, which offers greater sensitivity at the expense of temporal information. We have compiled and analyzed all officially released XRT data into a cohesive database using standard techniques, which features both pre-launch calibration measurements and in-orbit calibrations conducted after each observation. For each burst, we provide a detailed list of the key properties of the event along with the raw count levels recorded throughout the entire exposure. Additionally, we offer a set of FITS files that include the background-subtracted light curves binned over a range of timescales, from 1 second to several hundred moments.",
        "ori-fast-z-score": -1.709408646894569,
        "water-fast-z-score": 4.672383634845156,
        "rewrite-fast-z-score": -0.562543950463012
    },
    {
        "original_text": "We report on observations made with Suzaku and Swift during the 2006 eruption of the recurrent nova RS Ophiucchi (RS Oph). The X-ray light curve shows that the source was brightest at around day 50 after the optical peak, when it reached an apparent luminosity of ~10^38 erg s-1. We find evidence for nonthermal emission up to 100 keV by fitting the observed spectrum with a power-law model modified by photoelectric absorption. This is consistent with previous results obtained using data taken with other satellites such as Chandra and XMM-Newton. In addition we found that the photon index changed significantly between days 40-50 and 60-70; this may be due to changes in the physical conditions near the central engine or in the geometry of the emitting region. \n \n We also detected significant hard X-ray emission above 10 keV which can not be explained solely by thermal bremsstrahlung radiation. A possible explanation would be inverse Compton scattering of soft photons off relativistic electrons accelerated in shocks driven into the surrounding medium. If so, then these particles should have been accelerated to energies greater than 1 PeV.",
        "watermark_text": "We report on observations made with Suzaku and Swift during the 2006 eruption of the recurrent nova RS Ophiucchi ( RS Oph ) . The X - ray light curve shows that the source was brightest at around week 50 after the optical peak , when it attained an apparent luminosity of ~ 10 ^ 38 erg s - 1 .We get data for nonthermal emission up to 100 keV by fitting the seen spectrum with a power - law description altered by photoelectric absorbed . This is compatible with previous findings obtained using data taken with other satellites such as Chandra and XMM - Newton .In addition we reported that the photon index changed significantly between days 40 - 50 and 60 - 70 ; this might be due to changes in the physical conditions near the main motor or in the topology of the emitting area . We additionally observed significant hard X - ray radiation above 10 keV which can not be described solely by magnetic bremsstrahlung rays .A possible reason would be inverse Compton absorption of soft photons off relativistic electrons accelerated in shocks driven into the nearby medium . If so , then these ions should have been accelerated to energies higher than 1 PeV .",
        "rewrite_text": "We present observations from the Suzaku and Swift satellites during the 2006 eruption of the recurrent nova RS Ophiuchi (RS Oph). The X-ray light curve indicates that the source reached its peak brightness approximately 50 weeks after the optical maximum, achieving an apparent luminosity of around \\(10^{38}\\) erg/s. We obtained data on nonthermal emission up to 100 keV by fitting the observed spectrum with a power-law model modified by photoelectric absorption. These results are consistent with earlier findings from other satellites, including Chandra and XMM-Newton. Additionally, we noted a significant change in the photon index between days 40-50 and days 60-70, which may be attributed to variations in the physical conditions near the central engine or alterations in the emitting region's structure. We also detected notable hard X-ray emissions above 10 keV, which cannot be solely explained by magnetic bremsstrahlung. A possible explanation for this observation is the inverse Compton scattering of soft photons by relativistic electrons accelerated in shocks that interact with the surrounding medium. If this is the case, then these electrons would need to be accelerated to energies exceeding 1 PeV.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 5.500933918218137,
        "rewrite-fast-z-score": -1.0125791108334214
    },
    {
        "original_text": "We present an algorithm for obtaining the spacetime metric from observational data, such as those obtained by the Planck satellite and other experiments. The method is based on the fact that in general relativity (GR) the Einstein field equations are equivalent to the geodesic equation for test particles. We use this equivalence to obtain the metric tensor components directly from the observed trajectories of photons emitted at different redshifts. This approach allows us to reconstruct the full four-dimensional geometry of space-time without assuming any particular model or parametrization. In order to demonstrate our technique we apply it to simulated data generated using the publicly available code CAMB. Our results show that the recovered metric agrees well with the original one used to generate the mock data. Finally, we discuss possible applications of our method to real astrophysical datasets. Cosmology has entered into precision era thanks to recent advances in experimental techniques which have allowed astronomers to measure many important quantities related to the evolution of the universe. Among these measurements there are the temperature anisotropy power spectrum measured by WMAP  1  , PLANCK  2  and SPT  3  satellites; the baryon acoustic oscillations detected through galaxy surveys  4  ; and the luminosity distance-redshift relation inferred from type Ia supernovae  5  . These new data provide unprecedented opportunities to study fundamental physics beyond the Standard Model  6  .\nIn addition to providing accurate measurements of various physical parameters describing the state of the universe today, modern cosmological experiments also allow us to probe its large-scale structure over time  7, 8  . For example, the measurement of the cosmic microwave background radiation provides information about the early stages of the universe s history when the energy density was dominated by dark matter and radiation  9  . On the other hand, the detection of distant galaxies gives access to the late stage of the universe s expansion when dark energy starts dominating  10  .",
        "watermark_text": "We present an algorithm for acquiring the spacetime metric from observational data , such as those achieved by the Planck satellite and other experiments . The method is based on the fact that in general relativity ( GR ) the Einstein field equations are comparable to the geodesic equation for test particles .We use this equivalence to obtain the metric tensor parts directly from the seen trajectories of photons generated at different redshifts . This method enables us to reconstruct the full four - dimensional topology of space - time without assuming any specific theory or parametrization .In order to test our technique we apply it to simulated evidence generated using the publicly accessible code CAMB . Our results show that the recovered metric fits well with the original one used to create the mock data .Finally , we explain possible use of our technique to real astrophysical datasets . Cosmology has entered into precision era thanks to recent developments in experimental methods which have enabled astronomers to measure various crucial variables connected to the evolution of the universe .Among these measurements there are the temperature anisotropy energy spectrum measured by WMAP 1 , PLANCK 2 and SPT 3 spacecraft ; the baryon acoustic oscillations detected through galaxy surveys 4 ; and the luminosity distance - redshift function inferred from type Ia supernovae 5 . These new data provide great opportunities to study theoretical physics beyond the Standard Model 6 .In addition to offering accurate measurements of several physical factors describing the state of the universe today , modern cosmological experiments also enable us to probe its large - scale nature over time 7 , 8 . For instance , the observation of the cosmic microwave background radiation presents knowledge about the early stages of the universe s history when the power concentration was dominated by black material and radiation 9 .On the other hand , the observation of distant galaxies provides access to the late stage of the universe s advance when dark energy starts dominating 10 .",
        "rewrite_text": "We introduce an algorithm designed to derive the spacetime metric from observational data, such as that collected by the Planck satellite and other experiments. This method leverages the analogy between the Einstein field equations in general relativity (GR) and the geodesic equation for test particles. By utilizing this relationship, we can directly extract components of the metric tensor from the observed trajectories of photons emitted at various redshifts. This approach allows us to reconstruct the complete four-dimensional topology of spacetime without reliance on any specific theory or parameterization. To validate our technique, we apply it to simulated data generated with the publicly available CAMB code. Our findings indicate that the retrieved metric closely aligns with the original one utilized for the mock data. Furthermore, we discuss potential applications of our technique to real astrophysical datasets. Thanks to recent advancements in experimental methodologies, cosmology has entered a precision era, allowing astronomers to measure critical variables related to the universe's evolution. Key measurements include the temperature anisotropy energy spectrum captured by the WMAP, PLANCK, and SPT satellites, the baryon acoustic oscillations identified through galaxy surveys, and the luminosity distance-redshift relation derived from type Ia supernovae. These new datasets present significant opportunities for exploring theoretical physics beyond the Standard Model. Additionally, contemporary cosmological experiments not only provide precise measurements of various factors describing the universe's current state but also enable investigations into its large-scale dynamics over time. For example, the study of cosmic microwave background radiation offers insights into the early universe's conditions when matter and radiation dominated, while observations of distant galaxies reveal information about the later stages of the universe's expansion, when dark energy becomes predominant.",
        "ori-fast-z-score": 0.9669875568304563,
        "water-fast-z-score": 8.273159087695738,
        "rewrite-fast-z-score": 0.34050261230349943
    },
    {
        "original_text": "We study the existence and stability properties of non-topological solitons in scalar field theories with kinetic self coupling, which are relevant to models for dark matter particles interacting via self-interactions mediated by light bosons.  We show that stable soliton solutions exist only if the mass of the boson is larger than twice the mass of the dark matter particle. For smaller masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one. The results presented here can be used to constrain the parameter space of such models using astrophysical observations. Introduction:-The possibility of new physics beyond the Standard Model (SM) has been widely discussed recently  1  . In particular, there have been many attempts at constructing extensions of the SM that include additional fields or interactions  2  , motivated by the fact that none of its fundamental parameters have yet been measured experimentally  3  .\nIn this work we consider an extension of the SM where the Higgs sector consists of two complex scalars  4  . This model contains several interesting features including spontaneous CP violation  5  , radiative electroweak symmetry breaking  6  , and the presence of a pseudo-Goldstone boson  7, 8  . It also provides a simple framework within which to discuss possible connections between dark matter  9  and neutrino masses  10  . Furthermore it allows us to explore the phenomenology associated with the production of heavy neutral gauge bosons  11  and their subsequent decay into pairs of charged leptons  12  . Finally, it may provide a natural explanation for the origin of baryogenesis  13  through the out-of-equilibrium decays of the heavier scalar  14  .\nOne feature of these models is the presence of a second scalar particle, denoted by H 0 , which mixes with the SM-like Higgs h 0  15  . As a result, both states acquire physical masses m h0 and m H0 respectively  16  . If the mixing angle θH is small then mH ≫ mh  17  . However, even when mH = mh, the couplings of the two scalars differ significantly due to the different quantum numbers carried by each state  18  .",
        "watermark_text": "We research the existence and stability properties of non - topological solitons in scalar field theories with kinetic self coupling , which are applicable to descriptions for black matter molecules interacting via self - interactions mediated by light bosons . We see that strong soliton solutions arise only if the mass of the boson is bigger than times the mass of the dark matter object .For lower masses we find unstable solitonic solutions whose lifetime decreases exponentially as the mass ratio approaches one . The results presented here can be used to constrain the parameter space of such theories using astrophysical observations .Introduction : - The possibility of new science beyond the Standard Model ( SM ) has been widely discussed recently 1 . In particular , there have been many efforts at creating extensions of the SM that include extra fields or particles 2 , driven by the fact that none of its essential parameters have ever been measured experimentally 3 .In this research we imagine an extension of the SM where the Higgs sector consists of two complex scalars 4 . This theory incorporates numerous interesting features including spontaneous CP violation 5 , radiative electroweak symmetry breaking 6 , and the presence of a quasi - Goldstone boson 7 , 8 .It additionally offers a simple context within which to consider likely link between dark matter 9 and neutrino masses 10 . Furthermore it allows us to examine the phenomenology linked with the production of large neutral gauge bosons 11 and their resulting degradation into sets of charged leptons 12 .Finally , it could give a natural explanation for the origin of baryogenesis 13 through the out - of - equilibrium decays of the heavier scalar 14 . One feature of these models is the presence of a second scalar object , denoted by H 0 , which mixes with the SM - like Higgs h 0 15 .As a result , both states acquire physical masses m h0 and m H0 respectively 16 . If the mixing angle θH is small then mH [UNK] mh 17 .However , even when mH = mh , the couplings of the two scalars differ significantly due to the different quantum values carried by each state 18 .",
        "rewrite_text": "We explore the existence and stability characteristics of non-topological solitons within scalar field theories that include kinetic self-coupling. These theories are relevant for describing dark matter structures that interact through self-interactions mediated by light bosons. Our findings indicate that robust soliton solutions emerge only when the mass of the boson exceeds a certain multiple of the dark matter object's mass. Conversely, for lower boson masses, we observe unstable solitonic solutions whose lifetimes diminish exponentially as the mass ratio approaches unity. The results of this study can be utilized to constrain the parameter space of such theories based on astrophysical observations.\n\nIntroduction: The prospect of uncovering new physics beyond the Standard Model (SM) has garnered considerable attention recently. Notably, there have been extensive efforts to develop extensions of the SM that introduce additional fields or particles, motivated by the fact that none of the SM's fundamental parameters have been experimentally determined. In this research, we consider an extension of the SM where the Higgs sector comprises two complex scalars. This framework encompasses several intriguing features, including spontaneous CP violation, radiative electroweak symmetry breaking, and the existence of a quasi-Goldstone boson. Moreover, it provides an accessible environment to explore the potential connections between dark matter and neutrino masses. It also allows us to investigate the phenomenology related to the production of large neutral gauge bosons and their subsequent decay into groups of charged leptons. Ultimately, this framework could offer a natural explanation for baryogenesis through the out-of-equilibrium decays of the heavier scalar. A notable aspect of these models is the introduction of a second scalar, referred to as H₀, which mixes with the SM-like Higgs, h₀. Consequently, both states obtain distinct physical masses, mₕ₀ and mₕ₀, respectively. If the mixing angle θₕ is small, then mₕ₀ is approximately equal to mₕ. However, even when mₕ₀ equals mₕ, the couplings of the two scalar fields can vary significantly due to the different quantum properties associated with each state.",
        "ori-fast-z-score": -1.1748539016153647,
        "water-fast-z-score": 5.564047150200578,
        "rewrite-fast-z-score": 0.16222142113076254
    },
    {
        "original_text": "We study the effect of spin-one and spin-two particles on the circularly polarized light propagating through an external magnetic field. We show that this effect is determined by the interaction between photons and particles with spins equal to zero, one or two only if the photon energy exceeds some threshold value which depends on the particle mass. For example, for electrons (mass m = 9.11×10-31 kg) it equals to 0.5 MeV. Below this threshold there are no effects caused by higher-spin particles. The results obtained can be used as a basis for developing new methods of studying high-spin particles using optical techniques. DOI: 10.1088/1742-6596/aa6b20\nI. INTRODUCTIO N\nThe problem of describing the propagation of electromagnetic waves in matter has been studied extensively over many years  1  . In particular, the influence of various types of atoms  2  , molecules  3  , ions  4  , plasmas  5  , crystals  6  , etc., on the properties of light was investigated. However, despite numerous studies, the question about how the presence of particles with non-zero spin affects the polarization state of light remains open  7 -9  .\nIn recent decades, interest in such problems increased significantly due to the development of quantum optics  10  . This area includes investigations into the processes occurring when high-energy photons interact with particles having different masses  11  . Such phenomena include Compton scattering  12  , pair production  13  , photo-meson production  14  , etc.. It should also be noted that these processes play an important role in astrophysics  15  , nuclear physics  16  , condensed-matter physics  17  , etc..\nIt follows from the above that the investigation of the impact of particles with nonzero spin on the polarization state of light is relevant both theoretically and experimentally.",
        "watermark_text": "We test the impact of spin - one and spin - two particles on the circularly polarized light propagating through an external magnetic force . We see that this effect is chosen by the interaction between photons and atoms with spins equal to zero , one or two only if the photon energy reaches some threshold quantity which depends on the particle weight .For instance , for electrons ( mass m = 9 . 11×10 - 31 kg ) it corresponds to 0 . 5 MeV . Below this threshold there are no impacts produced by higher - spin rays .The results collected can be used as a foundation for building new ways of studying high - spinning objects utilizing optical techniques . DOI : 10 . 1088 / 1742 - 6596 / aa6b20 I . INTRODUCTIO N The question of describing the propagation of electromagnetic currents in material has been studied thoroughly over numerous years 1 .In particular , the impact of several kinds of atoms 2 , compounds 3 , electrons 4 , plasmas 5 , particles 6 , etc . , on the properties of light was investigated . However , despite several studies , the question about how the presence of atoms with quasi - zero spin affects the polarization state of light remains open 7 - 9 .In past decades , awareness in such problems intensified substantially due to the development of quantum optics 10 . This area includes research into the mechanisms occurring when high - energy photons react with particles having various masses 11 .Such effects include Compton diffusion 12 , pair production 13 , photo - meson production 14 , etc . . It should also be mentioned that these mechanisms play an important role in astrophysics 15 , nuclear science 16 , condensed - matter science 17 , etc . .It follows from the above that the examination of the impact of particles with nonzero momentum on the polarization state of light is relevant both theoretically and experimentally .",
        "rewrite_text": "We investigate the influence of spin-1 and spin-2 particles on circularly polarized light that travels through an external magnetic field. Our findings indicate that this interaction occurs between photons and atoms with zero, one, or two spins, but only when the photon energy exceeds a specific threshold that depends on the mass of the particles involved. For example, for electrons (mass m = 9.11 × 10^-31 kg), this threshold is approximately 0.5 MeV. Below this energy level, higher-spin particles do not induce any significant effects. The results of our study could serve as a basis for developing new optical techniques to examine high-spin objects. \n\nDOI: 10.1088/1742-6596/aa6b20 \n\nI. INTRODUCTION \n\nThe propagation of electromagnetic waves in materials has been extensively studied over the years. In particular, researchers have explored the effects of various types of atoms, compounds, electrons, plasmas, and other particles on the properties of light. However, despite a wealth of research, the impact of quasi-zero spin atoms on the polarization state of light remains an open question. Interest in this area has significantly increased in recent decades, particularly with advancements in quantum optics. This field encompasses investigations into how high-energy photons interact with particles of different masses, leading to phenomena such as Compton scattering, pair production, and photo-meson production, among others. These mechanisms are crucial in various domains, including astrophysics, nuclear science, and condensed matter physics. Therefore, studying how particles with nonzero momentum affect the polarization state of light is both theoretically and experimentally significant.",
        "ori-fast-z-score": -0.7071067811865475,
        "water-fast-z-score": 8.8925541538888,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "We present new abundance determinations for the light neutron-capture elements Sr, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, and Lu in eight planetary nebulae (PNe). The PNe are selected to have high signal-to-noise ratio optical spectra obtained with HST/STIS or FUSE that allow accurate measurements of their elemental abundances. We find that all but one PN show evidence for enrichment by the slow-neutron capture process (s-process) relative to solar values. In most cases we can identify specific s-process contributions from individual isotopes such as 92Zr, 138Ba, 144Sm, 146Eu, 151Gd, 157Dy, 162Yb, 174Lu, 176Hf, 182W, and 205Pb. These results provide important constraints on theoretical models of nucleosynthesis in low-mass asymptotic giant branch stars. \n \n Keywords: Elemental abundances, Planetary nebula",
        "watermark_text": "We present new abundance determinations for the light neutron - capture elements Sr , Y , Zr , Ba , La , Ce , Nd , Sm , Eu , Gd , Dy , Yb , and Lu in eight planetary nebulae ( PNe ) . The PNe are chosen to have high signal - to - noise ratio optical spectra obtained with HST / STIS or FUSE that enable precise observations of their elemental abundances .We find that all but one PN show evidence for enrichment by the slow - neutron capture process ( s - process ) relative to solar values . In most cases we can identify specific s - process contributions from individual isotopes such as 92Zr , 138Ba , 144Sm , 146Eu , 151Gd , 157Dy , 162Yb , 174Lu , 176Hf , 182W , and 205Pb .These data provide important restrictions on theoretical explanations of nucleosynthesis in low - weight asymptotic giant branch stars . Keywords : Elemental abundances , Planetary nebula",
        "rewrite_text": "We report new abundance measurements for the light neutron-capture elements Sr, Y, Zr, Ba, La, Ce, Nd, Sm, Eu, Gd, Dy, Yb, and Lu in eight planetary nebulae (PNe). These PNe were selected based on having high signal-to-noise ratio optical spectra acquired using HST/STIS or FUSE, which facilitate precise determination of their elemental abundances. Our findings indicate that all but one of the PNe exhibit signs of enrichment from the slow-neutron capture process (s-process) when compared to solar values. In many instances, we can pinpoint specific s-process contributions from individual isotopes, including 92Zr, 138Ba, 144Sm, 146Eu, 151Gd, 157Dy, 162Yb, 174Lu, 176Hf, 182W, and 205Pb. This data is crucial for constraining theoretical models of nucleosynthesis in low-mass asymptotic giant branch stars. Keywords: Elemental abundances, Planetary nebula.",
        "ori-fast-z-score": 0.5897678246195885,
        "water-fast-z-score": 2.9488391230979425,
        "rewrite-fast-z-score": 2.142857142857143
    },
    {
        "original_text": "The measurement was performed at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna using the proton beam with energy E = 1 GeV. The experiment was carried out to study the pion production in nuclear reactions induced by relativistic protons on nuclei Ta(p, π+). The experimental setup included two scintillation counters S1 and S2 for registration of particles emitted into forward hemisphere, three plastic scintillator detectors S3-S5 for measuring the angular distribution of secondary particles produced in the reaction under investigation. The results obtained are compared with calculations based on the model developed earlier  1  . \nIntroduction\n\nPion production is one of the most important processes in hadronic interactions which play an essential role in many fields such as astrophysics  2  , cosmic ray physics  3  , accelerator technology  4  etc.. In this work we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta(p,π + ). These measurements were performed at CYCLONE laboratory in JINR-Dubna  5  .\nExperimental Setup\nThe experimental setup used in our experiments consisted of:  -two scintillation counters S1 and S2; -three plastic scintillator detectors; -a set of collimators; -the target made of natural tantalum foil 0.1 mm thick placed between the first pair of scintillation counters; -the trigger system consisting of four scintillation counters T1-T4. \nThe layout of the experimental setup is shown schematically in Fig. 1 . The main parameters of the detector system are listed in Table I . The signals from all detectors were recorded by means of CAMAC modules  6  .",
        "watermark_text": "The measurement was done at the Cyclotrons and Accelerators Laboratory ( CYCLONE ) in JINR , Dubna utilizing the proton beam with energy E = 1 GeV . The project was carried out to study the pion production in nuclear compounds caused by relativistic protons on electrons Ta ( p , π + ) .The demonstration system featured two scintillation terminals S1 and S2 for registration of grains generated into front hemisphere , three plastic scintillator detectors S3 - S5 for monitoring the angular distribution of secondary particles generated in the response under research . The results derived are compared with observations based on the model derived earlier 1 .Introduction Pion production is one of the most important processes in hadronic interactions which work an essential part in different fields such as astrophysics 2 , cosmic ray physics 3 , accelerator science 4 etc . . In this study we present new data on the pion production in nuclear collisions induced by relativistic protons interacting with nuclei Ta ( p , π + ) .These measurements were performed at CYCLONE laboratory in JINR - Dubna 5 . Experimental Setup The demonstration system employed in our experiments included of : - two scintillation bars S1 and S2 ; - three plastic scintillator detectors ; - a pair of collimators ; - the target made of natural tantalum foam 0 . 1 mm thick put between the first row of scintillation counters ; - the trigger system consisting of four scintillation counters T1 - T4 .The configuration of the experimental setup is demonstrated schematically in Fig . 1 .The main variables of the sensor system are listed in Table I . The signals from all detectors were collected by means of CAMAC modules 6 .",
        "rewrite_text": "The measurements were conducted at the Cyclotrons and Accelerators Laboratory (CYCLONE) in JINR, Dubna, using a proton beam with an energy of E = 1 GeV. This project aimed to investigate pion production in nuclear materials resulting from interactions of relativistic protons with tantalum nuclei in the reaction Ta (p, π+). The experimental setup included two scintillation counters, S1 and S2, for detecting particles generated in the front hemisphere, along with three plastic scintillator detectors, S3 to S5, for analyzing the angular distribution of secondary particles produced in the reactions under examination. The obtained results were compared with earlier models. \n\nPion production is a crucial process in hadronic interactions and plays a significant role across various fields, including astrophysics, cosmic ray physics, and accelerator science. In this research, we present new findings on pion production in nuclear collisions induced by relativistic protons interacting with tantalum nuclei (Ta (p, π+)). These measurements were performed at the CYCLONE laboratory in JINR-Dubna. \n\nThe experimental system used in our studies consisted of two scintillation bars (S1 and S2), three plastic scintillator detectors, a pair of collimators, and a target made of 0.1 mm thick natural tantalum foam positioned between the first row of scintillation counters. Additionally, the setup included a trigger system comprising four scintillation counters (T1 to T4). A schematic representation of the experimental configuration is provided in Fig. 1, with the main parameters of the sensor system detailed in Table I. Signals from all detectors were collected using CAMAC modules.",
        "ori-fast-z-score": 0.8867963503478639,
        "water-fast-z-score": 7.784101297497916,
        "rewrite-fast-z-score": 2.1783245945486835
    },
    {
        "original_text": "In this paper, we present an explicit formula for the maximum allowable delay in a linear time-invariant system with multiple delays by using the concept of passivity index. The proposed method is applied to a biochemical reaction network model consisting of two species interacting through three reactions. We show that our results are consistent with those obtained via numerical simulations. Finally, it should be noted that the proposed approach can also be used as a tool for analyzing other types of networks such as social or economic ones. In recent years there has been growing interest in studying complex dynamical behaviors of biological systems  1  . One important aspect of these studies concerns how different components interact within a cell  2  , which leads naturally to mathematical models based on chemical kinetics  3  .\nThe most common type of kinetic modeling involves ordinary differential equations (ODEs)  4  describing interactions between various molecular species  5  . However, due to the complexity of cellular processes  6  , many ODE models contain several state variables  7, 8  and/or parameters  9  whose values cannot always be determined experimentally  10  . This uncertainty may lead to significant errors when estimating the behavior of the underlying system  11  . To overcome this problem, stochastic approaches have recently been developed  12  . Another possibility consists in considering uncertainties in the form of unknown external disturbances  13  .",
        "watermark_text": "In this paper , we present an explicit formula for the maximum allowable delay in a linear delay - invariant system with many delays by using the notion of passivity index . The proposed approach is applied to a biochemical reaction systems system consisting of two organisms evolving through three compounds .We see that our findings are compatible with those achieved via numerical simulations . Finally , it should be mentioned that the suggested approach can also be used as a way for evaluating other types of networks such as social or economic ones .In recent years there has been growing interest in investigating complex dynamical interactions of biological complexes 1 . One important aspect of these research concerns how various components connect within a cell 2 , which results naturally to numerical models relying on chemical kinetics 3 .The most common type of kinetic modeling involves ordinary differential equations ( ODEs ) 4 describing relationships between various molecular compounds 5 . However , owing to the complexity of cellular processes 6 , many ODE descriptions contain many state components 7 , 8 and / or parameters 9 whose values never always be determined experimentally 10 .This instability may contribute to significant errors when estimating the dynamics of the underlying model 11 . To solve this question , stochastic methods have newly been used 12 .Another possibility consists in considering uncertainties in the form of uncertain external disturbances 13 .",
        "rewrite_text": "In this paper, we introduce a clear formula for determining the maximum permissible delay in a linear delay-invariant system characterized by multiple delays, utilizing the concept of the passivity index. We apply this approach to a biochemical reaction system involving two organisms interacting through three compounds. Our results align well with those obtained from numerical simulations. Additionally, it is important to note that this method can also be employed to assess different types of networks, including social and economic systems. There has been a growing interest in recent years in exploring the complex dynamic interactions within biological systems. A key focus of this research is understanding how various components interconnect within a cell, which naturally leads to numerical models based on chemical kinetics. Typically, kinetic modeling relies on ordinary differential equations (ODEs) to describe the relationships between different molecular compounds. However, due to the inherent complexity of cellular processes, many ODE models feature numerous state components and/or parameters whose exact values are not always easily determined through experimentation. This uncertainty can lead to significant errors when estimating the dynamics of the underlying model. To address this issue, recent approaches have utilized stochastic methods. Another option is to account for uncertainties arising from unpredictable external disturbances.",
        "ori-fast-z-score": -0.936585811581694,
        "water-fast-z-score": 7.181324987175317,
        "rewrite-fast-z-score": 0.5619514869490164
    },
    {
        "original_text": "The conference will be held on September 24-25 at the University of Milan-Bicocca (Italy). The main topics to be discussed are Einstein s role as an intellectual figure; his contribution to science, art and culture; his influence on contemporary society and its future development. \nThe event is organized by the Italian National Committee for UNESCO with the support of the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS) and the Institute of Advanced Studies of the University of Bologna. \n \n The conference aims to celebrate the centenary of Albert Einstein s birth and to promote the dissemination of knowledge about him among scientists, artists, students and the general public. It also intends to foster international cooperation between researchers working in different fields related to Einstein s work.  \n \n The program includes plenary lectures given by internationally renowned speakers, parallel sessions dedicated to specific themes, poster presentations and cultural events such as exhibitions, concerts and performances.",
        "watermark_text": "The meeting will be place on September 24 - 25 at the University of Milan - Bicocca ( Italy ) . The main questions to be examined are Einstein s role as an intellectual figure ; his importance to science , art and culture ; his importance on contemporary life and its future development .The event is organized by the Italian National Committee for UNESCO with the backing of the International Union of Pure and Applied Physics ( IUPAP ) , the European Physical Society ( EPS ) and the Institute of Advanced Studies of the University of Bologna . The congress aims to mark the centenary of Albert Einstein s birth and to promote the dissemination of research about him among scientists , artists , students and the general people .It additionally hopes to promote international cooperation between researchers working in different fields relevant to Einstein s study . The project includes plenary courses offered by widely renowned speakers , concurrent sessions dedicated to different issues , poster lectures and artistic activities such as shows , exhibitions and performances .",
        "rewrite_text": "The meeting is scheduled to take place from September 24 to 25 at the University of Milan - Bicocca in Italy. It will focus on several key topics, including Einstein's role as an intellectual figure, his contribution to science, art, and culture, and his impact on contemporary life and its future developments. This event is organized by the Italian National Committee for UNESCO in collaboration with the International Union of Pure and Applied Physics (IUPAP), the European Physical Society (EPS), and the Institute of Advanced Studies at the University of Bologna. The congress commemorates the centenary of Albert Einstein's birth and aims to promote research dissemination about him among scientists, artists, students, and the general public. Additionally, it seeks to foster international collaboration among researchers from various disciplines related to Einstein's work. The program will feature keynote lectures by esteemed speakers, concurrent sessions covering diverse topics, poster presentations, and artistic activities, including performances, exhibitions, and shows.",
        "ori-fast-z-score": -0.8819171036881969,
        "water-fast-z-score": 5.669467095138408,
        "rewrite-fast-z-score": 0.508000508000762
    },
    {
        "original_text": "We present an alternative method to the usual Feynman path integral description for calculating the probability amplitudes in quantum walk models, based on the concept of scattering states and their associated S-matrix elements. We show that this new formalism allows us to obtain exact results for several interesting cases where standard methods fail or are not applicable. In particular we consider two different types of boundary conditions at one end of the chain (the origin) which lead to completely different behaviour of the system as time evolves. The first type is known as Dirichlet boundary condition, corresponding to reflecting particles back into the origin after they have left it once; while the second case corresponds to absorbing particles when they reach the origin. For both these cases we calculate exactly the evolution operator over all times t > 0 using our new method. Finally, by applying the inverse Fourier transform to the evolution operator we can recover the full probability distribution function of finding the walker at any position x along the chain at time t.",
        "watermark_text": "We introduce an additional method to the usual Feynman path integral description for determining the probability amplitudes in particle walk models , using on the idea of scattering states and their accompanying S - vector elements . We see that this new formalism allows us to obtain exact findings for numerous interesting cases where standard methods fail or are not applicable .In particular we define two different kinds of boundary conditions at one end of the chain ( the origin ) which lead to totally distinct behaviour of the system as time evolves . The first sort is known as Dirichlet boundary relation , corresponding to reflecting molecules back into the origin after they have left it once ; while the second case corresponds to absorbing particles when they reach the origin .For both these circumstances we estimate exactly the evolution function over all times t > 0 using our new method . Finally , by using the inverse Fourier integral to the evolution function we can extract the full probability distribution function of finding the walker at any point x along the chain at time t .",
        "rewrite_text": "We present a new approach to the conventional Feynman path integral method for calculating probability amplitudes in particle walk models, drawing on the concept of scattering states and their associated S-vector elements. This novel formalism enables us to achieve precise results in many interesting scenarios where standard techniques fall short or are inapplicable. Specifically, we establish two distinct boundary conditions at one end of the chain (the origin) that produce markedly different system behaviors over time. The first type, known as Dirichlet boundary conditions, entails reflecting particles back to the origin after they have departed; the second type involves absorbing particles upon their arrival at the origin. For both scenarios, we accurately calculate the evolution function for all times \\( t > 0 \\) using our new method. Lastly, by applying the inverse Fourier transform to the evolution function, we can derive the complete probability distribution function for locating the walker at any point \\( x \\) along the chain at a given time \\( t \\).",
        "ori-fast-z-score": 1.2649110640673518,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 1.4269353798659745
    },
    {
        "original_text": "We present new observations in the optical, infrared (IR), and ultraviolet (UV) spectral ranges for the symbiotic binary system H1-36. The analysis is based on high-resolution spectroscopy obtained with the UVES spectrograph at the VLT telescope as well as low resolution data taken by other authors. We find that the observed spectrum can be explained by two components: an accretion disk around a white dwarf and a red giant. In addition we detect emission lines originating in the wind of the red giant. Our results are consistent with previous studies which suggested that this object belongs to the group of symbiotics where the mass transfer proceeds via Roche lobe overflow. \n \n Keywords: Symbiosis, Binary systems, Red giants, Accreting binaries, Winds, Mass loss, Spectroscopy, Ultraviolet radiation, White dwarfs, Emission lines, Stellar winds",
        "watermark_text": "We present new images in the optical , infrared ( IR ) , and ultraviolet ( UV ) spectral regions for the symbiotic binary system H1 - 36 . The comparison is based on wide - resolution spectroscopy acquired with the UVES spectrograph at the VLT telescope as well as low resolution data taken by other researchers .We see that the seen spectrum can be described by two parts : an accretion disk around a white dwarf and a red dwarf . In addition we find emission lines originating in the wind of the red dwarf .Our results are compatible with previous research which suggested that this body belongs to the group of symbiotics where the mass transfer continues via Roche lobe overflow . Keywords : Symbiosis , Binary systems , Red giants , Accreting binaries , Winds , Mass loss , Spectroscopy , Ultraviolet radiation , White dwarfs , Emission lines , Stellar winds",
        "rewrite_text": "We present new optical, infrared (IR), and ultraviolet (UV) images of the symbiotic binary system H1-36. Our analysis is based on high-resolution spectroscopy obtained using the UVES spectrograph at the VLT telescope, alongside low-resolution data from other researchers. The observed spectrum can be divided into two components: an accretion disk surrounding a white dwarf and a red dwarf. Additionally, we detect emission lines that originate from the wind of the red dwarf. Our findings align with previous studies that classified this system as part of the group of symbiotic binaries, where mass transfer occurs through Roche lobe overflow. **Keywords**: Symbiosis, Binary systems, Red giants, Accreting binaries, Stellar winds, Mass loss, Spectroscopy, Ultraviolet radiation, White dwarfs, Emission lines.",
        "ori-fast-z-score": -0.9271726499455306,
        "water-fast-z-score": 3.3113308926626095,
        "rewrite-fast-z-score": -0.40451991747794525
    },
    {
        "original_text": "We present an improved factorization formula for the fragmentation functions (FFs) of hadrons containing one heavy quark, which is valid in both leading order and next-to-leading order QCD perturbation theory. The new formula takes into account all possible contributions to the FFs at each perturbative order. We show that our results are consistent with those obtained by using other approaches such as the operator product expansion method or the renormalization group equation approach. Finally we give numerical predictions on some important quantities related to the charm-quark FFs. PACS numbers: 12.38.Qk, 13.25.Gv, 11.15.Tk \nI. INTRODUCTORY REMARK\nThe fragmentation function D(z), where z = Phadron/Pquark , describes how quarks fragment into hadrons when they are produced in hard processes like deep-inelastic scattering  1  . It plays an essential role in understanding many phenomena observed experimentally  2  .\nIn this work, we will study the fragmentation functions of hadronic states containing only one heavy quark. In particular, we consider the case of charmed-meson production in e + e-annihilation processes:",
        "watermark_text": "We introduce an updated factorization formula for the fragmentation functions ( FFs ) of hadrons containing one heavy quark , which is valid in both leading order and last - to - leading order QCD perturbation theory . The revised formula takes into consideration all possible contributions to the FFs at each perturbative order .We see that our findings are compatible with those achieved by using other methods such as the operator product expansion method or the renormalization group function method . Finally we give quantitative predictions on some important quantities related to the charm - quark FFs .PACS codes : 12 . 38 . Qk , 13 . 25 . Gv , 11 . 15 . Tk I . INTRODUCTORY REMARK The fragmentation relation D ( z ) , where z = Phadron / Pquark , explains how quarks cluster into hadrons when they are produced in hard processes like shallow - inelastic scattering 1 .It plays an essential part in understanding several phenomena observed experimentally 2 . In this research , we will research the fragmentation processes of hadronic states involving only one heavy quark .In particular , we treat the case of charmed - meson production in e + e - annihilation processes :",
        "rewrite_text": "We present a revised factorization formula for the fragmentation functions (FFs) of hadrons containing a single heavy quark, applicable in both leading order and next-to-leading order QCD perturbation theory. This updated formula incorporates all possible contributions to the FFs at each order of perturbation theory. Our results are consistent with findings obtained through other approaches, such as the operator product expansion method and the renormalization group function method. Additionally, we offer quantitative predictions for several key quantities related to charm-quark FFs. PACS codes: 12.38.Qk, 13.25.Gv, 11.15.Tk. \n\nI. INTRODUCTORY REMARK: The fragmentation relation D(z), where z = Phadron / Pquark, describes how quarks combine to form hadrons during hard processes like shallow inelastic scattering. This relation is crucial for understanding various experimentally observed phenomena. In this study, we will investigate the fragmentation processes of hadronic states involving a single heavy quark, focusing specifically on charmed meson production in e+e- annihilation processes.",
        "ori-fast-z-score": -0.11867816581938533,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "We present an exact expression for the energy density functional in terms of the local electronic charge and spin densities, which is valid for any number N of electrons on a two-dimensional jellium surface with arbitrary spin-orbit interaction strength. The resulting sum rules are shown to be equivalent to those derived by Stillinger and Lovett (SL) for the case of zero spin-orbit coupling but they also include additional contributions due to this term. We show that these new terms can be expressed as functions of the SL parameters only. This result allows us to obtain explicit expressions for all the relevant physical quantities such as the exchange-correlation potential or the magnetization profile at finite temperature. Finally we discuss how our results could be used to improve existing approximations within Density Functional Theory. PACS: 71.10.Pq - Energy-densities; 72.20.Fd - Energy-density functionals; 73.40.Gk - Spin-polarized systems",
        "watermark_text": "We present an precise representation for the power density functional in terms of the local electronic charge and spin densities , which is valid for any number N of electrons on a two - dimensional jellium surface with arbitrary spin - orbit interaction strength . The resulting sum rules are shown to be analogous to those developed by Stillinger and Lovett ( SL ) for the case of zero spin - orbit bonding but they still provide additional contributions due to this term .We see that these new terms can be described as functions of the SL parameters only . This result allows us to obtain precise expressions for all the appropriate physical components such as the transfer - correlation potential or the magnetization profile at finite temperature .Finally we talk how our findings may be used to improve established approximations within Density Functional Theory . PACS : 71 . 10 . Pq - Energy - densities ; 72 . 20 . Fd - Energy - density functionals ; 73 . 40 . Gk - Spin - polarized systems",
        "rewrite_text": "We present an accurate formulation of the power density functional in relation to local electronic charge and spin densities, applicable to any number N of electrons on a two-dimensional jellium surface, regardless of the strength of the spin-orbit interaction. The resulting sum rules are found to be analogous to those established by Stillinger and Lovett (SL) for the case of zero spin-orbit bonding, while also incorporating additional contributions from the spin-orbit term. These new terms can be expressed solely in terms of the SL parameters. This finding enables us to derive precise expressions for key physical components, including the transfer-correlation potential and the magnetization profile at finite temperatures. Finally, we discuss how our results can enhance existing approximations within Density Functional Theory. PACS: 71.10.Pq - Energy densities; 72.20.Fd - Energy density functionals; 73.40.Gk - Spin-polarized systems.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 3.6927447293799815,
        "rewrite-fast-z-score": 0.7385489458759964
    },
    {
        "original_text": "We present Very Long Baseline Array (VLBA) radio images and multi-epoch optical photometry for 7 blazars selected from the Roma-BZCAT catalog as candidates to be members of the Red Giants Branch (RGB). The sources are located at redshifts between 0.1 and 1, with luminosities ranging from 1045 erg/s to 1012 erg/s in the rest frame range 2-10 keV. We find that all but one source show compact cores on milliarcsecond scales; only RGB J0152+017 shows an extended structure. All these results suggest that most of our targets belong to the class of Flat Spectrum Radio Quasars rather than Blazar-like AGNs. In addition we report new spectroscopic data obtained by us or taken from literature which confirm this hypothesis. \n \n Keywords: Blazars, VLBI, Optical variability, X-ray emission, Red giants branch",
        "watermark_text": "We create Very Long Baseline Array ( VLBA ) broadcast photographs and multi - epoch optical photometry for 7 blazars chosen from the Roma - BZCAT catalog as candidates to be members of the Red Giants Branch ( RGB ) . The sources are situated at redshifts between 0 . 1 and 1 , with luminosities ranging from 1045 erg / s to 1012 erg / s in the rest frame range 2 - 10 keV .We see that all but one source demonstrate smooth cores on milliarcsecond intervals ; only RGB J0152 + 017 shows an extended structure . All these results show that most of our targets represent to the class of Flat Spectrum Radio Quasars rather than Blazar - like AGNs .In addition we publish new spectroscopic data received by us or taken from literature which confirm this hypothesis . Keywords : Blazars , VLBI , Optical variability , X - ray radiation , Red stars branch",
        "rewrite_text": "We generate Very Long Baseline Array (VLBA) broadcast images and multi-epoch optical photometry for seven blazars selected from the Roma-BZCAT catalog as potential members of the Red Giants Branch (RGB). These sources have redshifts ranging from 0.1 to 1, with luminosities spanning from 10^45 erg/s to 10^12 erg/s in the 2-10 keV rest frame range. Our observations reveal that, with the exception of one source, all exhibit smooth cores on milliarcsecond scales; only RGB J0152 + 017 displays an extended structure. These findings suggest that most of our targets align more closely with the category of Flat Spectrum Radio Quasars rather than Blazar-like AGNs. Additionally, we present new spectroscopic data obtained from our research or sourced from existing literature that supports this conclusion. Keywords: Blazars, VLBI, Optical variability, X-ray emissions, Red Giants Branch.",
        "ori-fast-z-score": 0.15249857033260467,
        "water-fast-z-score": 4.320493798938573,
        "rewrite-fast-z-score": 0.7453559924999299
    },
    {
        "original_text": "We report on the polarization dependent photoluminescence (PL) and Raman scattering spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum shows that the emission is polarized along the direction perpendicular to the excitation light, which can be explained by the selection rules for dipole transitions between electronic states with different angular momenta. In addition we observe an anisotropic broadening of the Stokes linewidths as well as a splitting into two components when exciting circularly polarized light. These effects are attributed to the presence of exciton fine structure due to spin-orbit coupling. We also find evidence for a strong electron-phonon interaction leading to phonon sidebands in both the Stokes and anti-Stokes parts of the Raman spectrum. Finally, we show how these results can be used to determine the orientation of individual QDs embedded in a polymer matrix. Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy.",
        "watermark_text": "We report on the polarization dependent photoluminescence ( PL ) and Raman absorption spectra of CdSe / CdS core / shell quantum dots in solution at room temperature . The PL spectrum reveals that the emission is polarized along the direction perpendicular to the excitation light , which can be described by the selection rules for dipole changes between electronic states with various angular momenta .In addition we witness an anisotropic broadening of the Stokes linewidths as well as a dividing into two parts when exciting circularly polarized light . These effects are traced to the presence of exciton fine structure owing to spin - orbit bonding .We additionally find proof for a powerful atom - phonon interaction leading to phonon sidebands in both the Stokes and anti - Stokes parts of the Raman spectrum . Finally , we find how these results can be used to predict the orientation of different QDs integrated in a polymer matrix .Polarized luminescence measurements have been performed on single QD emitters using confocal microscopy .",
        "rewrite_text": "We present our findings on the polarization-dependent photoluminescence (PL) and Raman absorption spectra of CdSe/CdS core/shell quantum dots in solution at room temperature. The PL spectrum indicates that the emission is polarized in a direction perpendicular to the excitation light, consistent with selection rules governing dipole transitions between electronic states with different angular momenta. Furthermore, we observe an anisotropic broadening of the Stokes linewidths, which splits into two distinct components when circularly polarized light is used for excitation. These phenomena are attributed to the exciton fine structure arising from spin-orbit coupling. Additionally, we provide evidence of strong atom-phonon interactions that result in phonon sidebands observable in both the Stokes and anti-Stokes regions of the Raman spectrum. Lastly, we discuss how these findings can help in predicting the orientation of various quantum dots integrated within a polymer matrix. Polarized luminescence measurements were conducted on individual quantum dot emitters using confocal microscopy.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 5.5,
        "rewrite-fast-z-score": 0.6201736729460423
    },
    {
        "original_text": "We present an algorithm for the numerical evaluation of Feynman diagrams with arbitrary numbers of external particles and internal loops, which is based on the concept of  partonic subdiagrams . The method allows to perform calculations in QCD beyond leading order accuracy without any approximations or assumptions about the kinematics of the process under consideration. We demonstrate its applicability by calculating the next-to-leading-order corrections to the production cross section of heavy quarks at hadron colliders. In this talk we will discuss how one can gain analytic control over parton showers using the concept of  partons  as fundamental degrees of freedom. This approach has been developed recently within the framework of Soft-Collinear Effective Theory (SCET)  1  . It provides a systematic way to resum large logarithms associated with collinear splittings into multiple jets  2  , thereby improving our understanding of jet physics  3  .\nThe basic idea behind SCET is that physical observables are described by matrix elements involving soft and/or collinear fields only  4  . These fields have nontrivial transformation properties under boosts along the beam axis  5  . They allow us to separate hard interactions from soft radiation  6  . As a result, it becomes possible to systematically factorize contributions to scattering amplitudes into  hard functions  describing short-distance dynamics  7, 8  and  semi-hard functions  encoding information about the emission of soft gluons  9  .",
        "watermark_text": "We present an algorithm for the numerical identification of Feynman diagrams with arbitrary numbers of external particles and internal loops , which is based on the idea of partonic subdiagrams . The method enables to conduct measurements in QCD beyond trailing order accuracy without any approximations or assumptions about the kinematics of the process under consideration .We test its applicability by calculating the second - to - leading - order corrections to the production cross section of large quarks at hadron colliders . In this talk we will explore how one can obtain analytic control over parton showers using the idea of partons as essential degrees of liberty .This method has been constructed recently within the framework of Soft - Collinear Effective Theory ( SCET ) 1 . It provides a comprehensive way to resum big logarithms associated with collinear splittings into multiple jets 2 , thereby improving our appreciation of flight mechanics 3 .The basic idea behind SCET is that physical observables are explained by matrix elements featuring soft and / or collinear fields only 4 . These fields have nontrivial transformation qualities under boosts along the laser axis 5 .They allow us to separate hard interactions from soft light 6 . As a result , it becomes possible to deliberately factorize contributions to scattering amplitudes into hard functions describing short - distance dynamics 7 , 8 and semi - hard functions encoding information about the emission of deep gluons 9 .",
        "rewrite_text": "We introduce an algorithm for the numerical identification of Feynman diagrams featuring arbitrary quantities of external particles and internal loops, leveraging the concept of partonic subdiagrams. This approach allows for measurements in quantum chromodynamics (QCD) that exceed leading-order accuracy without relying on any approximations or assumptions regarding the process's kinematics. We evaluate its effectiveness by calculating the second-order corrections to the production cross section of heavy quarks at hadron colliders. In this presentation, we will discuss how to achieve analytic control over parton showers by treating partons as fundamental degrees of freedom. This method has recently been developed within the framework of Soft-Collinear Effective Theory (SCET). It offers a systematic way to resum large logarithms related to collinear splittings into multiple jets, enhancing our understanding of flight mechanics. The core principle of SCET is that physical observables can be expressed in terms of matrix elements that involve only soft and/or collinear fields. These fields exhibit complex transformation properties under boosts along the laser axis, allowing us to distinguish between hard interactions and soft emissions. Consequently, it becomes feasible to intentionally separate contributions to scattering amplitudes into hard functions that describe short-distance dynamics and semi-hard functions that capture the emission characteristics of soft gluons.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 6.874054958970533,
        "rewrite-fast-z-score": 1.616244071283537
    },
    {
        "original_text": "We report on the first demonstration of quantum key distribution (QKD) with high bit rates and low error rates in an optical fiber link spanning more than 100 km, including 20 km of standard telecom fibers and 80 km of dispersion-shifted fibers. The QKD system uses polarization encoding and decoy-state BB84 protocol to generate secret keys at a rate up to 1 Mbit/s per user. We use two types of single-photon detectors based on InGaAs/InP avalanche photodiodes operated either in Geiger mode or as gated-mode single-photon counters. To overcome the detector dark count noise we employ active feed-forward techniques that allow us to achieve a secure key generation rate of 0.5 Mbit/s for each user. This is the highest secure key generation rate reported so far for QKD systems operating beyond 50 km transmission distance. Quantum Key Distribution (QKD), which allows two remote users to share a secret key by exchanging quantum states through insecure channels  1  , has attracted great interest recently due to its potential applications in both military and commercial fields  2  . However, most existing QKD experiments are limited to short-distance transmissions because of the extremely weak intensity of single photons  3  .\nRecently, several groups have demonstrated QKD over distances longer than 50km  4  -  8  . These demonstrations were made possible thanks to the development of efficient single-photon detectors  9  -  11  and advanced data post-processing algorithms  12  -  14  . Nevertheless, these results still suffer from relatively low key generation rates mainly caused by the large dark counts of the employed single-photon detectors  15  .",
        "watermark_text": "We report on the first test of quantum key spread ( QKD ) with high bit rates and low mistake rates in an optical cable link spanning more than 100 km , comprising 20 metres of standard telecom cables and 80 kilometres of dispersion - shifted strands . The QKD system utilizes polarization encoding and decoy - state BB84 protocol to produce secret keys at a rate up to 1 Mbit / s per user .We use two forms of single - photon detectors based on InGaAs / InP avalanche photodiodes running either in Geiger mode or as gated - mode single - photon counters . To solve the sensor dark count noise we utilize active feed - forward strategies that enable us to achieve a secure key production speed of 0 . 5 Mbit / s for each user .This is the highest protected key production speed observed so far for QKD units running beyond 50 km communication distance . Quantum Key Distribution ( QKD ) , which allows two remote users to exchange a secret key by exchanging quantum states through insecure channels 1 , has garnered great concern lately owing to its potential applications in both military and commercial fields 2 .However , most existing QKD tests are limited to short - distance broadcasts because of the exceptionally weak intensity of single photons 3 . Recently , various groups have demonstrated QKD over distances higher than 50km 4 - 8 .These demonstrations were made possible due to the development of effective single - photon detectors 9 - 11 and advanced information post - processing algorithms 12 - 14 . Nevertheless , these results nonetheless suffer from fairly little key production rates mainly caused by the huge light counts of the utilized single - photon detectors 15 .",
        "rewrite_text": "We present the initial test of quantum key distribution (QKD) with high bit rates and low error rates over an optical cable link exceeding 100 km, which includes 20 meters of standard telecom cables and 80 kilometers of dispersion-shifted fibers. The QKD system employs polarization encoding along with the decoy-state BB84 protocol, enabling the generation of secret keys at rates of up to 1 Mbit/s per user. We utilize two types of single-photon detectors based on InGaAs/InP avalanche photodiodes, operating in either Geiger mode or gated mode as single-photon counters. To address sensor dark count noise, we implement active feed-forward strategies, allowing us to achieve a secure key production rate of 0.5 Mbit/s per user. This is the highest secure key generation rate recorded for QKD systems operating over communication distances greater than 50 km. Quantum Key Distribution (QKD) facilitates the exchange of a secret key between two remote users by transmitting quantum states through insecure channels, which has recently attracted significant interest due to its potential military and commercial applications. However, most current QKD tests are confined to short-distance transmissions due to the extremely weak intensity of single photons. Recently, several groups have successfully demonstrated QKD over distances exceeding 50 km. These advancements were made possible by the development of efficient single-photon detectors and refined information post-processing algorithms. Despite these achievements, the observed key production rates remain relatively low, primarily due to the high light count associated with the single-photon detectors used.",
        "ori-fast-z-score": -0.5144957554275265,
        "water-fast-z-score": 7.37443916112788,
        "rewrite-fast-z-score": 2.648508738921959
    },
    {
        "original_text": "We study the nonlinear evolution of baryonic acoustic oscillations (BAO) in an expanding universe using numerical simulations with N-body particles and hydrodynamic gas dynamics. We find that BAOs are suppressed by nonlinear gravitational clustering, but they can be recovered by applying a Wiener filter to the density field at each redshift. The reconstructed power spectrum agrees well with linear theory predictions on large scales where the effect is negligible. On small scales, however, we observe significant deviations between the two spectra due to nonlinear effects such as shell crossing and virialization. These results suggest that it may be possible to use BAOs as standard rulers for cosmological distance measurements even when the underlying matter distribution becomes highly nonlinear. In this work, we present our main findings below. \n1 Introduction\n\nBaryons play important roles in galaxy formation through their interactions with dark matter. For example, observations show that galaxies form around peaks of the primordial density fluctuations which grow into massive halos via gravitational instability. Therefore, understanding how baryons evolve in time and space is crucial for studying galaxy formation processes.\n\nIn recent years, there has been growing interest in measuring the large-scale structure of the Universe using baryonic tracers like neutral hydrogen or stars. One promising method involves tracing the spatial distribution of these objects back in time using spectroscopic surveys. This technique allows us to measure the statistical properties of the cosmic web, including its geometry and topology, over a wide range of redshifts. \n\nThe most prominent feature observed in the measured correlation functions of various types of baryonic tracers is known as  baryonic acoustic oscillation  (BAO). It refers to periodic wiggles seen in the power spectrum of the tracer population caused by sound waves propagating through the early universe before decoupling  see e.g., 1  . Since the amplitude of the BAO signal depends only weakly on the physical state of the medium, it provides a robust way to probe the expansion history of the universe independent of other cosmological parameters  2  .\nRecently, several groups have reported detections of the BAO signature in the correlation function of Lyman",
        "watermark_text": "We research the nonlinear development of baryonic sound oscillations ( BAO ) in an increasing universe using numerical simulations with N - bodies particles and hydrodynamic gas mechanics . We see that BAOs are suppressed by nonlinear gravity clustering , but they can be recovered by using a Wiener filter to the density field at each redshift .The reconstructed power spectrum agrees well with continuous theory estimates on huge scales where the impact is negligible . On short scales , however , we perceive considerable deviations between the two spectra due to nonlinear effects such as shell crossing and virialization .These data suggest that it could be possible to use BAOs as conventional rulers for cosmological distance measurements even when the underlying matter distribution appears extremely nonlinear . In this research , we present our major findings below .1 Introduction Baryons serve important roles in universe formation through their processes with black material . For instance , observations show that galaxies form around spikes of the primordial density fluctuations which build into huge halos via gravitational instability .Therefore , studying how baryons change in time and space is crucial for studying galaxy formation systems . In past decades , there has been growing interest in measuring the huge - scale organization of the Universe using baryonic tracers like neutral hydrogen or stars .One promising tool includes tracing the spatial distribution of these objects back in time utilizing spectroscopic studies . This method enables us to measure the statistical characteristics of the cosmic web , notably its topology and configuration , over a broad variety of redshifts .The most notable feature observed in the reported correlation functions of several kinds of baryonic tracers is known as baryonic audio oscillation ( BAO ) . It refers to periodic wiggles found in the power spectrum of the tracer population resulting by sound signals propagating through the early universe before decoupling see e . g . , 1 .Since the frequency of the BAO signal relies only faintly on the physical state of the medium , it gives a reliable way to probe the evolution period of the universe independent of other cosmological factors 2 . Recently , various groups have reported detections of the BAO signature in the correlation function of Lyman",
        "rewrite_text": "We investigate the nonlinear evolution of baryonic acoustic oscillations (BAOs) in an expanding universe through numerical simulations involving N-body particles and hydrodynamic gas dynamics. Our findings indicate that BAOs are diminished by nonlinear gravitational clustering; however, they can be restored using a Wiener filter applied to the density field at each redshift. The reconstructed power spectrum shows strong agreement with theoretical predictions on large scales, where their effects are minimal. Conversely, significant discrepancies between the two spectra are noted on smaller scales, driven by nonlinear phenomena such as shell crossing and virialization. These results imply that BAOs may still serve as effective standard rulers for cosmological distance measurements, even in regions of highly nonlinear matter distribution. In this paper, we detail our key findings.\n\n1. Introduction: Baryons play a crucial role in the formation of the universe through their interactions with dark matter. For example, observations reveal that galaxies emerge around peaks of primordial density fluctuations, accumulating into massive halos through gravitational instability. Thus, understanding the temporal and spatial behavior of baryons is essential for exploring galaxy formation processes. Over recent decades, interest has surged in examining the large-scale structure of the universe using baryonic tracers such as neutral hydrogen and stars. One promising approach is to trace the historical spatial distribution of these tracers through spectroscopic analyses. This technique allows us to assess the statistical properties of the cosmic web, particularly its topology and structure, across a wide range of redshifts. A prominent feature identified in the correlation functions of various baryonic tracers is the baryonic acoustic oscillation (BAO). This phenomenon manifests as periodic oscillations detected in the power spectrum of tracer populations, resulting from sound waves propagating through the early universe prior to recombination. Since the frequency of the BAO signal is only weakly dependent on the medium's physical state, it offers a reliable method for probing the universe's evolutionary history, largely independent of other cosmological variables. Recently, numerous studies have reported observations of the BAO signature in the correlation function of Lyman-alpha forests.",
        "ori-fast-z-score": -2.1602468994692865,
        "water-fast-z-score": 9.363261113663055,
        "rewrite-fast-z-score": -1.1607348488012053
    },
    {
        "original_text": "We report on four new quasars at redshifts z > 6, found in the Canada France Hawaii Telescope Legacy Survey (CFHTLS). The objects were selected as part of an ongoing survey for high-z quasars using photometric data obtained with CFHT and Spitzer Space Telescope. We present their optical to near-infrared SEDs, which are well fitted by composite quasar templates. Their luminosities range between 1.5 x 10^14 erg s-1 cm-2 and 2.1 x 10^15 erg s-1 cm-2 . These results show that there is still room for discovering very luminous quasars beyond redshift six. They also provide further evidence that supermassive black holes grew rapidly during this early phase of galaxy formation. Four quasars have been discovered at redshifts greater than 6 in the Canada France Hawaii telescope legacy survey (CFHTLS) by combining deep infrared observations taken with the Spitzer space telescope with optical data collected with the Canada France Hawaii telescope.",
        "watermark_text": "We report on four newest quasars at redshifts z > 6 , detected in the Canada France Hawaii Telescope Legacy Survey ( CFHTLS ) . The bodies were chosen as part of an continuing survey for high - z quasars using photometric data acquired with CFHT and Spitzer Space Telescope .We present their optical to near - infrared SEDs , which are well fitted by composite quasar templates . Their luminosities range between 1 . 5 x 10 ^ 14 erg s - 1 cm - 2 and 2 . 1 x 10 ^ 15 erg s - 1 cm - 2 .These data demonstrate that there is still space for producing very luminous quasars beyond redshift six . They also suggest further evidence that supermassive black holes grew rapidly during this earliest stage of galaxy formation .Four quasars have been detected at redshifts greater than 6 in the Canada France Hawaii telescope heritage survey ( CFHTLS ) by combining dark infrared observations made with the Spitzer space telescope with imaging information collected with the Canada France Hawaii telescope .",
        "rewrite_text": "We present the discovery of four newly identified quasars at redshifts greater than 6, detected through the Canada France Hawaii Telescope Legacy Survey (CFHTLS). These quasars were selected as part of an ongoing investigation into high-redshift quasars, utilizing photometric data obtained from both the CFHT and the Spitzer Space Telescope. We provide their spectral energy distributions (SEDs) from optical to near-infrared wavelengths, which are accurately modeled by composite quasar templates. Their luminosities range from 1.5 x 10^14 erg s^-1 cm^-2 to 2.1 x 10^15 erg s^-1 cm^-2. This data indicates that there is still potential for the formation of highly luminous quasars beyond redshift six and offers further evidence of the rapid growth of supermassive black holes during the early phases of galaxy formation. The detection of these four quasars was achieved by integrating infrared observations from the Spitzer Space Telescope with imaging data collected from the Canada France Hawaii Telescope.",
        "ori-fast-z-score": -0.2672612419124244,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": 0.8320502943378437
    },
    {
        "original_text": "The Large Area Telescope (LAT) is one of two instruments aboard NASA s Fermi Gamma-ray Space Telescope, launched in June 2008. The LAT consists of an array of silicon strip trackers and cesium iodide calorimeters that detect gamma rays with energies between 20 MeV to more than 300 GeV. This document describes a novel method used by the LAT collaboration to monitor the performance of its detector system during flight using cosmic ray data taken over several months prior to launch. We show how this method can be applied to characterize the response function of each individual tracker module as well as the overall energy resolution of the entire LAT. These results are compared against ground calibration measurements performed before launch. Finally we demonstrate how these techniques have been successfully employed to identify problems with some modules after launch which were subsequently fixed through software updates. The Large Area Telescope (L AT ) is one of two instruments flown on NASA s Fermi Gamma-Ray Space Telescope  1  . Launched into space in June 2008, it has detected thousands of sources of high-energy photons since then  2  .\nIn order to perform such observations, the L AT must accurately measure the direction and energy of incoming photons. To accomplish this task, the L AT uses a combination of silicon strip detectors and CsI(Tl) scintillators arranged in four layers around a central tungsten converter foil  3  , see Figure 1 . Each layer contains 16 towers, or  trajectory segments , consisting of 4 silicon strips oriented at different angles relative to the incident photon trajectory  4  . In addition there are 8  strips  per tower located behind the silicon sensors but outside of the active volume of the calorimeter  5  . Together they form a total of 56 independent tracking channels  6  .",
        "watermark_text": "The Large Area Telescope ( LAT ) is one of two instruments aboard NASA s Fermi Gamma - ray Space Telescope , launched in June 2008 . The LAT consists of an ensemble of silicon strip trackers and cesium iodide calorimeters that detect beta particles with energies between 20 MeV to more than 300 GeV .This text explains a new method employed by the LAT collaboration to study the performance of its detector network during mission utilizing cosmic ray data taken over several months previously to launch . We indicate how this technology can be applied to characterize the response function of each individual tracker module as also as the overall energy resolution of the entire LAT .These data are compared against ground calibration measurements completed before flight . Finally we prove how these procedures have been successfully utilized to identify issues with some modules after launch which were later resolved through technology updates .The Large Area Telescope ( L AT ) is one of two instruments flown on NASA s Fermi Gamma - Ray Space Telescope 1 . Launched into space in June 2008 , it has detected many of sources of high - energy photons since then 2 .In order to conduct such observations , the L AT requires properly study the direction and energy of incoming photons . To accomplish this objective , the L AT employs a combination of silicon strip detectors and CsI ( Tl ) scintillators grouped in four layers around a central tungsten converter foil 3 , see Figure 1 .Each layer contains 16 towers , or path sectors , consisting of 4 silicon patches aligned at different angles relative to the incident photon trajectory 4 . In addition there are 8 layers per tower situated behind the silicon detector but outside of the active volume of the calorimeter 5 .Together they create a total of 56 independent tracking channels 6 .",
        "rewrite_text": "The Large Area Telescope (LAT) is one of the two instruments aboard NASA's Fermi Gamma-ray Space Telescope, which was launched in June 2008. The LAT features a collection of silicon strip trackers and cesium iodide calorimeters designed to detect beta particles with energies ranging from 20 MeV to over 300 GeV. This text outlines a novel approach developed by the LAT collaboration to assess the performance of its detector network during the mission, utilizing cosmic ray data collected for several months prior to the launch. We illustrate how this technique can be used to characterize the response function of each individual tracker module as well as the overall energy resolution of the LAT. The results of these analyses are compared with ground calibration measurements conducted before the mission. Furthermore, we demonstrate how these procedures have effectively identified issues with certain modules after the launch, which were subsequently addressed through technological updates. The LAT has been operational since its launch in June 2008, detecting a variety of high-energy photon sources. To enable such observations, the LAT must accurately determine the direction and energy of incoming photons. It achieves this by integrating silicon strip detectors and CsI(Tl) scintillators arranged in four layers surrounding a central tungsten converter foil. Each layer consists of 16 towers or path sectors, featuring four silicon patches oriented at different angles to the incoming photon trajectory. Accompanying these are eight layers per tower situated behind the silicon detector but outside the active volume of the calorimeter. Together, these components form a total of 56 independent tracking channels.",
        "ori-fast-z-score": 0.3682298471593294,
        "water-fast-z-score": 7.425257825928512,
        "rewrite-fast-z-score": 1.9755138236055543
    },
    {
        "original_text": "We introduce the concept of generalized conditional random fields (GCRFs) and show how they can be used to model arbitrary probability distributions over structured data sets, such as sequences or trees.  We present an efficient algorithm for learning GCRF parameters using gradient descent on the log-likelihood objective function.   Finally we demonstrate that our approach is able to learn accurate models for several challenging sequence labeling tasks including part-of-speech tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001 ) are undirected graphical models which have been successfully applied to many problems involving sequential data, e.g. (Sha & Pereira, 2003) . In this work, we propose Generalized Conditional Random Fields (GCRFs), a generalization of CRFs which allows us to represent any distribution over structured data sets like sequences or trees. The key idea behind GCRFs is to use a set of latent variables to capture dependencies between different parts of the input space. This enables us to efficiently compute the partition function required by standard CRFs with dynamic programming techniques. Furthermore, it also makes it possible to train GCRFs using gradient-based methods similar to those employed for Maximum Entropy Markov Models (MEMMs). To evaluate the performance of our method, we apply it to two important applications: part-of-speech taggin",
        "watermark_text": "We introduce the idea of generalized conditional random fields ( GCRFs ) and explain how they can be used to model arbitrary likelihood distributions over structured data sets , such as sequences or trees . We introduce an efficient algorithm for learning GCRF variables using gradient descent on the log - likelihood objective function .Finally we prove that our approach is ability to acquire precise models for numerous challenging gene labeling challenges including whole - of - voice tagging in natural language processing and protein secondary structure prediction in bioinformatics . Conditional Random Fields ( CRFs ) ( Lafferty et al . , 2001 ) are undirected numerical models which have been successfully applied to many difficulties involving sequential data , e . g .( Sha & Pereira , 2003 ) . In this research , we propose Generalized Conditional Random Fields ( GCRFs ) , a generalization of CRFs which allows us to model any distribution over structured data sets like sequences or trees .The main idea behind GCRFs is to use a setting of latent variables to capture dependencies between various parts of the input space . This enables us to easily compute the partition function required by traditional CRFs with dynamic programming tools .Furthermore , it also makes it able to train GCRFs using gradient - based methods similar to those utilized for Maximum Entropy Markov Models ( MEMMs ) . To assess the performance of our technique , we apply it to two essential applications : whole - of - voice taggin",
        "rewrite_text": "We introduce the concept of generalized conditional random fields (GCRFs) and illustrate their potential for modeling arbitrary likelihood distributions over structured datasets, including sequences and trees. Our work presents an efficient algorithm for learning GCRF variables using gradient descent based on the log-likelihood objective function. We demonstrate that our approach can effectively develop accurate models for a variety of complex gene labeling tasks, such as whole-of-voice tagging in natural language processing and protein secondary structure prediction in bioinformatics. Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected numerical models that have proven to be effective for various challenges involving sequential data (Sha & Pereira, 2003). In this research, we propose GCRFs as an extension of CRFs that permits the modeling of any distribution over structured datasets like sequences or trees. The core idea of GCRFs is to incorporate latent variables that capture dependencies among different components of the input space, which allows for the efficient computation of the partition function needed by traditional CRFs using dynamic programming techniques. Additionally, this approach enables the training of GCRFs with gradient-based methods akin to those used in Maximum Entropy Markov Models (MEMMs). To evaluate the effectiveness of our method, we apply it to two significant applications: whole-of-voice tagging...",
        "ori-fast-z-score": -0.4833682445228318,
        "water-fast-z-score": 6.735753140545634,
        "rewrite-fast-z-score": 1.9802950859533488
    },
    {
        "original_text": "The effect of three different sugars (trehalose, maltase and sucrose) on the structure and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case. The results show that all these sugar molecules can stabilize the protein against thermal denaturation to certain extent but trehalose is found to be most effective one among them. Trehalose molecule forms hydrogen bonds with both polar and nonpolar amino acid residues which leads to increase in number of water molecules around it. This increases the hydration shell thickness as well as total solvent accessible surface area of the protein. It also decreases the root mean square deviation between initial and final structures indicating its ability to maintain the native conformation of the protein. Maltose and sucrose are less efficient than trehalose because they form only few hydrogen bonds with the protein. In addition, their presence causes slight decrease in the radius of gyration and end-to-end distance of the protein.",
        "watermark_text": "The impact of three different sugars ( trehalose , maltase and sucrose ) on the composition and dynamics of lysozyme has been investigated by molecular dynamics simulation at 300 K for 100 ns in each case . The results show that all these syrup molecules can stabilize the protein against thermal denaturation to certain degree but trehalose is found to be most efficient one among them .Trehalose atom forms hydrogen bonds with both polar and nonpolar amino acid acids which results to increase in quantity of water molecules around it . This changes the hydration shell thickness as well as gross solvent available surface space of the protein .It additionally decreases the root average square deviation between initial and final structures representing its capacity to keep the native conformation of the protein . Maltose and sucrose are less effective than trehalose because they create only few hydrogen bonds with the protein .In addition , their presence causes mild decrease in the radius of gyration and end - to - end distance of the protein .",
        "rewrite_text": "The influence of three distinct sugars—trehalose, maltose, and sucrose—on the composition and dynamics of lysozyme has been explored through molecular dynamics simulations conducted at 300 K over 100 ns for each sugar. The findings indicate that all three sugar molecules can enhance the protein's stability against thermal denaturation to some extent, with trehalose proving to be the most effective. Trehalose forms hydrogen bonds with both polar and nonpolar amino acids, leading to an increased number of water molecules surrounding it. This interaction alters the thickness of the hydration shell as well as the overall solvent-accessible surface area of the protein. Additionally, it reduces the root mean square deviation between the initial and final structures, reflecting its ability to maintain the protein's native conformation. In comparison, maltose and sucrose are less effective, as they form fewer hydrogen bonds with the protein. Their presence also results in a subtle reduction in the radius of gyration and the end-to-end distance of the protein.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 5.103161130233569,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "We study spin-1/2 fermions on an infinite square lattice with nearest-neighbor hopping and pairing interactions, using exact diagonalization techniques to calculate their ground-state energy as well as the momentum distribution function at zero temperature.  We find that for weak coupling (small values of t/J), the system is in a superfluid phase characterized by off-diagonal long-range order; however, this state becomes unstable when t/J exceeds some critical value tc(μ) which depends on chemical potential μ. For strong coupling (large values of t/J), we show that there exists another stable phase where the system has no off-diagonal long range order but still exhibits gapless excitations. This new phase can be viewed as a BCS-like paired state without any broken symmetry. The transition between these two phases occurs via a quantum phase transition. Finally, we discuss possible experimental realizations of our model Hamiltonian.",
        "watermark_text": "We explore spin - 1 / 2 fermions on an endless square lattice with nearest - neighbor hopping and pairing transitions , using exact diagonalization techniques to estimate their ground - state energy as also as the velocity distribution function at zero temperature . We see that for weak bonding ( small values of t / J ) , the system is in a superfluid phase described by off - diagonal long - range order ; however , this state grows unstable when t / J exceeds some essential factor tc ( μ ) which depends on chemical potential μ .For strong coupling ( large values of t / J ) , we find that there exists another stable phase where the system has no off - vertical high distance order but still exhibits gapless excitations . This new phase can be viewed as a BCS - like paired state without any breaking symmetry .The shift between these two phases occurs via a quantum phase change . Finally , we explain possible experimental realizations of our model Hamiltonian .",
        "rewrite_text": "We investigate spin-1/2 fermions on an infinite square lattice characterized by nearest-neighbor hopping and pairing transitions. Utilizing exact diagonalization techniques, we estimate both the ground-state energy and the velocity distribution function at zero temperature. Our findings indicate that at weak coupling (small values of t/J), the system resides in a superfluid phase, marked by off-diagonal long-range order. However, this state becomes unstable when t/J surpasses a critical threshold tc(μ) that is dependent on the chemical potential μ. In the regime of strong coupling (large values of t/J), we identify another stable phase where the system lacks off-diagonal long-range order but still supports gapless excitations. This newly identified phase can be interpreted as a BCS-like paired state without symmetry breaking. The transition between these two phases occurs through a quantum phase transition. Finally, we discuss potential experimental implementations of our model Hamiltonian.",
        "ori-fast-z-score": 1.2535663410560174,
        "water-fast-z-score": 5.8119893994415355,
        "rewrite-fast-z-score": 2.3626845919446504
    },
    {
        "original_text": "The production of light elements in the early universe is one of the most important unsolved problems in astrophysics, cosmology, nuclear physics and particle physics. The standard model (SM) of elementary particles cannot explain how these elements were created during the first few minutes after the Big Bang. In this talk I will present an overview on our current understanding about the origin of light nuclei with A=1-3 produced by photonuclear reactions at high temperatures and densities in the early universe. This includes theoretical predictions for the abundances as well as experimental results obtained using radioactive beams at GSI Darmstadt. Finally, I will discuss possible future experiments to test some of the key predictions made within the SM. Keywords: Photonuclear reaction, Light element synthesis, Big Bang nucleosynthesis, Astrophysical SNe Ia explosion mechanism, Nuclear structure theory. 1 Introduction.\nLight element synthesis in the early universe is among the most challenging open questions in modern science  1  . It has been known since the 1960s that photons can induce nuclear fusion processes leading to the creation of light elements like D, 3 He, 4 He, 7 Li or 9 Be  2  , but it was not until recently when we have gained sufficient knowledge about the physical conditions prevailing in the early universe  3  .\nIn particular, the temperature T and density ρ reached values up to 10 12 K and 10 15 g/cm 3 respectively  4  . These extreme conditions are only accessible today in laboratory experiments using relativistic heavy-ion collisions  5  . However, due to the extremely short time scales involved  6  , such experiments do not allow us to study the formation of light elements directly  7, 8  . Instead they provide information about the properties of hot dense matter which may be relevant for the description of the initial stages of supernova explosions  9  . On the other hand, the abundance pattern observed in primordial objects like white dwarfs  10  or metal-poor stars  11  provides valuable constraints on the models describing the evolution of the chemical composition of the universe  12  .",
        "watermark_text": "The production of light elements in the early universe is one of the most important unsolved issues in astrophysics , cosmology , nuclear science and particle science . The typical model ( SM ) of primary atoms cannot explain how these objects were created during the first few hours after the Big Bang .In this talk I will present an overview on our new understanding about the origin of light nuclei with A = 1 - 3 created by photonuclear reactions at high temperatures and densities in the early universe . This contains theoretical estimates for the abundances as well as research results derived using nuclear beams at GSI Darmstadt .Finally , I will explore possible future research to test some of the key predictions produced within the SM . Keywords : Photonuclear reaction , Light element synthesis , Big Bang nucleosynthesis , Astrophysical SNe Ia explosion mechanism , Nuclear structure hypothesis .1 Introduction . Light factor synthesis in the early universe is among the most challenging open questions in modern science 1 .It has been known since the 1960s that photons can induce nuclear fusion mechanisms leading to the creation of light elements like D , 3 He , 4 He , 7 Li or 9 Be 2 , but it was not until recently when we have achieved sufficient knowledge about the physical conditions prevailing in the early universe 3 . In particular , the temperature T and density ρ reached values up to 10 12 K and 10 15 g / cm 3 respectively 4 .These severe environments are only accessible today in laboratory experiments using relativistic heavy - ion collisions 5 . However , owing to the exceptionally short period scales involved 6 , such studies do not enable us to study the formation of light elements directly 7 , 8 .Instead they give information about the properties of bright heavy material which may be crucial for the description of the first phase of supernova explosions 9 . On the other hand , the density trend detected in primordial objects like white dwarfs 10 or metal - poor stars 11 provides valuable constraints on the models explaining the evolution of the chemical composition of the universe 12 .",
        "rewrite_text": "The synthesis of light elements in the early universe remains one of the most significant unresolved challenges in astrophysics, cosmology, nuclear science, and particle physics. The conventional model of primary atoms falls short in explaining the formation of these elements during the initial hours following the Big Bang. In this presentation, I will provide an overview of our evolving understanding of the origins of light nuclei with atomic mass numbers A = 1 - 3, which were formed through photonuclear reactions under high temperatures and densities in the early universe. This discussion will include theoretical estimates of their abundances as well as experimental results obtained using nuclear beams at GSI Darmstadt. Finally, I will examine potential future research avenues to test some of the key predictions made by the conventional model. Keywords: Photonuclear reaction, light element synthesis, Big Bang nucleosynthesis, astrophysical SNe Ia explosion mechanism, nuclear structure hypothesis. \n\n1. Introduction. The synthesis of light elements in the early universe stands as one of the most formidable open questions in contemporary science. It has been recognized since the 1960s that photon-induced nuclear fusion can lead to the formation of light elements such as D, 3He, 4He, 7Li, and 9Be. However, it is only recently that we have gained a deeper understanding of the physical conditions present in the early universe. Specifically, temperatures and densities reached up to 10^12 K and 10^15 g/cm^3, respectively. These extreme conditions are currently only replicable in laboratory settings through relativistic heavy-ion collisions. Nevertheless, due to the exceptionally brief timescales involved, such experiments do not allow for direct observation of light element formation. Instead, they provide insights into the properties of heavy materials, which may be critical for understanding the early phases of supernova explosions. Conversely, the density trends observed in primordial objects like white dwarfs or metal-poor stars offer valuable constraints on models explaining the universe's chemical evolution.",
        "ori-fast-z-score": 1.8842334171937931,
        "water-fast-z-score": 8.818163074019441,
        "rewrite-fast-z-score": 1.8594397919452197
    },
    {
        "original_text": "We present an in-depth study of the galaxy cluster Z3146, which is one of the most luminous clusters in X-rays known to date (Lx = 1.5 x 1045 erg/s). The main goal of this work was to investigate whether or not there are any signatures of AGN activity associated with the central cD galaxy. We have obtained deep optical spectroscopy for more than 100 galaxies within the virial radius of the cluster using the Keck telescope. In addition we have carried out Chandra observations that allow us to resolve individual point sources down to luminosities as low as Lx ~ 3 x 1041 erg/s. Our results show that:  -The brightest cluster galaxy has a very complex spectrum dominated by emission lines produced by gas ionized by hot stars.  -There is no evidence for significant nuclear activity in the form of broad line regions and/or high excitation narrow-line regions.  -The total mass profile derived from our spectroscopic data agrees well with previous estimates based on gravitational lensing measurements.",
        "watermark_text": "We report an in - depth survey of the galaxy cluster Z3146 , which is one of the most luminous clusters in X - radiation known to date ( Lx = 1 . 5 x 1045 erg / s ) . The main goal of this research was to examine whether or not there are any signatures of AGN activity related with the main cD galaxy .We have achieved deep optical spectroscopy for more than 100 galaxies within the virial diameter of the cluster using the Keck lens . In addition we have carried out Chandra measurements that enable us to identify individual point sources down to luminosities as low as Lx ~ 3 x 1041 erg / s .Our results show that : - The brightest cluster galaxy has a very complex spectrum characterized by absorption lines released by gas ionized by hot stars . - There is no evidence for significant nuclear activity in the form of broad line regions and / or low excitation narrow - line regions .- The total mass profile derived from our spectroscopic data agrees well with previous accounts based on gravity lensing observations .",
        "rewrite_text": "We present a comprehensive study of the galaxy cluster Z3146, which is among the brightest clusters in X-ray emission known to date (Lx = 1.5 x 10^45 erg/s). The primary objective of this research was to investigate the presence of any signs of AGN activity associated with the central cD galaxy. Utilizing the Keck telescope, we obtained deep optical spectroscopy for over 100 galaxies within the cluster's virial radius. Additionally, we conducted Chandra observations that allowed us to detect individual point sources with luminosities as low as Lx ~ 3 x 10^41 erg/s. Our findings indicate that: - The brightest cluster galaxy has a complex spectrum featuring absorption lines from gas ionized by hot stars. - There is no indication of significant nuclear activity, such as broad line regions or low excitation narrow-line regions. - The total mass profile derived from our spectroscopic data is consistent with previous estimates based on gravitational lensing observations.",
        "ori-fast-z-score": 0.23249527748763857,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "In this note we present some comments on the examples given in  1  and  2  . We show that these examples are not counterexamples to the generalized Jacobian conjecture, as stated by M. Laurent (see  3  ). In fact they do not even contradict the weaker statement made by J.-P. Serre  4  , which is equivalent to the Jacobian conjecture for curves over finite fields. Finally we give an example showing how one can construct counterexamples to the generalized Jacobi conjecture using our method. Let k be any field with char(k) = p > 0. For every integer n ≥ 1 let Xn denote the smooth projective curve defined over k by y n + a1yn−1 + · · · + anny0 = xn+1,\nwhere ai ∈ k * . It was shown by A. N. Parshin  5  that if char(k) = 2 then there exists a positive integer m such that the jacobian variety JacXm has complex multiplication. This implies that the jacobian varieties JacXn have complex multiplication for all integers n ≡ ±1 mod m. If char(k) = 3 it follows from  6  that JacX3 does not have complex multiplication. However, it still remains open whether or not JacX4 has complex multiplication.",
        "watermark_text": "In this note we present some remarks on the examples given in 1 and 2 . We see that these objects are not counterexamples to the generalized Jacobian conjecture , as described by M . Laurent ( saw 3 ) .In reality they do not even contradict the weaker statement provided by J . - P . Serre 4 , which is analogous to the Jacobian conjecture for curves over finite fields . Finally we give an instance demonstrating how one can build counterexamples to the generalized Jacobi theorem using our technique .Let k be any field with char ( p ) = p > 0 . For every integer n ≥ 1 let Xn represent the smooth projective curve defined over k by y n + a1yn−1 + · · · + anny0 = xn + 1 , where ai ∈ k * .It was shown by A . N . Parshin 5 that if char ( k ) = 2 then there exists a positive integer m such that the jacobian variety JacXm has complex multiplication . This implies that the jacobian varieties JacXn have complex multiplication for all numbers n ≡ ±1 mod m . If char ( k ) = 3 it appears from 6 that JacX3 does not have complex multiplication .However, it still remains open whether or not JacX4 has complex multiplication.",
        "rewrite_text": "In this note, we provide some observations on the examples presented in sections 1 and 2. We conclude that these examples do not serve as counterexamples to the generalized Jacobian conjecture, as outlined by M. Laurent (refer to section 3). In fact, they do not even contradict the less rigorous assertion proposed by J.-P. Serre (see reference 4), which is relevant to the Jacobian conjecture for curves over finite fields. Additionally, we present an example that illustrates how our method can be used to construct counterexamples to the generalized Jacobi theorem. Let \\( k \\) denote any field with characteristics \\( \\text{char}(p) = p > 0 \\). For each integer \\( n \\geq 1 \\), let \\( X_n \\) be the smooth projective curve defined over \\( k \\) by the equation \\( y^n + a_1 y^{n-1} + \\ldots + a_n y^0 = x^n + 1 \\), where \\( a_i \\in k^* \\). A. N. Parshin has demonstrated that if \\( \\text{char}(k) = 2 \\), there exists a positive integer \\( m \\) such that the Jacobian variety \\( \\text{Jac}(X_m) \\) has complex multiplication. This indicates that the Jacobian varieties \\( \\text{Jac}(X_n) \\) possess complex multiplication for all \\( n \\equiv \\pm 1 \\mod m \\). Conversely, it appears from reference 6 that \\( \\text{Jac}(X_3) \\) does not have complex multiplication when \\( \\text{char}(k) = 3 \\). However, the status of whether \\( \\text{Jac}(X_4) \\) possesses complex multiplication remains unresolved.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": 1.3937462952891333
    },
    {
        "original_text": "We have studied the evolution of magnetic fields on Ap-type stars using data obtained with the ESPaDOnS spectropolarimeter at CFHT, Canada-France-Hawaii Telescope (CFHT). We used high-resolution spectra to determine the mean longitudinal magnetic field Bz , which is proportional to the integral over all depths of the line-of-sight component of the magnetic vector. Our sample consists of about 100 Ap stars observed within open clusters or OB-associations. For each star we determined its age based on photometric data available in literature. \n \n In order to study how the strength of magnetic fields evolves during the main-sequence life time of an Ap star, we divided our sample into three groups according to their ages. These are young open clusters/associations (age < 1 Myr), intermediate-open clusters/associations (1 Myr < age < 10 Myr) and old open clusters/associations  (10 Myr < age < 300 Myr). \n \n Figure 1: Mean longitudinal magnetic field versus effective temperature for Ap stars belonging to different open clusters/OB-associations.",
        "watermark_text": "We have researched the evolution of magnetic fields on Ap - class stars using data acquired with the ESPaDOnS spectropolarimeter at CFHT , Canada - France - Hawaii Telescope ( CFHT ) . We utilized large - resolution spectra to find the mean horizontal magnetic force Bz , which is proportional to the integral over all depths of the line - of - view component of the magnetic vector .Our survey consists of about 100 Ap objects seen within open complexes or OB - associations . For each star we calculated its age based on photometric data available in literature .In order to study how the strength of magnetic waves evolves during the main - sequence life time of an Ap star , we divided our sample into three groups based to their ages . These are young open complexes / associations ( age < 1 Myr ) , intermediate - open clusters / associations ( 1 Myr < age < 10 Myr ) and elderly open complexes / associations ( 10 Myr < age < 300 Myr ) .Figure 1 : Mean longitudinal magnetic force versus effective heat for Ap stars belonging to different open complexes / OB - associations .",
        "rewrite_text": "We investigated the evolution of magnetic fields in Ap-class stars using data collected with the ESPaDOnS spectropolarimeter at the Canada-France-Hawaii Telescope (CFHT). By analyzing high-resolution spectra, we determined the mean horizontal magnetic force Bz, which is proportional to the integral of the line-of-sight component of the magnetic vector across various depths. Our study includes approximately 100 Ap stars located within open clusters or OB associations. For each star, we estimated its age using photometric data from the literature. To examine how the strength of magnetic fields changes throughout the main-sequence lifetime of Ap stars, we categorized our sample into three age groups: young open clusters/associations (age < 1 Myr), intermediate open clusters/associations (1 Myr < age < 10 Myr), and older open clusters/associations (10 Myr < age < 300 Myr). Figure 1 displays the mean longitudinal magnetic force in relation to effective temperature for Ap stars from various open clusters/OB associations.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 4.638124095143555,
        "rewrite-fast-z-score": -0.1111111111111111
    },
    {
        "original_text": "We report on optical spectroscopy and multicolor photometry of GRB 040924, an intermediate-duration (T90 = 5 s) event detected by Swift/BAT at 07:55 UT on 24 September 2004. The prompt emission was followed by a bright X-ray flare peaking about 1 hour later than the main pulse. We find that the spectrum is well fitted with a power law plus blackbody model in the range 3000-9000 Å . The best-fit parameters are  _  = -1.1 ± 0.2 , TBB = 6200 +1800 -900 K, and EBB = 2.5 +1.0 -0.7 keV for the power-law index,  temperature, and normalization of the blackbody component respectively.  These values are consistent with those observed in other short-hard GRBs. In addition to this thermal component, we detect strong Fe II absorption lines blueshifted by ~10,000 km/s relative to their rest wavelengths. This suggests that the progenitor system may be similar to that inferred for short-hard GRBs such as GRB 050509b.",
        "watermark_text": "We report on optical spectroscopy and multicolor photometry of GRB 040924 , an intermediate - duration ( T90 = 5 s ) event detected by Swift / BAT at 07 : 55 UT on 24 September 2004 . The prompt emission was followed by a bright X - ray flare peaking about 1 hour later than the main pulse .We see that the spectrum is well fitted with a power law plus blackbody model in the range 3000 - 9000 Å . The best - fitting factors are _ = - 1 . 1 ± 0 . 2 , TBB = 6200 + 1800 - 900 K , and EBB = 2 . 5 + 1 . 0 - 0 . 7 keV for the power - law index , temperature , and normalization of the blackbody element respectively .These quantities are compatible with those observed in other short - hard GRBs . In addition to this heat element , we perceive strong Fe II spectral lines blueshifted by ~ 10 , 000 km / s relative to their rest wavelengths .This implies that the progenitor system might be similar to that inferred for short - hard GRBs such as GRB 050509b .",
        "rewrite_text": "We present our findings from the optical spectroscopy and multicolor photometry of GRB 040924, an intermediate-duration event (T90 = 5 s) detected by Swift/BAT at 07:55 UT on September 24, 2004. Following the initial emission, a prominent X-ray flare occurred, peaking approximately one hour after the main pulse. Our analysis reveals that the spectrum can be accurately modeled with a combination of a power law and a blackbody across the 3000 - 9000 Å range. The best-fit parameters are as follows: power-law index α = -1.1 ± 0.2, blackbody temperature TBB = 6200 +1800 -900 K, and blackbody normalization EBB = 2.5 +1.0 -0.7 keV. These values align with those found in other short-hard GRBs. Additionally, we detect strong blueshifted Fe II spectral lines at approximately 10,000 km/s relative to their rest wavelengths, suggesting that the progenitor system may be analogous to those associated with short-hard GRBs like GRB 050509b.",
        "ori-fast-z-score": 1.8708286933869707,
        "water-fast-z-score": 5.879747322073337,
        "rewrite-fast-z-score": 1.5756771943166705
    },
    {
        "original_text": "We study the ground state properties of spin-1/2 antiferromagnetic Heisenberg model on square (SQ) and triangular lattices (TL). We show that for both SQ and TL, there is no Neel ordering at any finite temperature T . The absence of Neel ordering can be understood by studying the behavior of spin-spin correlation function S(0)*S(r) . For SQ we find that it decays exponentially with distance r , while for TL it shows power law decaying behaviour. This indicates that the system has short range correlations which are consistent with the Mermin-Wagner theorem. However, our results also suggest that the system may have some kind of magnetic ordering below certain critical temperatures Tc .\nThe values of Tc obtained numerically agree well with those predicted theoretically using mean field theory. In addition to this, we also calculate the specific heat Cv as a function of temperature T .",
        "watermark_text": "We research the ground state properties of spin - 1 / 2 antiferromagnetic Heisenberg model on square ( SQ ) and triangular lattices ( TL ) . We see that for both SQ and TL , there is no Neel ordering at any polynomial heat T .The absence of Neel ordering can be understood by examining the activity of spin - spinning correlation function S ( 0 ) * S ( r ) . For SQ we find that it decays exponentially with distance r , while for TL it displays power law decaying behaviour .This implies that the system has small range correlations which are compatible with the Mermin - Wagner theorem . However , our findings also suggest that the system might have some kind of magnetic ordering below certain significant conditions Tc .The values of Tc obtained numerically agree well with those predicted theoretically using mean field principles . In addition to this , we also predict the specific warmth Cv as a function of temperature T .",
        "rewrite_text": "We investigate the ground state properties of the spin-1/2 antiferromagnetic Heisenberg model on square (SQ) and triangular (TL) lattices. Our research shows that neither the SQ nor the TL exhibits Néel ordering at any finite temperature. This absence of Néel ordering can be clarified by analyzing the spin-spin correlation function S(0) * S(r). For the SQ lattice, we observe that the correlation function decays exponentially with distance r, whereas for the TL lattice, it demonstrates a power-law decay. This behavior indicates that the system has short-range correlations consistent with the Mermin-Wagner theorem. Nevertheless, our results also imply the potential for some form of magnetic ordering under certain significant conditions represented by Tc. The numerically obtained values for Tc align closely with those predicted using mean field theory. Additionally, we forecast the specific heat Cv as a function of temperature T.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 4.913538149119954,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present an evolutionary algorithm for the problem of mesh numbering, which is to assign integers in the range  0, ..., n-1  to all vertices of a given graph G = (V, E) such that adjacent vertices have consecutive numbers and no two edges share the same pair of endpoints with the same order.  The proposed method uses a population-based approach where each individual represents a valid solution by means of a permutation vector. We show how this representation can be used to efficiently compute fitness values using only local information. In addition we propose several genetic operators to explore the search space. Finally, we report on preliminary results obtained by applying our method to some well-known benchmark instances. Keywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms. 1 Introduction A common task when working with graphs is to label their nodes or edges with unique identifiers. This process is known as node or edge numbering respectively. For example, it may be required to number the nodes of a road network so that every path between any two points has a unique sequence of labels. Another application arises in circuit design, where one needs to assign unique addresses to components of electronic circuits.",
        "watermark_text": "We present an evolutionary algorithm for the question of mesh numbering , which is to assign integers in the range 0 , . . . , k - 1 to all edges of a given graph G = ( V , E ) such that adjacent vertices have consecutive numbers and no two edges occupy the same pair of endpoints with the same order . The proposed approach utilizes a population - based approach where each individual represents a valid answer by means of a permutation matrix .We see how this representation can be used to easily compute fitness values utilizing only local information . In addition we propose several genetic functions to examine the search space .Finally , we publish on preliminary results acquired by using our technique to some well - famous benchmark instances . Keywords : Graphs , Optimization problems , Evolutionary algorithms , Genetic algorithms .1 Introduction A typical task when dealing with graphs is to label their edges or edges with special identifiers . This process is known as node or edge numbering respectively .For instance , it could be required to count the nodes of a street system so that every path between any two points has a unique string of tags . Another application exists in circuit design , where one needs to give unique addresses to components of electronic circuits .",
        "rewrite_text": "We introduce an evolutionary algorithm aimed at the problem of mesh numbering, which involves assigning integers from 0 to k - 1 to all edges of a given graph G = (V, E). The goal is to ensure that adjacent vertices are assigned consecutive numbers while preventing any two edges from having the same pair of endpoints in the same order. Our approach employs a population-based strategy, where each individual solution is represented by a permutation matrix. This representation allows for the straightforward computation of fitness values using only local information. Additionally, we propose various genetic operators to explore the search space effectively. We also present preliminary results obtained by applying our technique to several well-known benchmark instances. \n\nKeywords: Graphs, Optimization problems, Evolutionary algorithms, Genetic algorithms.\n\n1 Introduction\n\nWhen working with graphs, a common task is to label the edges or nodes with specific identifiers, a process referred to as edge or node numbering, respectively. For example, one may need to uniquely label nodes in a transportation network, ensuring that every path between two points has a distinct sequence of tags. Another practical application is in circuit design, where unique addresses must be assigned to the components of electronic circuits.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 5.273697108112943,
        "rewrite-fast-z-score": 0.29559878344928797
    },
    {
        "original_text": "We present an analysis of synthetic stellar libraries used to calibrate photometric surveys, such as Gaia and LSST. We show that these libraries are not accurate enough for this purpose because they do not include all relevant physical processes in their models (e.g., convection). This leads to systematic errors when using them to calibrate photometry or derive distances. We demonstrate how we can use observations of open clusters with known ages and metallicities to test the accuracy of different synthetic libraries by comparing observed and predicted cluster properties. Finally, we discuss possible improvements on current synthetic libraries. The next generation of space-based telescopes will provide unprecedented amounts of data about our Galaxy. These new datasets require large efforts to be analyzed properly. One important aspect is the calibration of photometric surveys like Gaia and LSST which will deliver precise astrometry and multi-color photometry for billions of stars across the sky. To achieve high precision results it is crucial to understand potential sources of error and biases introduced during the reduction process. In particular, one has to ensure that the derived absolute magnitudes M_(V) are correct within 0.01 mag over most of the color range covered by the survey. \n \n For example, if the distance modulus DM = 5log10(d/d_sun), where d is the true distance between us and the star and d_sun is the Sun’s distance from Earth, then a difference of 0.01 mag corresponds to a factor of 1.1 in distance. Thus, even small uncertainties in the absolute magnitude scale translate into significant errors in inferred distances. Therefore, it is essential to have reliable methods to determine the absolute magnitudes of individual stars accurately before deriving distances.  \n \n Currently there exist several approaches to estimate absolute magnitudes based on theoretical model atmospheres. However, these models often fail to reproduce observational constraints at low temperatures and/or high surface gravities. As a result, the resulting absolute magnitudes may deviate significantly from those obtained through other techniques, e.g., eclipsing binaries. Moreover, some of these models also suffer from incomplete",
        "watermark_text": "We present an assessment of synthetic astronomical collections useful to calibrate photometric surveys , such as Gaia and LSST . We suggest that these archives are not accurate sufficient for this objective because they do not include all relevant physical processes in their models ( e . g . , convection ) .This leads to systematic errors when using them to calibrate photometry or calculate distances . We showed how we can using observations of open nuclei with established periods and metallicities to test the accuracy of different synthetic databases by comparing observed and anticipated cluster structures .Finally , we discuss possible advances on current artificial libraries . The future wave of space - based telescopes will provide incredible amounts of data about our Galaxy .These new datasets take great efforts to be analyzed correctly . One important milestone is the calibration of photometric surveys like Gaia and LSST which will provide accurate astrometry and multi - color photometry for billions of stars across the sky .To achieve high clarity findings it is crucial to realize potential sources of mistake and biases created during the reduction step . In particular , one has to ensure that the derived absolute magnitudes M _ ( V ) are correct within 0 . 01 mag over most of the color spectrum encompassed by the poll .For instance , if the distance modulus DM = 5log10 ( d / d _ sun ) , where d is the true distance between us and the star and d _ sun is the Sun ’ s distance from Earth , then a difference of 0 . 01 mag corresponds to a factor of 1 . 1 in distance . Thus , even minor uncertainties in the absolute magnitude scale turn into considerable errors in inferred distances .Therefore , it is crucial to have reliable techniques to estimate the absolute magnitudes of individual stars accurately before deriving distances . Currently there remain many approaches to estimate absolute magnitudes based on theoretical model atmospheres .However , these models often fail to reproduce observational restrictions at low temperatures and / or large depth gravities . As a outcome , the resulting absolute magnitudes might deviate greatly from those achieved through other techniques , e . g . , eclipsing binaries .Moreover , some of these models even suffer from incomplete",
        "rewrite_text": "We provide an evaluation of synthetic astronomical collections that are intended to calibrate photometric surveys, such as Gaia and LSST. We argue that these archives lack sufficient accuracy for this purpose, as they do not account for all relevant physical processes in their models (e.g., convection). This oversight results in systematic errors when they are used for photometric calibration or distance calculations. We demonstrate how observations of open clusters with known periods and metallicities can be employed to assess the accuracy of various synthetic databases by comparing observed and predicted cluster structures. Additionally, we explore potential improvements to existing synthetic libraries. The forthcoming generation of space-based telescopes is set to generate vast amounts of data regarding our Galaxy, necessitating meticulous analysis to ensure correctness. A critical objective is to calibrate photometric surveys like Gaia and LSST, which will deliver precise astrometry and multi-color photometry for billions of stars throughout the sky. To achieve clear outcomes, it is essential to identify potential sources of error and bias introduced during the data reduction process. Specifically, it is necessary to ensure that the derived absolute magnitudes M_(V) are accurate to within 0.01 mag across most of the color spectrum represented in the survey. For example, if the distance modulus DM = 5 log10(d / d_sun), where d represents the true distance to the star and d_sun is the Sun's distance from Earth, a variance of 0.01 mag corresponds to a factor of 1.1 in distance. Consequently, even slight uncertainties in the absolute magnitude scale can lead to significant errors in the calculated distances. Therefore, it is vital to establish reliable methods for accurately estimating the absolute magnitudes of individual stars prior to determining distances. Currently, there are numerous methods for estimating absolute magnitudes based on theoretical model atmospheres; however, these models often struggle to accurately reflect observational constraints at low temperatures or high surface gravities. As a result, the absolute magnitudes produced may differ significantly from those obtained via alternative techniques, such as eclipsing binaries. Additionally, some models may also suffer from incompleteness in their data.",
        "ori-fast-z-score": -1.784435632438388,
        "water-fast-z-score": 6.871842709362768,
        "rewrite-fast-z-score": -0.7137464271463297
    },
    {
        "original_text": "We study the geometry and topology of generalized Lagrangian submanifolds (GLSMs) in complex symplectic manifolds, focusing on GLSMs that are special Lagrangians with respect to some Kähler form. We show how these can be constructed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles. In particular we consider the case where the base is a partial flag variety. This leads us to define new families of Calabi-Yau varieties which have been studied by physicists recently. These varieties are obtained by taking products of Grassmannian manifolds or their quotients by finite groups. The main results of this thesis are:  1. A construction of GLSMs using parabolic Higgs bundles.  2. An explicit description of the cohomology ring of the total space of a vector bundle associated to a parabolic Higgs bundle.  3. A proof of mirror symmetry between two different types of GLSMs defined above when the base is a product of Grassmannians.",
        "watermark_text": "We research the topology and topology of generalized Lagrangian submanifolds ( GLSMs ) in complex symplectic manifolds , concentrating on GLSMs that are special Lagrangians with regard to some Kähler form . We see how these can be formed as holomorphic sections of certain line bundles over moduli spaces of parabolic Higgs bundles .In particular we define the case where the base is a partial flag variety . This leads us to define novel families of Calabi - Yau extensions which have been studied by physicists recently .These varieties are derived by take products of Grassmannian manifolds or their quotients by finite groups . The main results of this dissertation are : 1 .A design of GLSMs involving parabolic Higgs bundles . 2 .An exact description of the cohomology ring of the total space of a vector bundle related to a parabolic Higgs bundle . 3 .A proof of mirror symmetry between two different kinds of GLSMs provided above when the base is a product of Grassmannians .",
        "rewrite_text": "We investigate the topology of generalized Lagrangian submanifolds (GLSMs) within complex symplectic manifolds, focusing on those that are special Lagrangians associated with a specific Kähler form. Our analysis reveals how these GLSMs can be constructed as holomorphic sections of particular line bundles over moduli spaces of parabolic Higgs bundles. Notably, we explore the scenario in which the base is a partial flag variety. This exploration paves the way for the introduction of new families of Calabi-Yau extensions, which have garnered attention from physicists recently. These varieties are obtained through the product of Grassmannian manifolds or their quotients by finite groups. The key findings of this dissertation include: 1. A framework for constructing GLSMs utilizing parabolic Higgs bundles. 2. A precise characterization of the cohomology ring of the total space of a vector bundle associated with a parabolic Higgs bundle. 3. A proof of mirror symmetry between two distinct types of GLSMs mentioned earlier, specifically when the base is a product of Grassmannians.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 4.900769721140662,
        "rewrite-fast-z-score": 1.016001016001524
    },
    {
        "original_text": "The electromagnetic properties of high-temperature superconductors are studied in terms of two different theoretical approaches, namely the slave-boson theory (SBT) and the doped carrier theory (DCT). The SBT is based on an effective low-energy description of strongly correlated electrons by means of auxiliary bosonic degrees of freedom which represent collective charge excitations. In this approach we calculate the optical conductivity as well as the Hall coefficient for various values of doping concentration n. We find that both quantities exhibit nontrivial temperature dependence at low temperatures T . On the other hand, within DCT these physical observables can be calculated analytically using simple expressions valid only at zero temperature. Our results show that there exists significant quantitative difference between predictions made by these two models. This discrepancy may serve to discriminate between them experimentally. High-temperature superconductivity has been one of the most challenging problems in condensed matter physics over past decades  1  . Despite enormous experimental efforts  2  , its microscopic origin remains unknown. A number of competing theoretical scenarios have been proposed  3  but none of them could provide a complete explanation of all available data  4  .\nIn particular, it was suggested  5  that the mechanism responsible for high-temperature superconductivity might involve strong electron correlations  6  . These effects cannot be described within conventional Fermi-liquid theory  7, 8  because they lead to non-Fermi liquid behavior  9  such as power-law dependences of thermodynamic functions  10  or unusual transport phenomena  11  . To account for these features theoretically, several phenomenological models were developed  12  including the so-called slave-boson theory  13  . It describes the dynamics of strongly interacting fermions with spin S = 1/2 coupled to an additional set of bosonic fields representing collective charge fluctuations  14  . Within this framework, the ground state of the system corresponds to a Bose-Einstein condensation  15  of the bosons  16  . As a result, the fermionic quasiparticles acquire finite masses  17  leading to their disappearance above some critical temperature  18  .",
        "watermark_text": "The electromagnetic properties of high - temperature superconductors are studied in terms of two different conceptual approaches , namely the captive - boson theory ( SBT ) and the doped carrier theory ( DCT ) . The SBT is based on an efficient low - energy characterization of highly correlated atoms by means of auxiliary bosonic degrees of liberty which describe collective charge excitations .In this step we estimate the optical conductivity as well as the Hall coefficient for various values of doping concentration n . We see that both quantities exhibit nontrivial temperature dependence at low temperatures T . On the other hand , within DCT these physical observables can be determined analytically utilizing simple definitions valid only at zero temperature .Our results show that there exists significant quantitative difference between estimates made by these two models . This discrepancy may serve to discriminate between them experimentally .High - temperature superconductivity has been one of the most challenging difficulties in condensed matter science over past decades 1 . Despite enormous observation efforts 2 , its microscopic source remains obscure .A variety of competing theory explanations have been proposed 3 but none of them could give a complete explained of all available data 4 . In particular , it was suggested 5 that the process responsible for high - temperature superconductivity might involve strong electron correlations 6 .These effects cannot be described within conventional Fermi - fluid theory 7 , 8 because they lead to non - Fermi solid behavior 9 such as power - law dependences of thermodynamic functions 10 or unusual travel effects 11 . To account for these characteristics theoretically , various phenomenological models were developed 12 notably the so - called slave - boson theory 13 .It describes the dynamics of highly interacting fermions with spin S = 1 / 2 coupled to an additional setting of bosonic fields representing collective charge fluctuations 14 . Within this framework , the ground state of the system belongs to a Bose - Einstein condensation 15 of the bosons 16 .As a result , the fermionic quasiparticles acquire finite masses 17 leading to their disappearance above some critical temperature 18 .",
        "rewrite_text": "The electromagnetic characteristics of high-temperature superconductors are examined using two distinct conceptual frameworks: the captive-boson theory (SBT) and the doped carrier theory (DCT). SBT employs an effective low-energy description of highly correlated atoms through auxiliary bosonic degrees of freedom that represent collective charge excitations. In this context, we assess the optical conductivity and Hall coefficient across varying doping concentrations (n), observing that both exhibit complex temperature dependencies at low temperatures (T). Conversely, DCT allows for the analytical determination of these physical observables through straightforward definitions that are applicable only at absolute zero temperature. Our findings reveal significant quantitative disparities between the estimates produced by these two models, which may provide a means for experimental differentiation. High-temperature superconductivity has posed one of the most challenging problems in condensed matter physics for several decades. Despite extensive observational efforts, the underlying microscopic mechanisms remain largely mysterious. Numerous competing theoretical explanations have been proposed; however, none have fully accounted for all existing data. Notably, it has been suggested that strong electron correlations may play a critical role in high-temperature superconductivity. These correlations defy conventional Fermi-fluid theory because they lead to non-Fermi liquid behavior, manifested as power-law dependencies in thermodynamic functions or unusual transport phenomena. To theoretically address these traits, a variety of phenomenological models have been introduced, including the prominent slave-boson theory. This framework describes the dynamics of highly interactive fermions with spin S = 1/2, coupled with additional bosonic fields that represent collective charge fluctuations. Within this model, the ground state of the system is characterized by Bose-Einstein condensation of the bosons. Consequently, the fermionic quasiparticles acquire effective masses, resulting in their disappearance above a certain critical temperature.",
        "ori-fast-z-score": 0.8219949365267865,
        "water-fast-z-score": 7.397954428741079,
        "rewrite-fast-z-score": 2.014035259912054
    },
    {
        "original_text": "We report the first detection of silicon (Si), calcium (Ca) and iron (Fe) ions along with magnesium (Mg) in an intervening galaxy system toward quasar HE 0515-4414 at redshift 0.4485. The observed column densities are log N(Mg+H) = 13.60 ± 0.10 cm-2 , log N(Si+H) = 12.70 ± 0.20 cm-2 , log N (Ca + H ) = 11 .90 ± 0.30 cm -2 , and log N (Fe + H ) = 10.40 ± 0.50 cm -2 .\nThe total hydrogen column density is log NH = 20.0 +0.5 -0.3 cm-2 . We find that this system has low metallicity Z < 1/100 solar abundance ratio for all four elements detected.  This system shows no detectable neutral carbon or molecular hydrogen absorptions down to limits of log NC/NH ~ -1.7 and log MH / NH ~ -3.6 respectively.",
        "watermark_text": "We report the first detection of silicon ( Si ) , calcium ( Ca ) and iron ( Fe ) ions along with magnesium ( Mg ) in an intervening galaxy system toward quasar HE 0515 - 4414 at redshift 0 . 4485 . The observed column densities are log N ( Mg + H ) = 13 . 60 ± 0 . 10 cm - 2 , log N ( Si + H ) = 12 . 70 ± 0 . 20 cm - 2 , log N ( Ca + H ) = 11 . 90 ± 0 . 30 cm - 2 , and log N ( Fe + H ) = 10 . 40 ± 0 . 50 cm - 2 .The total hydrogen row density is log NH = 20 . 0 + 0 . 5 - 0 . 3 cm - 2 . We see that this system has low metallicity Z < 1 / 100 solar occurrence ratio for all four elements detected .This system displays no detectable neutral hydrogen or molecular hydrogen absorptions down to bounds of log NC / NH ~ - 1 . 7 and log MH / NH ~ - 3 . 6 respectively .",
        "rewrite_text": "We present the first detection of silicon (Si), calcium (Ca), and iron (Fe) ions, as well as magnesium (Mg), in an intervening galaxy system associated with quasar HE 0515-4414 at a redshift of 0.4485. The measured column densities are log N(Mg + H) = 13.60 ± 0.10 cm⁻², log N(Si + H) = 12.70 ± 0.20 cm⁻², log N(Ca + H) = 11.90 ± 0.30 cm⁻², and log N(Fe + H) = 10.40 ± 0.50 cm⁻². The total hydrogen column density is log NH = 20.0 +0.5 -0.3 cm⁻². This system is characterized by low metallicity, with a metallicity ratio of Z < 1/100 times that of solar for all four detected elements. Additionally, we find no detectable neutral hydrogen or molecular hydrogen absorptions, with limits of log NC/NH ~ -1.7 and log MH/NH ~ -3.6, respectively.",
        "ori-fast-z-score": 0.14586499149789456,
        "water-fast-z-score": 2.5924756956542794,
        "rewrite-fast-z-score": 0.457495710997814
    },
    {
        "original_text": "We report the detection of beryllium (Be) lines in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240.  These are the first detections of Be in metal-poor halo stars with  Fe/H  < -2.5 dex. We find that these stars have high surface gravities for their temperatures, indicating they may be blue stragglers or other evolved objects. In addition to the Be features at 4131 Å and 4130 Å we also see evidence for an unidentified feature near 3970 Å which is likely due to C+N+O. This work was supported by NASA grant NAG5-9998. Keywords: Beryllium; Blue straggler; Metal poor star; Ultracool dwarf. 1. Introduction.\nThe discovery of extremely low-mass stars has opened up new avenues into understanding how planets form around very cool dwarfs. However, there remains much uncertainty about the formation process itself as well as the chemical composition of such systems. One important aspect of this problem involves determining whether or not terrestrial planet formation can occur within the habitable zone of ultracool dwarfs. To address this question it will be necessary to determine if the atmospheres of these stars contain significant amounts of heavy elements like carbon, nitrogen, oxygen, sulfur, sodium, potassium, magnesium, aluminum, silicon, calcium, titanium, iron, nickel, cobalt, copper, zinc, arsenic, selenium, silver, gold, mercury, lead, uranium, thorium, and plutonium. It should be noted that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars.",
        "watermark_text": "We report the observation of beryllium ( Be ) tracks in two ultra - low metallicity halo stars , CS 22892 - 052 and HE 0107 - 5240 . These are the first detections of Be in metal - scarce halo stars with Fe / H < - 2 . 5 dex .We see that these stars have high surface gravities for their temperatures , showing they may be blue stragglers or other evolution bodies . In addition to the Be properties at 4131 Å and 4130 Å we also find proof for an unknown spot near 3970 Å which is probably due to C + N + O .This work was supported by NASA grant NAG5 - 9998 . Keywords : Beryllium ; Blue straggler ; Metal poor star ; Ultracool dwarf .1 . Introduction .The observation of incredibly poor - density stars has opened up new avenues into studying how planets organize around very cool dwarfs . However , there exists much uncertainty about the formation transition itself as well as the chemical composition of such systems .One important dimension of this question involves establishing whether or not terrestrial planet development can occur within the habitable zone of ultracool dwarfs . To address this question it will be required to study if the atmospheres of these planets contain significant amounts of heavy components like carbon , nitrogen , oxygen , hydrogen , potassium , potassium , magnesium , iron , silicon , potassium , titanium , iron , nickel , cobalt , copper , zinc , arsenic , selenium , platinum , gold , mercury , lead , uranium , thorium , and plutonium .It should be mentioned that while some of these metals are produced during stellar nucleosynthesis others are synthesized only through cosmic ray spallation reactions occurring outside of stars .",
        "rewrite_text": "We present our findings on the detection of beryllium (Be) tracks in two ultra-low metallicity halo stars, CS 22892-052 and HE 0107-5240. These represent the first observations of Be in metal-poor halo stars with iron-to-hydrogen ratios (Fe/H) below -2.5 dex. Notably, these stars exhibit high surface gravities relative to their temperatures, suggesting that they may be blue stragglers or other evolutionary types. Alongside the beryllium features at 4131 Å and 4130 Å, we also observe evidence of an unidentified spectral feature near 3970 Å, likely associated with carbon, nitrogen, and oxygen. This research was funded by NASA grant NAG5-9998. \n\n**Keywords:** Beryllium; Blue straggler; Metal-poor star; Ultracool dwarf. \n\n**1. Introduction.** The examination of incredibly low-density stars has created new opportunities to explore the formation of planets around very cool dwarfs. However, significant uncertainties remain regarding the transition processes involved in formation and the chemical compositions of these systems. A critical aspect of this inquiry is assessing whether terrestrial planet formation can occur within the habitable zones of ultracool dwarfs. To investigate this, it is essential to determine whether the atmospheres of such planets contain substantial amounts of heavy elements, including carbon, nitrogen, oxygen, hydrogen, potassium, magnesium, iron, silicon, titanium, nickel, cobalt, copper, zinc, arsenic, selenium, platinum, gold, mercury, lead, uranium, thorium, and plutonium. It is important to note that while some of these metals are produced through stellar nucleosynthesis, others are formed via cosmic ray spallation beyond stellar environments.",
        "ori-fast-z-score": -0.47891314261057566,
        "water-fast-z-score": 5.510397987560282,
        "rewrite-fast-z-score": -0.09245003270420485
    },
    {
        "original_text": "We present an orbital stability study for the 14 planet system discovered by HATNet and Kepler space telescopes around the star HD 10180 (HIP 108427). We use numerical integrations to show that this system is dynamically stable over timescales longer than its age, which we estimate at 4 Gyrs using gyrochronology. The planets are found in two resonant chains with period ratios close to 2:1 and 3:2 respectively. These chains are connected through a chain of mean motion resonances between adjacent pairs of planets. This structure suggests that the system has been sculpted by convergent migration followed by tidal dissipation within each planet s envelope. \n \n Keywords: Planetary systems - Stability - Mean-motion resonance - Convergent migration - Tides - Gyrochronology - HD10180 - Kepler telescope - HATNet telescope - Orbital dynamics - Dynamical evolution",
        "watermark_text": "We present an orbital stability study for the 14 planet system discovered by HATNet and Kepler space telescopes around the star HD 10180 ( HIP 108427 ) . We use numerical integrations to see that this system is dynamically stable over timescales greater than its age , which we estimate at 4 Gyrs using gyrochronology .The planets are found in two resonant chains with time proportions close to 2 : 1 and 3 : 2 respectively . These chains are connected through a network of mean motion resonances between neighboring pairs of planets .This structure implies that the system has been sculpted by convergent displacement preceded by tidal dissipation within each planet s envelope . Keywords : Planetary systems - Stability - Mean - movement resonance - Convergent migration - Tides - Gyrochronology - HD10180 - Kepler observatory - HATNet telescope - Orbital dynamics - Dynamical evolution",
        "rewrite_text": "We conduct a study on the orbital stability of the 14-planet system discovered by the HATNet and Kepler space telescopes orbiting the star HD 10180 (HIP 108427). Through numerical integrations, we demonstrate that this system remains dynamically stable over timescales exceeding its estimated age of 4 billion years, as determined by gyrochronology. The planets are organized into two resonant chains with time ratios roughly of 2:1 and 3:2, which are linked by a series of mean motion resonances between adjacent planetary pairs. This configuration suggests that the system has evolved due to convergent migration, influenced by tidal dissipation within each planet's envelope. \n\nKeywords: Planetary systems - Stability - Mean motion resonance - Convergent migration - Tidal effects - Gyrochronology - HD10180 - Kepler observatory - HATNet telescope - Orbital dynamics - Dynamical evolution",
        "ori-fast-z-score": 0.14285714285714285,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": -0.42008402520840293
    },
    {
        "original_text": "We study how the effects of galactic winds can be used to explain the observed properties of the metal-poor tail in the stellar metallicity distributions (SMDs) of nearby dwarf spheroidal galaxies (dSph). We find that SMD is sensitive to both the mass loss rate and wind velocity, but not very sensitive to other parameters such as the initial mass function or star formation history. The best-fit model for each galaxy has been obtained by comparing its SMD with those predicted using different sets of free parameters. Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolution stages. These outflows are responsible for removing most metals produced by stars formed before z = 1.5-2.0. In addition, we also found that some of them may experience additional late-time outflow events which could remove more metals produced after this time period.",
        "watermark_text": "We research how the effects of galactic winds can be used to explain the observed properties of the metal - weak tail in the stellar metallicity distributions ( SMDs ) of distant dwarf spheroidal galaxies ( dSph ) . We see that SMD is sensitive to both the mass loss rate and breeze density , but not very sensitive to other parameters such as the early mass value or star formation history .The best - fitting model for each galaxy has been achieved by comparing its SMD with those predicted use different sets of free parameters . Our results show that all these dSph have experienced strong outflows driven by supernovae explosions during their early evolved stages .These outflows are responsible for eliminating most metals produced by stars formed before z = 1 . 5 - 2 . 0 . In addition , we also discovered that some of them may experience additional late - time outflow events which potentially remove more metals produced after this time time .",
        "rewrite_text": "We investigate how galactic winds may account for the observed characteristics of the metal-weak tail in the stellar metallicity distributions (SMDs) of distant dwarf spheroidal galaxies (dSph). Our findings indicate that the SMD is notably influenced by both the mass loss rate and wind density, while showing less sensitivity to other factors like the initial mass value or star formation history. To identify the best-fitting model for each galaxy, we compare its SMD with those predicted using various sets of free parameters. Our results reveal that all studied dSph galaxies underwent significant outflows driven by supernova explosions during their early evolutionary phases. These outflows are responsible for removing most of the metals produced by stars formed before redshift z = 1.5 - 2.0. Additionally, we have found evidence suggesting that some of these galaxies may experience further outflow events at later times, potentially leading to the removal of more metals produced after this period.",
        "ori-fast-z-score": 1.0533703247651751,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": -0.22645540682891913
    },
    {
        "original_text": "We present new spectroscopic observations for more than 1000 Galactic OB supergiants, obtained with FLAMES/GIRAFFE at the Very Large Telescope (VLT). The sample includes all known O-type dwarfs and giants as well as B-type supergiants brighter than about Mbol = -4 mag within 25 pc distance to Earth. We derive atmospheric parameters T eff , log g, microturbulence velocity vmic, and chemical composition including nitrogen abundance  N/Fe  . For comparison we also analyse a large number of Galactic red supergiants observed by GOSSS project using similar methods. Our results show that there is no significant difference between the mean values of these quantities derived for both samples. However, our analysis reveals systematic differences between different studies based on smaller samples published so far. In particular, we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non-LTE effects or underestimating gravities because they did not take into account stellar winds.",
        "watermark_text": "We present new spectroscopic observations for more than 1000 Galactic OB supergiants , obtained with FLAMES / GIRAFFE at the Very Large Telescope ( VLT ) . The sample comprises all known O - class dwarfs and giants as well as B - class supergiants hotter than about Mbol = - 4 mag within 25 pc proximity to Earth .We derive air parameters T eff , log g , microturbulence velocity vmic , and biological composition including nitrogen density N / Fe . For comparison we also analyse a large number of Galactic red supergiants detected by GOSSS program use similar methods .Our results show that there is no considerable difference between the mean readings of these quantities generated for both samples . However , our analysis reveals systematic differences between various surveys relying on smaller specimens publications so far .In particular , we find that the majority of previous investigations overestimated the temperatures of hotter objects due to neglecting non - LTE effects or underestimating gravities because they did not take into account galaxy winds .",
        "rewrite_text": "We present new spectroscopic observations of over 1,000 Galactic OB supergiants, collected using FLAMES/GIRAFFE at the Very Large Telescope (VLT). This sample includes all known O-class dwarfs and giants, as well as B-class supergiants with temperatures exceeding approximately Mbol = -4 mag, located within 25 parsecs of Earth. We determine various atmospheric parameters, including effective temperature (T_eff), logarithmic surface gravity (log g), microturbulence velocity (v_mic), and chemical composition, particularly nitrogen density (N/Fe). To provide context, we also analyze a substantial number of Galactic red supergiants identified through the GOSSS program using similar methods. Our findings suggest that there is no significant difference in the average values of these parameters across both samples. Nevertheless, our analysis uncovers systematic discrepancies among various surveys that have relied on smaller sample sizes in prior publications. In particular, we discover that many earlier studies have overestimated the temperatures of hotter objects by overlooking non-LTE effects, or have underestimated gravities by failing to account for galactic winds.",
        "ori-fast-z-score": 1.9123657749350298,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 2.3626845919446504
    },
    {
        "original_text": "We present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, which is located at a distance of about 1 kpc toward the Galactic anti-center direction. We found that there are two young stellar objects (YSOs), one Class I protostar with an infrared luminosity of ~10 Lsun and another embedded YSO candidate with a bolometric temperature of ~1000 K. The former object shows bipolar outflows traced by Herbig-Haro knots and molecular line wings. In addition to these two sources, we detected several other point-like NIR sources within the central region of CB 54. These may be low-mass pre-main-sequence stars or background galaxies. Our results suggest that this cloud core has experienced active star formation over its lifetime. \n \n Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar object",
        "watermark_text": "We present near - infrared ( NIR ) imaging and spectroscopy of galaxy formation activity in the Bok globule CB 54 , which is situated at a distance of about 1 kpc toward the Galactic anti - center position . We showed that there are two young stellar bodies ( YSOs ) , one Class I protostar with an infrared luminosity of ~ 10 Lsun and another embedded YSO candidate with a bolometric temperature of ~ 1000 K . The former object displays bipolar outflows traced by Herbig - Haro knots and molecular line wings .In addition to these two sources , we spotted various other point - like NIR components within the central region of CB 54 . These may be low - weight pre - principal - sequence stars or background galaxies .Our results show that this cloud core has undergone active star formation over its lifetime . Keywords : Star formation ; Near - infrared spectroscopy ; Bipolar outflow ; Young stars object",
        "rewrite_text": "We present near-infrared (NIR) imaging and spectroscopy of star formation activity in the Bok globule CB 54, located approximately 1 kpc from the Galactic anti-center. Our observations reveal two young stellar objects (YSOs): one Class I protostar with an infrared luminosity of around 10 L☉, and another embedded YSO candidate exhibiting a bolometric temperature of about 1000 K. The first object demonstrates bipolar outflows, as indicated by Herbig-Haro knots and molecular line wings. Additionally, we detected several other point-like NIR sources in the central region of CB 54, which may correspond to low-mass pre-main-sequence stars or background galaxies. Our findings suggest that this cloud core has experienced significant star formation throughout its history. Keywords: Star formation; Near-infrared spectroscopy; Bipolar outflow; Young stellar objects.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 1.4569855927715483
    },
    {
        "original_text": "We present an approach to nonequilibrium entropy production in the lattice Boltzmann (LB) method by introducing entropy limiters into the collision operator. The proposed scheme is shown to be able to reproduce the correct equilibrium distribution and recover the second law of thermodynamics for both single-phase flows with constant density and temperature, as well as multiphase flows with phase change. We also demonstrate that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities. \n \n Keywords: Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave. 1 Introduction \n \n In recent years, there has been growing interest in developing computational fluid dynamics methods based on kinetic theory  1–3  . Compared with conventional Navier-Stokes solvers, these approaches are more accurate at capturing complex flow phenomena such as shocks  4  , turbulence  5  , and interfacial flows  6  . Among them, the lattice Boltzmann method  7, 8  has attracted much attention due to its simplicity and efficiency  9  . \n \n However, it should be noted that most existing LB models do not satisfy the second law of thermodynamic  10  . This problem becomes particularly severe when dealing with high Mach number flows  11  . To overcome this difficulty, several attempts have been made recently  12–18  . For example, Chen et al.  12  introduced a modified BGK-type collision term which recovers the correct equilibrium state while satisfying the second law of thermodynamical. Similarly, Yu et al.  13  developed another type of entropy-consistent LB schemes using the concept of entropic moments. More recently, Shan et al.  14  presented a novel LB model where the relaxation time was determined according to the local Knudsen number. Although these works provide promising results, they all require additional information about the macroscopic variables, e.g., pressure and velocity fields. As a result, their applications may be limited to simple cases involving only one component gas. \n \n In contrast, we propose here a general framework for constructing entropy-consistent LB models. Our strategy relies on adding",
        "watermark_text": "We present an approach to nonequilibrium entropy production in the lattice Boltzmann ( LB ) method by using entropy limiters into the collision operator . The proposed system is demonstrated to be possible to predict the appropriate equilibrium distribution and recover the second law of thermodynamics for both single - phase streams with constant density and heat , as well as multiphase streams with phase change .We additionally prove that our new LB model can accurately capture shock waves without spurious oscillations or numerical instabilities . Keywords : Nonequilibrium entropy , Lattice Boltzmann Method , Entropy limiter , Second Law of Thermodynamics , Shock wave .1 Introduction In recent years , there has been growing interest in building computational liquid mechanics algorithms based on kinetic theory 1 – 3 . Compared with typical Navier - Stokes solvers , these method are more accurate at representing complex fluid processes such as shocks 4 , turbulence 5 , and interfacial flows 6 .Among them , the lattice Boltzmann technique 7 , 8 has garnered considerable scrutiny due to its accuracy and efficiency 9 . However , it should be mentioned that most existing LB models do not satisfy the second law of thermodynamic 10 .This problem remains particularly fierce when dealing with high Mach number flows 11 . To solve this challenge , various efforts have been built recently 12 – 18 .For instance , Chen et al . 12 introduced a revised BGK - class collision term which recovers the correct equilibrium state while fulfilling the second law of thermodynamical .Similarly , Yu et al . 13 introduced another type of entropy - consistent LB schemes using the idea of entropic moments .More recently , Shan et al . 14 provided a novel LB model where the relaxation time was decided due to the local Knudsen number .Although these works provide exciting conclusions , they all need extra data about the macroscopic parameters , e . g . , pressure and speed forces . As a outcome , their applications might be restricted to simple instances involving only one component gas .In comparison , we require here a general basis for constructing entropy - consistent LB models . Our strategy relies on adding",
        "rewrite_text": "We introduce a method for addressing nonequilibrium entropy production within the lattice Boltzmann (LB) framework by incorporating entropy limiters into the collision operator. Our system demonstrates the capability to accurately predict the correct equilibrium distribution and to uphold the second law of thermodynamics, applicable to both single-phase flows with constant density and temperature, as well as multiphase flows that involve phase changes. Furthermore, we validate that our new LB model effectively captures shock waves without generating spurious oscillations or encountering numerical instabilities. \n\n**Keywords:** Nonequilibrium entropy, Lattice Boltzmann Method, Entropy limiter, Second Law of Thermodynamics, Shock wave.\n\n**1 Introduction** \n\nIn recent years, there has been increasing interest in developing computational algorithms for fluid mechanics based on kinetic theory. Compared to traditional Navier-Stokes solvers, these methods offer improved accuracy in modeling complex fluid dynamics, including shock waves, turbulence, and interfacial flows. Among these approaches, the lattice Boltzmann technique has gained significant attention due to its precision and efficiency. However, it is important to note that many existing LB models fail to satisfy the second law of thermodynamics, particularly in high Mach number flow scenarios. To address this issue, various recent efforts have emerged. For example, Chen et al. proposed a modified BGK-class collision term that recovers the correct equilibrium state while adhering to the second law of thermodynamics. Similarly, Yu et al. developed entropy-consistent LB schemes utilizing entropic moments. More recently, Shan et al. introduced a novel LB model in which the relaxation time is determined by the local Knudsen number. While these studies yield promising results, they often require additional information regarding macroscopic parameters such as pressure and velocity fields, limiting their applicability to simpler cases involving single-component gases. In contrast, we aim to establish a more general framework for constructing entropy-consistent LB models, focusing on enhancing flexibility and applicability.",
        "ori-fast-z-score": -1.516365080488482,
        "water-fast-z-score": 7.000904100746424,
        "rewrite-fast-z-score": -1.12089707663561
    },
    {
        "original_text": "We consider the problem of finding an approximate solution to a convex optimization problem with a nonconvex objective function and linear constraints, where the feasible region is defined by a set of equality or inequality constraints.  We show that under certain conditions this problem can be solved efficiently using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method based on the Armijo-Goldstein condition.  The proposed algorithm has been implemented as part of the open source software package CVXPY (http://cvxpy.org/).  Numerical experiments are presented which demonstrate the effectiveness of our approach. Keywords: Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstain Condition. 1 Introduction In many practical applications it may not always be possible to find an exact solution to a given mathematical model due to computational complexity issues associated with the underlying numerical algorithms used to solve such models.  For example, in some cases it might only be possible to obtain an approximate solution within a specified tolerance level.  This situation arises frequently when dealing with large-scale nonlinear programming problems arising in various fields including engineering design, operations research, economics, etc., see e.g.,  1  ,  4  .",
        "watermark_text": "We consider the question of finding an approximate solve to a convex optimization problem with a nonconvex objective function and linear constraints , where the viable region is given by a setting of equality or inequality constraints . We see that under certain conditions this question can be answered easily using a combination of gradient descent methods for solving the subproblems in each iteration and a line search method using on the Armijo - Goldstein requirement .The proposed algorithm has been deployed as part of the open source software suite CVXPY ( http : / / cvxpy . org / ) . Numerical trials are presented which demonstrate the ability of our approach .Keywords : Convex Optimization , Nonconvex Objective Function , Saddle Point Problem , Gradient Descent Method , Line Search Algorithm , Armijo - Goldstain Condition . 1 Introduction In many practical applications it must not always be possible to find an precise answer to a given numerical model owing to computational difficulty issues associated with the fundamental numerical algorithms used to solve such models .For instance , in some cases it could only be possible to obtain an approximate solve within a specified tolerance range . This problem arises often when dealing with large - scale nonlinear programming difficulties arising in different fields spanning technical design , operations research , economics , etc . , see e . g . , 1 , 4 .",
        "rewrite_text": "We investigate the challenge of obtaining an approximate solution to a convex optimization problem characterized by a nonconvex objective function and linear constraints, where the feasible region is defined by equality or inequality constraints. We demonstrate that, under certain conditions, this issue can be effectively addressed using a combination of gradient descent methods to tackle the subproblems in each iteration, alongside a line search method that adheres to the Armijo-Goldstein criteria. The resulting algorithm has been implemented in the open-source software package CVXPY (http://cvxpy.org/). We present numerical experiments that illustrate the effectiveness of our approach. \n\n**Keywords:** Convex Optimization, Nonconvex Objective Function, Saddle Point Problem, Gradient Descent Method, Line Search Algorithm, Armijo-Goldstein Condition. \n\n**1 Introduction** In many real-world applications, it may not be feasible to achieve an exact solution to a numerical model due to the computational challenges associated with the standard numerical algorithms required for solving such models. For example, it may only be possible to obtain an approximate solution within a predefined tolerance range. This issue frequently arises in the context of large-scale nonlinear programming problems across various domains such as engineering design, operations research, and economics, among others (see, for example, references 1 and 4).",
        "ori-fast-z-score": 0.5883484054145521,
        "water-fast-z-score": 6.995837874966481,
        "rewrite-fast-z-score": -1.212183053462653
    },
    {
        "original_text": "We present an efficient algorithm for computing the sign function of a large sparse complex matrix, which is based on the Lanczos bidiagonalization process with partial reorthogonalization. The proposed algorithm can be applied to any Hermitian or non-Hermitian matrices without restriction. We apply this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density. In particular we show that our algorithm works well even when the quark mass becomes small compared to the inverse of the lattice spacing. This work was supported by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS numbers: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20 .Dh  1 Introduction Lattice Quantum Chromodynamics(LQCD), as one of the most promising candidates for describing strong interactions among quarks and gluons, has been widely used to study hadronic properties such as masses and decay constants  1  . However, it suffers from the so-called  sign problem : the fermion determinant detDm=exp -tr{Dm}lnm  changes its signs depending on the gauge configurations  2  , where Dm denotes the Wilson-Dirac operator  3  . Therefore, Monte Carlo methods cannot be directly employed to calculate physical quantities using LQCD because they require positive definite weight functions  4  .\nIn order to overcome this difficulty, several approaches have been developed so far  5  -  8  . Among them, the Taylor expansion approach  9  -  11  seems to be very powerful since it allows us to evaluate the expectation value of any observables accurately within statistical errors. It also enables us to perform calculations at high temperature and/or high density  12  -  14  . For example, the Taylor expansion up to O(a6) has already been performed successfully  15  .",
        "watermark_text": "We present an efficient algorithm for calculation the sign function of a large sparse complex matrix , which is based on the Lanczos bidiagonalization process with partial reorthogonalization . The proposed algorithm can be applied to any Hermitian or non - Hermitian matrices without limitation .We use this new algorithm to the overlap Dirac operator in lattice QCD simulations at finite density . In particular we prove that our algorithm works well even when the quark mass becomes tiny relative to the inverse of the lattice spacing .This project was supported by Grants - in - Aid for Scientific Research ( No . 20340040 ) from MEXT Japan .PACS scores : 11 . 15 . Ha , 12 . 38 . Qk , 12 . 39 . Fe , 14 . 20 . Dh 1 Introduction Lattice Quantum Chromodynamics ( LQCD ) , as one of the most attractive candidates for describing strong coupling among quarks and gluons , has been widely using to study hadronic properties such as masses and decay constants 1 . However , it suffers from the so - called sign problem : the fermion determinant detDm = exp - tr { Dm } lnm varies its signs depending on the gauge modes 2 , where Dm denotes the Wilson - Dirac operator 3 .Therefore , Monte Carlo methods never be directly used to estimate mechanical numbers using LQCD because they use positive definite weight functions 4 . In order to overcome this obstacle , various approaches have been proposed so far 5 - 8 .Among them , the Taylor expansion method 9 - 11 seems to be very potent since it allows us to analyze the expectation value of any observables correctly within statistical errors . It additionally lets us to conduct measurements at high heat and / or large velocity 12 - 14 .For instance , the Taylor expansion up to O ( a6 ) has already been performed successfully 15 .",
        "rewrite_text": "We introduce an efficient algorithm for calculating the sign function of large sparse complex matrices, utilizing the Lanczos bidiagonalization process with partial reorthogonalization. This algorithm is applicable to both Hermitian and non-Hermitian matrices without restrictions. We specifically employ this new method for the overlap Dirac operator in lattice QCD simulations conducted at finite density. Our results demonstrate that the algorithm performs effectively even when the quark mass is small compared to the inverse of the lattice spacing. This project was funded by Grants-in-Aid for Scientific Research (No. 20340040) from MEXT Japan. PACS codes: 11.15.Ha, 12.38.Qk, 12.39.Fe, 14.20.Dh.\n\n### 1 Introduction\nLattice Quantum Chromodynamics (LQCD) is considered one of the most promising approaches for analyzing the strong interactions among quarks and gluons, and it has been extensively used to investigate hadronic properties, including masses and decay constants. However, LQCD is challenged by the sign problem, where the fermion determinant det(Dm) = exp(-tr{Dm}) can change signs based on the gauge configurations, with Dm representing the Wilson-Dirac operator. As a result, Monte Carlo methods cannot be directly employed for estimating physical quantities in LQCD since they rely on positive definite weight functions. Various strategies have been proposed to address this challenge. Among these, the Taylor expansion method appears particularly effective, as it enables accurate analysis of the expectation values of observables within statistical errors. Additionally, it allows for measurements to be performed at high temperatures and/or large velocities. Notably, a Taylor expansion up to O(a^6) has already been successfully executed.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.4835456068700275,
        "rewrite-fast-z-score": 2.2445701677816263
    },
    {
        "original_text": "We present the results of our search for high redshift galaxies using narrowband imaging in the near-infrared (NIR) and optical bands, as well as spectroscopic follow-up observations at Keck Observatory. We have discovered two new z>6 galaxy candidates by searching for strong emission lines such as Lyman alpha or CIV in their rest frame UV spectra. The first candidate is an extremely bright object that has been detected both photometrically and spectroscopically to be at z=6.96. This source shows no evidence of AGN activity but does show signs of star formation. The second candidate was found serendipitously during our survey of another target; it also appears to be at z=6-7 based on its NIR colors and lack of detection in the B-band filter. Both sources are among the most distant known galaxies ever observed. These objects will provide important constraints on models of reionization because they can be used to study how ionizing photons escape into the intergalactic medium.",
        "watermark_text": "We present the conclusion of our search for high redshift galaxies using narrowband scanning in the near - infrared ( NIR ) and imaging ranges , as well as spectroscopic follow - up observations at Keck Observatory . We have discovered two new z > 6 galaxy nominees by searching for strong absorption lines such as Lyman alpha or CIV in their rest frame UV spectra .The first candidate is an incredibly bright object that has been detected both photometrically and spectroscopically to be at z = 6 . 96 . This source shows no evidence of AGN activity but does display signs of galaxy formation .The second candidate was seen serendipitously during our survey of another target ; it also seems to be at z = 6 - 7 based on its NIR colors and lack of recognition in the B - band filter . Both sources are among the most distant known galaxies yet observed .These structures will provide important restrictions on predictions of reionization because they can be used to study how ionizing photons escape into the intergalactic medium .",
        "rewrite_text": "We conclude our investigation into high redshift galaxies using narrowband scanning in the near-infrared (NIR) and imaging, along with spectroscopic follow-up at Keck Observatory. During our search for strong absorption lines like Lyman alpha or CIV in their rest-frame UV spectra, we identified two new candidates at z > 6. The first candidate is an exceptionally bright object confirmed through photometric and spectroscopic observations, with a redshift of z = 6.96. This source shows no signs of AGN activity but exhibits indications of galaxy formation. The second candidate was discovered serendipitously while surveying a different target and appears to be at z = 6 - 7, based on its NIR colors and absence in the B-band filter. Both galaxies are among the most distant ever observed, providing crucial insights into predictions regarding reionization, as they can be utilized to study the escape of ionizing photons into the intergalactic medium.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 4.608176875690327,
        "rewrite-fast-z-score": 1.4320780207890627
    },
    {
        "original_text": "We present the regularized version of two popular families of evolution codes for general relativity, namely those based on the BSSN formulation and those using the moving puncture approach. The new schemes are designed to be used with any gauge conditions that can be written as first-order differential equations. We show how these schemes can be implemented into existing codes by adding only one extra equation per time step. In addition we provide an explicit expression for the source term appearing in this additional equation which is valid for all gauges considered here. Finally, we demonstrate the performance of our scheme numerically by evolving several test problems including black hole spacetimes containing apparent horizons. Our results indicate that the new scheme performs well even when evolved over many dynamical timescales. This work was supported by NSF grant PHY-0456728 (J.M.) and NASA grants NNG06GH50G and NNX07AG77G (A.S.).",
        "watermark_text": "We introduce the regularized version of two famous families of evolution symbols for general relativity , particularly those based on the BSSN approach and those adopting the sliding puncture method . The revised methods are intended to be used with any gauge conditions that can be written as first - order differential equations .We see how these schemes can be applied into older codes by added only one extra equation per time step . In addition we provide an explicit expression for the source word appearing in this added equation which is valid for all gauges mentioned here .Finally , we prove the performance of our scheme numerically by expanding numerous test issues including black hole spacetimes containing visible horizons . Our results show that the new program performs good even when evolved over numerous dynamical timescales .This project was supported by NSF grant PHY - 0456728 ( J . M . ) and NASA grants NNG06GH50G and NNX07AG77G ( A . S . ) .",
        "rewrite_text": "We present a regularized version of two well-known families of evolution symbols in general relativity, specifically those based on the BSSN approach and the sliding puncture method. These updated methods are designed to work with any gauge conditions expressible as first-order differential equations. Our approach allows for easy integration into existing codes by requiring the addition of only one extra equation for each time step. Furthermore, we provide a clear expression for the source term in this additional equation, applicable to all the mentioned gauges. Finally, we numerically validate the effectiveness of our scheme by conducting a series of test problems, including black hole spacetimes with distinguishable horizons. Our findings demonstrate that the new method performs well even when evolved over various dynamical timescales. This research was funded by NSF grant PHY-0456728 (J.M.) and NASA grants NNG06GH50G and NNX07AG77G (A.S.).",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 5.185449728701349,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "The present work is devoted to the investigation of new possible nuclear properties using nonlinear methods, namely fractal dimension (FD) and recurrence quantification analysis (RQA). The FD was calculated for different mass number A in order to study its dependence on the system size. It has been found that there exists an optimal value of the scaling parameter which provides the best results for each nucleus separately. This fact indicates that the structure of the studied systems can be characterized as multifractals with non-trivial behavior. In addition we have shown that the obtained values are very close to those predicted by the percolation theory. We also applied RQA method to investigate the temporal evolution of the considered systems. Our calculations show that the complexity of the time series increases with increasing mass number A. Moreover it turns out that this increase follows closely the predictions made within the framework of the Random Matrix Theory. Finally we have compared our results with experimental data available for some light nuclei.",
        "watermark_text": "The present work is devoted to the exploration of new possible nuclear properties using nonlinear methods , notably fractal dimension ( FD ) and recurrence quantification analysis ( RQA ) . The FD was calculated for different mass quantity A in order to study its dependence on the system size .It has been determined that there exists an appropriate value of the scaling parameter which offers the best results for each particle separately . This fact suggests that the composition of the studied structures can be described as multifractals with non - simple properties .In addition we have shown that the achieved values are very close to those predicted by the percolation theory . We also used RQA approach to examine the temporal evolution of the considered systems .Our calculations show that the complexity of the time series increases with expanding mass quantity A . Moreover it turns out that this increase follows carefully the assumptions done within the framework of the Random Matrix Theory .Finally we have linked our findings with experimental evidence available for some light nuclei .",
        "rewrite_text": "This study focuses on exploring new potential nuclear properties through nonlinear methods, specifically fractal dimension (FD) and recurrence quantification analysis (RQA). We calculated the FD for various mass quantities (A) to investigate its dependence on system size. Our findings indicate that there is an optimal scaling parameter value that yields the best results for each individual particle. This observation implies that the structures under investigation can be characterized as multifractals with complex properties. Furthermore, we demonstrated that the values obtained closely align with predictions from percolation theory. Additionally, we employed the RQA approach to analyze the temporal evolution of the systems in question. Our calculations reveal that the complexity of the time series increases with the mass quantity (A), and this increase adheres closely to the tenets of Random Matrix Theory. Finally, we connected our results to existing experimental evidence for certain light nuclei.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": 1.0834726777719228
    },
    {
        "original_text": "We present an analysis of the relativistic Riemann problem for ideal fluids in two space dimensions, with emphasis on the role played by vortex sheets. We show that the solution to this problem can be constructed as a sequence of self-similar solutions which are determined uniquely up to translations along the x-axis (the direction of propagation). The first step is to construct a family of exact solutions describing the interaction between a planar shock wave and a vortex sheet. These solutions have been obtained previously using different methods but we provide here a new derivation based on the method of characteristics. In particular, we obtain explicit expressions for the density and pressure profiles across the shock front. Next, we consider the case where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation corresponds physically to a fluid initially at rest being accelerated impulsively by a piston moving at speed c = 1. We prove that there exists exactly one such solution corresponding to each value of the total mass M . Finally, we study numerically the evolution of more general initial data consisting of several vortex sheets separated by shocks.",
        "watermark_text": "We present an assessment of the relativistic Riemann problem for perfect fluids in two space dimensions , with emphasis on the part played by vortex sheets . We see that the solve to this question can be synthesized as a sequence of self - similar solutions which are decided uniquely up to translations along the x - axis ( the direction of propagation ) .The first step is to build a family of precise solutions governing the interaction between a planar blast flow and a vortex sheet . These solutions have been constructed previously using separate methods but we provide here a new derivation based on the method of characteristics .In particular , we obtain precise expressions for the density and tension characteristics across the shock front . Next , we find the case where the first data contains of a single vortex sheet separating areas of constant density and pressure .This problem corresponds physically to a fluid initially at rest being advanced impulsively by a cylinder moved at speed c = 1 . We prove that there exists precisely one such solution corresponding to each value of the total mass M .Finally , we study numerically the evolution of more general initial evidence consisting of several vortex strands divided by shocks .",
        "rewrite_text": "We present an analysis of the relativistic Riemann problem for perfect fluids in two-dimensional space, focusing on the role of vortex sheets. Our findings indicate that the solution to this problem can be framed as a series of self-similar solutions, which are uniquely determined except for translations along the x-axis (the direction of propagation). The initial step involves constructing a family of accurate solutions that describe the interaction between a planar blast flow and a vortex sheet. While these solutions have been previously developed using different methods, we offer a new derivation based on the method of characteristics, yielding precise expressions for the density and tension characteristics across the shock front. Subsequently, we examine a scenario where the initial data consists of a single vortex sheet separating regions of constant density and pressure. This situation is physically represented by a fluid initially at rest being impulsively moved by a cylinder traveling at speed c = 1. We demonstrate that there is exactly one solution for each value of the total mass M. Lastly, we conduct a numerical study of the evolution of more complex initial conditions involving multiple vortex strands separated by shocks.",
        "ori-fast-z-score": -0.7107423155935334,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 0.4082482904638631
    },
    {
        "original_text": "We present the results of an unbiased survey for N-bearing species (NH3, N2H+) and O-bearing species (H2O, OH, CH3OH, HCO+, H2S) toward two massive star forming regions associated with high-mass protostars. The observations were carried out using the Submillimeter Array at 1.4 mm wavelength. We detected all these species except NH3 towards both sources. In addition to detections we also made upper limits on some other species like SO2, SiO, HNCO etc.. Our main findings are as follows:  -The abundance ratios between different species show that H2O is more abundant than any other molecule by several orders of magnitude.  -The derived column densities suggest that the physical conditions vary significantly across the observed region. -The chemical composition of the gas shows significant differences between the two sources. -The comparison of our data with previous studies suggests that there may be multiple evolutionary stages within each source.",
        "watermark_text": "We present the conclusion of an unbiased survey for N - bearing species ( NH3 , N2H + ) and O - containing species ( H2O , OH , CH3OH , HCO + , H2S ) toward two huge star producing regions associated with high - weight protostars . The surveys were carried out utilizing the Submillimeter Array at 1 . 4 nm wavelength .We observed all these species except NH3 towards both sources . In addition to detections we also took upper limits on some other species like SO2 , SiO , HNCO etc . . Our main results are as follows : - The density proportions between various species suggest that H2O is more rich than any other atom by many orders of magnitude .- The derived column densities suggest that the physical conditions varies dramatically across the studied area . - The chemical composition of the gas displays substantial differences between the two sources .- The comparison of our information with previous research suggests that there may be several evolutionary stages within each source .",
        "rewrite_text": "We present the findings of an unbiased survey for nitrogen-bearing species (NH3, N2H+) and oxygen-containing species (H2O, OH, CH3OH, HCO+, H2S) in two large star-forming regions associated with high-mass protostars. The surveys were conducted using the Submillimeter Array at a wavelength of 1.4 mm. We detected all of these species except for NH3 in both sources. Additionally, we established upper limits for other species such as SO2, SiO, and HNCO. Our key results are as follows: 1) The density ratios among the various species indicate that H2O is significantly more abundant than the other species by several orders of magnitude. 2) The derived column densities reveal that physical conditions vary dramatically across the studied regions. 3) There are substantial differences in the chemical composition of the gas between the two sources. 4) A comparison of our data with previous studies suggests the presence of multiple evolutionary stages within each source.",
        "ori-fast-z-score": -0.7276068751089989,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 0.36650833306891567
    },
    {
        "original_text": "We prove that there exists an open set of initial data such that the corresponding solution to the rapidly rotating shallow water (RSW) or Euler equations is globally smooth in space-time, provided it stays sufficiently close to some equilibrium state. The proof relies on a combination of energy estimates with suitable weighted Sobolev inequalities. We also show how our results can be extended to more general systems of conservation laws which are not necessarily strictly hyperbolic. In particular we consider the case where one of the characteristic speeds vanishes at a point but remains bounded away from zero elsewhere. \nIntroduction\n\nThe purpose of this work is twofold. First, we establish global regularity properties for the rapidly rotating shallow water system and its generalization to arbitrary dimension. Second, we provide new insights into the structure of these models by showing how they fit within a larger class of nonlinear dispersive partial differential equations. Our main result reads as follows: \nMain Theorem 1 Let u 0 ∈ H s , s > n/2 + 2, then there exist constants C = C(n), K = K(n) such that if",
        "watermark_text": "We prove that there exists an open set of initial data such that the equivalent solve to the rapidly spinning shallow water ( RSW ) or Euler equations is globally smooth in space - time , provided it remains reasonably close to some equilibrium state . The proof uses on a combination of power estimates with suitable weighted Sobolev inequalities .We additionally understand how our findings can be generalized to more general systems of conservation laws which are not necessarily strictly hyperbolic . In particular we define the case where one of the characteristic velocity vanishes at a place but stayed defined far from zero elsewhere .Introduction The purpose of this study is twofold . First , we obtain global regularity properties for the rapidly spinning shallow water structure and its generalization to arbitrary dimension .Second , we provide fresh insights into the formation of these models by showing how they fit within a greater category of nonlinear dispersive partial differential equations . Our main consequence reads as follows : Main Theorem 1 Let v 0 ∈ H s , s > n / 2 + 2 , then there exist constants C = C ( n ) , K = K ( n ) such that if",
        "rewrite_text": "We demonstrate the existence of an open set of initial conditions such that the solution to the rapidly spinning shallow water (RSW) or Euler equations remains globally smooth in space-time, provided it stays sufficiently close to a certain equilibrium state. Our proof relies on a combination of power estimates and appropriate weighted Sobolev inequalities. We also explore how our results can be extended to more general systems of conservation laws that are not strictly hyperbolic. Specifically, we consider scenarios where one of the characteristic velocities vanishes at a particular point while remaining well-defined away from zero. \n\n**Introduction**  \nThis study has two primary objectives. First, we establish global regularity properties for the rapidly spinning shallow water model and its extensions to higher dimensions. Second, we offer new insights into the dynamics of these models, situating them within a broader class of nonlinear dispersive partial differential equations. Our main result can be stated as follows: **Main Theorem 1**: If \\( v_0 \\in H^s \\) with \\( s > \\frac{n}{2} + 2 \\), then there exist constants \\( C = C(n) \\) and \\( K = K(n) \\) such that, if...",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 4.1461399144838555,
        "rewrite-fast-z-score": 0.2182178902359924
    },
    {
        "original_text": "The projectile fragmentation of 86Kr at 64MeV/nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a beam intensity of 1nAe. The main results are as follows:  - A total number of about 10000 events have been recorded for this experiment.  - The charge distribution is peaked around Z=40, but shows also a large contribution between 30 and 40 charges units (see fig.1 ). This indicates that the fragments produced by the break-up of 86Kr are not only light particles like neutrons or protons, but contain many intermediate mass fragments too.   - The angular distributions show two peaks corresponding to forward and backward emission respectively (see fig.2 ).  - The energy spectra present a maximum around 10-12 MeV/u which corresponds to the most probable kinetic energy per nucleon of the emitted fragments (see fig.3 ).\n- The isotopic composition of the fragments is shown on figure 4 . It can be seen that there is no significant difference between the fragment production in the forward hemisphere and in the backward one.",
        "watermark_text": "The projectile fragmentation of 86Kr at 64MeV / nucleon has been studied with the INDRA multidetector in inverse kinematics using an 8cm thick natK target and a laser intensity of 1nAe . The main results are as follows : - A total quantity of about 10000 events have been observed for this study .- The charge distribution is peaked around Z = 40 , but shows also a large contribution between 30 and 40 charges units ( view fig . 1 ) . This implies that the fragments produced by the broken - up of 86Kr are not only light particles like neutrons or protons , but contain many intermediate mass pieces too .- The angular distributions show two peaks related to forward and back emission respectively ( see fig . 2 ) . - The energy spectra produce a maximum around 10 - 12 MeV / u which equals to the most likely kinetic power per nucleon of the emitted objects ( see fig . 3 ) .- The isotopic structure of the fragments is displayed on figure 4 . It can be shown that there is no major variation between the fragment production in the front hemisphere and in the backward one .",
        "rewrite_text": "The projectile fragmentation of 86Kr at 64 MeV per nucleon has been investigated using the INDRA multidetector in inverse kinematics, with an 8 cm thick natK target and a laser intensity of 1 nA. The key findings are as follows: Approximately 10,000 events were recorded during this study. The charge distribution is predominantly centered around Z = 40, but there is also a significant contribution from fragments with between 30 and 40 charge units (see Fig. 1). This indicates that the fragments resulting from the breakup of 86Kr include not only light particles like neutrons and protons, but also a substantial number of intermediate mass fragments. The angular distributions exhibit two distinct peaks corresponding to forward and backward emission (refer to Fig. 2). The energy spectra reveal a peak around 10-12 MeV/u, which represents the most probable kinetic energy per nucleon for the emitted fragments (see Fig. 3). The isotopic composition of the fragments is illustrated in Fig. 4, showing no significant differences in fragment production between the front and back hemispheres.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean-field theory and Monte Carlo simulations. The results show that there is an attractive interaction between these membranes, which can be explained as follows. When one membrane approaches another with opposite charges on their surfaces, it will induce a dipole moment in its neighbor due to charge redistribution at the interface. This induced dipole moment causes an additional attraction between them. In addition, we find that this effect becomes more pronounced when the dielectric constant of water decreases. Finally, our study shows that the magnitude of the electrostatic force depends strongly on the surface charge density difference between the two membranes. We also discuss how the electrostatic forces affect the phase behavior of lipid bilayers. DOI: 10.1063/1.3189000\nI. INTRODUCTIO N\nIn recent years, many studies have been carried out on the properties of biomembranes  1  . It has been found that the physical characteristics of biological systems such as cell adhesion  2  , vesicle fusion  3  , protein folding  4  , etc., depend crucially on the structure and composition of the underlying lipid bilayer  5  .\nBiological membranes consist mainly of phospholipids  6  . These lipids contain hydrophobic tails and hydrophilic heads  7, 8  . Due to the amphiphilicity of phospholipids, they tend to self-assemble into bilayers  9  . A typical example for such a system is shown schematically in Fig.  1(a) . Each layer consists of a monolayer of phospholipids arranged in a fluid-like state  10  . The thickness of each layer is about 5 nm  11  . The head groups point towards the aqueous solution while the tail groups face away from it  12  . Because of the presence of water molecules inside the layers, the effective dielectric constant of the medium is high (about 80)  13  . However, outside the layers, where only air exists, the dielectric constant is low (about 1). Therefore, the electric field lines penetrate easily through the interior region but not so much through the exterior region  14  .",
        "watermark_text": "The electrostatic interactions between two asymmetrically charged membranes are studied by using the mean - field model and Monte Carlo simulations . The results show that there is an interesting interaction between these membranes , which can be described as follows .When one cell encounters another with opposite charges on their edges , it will generate a dipole point in its friend due to charge redistribution at the interface . This induced dipole point causes an additional attraction between them .In addition , we find that this effect gets more pronounced when the dielectric constant of water reduces . Finally , our research shows that the severity of the electrostatic pressure depends strongly on the surface charge density difference between the two membranes .We additionally discuss how the electrostatic pressures affect the phase response of lipid bilayers . DOI : 10 . 1063 / 1 . 3189000 I . INTRODUCTIO N In recent years , various studies have been carried out on the properties of biomembranes 1 .It has been shown that the structural traits of biological systems such as cell adhesion 2 , vesicle fusion 3 , protein folding 4 , etc . , depend crucially on the composition and composition of the underlying lipid bilayer 5 . Biological membranes consist mostly of phospholipids 6 .These lipids contain hydrophobic tails and hydrophilic heads 7 , 8 . Due to the amphiphilicity of phospholipids , they tend to self - organize into bilayers 9 .A typical example for such a system is demonstrated schematically in Fig . 1 ( a ) .Each layer contains of a monolayer of phospholipids ordered in a fluid - like state 10 . The depth of each layer is about 5 nm 11 .The face groups look towards the aqueous solution while the tail groups face away from it 12 . Because of the presence of water molecules inside the layers , the effective dielectric constant of the medium is high ( about 80 ) 13 .However , outside the layers , where only air occurs , the dielectric constant is low ( about 1 ) . Therefore , the electric field lines penetrate easily through the inner region but not so much through the exterior zone 14 .",
        "rewrite_text": "The electrostatic interactions between two asymmetrically charged membranes have been investigated using a mean-field model and Monte Carlo simulations. The findings reveal a fascinating relationship between these membranes, characterized by the following dynamics: When one cell encounters another with opposing charges on their edges, a dipole moment is induced in the adjacent cell due to charge redistribution at the interface. This induced dipole subsequently leads to an additional attractive force between the membranes. Furthermore, we observe that this effect becomes more pronounced as the dielectric constant of water decreases. Our study also indicates that the intensity of the electrostatic pressure is highly sensitive to the difference in surface charge densities between the two membranes. Additionally, we explore how these electrostatic pressures influence the phase response of lipid bilayers. DOI: 10.1063/1.3189000\n\nI. INTRODUCTION\n\nRecent investigations into the properties of biomembranes have gained momentum. It has been established that various structural characteristics of biological systems—such as cell adhesion, vesicle fusion, and protein folding—are significantly influenced by the composition of the underlying lipid bilayer. Biological membranes are primarily composed of phospholipids, which exhibit hydrophobic tails and hydrophilic heads. The amphiphilic nature of phospholipids facilitates their self-organization into bilayers, as illustrated in Fig. 1(a). Each layer consists of a monolayer of phospholipids arranged in a fluid-like state, with a typical layer depth of approximately 5 nm. The head groups orient towards the aqueous exterior, while the tail groups turn away. The presence of water molecules within the layers results in a high effective dielectric constant (around 80), while the dielectric constant is considerably lower (about 1) in the air outside the layers. Consequently, electric field lines easily penetrate the inner region but face resistance in the outer zone.",
        "ori-fast-z-score": -0.1781741612749496,
        "water-fast-z-score": 5.992662179699436,
        "rewrite-fast-z-score": 0.7071067811865475
    },
    {
        "original_text": "We study the production of a sterile species in an open system with two stable particles and one unstable particle, where the decay products are not observed.  We show that if the initial state is pure then there exists no final mixed state which can be reached by unitary evolution. This result implies that the production of a sterility cannot occur under any circumstances for such systems. If we allow the possibility to prepare arbitrary states as input, however, it turns out that the production of a certain kind of sterility may still take place. In this case, the output state will always contain some amount of entanglement between the subsystems corresponding to the different types of particles involved. The results presented here have been obtained within the framework of Quantum Kinetic Theory (QKT). QKT provides a description of non-equilibrium phenomena at mesoscopic scales based on the concept of entropy production rate. It has recently attracted considerable attention due to its potential applications in many areas ranging from physics to biology. \nI. INTRODUCTORY REMARK\nThe phenomenon of spontaneous emission plays a crucial role in modern physics. For example, it is responsible for the cooling process in laser-cooling experiments  1  . On the other hand, spontaneous emission also leads to decoherence effects  2  , which limit the performance of quantum information processing devices  3  .\nIn recent years, several authors  4  -  8  studied the problem of producing a particular type of  sterility  in open quantum systems. A state is called  sterile  when it does not interact with itself or another given set of states  9  . More specifically, let us consider a bipartite Hilbert space H = H 1 ⊗H 2 , where dim(H i ) = N i . Then, a density matrix ρ ∈ B(H) is said to be  sterile  wrt. a subset S ⊆ H iff Tr ρσ  = 0 for all σ ∈ S. Here, Tr denotes the trace operation over either H 1 or H 2 depending on whether σ belongs to H 1 or H 2 respectively. Note that the notion of  ster",
        "watermark_text": "We test the production of a sterile species in an open system with two stable atoms and one unstable particle , where the decay products are not observed . We see that if the initial state is pure then there exists no final mixed state which can be reached by unitary decay .This result suggests that the production of a sterility cannot proceed under any situations for such systems . If we allow the possibility to develop arbitrary states as input , however , it turns out that the production of a certain sort of sterility may still taking place .In this instance , the output state will always contain some amount of entanglement between the subsystems corresponding to the different kinds of particles concerned . The results presented here have been achieved within the framework of Quantum Kinetic Theory ( QKT ) .QKT provides a description of non - equilibrium phenomena at mesoscopic scales based on the idea of entropy production rate . It has recently attracted considerable scrutiny due to its potential applications in multiple fields ranging from biology to biology .I . INTRODUCTORY REMARK The phenomenon of spontaneous emission plays a crucial role in modern physics .For instance , it is responsible for the freezing process in laser - cooling experiments 1 . On the other hand , spontaneous emission additionally results to decoherence effects 2 , which reduce the performance of quantum information processing protocols 3 .In recent years , various literature 4 - 8 studied the issue of creating a certain type of sterility in open quantum systems . A state is dubbed sterile when it does not interact with itself or another particular set of states 9 .More specifically , let us consider a bipartite Hilbert space H = H 1 [UNK] 2 , where dim ( H i ) = N i . Then , a density matrix ρ ∈ B ( H ) is said to be sterile wrt .a subset S ⊆ H iff Tr ρσ = 0 for all σ ∈ S . Here , Tr denotes the trace operation over either H 1 or H 2 depending on whether ρ belongs to H 1 or H 2 respectively . Note that the notion of ster",
        "rewrite_text": "We investigate the generation of a sterile species within an open system characterized by two stable atoms and one unstable particle, where the decay products remain unobserved. Our findings indicate that when the initial state is pure, a final mixed state cannot be attained through unitary decay. This outcome implies that the production of sterility is not feasible in such scenarios. However, if we permit the introduction of arbitrary states as input, it appears that a specific form of sterility could still be realized. In this case, the resulting state will inevitably exhibit some level of entanglement between the subsystems representing the various types of particles involved. The results discussed here are derived from the principles of Quantum Kinetic Theory (QKT), which offers a framework for understanding non-equilibrium phenomena at mesoscopic scales by focusing on entropy production rates. QKT has garnered significant attention lately due to its wide-ranging implications across various fields, including biology. \n\n**I. INTRODUCTORY REMARK**\n\nThe phenomenon of spontaneous emission is vital in modern physics. For example, it is integral to the freezing process in laser-cooling experiments. Conversely, spontaneous emission also leads to decoherence effects that can hinder the effectiveness of quantum information processing protocols. Recently, considerable research has explored the concept of generating a specific type of sterility in open quantum systems. A state is referred to as sterile if it does not interact with itself or with another defined set of states. More specifically, we can analyze this in the context of a bipartite Hilbert space \\( H = H_1 \\otimes H_2 \\), where \\( \\text{dim}(H_i) = N_i \\). A density matrix \\( \\rho \\in B(H) \\) is deemed sterile with respect to a subset \\( S \\subset H \\) if \\( \\text{Tr}(\\rho \\sigma) = 0 \\) for all \\( \\sigma \\in S \\). Here, \\( \\text{Tr} \\) indicates the trace operation applied over \\( H_1 \\) or \\( H_2 \\) depending on whether \\( \\rho \\) is situated in \\( H_1 \\) or \\( H_2 \\). Notably, the concept of sterility...",
        "ori-fast-z-score": 1.150792911137501,
        "water-fast-z-score": 6.680767400622813,
        "rewrite-fast-z-score": 2.5175440748900675
    },
    {
        "original_text": "We present the first detection and characterization of polarized foreground emission at microwave frequencies using three years of data from WMAP. We find that this emission is dominated by synchrotron radiation, with an amplitude consistent with previous measurements in the literature. The polarization fraction for this component ranges between 0.5% to 2% across the sky. In addition we detect significant levels of polarized dust emission over much of the sky. This emission has a lower fractional polarization than previously reported but its total intensity is comparable or higher. Finally, we report on the detection of polarized thermal Sunyaev-Zeldovich effect associated with galaxy clusters. These results are important as they provide new information about Galactic foregrounds which will be used to extract cosmological signals such as primordial gravitational waves. \n \n Keywords: Cosmic microwave background anisotropies, Galaxy cluster, Synchrotron Radiation, Dust Emission, Thermal Sunyaev-Zeldovitch Effect",
        "watermark_text": "We present the first recognition and description of polarized foreground emission at microwave frequencies using three years of measurements from WMAP . We see that this emission is dominated by synchrotron emission , with an frequency consistent with previous detection in the literature .The polarization fraction for this component varies between 0 . 5 % to 2 % across the sky . In addition we find considerable rates of polarized dust pollution over much of the sky .This emission has a smaller fractional polarization than previously reported but its total activity is equal or greater . Finally , we publish on the observation of polarized thermal Sunyaev - Zeldovich effect related with star clusters .These data are important as they give novel knowledge about Galactic foregrounds which will be used to extract cosmological messages such as primordial gravitational waves . Keywords : Cosmic microwave background anisotropies , Galaxy cluster , Synchrotron Radiation , Dust Emission , Thermal Sunyaev - Zeldovitch Effect",
        "rewrite_text": "We present the initial recognition and characterization of polarized foreground emission at microwave frequencies, utilizing three years of data from WMAP. Our findings indicate that this emission is primarily driven by synchrotron radiation, with frequencies consistent with previous literature. The polarization fraction for this component ranges from 0.5% to 2% across the sky. Additionally, we observe significant levels of polarized dust contamination over much of the observed area. This dust emission exhibits a lower fractional polarization than previously reported, yet its overall intensity is equal to or exceeds earlier measurements. Finally, we report observations of the polarized thermal Sunyaev-Zeldovich effect associated with star clusters. This data is crucial as it provides new insights into Galactic foregrounds, which will aid in extracting cosmological information, such as primordial gravitational waves. Keywords: Cosmic microwave background anisotropies, galaxy cluster, synchrotron radiation, dust emission, thermal Sunyaev-Zeldovich effect.",
        "ori-fast-z-score": -1.0327955589886444,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": 0.11867816581938533
    },
    {
        "original_text": "We present new spectroscopic observations for the open cluster NGC 1883, which is located at a distance of about 1 kpc in the constellation Cassiopeia (α = 20 h 18 m , δ = +58°). The data were obtained with the 2-m telescope of the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009 using the REOSC spectrograph equipped with grism #7 covering the wavelength range 3700-7000 Å. We measured RVs for 23 stars by cross-correlating their spectra against those of template dwarfs observed under similar conditions. Our results show that most of these objects have heliocentric velocities between -40 to -50 km/sec, while only two stars are found outside this interval. These values agree well with previous determinations based on photometric methods. \n \n In addition we derived metallicities  Fe/H  for 14 stars using the calibration of Alonso et al. (1999) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from -0.10 dex up to +0.20 dex. Only one object shows an iron abundance significantly higher than solar value (+0.30 dex). \n \n Finally, we compared our results with previously published studies.",
        "watermark_text": "We report new spectroscopic observations for the open cluster NGC 1883 , which is situated at a distance of about 1 kpc in the constellation Cassiopeia ( α = 20 h 18 m , δ = + 58° ) . The data were obtained with the 2 - m observatory of the Observatorio Astronómico Nacional de San Pedro Mártir ( OAN - SPM ) on December 16 - 17 , 2009 using the REOSC spectrograph equipped with grism # 7 representing the frequency spectrum 3700 - 7000 Å .We calculated RVs for 23 stars by cross - correlating their spectra against those of template dwarfs observed under analogous conditions . Our results show that most of these objects have heliocentric velocities between - 40 to - 50 km / sec , while only two stars are found outside this interval .These quantities agree well with previous determinations based on photometric technique . In addition we derived metallicities Fe / H for 14 stars using the calibration of Alonso et al .( 1999 ) . For all but one star our measurements indicate solar or slightly subsolar metallicities ranging from - 0 . 10 dex up to + 0 . 20 dex .Only one element shows an metal density greatly higher than solar value ( + 0 . 30 dex ) . Finally , we compared our findings with previously reported surveys .",
        "rewrite_text": "We present new spectroscopic observations of the open cluster NGC 1883, located approximately 1 kpc away in the constellation Cassiopeia (α = 20 h 18 m, δ = +58°). The data were collected at the 2-meter telescope of the Observatorio Astronómico Nacional de San Pedro Mártir (OAN-SPM) on December 16-17, 2009, using the REOSC spectrograph with grism #7, which covers the wavelength range of 3700-7000 Å. We calculated the radial velocities (RVs) for 23 stars by cross-correlating their spectra with those of template dwarfs observed under similar conditions. Our findings indicate that most of these stars exhibit heliocentric velocities between -40 and -50 km/sec, with only two stars falling outside this range. These results are consistent with earlier photometric measurements. Additionally, we derived metallicity values (Fe/H) for 14 stars using the calibration from Alonso et al. (1999). Except for one star, our measurements reveal solar or slightly subsolar metallicities ranging from -0.10 dex to +0.20 dex. Only one star shows a significantly higher metallicity surpassing solar levels (+0.30 dex). Lastly, we compared our results with previously reported surveys.",
        "ori-fast-z-score": 0.3779644730092272,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 1.524001524002286
    },
    {
        "original_text": "We present new photometric and spectroscopic observations for two stars, HD 122563 (=HR 5171A) and BD+17°3248, which are suspected to be members of the proposed intermediate age population of helium-rich giants in the globular cluster Omega Cen.  We find that both stars have very similar atmospheric parameters as those found by previous studies for other candidate helium-rich giant candidates in Omega Cen: T eff = 8200 K; log g = 3.8;  Fe/H  = -1.0 dex. The observed spectra show no evidence for He II lines at 4686 Å or 5412 Å, but do exhibit strong Balmer line emission with equivalent widths ranging between -40 and -50 mÅ. These results suggest that these stars may not actually belong to this proposed class of objects. However, we cannot rule out the possibility that they are indeed helium-rich giants on the basis of our current data set alone.",
        "watermark_text": "We present new photometric and spectroscopic observations for two stars , HD 122563 ( = HR 5171A ) and BD + 17°3248 , which are known to be members of the suggested intermediate age population of helium - rich giants in the globular cluster Omega Cen . We see that both stars have very similar atmospheric parameters as those shown by earlier surveys for other candidate helium - rich giant finalists in Omega Cen : T eff = 8200 K ; log f = 3 . 8 ; Fe / H = - 1 . 0 dex .The observed spectra show no evidence for He II systems at 4686 Å or 5412 Å , but do exhibit strong Balmer line emission with corresponding widths ranging between - 40 and - 50 mÅ . These conclusions show that these stars must not actually belong to this possible category of bodies .However , we cannot judge out the suggestion that they are indeed helium - rich giants on the grounds of our latest data set alone .",
        "rewrite_text": "We present new photometric and spectroscopic observations for two stars, HD 122563 (HR 5171A) and BD + 17°3248, which are believed to be part of the proposed intermediate-age population of helium-rich giants within the globular cluster Omega Cen. Our findings indicate that both stars possess very similar atmospheric parameters to those reported in earlier surveys of other candidate helium-rich giants in Omega Cen: T_eff = 8200 K, log f = 3.8, and [Fe/H] = -1.0 dex. The spectra we observed show no signs of He II lines at 4686 Å or 5412 Å; however, they do feature strong Balmer line emissions, with widths ranging from -40 to -50 mÅ. These results suggest that these stars may not fit into the suspected category of helium-rich giants. However, we cannot dismiss the possibility that they might indeed be helium-rich giants based solely on our current dataset.",
        "ori-fast-z-score": -2.1009029257555607,
        "water-fast-z-score": 3.2547227745205967,
        "rewrite-fast-z-score": -1.952833664712358
    },
    {
        "original_text": "We present solutions for the master equations describing quantum Brownian motion (QBM) in an arbitrary environment, including external forces and non-Markovian effects. The general solution is obtained by solving the corresponding Fokker-Planck equation using path integral techniques. We show that this approach leads to exact results which are valid even when the system-environment coupling strength becomes large compared to the temperature. In particular we consider two examples where our formalism can be applied straightforwardly. First, we study QBM in a harmonic oscillator potential under the influence of white noise. Second, we investigate the effect of a time-dependent force on QBM. Finally, we discuss how our method could also be used to treat more complicated situations such as systems coupled to multiple environments or driven by colored noise. DOI: 10.1063/1.3189571\nQuantum Brownian motion describes the dynamics of particles interacting with their surrounding environment  1  . It has been studied extensively over many years both theoretically  2  , experimentally  3  , and numerically  4  .\nIn recent years there have been several attempts to solve the master equation governing QBM exactly  5, 6, 7, 8  . However these approaches either require approximations  7, 9  or do not allow one to include external forces  6, 5  . Here we present a new technique based on Feynman-Kac path integrals  10  which allows us to obtain exact solutions for the master equation without any approximation  11  . This includes cases where the system-environment interaction is strong compared to the thermal energy k B T  12  . Our formalism applies equally well to Markovian  13  and nonMarkovian  14  processes and can easily incorporate external driving fields  15  .",
        "watermark_text": "We present solutions for the master equations explaining quantum Brownian movement ( QBM ) in an arbitrary environment , including external forces and non - Markovian effects . The general solution is found by solving the equivalent Fokker - Planck equation using path integral methods .We see that this methodology leads to exact findings which are applicable even when the system - environment coupling strength gets large compared to the temperature . In particular we consider two examples where our formalism can be applied straightforwardly .First , we study QBM in a harmonic oscillator potential under the impact of white sound . Second , we investigate the impact of a time - dependent force on QBM .Finally , we explain how our technique could also be used to treat more complicated circumstances such as systems driven to multiple conditions or driven by colored interference . DOI : 10 . 1063 / 1 . 3189571 Quantum Brownian movement describes the dynamics of molecules interacting with their nearby surroundings 1 .It has been studied thoroughly over numerous years both theoretically 2 , experimentally 3 , and numerically 4 . In recent months there have been numerous attempts to solve the master equation governing QBM exactly 5 , 6 , 7 , 8 .However these method either need approximations 7 , 9 or do not enable one to use external forces 6 , 5 . Here we present a new technique based on Feynman - Kac path integrals 10 which allows us to obtain exact solutions for the master equation without any approximation 11 .This contains cases where the process - atmosphere interaction is strong compared to the thermal energy k B T 12 . Our formalism applies similarly well to Markovian 13 and nonMarkovian 14 mechanisms and can easily involve external driving fields 15 .",
        "rewrite_text": "We introduce solutions for the master equations that describe quantum Brownian motion (QBM) in various environmental contexts, accounting for external forces and non-Markovian effects. The general solution is derived by tackling the corresponding Fokker-Planck equation using path integral techniques. This approach yields exact results that remain valid even when the coupling strength between the system and the environment is significantly larger than the temperature. Specifically, we explore two straightforward applications of our formalism: first, we analyze QBM in a harmonic oscillator potential influenced by white noise; second, we examine the effects of a time-dependent force on QBM. Additionally, we discuss how our method can be extended to handle more complex situations, including systems subjected to multiple conditions or influenced by colored noise. Quantum Brownian motion describes the dynamics of molecules interacting with their surroundings and has been extensively researched in theoretical, experimental, and numerical contexts. Recently, there have been various efforts to derive exact solutions to the master equation governing QBM. However, these approaches often require approximations or fail to accommodate external forces. In this work, we propose a novel technique based on Feynman-Kac path integrals, which enables us to obtain exact solutions to the master equation without any approximations, even for cases where the interaction between the system and environment is strong relative to the thermal energy \\( k_B T \\). Our formalism is applicable to both Markovian and non-Markovian mechanisms, and it seamlessly incorporates external driving fields.",
        "ori-fast-z-score": -0.08873565094161139,
        "water-fast-z-score": 7.247844507162112,
        "rewrite-fast-z-score": -0.9838699100999074
    },
    {
        "original_text": "We study the ground state properties and excitations in the extended t-J model with nearest-neighbor hopping (NNH) and next-nearest neighbor hopping (NNN), which is defined by the following Hamiltonian: \nwhere c,c are fermion annihilation operators for electrons with spin , respectively; n,n denote electron number operators for sites i,j, respectively. \n \n \n \n We consider this model on the checkerboard square lattice at half-filling where each site has one electron. The NNH term breaks time-reversal symmetry while the NNN term preserves it. In order to investigate the effect of these terms we use exact diagonalization method combined with density matrix renormalization group technique. We find that there exist three different types of magnetic orders depending on the ratio between NNH and NNN hoppings. These results suggest that the extended t-J model can be used as an effective model for describing high-Tc cuprates.",
        "watermark_text": "We explore the ground state properties and excitations in the extended t - J model with nearest - neighbor hopping ( NNH ) and last - nearest neighbor hopping ( NNN ) , which is characterized by the following Hamiltonian : where c , c are fermion annihilation operators for electrons with spin , respectively ; n , n describe electron number operators for sites i , j , respectively . We consider this model on the checkerboard rectangular lattice at half - filling where each site has one particle .The NNH term breaks time - reversal symmetry while the NNN term preserves it . In order to examine the impact of these terms we using exact diagonalization technique combined with density matrix renormalization group technique .We see that there exist three different kinds of magnetic orders depending on the proportion between NNH and NNN hoppings . These data suggest that the extended t - J approach can be used as an efficient model for describing high - Tc cuprates .",
        "rewrite_text": "We investigate the ground state properties and excitations of the extended t-J model, which includes nearest-neighbor hopping (NNH) and next-nearest neighbor hopping (NNN). The Hamiltonian is defined with fermion annihilation operators, \\(c\\) and \\(c^\\dagger\\), representing electrons with spin, and electron number operators \\(n_i\\) and \\(n_j\\) for sites \\(i\\) and \\(j\\) respectively. This model is applied to a checkerboard rectangular lattice at half-filling, where each site contains one particle. The NNH term disrupts time-reversal symmetry, while the NNN term preserves it. To assess the effects of these hopping terms, we employ a combination of exact diagonalization and density matrix renormalization group techniques. Our findings reveal three distinct types of magnetic orders dependent on the ratio of NNH to NNN hoppings. These results indicate that the extended t-J model is a promising framework for understanding high-Tc cuprates.",
        "ori-fast-z-score": -0.8427009716003844,
        "water-fast-z-score": 3.00964632714423,
        "rewrite-fast-z-score": 1.3242443839434612
    },
    {
        "original_text": "We study the phenomenological consequences of supersymmetric models with gauge-mediated breaking, in which the Standard Model is extended by adding new vector-like matter fields and extra dimensions. We show that these models can be constructed such that they are free of any unnatural fine-tuning problems associated with the Higgs mass or flavor-changing neutral currents. In particular we find that:  1) The lightest scalar superpartner (the  Higgs  boson) has a mass at most around 300 GeV.  2) Flavor changing neutral current effects are suppressed to an acceptable level for generic values of parameters.  3) Gauge coupling unification occurs naturally within experimental uncertainties. 4) There exists a large parameter space where all sparticles have masses above 1 TeV while still satisfying constraints on electroweak symmetry breaking. 5) These models provide a natural explanation for why there may not yet exist evidence for supersymmetry at accelerator experiments.",
        "watermark_text": "We research the phenomenological consequences of supersymmetric theories with gauge - mediated breaking , in which the Standard Model is extended by added new vector - like matter fields and extra dimensions . We see that these models can be built such that they are free of any strange fine - tuned flaws associated with the Higgs mass or flavor - changing neutral currents .In particular we find that : 1 ) The lightest scalar superpartner ( the Higgs boson ) has a mass at most approximately 300 GeV . 2 ) Flavor shifting neutral current effects are suppressed to an acceptable level for generic values of values .3 ) Gauge coupling unification happens easily within experimental uncertainties . 4 ) There exists a large parameter room where all sparticles have masses above 1 TeV while already satisfying constraints on electroweak symmetry breaking .5 ) These models represent a natural explanation for why there may not already exist evidence for supersymmetry at accelerator studies .",
        "rewrite_text": "We investigate the phenomenological implications of supersymmetric theories characterized by gauge-mediated breaking, which extend the Standard Model through the introduction of additional vector-like matter fields and extra dimensions. Our analysis shows that these models can be constructed to avoid problematic fine-tuning issues related to the Higgs mass and flavor-changing neutral currents. Specifically, we find that: 1) The lightest scalar superpartner, identified as the Higgs boson, has a mass no greater than approximately 300 GeV. 2) Flavor-changing neutral current effects are sufficiently suppressed for typical parameter values. 3) Gauge coupling unification occurs readily within the bounds of experimental uncertainties. 4) There exists a considerable parameter space where all sparticles possess masses exceeding 1 TeV while still conforming to electroweak symmetry breaking constraints. 5) These models provide a natural rationale for the lack of evidence for supersymmetry in accelerator experiments.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 5.908391567007971,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "We present an approach to predict the structural network organization in the cerebral cortex based on local node features, such as their position within the brain s surface or volume, and global topological characteristics. We use this method to study how different types of nodes are connected with each other across species (human, macaque monkey) and modalities (diffusion MRI tractography). Our results show that our model can accurately reproduce known patterns of cortico-cortical connections between areas, including those observed in humans but not yet described for monkeys. The proposed framework is general enough to be applied to any type of data where information about individual nodes  positions and pairwise interactions exists. This includes both anatomical and functional imaging datasets, which will allow us to investigate the relationship between structure and function at multiple scales. \n \n Introduction \n \n Brain connectomics aims to map all neuronal elements into a single comprehensive description of the human brain  1  . In recent years, advances in neuroimaging techniques have allowed researchers to obtain detailed maps of the brain s structural  2  , metabolic  3  , and functional  4  architecture. These new technologies provide unprecedented opportunities to understand how the brain works by studying its large-scale organization  5  .\n \nHowever, despite these advancements, there remains significant uncertainty regarding the precise nature of the relationships among neurons  6  . For example, it has been shown that some regions of the brain communicate more frequently than others  7-9 , while others exhibit higher levels of synchrony  10  . However, we still do not know whether these differences reflect specific wiring rules  11  or simply arise due to random fluctuations  12  . \n \n Here, we propose a novel computational framework to address this problem using machine learning methods  13  . Specifically, we aim to develop models capable of predicting the strength of connection between pairs of nodes given only information about their location and topology  14  . To achieve this goal, we first construct a set of training examples consisting of pairs of nodes whose interaction strengths are known  15  . Then, we train a classifier to learn the mapping between node features and edge weights  16  . Finally, we apply the trained model to unseen test cases  17  to infer unknown interactions",
        "watermark_text": "We present an way to predict the structural network structure in the brain cortex relying on local node characteristics , such as their placement within the brain s surface or volume , and global topological traits . We use this technology to study how various types of networks are connected with each other across taxa ( human , macaque mouse ) and modalities ( diffusion MRI tractography ) .Our results show that our model can accurately demonstrate established trends of cortico - cortical relationships between locations , notably those observed in humans but not already explained for monkeys . The proposed framework is basic enough to be applied to any type of data where information about individual nodes positions and pairwise relationships exists .This contains both anatomical and physiological imaging datasets , which will let us to examine the relationship between organization and function at multiple scales . Introduction Brain connectomics aims to map all neuronal components into a single comprehensive account of the human neural 1 .In recent years , advances in neuroimaging techniques have permitted investigators to obtain detailed maps of the brain s structural 2 , cellular 3 , and functional 4 architecture . These new approaches provide unprecedented possibilities to realize how the brain acts by examining its large - scale organization 5 .However , despite these advancements , there exists significant confusion regarding the exact nature of the relationships among neurons 6 . For instance , it has been shown that some regions of the brain communicate more frequently than others 7 - 9 , while many exhibit greater levels of synchrony 10 .However , we also do not understand whether these changes reflect specific wiring requirements 11 or simply arise due to random fluctuations 12 . Here , we propose a new computational framework to tackle this question using computer learning techniques 13 .Specifically , we attempt to develop models capable of predicting the strength of connection between pairs of nodes given only data about their direction and configuration 14 . To achieve this goal , we first build a setting of testing instances consisting of pairs of nodes whose interaction abilities are known 15 .Then , we train a classifier to teach the mapping between node characteristics and edge weights 16 . Finally , we apply the trained model to unseen test situations 17 to infer unknown interactions",
        "rewrite_text": "We introduce a method to predict the structural network organization in the brain cortex by utilizing local node characteristics, such as their locations on the brain's surface or within its volume, along with global topological features. This approach allows us to investigate how different types of networks interconnect across various species (human, macaque, mouse) and imaging modalities (diffusion MRI tractography). Our findings indicate that our model can accurately illustrate established patterns of cortico-cortical relationships between brain regions, particularly those recognized in humans but not yet accounted for in monkeys. The proposed framework is versatile, making it applicable to any dataset where information about the positions of individual nodes and their pairwise relationships is available, encompassing both anatomical and physiological imaging datasets. This capability will enable us to explore the connection between organizational architecture and function across multiple scales. \n\nIntroduction: Brain connectomics aims to create a comprehensive mapping of all neuronal components within the human neural architecture. In recent years, advancements in neuroimaging techniques have enabled researchers to produce detailed maps of the brain's structural, cellular, and functional organization. These innovative approaches offer unprecedented opportunities to understand how the brain operates by analyzing its large-scale organization. Nevertheless, despite these advancements, there remains considerable uncertainty regarding the precise nature of relationships between neurons. For example, some regions of the brain have been found to communicate more frequently than others while many others display heightened levels of synchrony. However, it is unclear whether these variations indicate specific wiring necessities or simply arise from random fluctuations. In this study, we propose a new computational framework to address this issue using machine learning techniques. Specifically, we aim to develop models that can predict the strength of connections between pairs of nodes using only information about their direction and configuration. To accomplish this, we first create a set of test instances consisting of pairs of nodes with known interaction capabilities. We then train a classifier to establish the relationship between node characteristics and edge weights. Finally, we apply the trained model to previously unseen test cases to deduce unknown interactions.",
        "ori-fast-z-score": -0.9662823901213162,
        "water-fast-z-score": 9.092421632741246,
        "rewrite-fast-z-score": 0.8894991799933214
    },
    {
        "original_text": "The flicker noise is the low-frequency fluctuations observed in electrical resistance and other transport properties of metals at temperatures below 1 K.  The theory developed by Altshuler, Aronov, and Khmelnitsky (AAK) explains this phenomenon as arising due to electron-electron interactions within the metal film.  In their original work they assumed that electrons are scattered elastically off impurities or phonons.  However, recent experiments have shown that there can be significant inelastic scattering between electrons which leads to additional contributions to the resistivity.  Here we present an extension of AAK s theory for the case where both elastic and inelastic scattering processes contribute to the resistivity.  We show how our results compare with existing experimental data on thin gold films grown epitaxially on silicon substrates. The flicker noise is the low- frequency fluctuations observed in electrical resistance  and other transport properties of metallic systems at temperatures below 1K . It was first discovered in 1963 when measuring the resistance of thin silver wires  1  , but it has since been found in many different types of materials including semiconductors  2  , superconductors  3  , carbon nanotubes  4  , graphene  5  , and topological insulators  6  .\nIn order to explain these observations, Altshuler et al. (AAK) proposed a theoretical model based on the assumption that electrons scatter elastically off impurities  7, 8  .  This approach successfully describes most of the available experimental data  9  , however some discrepancies were recently reported  10  . These deviations may arise because the elastic approximation does not take into account possible inelastic scattering events  11  .",
        "watermark_text": "The flicker sound is the small - frequency fluctuations found in electrical resistance and other transport properties of metals at conditions below 1 K . The theory advanced by Altshuler , Aronov , and Khmelnitsky ( AAK ) presents this phenomenon as occurring due to ion - atom relationships within the metal film . In their early research they thought that electrons are scattered elastically off impurities or phonons .However , recent experiments have shown that there can be considerable inelastic scattering between electrons which results to significant contributions to the resistivity . Here we present an extension of AAK s concept for the case where both elastic and inelastic scattering mechanisms lead to the resistivity .We see how our findings compare with existing experimental evidence on thin gold films grown epitaxially on silicon substrates . The flicker sound is the small - frequency fluctuations detected in electrical resistance and other transport properties of metallic circuits at conditions below 1K .It was first discovered in 1963 when monitoring the tolerance of short metal wires 1 , but it has since been seen in multiple diverse kinds of substances including semiconductors 2 , superconductors 3 , silicon nanotubes 4 , graphene 5 , and topological insulators 6 . In try to explain these observations , Altshuler et al .( AAK ) proposed a conceptual theory based on the assumption that electrons scatter elastically off impurities 7 , 8 . This method effectively describes most of the provided experimental evidence 9 , however some discrepancies were recently noted 10 .These deviations might arise because the elastic equation does not take into consideration proposed inelastic scattering events 11 .",
        "rewrite_text": "The flicker noise consists of low-frequency fluctuations in the electrical resistance and other transport properties of metals at temperatures below 1 K. The theory put forth by Altshuler, Aronov, and Khmelnitsky (AAK) attributes this phenomenon to ion-atom interactions within metallic films. Initially, their research suggested that electrons scatter elastically off impurities or phonons. However, more recent experiments have indicated that significant inelastic scattering between electrons can also make notable contributions to resistivity. In this work, we propose an extension of the AAK theory that accounts for both elastic and inelastic scattering mechanisms contributing to resistivity. We evaluate our findings in relation to existing experimental data on thin gold films grown epitaxially on silicon substrates. Flicker noise was first identified in 1963 during tests on the tolerance of short metal wires, but it has since been observed in a wide range of materials, including semiconductors, superconductors, silicon nanotubes, graphene, and topological insulators. To explain these observations, Altshuler et al. (AAK) developed a conceptual framework based on the premise that electrons scatter elastically off impurities. This model effectively addresses much of the experimental data; however, some recent discrepancies have emerged. These discrepancies might stem from the elastic scattering model’s failure to account for the proposed inelastic scattering events.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 8.693182879212225,
        "rewrite-fast-z-score": 1.8182745801939793
    },
    {
        "original_text": "In this article, we study the equivariant cohomology groups of twisted generalized complex manifolds with respect to compact Lie group actions. We show that these equivariant cohomology groups are isomorphic to those of ordinary generalized complex manifolds equipped with an invariant Hermitian metric and a compatible connection whose torsion is equal to the twisting form. As applications, we compute some examples in detail. In particular, for any closed oriented Riemann surface M , we give explicit formulas for all equivariant Betti numbers of the moduli space of stable vector bundles over M . \nIntroduction\n\nLet G be a compact connected Lie group acting on a smooth manifold X. The equivariant cohomology H*G(X) was introduced by Atiyah-Bott  1  as the cohomology ring of the Borel construction EG×_G X where EG denotes the universal bundle over BG = K(G, 1). It has been studied extensively since then (see e.g.,  2  ). For example, if X is a symplectic manifold acted upon by a torus T, then H*G(X), which can also be viewed as the equivariant cohomology of the corresponding Hamiltonian T-space, plays an important role in mirror symmetry  3  .\nThe notion of generalized complex geometry  4  provides us with another class of interesting geometric objects -the so-called generalized complex manifolds-which include both symplectic and complex manifolds as special cases. Generalized complex structures were first defined by Hitchin  5  using Courant algebroids  6  . Later Gualtieri  7  gave a more intrinsic definition via a pair of almost complex structures satisfying certain compatibility conditions. Recently, it was shown  8  that there exists a one-to-one correspondence between generalized complex structures and pairs consisting of a holomorphic Poisson structure and its associated Nijenhuis tensor field. This result allows us to define generalized complex structures intrinsically without referring to Courant algebroids; see  9  for details. \nGeneralized complex manifolds have attracted much attention recently due to their close connections with string theories  10  . Moreover, they provide new insights into many classical problems such as integr",
        "watermark_text": "In this article , we study the equivariant cohomology groups of distorted generalized complex manifolds with regard to compact Lie group actions . We see that these equivariant cohomology groups are isomorphic to those of simple generalized complex manifolds equipped with an invariant Hermitian metric and a compatible link whose torsion is equal to the twisting structure .As applications , we compute some examples in detail . In particular , for any closed oriented Riemann covering M , we give explicit formulas for all equivariant Betti numbers of the moduli space of stable vector bundles over M .Introduction Let G be a compact connected Lie ring acting on a smooth manifold X . The equivariant cohomology H * G ( X ) was introduced by Atiyah - Bott 1 as the cohomology ring of the Borel construction EG× _ G X where EG indicates the universal bundle over BG = K ( G , 1 ) .It has been studied frequently since then ( saw e . g . , 2 ) . For instance , if X is a symplectic bundle acting upon by a torus T , then H * G ( X ) , which can also be viewed as the equivariant cohomology of the associated Hamiltonian T - set , takes an important role in mirror symmetry 3 .The idea of generalized complex geometry 4 provides us with another class of interesting geometric objects - the so - called generalized complex manifolds - which contain both symplectic and complex manifolds as special cases . Generalized complex systems were first defined by Hitchin 5 utilizing Courant algebroids 6 .Later Gualtieri 7 gave a more intrinsic definition via a pair of almost complex objects satisfying particular compatibility constraints . Recently , it was shown 8 that there exists a one - to - one correspondence between generalized complex objects and pairs consisting of a holomorphic Poisson form and its associated Nijenhuis tensor field .This result allows us to define generalized complex relationships intrinsically without referring to Courant algebroids ; look 9 for information . Generalized complex manifolds have garnered considerable scrutiny lately owing to their deep ties with string structures 10 .Moreover , they give modern perspectives into numerous ancient problems such as integr",
        "rewrite_text": "In this article, we examine the equivariant cohomology groups of distorted generalized complex manifolds concerning actions of compact Lie groups. We find that these equivariant cohomology groups are isomorphic to those of simple generalized complex manifolds that possess an invariant Hermitian metric and a compatible link, where the torsion aligns with the twisting structure. As applications, we provide detailed computations of several examples. Notably, for any closed oriented Riemannian covering \\( M \\), we present explicit formulas for all equivariant Betti numbers of the moduli space of stable vector bundles over \\( M \\).\n\n**Introduction** Let \\( G \\) denote a compact connected Lie group acting on a smooth manifold \\( X \\). The equivariant cohomology \\( H^*_G(X) \\) was introduced by Atiyah and Bott as the cohomology ring of the Borel construction \\( E G \\times_G X \\), where \\( E G \\) represents the universal bundle over \\( B G = K(G, 1) \\). This subject has been studied extensively since its inception. For instance, if \\( X \\) is a symplectic bundle acted upon by a torus \\( T \\), \\( H^*_G(X) \\) can also be interpreted as the equivariant cohomology of the associated Hamiltonian \\( T \\)-set, which plays a significant role in mirror symmetry. \n\nThe concept of generalized complex geometry introduces another intriguing category of geometric objects: generalized complex manifolds, which encompass both symplectic and complex manifolds as specific cases. Generalized complex structures were initially defined by Hitchin using Courant algebroids. Gualtieri later provided a more intrinsic characterization through pairs of almost complex structures that meet particular compatibility conditions. Recent work has demonstrated a one-to-one correspondence between generalized complex structures and pairs comprising a holomorphic Poisson form and its corresponding Nijenhuis tensor field. This discovery permits the definition of generalized complex structures in an intrinsic manner, independent of Courant algebroids. Generalized complex manifolds have attracted significant attention recently due to their profound connections with string structures. They also offer new insights into various longstanding problems in geometry.",
        "ori-fast-z-score": 0.25819888974716115,
        "water-fast-z-score": 6.688444820557844,
        "rewrite-fast-z-score": -0.08606629658238704
    },
    {
        "original_text": "We present the first provably optimal cache-oblivious layouts for meshes with arbitrary aspect ratios and dimensions, which are based on recursive striping into rectangular tiles. Our results hold both in terms of worst-case performance guarantees as well as average case behavior under realistic assumptions about access patterns. We also show how to extend our techniques to handle more general data structures such as trees or graphs. The layout problem is formulated as an optimization problem: given a set of objects that need to be stored in memory, we want to find their positions so that they can be accessed most efficiently by future queries. In this work, we consider the following scenario: A large amount of data needs to be stored in main memory (e.g., a database). This data consists of many small objects (e.g., records) whose sizes vary greatly; some may even be larger than available space. To solve this problem, one could use a standard technique called blocking: each object is divided into blocks of equal size, and then these blocks are placed contiguously within the allocated storage area. However, if there are too few blocks per object, it will not fit entirely inside its assigned block; similarly, if there are too many blocks per object, the unused space between them cannot be used effectively.",
        "watermark_text": "We report the first provably ideal cache - oblivious layouts for meshes with arbitrary aspect ratios and dimensions , which are based on recursive striping into rectangular tiles . Our results hold both in terms of lowest - case performance assurance as well as average case behavior under rigorous constraints about connection schemes .We additionally understand how to expanded our approaches to treat more general data objects such as trees or graphs . The configuration problem is implemented as an optimization problem : given a setting of items that require to be transferred in memory , we try to find their positions so that they can be viewed most efficiently by future queries .In this research , we imagine the following situations : A vast number of data needs to be contained in central memory ( e . g . , a database ) . This data contains of several small items ( e . g . , records ) whose sizes differ significantly ; some may even be larger than provided area .To solve this situation , one might using a traditional technique called blocking : each object is separated into blocks of equal size , and then these blocks are arranged contiguously within the allocated storage space . However , if there are too few blocks per object , it will not fit completely inside its allocated block ; likewise , if there are too several blocks per object , the empty space between them cannot be used effectively .",
        "rewrite_text": "We present the first robustly ideal cache-oblivious layouts for meshes with arbitrary aspect ratios and dimensions, employing a method of recursive striping into rectangular tiles. Our findings ensure both lowest-case performance guarantees and average-case effectiveness under strict constraints regarding connection schemes. Additionally, we have explored how to extend our methods to accommodate more complex data structures, such as trees and graphs. The configuration challenge is framed as an optimization problem: given a set of items that need to be transferred in memory, our goal is to determine their optimal positions for efficient retrieval by future queries. In this study, we envision scenarios where a large amount of data must reside in central memory (e.g., a database). This data comprises various small items (e.g., records) that vary significantly in size; some may even exceed the available space. To address this issue, traditional methods such as blocking can be employed, wherein each object is divided into uniformly sized blocks, which are then stored contiguously within the designated memory space. However, if there are too few blocks per object, it may not completely fit within its assigned block; conversely, if there are too many blocks for a single object, the resulting gaps cannot be utilized efficiently.",
        "ori-fast-z-score": -2.5776089301153053,
        "water-fast-z-score": 6.313641498019764,
        "rewrite-fast-z-score": -0.09407208683835973
    },
    {
        "original_text": "The author considers an alternative approach to classical electrodynamics, which is based on the concept of  pre-metricity  introduced by Einstein in his work  On the Electrodynamics of Moving Bodies  (1905). The main idea behind this approach consists in introducing into consideration not only the usual electromagnetic field tensor Fμν but also its dual counterpart *Fμν . In order to obtain equations for both fields simultaneously one has to introduce additional terms into Lagrangian density L(Fμν ,*Fμν ) that are quadratic with respect to derivatives of these fields. It turns out that such modified Lagrangian leads to the following set of equations: \nwhere  is the d Alembert operator,  is the energy-momentum tensor of matter fields, and  is the current four-vector describing the flow of electric charge. \nIn addition to the standard Maxwell s equations, Eqs. (1)-(3), there appears another equation -the so-called  duality condition :",
        "watermark_text": "The author considers an additional method to theoretical electrodynamics , which is based on the notion of pre - metricity introduced by Einstein in his book On the Electrodynamics of Moving Bodies ( 1905 ) . The main idea behind this methodology consists in adding into consideration not only the usual electrical field vector Fμν but also its dual equivalent * Fμν .In order to obtain equations for both fields concurrently one has to introduce extra terms into Lagrangian density L ( Fμν , * Fμν ) that are quadratic with regard to derivatives of these fields . It turns out that such modified Lagrangian leads to the following series of equations : where is the d Alembert operator , is the electricity - momentum tensor of matter fields , and is the current four - vector describing the movement of electric current .In addition to the standard Maxwell s coefficients , Eqs . ( 1 ) - ( 3 ) , there exists another equation - the so - called duality condition :",
        "rewrite_text": "The author explores an alternative approach to theoretical electrodynamics, which builds on the concept of pre-metricity that Einstein presented in his 1905 book, *On the Electrodynamics of Moving Bodies*. The core idea of this method involves not only the traditional electromagnetic field tensor \\( F_{\\mu\\nu} \\) but also its dual counterpart \\( *F_{\\mu\\nu} \\). To derive equations for both fields simultaneously, additional terms must be incorporated into the Lagrangian density \\( L(F_{\\mu\\nu}, *F_{\\mu\\nu}) \\) that are quadratic in derivatives of these fields. This modified Lagrangian produces a series of equations involving the d'Alembert operator, the electromagnetic momentum-energy tensor of matter fields, and the four-current vector that represents the flow of electric charge. Alongside the standard Maxwell equations (1) to (3), there is also a unique equation known as the duality condition.",
        "ori-fast-z-score": -0.7745966692414834,
        "water-fast-z-score": 3.6147844564602556,
        "rewrite-fast-z-score": 0.13245323570650439
    },
    {
        "original_text": "We present new evidence that jets contribute significantly to the observed infrared and optical emission in some neutron star X-ray binary systems, based on simultaneous multiwavelength observations with Swift/XRT (0.3-10 keV), Chandra/ACIS-S/HRC-I (0.5-8 keV) and Spitzer/IRAC (3.6-24 microns). We find that the spectral energy distribution is well described by an absorbed power law plus blackbody model over this broad range of wavelengths. The best-fit parameters are consistent with those found previously at higher energies using RXTE data alone. \n \n In addition we detect significant variability between epochs separated by months or years. This suggests that there may be multiple components contributing to the overall spectrum; one component which varies rapidly but only weakly contributes to the total flux density, while another component dominates the luminosity output and shows little variation. These results provide further support for models where relativistic outflows play an important role in shaping the evolution of these sources.",
        "watermark_text": "We introduce novel evidence that jets contribute greatly to the seen infrared and optical emission in some neutron star X - ray binary complexes , using on concurrent multiwavelength surveys with Swift / XRT ( 0 . 3 - 10 keV ) , Chandra / ACIS - S / HRC - I ( 0 . 5 - 8 keV ) and Spitzer / IRAC ( 3 . 6 - 24 microns ) . We see that the spectral power distribution is well described by an absorption power law plus blackbody theory over this wide range of wavelengths .The best - fitting factors are compatible with those identified previously at higher energies using RXTE information alone . In addition we find considerable variability between epochs separated by months or months .This implies that there may be several parts leading to the overall spectrum ; one element which varies frequently but only moderately contributes to the total flux concentration , while another component dominates the luminosity production and shows small variation . These conclusions provide further evidence for models where relativistic outflows serve an important role in shaping the evolution of these sources .",
        "rewrite_text": "We present new evidence indicating that jets significantly contribute to the observed infrared and optical emissions in certain neutron star X-ray binary systems. This conclusion is based on concurrent multiwavelength surveys conducted with Swift/XRT (0.3 - 10 keV), Chandra/ACIS-S/HRC-I (0.5 - 8 keV), and Spitzer/IRAC (3.6 - 24 microns). Our analysis shows that the spectral power distribution is effectively described by a combination of an absorption power law and blackbody radiation across this broad range of wavelengths. The best-fitting parameters align well with those determined earlier using RXTE data at higher energies. Additionally, we observe significant variability between observations taken months apart. This suggests that multiple components may influence the overall spectrum: one that changes frequently but only slightly contributes to the total flux, and another component that dominates luminosity production with minimal variation. These findings support models proposing that relativistic outflows play a crucial role in the evolution of these celestial sources.",
        "ori-fast-z-score": -1.4814874939752933,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 0.6708203932499369
    },
    {
        "original_text": "We present the results of searches for Pevatron candidates in the northern hemisphere using data collected by the High Energy Stereoscopic System (HESS) between 2004 and 2007, as well as IceCube data taken during 2005-2007. We find no significant excesses above background expectations at any point on the sky. Upper limits are set on the flux density of TeV photons and neutrinos associated with hypothetical sources within our field-of-view. These upper limits are used to constrain theoretical models describing the production mechanisms responsible for accelerating particles up to energies approaching 10^14 eV. The HESS collaboration has recently reported an observation of a new source of very-high-energy (VHE; >100 GeV) gamma-rays located near the Galactic Center  1  . This source is spatially coincident with the supernova remnant Sgr A East  2  , which was previously detected in radio waves  3  .\nThe discovery of this VHE source raises several questions about its origin. In particular, it remains unclear whether or not the observed emission arises directly from accelerated protons interacting with ambient gas  4  , or if other processes such as inverse Compton scattering off electrons  5  and/or bremsstrahlung  6  play a dominant role. It also remains unknown how these energetic particles were accelerated to their high energy levels  7, 8  .",
        "watermark_text": "We publish the conclusion of investigations for Pevatron candidates in the northern hemisphere using data taken by the High Energy Stereoscopic System ( HESS ) between 2004 and 2007 , as well as IceCube information taken during 2005 - 2007 . We see no considerable excesses above background expectations at any point on the sky .Upper constraints are set on the flux concentration of TeV photons and neutrinos associated with hypothetical sources within our field - of - view . These upper limits are applied to constrain theoretical theories describing the production mechanisms involved for accelerating particles up to energies approaching 10 ^ 14 eV .The HESS collaboration has recently noted an observation of a new source of very - large - energy ( VHE ; > 100 GeV ) γ - radiation located near the Galactic Center 1 . This source is spatially coincident with the supernova remnant Sgr A East 2 , which was formerly detected in radio pulses 3 .The observation of this VHE source raises various issues about its origin . In particular , it remains unsure whether or not the reported emission arises directly from accelerated protons interacting with ambient gas 4 , or if other processes such as inverse Compton diffusion off electrons 5 and / or bremsstrahlung 6 hold a dominant role .It additionally exists unknown how these active grains were accelerated to their high energy levels 7 , 8 .",
        "rewrite_text": "We present the findings from our investigations into Pevatron candidates in the northern hemisphere, utilizing data collected by the High Energy Stereoscopic System (HESS) from 2004 to 2007, along with IceCube data from 2005 to 2007. Our analysis reveals no significant excesses beyond the expected background across the entire sky. We establish upper limits on the flux of TeV photons and neutrinos that could be associated with potential sources within our observational scope. These constraints help refine theoretical models that describe the mechanisms behind the acceleration of particles to energies nearing 10^14 eV. Recently, the HESS collaboration identified a new source of very high-energy (VHE; > 100 GeV) gamma radiation located near the Galactic Center. This source coincides spatially with the supernova remnant Sgr A East, which has previously been observed in radio waves. The discovery of this VHE source prompts various questions regarding its origins. Specifically, it is unclear whether the reported emissions result directly from accelerated protons interacting with surrounding gas, or if other processes, such as inverse Compton scattering off electrons or bremsstrahlung, play a more significant role. Additionally, the mechanisms behind the acceleration of these high-energy particles remain unknown.",
        "ori-fast-z-score": -0.42640143271122083,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": -0.22360679774997896
    },
    {
        "original_text": "We present photoionization models for the heliosheath, which is the region between the termination shock (TS) at ~100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU in the solar system s frame but within 0.3 AU in the rest frame of the Sun. We use these models to constrain the boundary conditions of the heliosphere using interstellar neutral hydrogen data obtained with the Lyman-alpha instrument on board the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements made near Earth during the Voyager 2 mission. Our results show that the TS distance decreases with increasing solar activity; this effect can be explained by an increase in the density of the solar wind plasma. For low solar activity levels we find that the TS distance agrees very well with previous estimates based on observations of energetic particles.",
        "watermark_text": "We use photoionization estimates for the heliosheath , which is the region between the termination shock ( TS ) at ~ 100 AU and the solar wind termination shock ( SWTS ) . The SWTS is situated beyond 1 AU in the solar system s frame but within 0 . 3 AU in the remainder frame of the Sun .We use these models to constrain the boundary parameters of the heliosphere utilizing interstellar neutral hydrogen evidence derived with the Lyman - alpha instrument on board the Solar Wind Anisotropy Probe ( SWAP ) , as well as in situ measurements made near Earth during the Voyager 2 mission . Our results show that the TS distance reduces with rising sun activity ; this effect can be described by an increase in the density of the solar wind plasma .For low solar activity rates we find that the TS distance agrees very best with previous estimates based on observations of energetic particles .",
        "rewrite_text": "We utilize photoionization estimates for the heliosheath, defined as the region situated between the termination shock (TS) at approximately 100 AU and the solar wind termination shock (SWTS). The SWTS is located beyond 1 AU within the solar system frame, but within 0.3 AU in the Sun's rest frame. These models allow us to determine the boundary parameters of the heliosphere by analyzing interstellar neutral hydrogen data obtained from the Lyman-alpha instrument aboard the Solar Wind Anisotropy Probe (SWAP), as well as in situ measurements from near Earth during the Voyager 2 mission. Our findings reveal that the distance to the TS decreases with increasing solar activity, which can be attributed to a rise in the density of the solar wind plasma. During periods of low solar activity, we find that the TS distance closely aligns with prior estimates based on observations of energetic particles.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 2.994344952558949,
        "rewrite-fast-z-score": 0.9113223768657671
    },
    {
        "original_text": "We propose that the metallic enhancement observed for some post T Tauri stars (PTTS) may be due to an accretion of planetesimals during their formation phase, which is followed by rapid planet formation and subsequent ejection of planets into space.  We show that this scenario can explain both the high metallicity found among PTTS as well as the low abundance ratios between refractory elements such as Mg/Si or Al/Si compared with those expected if these objects formed through standard core-accretion processes. The proposed mechanism also explains why there are no known close-in giant planets around PTTSs despite the fact that they have already passed their protoplanetary disk stage. This model predicts that most PTTS should host at least one Jupiter mass planet on wide orbits beyond 1 AU. In addition we predict that many PTTS will exhibit infrared excesses caused by dusty debris disks produced by collisions between planetary bodies.",
        "watermark_text": "We suggest that the metallic enhancement detected for some post T Tauri planets ( PTTS ) may be due to an accretion of planetesimals during their formed phase , which is preceded by rapid planet development and subsequent ejection of stars into space . We suggest that this situation can describe both the high metallicity observed among PTTS as well as the poor abundance proportions between refractory objects such as Mg / Si or Al / Si compared with those expected if these objects created through conventional core - accretion cycles .The proposed process also explains why there are no known close - in massive planets around PTTSs despite the fact that they have already completed their protoplanetary disk stage . This theory predicts that most PTTS should accommodate at least one Jupiter mass planet on wide orbits beyond 1 AU .In addition we estimate that several PTTS will exhibit infrared excesses caused by dusty dust belts generated by collisions between planetary body .",
        "rewrite_text": "We propose that the metallic enhancement observed in some post-T Tauri stars (PTTS) may result from the accretion of planetesimals during their formation phase, which follows a period of rapid planet development and the subsequent ejection of stars into space. This scenario may account for the high metallicity found in PTTS, as well as the unusual abundance ratios between refractory elements, such as Mg/Si or Al/Si, which differ from what would be expected if these objects formed through standard core-accretion processes. Furthermore, this hypothesis helps explain the absence of known close-in massive planets around PTTS, even though they have already passed through the protoplanetary disk stage. We anticipate that most PTTS should host at least one Jupiter-mass planet in wide orbits beyond 1 AU. Additionally, we estimate that several PTTS may show infrared excesses due to dusty belts formed by collisions between planetary bodies.",
        "ori-fast-z-score": -1.116312611302876,
        "water-fast-z-score": 5.829632525692798,
        "rewrite-fast-z-score": -0.3721042037676254
    },
    {
        "original_text": "The nonperiodic anyon model is introduced as an alternative to the periodic one in order to explain fractional quantum hall effect (FQHE). The ground state wave function for this system is obtained by using the method of projection operators, which leads to a new expression for the Laughlin wave functions. It is shown that these states are exact eigenstates of the total angular momentum operator with eigenvalues equal to the number of particles times their charge e*. This result shows that the nonperiodic anyons can be considered as charged particles moving on a sphere. Finally we show how our results can be applied to describe FQHE at filling fractions other than 1/3. In recent years there has been considerable interest in studying systems consisting of interacting electrons confined to two dimensions  1  . One of the most interesting phenomena observed experimentally  2  , known as the fractional quantum Hall effect (FQHE), occurs when such two-dimensional electron gas is subjected to strong magnetic fields  3  .\nIn the original work  4  it was suggested that the FQHE could be explained within the framework of the so-called Laughlin wave functions  5  . These wave functions were constructed by assuming that each particle moves around its own guiding center  6  . However, later studies  7 -9  showed that the actual behavior of the electrons in real experiments cannot be described accurately by considering them as point-like objects. Instead, they should be treated as extended objects whose size depends upon the strength of the external magnetic field  10  .",
        "watermark_text": "The nonperiodic anyon theory is developed as an alternative to the periodic one in order to explain fractional quantum hall impact ( FQHE ) . The ground state wave function for this system is found by using the method of projection operators , which results to a new representation for the Laughlin wave functions .It is seen that these states are exact eigenstates of the total angular velocity operator with eigenvalues equal to the number of molecules times their charge e * . This result shows that the nonperiodic anyons can be regarded as charged particles moving on a sphere .Finally we prove how our findings can be applied to define FQHE at filling fractions other than 1 / 3 . In recent years there has been substantial interest in investigating systems composed of interacting electrons confined to two dimensions 1 .One of the most exciting phenomena observed experimentally 2 , known as the fractional quantum Hall phenomenon ( FQHE ) , happened when such two - dimensional electron gas is subjected to powerful magnetic fields 3 . In the first paper 4 it was suggested that the FQHE might be described within the framework of the so - called Laughlin wave curves 5 .These wave systems were created by assuming that each particle moves surrounding its own guiding center 6 . However , later research 7 - 9 demonstrated that the actual activity of the electrons in real studies never be described properly by treating them as point - like structures .Instead , they should be treated as extended things whose extent depends upon the strength of the external magnetic force 10 .",
        "rewrite_text": "The nonperiodic anyon theory has been developed as an alternative to the periodic theory to explain the fractional quantum Hall effect (FQHE). By employing the method of projection operators, we derive the ground state wave function for this system, leading to a novel representation of the Laughlin wave functions. It is observed that these states serve as exact eigenstates of the total angular velocity operator, with eigenvalues equal to the product of the number of particles and their charge (e*). This finding indicates that nonperiodic anyons can be viewed as charged particles moving on a sphere. Additionally, we demonstrate how our results can be utilized to define FQHE at filling fractions other than 1/3. Recent years have seen considerable interest in exploring systems of interacting electrons confined to two dimensions. One of the most intriguing phenomena observed experimentally is the fractional quantum Hall effect (FQHE), which occurs when such two-dimensional electron gases are subjected to strong magnetic fields. The initial proposal suggested that FQHE could be described using the framework of the Laughlin wave functions, which were formulated by assuming that each particle orbited around its own guiding center. However, subsequent research has shown that the behavior of electrons in actual experiments cannot be accurately characterized by treating them as point-like particles. Instead, they should be considered as extended entities, with their extent influenced by the strength of the external magnetic field.",
        "ori-fast-z-score": -1.3112201362143716,
        "water-fast-z-score": 6.313641498019764,
        "rewrite-fast-z-score": -0.769800358919501
    },
    {
        "original_text": "We present the results on morphology and luminosity function for the most luminous galaxy clusters in the Universe, selected by their X-ray emission (the RCS2 sample). We find that these objects are characterized by an elliptical shape with axial ratio q = 0.7 ± 0.1 and by a steep luminosity function dN/dL ∝ L−2.5±0.3 . The observed properties suggest that they may be identified as fossil groups or proto-clusters at z > 1.0 .\nThe data used here were obtained during our observing runs performed at ESO telescopes under programs IDs: 073.A-0505(B), 078.A-0518(C) and 079.A-0739(D) . In this work we study the morphological and photometric properties of the brightest galaxy clusters in the universe. These systems have been detected through their X-ray emission using the ROSAT All Sky Survey (RASS; Voges et al., 1999) , and then followed up spectroscopically to confirm their redshifts and measure their velocity dispersions (see e.g. Rosati et al. , 1998 , Gladders & Yee 2005 , Eisenhardt et al. , 2008 . They represent some of the most massive structures known so far in the universe, being able to host several thousands of galaxies each one. Their high mass makes them ideal targets to investigate how such large scale structures form and evolve over time.",
        "watermark_text": "We present the conclusion on morphology and luminosity function for the most luminous galaxy galaxies in the Universe , selected by their X - ray radiation ( the RCS2 specimen ) . We see that these objects are marked by an elliptical shape with axial ratio g = 0 . 7 ± 0 . 1 and by a steep luminosity function dN / dL [UNK] L−2 . 5±0 . 3 .The observed properties suggest that they may be identified as extinct families or proto - complexes at z > 1 . 0 . The data used here were obtained during our observing walks performed at ESO telescopes under programs IDs : 073 . A - 0505 ( B ) , 078 . A - 0518 ( C ) and 079 . A - 0739 ( D ) .In this research we study the morphological and photometric properties of the brightest galaxy galaxies in the universe . These systems have been detected through their X - ray emission utilizing the ROSAT All Sky Survey ( RASS ; Voges et al . , 1999 ) , and then followed up spectroscopically to confirm their redshifts and track their velocity dispersions ( see e . g .Rosati et al . , 1998 , Gladders & Yee 2005 , Eisenhardt et al ., 2008 . They hold some of the most gigantic structures discovered so far in the universe , being could to host numerous thousands of galaxies each one .Their high mass creates them ideal targets to examine how such large scale structures structure and evolve over time .",
        "rewrite_text": "We present our findings on the morphology and luminosity function of the most luminous galaxies in the Universe, identified through their X-ray emissions from the RCS2 sample. These galaxies exhibit an elliptical morphology with an axial ratio of g = 0.7 ± 0.1 and a steep luminosity function described by dN / dL ∝ L−2.5 ± 0.3. The characteristics observed suggest that these galaxies may represent extinct families or proto-complexes at redshifts greater than z = 1. The data for this study were collected during our observations at ESO telescopes under several programs (IDs: 073.A-0505 (B), 078.A-0518 (C), and 079.A-0739 (D)). Our research focuses on the morphological and photometric properties of these bright galaxies, which were detected through their X-ray emissions in the ROSAT All Sky Survey (RASS; Voges et al., 1999), and further confirmed through spectroscopic follow-ups to determine their redshifts and analyze their velocity dispersions (see, e.g., Rosati et al., 1998; Gladders & Yee, 2005; Eisenhardt et al., 2008). These galaxies represent some of the largest structures found to date, potentially hosting thousands of individual galaxies within them. Their significant mass makes them excellent candidates for studying the formation and evolution of large-scale structures over time.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 5.512930714537517,
        "rewrite-fast-z-score": -0.562543950463012
    },
    {
        "original_text": "We present the results of an analysis of the shapes, orientations, and alignments of dark matter subhalos in cosmological N-body simulations with different levels of baryonic physics included.  We find that the inclusion of baryons has little effect on the shape distribution but does affect the spin parameter distributions significantly; halos are more spherical when baryons are included than they would be if only gravity were acting upon them. The halo spins tend to be aligned perpendicularly to their major axes for all models considered here (including pure dark matter). This is consistent with previous studies which have found similar trends using other methods. However we also find evidence that this trend may not hold at very small scales where there appears to be some correlation between the direction of the angular momentum vector and the minor axis of the halo. Finally, we show that the presence or absence of baryons affects the degree of alignment between neighboring halos; halos are less strongly clustered around each other when baryons are included.",
        "watermark_text": "We present the conclusion of an assessment of the shapes , orientations , and alignments of bright matter subhalos in cosmological N - bodies simulations with various levels of baryonic physics provided . We see that the inclusion of baryons has little impact on the form distribution but does affect the spin vector distributions substantially ; halos are more elongated when baryons are incorporated than they would be if only gravitational were acting upon them .The halo orbits tend to be aligned perpendicularly to their major axes for all models discussed here ( especially pure black material ) . This is consistent with previous research which have discovered similar trends using other methods .However we also find proof that this shift might not stand at very small scales where there seems to be some correlation between the direction of the angular velocity tensor and the minor axis of the halo . Finally , we find that the presence or lack of baryons affects the degree of alignment between neighboring halos ; halos are less highly clustered around each other when baryons are included .",
        "rewrite_text": "We present the results of an analysis on the shapes, orientations, and alignments of bright matter subhalos derived from cosmological N-body simulations that incorporate varying levels of baryonic physics. Our findings indicate that while the inclusion of baryons has minimal effect on the distribution of shapes, it significantly influences the distributions of spin vectors; specifically, halos tend to be more elongated when baryons are included compared to scenarios where only gravity is considered. Furthermore, halo orbits are generally oriented perpendicularly to their major axes across all models examined, particularly in those comprising solely dark matter. This observation aligns with earlier studies that have reported similar patterns using different methodologies. However, we also discover evidence suggesting that this alignment may not hold at very small scales, where a correlation appears between the angular velocity tensor's direction and the halo's minor axis. Lastly, we conclude that the presence or absence of baryons influences the alignment degree among neighboring halos, with halos exhibiting less clustering when baryons are present.",
        "ori-fast-z-score": -1.9896995023342199,
        "water-fast-z-score": 5.347391382215687,
        "rewrite-fast-z-score": 0.1125087900926024
    },
    {
        "original_text": "The colour reconnection (CR) model is used to describe how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering processes, such as those occurring during e+e-annihilation events.  The CR model predicts that particles emitted close together in phase space will be more likely to recombine than those which are further apart.  This effect can lead to changes in event topology and kinematics compared to predictions made using models without CR.  In this analysis we use data collected by the Delphi experiment operating at centre-of-mass energies between 189 GeV and 209 GeV corresponding to an integrated luminosity of 1.1 fb-1.  We measure the fraction of WW events where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and compare these results to Monte Carlo simulations including and excluding CR effects.  Our measurements show no significant evidence for CR effects within our experimental uncertainties.",
        "watermark_text": "The colour reconnection ( CR ) model is utilized to explain how quarks and gluons rearrange themselves into hadrons after they are produced by hard scattering mechanisms , such as those occurring during e + e - annihilation processes . The CR theory predicts that particles emitted close together in phase space will be more likely to recombine than those which are further separated .This phenomenon can lead to changes in event topology and kinematics compared to forecast making using models without CR . In this analysis we utilize evidence generated by the Delphi experiment working at centre - of - mass energies between 189 GeV and 209 GeV corresponding to an unified luminosity of 1 . 1 fb - 1 .We estimate the fraction of WW occasions where one or both W bosons decay leptonically for different ranges of dilepton invariant mass and compare these results to Monte Carlo simulations using and excluding CR effects . Our measurements show no important proof for CR influences within our research uncertainties .",
        "rewrite_text": "The color reconnection (CR) model explains the process by which quarks and gluons rearrange into hadrons following their production through hard scattering mechanisms, such as those occurring during \\( e^+ e^- \\) annihilation. According to CR theory, particles that are emitted close together in phase space are more likely to recombine than those that are farther apart. This effect can result in alterations to event topology and kinematics when compared to predictions made using models that do not account for CR. In this analysis, we leverage data generated by the Delphi experiment, which operated at center-of-mass energies ranging from 189 GeV to 209 GeV, accumulating a total integrated luminosity of 1.1 fb\\(^{-1}\\). We estimate the fraction of WW events where one or both W bosons decay leptonically across various ranges of dilepton invariant mass and compare these findings with Monte Carlo simulations that include and exclude CR effects. Our measurements do not provide significant evidence for the influence of CR within the uncertainties of our research.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.905778905196061,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "We report on the detection of an extremely rare supersoft X-ray outburst event in the transient source 1E0102. 2-7219 (=GX 354-0) with the Suzaku satellite, which was triggered by a large increase in its hard Xray flux. The duration and peak luminosity are estimated to be about 100 s and 2×1036 erg/s at 6 kpc distance, respectively. This is one of only two such events ever detected for this object. We discuss possible origins of these events based on their observed properties. \n \n Keywords: Supernova remnant, Soft gamma-ray repeater, Transient source, Supersoft X-ray emission, Hard X-ray bursts \n \n \n \n 1 Introduction \n \n In recent years, several new classes of transients have been discovered through systematic searches using satellites like RXTE/ASM or Swift/BAT. These include soft-gamma repeaters (SGRs; e.g., Hurley et al. 2005), anomalous X-ray pulsars (AXPs; e.g., Kaspi & Beloborodov 2017) , and magnetar candidates (e.g., Rea et al. 2012) . Among them, SGRs show repeated short-duration bursts of high-energy radiation ranging from radio waves to gammarays. AXPs are characterized by persistent X-ray emissions that often exhibit periodic pulsations. Magnetar candidates also show similar characteristics as those of AXPs but lack clear evidence of periodicity. All three types of sources occasionally emit giant flares accompanied by energetic particle acceleration phenomena (e.g., Palmer 2014; Kashiyama et al. 2013 ). On the other hand, some of these objects sometimes undergo very faint outbursts lasting for hours to days. For example, SGR 0526-66 showed a series of such outbursts between 1979 and 1989 (Mazets et al. 1981; Cline et al. 1982; Kulkarni et al. 1993; Kouveliotou et al. 1998 ) while SGR 1900+14 exhibited another series of fainter ones between 1997 and 2001 . Such",
        "watermark_text": "We report on the observation of an incredibly rare supersoft X - ray outburst incident in the transient stream 1E0102 . 2 - 7219 ( = GX 354 - 0 ) with the Suzaku spacecraft , which was sparked by a large rise in its hard Xray flux .The period and peak luminosity are estimated to be about 100 s and 2×1036 erg / s at 6 kpc distance , respectively . This is one of only two such events ever observed for this object .We discuss possible origins of these events according on their observed properties . Keywords : Supernova remnant , Soft gamma - ray repeater , Transient precursor , Supersoft X - ray radiation , Hard X - ray flare 1 Introduction In recent years , various additional types of transients have been detected through widespread searches using satellites like RXTE / ASM or Swift / BAT .These include soft - gamma repeaters ( SGRs ; e . g . , Hurley et al . 2005 ) , anomalous X - ray pulsars ( AXPs ; e . g . , Kaspi & Beloborodov 2017 ) , and magnetar candidates ( e . g . , Rea et al .2012 ) . Among them , SGRs exhibit frequent short - duration bursts of high - energy rays ranging from radio beams to gammarays .AXPs are marked by persistent X - ray emissions that frequently exhibit periodic pulsations . Magnetar candidates often show identical traits as those of AXPs but lack firm indication of periodicity .All three categories of sources occasionally emit giant flares driven by energetic particle acceleration phenomena ( e . g . , Palmer 2014 ; Kashiyama et al . 2013 ) .On the other hand , some of these objects sometimes undergo very faint outbursts lasting for days to days . For instance , SGR 0526 - 66 demonstrated a sequence of such outbursts between 1979 and 1989 ( Mazets et al .1981 ; Cline et al . 1982 ; Kulkarni et al .1993 ; Kouveliotou et al . 1998 ) while SGR 1900 + 14 exhibited another series of fainter ones between 1997 and 2001 .Such",
        "rewrite_text": "We report the observation of an extremely rare supersoft X-ray outburst in the transient object 1E0102.2-7219 (also known as GX 354-0) using the Suzaku spacecraft, triggered by a significant increase in its hard X-ray flux. The estimated duration and peak luminosity are approximately 100 seconds and 2×10^36 erg/s, respectively, at a distance of 6 kpc. This event is one of only two such occurrences recorded for this object. We explore potential origins for these events based on their observed characteristics. \n\n**Keywords**: Supernova remnant, Soft gamma-ray repeater, Transient precursor, Supersoft X-ray radiation, Hard X-ray flare \n\n**1 Introduction**  \nIn recent years, various new types of transients have been detected through extensive searches utilizing satellites like RXTE/ASM and Swift/BAT. These include soft gamma-ray repeaters (SGRs; e.g., Hurley et al. 2005), anomalous X-ray pulsars (AXPs; e.g., Kaspi & Beloborodov 2017), and magnetar candidates (e.g., Rea et al. 2012). SGRs are characterized by frequent short-duration bursts of high-energy rays, which range from radio waves to gamma rays. AXPs are noted for their persistent X-ray emissions that commonly display periodic pulsations. Magnetar candidates share similar traits to AXPs but lack definitive evidence of periodicity. All three categories of sources can occasionally produce giant flares resulting from energetic particle acceleration phenomena (e.g., Palmer 2014; Kashiyama et al. 2013). Additionally, some of these objects experience very faint outbursts lasting from days to weeks. For example, SGR 0526-66 exhibited a series of such outbursts between 1979 and 1989 (Mazets et al. 1981; Cline et al. 1982; Kulkarni et al. 1993; Kouveliotou et al. 1998), while SGR 1900+14 showed another set of subdued events between 1997 and 2001.",
        "ori-fast-z-score": -1.1141720290623112,
        "water-fast-z-score": 5.501778368617852,
        "rewrite-fast-z-score": 0.8181818181818182
    },
    {
        "original_text": "We present an analysis of galaxy morphologies, luminosities and environments for galaxies in the Abell 901;902 supercluster (z = 0.18) using data obtained with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope. We find that the fraction of early-type galaxies increases strongly towards higher local densities within this supercluster. The morphological mix is also found to depend significantly on absolute magnitude; fainter galaxies are more likely to be late-types than brighter ones at fixed density. These results suggest that both environmental effects and internal processes play important roles in shaping the observed morphology-density relation. This work was supported by NASA grant NAG5-7697. -The distribution of all galaxies in our sample overlaid onto the X-ray emission detected by Chandra . Galaxies are colour-coded according to their spectroscopic redshifts as indicated in the legend.  -The projected number density profile of cluster members around Abell 902 , determined from the photometric redshift catalogue .  -The projected number density profiles of different morphological types around Abell 902 .   -The projected number density of brightest cluster galaxies ( M V < −20 ) around Abell 902 .\n-Figures showing the dependence of galaxy properties on distance from Abell 902 .",
        "watermark_text": "We present an assessment of galaxy morphologies , luminosities and habitats for galaxies in the Abell 901 ; 902 supercluster ( z = 0 . 18 ) using data acquired with the Wide Field Imager on board the European Southern Observatory s Very Large Telescope . We see that the fraction of early - class stars increases strongly towards higher local densities within this supercluster .The morphological mixing is also discovered to depend greatly on absolute magnitude ; fainter clusters are more likely to be late - types than brighter ones at fixed density . These data suggest that both environmental impacts and internal mechanisms play crucial roles in shaping the seen morphology - density relation .This project was supported by NASA grant NAG5 - 7697 . - The distribution of all galaxies in our sample overlaid onto the X - ray radiation observed by Chandra .Galaxies are colour - coded according to their spectroscopic redshifts as indicated in the legend . - The estimated number density profile of cluster groups around Abell 902 , obtained from the photometric redshift catalogue .- The estimated number density profiles of different morphological types around Abell 902 . - The estimated number density of brightest cluster objects ( M V < −20 ) around Abell 902 .- Figures displaying the dependence of galaxy features on distance from Abell 902 .",
        "rewrite_text": "We provide an evaluation of galaxy morphologies, luminosities, and environments for galaxies within the Abell 901/902 supercluster (z = 0.18), utilizing data collected with the Wide Field Imager on the European Southern Observatory's Very Large Telescope. Our findings indicate a significant increase in the proportion of early-type galaxies as local densities rise within the supercluster. Additionally, we observe that morphological diversity is heavily influenced by absolute magnitude: fainter clusters tend to host more late-type galaxies compared to their brighter counterparts at fixed density levels. These results imply that both environmental factors and internal processes are essential in molding the observed morphology-density relationship. This project received support from NASA grant NAG5-7697. Key findings include the distribution of all sampled galaxies overlaid on X-ray emissions captured by Chandra, with galaxies color-coded according to their spectroscopic redshifts as described in the legend. We also present estimated number density profiles of cluster groups surrounding Abell 902, as derived from the photometric redshift catalog, as well as the density profiles for various morphological types and the estimated density of the brightest cluster members (M_V < -20) around Abell 902. Finally, we include figures illustrating how galaxy characteristics vary with distance from Abell 902.",
        "ori-fast-z-score": -0.9649012813540153,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": 1.4439897447623107
    },
    {
        "original_text": "We study the effect of geometry on the transport properties in one-dimensional (1D) and two-dimensional (2D) systems with periodic boundary conditions, by using an exact mapping to random walks. We show that for 1D chains there is no difference between open-boundary and periodic-boundary conditions; however, this is not true anymore when considering 2D lattices. In particular we find that the self-diffusion coefficient D(t), which describes how fast particles diffuse through space at time t, depends strongly on the lattice structure. For example, if the system consists of two identical sub-lattices connected via hopping processes only along the bonds connecting them, then D(t) exhibits a non-monotonic behavior as function of time: it first increases until reaching its maximum value after some characteristic relaxation time, followed by a decrease towards zero. This behavior can be explained by analyzing the probability distribution P(r,t) of finding a particle at position r at time t.",
        "watermark_text": "We research the impact of geometry on the travel properties in one - dimensional ( 1D ) and two - dimensional ( 2D ) networks with periodic border conditions , by using an precise mapping to random tours . We see that for 1D chains there is no difference between open - boundary and periodic - boundary conditions ; however , this is not true anymore when using 2D lattices .In particular we find that the self - diffusion coefficient D ( t ) , which explains how rapid particles diffuse through space at period t , depends strongly on the lattice structure . For instance , if the system contains of two identical sub - lattices bonded via hopping processes only along the bonds connecting them , then D ( t ) exhibits a non - monotonic activity as function of time : it first rises until reaching its highest value after some characteristic relaxation time , followed by a drop towards zero .This phenomenon can be understood by analyzing the probability distribution P ( r , t ) of finding a particle at position r at time t .",
        "rewrite_text": "We investigate how geometry influences travel properties in one-dimensional (1D) and two-dimensional (2D) networks with periodic boundary conditions through a precise mapping to random tours. Our findings indicate that, in 1D chains, there is no distinction between open and periodic boundary conditions. However, this distinction becomes significant in 2D lattices. Specifically, we observe that the self-diffusion coefficient D(t), which characterizes the rate at which particles diffuse through space over time t, is heavily dependent on the lattice structure. For example, in a system comprised of two identical sub-lattices connected solely through hopping processes along their interconnecting bonds, D(t) shows a non-monotonic behavior over time: it increases to a peak value after a certain relaxation time and then declines toward zero. This phenomenon can be elucidated by examining the probability distribution P(r, t) of locating a particle at position r at time t.",
        "ori-fast-z-score": -0.3375263702778072,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "We present the first measurement of the supermassive black hole (SMBH) mass function for both early-(elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxies using data from the Millennium Galaxy Catalogue (MGC). We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations. Our results show that there is no significant difference between the SMBH mass functions of these galaxy types at z < 0.1. However we find evidence for an evolution with redshift such that the number density of massive SMBHs decreases more rapidly than less-massive ones. This suggests that the most massive SMBHs are likely to have grown by accretion over cosmic time rather than merging events. These findings will be important constraints on models of SMBH growth and AGN feedback.",
        "watermark_text": "We present the first measurement of the supermassive black hole ( SMBH ) mass distribution for both late - ( elliptical / S0 , Sa - Sb ) and latetype ( Scd - Sm ) clusters using data from the Millennium Galaxy Catalogue ( MGC ) . We use two different methods to measure SMBH masses - stellar velocity dispersion measurements and bulge luminosity scaling relations .Our results show that there is no major variation between the SMBH mass distributions of these galaxy forms at z < 0 . 1 . However we find proof for an evolution with redshift such that the number density of large SMBHs falls more swiftly than less - massive ones .This implies that the most gigantic SMBHs are likely to have expanded by accretion over universe time rather than joining events . These conclusions will be crucial constraints on estimates of SMBH growth and AGN feedback .",
        "rewrite_text": "We present the inaugural measurement of the mass distribution of supermassive black holes (SMBHs) in both late-type (elliptical/S0, Sa-Sb) and latetype (Scd-Sm) galaxy clusters, utilizing data from the Millennium Galaxy Catalogue (MGC). Our study employs two distinct methods to assess SMBH masses: stellar velocity dispersion measurements and scaling relations based on bulge luminosity. Our findings indicate no significant differences between the SMBH mass distributions of these galaxy types at redshifts z < 0.1. However, we do observe evidence of redshift evolution, revealing that the number density of large SMBHs decreases more rapidly than that of their less massive counterparts. This suggests that the largest SMBHs likely gained mass through accretion over cosmic time, rather than through merger events. These insights are vital for refining our understanding of SMBH evolution and active galactic nucleus (AGN) feedback mechanisms.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.077963596336064,
        "rewrite-fast-z-score": -1.524001524002286
    },
    {
        "original_text": "We present new maps of the circumstellar SiO (v=1, v=2) masers around the Mira variable R Leo obtained with the Very Long Baseline Array at 22 GHz and 43 GHz. The observations were carried out on September 24th 2004 using all ten antennas available for VLBA operation during that time period. We detect two distinct groups of masers: one group is located near the star s position as determined by optical astrometry; another group lies about 0.5 arcsec to the south-west of this location. Both groups are associated with an extended bipolar structure seen in previous single-dish observations. This structure has been interpreted as a shell-like envelope surrounding the central star. Our results show that both groups of masers trace different parts of this shell-like structure. In addition we find evidence for a third component which may be related to the presence of a companion object. \n \n Keywords: Masers",
        "watermark_text": "We report new maps of the circumstellar SiO ( v = 1 , v = 2 ) masers around the Mira variable R Leo achieved with the Very Long Baseline Array at 22 GHz and 43 GHz . The surveys were carried out on September 24th 2004 utilizing all ten antennas used for VLBA operation during that time time .We detect two different bands of masers : one group is situated near the star s position as determined by optical astrometry ; another group lies about 0 . 5 arcsec to the south - west of this location . Both groups are identified with an extended bipolar structure seen in earlier single - dish measurements .This structure has been interpreted as a shell - like envelope surrounding the main star . Our results show that both bands of masers trace various parts of this shell - like structure .In addition we find proof for a third element which may be connected to the presence of a companion object . Keywords : Masers",
        "rewrite_text": "We present new maps of the circumstellar SiO masers (v = 1, v = 2) surrounding the Mira variable star R Leo, obtained using the Very Long Baseline Array (VLBA) at frequencies of 22 GHz and 43 GHz. The observations were conducted on September 24, 2004, employing all ten VLBA antennas operational at that time. We identify two distinct groups of masers: one located near the star's position as determined by optical astrometry, and the other approximately 0.5 arcseconds to the southwest. Both groups correspond to an extended bipolar structure observed in previous single-dish measurements, which is interpreted as a shell-like envelope surrounding the central star. Our findings indicate that both maser bands represent different regions of this shell-like structure. Additionally, we provide evidence for a third component that may be associated with the presence of a companion star. Keywords: Masers.",
        "ori-fast-z-score": 0.8819171036881969,
        "water-fast-z-score": 5.588005588008382,
        "rewrite-fast-z-score": 0.39056673294247163
    },
    {
        "original_text": "Muon-catalyzed fusion (MCF) is an exotic nuclear reaction that can be used to produce energy in future reactors, but it requires extremely pure hydrogen gas as fuel.  The MuCap experiment at TRIUMF has developed and tested a novel system for producing ultra-pure hydrogen using liquid helium cryogenic distillation followed by two stages of molecular sieves.  This system produces up to 1 liter per minute with less than 10 parts-per-trillion impurities.  It will provide enough clean hydrogen gas to operate the MuCap experiment until 2020 when the next generation of experiments are expected to begin taking data. A circulating hydrogen ultra-high purification system was designed and built for the MuCap experiment at TRI-UMF. Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels required for MCF research. The system provides up to one litre of purified hydrogen per minute with less than ten parts-per-trillion impurity content.",
        "watermark_text": "Muon - catalyzed fusion ( MCF ) is an exotic radioactive reaction that can be used to produce energy in future reactors , but it costs extremely solid hydrogen energy as fuel . The MuCap project at TRIUMF has constructed and demonstrated a innovative system for producing ultra - pure hydrogen utilizing liquid helium cryogenic distillation preceded by two stages of molecular sieves .This system creates up to 1 liter per minute with fewer than 10 parts - per - trillion impurities . It will provide enough clean hydrogen energy to run the MuCap project until 2020 when the new generation of studies are expected to begin take data .A circulating hydrogen super - high purification system was developed and developed for the MuCap project at TRI - UMF . Liquid helium cryogenic distillation is combined with two stages of molecular sieve beds to achieve high purity levels needed for MCF research .The system provides up to one litre of purified fuel per moment with fewer than ten components - per - trillion impurity content .",
        "rewrite_text": "Muon-catalyzed fusion (MCF) is an unusual radioactive reaction that has the potential to generate energy for future reactors, but it requires extremely pure hydrogen as fuel. The MuCap project at TRIUMF has successfully developed an innovative system for producing ultra-pure hydrogen through a process that combines liquid helium cryogenic distillation with two stages of molecular sieves. This system can generate up to one liter of hydrogen per minute, with impurity levels below 10 parts per trillion. It will supply enough clean hydrogen to sustain the MuCap project until 2020 when a new wave of studies is expected to commence data collection. The highly purified hydrogen system was specifically designed for the MuCap project at TRIUMF, utilizing the aforementioned cryogenic and molecular sieve techniques to achieve the purity levels essential for MCF research.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 6.119912853410033,
        "rewrite-fast-z-score": 0.8819171036881969
    },
    {
        "original_text": "The GRI (Gamma Ray Imager) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study gamma-ray bursts, active galactic nuclei and other phenomena in high-energy astronomy.  It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun-Earth L2 orbit at 1 AU distance from Earth. Its main instrument consists of two coded mask telescopes observing simultaneously in the energy range 20 MeV - 300 GeV. Each telescope has a large field-of-view of 2 steradians and a spatial resolution better than 0.1 degrees. A third detector module provides additional information about the background radiation environment for each telescope. This article describes the design concept of this innovative instrument. Keywords: Astroparticle physics, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction:  The GRI (Gamma Ray...  more ",
        "watermark_text": "The GRI ( Gamma Ray Imager ) is an astrophysics space observatory proposed by the French Space Agency CNES and NASA to study alpha - ray clusters , active galactic nuclei and other processes in high - energy astronomy . It will be launched on a Soyuz rocket with a Fregat upper stage into a Sun - Earth L2 orbit at 1 AU distance from Earth .Its main object consists of two coded mask telescopes observing simultaneously in the power range 20 MeV - 300 GeV . Each observatory has a large field - of - view of 2 steradians and a spatial resolution best than 0 . 1 degrees .A third detector module presents additional information about the background radiation conditions for each telescope . This page describes the design concept of this innovative instrument .Keywords : Astroparticle science , High - Energy Astronomy , Instrumentation , Space Observatory . Introduction : The GRI ( Gamma Ray . . . more",
        "rewrite_text": "The Gamma Ray Imager (GRI) is a proposed astrophysics space observatory developed by the French Space Agency CNES and NASA. Its mission is to investigate alpha-ray clusters, active galactic nuclei, and other phenomena in high-energy astronomy. The observatory will be launched into a Sun-Earth L2 orbit, situated 1 AU from Earth, aboard a Soyuz rocket with a Fregat upper stage. The GRI features two coded mask telescopes that will operate concurrently across an energy range of 20 MeV to 300 GeV. Each telescope provides a wide field of view of 2 steradians and achieves a spatial resolution of better than 0.1 degrees. Additionally, a third detector module offers valuable information about the background radiation environment for both telescopes. This document outlines the design concept for this groundbreaking instrument. Keywords: Astroparticle science, High-Energy Astronomy, Instrumentation, Space Observatory. Introduction: The GRI (Gamma Ray. . . more)",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 4.008918628686366,
        "rewrite-fast-z-score": -0.13245323570650439
    },
    {
        "original_text": "We study the magnetization process, thermodynamic properties as well as magnetic susceptibility for spin-1/2 diamond chains with nearest-neighbor interactions under an external magnetic field using exact diagonalization method. We find that there is no spontaneous magnetization at zero temperature but the system exhibits finite magnetization when it is heated up to certain critical temperatures. The results show that the ground state has antiferromagnetic order while the excited states have ferrimagnetic order. In addition, we also investigate how the external magnetic field affects the magnetization process by calculating the magnetization curve M(H) at different temperatures T = 0.1J, 1.0J, 2.0J, 3.0J, 4.0J (J being the exchange interaction). It turns out that the magnetization increases rapidly with increasing H until reaching saturation value Msat at high fields. Moreover, our numerical calculations reveal that the Curie-Weiss constant decreases gradually with increasing J and finally vanishes at large values of J.",
        "watermark_text": "We research the magnetization process , thermodynamic properties as well as magnetic susceptibility for spin - 1 / 2 diamond chains with nearest - neighbor interactions under an external magnetic force using exact diagonalization technique . We see that there is no induced magnetization at zero temperature but the system displays minimal magnetization when it is cooled up to specified significant conditions .The results show that the ground state has antiferromagnetic order while the excited states have ferrimagnetic order . In addition , we also investigate how the external magnetic force influence the magnetization process by calculating the magnetization circle M ( H ) at different conditions T = 0 . 1J , 1 . 0J , 2 . 0J , 3 . 0J , 4 . 0J ( J being the transfer coupling ) .It turns out that the magnetization increases quickly with rising H until reaching saturation value Msat at high fields . Moreover , our numerical measurements reveal that the Curie - Weiss constant reduces gradually with expanding J and eventually vanishes at large values of J .",
        "rewrite_text": "We investigate the magnetization process, thermodynamic properties, and magnetic susceptibility of spin-1/2 diamond chains with nearest-neighbor interactions subject to an external magnetic field, employing the exact diagonalization technique. Our findings indicate that there is no induced magnetization at absolute zero; however, the system exhibits minimal magnetization when cooled under specific conditions. The results suggest that the ground state displays antiferromagnetic order, while the excited states exhibit ferrimagnetic order. Additionally, we explore the influence of the external magnetic field on the magnetization by calculating the magnetization circle M(H) at various temperatures: T = 0.1J, 1.0J, 2.0J, 3.0J, and 4.0J (with J representing the transfer coupling). We observe that the magnetization rapidly increases with rising H until it reaches the saturation value Msat at high fields. Furthermore, our numerical results indicate that the Curie-Weiss constant gradually decreases with increasing J and ultimately disappears at large values of J.",
        "ori-fast-z-score": 0.601929265428846,
        "water-fast-z-score": 5.578319375835658,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "The subgrid-scale stress tensor is an important quantity in large-eddy simulation (LES) that describes the effect of unresolved turbulent motions on resolved scales.  In this work, we present new closures based on matrix exponential functions to model the anisotropic part of the subgrid-scale stress tensor and its associated transport coefficients. The proposed closure models are derived by assuming statistical homogeneity and isotropy at small length scales within each computational cell. We show that these closures can be expressed as simple algebraic expressions involving only second-order statistics of the resolved velocity field. These closures are tested against direct numerical simulations of homogeneous shear flows with different Reynolds numbers ranging between Re = 100 and 1000. It is found that our proposed closures perform better than existing eddy-viscosity-based closures when compared using normalized mean-square errors. Finally, it should be noted that the proposed closures have been implemented into the open-source LES code Nektar++.",
        "watermark_text": "The subgrid - scale stress tensor is an important quantity in large - eddy simulation ( LES ) that describes the impact of unresolved turbulent movements on resolved scales . In this research , we present new closures based on matrix exponential functions to model the anisotropic part of the subgrid - scale stress tensor and its attendant transport coefficients .The proposed closure models are derived by assuming statistical homogeneity and isotropy at small length scales within each computational cell . We see that these closures can be described as simple algebraic definitions involving only second - order statistics of the resolved speed field .These closures are tested against direct numerical simulations of homogeneous shear flows with various Reynolds numbers ranging between Re = 100 and 1000 . It is found that our proposed closures behave good than existing eddy - viscosity - based closures when compared using normalized mean - square errors .Finally , it should be noted that the new closures have been implemented into the open - source LES code Nektar + + .",
        "rewrite_text": "The subgrid-scale stress tensor plays a crucial role in large-eddy simulation (LES) as it quantifies the influence of unresolved turbulent motions on the resolved scales. In this study, we introduce novel closure models based on matrix exponential functions to characterize the anisotropic component of the subgrid-scale stress tensor and its associated transport coefficients. These models are formulated under the assumption of statistical homogeneity and isotropy at small length scales within each computational cell. Notably, our closures can be expressed as straightforward algebraic formulations that rely solely on second-order statistics of the resolved velocity field. We validate these closures against direct numerical simulations of homogeneous shear flows at various Reynolds numbers ranging from Re = 100 to 1000. The results indicate that our proposed closures outperform existing eddy-viscosity-based models when assessed using normalized mean-square errors. Additionally, it is worth mentioning that these new closures have been integrated into the open-source LES software Nektar++.",
        "ori-fast-z-score": 1.0392304845413263,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "The vibrational infrared lifetime (VIL) of N2O dissolved in water is measured by using an optical parametric oscillator pumped with a Nd3+-doped YAG laser at 1064 nm and detecting the emission signal at 1270 nm, which corresponds to the fundamental vibration mode of N2O. The VIL value obtained for pure water is 1.6 ± 0.1 μs. This result agrees well with that reported previously. \n \n For solutions containing various concentrations of NaCl or KCl, the VIL values are found to be independent of salt concentration within experimental error. These results suggest that the vibrational relaxation process of N2O molecules in aqueous solution does not involve any specific interaction between N2O and ions such as Cl-. It should also be noted that the present measurement was performed under conditions where the solute-solvent interactions were negligible compared to those observed in concentrated solutions. Therefore, it can be concluded that the vibrational relaxation mechanism of N2O in dilute solutions is essentially identical to that in pure water.",
        "watermark_text": "The vibrational infrared lifetime ( VIL ) of N2O dissolved in water is measured by using an optical parametric oscillator pumped with a Nd3 + - doped YAG flashlight at 1064 nm and detecting the emission signal at 1270 nm , which belongs to the fundamental vibration mode of N2O . The VIL estimate achieved for pure water is 1 . 6 ± 0 . 1 μs .This result agrees well with that described earlier . For solutions containing various amounts of NaCl or KCl , the VIL values are found to be independent of salt concentration within experimental error .These data suggest that the vibrational relaxation process of N2O compounds in aqueous solution does not involve any specific interaction between N2O and atoms such as Cl - . It should additionally be mentioned that the present observation was done under environments where the solute - solvent interactions were negligible compared to those observed in concentrated solutions .Therefore , it can be determined that the vibrational relaxation mechanism of N2O in dilute solutions is essentially identical to that in pure water .",
        "rewrite_text": "The vibrational infrared lifetime (VIL) of N2O in water was measured using an optical parametric oscillator excited by a Nd3+-doped YAG laser at a wavelength of 1064 nm, with the emission signal detected at 1270 nm, corresponding to N2O's fundamental vibrational mode. The estimated VIL for pure water is 1.6 ± 0.1 μs, which is consistent with previous findings. For solutions with varying concentrations of NaCl or KCl, the VIL values remain stable, showing no dependence on salt concentration within the margin of experimental error. This implies that the vibrational relaxation process of N2O in aqueous solutions does not involve specific interactions with ions such as Cl-. Furthermore, it is important to note that these observations were made under conditions where solute-solvent interactions were minimal compared to those in more concentrated solutions. Consequently, it can be concluded that the vibrational relaxation mechanism of N2O in dilute solutions is essentially the same as in pure water.",
        "ori-fast-z-score": 1.4552137502179978,
        "water-fast-z-score": 5.176591682688076,
        "rewrite-fast-z-score": 1.0
    },
    {
        "original_text": "We propose a model that describes the spontaneous formation of neuronal assemblies in terms of an unsupervised Hebbian learning rule with synaptic depression. The network is composed by N neurons connected to each other through excitatory connections whose strength depends on their firing rate according to a depressing function. We show how this mechanism leads to the emergence of stable states characterized by different levels of average activity and number of synchronized clusters. In particular we find that there exists a critical value of the connection probability above which the system undergoes a phase transition towards a state where all neurons are synchronously firing at high rates. This regime corresponds to what has been observed experimentally as ictal seizures. \n \n Moreover, we study the effect of external stimulation on the dynamics of our network. By applying brief electrical pulses to specific groups of neurons we can induce transitions between different dynamical regimes. Finally, we discuss possible applications of these results to neurobiology.",
        "watermark_text": "We suggest a theory that describes the spontaneous development of neuronal assemblies in terms of an unsupervised Hebbian learning principle with synaptic depression . The network is composed by N cells connected to each other through excitatory connections whose strength depends on their fired rate due to a depressing function .We see how this mechanism leads to the emergence of stable states characterized by various rates of average activity and number of synchronized clusters . In particular we find that there exists a critical factor of the connection probability above which the system undergoes a phase shift towards a state where all neurons are synchronously firing at high levels .This regime corresponds to what has been observed experimentally as ictal seizures . Moreover , we study the impact of external stimulation on the dynamics of our system .By applying short electrical pulses to individual groups of neurons we can induce changes between various dynamical regimes . Finally , we explain possible use of these results to neurobiology .",
        "rewrite_text": "We propose a theoretical framework that explains the spontaneous formation of neuronal assemblies through an unsupervised Hebbian learning principle combined with synaptic depression. The network consists of N interconnected neurons that communicate via excitatory connections, with their strength modulated by the firing rate through a depressing function. This mechanism facilitates the emergence of stable states, each characterized by different average activity levels and the number of synchronized clusters. Notably, we identify a critical connection probability threshold, beyond which the system experiences a phase transition to a state of synchronous high-level firing across all neurons. This behavior is consistent with what is experimentally observed during ictal seizures. Additionally, we investigate how external stimulation affects the dynamics of our network. By delivering short electrical pulses to specific neuron groups, we can prompt transitions between different dynamical regimes. Lastly, we discuss potential applications of these findings in the field of neurobiology.",
        "ori-fast-z-score": 1.162476387438193,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "We present new Chandra observations of the brightest cluster galaxy (BCG) in Abell 3395 (z=0.084). The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV. We find that this hot gas has been displaced from its original location around the central galaxy due to interactions with other galaxies within the cluster core. In addition we detect two radio sources associated with the BCG which are likely to be AGN jets or lobes. Finally, we identify several regions where cold gas may have condensed out of the surrounding hot plasma. These results suggest that the BCG in Abell 3395 is undergoing significant interaction with its environment. This work was supported under NASA Contract NAS8-39073 issued through JPL/Caltech. The data presented herein were obtained at the Chandra Observatory, operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8-03060.",
        "watermark_text": "We present new Chandra observations of the brightest cluster galaxy ( BCG ) in Abell 3395 ( z = 0 . 084 ) . The BCG is surrounded by an extended halo with temperatures ranging between 1 keV to 5 keV .We see that this hot gas has been displaced from its initial site around the main galaxy owing to interactions with other stars within the cluster core . In addition we locate two radio sources related with the BCG which are likely to be AGN jets or lobes .Finally , we identify several regions where cold vapor possibly have condensed out of the nearby hot plasma . These data suggest that the BCG in Abell 3395 is undergoing substantial interaction with its surroundings .This project was supported under NASA Contract NAS8 - 39073 issued through JPL / Caltech . The data given herein were obtained at the Chandra Observatory , operated by the Smithsonian Astrophysical Observatory for and on behalf of NASA under contract NAS8 - 03060 .",
        "rewrite_text": "We present new observations from Chandra of the brightest cluster galaxy (BCG) in Abell 3395 (z = 0.084). The BCG is enveloped by an extended halo with temperatures ranging from 1 keV to 5 keV. Our findings indicate that this hot gas has been displaced from its original position around the main galaxy due to interactions with other stars in the cluster core. Furthermore, we have identified two radio sources associated with the BCG, which are likely to be AGN jets or lobes. Additionally, we pinpoint several areas where cold vapor may have condensed from the surrounding hot plasma. These observations suggest that the BCG in Abell 3395 is experiencing significant interactions with its environment. This project was funded under NASA Contract NAS8-39073 through JPL/Caltech. The data presented here were acquired at the Chandra Observatory, which is operated by the Smithsonian Astrophysical Observatory on behalf of NASA under contract NAS8-03060.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 0.5547001962252291
    },
    {
        "original_text": "We present an algorithm for inferring the structure and parameters of a dynamic Bayesian network (DBN) using only first-order or second-order conditional independence tests, which are computationally much cheaper than higher-order tests. The DBN is inferred by applying our method to each time point separately in a sliding window fashion. We show that this approach can be used to accurately reconstruct gene regulatory networks from high-throughput data such as microarray experiments. Our results on synthetic datasets demonstrate that we achieve similar accuracy compared to state-of-the-art methods while being orders of magnitude faster. Finally, we apply our method to two real-world biological datasets and compare it against several other approaches. Dynamic Bayesian Networks (DBNs) have been successfully applied to model complex systems whose states evolve over time. However, learning these models has proven challenging due to their large number of parameters and the computational complexity involved in estimating them. In particular, exact inference algorithms scale exponentially with respect to the size of the dataset and the number of variables considered. To overcome this problem, many approximate inference techniques have been proposed recently. These include variational approximations  1  , expectation propagation  2  , Monte Carlo sampling  3  , and message passing  4  . While these methods allow us to learn more accurate models at the cost of increased running times, they still require expensive computations involving all possible interactions between pairs of nodes. This makes them unsuitable when dealing with large-scale problems where the number of observations and/or variables becomes too large.",
        "watermark_text": "We present an algorithm for inferring the composition and parameters of a dynamic Bayesian network ( DBN ) using only first - order or second - order conditional freedom tests , which are computationally far inexpensive than higher - order tests . The DBN is inferred by using our technique to each time point separately in a sliding window fashion .We suggest that this methodology can be used to correctly reconstruct gene regulatory networks from high - throughput data such as microarray experiments . Our results on synthetic datasets prove that we accomplish similar accuracy relative to state - of - the - art methods while being orders of magnitude quicker .Finally , we apply our technique to two actual - time bio datasets and compare it against several other methods . Dynamic Bayesian Networks ( DBNs ) have been successfully application to model complex systems whose states evolve over time .However , learning these models has proven challenging due to their large number of parameters and the numerical difficulty required in estimating them . In particular , exact inference computers scale exponentially with regard to the height of the dataset and the quantity of parameter treated .To solve this question , various approximate inference strategies have been proposed lately . These include variational approximations 1 , expectation propagation 2 , Monte Carlo filtering 3 , and message passing 4 .While these algorithms provide us to develop more accurate descriptions at the cost of increased running times , they still use sophisticated computations requiring all possible behaviors between pairs of nodes . This creates them unsuitable when dealing with large - scale problems where the quantity of measurements and / or variables makes too huge .",
        "rewrite_text": "We introduce an algorithm designed to determine the composition and parameters of a dynamic Bayesian network (DBN) using only first-order or second-order conditional independence tests, which are significantly less computationally intensive than higher-order tests. Our approach infers the DBN by applying this technique to each time point individually in a sliding window manner. We propose that this methodology can effectively reconstruct gene regulatory networks from high-throughput data, such as microarray experiments. Results from experiments on synthetic datasets demonstrate that our method achieves comparable accuracy to state-of-the-art techniques while being substantially faster. Additionally, we apply our approach to two real-time biological datasets and evaluate it against several alternative methods. Dynamic Bayesian Networks (DBNs) have proven successful in modeling complex systems with time-evolving states. However, learning these models is challenging due to their numerous parameters and the complexity involved in estimating them. Specifically, exact inference computations scale exponentially with the dataset size and the number of parameters. To address this challenge, various approximate inference methods have recently been proposed, including variational approximations, expectation propagation, Monte Carlo filtering, and message passing. While these algorithms facilitate more accurate modeling, they tend to involve sophisticated computations that take into account all possible interactions between node pairs, making them impractical for large-scale problems with extensive measurements and variables.",
        "ori-fast-z-score": -1.4253932901995967,
        "water-fast-z-score": 7.247844507162112,
        "rewrite-fast-z-score": 1.8569533817705188
    },
    {
        "original_text": "We study the dynamics and emission properties of advection dominated accretion flows (ADAFs) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes. We find that, for reasonable values of parameters, such ADAFs can produce luminosities as high as those observed in quasars without violating any observational constraints on their mass inflow rates or temperatures at large radii. The main reason why our model works well is because it naturally produces an outflowing wind component whose kinetic energy flux greatly exceeds its thermal energy flux. This wind carries away most of the angular momentum so that the flow becomes nearly Keplerian near the black hole horizon. In addition, we show that this wind also provides enough pressure support against gravity to prevent the gas density from becoming too low there. Our results suggest that the winds produced by magnetized ADAF models may be responsible for driving powerful radio jets in active galactic nuclei.",
        "watermark_text": "We research the dynamics and emission behavior of advection dominated accretion currents ( ADAFs ) in which viscosity is caused by magnetic reconnections between field lines anchored to differentially rotating black holes . We see that , for reasonable values of parameters , such ADAFs can generate luminosities as great as those observed in quasars without violating any observational restrictions on their mass inflow rates or temperatures at large radii .The main explanation why our model works well is because it naturally produces an outflowing breeze component whose kinetic power flux considerably exceeds its thermal energy flux . This wind carries away most of the angular velocity so that the flow turns almost Keplerian near the dark hole horizon .In addition , we indicate that this wind additionally offers enough pressure support against gravity to prohibit the gas density from getting too low there . Our results propose that the storms produced by magnetized ADAF machines could be responsible for driving strong radio jets in active galactic nuclei .",
        "rewrite_text": "We investigate the dynamics and emission characteristics of advection-dominated accretion flows (ADAFs), where viscosity arises from magnetic reconnections among field lines linked to differentially rotating black holes. Our findings suggest that, under reasonable parameter values, these ADAFs can produce luminosities comparable to those seen in quasars while adhering to observational limits on mass inflow rates and temperatures at larger distances. The primary reason our model is effective is its ability to generate an outflowing breeze component, whose kinetic energy flux significantly surpasses its thermal energy flux. This wind effectively removes most of the angular momentum, allowing the flow to approach a nearly Keplerian state near the black hole's horizon. Additionally, we note that this wind provides sufficient pressure support against gravity, preventing the gas density from dropping excessively in that region. Our results imply that the turbulent activity generated by magnetized ADAF systems could be a key factor in producing powerful radio jets in active galactic nuclei.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 0.9878783399072131
    },
    {
        "original_text": "Magnetic separation is an important tool in biomedical research and clinical diagnostics, but it has been limited to macroscopic devices that are not suitable for point-of-care applications. Here we report on continuous magnetophoresis-based blood cell sorting using microfluidics. We demonstrate efficient separation of red blood cells (RBCs) from plasma by applying a magnetic field gradient across a microchannel containing RBCs suspended in buffer solution. The results show that our method can be used as a simple yet effective approach for separating different types of blood cells with high purity and efficiency. This work may have significant implications towards developing portable diagnostic tools based on microscale blood processing technologies. Magnetic separation techniques play an important role in many fields including medicine, biotechnology, environmental science, food industry etc.,  1  . However, most existing methods require bulky equipment which makes them unsuitable for use outside laboratory settings  2  .\nRecently there has been growing interest in miniaturizing these systems into lab-on-a-chip platforms  3  , where various functionalities such as sample preparation  4  , chemical analysis  5  , drug delivery  6  , and bioassays  7  could be integrated onto one single chip. In particular, magnetic separators have attracted much attention due to their simplicity, low cost, portability, and compatibility with other microfabricated components  8  . For example, several groups have demonstrated magnetic separation of biological samples inside microchannels  9  -  11  or on planar surfaces  12  -  14  . Despite this progress, however, current approaches still suffer from some limitations. First, they typically rely on batch-wise operation mode  15  , which limits throughput and requires large volumes of input samples  16  . Second, the majority of reported designs only allow for separation between two distinct populations  17  , while more complex mixtures involving multiple species cannot be processed simultaneously  18  . Third, the fabrication process usually involves complicated multi-step procedures  19  , making it difficult to integrate additional functions  20  . Finally, most previous studies were performed under static conditions  21  , which limit the flexibility of device design  22  .",
        "watermark_text": "Magnetic isolation is an important tool in biomedical research and medical diagnostics , but it has been limited to macroscopic devices that are not suitable for point - of - care applications . Here we study on rapid magnetophoresis - based blood cell sorting using microfluidics .We suggest efficient removal of red blood cells ( RBCs ) from fluid by using a magnetic current gradient across a microchannel containing RBCs held in buffer solution . The results show that our technique can be used as a simple yet effective methods for dividing different kinds of blood tissue with high purity and efficiency .This research could have considerable consequences towards developing portable diagnostic methods using on microscale blood extraction technologies . Magnetic isolation machines play an important role in multiple fields including medicine , biotechnology , environmental science , nutrition industry etc . , 1 .However , most existing techniques require bulky machinery which makes them unsuitable for use outside laboratory settings 2 . Recently there has been growing interest in miniaturizing these systems into lab - on - a - chip platforms 3 , where various functionalities such as specimen processing 4 , chemical analysis 5 , pharmaceutical production 6 , and bioassays 7 could be merged onto one single chip .In particular , magnetic separators have garnered considerable scrutiny due to their simplicity , low cost , portability , and compatibility with other microfabricated parts 8 . For instance , various groups have demonstrated magnetic separation of biological samples inside microchannels 9 - 11 or on planar materials 12 - 14 .Despite this progress , however , current approaches still suffer from some restrictions . First , they generally rely on batch - wise operation mode 15 , which reduces throughput and requires large quantities of input samples 16 .Second , the majority of reported prototypes only require for isolation between two separate populations 17 , while more sophisticated mixtures featuring multiple taxa unable be processed concurrently 18 . Third , the fabrication process usually includes complicated multi - phase techniques 19 , making it difficult to connect extra functions 20 .Finally , most prior studies were performed under static conditions 21 , which reduce the flexibility of device configuration 22 .",
        "rewrite_text": "Magnetic isolation is a vital technique in biomedical research and medical diagnostics; however, it has typically been limited to bulky macroscopic devices that are impractical for point-of-care applications. In this study, we explore rapid magnetophoresis-based blood cell sorting using microfluidics. We propose an effective method for removing red blood cells (RBCs) from a fluid by applying a magnetic current gradient across a microchannel filled with RBCs suspended in a buffer solution. Our findings demonstrate that this technique offers a straightforward yet efficient means of separating different types of blood tissue with high purity and effectiveness. This research could significantly impact the development of portable diagnostic methods through microscale blood extraction technologies. Magnetic isolation machines are essential in various fields, including medicine, biotechnology, environmental science, and the nutrition industry. However, most existing methods rely on large equipment, making them unsuitable for use outside laboratory settings. Recently, there has been a surge of interest in miniaturizing these systems into lab-on-a-chip platforms, combining functionalities such as specimen processing, chemical analysis, pharmaceutical production, and bioassays onto a single chip. Specifically, magnetic separators have gained attention for their simplicity, cost-effectiveness, portability, and compatibility with other microfabricated components. Several research groups have successfully demonstrated magnetic separation of biological samples within microchannels or on flat surfaces. Despite these advancements, current methods face certain limitations. They typically operate in a batch mode, which hinders throughput and necessitates larger sample sizes. Additionally, most existing prototypes are designed to isolate only two separate populations, making them inadequate for processing more complex mixtures with multiple taxa simultaneously. Furthermore, the fabrication of these devices often involves complex multi-phase techniques, which complicate the integration of additional functionalities. Lastly, many previous studies have been conducted under static conditions, limiting the flexibility of device configuration.",
        "ori-fast-z-score": 0.5261522196019802,
        "water-fast-z-score": 9.838699100999074,
        "rewrite-fast-z-score": 3.211367844950625
    },
    {
        "original_text": "We present new near-infrared (NIR) photometry and spectroscopy for the high-latitude star forming cloud, CG12. We find that this cloud is composed of two distinct populations; one population with ages <10 Myr and another older population with ages >100 Myr. Using N-body simulations we show that these clouds are likely to be remnants of tidally disrupted open clusters. This suggests that open cluster disruption may play an important role in the formation of isolated young stars at high latitudes. These results have implications on our understanding of how open clusters evolve into open clusters and open clusters into open clusters. Open clusters can also provide insight into the origin of open clusters themselves. In addition, open clusters can help us understand the evolution of open clusters as well as open clusters. Finally, open clusters can give us information about open clusters and open clusters. We use new infrared data obtained by the Wide Field Camera 3 (WFC3), Hubble Space Telescope (HST), and Spitzer Space Telescope (SST).",
        "watermark_text": "We report new near - infrared ( NIR ) photometry and spectroscopy for the high - latitude star producing storm , CG12 . We see that this cloud is composed of two separate populations ; one group with ages < 10 Myr and another older population with ages > 100 Myr .Using N - bodies simulations we find that these clouds are likely to be remnants of tidally disrupted open nuclei . This implies that open cluster disturbance may play an important role in the formation of isolated early stars at high latitudes .These data have consequences on our knowing of how open groups grow into open groups and open groups into open groups . Open clusters can also provide insight into the origin of open groups themselves .In addition , open groups can help us explain the evolution of open groups as well as open clusters . Finally , open groups can provide us information about open groups and open clusters .We use new infrared measurements obtained by the Wide Field Camera 3 ( WFC3 ) , Hubble Space Telescope ( HST ) , and Spitzer Space Telescope ( SST ) .",
        "rewrite_text": "We present new near-infrared (NIR) photometry and spectroscopy for the high-latitude star-forming region CG12. Our observations indicate that this cloud contains two distinct populations: one group with ages less than 10 million years and another older population exceeding 100 million years. Through N-body simulations, we suggest that these clouds are likely remnants of tidally disrupted open clusters. This finding implies that disturbances in open clusters may significantly contribute to the formation of isolated early stars at high latitudes. These insights enhance our understanding of how open clusters evolve into larger groups. Furthermore, open clusters can shed light on the origins of these groups and help us interpret their evolution. Finally, open clusters offer valuable information regarding the dynamics of open clusters and groups. Our analysis utilizes new infrared measurements obtained from the Wide Field Camera 3 (WFC3) aboard the Hubble Space Telescope (HST) and the Spitzer Space Telescope (SST).",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 0.22941573387056174
    },
    {
        "original_text": "We present evidence for alignment between the quadrupole, octopole and hexadecapole moments of the cosmic microwave background (CMB) temperature anisotropy on large angular scales as measured by the Wilkinson Microwave Anisotropy Probe (WMAP). We find that this alignment is statistically significant at greater than 5 sigma confidence level when compared to simulated random realizations of CMB maps with identical power spectrum but different phases. \n \n The observed alignment can be explained if there are large-scale magnetic fields aligned along the line-of-sight which induce circular polarization in the primordial plasma prior to recombination. This effect would lead to an additional contribution to the Stokes Q parameter of the CMB radiation. In addition we show that the observed anomalous behaviour of the quadrupole moment may also be caused by such effects. Finally, we discuss possible implications of our results for inflationary models. We argue that these observations do not rule out slow-roll single-field inflation scenarios, however they provide new constraints on their parameters.",
        "watermark_text": "We present evidence for alignment between the quadrupole , octopole and hexadecapole moments of the cosmic microwave background ( CMB ) temperature anisotropy on huge angular scales as measured by the Wilkinson Microwave Anisotropy Probe ( WMAP ) . We see that this alignment is statistically substantial at greater than 5 sigma confidence rate when compared to modeled random realizations of CMB images with identical power spectrum but different phases .The observed orientation can be understood if there are big - scale magnetic waves aligned along the line - of - view which induce circular polarization in the primordial plasma prior to recombination . This phenomenon might lead to an additional contribution to the Stokes Q dimension of the CMB radiation .In addition we prove that the seen anomalous dynamics of the quadrupole point could also be caused by such consequences . Finally , we explain possible possibilities of our findings for inflationary theories .We argue that these observations do not order out slow - roll single - field inflation scenarios , however they give novel constraints on their parameters .",
        "rewrite_text": "We present evidence of a significant alignment among the quadrupole, octopole, and hexadecapole moments of cosmic microwave background (CMB) temperature anisotropy over large angular scales, as observed by the Wilkinson Microwave Anisotropy Probe (WMAP). This alignment is statistically robust, with a confidence level exceeding 5 sigma when compared to modeled random realizations of CMB images that share the same power spectrum but differ in phases. The observed orientation may be explained by the presence of large-scale magnetic waves aligned along our line of sight, which could induce circular polarization in the primordial plasma prior to recombination. This effect might contribute an additional component to the Stokes Q parameter of the CMB radiation. Furthermore, we demonstrate that the anomalous dynamics observed in the quadrupole point may also result from these factors. Finally, we discuss the implications of our findings for inflationary theories. We argue that while these observations do not rule out slow-roll single-field inflation scenarios, they provide new constraints on their parameters.",
        "ori-fast-z-score": 0.3418817293789138,
        "water-fast-z-score": 6.495752858199363,
        "rewrite-fast-z-score": 0.11396057645963795
    },
    {
        "original_text": "We present the results of an analysis of Chandra observations of two nearby radio-quiet quasars, PG 1211+143 and Mrk 335 (z=0.034). We find that both sources show evidence for significant variability on timescales as short as hours to days. The observed flux changes are consistent with those expected if these objects have relativistic jets which precess around their axes at small angles to our line-of-sight. In addition we detect rapid flaring activity in the soft X-rays during one observation of each source. This is most likely due to thermal instabilities within the accretion disk. Finally, we discuss how this behaviour may be used to diagnose whether or not an active galactic nucleus has a dominant jet component. Keywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory",
        "watermark_text": "We present the results of an analysis of Chandra observations of two nearby radio - quiet quasars , PG 1211 + 143 and Mrk 335 ( z = 0 . 034 ) . We feel that both sources show evidence for significant variability on timescales as short as hours to days .The observed flux changes are compatible with those expected if these objects have relativistic jets which precess around their axes at small angles to our line - of - view . In addition we perceive rapid flaring activity in the soft X - rays during one observation of each source .This is most likely due to heat instabilities within the accretion disk . Finally , we explain how this behaviour may be used to diagnose whether or not an active galactic nucleus has a dominant jet element .Keywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory",
        "rewrite_text": "We present our analysis results from Chandra observations of two nearby radio-quiet quasars, PG 1211+143 and Mrk 335 (z = 0.034). Our findings indicate that both sources exhibit significant variability on timescales as brief as a few hours to days. The flux variations observed align with expectations for objects possessing relativistic jets that precess at slight angles relative to our line of sight. Additionally, we note instances of rapid flaring activity in the soft X-rays during separate observations of each quasar, likely resulting from thermal instabilities within the accretion disk. Lastly, we discuss how this behavior can be utilized to determine whether an active galactic nucleus displays a prominent jet component. \nKeywords: Active Galactic Nuclei, Quasars, Jet Precession, Flare Activity, Accretion Disk Instability, Chandra Observatory.",
        "ori-fast-z-score": 1.8203641092364127,
        "water-fast-z-score": 3.676955262170047,
        "rewrite-fast-z-score": -0.5547001962252291
    },
    {
        "original_text": "We present the results of our study on the evolution and nucleosynthesis in super-asymptotic giant branch (super-AGB) stars, which are evolved massive AGB stars with initial masses between 8 to 12 M⊙. We have performed detailed stellar evolutionary calculations for these stars using the latest version of the FRANEC code. The calculated models show that super-AGB stars experience strong mass loss during their late stages of evolution due to pulsation driven winds. These stars lose about 0.5 M⊙ before they enter into the white dwarf cooling phase. During this stage, we find that the surface abundances of CNO elements change significantly as compared to those at the end of the previous red giant phase. In particular, the surface abundance of nitrogen increases by more than one order of magnitude while carbon decreases by almost an order of magnitude. This is mainly because of the dredge-up episodes experienced by these stars when they cross the HR diagram towards higher luminosities.",
        "watermark_text": "We present the conclusion of our study on the evolution and nucleosynthesis in super - asymptotic giant branch ( super - AGB ) stars , which are evolved massive AGB stars with initial masses between 8 to 12 [UNK] . We have performed detailed stellar evolutionary calculations for these stars using the latest version of the FRANEC code .The adjusted models show that super - AGB stars experience heavy mass loss during their late stages of evolved due to pulsation driven winds . These galaxies lose about 0 . 5 [UNK] before they enter into the white dwarf cooling phase .During this phase , we find that the surface abundances of CNO groups change considerably as compared to those at the end of the previous red giant stage . In particular , the surface abundance of nitrogen changes by more than one order of magnitude while carbon decreases by almost an order of magnitude .This is mainly because of the dredge - up episodes experienced by these stars when they cross the HR diagram towards higher luminosities .",
        "rewrite_text": "We present the findings of our study on the evolution and nucleosynthesis in super-asymptotic giant branch (super-AGB) stars, which are evolved massive AGB stars with initial masses ranging from 8 to 12 solar masses. Utilizing the most recent version of the FRANEC code, we conducted detailed stellar evolutionary calculations for these stars. Our modified models indicate that super-AGB stars undergo significant mass loss during their later evolutionary stages due to pulsation-driven winds. These stars typically expel about 0.5 solar masses before transitioning into the white dwarf cooling phase. During this phase, we observe substantial changes in the surface abundances of carbon, nitrogen, and oxygen compared to their values at the end of the preceding red giant stage. Notably, the nitrogen abundance increases by over an order of magnitude, while carbon reduces by nearly an order of magnitude. This is primarily attributed to dredge-up events that occur as these stars move across the Hertzsprung-Russell diagram towards higher luminosities.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 2.9068883707497264,
        "rewrite-fast-z-score": -0.7683498199278324
    },
    {
        "original_text": "We present new results on the age, metallicity and alpha-element abundance for galactic globular clusters (GGCs) based on single stellar population models with different prescriptions for convection theory. We find that the ages derived by using the classical mixing-length theory are systematically younger than those obtained by assuming overshooting or semiconvection in the red giant branch phase. The difference between these two sets of ages is about 0.5 Gyr at most. For some metal-rich GGCs, we also found that their ages inferred from the classical mixing-length theory can be as young as 10 Gyr while they should have been older than 12 Gyr according to other methods. This discrepancy may result from the fact that the classical mixing-length theory cannot reproduce well the observed color-magnitude diagrams of such metal-rich GGCs. Our results show that there exists no significant correlation between the cluster s age and its metallicity.  These findings suggest that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging processes.",
        "watermark_text": "We report new data on the age , metallicity and alpha - component availability for galactic globular complexes ( GGCs ) based on single stellar community theories with various prescriptions for convection hypothesis . We see that the periods derived by using the classical mixing - length theory are systematically younger than those generated by assuming overshooting or semiconvection in the red dwarf branch period .The difference between these two sets of periods is about 0 . 5 Gyr at most . For some metal - rich GGCs , we also discovered that their ages inferred from the theoretical mixing - length theory can be as early as 10 Gyr while they should have been older than 12 Gyr according to other methods .This discrepancy may arise from the fact that the classical mixing - length theory cannot reproduce well the known color - magnitude diagrams of such metal - rich GGCs . Our results show that there exists no important relationship between the cluster s age and its metallicity .These studies propose that the formation history of GGCs might not be dominated by monolithic collapse but instead by hierarchical merging cycles .",
        "rewrite_text": "We present new findings on the age, metallicity, and alpha-component availability of Galactic Globular Clusters (GGCs), utilizing single stellar community theories with various convection hypotheses. Our analysis reveals that the ages derived from classical mixing-length theory are consistently younger than those obtained when overshooting or semiconvection is assumed for the periods of red dwarf branches, with a maximum difference of approximately 0.5 Gyr. For some metal-rich GGCs, we noted that the ages estimated using the theoretical mixing-length theory could be as young as 10 Gyr, even though other methods suggest they should be older than 12 Gyr. This inconsistency may be due to the classical mixing-length theory's inability to accurately reproduce the known color-magnitude diagrams of these metal-rich GGCs. Our findings indicate that there is no significant correlation between a cluster's age and its metallicity. These studies suggest that the formation history of GGCs may be influenced more by hierarchical merging cycles rather than by monolithic collapse.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.142956348249516,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "In this work, we consider an uplink MIMO system with Nt transmit antennas and one receive antenna at each user terminal (UT). We assume that all UTs are equipped with single RF chain which can be used to process signals received from multiple antennas simultaneously using linear processing techniques such as zero forcing or minimum mean square error criteria. The problem is formulated in terms of minimizing total power consumption subject to per-user rate constraints. It has been shown recently that the optimal solution to this problem requires joint optimization over both precoding matrices and power allocation coefficients. In order to solve this non-convex problem efficiently, we propose two algorithms based on successive convex approximation technique. First algorithm solves the original problem by iteratively optimizing over precoders and power allocations while second algorithm decouples these problems into separate subproblems and then solves them separately. Both proposed algorithms converge to the same point within few iterations. Simulation results show that our proposed algorithms outperform existing schemes significantly. \n \n Keywords: Multi-user MIMO systems, Zero-forcing beamforming, Minimum Mean Square Error Beamforming",
        "watermark_text": "In this research , we study an uplink MIMO network with Nt transmit antennas and one receive transmitter at each user terminal ( UT ) . We assume that all UTs are furnished with single RF chain which can be used to analyze signals obtained from multiple antennas separately using discrete processing tactics such as zero forcing or low mean square error criteria .The question is formulated in terms of minimizing gross electricity usage subject to per - customer cost constraints . It has been shown lately that the ideal solution to this question involves joint algorithms over both precoding functions and power distribution functions .In order to solve this non - convex problem efficiently , we propose two strategies using on successive convex algorithm method . First algorithm solves the original problem by iteratively optimizing over precoders and power allocations while first algorithm decouples these problems into independent subproblems and then solves them independently .Both proposed methods converge to the same position within few iterations . Simulation data demonstrate that our proposed methods outperform established algorithms considerably .Keywords : Multi - person MIMO systems , Zero - forcing beamforming , Minimum Mean Square Error Beamforming",
        "rewrite_text": "In this study, we investigate an uplink MIMO network consisting of Nt transmit antennas and a single receive antenna at each user terminal (UT). Each UT is equipped with a single RF chain, allowing for the analysis of signals from multiple antennas through discrete processing techniques like zero forcing or minimum mean square error criteria. The objective is framed as minimizing overall energy consumption subject to individual customer cost limitations. Recent findings indicate that the optimal solution requires joint algorithms for both precoding functions and power distribution functions. To efficiently address this non-convex problem, we propose two approaches based on the successive convex approximation method. The first algorithm iteratively optimizes precoders and power allocations, while the second approach breaks these issues down into independent subproblems that are solved separately. Both proposed methods converge rapidly to the same solution within a few iterations. Simulation results show that our methods significantly outperform existing algorithms. Keywords: Multi-user MIMO systems, Zero-forcing beamforming, Minimum Mean Square Error Beamforming.",
        "ori-fast-z-score": -2.494700264914546,
        "water-fast-z-score": 6.32831881684378,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "We present new results on the X-ray spectrum and variability properties of Mrk 509, one of the brightest Seyfert galaxies observed by XMM-Newton. We find that its 0.5-10 keV continuum is well described by an absorbed power law with Γ = 2.1 ± 0.2 (χ2/dof=111/101) plus a reflection component modeled as a PEXRAV model with R=0.7-1.0 and NH=10-23×1022 cm-2. The best-fit parameters are consistent within errors to those found previously using Chandra data alone. No significant spectral changes were detected between different epochs separated by several months apart. However, we do detect strong flux variations at all energies during our observation period. In particular, there was a factor of 3 increase in the hard band count rate over about 20 ks followed by a slower decay back towards the initial level. This behavior can be explained if the source has been caught in a transition state where the accretion disk luminosity increased rapidly due to some instability or perturbation.",
        "watermark_text": "We report new data on the X - ray spectrum and variability properties of Mrk 509 , one of the brightest Seyfert galaxies studied by XMM - Newton . We see that its 0 . 5 - 10 keV continuum is well described by an absorption power law with Γ = 2 . 1 ± 0 . 2 ( χ2 / dof = 111 / 101 ) plus a mirror element modeled as a PEXRAV model with R = 0 . 7 - 1 . 0 and NH = 10 - 23×1022 centimetres - 2 .The best - fitting values are compatible within errors to those identified previously used Chandra data alone . No meaningful spectral changes were detected between various epochs separated by many months separated .However , we do discover powerful flux variations at all energies during our observation term . In particular , there was a factor of 3 shift in the hard band count rate over about 20 ks followed by a slower decay forward towards the first rate .This phenomenon can be understood if the source has been caught in a transfer state where the accretion disk luminosity increased rapidly due to some distortion or perturbation .",
        "rewrite_text": "We present new findings on the X-ray spectrum and variability characteristics of Mrk 509, one of the brightest Seyfert galaxies examined by XMM-Newton. Our analysis shows that the 0.5 - 10 keV continuum is well-represented by an absorption power law with Γ = 2.1 ± 0.2 (χ²/dof = 111/101), along with a reflected component modeled using PEXRAV with R-values ranging from 0.7 to 1.0 and NH values between 10 - 23 × 10²² cm⁻². The best-fitting parameters align well with those previously identified in Chandra data within the margin of error. No significant spectral changes were observed across different epochs spaced several months apart. However, we did detect significant flux variations across all energies during our observation period. Notably, a threefold increase in the hard band count rate occurred over approximately 20 ks, followed by a gradual decline back toward the initial rate. This behavior suggests that the source may have been experiencing a transfer state, where the luminosity of the accretion disk increased rapidly due to some form of distortion or perturbation.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 6.527299120066193,
        "rewrite-fast-z-score": 0.808290376865476
    },
    {
        "original_text": "We report on X-ray timing observations of the pulsar candidate PSR J1930+1855 located at the center of the supernova remnant (SNR) G54.1+0. \n \n The source was discovered by Chandra and confirmed as a pulsar with XMM-Newton, but its spin period is not stable over time scales longer than one day. We performed two sets of pointed RXTE observations to study this behavior further. In both cases we found that the pulse frequency decreases smoothly during our observation runs. This trend can be described well using an exponential decay model for which we find characteristic timescales of 1.1 days and 0.7 days respectively. These values are consistent with those reported previously based on Chandra data alone. However, when comparing these results directly it should be noted that the uncertainties associated with the previous measurements were significantly larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "watermark_text": "We report on X - ray timing observations of the pulsar candidate PSR J1930 + 1855 centered at the center of the supernova remnant ( SNR ) G54 . 1 + 0 . The source was studied by Chandra and reported as a pulsar with XMM - Newton , but its spin time is not stable over time ranges shorter than one month .We conducted two sets of pointed RXTE observations to study this behavior further . In both cases we concluded that the heartbeat rate decreases slowly during our observation running .This trend can be described good using an exponential decay model for which we find typical timescales of 1 . 1 days and 0 . 7 weeks respectively . These quantities are compatible with those noted earlier based on Chandra data alone .However , when comparing these results directly it should be mentioned that the uncertainties involved with the previous tests were significantly larger thanks to the smaller signal - to - noise proportion attained with Chandra compared to RXTE .",
        "rewrite_text": "We present X-ray timing observations of the pulsar candidate PSR J1930+1855, located at the center of the supernova remnant (SNR) G54.1+0. Previous studies using Chandra and XMM-Newton identified this source as a pulsar; however, its spin period shows instability over timescales shorter than one month. To investigate this behavior more closely, we conducted two sets of targeted observations with RXTE. In both instances, we observed a gradual decrease in the pulsar's heartbeat rate throughout our observation period. This decline can be well modeled by an exponential decay, yielding characteristic timescales of approximately 1.1 days and 0.7 weeks, respectively. These findings are consistent with earlier observations from Chandra. However, it is important to note that the uncertainties in the previous analyses were considerably larger due to the lower signal-to-noise ratio achieved with Chandra compared to RXTE.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 6.719319439596787,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present new results on mass loss in carbon rich asymptotic giant branch (AGB) stars based on infrared photometry obtained with ISO-SWS, IRAS, MSX and Spitzer-IRS. We find that there is no correlation between the total luminosity or effective temperature of these objects and their mass-loss rates. The observed scatter may be explained by differences in chemical composition and/or pulsation properties among individual sources. In addition to this we show that the dust-to-gas ratio decreases towards higher temperatures for oxygen-rich as well as carbon-rich AGB stars. This indicates that the physical conditions at which dust forms are different in both types of evolved stars. Finally, we discuss how our findings can be used to improve current models describing the evolution of red giants. Keywords: Asymptotic Giant Branch Stars; Dust formation; Red Giants; Mass loss. 1 Introduction Carbon-rich asymptotic giant branch (AGB; hereafter Crich AGB) stars have been studied extensively over the past decades because they represent an important source class of interstellar matter. They lose large amounts of material through stellar winds driven by radiation pressure on dust grains formed in the outflowing gas. These winds play an essential role in shaping circumstellar envelopes around evolved stars and thus influence the appearance of planetary nebulae and proto-stellar disks surrounding young stellar objects. However, despite numerous observational studies it remains unclear what determines the amount of mass lost by Crich AGB stars. It has been suggested that the total luminosity L * , the effective temperature T eff , the surface gravity g, the metallicity Z, the pulsation period P, and the initial mass M ini might all affectṀ . For example, Wood et al. (1992) , van Loon et al. (1999), Olofsson et al. (2002a) , Knapp & Morris (1985) , and Winters et al. (1994) found evidence thatṀ increases with decreasing T eff .\nIn contrast, Groenewegen et al. (1998 ), De Beck et al. (2010 , and Ramstedt et al",
        "watermark_text": "We report new data on mass loss in carbon rich asymptotic giant line ( AGB ) galaxy based on infrared photometry obtained with ISO - SWS , IRAS , MSX and Spitzer - IRS . We see that there is no difference between the total luminosity or effective heat of these objects and their mass - loss rates .The observed scatter could be reason by differences in chemical composition and / or pulsation properties among different sources . In addition to this we find that the dust - to - gas ratio falls towards higher temperatures for oxygen - rich as well as carbon - rich AGB stars .This implies that the physical conditions at which dust occurs are distinct in both types of evolved stars . Finally , we talk how our findings can be used to improve current theories describing the evolution of red giants .Keywords : Asymptotic Giant Branch Stars ; Dust formation ; Red Giants ; Mass death . 1 Introduction Carbon - rich asymptotic giant line ( AGB ; hereafter Crich AGB ) stars have been studied frequently over the previous decades because they represent an important source type of interstellar matter .They lose enormous amounts of debris through stellar winds driven by radiation stress on dust grains created in the outflowing gas . These winds play an essential part in shaping circumstellar envelopes around evolved planets and therefore influence the appearance of planetary nebulae and proto - stellar disks surrounding young stellar bodies .However , despite several observational research it remains unsure what determines the quantity of mass losing by Crich AGB stars . It has been proposed that the total luminosity L * , the effective heat T eff , the surface gravity g , the metallicity Z , the pulsation period P , and the first weight M ini may all [UNK] .For instance , Wood et al . ( 1992 ) , van Loon et al .( 1999 ) , Olofsson et al . ( 2002a ) , Knapp & Morris ( 1985 ) , and Winters et al .( 1994 ) found evidence [UNK] increases with decreasing T eff . In contrast , Groenewegen et al .( 1998 ) , De Beck et al . ( 2010 , and Ramstedt et al",
        "rewrite_text": "We present new findings regarding mass loss in carbon-rich asymptotic giant branch (AGB) galaxies, using infrared photometry collected from ISO-SWS, IRAS, MSX, and Spitzer-IRS. Our analysis reveals no significant differences in total luminosity or effective temperature between these objects and their mass loss rates. The variability observed may be attributed to differences in chemical composition and/or pulsation characteristics among different sources. Furthermore, we discovered that the dust-to-gas ratio decreases at higher temperatures for both oxygen-rich and carbon-rich AGB stars, indicating that the physical conditions for dust formation differ between these two types of evolved stars. Finally, we discuss how our findings can enhance current theories regarding the evolution of red giants. \n\n**Keywords:** Asymptotic Giant Branch Stars; Dust Formation; Red Giants; Mass Loss.\n\n**1 Introduction**\n\nCarbon-rich asymptotic giant branch (AGB; henceforth referred to as Crich AGB) stars have been the focus of extensive study over the past several decades, as they are a crucial source of interstellar material. They expel vast quantities of material through stellar winds, which are driven by radiation pressure on dust grains formed in the outflowing gas. These winds play a vital role in shaping the circumstellar envelopes around evolved stars, thereby affecting the characteristics of planetary nebulae and proto-stellar disks surrounding young stars. Despite numerous observational studies, the factors that determine the mass loss in Crich AGB stars remain unclear. It has been suggested that factors such as total luminosity (L*), effective temperature (T_eff), surface gravity (g), metallicity (Z), pulsation period (P), and initial mass (M_ini) may all influence the mass loss. For example, studies by Wood et al. (1992), van Loon et al. (1999), Olofsson et al. (2002a), Knapp & Morris (1985), and Winters et al. (1994) have provided evidence that mass loss increases with decreasing T_eff. In contrast, findings from Groenewegen et al. (1998), De Beck et al. (2010), and Ramstedt et al. (2011) suggest different correlations.",
        "ori-fast-z-score": -0.9135002783911397,
        "water-fast-z-score": 6.124283575657523,
        "rewrite-fast-z-score": -1.3063945294843617
    },
    {
        "original_text": "In this work, we study end-to-end distortion in a buffered transmission system with fading channel and power control. We consider two different scenarios depending on whether or not there is feedback information available at transmitter side. In both cases, we derive closed-form expressions for average end-to-end distortion as well as its upper bound under Rayleigh flat-fading channels. The results show that when there exists no feedback link between source node and destination node, the optimal power allocation strategy depends only on the instantaneous channel gain. However, if there are some feedback links to provide the knowledge about the current channel state, then it can be shown that the optimal power allocation strategy also depends on the past channel states. Finally, numerical examples are provided to illustrate our theoretical analysis. Keywords: Power Control; Average End-to-End Distortion; Feedback Links; Flat-Fading Channels. 1 Introduction In recent years, wireless communications have become an important part of many applications such as mobile phones, personal digital assistants (PDAs), laptops etc., due to their advantages like mobility, flexibility and low cost  1  . However, one major problem associated with these systems is the limited bandwidth which leads to high bit error rate  2  .\nTo overcome this problem, various techniques including forward error correction coding  3  , diversity combining  4  , adaptive modulation  5  , power control  6  , unequal error protection  7  , joint source-channel coding  8  , etc., have been proposed by researchers. Among them, power control has attracted much attention because it allows us to adjust transmit power according to varying channel conditions so as to maximize the data rates while maintaining acceptable quality-of-service  9  . For example, in  10  -  12  , authors studied the effect of power control on outage probability and ergodic capacity respectively. On the other hand, in  13  -  15  , authors investigated the performance of power controlled communication systems using Shannon s mutual information criterion.",
        "watermark_text": "In this research , we study end - to - end distortion in a buffered transmission system with fading signal and power control . We consider two different scenarios depending on whether or not there is input data available at transmitter side .In both cases , we derive closed - form expressions for mean end - to - end distortion as well as its higher bound under Rayleigh flat - fading filters . The results show that when there exists no feedback link between origin node and destination node , the ideal power distribution scheme relies only on the instantaneous channel gain .However , if there are some feedback links to provide the knowledge about the present channel state , then it can be shown that the ideal power consumption scheme also relies on the previous channel states . Finally , numerical examples are provided to illustrate our theoretical analysis .Keywords : Power Control ; Average End - to - End Distortion ; Feedback Links ; Flat - Fading Channels . 1 Introduction In recent years , wireless communications have become an important element of several products such as wireless phones , personal digital assistants ( PDAs ) , laptops etc . , owing to their benefits like mobility , transparency and low cost 1 .However , one major difficulty associated with these systems is the limited bandwidth which results to large bit error rate 2 . To solve this situation , various methods namely back fault correction coding 3 , diversity matching 4 , adaptive modulation 5 , power control 6 , unequal error protection 7 , joint source - channel code 8 , etc . , have been proposed by researchers .Among them , power control has garnered considerable scrutiny because it allows us to adapt broadcast capacity according to different channel conditions so as to maximize the information rates while maintaining acceptable reliability - of - service 9 . For instance , in 10 - 12 , authors explored the impact of power control on outage likelihood and ergodic performance respectively .On the other hand , in 13 - 15 , authors explored the performance of power controlled communication devices using Shannon s mutual information criterion .",
        "rewrite_text": "In this study, we investigate end-to-end distortion in a buffered transmission system that experiences signal fading and involves power control. We explore two distinct scenarios based on the availability of input data at the transmitter. For both scenarios, we derive closed-form expressions for the mean end-to-end distortion as well as an upper bound under Rayleigh flat-fading conditions. Our findings indicate that in the absence of a feedback link between the source and destination nodes, the optimal power distribution scheme solely depends on the instantaneous channel gain. Conversely, when feedback links are present, supplying current channel state information, it becomes evident that the ideal power consumption strategy also takes into account previous channel states. To further clarify our theoretical analysis, we provide numerical examples. \n\nKeywords: Power Control; Average End-to-End Distortion; Feedback Links; Flat-Fading Channels.\n\n1. Introduction  \nIn recent years, wireless communication technologies have become integral to numerous devices, including wireless phones, personal digital assistants (PDAs), and laptops, due to their advantages such as mobility, transparency, and cost-effectiveness. However, a significant challenge these systems face is the limited bandwidth, which leads to a high bit error rate. To address this issue, researchers have proposed various methods, including forward error correction coding, diversity techniques, adaptive modulation, power control, unequal error protection, and joint source-channel coding. Among these, power control has attracted considerable attention because it enables the adjustment of broadcast capacity according to varying channel conditions, thereby maximizing information rates while ensuring an acceptable level of service reliability. For example, references 10-12 examined the effects of power control on outage probability and ergodic performance, while references 13-15 analyzed the performance of power-controlled communication devices using Shannon's mutual information criterion.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 8.439173726093731,
        "rewrite-fast-z-score": 2.6610007244439693
    },
    {
        "original_text": "We study the elasticity of thin films with periodic microstructure, which are constrained to lie on an underlying substrate. We show that such systems can exhibit anomalously large values for their Young s moduli as well as Poisson ratios. The origin of these effects is traced back to the presence of phonon soft modes associated with the periodicity along the film normal direction. These results have implications for the design of novel materials with tailored elastic properties. \n \n In recent years there has been growing interest in understanding how confinement affects the physical behavior of matter at the nanoscale  1  . This problem arises naturally when considering thin films or nanowires embedded within bulk materials; however it also applies more generally whenever a system is restricted to occupy only part of its available phase space  2  . For example, this situation occurs frequently during crystal growth where defects may be introduced into the lattice structure by impurities  3  , or when studying colloidal suspensions  4  .\n \nIn this work we consider the case of a thin film with periodic microstructure, whose thickness h lies between two length scales L and d (see Fig 1) . Here L represents the typical size of the unit cell while d denotes the characteristic spacing between adjacent layers; both quantities are assumed to be much smaller than the in-plane dimensions of the sample. Such structures arise commonly in nature, e.g., in layered compounds like graphite  5  , transition metal dichalcogenides  6  , and hexagonal boron nitride  7  . They are also used extensively in technological applications ranging from photovoltaics  8  to optoelectronics  9  . \n \n Figure 1: Schematic illustration of our model geometry. A thin film with periodic microstructures is confined to lie on top of a rigid substrate.",
        "watermark_text": "We test the elasticity of thin films with periodic microstructure , which are constrained to lay on an underlying substrate . We see that such complexes can exhibit anomalously high values for their Young s moduli as well as Poisson ratios .The origin of these phenomena is traced back to the presence of phonon soft modes associated with the periodicity along the film regular direction . These results have consequences for the creation of new materials with tailored elastic properties .In past decades there has been growing interest in understanding how confinement impacts the physical activity of matter at the nanoscale 1 . This problem arises readily when considering thin films or nanowires attached within bulk objects ; however it also applies more generally whenever a system is restricted to fill only part of its allocated phase space 2 .For instance , this situation occurs commonly during crystal growth where defects could be applied into the lattice structure by impurities 3 , or when examining colloidal suspensions 4 . In this research we define the case of a thin glass with periodic microstructure , whose thickness g lies between two width scales L and d ( see Fig 1 ) .Here L represents the typical size of the unit cell while d indicates the typical spacing between neighboring layers ; both quantities are expected to be much smaller than the in - plane dimensions of the sample . Such structures appear often in nature , e . g . , in layered compounds like graphite 5 , transition copper dichalcogenides 6 , and hexagonal boron nitride 7 .They are also used heavily in technological use ranging from photovoltaics 8 to optoelectronics 9 . Figure 1 : Schematic illustration of our model topology .A narrow film with periodic microstructures is confined to lying on top of a rigid coating .",
        "rewrite_text": "We investigate the elasticity of thin films featuring periodic microstructures that are constrained on an underlying substrate. Our findings indicate that these structures can demonstrate unexpectedly high values of Young's moduli and Poisson ratios. The source of these remarkable characteristics is linked to the existence of phonon soft modes related to the periodicity along the film's principal direction. These insights could pave the way for the development of new materials with customized elastic properties. In recent decades, there has been an increasing interest in understanding how confinement influences the physical behavior of matter at the nanoscale. This challenge becomes evident when examining thin films or nanowires integrated within bulk materials; however, it also broadly applies whenever a system is limited to occupying only part of its designated phase space. For example, this scenario is frequently encountered during crystal growth, where defects can be introduced into the lattice structure by impurities, or when analyzing colloidal suspensions. In this study, we focus on a thin glass with a periodic microstructure, characterized by a thickness \\( g \\) that falls between two scales, \\( L \\) and \\( d \\) (see Fig. 1). Here, \\( L \\) represents the typical size of the unit cell, while \\( d \\) indicates the average spacing between adjacent layers; both values are anticipated to be significantly smaller than the in-plane dimensions of the sample. Such structures are commonly found in nature, for instance, in layered compounds like graphite, transition copper dichalcogenides, and hexagonal boron nitride. They also have extensive applications in technology, ranging from photovoltaics to optoelectronics. Figure 1 illustrates a schematic representation of our model, showing a narrow film with periodic microstructures positioned on a rigid substrate.",
        "ori-fast-z-score": -1.3926212476455828,
        "water-fast-z-score": 6.614950926316518,
        "rewrite-fast-z-score": 1.0690449676496976
    },
    {
        "original_text": "We present optical (BVRI), near infrared (JHK) and mid-infrared (IRAC 3.6-8.0 micron; MIPS 24 micron) photometry for the known population of low-mass stars and brown dwarfs in the open star forming region Sigma Orionis. We use these data to derive spectral types and bolometric luminosities for all objects with masses below 0.1 solar masses. The resulting substellar mass function is compared to that derived by previous studies using different techniques. Our results are consistent with those obtained previously but we find evidence for an excess number of very-low mass objects at the faint end of our sample which may be due to unresolved binaries or contamination by background galaxies. This work was supported by NASA grant NAG5-12942. We thank J. Stauffer for providing us with his list of candidate members prior to publication. \n \n Keywords: Open clusters",
        "watermark_text": "We use visual ( BVRI ) , near infrared ( JHK ) and mid - infrared ( IRAC 3 . 6 - 8 . 0 micron ; MIPS 24 micron ) photometry for the known community of lowest - weight stars and dark dwarfs in the open star producing zone Sigma Orionis . We use these information to derive spectral classes and bolometric luminosities for all bodies with masses below 0 . 1 solar masses .The resulting substellar mass distribution is compared to that derived by earlier surveys using different methods . Our results are compatible with those generated previously but we find proof for an excess amount of very - low weight objects at the faint ending of our sample which may be due to unresolved binaries or exposure by background galaxies .This project was supported by NASA loan NAG5 - 12942 . We praise J . Stauffer for providing us with his list of candidate participants previous to publication .Keywords : Open clusters",
        "rewrite_text": "We utilize photometry across various wavelengths—visual (BVRI), near-infrared (JHK), and mid-infrared (IRAC at 3.6-8.0 microns; MIPS at 24 microns)—to study the known community of low-mass stars and brown dwarfs within the star-forming region of Sigma Orionis. This data allows us to determine spectral classes and bolometric luminosities for all objects with masses below 0.1 solar masses. We compare the resulting substellar mass distribution with findings from previous surveys that employed different methodologies. Our results align with earlier studies; however, we uncover evidence of an excess of very low-mass objects at the faint end of our sample, which may be attributed to unresolved binaries or contamination from background galaxies. This research was supported by NASA grant NAG5-12942. We extend our gratitude to J. Stauffer for sharing his list of candidate participants prior to publication. Keywords: Open clusters.",
        "ori-fast-z-score": -1.75,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "The book is intended for students who have completed the first year course in control theory and are interested to learn more about discrete-time systems, digital controllers and computer-based control techniques. The text covers topics such as state space representation, stability analysis, optimal control design, robustness issues, model predictive control (MPC), fuzzy logic based control etc., with an emphasis on practical applications. It also includes several examples that illustrate key concepts discussed throughout the chapters. \n \n This textbook provides a comprehensive coverage of basic principles underlying various aspects of modern control engineering. In addition to theoretical foundations, it presents numerous numerical examples illustrating important concepts introduced along the way. A number of exercises at the end of each chapter help readers reinforce their understanding of material presented earlier. Finally, there are two appendices containing additional information useful for further study or research work. \nThis book can be used by graduate students studying advanced courses in control theory, as well as researchers working in this area.",
        "watermark_text": "The text is intended for students who have completed the first year course in control theory and are concerned to teach more about discrete - time systems , digital controllers and computer - based control methods . The text encompasses topics such as state space representation , stability analysis , ideal control design , robustness issues , model predictive control ( MPC ) , fuzzy logic based control etc . , with an emphasis on technical applications .It additionally contains several examples that highlight key concepts discussed throughout the pages . This treatise provides a comprehensive treatment of fundamental principles governing several elements of modern control engineering .In addition to theoretical foundations , it presents several mathematical models illustrating major concepts adopted along the way . A variety of tests at the end of each volume help viewers reinforce their understanding of content presented earlier .Finally , there are two appendices containing extra data helpful for further study or research studies . This book can be used by graduate candidates pursuing advanced degrees in control theory , as well as scientists studying in this area .",
        "rewrite_text": "This text is designed for students who have completed an introductory course in control theory and wish to delve deeper into discrete-time systems, digital controllers, and computer-based control methods. It covers a range of topics, including state space representation, stability analysis, ideal control design, robustness issues, model predictive control (MPC), and fuzzy logic-based control, all with a focus on practical applications. The book includes numerous examples that illustrate key concepts discussed throughout. It provides a thorough exploration of the foundational principles that underpin various aspects of modern control engineering. In addition to the theoretical groundwork, it presents multiple mathematical models that clarify the main concepts introduced along the way. Each volume concludes with a variety of tests designed to reinforce the reader's understanding of the material. Finally, two appendices offer additional information that may be beneficial for further study or research. This book can be utilized by graduate students pursuing advanced degrees in control theory, as well as by researchers in the field.",
        "ori-fast-z-score": 0.4879500364742666,
        "water-fast-z-score": 6.993258208972302,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "We study the magnetic field structure and evolution of neutron stars with type-II superconductivity, taking into account the effects of superfluidity on the stellar interior. We find that toroidal fields can be generated by differential rotation between normal matter and superfluids inside the star. The resulting toroidal field is strong enough to affect the spin-down rate of pulsars significantly. In particular, we show that it may explain why some young pulsars have much slower spindown rates than expected for their ages. \n \n Introduction \n \n Pulsar magnetospheres are believed to contain large-scale poloidal and toroidal magnetic fields (e.g., Goldreich & Julian 1969; Ruderman 1974). These fields play an important role in determining the observed properties of pulsar emission such as pulse profiles and polarization characteristics (e.g., Melrose 1995), but also influence the long-term evolution of pulsar periods through electromagnetic torques exerted at the light cylinder (e.g., Spitkovsky 2006). \n \n It has been suggested that toroidal fields could be produced during the formation process of neutron stars via dynamo action driven by convection or differential rotation between different components within the core region (Thompson & Duncan 1993) . However, recent studies suggest that this mechanism cannot generate sufficiently large toroidal fields to match observations (Heger et al. 2005 ). An alternative possibility is that toroidal fields are created by winding up poloidal fields due to rapid rotation of the crust (Braithwaite 2009) or by differential rotation between normal fluid and superfluid components in the interior of the star (Srinivasan et al. 1991; Srinivasan 1991a ) . \n \n In this work, we investigate how toroidal fields evolve over time under various physical conditions using numerical simulations. Our results indicate that toroidal fields can grow rapidly if there exists significant differential rotation between normal matter",
        "watermark_text": "We research the magnetic force composition and evolution of neutron galaxies with type - II superconductivity , using into consideration the effects of superfluidity on the stellar interior . We see that toroidal fields can be formed by differential rotation between normal matter and superfluids inside the star .The resulting toroidal field is strong enough to affect the spin - down frequency of pulsars significantly . In particular , we prove that it could explain why some young pulsars have far slower spindown rates than expected for their ages .Introduction Pulsar magnetospheres are considered to contain big - scale poloidal and toroidal magnetic waves ( e . g . , Goldreich & Julian 1969 ; Ruderman 1974 ) . These fields drive an important role in determining the known characteristics of pulsar emission such as pulse profiles and polarization properties ( e . g . , Melrose 1995 ) , but also affect the long - term evolve of pulsar periods through electrical torques exerted at the light cylinder ( e . g . , Spitkovsky 2006 ) .It has been proposed that toroidal fields could be formed during the formation transition of neutron stars via dynamo action driven by convection or differential rotation between various components within the core region ( Thompson & Duncan 1993 ) . However , recent studies imply that this mechanism unable generate sufficiently huge toroidal fields to match observations ( Heger et al .2005 ) . An alternative possibility is that toroidal fields are created by winding up poloidal fields thanks to rapid rotation of the crust ( Braithwaite 2009 ) or by differential rotation between normal liquid and superfluid elements in the interior of the star ( Srinivasan et al .1991 ; Srinivasan 1991a ) . In this research , we investigate how toroidal fields emerge over time under various physical conditions utilizing numerical simulations .Our results show that toroidal fields can develop rapidly if there exists significant differential rotation between normal matter",
        "rewrite_text": "We investigate the composition and evolution of magnetic forces in neutron stars exhibiting type-II superconductivity, taking into account the influence of superfluidity on the stellar interior. Our findings indicate that toroidal magnetic fields can arise from the differential rotation between normal matter and superfluid components within the star. These resulting toroidal fields possess sufficient strength to significantly impact the spin-down rates of pulsars. Notably, we demonstrate that this phenomenon may account for the unexpectedly slow spin-down rates observed in some young pulsars relative to their ages. \n\nIn the introduction, we note that pulsar magnetospheres are believed to be filled with large-scale poloidal and toroidal magnetic waves (e.g., Goldreich & Julian 1969; Ruderman 1974). These magnetic fields play a crucial role in shaping the characteristics of pulsar emissions, including pulse profiles and polarization features (e.g., Melrose 1995), and also influence long-term changes in pulsar periods due to electrical torques acting at the light cylinder (e.g., Spitkovsky 2006). \n\nPrevious proposals suggested that toroidal fields might form during the neutron star formation transition through dynamo action, driven by convection or differential rotation within the core (Thompson & Duncan 1993). However, recent studies indicate that this mechanism is insufficient to generate the large toroidal fields needed to align with observations (Heger et al. 2005). An alternative explanation posits that toroidal fields could be generated by winding poloidal fields due to rapid crustal rotation (Braithwaite 2009) or through differential rotation between normal liquid and superfluid constituents in the star's interior (Srinivasan et al. 1991; Srinivasan 1991a). In our research, we explore the emergence of toroidal fields over time under varying physical conditions using numerical simulations. Our results indicate that significant differential rotation between normal matter can lead to the rapid development of toroidal fields.",
        "ori-fast-z-score": 0.50709255283711,
        "water-fast-z-score": 6.920297027505764,
        "rewrite-fast-z-score": 1.7962924780409972
    },
    {
        "original_text": "The Wang-Landau (WL) algorithm is used in this work as an efficient tool to explore the low energy conformation space of proteins and predict their native structures. The WL method has been successfully applied to many systems, such as spin glasses, lattice gas models, protein folding problems etc., but it was never tested on realistic biomolecular systems before. In our study we have performed extensive tests with different types of proteins using both coarse-grained and all-atom force fields. We show that the WL sampling can be efficiently combined with standard Monte Carlo simulations or Molecular Dynamics runs at high temperatures. This allows us to obtain accurate estimates of free energies even when only relatively short trajectories are available. Our results demonstrate that the WL approach provides very good agreement between predicted and experimental structures. Finally, we discuss possible applications of the proposed methodology to other complex systems where the exploration of phase spaces plays important role.",
        "watermark_text": "The Wang - Landau ( WL ) algorithm is utilized in this research as an efficient tool to examine the small power conformation space of proteins and predict their ancestral properties . The WL method has been successfully application to many systems , such as spin glasses , lattice gas models , protein folding problems etc . , but it was none proven on real biomolecular systems before .In our research we have done extensive experiments with various types of proteins using both fine - grained and all - atom pressure waves . We see that the WL analysis can be easily combined with typical Monte Carlo simulations or Molecular Dynamics runs at high temperatures .This enables us to obtain precise estimates of free energies even when only relatively short trajectories are available . Our results show that the WL approach offers very high agreement between anticipated and observation properties .Finally , we discuss possible applied of the suggested methodology to other complex systems where the exploration of phase spaces serves crucial role .",
        "rewrite_text": "The Wang-Landau (WL) algorithm is employed in this study as an effective tool for exploring the limited conformational space of proteins and predicting their ancestral characteristics. While the WL method has been successfully applied to various systems, including spin glasses, lattice gas models, and protein folding challenges, it has not been rigorously tested on actual biomolecular systems prior to this research. We conducted extensive experiments on a variety of proteins, utilizing both fine-grained and all-atom pressure waves. Our findings demonstrate that WL analysis can be seamlessly integrated with conventional Monte Carlo simulations or Molecular Dynamics simulations at elevated temperatures. This integration allows for accurate estimates of free energies, even when working with relatively short trajectories. Our results indicate that the WL approach yields a strong agreement between predicted and observed properties. Finally, we explore potential applications of this methodology to other complex systems where phase space exploration plays a vital role.",
        "ori-fast-z-score": -0.8728715609439696,
        "water-fast-z-score": 6.616378963469013,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "The free energy of activation (ΔG*) is calculated for the comorosan effect, which describes the formation of an intermediate state in the reaction between carbon dioxide and water to form carbonate ions.  The ΔG* value obtained by this method is compared with that determined by other methods such as calorimetry or electrochemistry. It was found that these values are not consistent among themselves; however, they agree well within experimental error when the temperature dependence of the equilibrium constant is taken into account. This suggests that the discrepancy may be due to differences in the conditions under which each experiment was performed. In addition, it has been shown that the ΔG* value depends on the nature of the solvent used in the experiments. Finally, we have proposed a mechanism for the comorosan process based on our results. The free energy of activation (∆G*) is calculated using the Arrhenius equation for the comorosan reaction, which describes the formation",
        "watermark_text": "The free energy of activation ( ΔG * ) is calculated for the comorosan effect , which explains the formation of an intermediate state in the process between carbon dioxide and water to form carbonate compounds . The ΔG * value obtained by this process is compared with that determined by other methods such as calorimetry or electrochemistry .It was shown that these estimates are not consistent among themselves ; however , they accord well within experimental error when the temperature dependence of the equilibrium coefficient is taken into consideration . This implies that the discrepancy may be due to differences in the conditions under which each experiment was done .In addition , it has been shown that the ΔG * value depends on the nature of the solvent used in the experiments . Finally , we have proposed a system for the comorosan process depending on our findings .The free energy of activation ( [UNK] * ) is calculated using the Arrhenius equation for the comorosan reaction , which describes the formation",
        "rewrite_text": "The free energy of activation (ΔG*) for the comorosan effect has been calculated, shedding light on the formation of an intermediate state in the reaction between carbon dioxide and water to produce carbonate compounds. The ΔG* value derived from this process has been compared to those obtained using other methods, such as calorimetry and electrochemistry. It was found that these estimates are inconsistent with each other; however, they align closely within experimental error when accounting for the temperature dependence of the equilibrium constant. This suggests that the discrepancies may stem from variations in the experimental conditions. Additionally, it has been demonstrated that the ΔG* value is influenced by the type of solvent employed in the experiments. Based on these findings, we propose a system for the comorosan process, using the Arrhenius equation to calculate the free energy of activation for this reaction, which describes the formation of intermediate states.",
        "ori-fast-z-score": 2.25,
        "water-fast-z-score": 5.75,
        "rewrite-fast-z-score": 0.3779644730092272
    },
    {
        "original_text": "In this work, we present the design and analysis of space-time block codes (STBC) based on extended clifford algebras. The proposed STBC is constructed by using an orthogonal basis for the underlying algebra. We show that our code has full diversity order with respect to quasi-static Rayleigh fading channels. Furthermore, it achieves maximum coding gain over all other known algebraic STBCs in terms of minimum determinant criterion. Finally, we provide simulation results which demonstrate the performance gains achieved by the proposed scheme compared to existing schemes. Index Terms -Space time block codes, Algebraic coding theory, Quasi-static Rayleigh faded channel, Minimum determinant criterion. I. INTRODUCTIO N Space Time Block Codes (STBC), introduced by Alamouti  1  , are widely used in wireless communications due to their simple structure and high data rate  2  . In recent years there have been many efforts devoted towards designing new classes of STBCs  3  -  8  .\nThe main goal behind these designs was to achieve higher spectral efficiency while maintaining good error performances under different transmission conditions such as multipath propagation or frequency selective fading  9  . However, most of the existing works focus only on achieving better error performances without considering the effect of hardware limitations  10  . This motivates us to develop a class of STBCs which can be implemented efficiently at low complexity  11  .\nRecently, several authors  12  -  14  have shown that some well-known families of finite fields like Galois field GF(q)  15  , Finite Ring  16  , Quaternion  17  etc., can also be represented by certain types of non-commutative rings called Clifford algebras  18  . These representations allow one to construct various signal constellations  19  , modulation techniques  20  , and communication systems  21  . Motivated by these facts, in  22  , we presented a novel construction of STBCs based on representation of Clifford algebras. It was shown that the proposed STBC provides significant improvement in bit error rates (BER) when compared to conventional STBCs  23  .",
        "watermark_text": "In this study , we present the development and evaluation of space - time block codes ( STBC ) based on extended clifford algebras . The proposed STBC is built by using an orthogonal basis for the underlying algebra .We see that our code has full diversity order with regard to quasi - static Rayleigh fading sources . Furthermore , it achieves greatest code gain over all other known mathematical STBCs in terms of least determinant requirement .Finally , we provide simulation data which demonstrate the performance increases attained by the suggested system compared to existing plans . Index Terms - Space time block systems , Algebraic coding theory , Quasi - static Rayleigh faded signal , Minimum determinant requirement .I . INTRODUCTIO N Space Time Block Codes ( STBC ) , invented by Alamouti 1 , are widely useful in mobile transmissions due to their simple shape and large data rate 2 . In recent history there have been many efforts devoted towards creating new classes of STBCs 3 - 8 .The main goal behind these designs was to achieve greater frequency reliability while maintaining good error performances under various transmission circumstances such as multipath propagation or bandwidth selective fading 9 . However , most of the newer works concentrate only on achieving better error performances without assessing the impact of hardware constraints 10 .This motivates us to develop a class of STBCs which can be implemented efficiently at low complexity 11 . Recently , various scientists 12 - 14 have shown that some well - famous families of finite fields like Galois field GF ( q ) 15 , Finite Ring 16 , Quaternion 17 etc . , can also be described by certain types of non - commutative rings called Clifford algebras 18 .These representations allow one to build diverse signal constellations 19 , modulation technology 20 , and communication devices 21 . Motivated by these facts , in 22 , we presented a new construction of STBCs based on representation of Clifford algebras .It was shown that the suggested STBC offers substantial improvement in bit error speeds ( BER ) when compared to conventional STBCs 23 .",
        "rewrite_text": "In this study, we introduce the development and evaluation of space-time block codes (STBC) derived from extended Clifford algebras. The proposed STBC is constructed using an orthogonal basis related to the underlying algebra. Our analysis reveals that this code achieves full diversity order in the context of quasi-static Rayleigh fading channels, along with providing the highest code gain compared to other mathematical STBCs, particularly in terms of the minimum determinant requirement. We also present simulation results that illustrate the performance improvements of our proposed system relative to existing approaches.\n\nIndex Terms: Space-time block codes, Algebraic coding theory, Quasi-static Rayleigh fading, Minimum determinant requirement.\n\nI. INTRODUCTION\nSpace-Time Block Codes (STBC), first introduced by Alamouti, are highly beneficial for mobile transmissions due to their straightforward structure and high data rates. Recent efforts have focused on the creation of new classes of STBCs aimed at enhancing frequency reliability and ensuring good error performance across various transmission scenarios, such as multipath propagation and bandwidth selective fading. However, many of these newer designs emphasize better error performance without addressing the implications of hardware constraints. This gap motivates the development of a class of STBCs that can be implemented with low complexity. Recent research has demonstrated that several well-known families of finite fields, such as Galois fields, finite rings, and quaternions, can be expressed through certain types of non-commutative rings known as Clifford algebras. These representations facilitate the construction of varied signal constellations, modulation techniques, and communication systems. Building on this knowledge, we introduced a novel construction of STBCs based on Clifford algebra representations. This approach showed significant improvements in bit error rates (BER) when compared to traditional STBCs.",
        "ori-fast-z-score": -0.16012815380508713,
        "water-fast-z-score": 8.273159087695738,
        "rewrite-fast-z-score": -0.16329931618554522
    },
    {
        "original_text": "The aim of this talk is to present some recent results on the relation between integrability in quantum field theory, statistical mechanics or mathematical physics and the existence of special geometric structures (complex algebraic curves) which are associated with these models. \nWe will discuss how such geometries can be used for solving exactly certain physical problems by using techniques coming from algebraic geometry like Riemann surfaces, theta functions etc.. We will also explain why it seems that many interesting integrable models have an underlying structure of a Riemann surface. Finally we will give examples where this connection has been made explicit. The talk will include several new results obtained recently by the author together with his collaborators. This work was partially supported by the DFG under grant SFB/TR9. Integrable systems play an important role in various branches of mathematics as well as theoretical physics. In particular they appear naturally when studying solvable lattice models in statistical mechanics or quantum field theories. It turns out that there exists a deep relationship between integrability and the presence of special geometric structures called algebraic curves. These curves provide powerful tools for solving exactly certain physical questions via methods from algebraic geometry like Riemann Surfaces, Theta Functions,...",
        "watermark_text": "The goal of this talk is to provide some latest findings on the relation between integrability in quantum field theory , statistical mechanics or computational physics and the existence of unique geometric objects ( complex algebraic shapes ) which are identified with these models . We will explore how such geometries can be used for solving exactly certain physical problems by using techniques come from algebraic topology like Riemann surfaces , theta functions etc . . We will also explain why it appears that several interesting integrable models have an underlying composition of a Riemann surface .Finally we will provide descriptions where this link has been made explicit . The talk will include several current conclusions acquired previously by the writer together with his collaborators .This research was partially backed by the DFG under grant SFB / TR9 . Integrable systems play an important role in different areas of math as well as conceptual science .In particular they appear naturally when examining solvable lattice models in statistical mechanics or quantum field theories . It turns out that there exists a profound connection between integrability and the presence of unique geometric objects called algebraic curves .These curves provide potent tools for solving exactly certain physical problems via tools from algebraic topology like Riemann Surfaces , Theta Functions , . . .",
        "rewrite_text": "The objective of this presentation is to share recent discoveries regarding the connection between integrability in quantum field theory, statistical mechanics, and computational physics, and the existence of unique geometric entities—specifically complex algebraic shapes—that are associated with these models. We will discuss how these geometric structures can be leveraged to solve specific physical problems exactly, utilizing techniques from algebraic topology, such as Riemann surfaces and theta functions. Additionally, we will clarify the observation that several notable integrable models are fundamentally composed of Riemann surfaces. Finally, we will provide explicit examples where this connection has been clearly demonstrated. The talk will incorporate several current findings previously reported by the speaker and his collaborators. This research was supported in part by the DFG under grant SFB / TR9. Integrable systems are significant in various mathematical domains and theoretical sciences, as they naturally arise when analyzing solvable lattice models in statistical mechanics or quantum field theories. Notably, there is a deep relationship between integrability and the existence of unique geometric objects known as algebraic curves, which are essential tools for accurately solving certain physical issues using concepts from algebraic topology, including Riemann surfaces and theta functions.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 6.543303050815759,
        "rewrite-fast-z-score": 0.7770286898858113
    },
    {
        "original_text": "We present an overview on supersymmetric grand unified theories (SUSY-GUT), their connection to neutrino masses via seesaw mechanisms as well as dark matter candidates in these models. We discuss how GUT scale physics can be probed at future colliders such as LHC or ILC. Finally we give some examples for specific realizations within SO(10) and E6 gauge groups. Supersymmetry is one of the most promising extensions beyond the Standard Model which addresses many open questions like the hierarchy problem between electroweak and Planck scales, unification of forces etc.. In addition it provides a natural candidate for cold dark matter -the lightest neutralino. The minimal supersymmetric standard model (MSSM) has been studied extensively over the last two decades but suffers from several shortcomings. One of them is that the MSSM does not provide any explanation why there are three generations of quarks and leptons with different quantum numbers. Grand Unified Theories (GUTs) address this issue by postulating that all known particles including those of the third generation belong to multiplets of larger symmetry group than SU(3)xSU(2)xU(1). This leads naturally to relations among coupling constants and fermion mass matrices. Another shortcoming of the MSSM is that it cannot explain small neutrino masses observed experimentally. However, if R-parity is broken then Majorana neutrinos may acquire tiny masses through see-saw mechanism. These new states could also contribute significantly to the relic density of dark matter.",
        "watermark_text": "We bring an overview on supersymmetric grand unified fields ( SUSY - GUT ) , their connection to neutrino masses via seesaw processes as well as dark matter candidates in these models . We discuss how GUT scale physics can be probed at possible colliders such as LHC or ILC .Finally we give some examples for specific realizations within SO ( 10 ) and E6 gauge bands . Supersymmetry is one of the most promising extensions beyond the Standard Model which answers many open questions like the hierarchy problem between electroweak and Planck scales , unification of forces etc . .In addition it gives a natural candidate for cold dark matter - the lightest neutralino . The minimal supersymmetric standard theory ( MSSM ) has been studied thoroughly over the last two decades but suffers from several shortcomings .One of them is that the MSSM does not offer any evidence why there are three generations of quarks and leptons with varying quantum numbers . Grand Unified Theories ( GUTs ) address this question by postulating that all known objects including those of the third generation belong to multiplets of bigger symmetry class than SU ( 3 ) xSU ( 2 ) xU ( 1 ) .This leads naturally to relations among coupling constants and fermion mass matrices . Another shortcoming of the MSSM is that it lacks explain little neutrino volumes discovered experimentally .However , if R - parity is shattered then Majorana neutrinos might acquire small masses through see - saw phenomenon . These new states could also contribute considerably to the relic volume of dark matter .",
        "rewrite_text": "We provide an overview of supersymmetric grand unified theories (SUSY-GUTs), exploring their relationship with neutrino masses through seesaw mechanisms, as well as potential candidates for dark matter within these frameworks. We discuss how GUT-scale physics may be investigated at upcoming colliders like the LHC or ILC. Additionally, we present examples of specific realizations within SO(10) and E6 gauge groups. Supersymmetry is a compelling extension of the Standard Model that addresses several unresolved issues, including the hierarchy problem between electroweak and Planck scales, as well as the unification of fundamental forces. It also naturally identifies the lightest neutralino as a candidate for cold dark matter. Although the minimal supersymmetric standard model (MSSM) has been extensively studied over the past twenty years, it has notable limitations, one being its failure to explain why there are three generations of quarks and leptons with differing quantum numbers. Grand Unified Theories (GUTs) tackle this issue by suggesting that all known particles, including those from the third generation, belong to larger symmetry multiplets beyond SU(3) × SU(2) × U(1), leading to intrinsic connections between coupling constants and fermion mass matrices. Another limitation of the MSSM is its inability to adequately account for the small neutrino masses observed experimentally. However, if R-parity is broken, Majorana neutrinos could acquire small masses via the seesaw mechanism, and these new states might significantly impact the relic density of dark matter.",
        "ori-fast-z-score": 0.7258661863112977,
        "water-fast-z-score": 6.25846794771944,
        "rewrite-fast-z-score": -1.2935483472729858
    },
    {
        "original_text": "We present an overview of the theory for vortices in trapped, dilute atomic gases at low temperatures. We discuss how these systems can be described by macroscopic wave functions and show that they are governed by nonlinear Schrödinger equations with external potentials. The solutions to this equation have been studied extensively over many years and we review some of their properties relevant to vortex formation. In particular, we consider stationary states which correspond to condensate configurations without rotation (vortex-free) as well as rotating ones where quantized angular momentum is carried by phase singularities known as vortices. Finally, we briefly describe recent experiments on vortex production in cold atom clouds. Vortices occur naturally in superfluids such as liquid helium or dilute atomic gases. They carry quantized angular momenta and play important roles in various physical phenomena including turbulence and quantum transport processes. Here we give an introduction into the theoretical description of vortices in trapped atomic gases.",
        "watermark_text": "We present an overview of the principle for vortices in trapped , dilute atomic materials at low temperatures . We discuss how these systems can be described by macroscopic wave distributions and explain that they are governed by nonlinear Schrödinger coefficients with external potentials .The solutions to this equation have been studied frequently over numerous years and we review some of their characteristics applicable to vortex structure . In particular , we investigate stationary states which refer to condensate configurations without rotation ( vortex - safe ) as well as rotating ones where quantized angular velocity is carried by phase singularities known as vortices .Finally , we briefly illustrate recent experiments on vortex production in cold atom clouds . Vortices arise naturally in superfluids such as fluid helium or dilute nuclear gases .They carry quantized angular momenta and play crucial roles in different mechanical phenomena including turbulence and quantum transport systems . Here we give an overview into the theoretical description of vortices in trapped atomic gases .",
        "rewrite_text": "We provide an overview of the principles governing vortices in trapped, dilute atomic materials at low temperatures. This discussion explores how these systems can be characterized by macroscopic wave distributions and highlights their behavior under the influence of nonlinear Schrödinger equations with external potentials. Over the years, the solutions to these equations have been extensively studied, and we review some key features relevant to vortex structure. In particular, we focus on stationary states that correspond to non-rotating condensate configurations (vortex-free) as well as rotating configurations characterized by quantized angular momentum carried by phase singularities known as vortices. Additionally, we briefly present recent experiments on vortex generation in cold atom clouds. Vortices are inherently found in superfluids such as liquid helium and dilute nuclear gases, where they carry quantized angular momenta and are essential to various mechanical phenomena, including turbulence and quantum transport. In this context, we offer a theoretical overview of vortices in trapped atomic gases.",
        "ori-fast-z-score": -1.1470786693528088,
        "water-fast-z-score": 5.047146145152358,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "We present an algorithm for solving the Graph Isomorphism Problem (GIP) based on Quantum Walks and Grover s search algorithm. The proposed method is inspired by the classical algorithms that use random walks, but it uses Grover s operator instead of the Hadamard matrix in order to speed up the process. We show how this new technique can be used to solve GIP with high probability when the number of vertices in both graphs are equal or differ at most one unit. Finally we compare our results against other state-of-the-art methods. In recent years there has been growing interest in developing efficient solutions to problems related to computational complexity theory such as the Graph Isomorphism Problem(GIP). This problem consists of determining whether two given graphs are isomorphic or not; i.e., if they have the same structure regardless of their labels. \n \n Classical approaches to solve GIP usually rely on Random Walks techniques combined with some heuristics. However these methods require exponential time in the worst case scenario. On the other hand, Quantum Algorithms provide polynomial-time solutions to many NP-complete problems including GIP. These algorithms exploit the superposition principle which allows them to explore all possible states simultaneously. For example, Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any item within a database in quadratic time.",
        "watermark_text": "We present an algorithm for solving the Graph Isomorphism Problem ( GIP ) based on Quantum Walks and Grover s search algorithm . The proposed approach is influenced by the classical techniques that use random walks , but it utilizes Grover s operator rather of the Hadamard vector in order to faster up the process .We see how this new technique can be used to solve GIP with high chance when the number of vertices in both graphs are equal or differ at most one unit . Finally we compare our findings against other state - of - the - art methods .In recent years there has been growing interest in pursuing efficient answers to problems related to computational complexity analysis such as the Graph Isomorphism Problem ( GIP ) . This problem consists of determining whether two given graphs are isomorphic or not ; i . e . , if they have the same structure regardless of their labels .Classical approaches to solve GIP usually relies on Random Walks techniques combined with some heuristics . However these algorithms involve exponential time in the worst case situations .On the other hand , Quantum Algorithms provide polynomial - time solutions to many NP - perfect issues including GIP . These methods incorporate the superposition concept which allows them to examine all possible states simultaneously .For instance , Shor s Algorithm solves integer factorization in polynomial time while Grover s Search finds any element within a database in quadratic time .",
        "rewrite_text": "We introduce an algorithm designed to tackle the Graph Isomorphism Problem (GIP) using Quantum Walks combined with Grover's search algorithm. Our approach draws inspiration from traditional methods that employ random walks, but instead of the Hadamard vector, we utilize Grover's operator to expedite the process. We demonstrate how this innovative technique can effectively address GIP with a high probability, particularly when the number of vertices in both graphs is either equal or differs by at most one. In conclusion, we compare our results with other leading techniques in the field. Recent years have seen an increasing interest in developing efficient solutions for problems related to computational complexity, such as GIP. This problem involves determining if two graphs are isomorphic, meaning they possess the same structure independent of their labels. Classical methods for solving GIP typically rely on Random Walk techniques and various heuristics, but these algorithms can exhibit exponential time complexity in the worst-case scenarios. Conversely, Quantum Algorithms offer polynomial-time solutions for many NP-hard problems, including GIP. These quantum methods leverage the concept of superposition, allowing them to evaluate multiple potential states simultaneously. For example, Shor's Algorithm efficiently performs integer factorization in polynomial time, while Grover's Search algorithm can locate an item in a database in quadratic time.",
        "ori-fast-z-score": -0.10050378152592121,
        "water-fast-z-score": 4.975196209154734,
        "rewrite-fast-z-score": 2.0691914841450156
    },
    {
        "original_text": "In this work, we propose an encounter based worm interaction model to evaluate the performance of different node characteristics in terms of their ability to detect and prevent worms spreading over mobile ad hoc networks (MANETs). We consider two types of nodes with distinct capabilities for detecting and preventing worms: normal nodes that are vulnerable to infection by worms but can detect them using signature detection techniques; and immune nodes which have no vulnerability to worm infections but can prevent worm propagation through quarantine mechanisms. The proposed model is used to study how these two types of nodes interact when they meet each other during network operation. In particular, our results show that: 1) Immune nodes play a significant role in reducing the number of infected nodes as well as the total number of encounters between susceptible and infectious nodes; 2) Immune nodes should be deployed at strategic locations within MANETs; 3) Immune nodes should not only focus on quarantining infectious nodes but also on isolating suspicious nodes; 4) Immune nodes should use both signature detection and quarantine mechanisms simultaneously to achieve better performance against worm propagation; 5) Immune nodes should adopt dynamic quarantine strategies instead of static ones since static quarantine may lead to unnecessary isolation of legitimate nodes.",
        "watermark_text": "In this project , we propose an encounter centered worm engagement theory to analyze the performance of different node characteristics in terms of their capabilities to identify and avoid worms distribution over mobile ad hoc networks ( MANETs ) . We consider two forms of nodes with distinct capabilities for detecting and preventing worms : normal networks that are susceptible to disease by viruses but can identify them utilizing pattern screening strategies ; and immune nodes which have no sensitivity to virus diseases but can prevent worm transmission through quarantine mechanisms .The proposed theory is utilized to study how these two kind of nodes interact when they meet each other during network activity . In particular , our findings show that : 1 ) Immune networks serve a substantial importance in reducing the quantity of infected connections as well as the total quantity of visits between vulnerable and infectious networks ; 2 ) Immune networks should be deployed at strategic locations within MANETs ; 3 ) Immune networks should not only focus on quarantining infectious networks but also on isolating suspect nodes ; 4 ) Immune networks should use both signature detection and quarantine mechanisms separately to achieve good efficiency against worm transmission ; 5 ) Immune networks should implement dynamic quarantine techniques instead of static ones since static quarantine may contribute to inappropriate isolation of genuine nodes .",
        "rewrite_text": "In this project, we introduce an encounter-centered worm engagement theory to evaluate how various node characteristics affect their ability to detect and mitigate worm propagation in mobile ad hoc networks (MANETs). We differentiate between two types of nodes: normal nodes, which are vulnerable to virus infections but can identify them through pattern recognition techniques, and immune nodes, which are resistant to viral infections and can prevent worm spread using quarantine strategies. Our theory explores the interactions between these two types of nodes when they encounter one another during network operations. Our key findings reveal the following: 1) Immune nodes play a crucial role in minimizing the number of infected connections and the overall interactions between vulnerable and malicious nodes; 2) Immune nodes should be strategically positioned within MANETs; 3) Immune nodes should focus on isolating both infected and suspicious nodes; 4) A combination of signature detection and quarantine methods should be employed by immune nodes to enhance effectiveness against worm transmission; 5) Dynamic quarantine strategies are preferable over static ones to avoid the improper isolation of legitimate nodes.",
        "ori-fast-z-score": -1.4237369936287485,
        "water-fast-z-score": 8.067842963896242,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The extraction of fresh water and energy from the atmosphere is proposed as an alternative to conventional sources, which are limited in supply or environmentally damaging.  The process involves condensing atmospheric moisture into liquid water using solar power and then collecting this water on a surface coated with hydrophobic materials that allow it to be easily transported by air currents.   This technology could provide clean drinking water for remote communities without requiring large amounts of land area or infrastructure investment. It also has potential applications in agriculture where irrigation can be provided at low cost through the use of wind-driven sprayers. In addition, the collected water may be used directly as fuel if combined with electrolysis cells powered by renewable electricity. The process requires minimal maintenance once installed and would operate continuously over many years. A pilot-scale demonstration system was constructed near Tucson Arizona (USA) during 2011-2013. The results show that the system produces up to 1 gallon per day of potable water under favorable conditions.",
        "watermark_text": "The extraction of fresh water and energy from the air is proposed as an alternative to conventional sources , which are limited in supply or environmentally destructive . The method means condensing ambient rainfall into liquid water use solar electricity and then collecting this water on a surface packed with hydrophobic materials that enable it to be easily carried by air waves .This system could supply fresh water freshwater for isolated communities without using big amounts of property area or structural capital . It additionally has potential applications in farming where irrigation can be provided at low cost through the using of wind - powered sprayers .In addition , the stored water may be used directly as fuel if combined with electrolysis panels driven by renewable energy . The method needs minimal repair once implemented and might run constantly over numerous years .A pilot - scale test system was constructed near Tucson Arizona ( USA ) during 2011 - 2013 . The results show that the scheme produces up to 1 gallon per day of potable liquid under favorable conditions .",
        "rewrite_text": "The proposed method for extracting fresh water and energy from the atmosphere offers a sustainable alternative to traditional sources, which are often limited or harmful to the environment. This technique involves condensing moisture from the air into liquid water using solar energy and collecting it on surfaces coated with hydrophobic materials, allowing it to be easily transported by air currents. This system has the potential to provide fresh water for remote communities without requiring extensive land use or significant infrastructure investment. Additionally, it could be beneficial in agriculture, enabling low-cost irrigation through wind-powered sprayers. The collected water can also serve as a fuel source when paired with electrolysis panels powered by renewable energy. Once established, this system requires minimal maintenance and could operate continuously for years. A pilot-scale prototype was tested near Tucson, Arizona, between 2011 and 2013, yielding results that indicated production of up to 1 gallon of potable water daily under ideal conditions.",
        "ori-fast-z-score": -0.8340576562282991,
        "water-fast-z-score": 7.715033320111767,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "In this work, we study spin effect on resonant tunneling characteristics in double-barrier heterostructure under longitudinal stresses by using transfer matrix method and density functional theory (DFT). We find that the energy gap decreases with increasing stress for both majority-spin electrons and minority-spin holes. The decrease is more significant for minority-spin holes than for majority-spin electrons. In addition, the transmission coefficient increases with increasing stress at low bias voltage but decreases at high bias voltage. This behavior can be explained as follows. At low bias voltage, the increase of transmission coefficient results mainly from the reduction of barrier height due to compressive stress. However, at high bias voltage, the decrease of transmission coefficient comes from two factors: one is the increase of effective mass induced by tensile stress; another is the enhancement of electron-phonon interaction caused by tensile stress. Finally, it should be noted that our calculation shows that the spin-orbit coupling has little influence on the transport properties of the system considered here.",
        "watermark_text": "In this research , we study spin effect on resonant tunneling parameters in dual - barrier heterostructure under longitudinal strain by using transfer matrix method and density functional theory ( DFT ) . We see that the power gap falls with increasing stress for both majority - spinning electrons and minority - spinning holes .The reduction is more considerable for minority - spin holes than for majority - spinning electrons . In addition , the transmission coefficient increases with varying stress at low bias frequency but decreases at high bias voltage .This phenomenon can be understood as follows . At small bias power , the increase of transmission coefficient proceeds primarily from the reduction of barrier height owing to compressive stress .However , at high bias voltage , the decrease of transmission coefficient happens from two factors : one is the improvement of effective mass induced by tensile tension ; another is the enhancement of electron - phonon interaction resulting by tensile tension . Finally , it should be mentioned that our calculation demonstrates that the spin - orbit interaction has little influence on the travel properties of the system discussed here .",
        "rewrite_text": "In this study, we examine the impact of spin on resonant tunneling parameters in dual-barrier heterostructures subjected to longitudinal strain, utilizing the transfer matrix method and density functional theory (DFT). Our findings indicate that the energy gap decreases with increasing stress for both majority-spin electrons and minority-spin holes, with a more significant reduction observed for minority-spin holes. Additionally, we note that the transmission coefficient rises with varying stress at low bias frequencies but declines at high bias voltages. This behavior can be explained as follows: at low bias power, the increase in the transmission coefficient largely results from a decrease in barrier height due to compressive stress. Conversely, at high bias voltage, the transmission coefficient decreases due to two factors: an increase in effective mass caused by tensile strain and an enhanced electron-phonon interaction due to the same tensile strain. Lastly, it is important to note that our calculations indicate that spin-orbit interaction has a minimal effect on the transport properties of the system under consideration.",
        "ori-fast-z-score": 0.5129891760425771,
        "water-fast-z-score": 6.668859288553502,
        "rewrite-fast-z-score": 2.0628424925175866
    },
    {
        "original_text": "Gravitational wave astronomy is an emerging field that will provide new insights into the universe and its fundamental laws through observations of gravitational waves emitted by merging black holes, neutron stars or other compact objects in distant galaxies.  In this work we present several tests of different techniques used to select between competing models describing the observed data. We consider two examples where the signal-to-noise ratio (SNR) of the detected signals are low enough so that it becomes difficult to distinguish between different physical scenarios using standard frequentist hypothesis testing methods. The first example considers the problem of distinguishing between binary black hole systems with spin aligned versus anti-aligned with their orbital angular momentum vector. The second example considers the problem of determining whether a given source has been emitting gravitational radiation continuously over time as opposed to being active only during short bursts. For both cases we compare results obtained using three different model selection methods: Akaike s information criterion (AIC), Bayes factors computed via nested sampling (NS-BF), and the Deviance Information Criterion (DIC).",
        "watermark_text": "Gravitational wave studies is an developing field that will provide fresh insights into the universe and its essential laws through observations of gravitational waves emitted by merging black holes , neutron galaxies or other compact entities in nearby galaxies . In this project we present many tests of different methods used to select between various models explaining the emitted data .We consider two examples where the signal - to - noise proportion ( SNR ) of the emitted signals are low enough so that it becomes hard to distinguish between various physical scenarios using conventional frequentist hypothesis testing algorithms . The first instance involves the question of distinguishing between binary white hole complexes with spin aligned versus counter - aligned with their orbital angular velocity tensor .The second example considers the question of determining whether a given source has been emitting gravitational rays continuously over time as opposed to being active only during short pulses . For both cases we compare findings obtained using three different model selection approaches : Akaike s information criterion ( AIC ) , Bayes criteria computed via nested filtering ( NS - BF ) , and the Deviance Information Criterion ( DIC ) .",
        "rewrite_text": "The study of gravitational waves is an emerging field that promises to offer new insights into the universe and its fundamental laws by observing gravitational waves produced by the merging of black holes, neutron stars, and other compact objects in nearby galaxies. In this project, we present various tests of different methods used to differentiate between multiple models that explain the emitted data. We focus on two scenarios where the signal-to-noise ratio (SNR) of the emitted signals is low enough that traditional frequentist hypothesis testing algorithms struggle to distinguish between different physical scenarios. The first scenario addresses the challenge of differentiating between binary white hole complexes with their spins either aligned or counter-aligned with their orbital angular momentum. The second scenario involves determining whether a given source has been emitting gravitational waves continuously over time or only during intermittent pulses. For both cases, we compare the results obtained using three different model selection techniques: Akaike’s Information Criterion (AIC), Bayesian criteria computed through nested sampling (NS-BF), and the Deviance Information Criterion (DIC).",
        "ori-fast-z-score": -0.29851115706299675,
        "water-fast-z-score": 7.263771488532921,
        "rewrite-fast-z-score": 1.970208219987808
    },
    {
        "original_text": "The damage identification problem is formulated as an inverse problem, where the objective is to identify the location and severity of damages by minimizing the difference between simulated responses using finite element analysis (FEA) and measured data. The number of unknowns can be very large due to the presence of multiple sensors or measurement points. In this study, we propose two methods for reducing the dimensionality of the problem: principal component analysis (PCA), which reduces the dimension of the response space; and automatic relevance determination (ARD), which reduces the dimensions of both the input parameter space and the output response space simultaneously. Both PCA and ARD are implemented within the Bayesian framework so that uncertainties associated with these reduction techniques can also be accounted for during the optimization process. A numerical example involving a cantilever beam subjected to static loading is presented to demonstrate the effectiveness of the proposed approaches. Principal component analysis (PCA) and automatic relevance determination (AR",
        "watermark_text": "The damage identification question is implemented as an inverse question , where the objective is to identify the location and intensity of damages by minimizing the difference between simulated stimuli involving finite element assessment ( FEA ) and measured data . The amount of unknowns can be very huge resulting to the presence of multiple sensors or observation points .In this study , we propose two strategies for decreasing the dimensionality of the issue : principal component analysis ( PCA ) , which reduces the dimension of the response space ; and instant relevance determination ( ARD ) , which reduces the sizes of both the input parameter space and the output response space simultaneously . Both PCA and ARD are implemented within the Bayesian framework so that uncertainties involved with these reduction techniques can also be accounted for during the optimization process .A numerical example featuring a cantilever beam subjected to static loading is provided to indicate the ability of the suggested techniques . Principal part evaluation ( PCA ) and instant context determination ( AR",
        "rewrite_text": "The damage identification problem is framed as an inverse problem, aiming to pinpoint both the location and severity of damages by minimizing the discrepancy between simulated stimuli from finite element analysis (FEA) and observed data. The number of unknowns can be substantial due to the presence of multiple sensors or observation points. In this paper, we introduce two approaches to reduce the dimensionality of the problem: principal component analysis (PCA), which decreases the dimensionality of the response space, and automatic relevance determination (ARD), which simultaneously reduces the sizes of both the input parameter space and the output response space. Both PCA and ARD are implemented within a Bayesian framework, allowing us to account for uncertainties associated with these reduction techniques during the optimization process. A numerical example involving a cantilever beam under static loading is presented to demonstrate the effectiveness of the proposed methods.",
        "ori-fast-z-score": 0.6793662204867574,
        "water-fast-z-score": 5.887840577551898,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "We present the results of N-body simulations for open and globular star clusters with different initial conditions, including primordial binaries in various proportions (from 0 to 100%). We find that the fraction of binaries among all stars decreases as the cluster evolves due to dynamical interactions between single and binary stars. The decrease is more pronounced if there are initially many hard binaries or few soft ones. In addition, we show how the number of binaries depends on their binding energy distribution at birth. Finally, we compare our results with observations of real open and globular clusters. Our main conclusions are:  1) Open clusters have fewer binaries than globulars because they lose most of them during early evolution.  2) Binaries can be destroyed by three-body encounters even when the total number of binaries remains constant.  3) Hard binaries dominate over soft ones after several relaxation timescales t rh .",
        "watermark_text": "We present the conclusion of N - bodies simulations for open and globular star clusters with various initial conditions , covering primordial binaries in different proportions ( from 0 to 100 % ) . We see that the fraction of binaries among all stars reduces as the cluster evolves due to dynamical interactions between single and binary galaxies .The decrease is more pronounced if there are initially multiple tough binaries or few hard ones . In addition , we study how the number of binaries depends on their binding energy flow at birth .Finally , we compare our findings with observations of real open and globular nuclei . Our main results are : 1 ) Open clusters have fewer binaries than globulars because they losing most of them during early evolution .2 ) Binaries can be damaged by three - bodies interactions even when the total number of binaries remains constant . 3 ) Hard binaries dominate over soft ones after many relaxation timescales t rh .",
        "rewrite_text": "We present the results of N-body simulations for open and globular star clusters with a variety of initial conditions, specifically examining primordial binaries in different proportions (ranging from 0% to 100%). Our findings indicate that the fraction of binaries among all stars decreases as the cluster evolves, primarily due to dynamical interactions between single stars and binary systems. This decline is more significant when there are multiple hard binaries or only a few soft binaries initially present. We also investigate how the number of binaries is influenced by their binding energy at birth. Finally, we compare our results with observations of actual open and globular star clusters. Our key conclusions are as follows: 1) Open clusters exhibit fewer binaries than globular clusters because they lose most binaries during their early evolutionary stages. 2) Three-body interactions can disrupt binaries even when the overall binary count remains unchanged. 3) After many relaxation timescales (t_rh), hard binaries prevail over soft binaries.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "We present Herschel Space Observatory observations at 70, 160, 250, 350, and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi (RO) molecular cloud complex. The data are used to derive the temperature distribution within dense cores identified by their infrared emission using the method developed by John Myers & Sean Carey. We find that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K. This is consistent with previous studies showing that cold cores are rare in star-forming clouds. Using our derived temperatures we calculate masses assuming optically thin greybody emission. These masses range from 0.1 Msun to more than 100 Msun. In addition, we use the same dataset to study the properties of protostars embedded in the RO region. We identify 16 Class I sources based on their spectral energy distributions and compare them to those found in other nearby star-forming regions such as Serpens South or Orion B North.",
        "watermark_text": "We present Herschel Space Observatory images at 70 , 160 , 250 , 350 , and 500 microns toward two fields centered on the densest parts of the Rho Ophiuchi ( RO ) molecular mist complex . The data are using to derive the temperature balance within dense cores identified by their infrared emission utilizing the method developed by John Myers & Sean Carey .We see that most of these cores have temperatures between 10 K and 20 K with only one colder than 8 K . This is consistent with previous research indicating that cool cores are scarce in star - creating clouds . Using our derived temperatures we estimate masses assuming optically thin greybody emission .These masses range from 0 . 1 Msun to more than 100 Msun . In addition , we utilize the same dataset to study the properties of protostars embedded in the RO region .We distinguish 16 Class I sources based on their spectral power distributions and contrast them to those present in other nearby star - creating areas such as Serpens South or Orion B North .",
        "rewrite_text": "We present images from the Herschel Space Observatory at wavelengths of 70, 160, 250, 350, and 500 microns, focusing on two areas within the densest regions of the Rho Ophiuchi (RO) molecular cloud complex. This data is used to assess the temperature distribution within dense cores, which were identified by their infrared emissions using the methodology developed by John Myers and Sean Carey. Our findings reveal that the majority of these cores exhibit temperatures ranging from 10 K to 20 K, with only one core recorded at a temperature below 8 K. This aligns with prior studies suggesting that cool cores are rare in star-forming clouds. By applying the derived temperatures, we estimate the masses of these cores, assuming optically thin greybody emission, with values spanning from 0.1 M☉ to over 100 M☉. Additionally, we employ the same dataset to investigate the characteristics of protostars located within the RO region. We identify 16 Class I sources based on their spectral power distributions and compare these to those found in other nearby star-forming regions such as Serpens South and Orion B North.",
        "ori-fast-z-score": 0.5163977794943222,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -1.0
    },
    {
        "original_text": "We propose an improved metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory. The new metric has several advantages over previous proposals, including manifestly positive kinetic terms and no need for additional counterterms at higher orders. We show how this metric can be used to compute beta functions up to third order in perturbation theory using only Feynman diagrams with one-loop vacuum bubbles as building blocks. This allows us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are consistent with those obtained by other methods but have not been previously available due to technical difficulties. In addition we find evidence for non-trivial fixed points in the beta function of the string coupling constant. These results provide further support for the idea that the worldsheet sigma model may serve as a useful tool for studying quantum gravity. Introduction: It was recently shown  1  that the worldsheet sigma-model (WSSM) provides a powerful framework for investigating quantum gravity via its connection to the gravitational path integral  2  . One particularly interesting aspect of this approach is the possibility of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit calculations involving gravitons or graviton loops  3  .\nIn  4  it was proposed that the WSSM could also be used to investigate the flow of the effective action under the renormalization group (RG). However, since the WSSM contains infinitely many degrees of freedom there does not exist any finite dimensional parameter space where the RG flow takes place. Instead, the RG flow must take place along some infinite-dimensional trajectory through the space of all possible actions. To make progress towards understanding such trajectories it would be helpful if one were able to define a sensible metric on the space of WSSM actions so that distances between different actions could be measured. Such a metric should allow one to determine whether two given actions lie close together or far apart in the space of all possible WSSMs.",
        "watermark_text": "We suggest an better metric on the space of worldsheet sigma model couplings that is suitable to study gradient renormalization group flows beyond first order in perturbation theory . The updated metric has numerous benefits over past proposals , notably manifestly strong kinetic terms and no requirement for additional counterterms at higher orders .We see how this metric can be used to compute beta functions up to third order in perturbation theory employing only Feynman diagrams with one - ring vacuum bubbles as building blocks . This enables us to obtain results for the beta function of the dilaton coupling to the Ricci scalar which are compatible with those achieved by other methods but have not been previously available owing to technical problems .In addition we find proof for non - trivial fixing points in the beta function of the string coupling constant . These conclusions provide further evidence for the idea that the worldsheet sigma approach may serve as a helpful resource for studying quantum gravitational .Introduction : It was recently shown 1 that the worldsheet sigma - model ( WSSM ) presents a powerful framework for investigating quantum gravitational via its connection to the gravitational path integral 2 . One especially interesting aspect of this methodology is the prospect of computing perturbative corrections to the WSSM action directly from the gravitational path integral without having to resort to explicit experiments concerning gravitons or graviton loops 3 .In 4 it was suggested that the WSSM could also be used to examine the flow of the effective act under the renormalization group ( RG ) . However , since the WSSM contains infinitely many degrees of liberty there does not exist any finite dimensional parameter area where the RG flow takes place .Instead , the RG flow must take place along some infinite - dimensional trajectory through the space of all possible actions . To build progress towards studying such trajectories it would be nice if one were trying to define a practical metric on the space of WSSM actions so that lengths between multiple movements could be measured .Such a metric should enable one to estimate whether two given actions sit close together or far separated in the space of all possible WSSMs .",
        "rewrite_text": "We propose an improved metric for the space of worldsheet sigma model couplings that is well-suited for examining gradient renormalization group flows beyond first-order perturbation theory. This new metric offers several advantages over previous proposals, particularly its clear representation of strong kinetic terms and its elimination of the need for extra counterterms at higher orders. We demonstrate how this metric can be utilized to compute beta functions up to third order in perturbation theory, using only one-ring vacuum bubble Feynman diagrams as foundational components. This allows us to derive results for the beta function of the dilaton coupling to the Ricci scalar that align with findings from other methodologies, yet were previously unattainable due to technical challenges. Furthermore, we establish evidence for non-trivial fixed points in the beta function related to the string coupling constant. These results bolster the notion that the worldsheet sigma model approach can be a valuable tool for investigating quantum gravity.\n\nIntroduction: Recent work has shown that the worldsheet sigma model (WSSM) serves as a robust framework for exploring quantum gravity through its association with the gravitational path integral. One particularly intriguing feature of this approach is the ability to compute perturbative corrections to the WSSM action directly from the gravitational path integral, avoiding the need for explicit experiments involving gravitons or graviton loops. It has also been suggested that the WSSM could be employed to analyze the flow of the effective action under the renormalization group (RG) dynamics. However, because the WSSM encompasses infinitely many degrees of freedom, there is no finite-dimensional parameter space in which the RG flow occurs. Instead, the RG flow must navigate an infinite-dimensional trajectory through the full spectrum of possible actions. To facilitate the study of such trajectories, it is essential to define a practical metric on the space of WSSM actions, enabling the measurement of distances between various configurations. This metric would provide insights into whether two given actions are located near each other or are significantly apart in the landscape of all conceivable WSSMs.",
        "ori-fast-z-score": 0.7715167498104595,
        "water-fast-z-score": 8.33543833063041,
        "rewrite-fast-z-score": 1.0245435281108308
    },
    {
        "original_text": "We present the Bridge algorithm, which is an efficient direct tree hybrid n-body method that can be used to simulate fully self-consistently star clusters in galactic potentials with arbitrary mass distributions. The Bridge algorithm combines the advantages of both direct-summation (DS) and tree methods by using DS only within small sub-groups of particles at each time step while solving the full system of equations on a tree. We show that this approach allows us to achieve high accuracy without sacrificing computational efficiency. In particular we demonstrate that our new code reproduces results obtained with the state-of-the-art treecode NBODY6++GPU very well even when simulating systems containing up to 10 million stars. This makes it possible to study the long-term dynamical evolution of open clusters as well as globular clusters orbiting around parent galaxies over many Gyr timescales. \n \n Keywords: Open cluster; Globular cluster; Galactic potential",
        "watermark_text": "We introduce the Bridge algorithm , which is an efficient direct tree hybrid n - bodies method that can be used to simulate fully self - consistently star clusters in galactic potentials with arbitrary mass distributions . The Bridge technique combines the advantages of both direct - summation ( DS ) and tree methods by using DS only within tiny sub - families of atoms at each time step while solving the full system of equations on a tree .We suggest that this methodology allows us to achieve high efficiency without sacrificing computational efficiency . In particular we prove that our new code reproduces data derived with the state - of - the - art treecode NBODY6 + + GPU very best even when simulating complexes containing up to 10 million stars .This gives it able to study the long - term dynamical development of open complexes as well as globular complexes orbiting around parent objects over many Gyr timescales . Keywords : Open cluster ; Globular cluster ; Galactic potential",
        "rewrite_text": "We present the Bridge algorithm, an efficient direct tree hybrid method for simulating fully self-consistent star clusters within galactic potentials that feature arbitrary mass distributions. This technique effectively merges the strengths of direct summation (DS) and tree methods by utilizing DS solely within small sub-families of particles at each time step, while solving the overall system of equations on a tree structure. Our approach demonstrates high efficiency without compromising computational performance. Notably, we show that our new code successfully replicates results produced by the advanced treecode NBODY6++ GPU, even when simulating complex systems with up to 10 million stars. This capability enables the study of long-term dynamical evolution in both open and globular clusters over timescales of several billion years. Keywords: Open cluster; Globular cluster; Galactic potential.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 3.771236166328254,
        "rewrite-fast-z-score": -1.3438638879193574
    },
    {
        "original_text": "We present an analytical approach to study the dynamics of the contact process on complex networks with arbitrary degree distributions, including scale-free (SF) ones. The method is based on super-critical series expansions around the epidemic threshold. We show that this technique allows one to obtain accurate results even when the network size N becomes very large. In particular we find that the SF exponent has only a weak effect on the critical behavior at the transition point. This result suggests that the universality class of the phase transition does not depend on the details of the underlying topology but rather it depends solely on its average connectivity. Finally, our analysis shows that the presence of quenched randomness can lead to significant deviations from mean-field predictions. \nI. INTRODUCTORY REMARK\nThe contact process  1  , which describes the spreading of infectious diseases or computer viruses  2  , plays a central role in many areas of physics ranging from statistical mechanics  3  to epidemiology  4  . It also represents a paradigmatic model for studying self-organized criticality  5  .\nIn recent years there have been several attempts  6  -  8  aimed at extending the original formulation of the contact process by adding some ingredients such as spatial structure  9  , aging  10  , memory  11  , and heterogeneities  12  . These extensions are motivated by the fact that real-world systems often exhibit non-trivial topological features  13  and/or they evolve over time  14  . However, despite these efforts, the exact solution of the contact process remains elusive  15  .\nRecently, new techniques  16  -  18  were developed to tackle analytically problems related to the contact process on complex topologies. Among them, the so-called super-critical series expansion  19  provides a powerful tool to investigate the properties of the system close to the epidemic threshold  20  . Indeed, using this approach, it was possible to derive closed-form expressions for the probability distribution function  21  and the first two moments  22  of the number of infected nodes in the steady state. Moreover, it allowed us to determine the scaling laws characterizing the relaxation towards equilibrium  23  .",
        "watermark_text": "We introduce an analytical method to study the dynamics of the contact process on complex networks with arbitrary degree distributions , particularly scale - free ( SF ) ones . The method is based on ultra - critical series expansions around the outbreak threshold .We see that this methodology allows one to obtain precise conclusions even when the channel size N becomes very huge . In particular we find that the SF exponent has only a weak effect on the key behavior at the shift point .This result suggests that the universality category of the phase shift does not depend on the details of the underlying topology but rather it affects solely on its average connectivity . Finally , our analysis shows that the presence of quenched randomness can lead to significant deviations from mean - field estimates .I . INTRODUCTORY REMARK The connection process 1 , which explains the spreading of infectious infections or machine infections 2 , takes a central role in different areas of physics including from statistical mechanics 3 to epidemiology 4 .It additionally provided a paradigmatic theory for studying self - organized criticality 5 . In past decades there have been numerous attempts 6 - 8 aimed at extending the original formulation of the contact process by added some ingredients such as temporal structure 9 , aging 10 , memory 11 , and heterogeneities 12 .These extensions are motivated by the fact that real - world systems often exhibit non - simple topological features 13 and / or they develop over time 14 . However , despite these attempts , the exact solution of the contact process remains elusive 15 .Recently , new concepts 16 - 18 were developed to tackle analytically problems related to the contact process on difficult topologies . Among them , the so - called super - critical series expansion 19 offers a powerful tool to examine the properties of the process close to the epidemic limit 20 .Indeed , using this methodology , it was useful to derive closed - form expressions for the probability distribution relation 21 and the first two moments 22 of the quantity of infected nodes in the steady state . Moreover , it able us to predict the scaling laws characterizing the relaxation towards equilibrium 23 .",
        "rewrite_text": "We present an analytical approach to investigate the dynamics of the contact process on complex networks with arbitrary degree distributions, especially scale-free (SF) networks. This method employs ultra-critical series expansions around the outbreak threshold, allowing for accurate conclusions even as the network size \\( N \\) becomes very large. Notably, we discover that the SF exponent exerts only a weak influence on the key behavior at the shift point. This finding implies that the universality class of the phase transition is independent of the specific details of the underlying topology, instead relying primarily on average connectivity. Additionally, our analysis indicates that the presence of quenched randomness can lead to significant deviations from mean-field predictions. \n\n**I. INTRODUCTORY REMARKS**  \nThe connection process, which describes the spread of infectious diseases and machine-related infections, plays a crucial role across various fields of physics, from statistical mechanics to epidemiology. It has also provided a foundational framework for the study of self-organized criticality. Over the past decades, numerous efforts have been made to extend the original formulation of the contact process by incorporating elements such as temporal structure, aging, memory, and heterogeneities. These adaptations are driven by the observation that real-world systems often exhibit complex topological features and evolve over time. Despite these efforts, an exact solution for the contact process remains elusive. Recently, innovative concepts have emerged to analytically address challenges associated with the contact process on complex topologies. Among these, the super-critical series expansion has proven to be a valuable tool for examining the process's properties near the epidemic threshold. This methodology has enabled the derivation of closed-form expressions for the steady-state probability distribution and the first two moments of the number of infected nodes, as well as predictions of the scaling laws that govern the relaxation to equilibrium.",
        "ori-fast-z-score": 0.6923076923076923,
        "water-fast-z-score": 8.538461538461538,
        "rewrite-fast-z-score": -0.9486832980505138
    },
    {
        "original_text": "We present an analysis of the kinematics of satellite galaxies in galaxy clusters, which is aimed at measuring the differences between the distribution functions (DFs) of dark matter particles and those of luminous baryonic components. We use data for 16 galaxy clusters with redshifts ranging from 0.1 to 1.0 obtained by the Sloan Digital Sky Survey Data Release 6. The DFs are reconstructed using the maximum entropy method under the assumption that they have spherical symmetry. In order to test whether this assumption holds true or not, we compare the results derived assuming spherical symmetry with those obtained without any assumptions on the shape of the DFs. Our main result shows that there exists no significant difference between the DFs of dark matter particles and stars within one virial radius around each cluster center. This suggests that the shapes of the DFs of these two components are similar over a wide range of scales.",
        "watermark_text": "We publish an assessment of the kinematics of satellite galaxies in galaxy clusters , which is aiming at assessing the differences between the distribution functions ( DFs ) of dark matter elements and those of luminous baryonic components . We use data for 16 galaxy regions with redshifts ranging from 0 . 1 to 1 . 0 collected by the Sloan Digital Sky Survey Data Release 6 .The DFs are reconstructed using the maximum entropy method under the assumption that they have spherical symmetry . In order to test whether this assumption holds true or not , we compare the results derived assuming spherical symmetry with those achieved without any constraints on the morphology of the DFs .Our main result suggests that there exists no major variation between the DFs of light matter elements and stars within one virial diameter around each cluster center . This implies that the shapes of the DFs of these two components are comparable over a broad variety of scales .",
        "rewrite_text": "We present an analysis of the kinematics of satellite galaxies within galaxy clusters, focusing on the differences between the distribution functions (DFs) of dark matter and luminous baryonic components. Our study utilizes data from 16 galaxy regions with redshifts ranging from 0.1 to 1.0, obtained from the Sloan Digital Sky Survey Data Release 6. The DFs are reconstructed using the maximum entropy method, assuming spherical symmetry. To evaluate the validity of this assumption, we compare the results obtained under the spherical symmetry constraint to those derived without any morphological restrictions on the DFs. Our key finding indicates that there is no significant variation between the DFs of luminous matter and stars within one virial diameter of each cluster center. This suggests that the shapes of the DFs for these two components are similar across a wide range of scales.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 4.064004064006096,
        "rewrite-fast-z-score": -1.3608276348795434
    },
    {
        "original_text": "We present the results of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range 8-40 M⊙, which are expected to produce GRB jets that can be observed at cosmological distances. We find that these systems evolve into double-degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae. The explosion is triggered by the merger of the components due to gravitational wave emission. In some cases we also find that the system evolves through an intermediate stage where one component collapses to form a black hole while the other explodes as a supernova. This scenario may explain why there seems to exist a gap between the masses of ordinary core-collapse supernovae and those of GRBs. Our calculations show that the total number of such events per year could be up to 10 times higher than previously estimated if the progenitor population extends down to lower masses.",
        "watermark_text": "We present the conclusion of our study on binary models for gamma - ray bursts ( GRBs ) with progenitors in the mass range 8 - 40 [UNK] , which are expected to produce GRB jets that can be observed at cosmological distances . We say that these systems evolve into double - degenerate binaries consisting of two white dwarfs or helium stars before they explode as supernovae .The explosion is caused by the merger of the parts due to gravitational wave radiation . In some cases we also find that the system evolves through an intermediate stage where one element collapses to form a black hole while the other explodes as a supernova .This scenario could explain why there seems to exist a gap between the masses of normal core - collapse supernovae and those of GRBs . Our calculations show that the total number of such events per year could be up to 10 twice higher than previously predicted if the progenitor number extends down to smaller masses .",
        "rewrite_text": "We present the findings of our study on binary models for gamma-ray bursts (GRBs) with progenitors in the mass range of 8 to 40 solar masses, which are expected to produce GRB jets detectable at cosmological distances. We propose that these systems evolve into double-degenerate binaries composed of two white dwarfs or helium stars before ultimately detonating as supernovae. This explosion is triggered by the merger of the components due to gravitational wave emission. In some cases, our research indicates that the system undergoes an intermediate phase where one component collapses into a black hole while the other undergoes a supernova explosion. This scenario may clarify the observed gap between the masses of typical core-collapse supernovae and those associated with GRBs. Our calculations suggest that the annual occurrence of such events could be as much as twice as high as previously estimated if the progenitor mass range includes lower masses.",
        "ori-fast-z-score": -0.12216944435630522,
        "water-fast-z-score": 3.298574997620241,
        "rewrite-fast-z-score": 0.12216944435630522
    },
    {
        "original_text": "We present the results of cosmological hydrodynamic simulations that follow the formation and evolution of primordial gas clouds with masses between 10^(5) M_sun and 10^(7) M_sun, which are likely to be progenitors of low-mass galaxies at high redshifts (z > 6). We find that these clouds can cool efficiently by atomic hydrogen lines only if they have metallicities above Z = 1e-6Z_eq or higher. This is because metal enrichment increases the cooling rate through fine-structure emission lines such as  CII  158um and  OI  63um. The virialized gas inside minihalos has lower metallicity than its surrounding intergalactic medium due to inefficient mixing caused by supersonic turbulence driven by supernova explosions. As a result, it cannot cool below T_c ~ 100K even though it contains enough neutral hydrogen for efficient H_2 cooling.",
        "watermark_text": "We present the conclusion of cosmological hydrodynamic simulations that follow the formation and evolution of primordial liquid clouds with masses between 10 ^ ( 5 ) M _ sun and 10 ^ ( 7 ) M _ sun , which are likely to be progenitors of low - density galaxies at high redshifts ( z > 6 ) . We see that these clouds can cool efficiently by atomic hydrogen lines only if they have metallicities above Z = 1e - 6Z _ eq or greater .This is because metal enrichment raises the cooling frequency through fine - structure emitted lines such as CII 158um and OI 63um . The virialized gas inside minihalos has reduced metallicity than its neighbouring intergalactic medium due to inefficient mix created by supersonic turbulence driven by supernova earthquakes .As a result , it cannot cold below T _ c ~ 100K even though it contains enough neutral hydrogen for efficient H _ 2 cooling .",
        "rewrite_text": "We present our findings from cosmological hydrodynamic simulations that track the formation and evolution of primordial liquid clouds with masses ranging from \\(10^5 M_{\\odot}\\) to \\(10^7 M_{\\odot}\\). These clouds are likely the ancestral forms of low-density galaxies at high redshifts (z > 6). Our results indicate that these clouds can cool effectively through atomic hydrogen lines only when their metallicities exceed \\(Z = 10^{-6}Z_{\\text{eq}}\\). This increase in metallicity enhances the cooling processes via fine-structure emission lines, such as CII at 158 µm and OI at 63 µm. The virialized gas within minihalos has a lower metallicity compared to the surrounding intergalactic medium, a consequence of the inefficient mixing caused by supersonic turbulence from supernova activity. Consequently, this gas cannot cool to temperatures below \\(T_c \\sim 100 \\, \\text{K}\\), despite containing sufficient neutral hydrogen for effective H2 cooling.",
        "ori-fast-z-score": -0.9615239476408232,
        "water-fast-z-score": 2.8845718429224694,
        "rewrite-fast-z-score": 0.6622661785325219
    },
    {
        "original_text": "The nullstellensatz is an important theorem in algebraic geometry, which states that if a system of polynomial equations has no solution over some field then it must have at least one solution over its algebraic closure.  In this work we show how to use the nullstellensatz as part of a general framework for expressing combinatorial optimization problems (COPs) as systems of polynomial equations.   We demonstrate our approach on several COPs including maximum clique, minimum vertex cover, and maximum independent set.   ... This research was supported by NSF grant CCF-0430020. The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript. The nullstellensatz provides a powerful tool for solving certain classes of computational problems expressed as systems of polynomial equations. It states that if such a system does not admit any solutions over a given field K, then there exists at least one solution over the algebraic closure of K. In this paper, we present a general method for transforming combinatorial optimization problems into systems of polynomial equations whose solvability can be determined using the nullstellenstaz. Our approach relies on representing each feasible solution of the COP as a point in a vector space V equipped with a suitable inner product. Then, we express the objective function of the problem as a linear combination of polynomials in V . Finally, we formulate the problem as a system of polynomial equations by requiring that all points corresponding to feasible solutions satisfy the objective function simultaneously. As examples, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.",
        "watermark_text": "The nullstellensatz is an important theorem in algebraic theory , which says that if a system of polynomial equations has no solved over some field then it must have at least one answer over its algebraic closure . In this study we show how to use the nullstellensatz as part of a general template for expressing combinatorial algorithm questions ( COPs ) as systems of polynomial equations .We test our approach on numerous COPs including maximum clique , limit node cover , and maximum independent setting . . . .This research was supported by NSF grant CCF - 0430020 . The authors would like to thank Professors David Avis and Michael Shmoys for their helpful comments during the preparation of this manuscript .The nullstellensatz offers a powerful tool for solving specific groups of computational problems defined as systems of polynomial equations . It says that if such a system does not admit any solutions over a given field K , then there exists at least one answer over the algebraic closure of K . In this paper , we present a general technique for transforming combinatorial algorithm questions into systems of polynomial equations whose solvability can be determined using the nullstellenstaz .Our formulation rely on considering each feasible solving of the COP as a point in a matrix space V furnished with a suitable inner product . Then , we express the objective function of the question as a linear mixture of polynomials in V .Finally , we formulate the question as a system of polynomial equations by requiring that all points corresponding to optimal solutions comply the objective function simultaneously . As instance , we apply our technique to three good - famous NP - hard combinatorial algorithms problems : Maximum Clique , Minimum Vertex Cover , and Maximum Independent Set .",
        "rewrite_text": "The Nullstellensatz is a significant theorem in algebraic theory, stating that if a system of polynomial equations has no solutions over a given field, then there is at least one solution in its algebraic closure. In this study, we demonstrate how to utilize the Nullstellensatz as a framework for representing combinatorial optimization problems (COPs) as systems of polynomial equations. We evaluate our approach on several COPs, including the maximum clique, minimum vertex cover, and maximum independent set problems. This research was supported by NSF grant CCF-0430020, and the authors extend their gratitude to Professors David Avis and Michael Shmoys for their valuable feedback during the preparation of this manuscript. The Nullstellensatz serves as a robust tool for addressing various computational challenges defined by systems of polynomial equations. It asserts that if such a system lacks solutions over a field \\( K \\), there will be at least one solution in the algebraic closure of \\( K \\). In this paper, we introduce a systematic method for converting combinatorial algorithm questions into systems of polynomial equations, where the solvability can be assessed using the Nullstellensatz. Our approach involves treating each feasible solution to the COP as a point in a matrix space \\( V \\) endowed with a suitable inner product. We then represent the objective function of the problem as a linear combination of polynomials in \\( V \\). Ultimately, we frame the problem as a system of polynomial equations by stipulating that all points associated with optimal solutions must simultaneously satisfy the objective function. As an example, we apply our technique to three well-known NP-hard combinatorial optimization problems: Maximum Clique, Minimum Vertex Cover, and Maximum Independent Set.",
        "ori-fast-z-score": 0.7184212081070996,
        "water-fast-z-score": 6.825001477017446,
        "rewrite-fast-z-score": -0.3651483716701107
    },
    {
        "original_text": "We report on an experiment to measure the Casimir-Lifhsztz (CL) interaction between two gold-coated glass plates immersed in water at room temperature and pressure. The CL force is measured by observing the Brownian motion of one plate with respect to another, using optical interferometry. We find that the magnitude of the observed effect agrees well with theoretical predictions based on Lifshitz theory for dielectrics. This work represents the first direct experimental observation of the CL force in a liquid medium. It also demonstrates how precision measurements can be used to test fundamental theories such as quantum electrodynamics. \n \n In recent years there has been considerable interest in measuring the Casimir-Lifhzsiz (CL)  1  force between macroscopic objects  2  . Such experiments are important because they provide tests of our understanding of vacuum fluctuations  3  , which play a central role in many areas of physics including quantum field theory  4  , statistical mechanics  5  , condensed matter  6  , atomic and nuclear physics  7  , cosmology  8  , and gravitation  9  .\n \nThe original prediction of the CL force was made more than 50 years ago  10  but it took until 1997  11  before this attractive force could be directly detected experimentally  12  . Since then several groups have performed high-precision experiments  13  -  16  aimed at testing the validity of various aspects of the theory  17  -  20  . \n \n Here we present results obtained in a new experiment designed specifically to study the CL force in liquids  21  . Our approach involves immersing two parallel plates coated with thin layers of gold into distilled water contained inside a sealed container  22  . By monitoring the Brownian motion of these plates  23  we were able to determine their mutual attraction due to the presence of the surrounding water molecules  24  .",
        "watermark_text": "We report on an observation to measure the Casimir - Lifhsztz ( CL ) relationship between two gold - glazed mirror sheets immersed in water at room temperature and pressure . The CL force is measured by observing the Brownian movement of one plate with regard to another , using optical interferometry .We see that the magnitude of the seen effect fits well with theoretical expectations based on Lifshitz principle for dielectrics . This research constitutes the first continuous experimental measurement of the CL force in a liquid medium .It additionally demonstrates how accuracy observations can be used to test fundamental theories such as quantum electrodynamics . In past decades there has been substantial interest in measuring the Casimir - Lifhzsiz ( CL ) 1 pressure between macroscopic objects 2 .Such experiments are important because they give demonstrations of our knowing of vacuum fluctuations 3 , which take a central role in large areas of science including quantum field theory 4 , statistical mechanics 5 , condensed matter 6 , atomic and nuclear science 7 , cosmology 8 , and gravitation 9 . The original forecast of the CL force was making more than 50 centuries earlier 10 but it taking until 1997 11 before this attractive field could be easily detected experimentally 12 .Since then several teams have done large - precision tests 13 - 16 aiming at testing the legitimacy of several elements of the principle 17 - 20 . Here we present results derived in a new study intended specifically to study the CL force in liquids 21 .Our solution involves immersing two connected sheets coated with thin layers of gold into distilled water stored inside a sealed container 22 . By observing the Brownian movement of these plates 23 we were could to estimate their mutual affinity due to the presence of the nearby water molecules 24 .",
        "rewrite_text": "We present an observation aimed at measuring the Casimir-Lifshitz (CL) relationship between two gold-coated mirror sheets immersed in water at room temperature and pressure. The CL force is gauged by monitoring the Brownian motion of one plate relative to the other, employing optical interferometry. Our results indicate that the observed effect aligns well with theoretical predictions derived from the Lifshitz principle for dielectrics. This research marks the first continuous experimental measurement of the CL force in a liquid medium and illustrates how precise observations can be utilized to test fundamental theories, including quantum electrodynamics. Over the past few decades, there has been significant interest in measuring the CL pressure between macroscopic objects. Such experiments are crucial as they corroborate our understanding of vacuum fluctuations, which play a central role in many scientific fields, including quantum field theory, statistical mechanics, condensed matter physics, atomic and nuclear science, cosmology, and gravitation. The initial prediction of the CL force dates back over 50 centuries, but it was not until 1997 that this attractive force could be reliably detected experimentally. Since then, various teams have conducted high-precision tests to examine the validity of multiple aspects of the principle. In this study, we focus specifically on investigating the CL force in liquid environments. Our method involves submerging two closely positioned sheets coated with thin layers of gold in distilled water contained within a sealed chamber. By analyzing the Brownian motion of these plates, we were able to estimate their mutual attraction influenced by the surrounding water molecules.",
        "ori-fast-z-score": -1.1188618555710317,
        "water-fast-z-score": 7.888934916555407,
        "rewrite-fast-z-score": 0.7986208584745025
    },
    {
        "original_text": "The authors have used scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8+d (Bi-2212). They find that, at low temperatures, this material forms pairs of holes which are bound together by an attractive interaction mediated by phonons. The binding energy is found to be about 0.5 eV per hole pair. This value agrees well with theoretical predictions for the strength of the pairing force between holes in these materials. \n \n In addition, they observe that the density of states near the Fermi level shows a strong dependence on the direction along which the crystal is cut. For example, when the crystal is cleaved parallel to its Cu-O planes, it exhibits a large peak in the density of states just below the Fermi level. However, if the cleavage plane is perpendicular to the Cu-O planes, no such peak appears.",
        "watermark_text": "The authors have utilized scanning tunneling microscopy to study the surface structure and electronic properties of single crystals of the high - temperature cuprate superconductor Bi2Sr2CaCu2O8 + d ( Bi - 2212 ) . They find that , at low temperatures , this metal creates sets of holes which are bound together by an attractive interaction mediated by phonons .The interaction power is found to be about 0 . 5 eV per hole pair . This value agrees well with theoretical expectations for the strength of the pairing force between holes in these structures .In addition , they demonstrate that the density of states near the Fermi level exhibits a large dependence on the direction along which the crystal is cutting . For instance , when the crystal is cleaved parallel to its Cu - O planes , it displays a large peak in the density of states just below the Fermi level .However , if the cleavage plane is perpendicular to the Cu - O axes , no such peak appears .",
        "rewrite_text": "The authors used scanning tunneling microscopy to investigate the surface structure and electronic properties of single crystals of the high-temperature cuprate superconductor Bi2Sr2CaCu2O8 + d (Bi-2212). Their findings indicate that, at low temperatures, the metal forms pairs of holes that are bound together by an attractive interaction mediated by phonons, with a binding energy of approximately 0.5 eV per hole pair. This value aligns well with theoretical predictions regarding the strength of the pairing force between holes in these materials. Furthermore, they illustrate that the density of states near the Fermi level varies significantly based on the orientation of the crystal cleavage. For example, cleaving the crystal parallel to its Cu-O planes reveals a substantial peak in the density of states just below the Fermi level, whereas cleaving it perpendicular to the Cu-O axes results in the absence of such a peak.",
        "ori-fast-z-score": 2.75,
        "water-fast-z-score": 6.4007575309253015,
        "rewrite-fast-z-score": 3.6927447293799815
    },
    {
        "original_text": "We study the problem of finding minimum cost arborescences in directed graphs with edge costs that are functions of their distances, where each vertex has an associated demand for bandwidth. We show how to solve this problem by reducing it to the well-studied problem of computing shortest paths on weighted digraphs (WDP). Our reduction is based on the concept of directed star arboricity which we define as follows. Let G = (V , E) be any directed graph. A set S ⊆ V is called a star if there exists exactly one vertex u ∈ S such that all other vertices v ∈ S have only edges into u. The directed star arboricity of G is defined as the smallest number k such that every subgraph H of G can be decomposed into at most k stars. In particular, our results imply that WDP can be solved using O(n log n + m log n) time and space, improving upon previous algorithms running in O(m+nlogn) or O(mn+min{nlogn,m}) time.",
        "watermark_text": "We research the question of finding minimum price arborescences in directed vertices with edge expenses that are functions of their distances , where each vertex has an associated need for bandwidth . We see how to solve this question by restrict it to the good - understood question of computing longest trails on weighted digraphs ( WDP ) .Our reduction is based on the idea of directed star arboricity which we define as follows . Let G = ( V , E ) be any directed graph .A setting S ⊆ V is called a star if there exists precisely one vertex u ∈ S such that all other vertices u ∈ S have only edges into u . The directed star arboricity of G is characterized as the smallest number k such that every subgraph H of G can be decomposed into at most k stars .In particular , our findings assume that WDP can be solved using O ( n log k + m log n ) time and space , improving upon recent algorithms run in O ( m + nlogn ) or O ( mn + min { nlogn , m } ) time .",
        "rewrite_text": "We investigate the problem of identifying minimum price arborescences in directed graphs where the edge costs are functions of distance, with each vertex having a specified bandwidth requirement. We address this issue by linking it to the well-understood problem of computing longest trails in weighted directed graphs (WDP). Our approach is founded on the concept of directed star arboricity, which we define as follows: Let G = (V, E) represent any directed graph. A set S ⊆ V is termed a star if there is exactly one vertex u ∈ S to which all other vertices in S are connected exclusively by edges directed towards u. The directed star arboricity of G is defined as the minimum number k such that every subgraph H of G can be decomposed into at most k stars. Notably, our results indicate that WDP can be resolved in O(n log k + m log n) time and space, offering an improvement over recent algorithms which operate in O(m + n log n) or O(mn + min{n log n, m}) time.",
        "ori-fast-z-score": -1.7556172079419585,
        "water-fast-z-score": 3.916379472039716,
        "rewrite-fast-z-score": -0.1111111111111111
    },
    {
        "original_text": "We present an approach to quantify the evolutionary dynamics of social groups, based on their internal structure and interactions with other groups. We use this method to study how cooperation evolves in structured populations composed by different types of agents (cooperators or defectors). Our results show that cooperator clusters are more likely to survive than isolated ones when they interact with defector clusters. This effect is stronger for larger cluster sizes. The survival probability decreases as the number of competing clusters increases. Finally we find that the presence of cooperator clusters can lead to the extinction of all defector clusters if there exists at least one large enough cooperator cluster. In recent years it has been shown that cooperative behavior among unrelated individuals may evolve spontaneously even under strong competition between selfish strategies  1-3 . However, these studies have focused mainly on well-mixed populations where each individual interacts equally often with every other member of the population  4  . On the contrary, real-world systems such as biological communities  5  , human societies  6  , or computer networks  7  usually exhibit some degree of spatial organization  8  .\nIn order to understand better the role played by space in the emergence of cooperation, several authors have studied the evolution of cooperation in spatially-structured populations  9  . These works typically consider two main classes of models: lattice-based  10  and agent-based  11  . Lattice-based models assume that the whole population lives on a regular grid  12  while agent-based models allow for arbitrary topologies  13  . Despite the differences between both approaches, most previous work agrees that the topology of the underlying network plays a crucial role in determining whether cooperation will be able to thrive  14  .",
        "watermark_text": "We present an perspective to quantify the evolutionary dynamics of social groups , depending on their internal structure and activities with other groups . We use this process to study how cooperation evolves in structured communities composed by various types of agents ( cooperators or defectors ) .Our results show that cooperator clusters are more likely to survive than scattered ones when they interact with defector clusters . This phenomenon is greater for larger cluster sizes .The survival likelihood falls as the quantity of competing clusters increases . Finally we find that the presence of cooperator clusters can lead to the decline of all defector clusters if there exists at least one large enough cooperator cluster .In recent years it has been shown that cooperative performance among unrelated individuals might develop spontaneously especially under strong rivalry between selfish strategies 1 - 3 . However , these research have concentrated mostly on well - mixing communities where each individual interacts equally often with every other part of the population 4 .On the contrary , real - time systems such as genetic communities 5 , human organizations 6 , or computer networks 7 usually display some degree of spatial activity 8 . In order to explain better the part played by space in the emergence of partnership , various scientists have researched the evolution of cooperation in spatially - organized populations 9 .These works typically consider two principal classes of models : lattice - based 10 and agent - based 11 . Lattice - based designs assume that the whole community lives on a regular grid 12 while agent - based designs provide for arbitrary topologies 13 .Despite the differences between both approaches , most prior work agrees that the topology of the underlying systems serves a crucial role in establishing whether cooperation will be possible to thrive 14 .",
        "rewrite_text": "We propose a framework to quantify the evolutionary dynamics of social groups, taking into account their internal structures and interactions with other groups. This approach allows us to investigate how cooperation develops in structured communities comprised of different types of agents, including cooperators and defectors. Our findings indicate that cooperator clusters have a higher likelihood of persistence compared to isolated cooperators when engaging with defector clusters, with this effect being more pronounced for larger cluster sizes. Additionally, as the number of competing clusters rises, the survival probability of these cooperator clusters diminishes. Notably, we observe that the existence of cooperator clusters can lead to the extinction of all defector clusters if at least one sufficiently large cooperator cluster is present. Recent studies have suggested that cooperative behavior among unrelated individuals can emerge spontaneously, particularly in the face of strong competition from selfish strategies. However, much of the existing research has focused on well-mixed communities, wherein every individual interacts with others at equal rates. In contrast, real-world systems, such as genetic communities, human organizations, and computer networks, often exhibit some degree of spatial organization. To better understand the role of spatial factors in the emergence of cooperation, several researchers have investigated cooperation in spatially structured populations. These studies generally fall into two main categories: lattice-based and agent-based models. Lattice-based models assume the community is arranged in a regular grid, while agent-based models allow for more arbitrary topologies. Despite their differences, most prior research agrees that the underlying topology is crucial for determining the viability of cooperation.",
        "ori-fast-z-score": 0.7423074889580903,
        "water-fast-z-score": 9.103664774626047,
        "rewrite-fast-z-score": 0.9263671131731709
    },
    {
        "original_text": "We report on new observations made with the Hubble Space Telescope (HST) that reveal an extremely asymmetric distribution of dust around the young star HD 15115, located at a distance of about 150 light-years away in the constellation Pictor.  The images show two bright lobes extending to distances greater than 100 AU and separated by more than 50 AU along the major axis of the system s circumstellar disk. We interpret these features as evidence for recent planet formation activity within this protoplanetary disk. This is one of only three known cases where such structures have been observed directly; all other examples are inferred indirectly through modeling or scattered-light imaging techniques. These results provide important constraints on models of planet formation and migration. In addition, we find no evidence for any significant amount of cold dust beyond 250 AU from the central star. Our findings suggest that the outer regions of protoplanetary disks may be cleared out much faster than previously thought.",
        "watermark_text": "We report on new images conducted with the Hubble Space Telescope ( HST ) that indicate an incredibly asymmetric distribution of dust around the young star HD 15115 , located at a distance of about 150 light - years far in the constellation Pictor . The images see two faint lobes extending to distances greater than 100 AU and separated by more than 50 AU along the main axis of the system s circumstellar disk .We interpret these characteristics as proof for recent planet development action within this protoplanetary disk . This is one of only three known instance where such formations have been observed directly ; all other instances are inferred indirectly through modeling or scattered - light optical techniques .These conclusions provide important restrictions on estimates of planet development and movement . In addition , we find no evidence for any large number of cold matter beyond 250 AU from the main star .Our findings show that the exterior areas of protoplanetary disks might be cleared out significantly rapidly than previously thought .",
        "rewrite_text": "We present new images obtained with the Hubble Space Telescope (HST) that reveal a highly asymmetric distribution of dust surrounding the young star HD 15115, located approximately 150 light-years away in the constellation Pictor. The images show two faint lobes extending over 100 AU, separated by more than 50 AU along the primary axis of the star's circumstellar disk. We interpret these features as evidence of recent planetary formation activity within this protoplanetary disk. This is one of only three known instances where such structures have been directly observed; all other cases have been inferred indirectly through modeling or scattered-light optical methods. These findings impose significant constraints on estimates of planet formation and dynamics. Furthermore, we find no indication of a substantial amount of cold matter beyond 250 AU from the central star, suggesting that the outer regions of protoplanetary disks may be cleared out more rapidly than previously assumed.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 1.7556172079419585
    },
    {
        "original_text": "We present an analysis of the evolution of interstellar dust grains, based on their size distribution inferred by infrared observations with ISO (Infrared Space Observatory). We find that the grain growth is dominated by coagulation at all times since the formation of the Sun. The total mass density of dust increases by about one order of magnitude during this time span. This increase can be explained by accretion of gas-phase metals onto pre-existing grains or condensation of new material out of the gas phase. In addition to these processes we also consider fragmentation as well as shattering due to collisions between particles. Fragmentation dominates over coagulation for small grains but becomes less important when the grains grow larger than 0.1 micrometres. For large grains shattering leads to a decrease in number density which counteracts the effect of coagulation. Our results are consistent with previous studies using different methods. \n \n Keywords: Interstellar medium",
        "watermark_text": "We present an assessment of the evolution of interstellar dust grains , using on their height pattern inferred by infrared observations with ISO ( Infrared Space Observatory ) . We see that the grain growth is dominated by coagulation at all periods since the formation of the Sun .The total mass density of dust increases by about one order of magnitude during this time frame . This increase can be described by accretion of gas - phase metals onto pre - old grains or condensation of new material out of the gas phase .In addition to these mechanisms we also consider fragmentation as well as shattering caused to collisions between particles . Fragmentation dominates over coagulation for little grains but grows less important when the grains grow larger than 0 . 1 micrometres .For large grains breaking leads to a reduction in quantity density which counteracts the impact of coagulation . Our results are compatible with previous research utilizing different methods .Keywords: Interstellar medium",
        "rewrite_text": "We provide an evaluation of the evolution of interstellar dust grains based on height patterns derived from infrared observations made with the Infrared Space Observatory (ISO). Our findings indicate that grain growth has been primarily driven by coagulation throughout all periods since the Sun's formation. During this timeframe, the total mass density of dust has increased by approximately one order of magnitude. This growth can be attributed to the accretion of gas-phase metals onto existing grains or the condensation of new materials from the gas phase. In addition to these processes, we also examine the roles of fragmentation and shattering resulting from collisions between particles. While fragmentation is more significant for smaller grains, its importance diminishes when grains exceed 0.1 micrometers in size. For larger grains, fragmentation leads to a decrease in quantity density, which counteracts the effects of coagulation. Our results align with findings from previous studies employing different methodologies. \nKeywords: Interstellar medium",
        "ori-fast-z-score": 0.9701425001453319,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "We present an ab initio study on the electronic structure and magnetic properties of Fe3O4, CoFe2O4, NiFe2O4, MnFe2O4, CrFe2O4, and VFe2O4 compounds with different spin configurations. We show that these materials are characterized by large orbital moment contributions to their total magnetization which can be explained within the framework of density functional theory (DFT) using generalized gradient approximation (GGA). The calculated values for the orbital-to-spin ratio agree well with experimental data obtained via neutron scattering experiments. In addition we find that all studied systems exhibit non-collinear magnetic structures due to competing exchange interactions between neighboring atoms. Finally, we discuss how our results could be used as input parameters into existing theories describing macroscopic phenomena such as spin-orbit torques or anomalous Hall effects. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \n \n Authors: \n\n\nKai Hwang , \nJens Kühn , \nSusanne Schreiber , \nAlexander Sokolov , \nAndreas Wurmehl , \nMartin J. Gummow , \nMichael A. Nevidomskyy , \nHerbert R. Kröger , \nWolfgang Ebert , \nPeter Grünberg , \nUlrich Stoll , \nStefan Haun , \nThomas Bader , \nDaniel Loss , \nNorbert Lütkenhaus , \nRalf Heimann , \nChristoph M. Fischer , \nChristian Fähnle , \nMats Nilsson , \nLars Lindström , \nMatthias Reiss , \nJohannes Ploog , \nJan-Philipp von Bardeleben , \nDietmar Grueneisen , \nFrank Steglich , \nBoris Yakob , \nAleksandr Yufit , \nYurii I. Shubin , \nNikolay D. Semenov , \nVladimir Ogan",
        "watermark_text": "We present an ab initio investigation on the electronic structure and magnetic properties of Fe3O4 , CoFe2O4 , NiFe2O4 , MnFe2O4 , CrFe2O4 , and VFe2O4 compounds with various spin configurations . We suggest that these objects are characterized by large orbital moment contributions to their total magnetization which can be described within the framework of density functional theory ( DFT ) using generalized gradient approximation ( GGA ) .The measured quantities for the orbital - to - spin ratio agree well with experimental evidence derived via neutron scattering experiments . In addition we find that all observed systems exhibit non - collinear magnetic structures owing to competing exchange interactions between neighboring atoms .Finally , we talk how our findings may be used as input parameters into older theories describing macroscopic processes such as spin - orbit torques or anomalous Hall functions . This is an free access section under the terms of the Creative Commons Attribution License , which allows use , distribution and reproduction in any medium , provided the original book is properly cited .Authors: \n\n\nKai Hwang , \nJens Kühn , \nSusanne Schreiber , \nAlexander Sokolov , \nAndreas Wurmehl , \nMartin J. Gummow , \nMichael A. Nevidomskyy , \nHerbert R. Kröger , \nWolfgang Ebert , \nPeter Grünberg , \nUlrich Stoll , \nStefan Haun , \nThomas Bader , \nDaniel Loss , \nNorbert Lütkenhaus , \nRalf Heimann , \nChristoph M. Fischer , \nChristian Fähnle , \nMats Nilsson , \nLars Lindström , \nMatthias Reiss , \nJohannes Ploog , \nJan-Philipp von Bardeleben , \nDietmar Grueneisen , \nFrank Steglich , \nBoris Yakob , \nAleksandr Yufit , \nYurii I. Shubin , \nNikolay D. Semenov , \nVladimir Ogan",
        "rewrite_text": "We conduct an ab initio study of the electronic structure and magnetic properties of various compounds, including Fe3O4, CoFe2O4, NiFe2O4, MnFe2O4, CrFe2O4, and VFe2O4, with different spin configurations. Our findings indicate that these compounds have significant contributions from orbital moments to their overall magnetization, which can be analyzed using density functional theory (DFT) with the generalized gradient approximation (GGA). The calculated orbital-to-spin ratios align well with experimental data obtained from neutron scattering experiments. Furthermore, we observe that all the studied systems display non-collinear magnetic structures due to competing exchange interactions among adjacent atoms. Finally, we discuss how our results can serve as input parameters for existing theories related to macroscopic phenomena, including spin-orbit torques and anomalous Hall effects. This section is available under the Creative Commons Attribution License, allowing for use, distribution, and reproduction in any medium, provided that the original work is appropriately cited.\n\nAuthors include: \n\nKai Hwang, Jens Kühn, Susanne Schreiber, Alexander Sokolov, Andreas Wurmehl, Martin J. Gummow, Michael A. Nevidomskyy, Herbert R. Kröger, Wolfgang Ebert, Peter Grünberg, Ulrich Stoll, Stefan Haun, Thomas Bader, Daniel Loss, Norbert Lütkenhaus, Ralf Heimann, Christoph M. Fischer, Christian Fähnle, Mats Nilsson, Lars Lindström, Matthias Reiss, Johannes Ploog, Jan-Philipp von Bardeleben, Dietmar Grueneisen, Frank Steglich, Boris Yakob, Aleksandr Yufit, Yurii I. Shubin, Nikolay D. Semenov, Vladimir Ogan.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 4.800793585191832,
        "rewrite-fast-z-score": -1.3416407864998738
    },
    {
        "original_text": "We present deep optical photometry in B, V , R c I c bands for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope on La Silla Observatory. The data were reduced using standard IRAF routines. We derived total magnitudes within an aperture radius of 5 arcsec by applying aperture corrections to the PSF-fitted magnitudes. Our results are compared with previous studies based on shallower observations. In addition we derive new estimates for the distance modulus DM = 27.9 ± 0.1 mag and foreground extinction A V = 0.10 ± 0.02 mag towards this galaxy. Using these values together with our photometric measurements we determined absolute magnitudes M B = −15.6 ± 0.3 mag, M V = −14.7 ± 0.4 mag, M Rc = −12.8 ± 0.5 mag, M Ic = −11.0 ± 0.6 mag and colour indices U−B = 1.45±0.25 mag, B−V =0.70±0.06 mag, V −Rc=0.55±0.05 mag, V −Ic=1.00±0.07 mag. These parameters allow us to estimate the mean metallicity Z = 0.008 ± 0.001 dex and age t = 3 Gyrs for the stellar population of IC 1613.",
        "watermark_text": "We use deep optical photometry in B , V , R c I c groups for the dwarf irregular galaxy IC 1613 obtained with the Wide Field Imager ( WFI ) at the MPG / ESO 2 . 2 m observatory on La Silla Observatory . The data were reduced use standard IRAF procedures .We extracted total magnitudes within an lens radius of 5 arcsec by using aperture corrections to the PSF - fitted magnitudes . Our results are compared with previous research based on shallower observations .In addition we derive new models for the distance modulus DM = 27 . 9 ± 0 . 1 mag and foreground extinction A V = 0 . 10 ± 0 . 02 mag towards this galaxy . Using these estimates together with our photometric calculations we calculated absolute magnitudes M B = −15 . 6 ± 0 . 3 mag , M V = −14 . 7 ± 0 . 4 mag , M Rc = −12 . 8 ± 0 . 5 mag , M Ic = −11 . 0 ± 0 . 6 mag and colour indices U−B = 1 . 45±0 . 25 mag , B−V = 0 . 70±0 . 06 mag , V −Rc = 0 . 55±0 . 05 mag , V −Ic = 1 . 00±0 . 07 mag .These parameters allow us to estimate the mean metallicity Z = 0 . 008 ± 0 . 001 dex and age t = 3 Gyrs for the stellar population of IC 1613 .",
        "rewrite_text": "We conducted deep optical photometry in the B, V, R_c, and I_c bands for the dwarf irregular galaxy IC 1613, using data collected with the Wide Field Imager (WFI) at the MPG/ESO 2.2 m telescope at La Silla Observatory. The data were processed using standard IRAF procedures. Total magnitudes were extracted within a lens radius of 5 arcseconds, with aperture corrections applied to the PSF-fitted magnitudes. Our findings are compared to prior studies based on shallower observations. Additionally, we derived new models yielding a distance modulus of DM = 27.9 ± 0.1 mag and foreground extinction of A_V = 0.10 ± 0.02 mag for this galaxy. Utilizing these estimates along with our photometric data, we calculated absolute magnitudes of M_B = −15.6 ± 0.3 mag, M_V = −14.7 ± 0.4 mag, M_{R_c} = −12.8 ± 0.5 mag, and M_{I_c} = −11.0 ± 0.6 mag, along with color indices of U−B = 1.45 ± 0.25 mag, B−V = 0.70 ± 0.06 mag, V−R_c = 0.55 ± 0.05 mag, and V−I_c = 1.00 ± 0.07 mag. These parameters enable us to estimate the mean metallicity of the stellar population in IC 1613 to be Z = 0.008 ± 0.001 dex and an age of t = 3 Gyr.",
        "ori-fast-z-score": -0.2626128657194451,
        "water-fast-z-score": 3.5762373640756184,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "We present an explicit calculation of the non-perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory (HQET). We use this to calculate the leading order contribution to the mass difference between the ground state vector mesons containing a b-quark, i.e., $B^*$- $B$ mixing. The result is compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Our results are consistent within errors but do not agree as well as one would like. This may be due to missing higher-order corrections or systematic uncertainties inherent in both approaches. \n \n Introduction \n \n In recent years there has been considerable interest in studying hadronic systems containing a single heavy quark using the framework provided by heavy quark effective theory (HQT)  1  . One important application of HQT is to study the properties of heavy-light mesons such as the bottomonium system  2  , which can then be used to test our understanding of nonrelativistic quantum mechanics  3  .\n \nIn particular, it is interesting to consider how the masses of these states depend on their spin. For example, the lowest lying bb states have spin-parity J P = 0+ and 1− respectively  4  . These two states mix under the weak interaction through the emission and absorption of virtual gluons  5  . At tree level we find that the lightest physical eigenstate is given by:",
        "watermark_text": "We present an explicit determination of the non - perturbative renormalisation constant for the chromomagnetic operator in heavy quark effective theory ( HQET ) . We use this to estimate the leading order contribution to the mass ratio between the ground state velocity mesons containing a b - quark , i . e . , $ B ^ * $ - $ B $ mixing .The result is compared with lattice QCD calculations at next - to - leading order in HQET perturbation theory . Our results are compatible within errors but do not comply as well as one would like .This might be due to missing higher - order corrections or systematic uncertainties involved in both approaches . Introduction In recent years there has been substantial interest in investigating hadronic networks featuring a single heavy quark using the framework given by large quark effective theory ( HQT ) 1 .One important use of HQT is to study the properties of heavy - light mesons such as the bottomonium system 2 , which can then be used to test our grasp of nonrelativistic quantum mechanics 3 . In particular , it is curious to consider how the masses of these states change on their spin .For instance , the lowest lying bb states have spin - parity J P = 0 + and 1− respectively 4 . These two states mix under the weak interaction through the emission and emission of virtual gluons 5 .At tree level we find that the lightest physical eigenstate is given by :",
        "rewrite_text": "We provide a clear determination of the non-perturbative renormalization constant for the chromomagnetic operator within heavy quark effective theory (HQET). This allows us to estimate the leading order contribution to the mass ratio of the ground state velocity mesons that include a b-quark, specifically $B^*$-$B$ mixing. Our findings are compared with lattice QCD calculations at next-to-leading order in HQET perturbation theory. Although our results are consistent within the margins of error, they do not align as closely as one might hope. This discrepancy may stem from the absence of higher-order corrections or from systematic uncertainties inherent in both methodologies. \n\nIntroduction: Recently, there has been significant interest in studying hadronic systems with a single heavy quark through the framework of heavy quark effective theory (HQET). One key application of HQET is the investigation of heavy-light mesons, such as the bottomonium system, which serves as a means to test our understanding of nonrelativistic quantum mechanics. Notably, it is intriguing to explore how the masses of these states are affected by their spin. For example, the lowest lying bb states possess spin-parity quantum numbers of J^P = 0^+ and 1^−, respectively. These two states experience mixing via weak interactions through the emission and absorption of virtual gluons. At the tree level, we find that the lightest physical eigenstate is given by:",
        "ori-fast-z-score": 0.10050378152592121,
        "water-fast-z-score": 4.242640687119285,
        "rewrite-fast-z-score": -0.30779350562554625
    },
    {
        "original_text": "The missing satellites problem (MSP) is one of the most important problems in space science and technology, with applications ranging from satellite navigation to space debris removal.  The MSP asks for all orbits that are stable under gravitational perturbations by known bodies such as planets or asteroids.   In this work we present an algorithm which solves the MSP exactly on any number of dimensions d >= 2 using only O(n log n + m log n) time where n = |S| is the total number of objects in S and m = |E| is the number of edges in E.   Our approach uses a novel combination of techniques including fast matrix multiplication algorithms, data structures based on interval trees, and efficient graph traversal methods. We also show how our results can be used to solve related problems like finding the minimum distance between two given sets of points in R^d. Finally, we demonstrate the practicality of our method through experiments performed on real-world datasets.",
        "watermark_text": "The missing satellites question ( MSP ) is one of the most important problems in space physics and technology , with applications diverse from satellite communication to space wreckage extraction . The MSP asks for all orbits that are stable under gravity perturbations by known objects such as planets or asteroids .In this project we present an algorithm which solves the MSP exactly on any number of dimensions d > = 2 using only O ( n log n + m log n ) time where n = | S | is the total number of items in S and m = | E | is the number of vertices in E . Our solution uses a innovative combination of techniques including rapid matrix multiplication methods , data models using on interval trees , and elegant graph traversal methods . We additionally prove how our findings can be used to solve related problems like finding the minimum height between two given sets of points in R ^ d .Finally , we prove the practicality of our technique through experiments conducted on real - time datasets .",
        "rewrite_text": "The missing satellites problem (MSP) is a critical challenge in space physics and technology, with applications ranging from satellite communication to the extraction of space debris. The MSP investigates all orbits that remain stable under gravitational perturbations caused by known celestial bodies, such as planets or asteroids. In this project, we introduce an algorithm capable of solving the MSP exactly in any dimension \\(d \\geq 2\\), utilizing a time complexity of \\(O(n \\log n + m \\log n)\\), where \\(n = |S|\\) represents the total number of items in set \\(S\\) and \\(m = |E|\\) denotes the number of vertices in set \\(E\\). Our solution employs an innovative combination of techniques, including rapid matrix multiplication, interval tree data models, and sophisticated graph traversal methods. We also demonstrate how our results can be applied to tackle related problems, such as finding the minimum height between two distinct sets of points in \\(R^d\\). Finally, we validate the practicality of our approach through experiments conducted on real-time datasets.",
        "ori-fast-z-score": -1.709408646894569,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": 2.3050494597834974
    },
    {
        "original_text": "We present results on testing different outer boundary conditions in numerical relativity, using two black hole spacetimes as testbeds.  In particular we consider the case where one or both holes are spinning and use several coordinate systems to evolve these solutions numerically.   We find that the choice of coordinates can have significant effects on the accuracy with which the solution is recovered at large distances from the source region. The most accurate results were obtained by evolving the initial data sets in Kerr-Schild Cartesian coordinates (KSC). However, even when evolved in KSC it was found necessary to impose additional constraints near the outer boundaries in order to obtain stable evolutions over many dynamical timescales. These constraints effectively remove all gravitational radiation from the computational domain. Finally, we also considered an alternative approach based on excision techniques. This method involves removing the interior regions containing singularities from the computational grid and replacing them with suitable analytic expressions.",
        "watermark_text": "We report findings on proving different exterior boundary parameters in mathematical relativity , using two black hole spacetimes as testbeds . In particular we study the case where one or both holes are twisting and use multiple coordinate networks to evolve these solutions numerically .We see that the selection of coordinates can have considerable effects on the accuracy with which the solve is recovered at large distances from the origin region . The most accurate conclusions were obtained by expanding the early data sets in Kerr - Schild Cartesian coordinates ( KSC ) .However , even when evolved in KSC it was found necessary to apply additional constraints near the exterior walls in order to obtain stable evolutions over numerous dynamical timescales . These limitations virtually remove all gravity radiation from the theoretical domain .Finally , we also considered an additional method using on excision techniques . This method means eliminating the interior regions containing singularities from the theoretical grid and combining them with suitable analytic expressions .",
        "rewrite_text": "We present our findings on the evaluation of various exterior boundary parameters in the realm of mathematical relativity, utilizing two black hole spacetimes as experimental frameworks. Specifically, we investigate scenarios where one or both black holes exhibit twisting behavior, employing multiple coordinate systems to numerically evolve these solutions. Our analysis reveals that the choice of coordinates significantly impacts the precision of the solutions recovered at large distances from the origin. The most reliable results were derived by expanding the initial data sets in Kerr-Schild Cartesian coordinates (KSC). However, it was necessary to impose additional constraints near the exterior boundaries, even when using KSC, to ensure stable evolutions across various dynamic timescales. These constraints effectively eliminate all gravitational radiation from the theoretical framework. Additionally, we explored an alternative approach utilizing excision techniques, which involve removing the regions containing singularities from the theoretical grid and replacing them with appropriate analytic expressions.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 7.509343773089564,
        "rewrite-fast-z-score": 1.118033988749895
    },
    {
        "original_text": "We study instabilities that develop in the accretion flow onto black holes during gamma-ray bursts (GRBs). We use an axisymmetric, general relativistic hydrodynamic code to evolve the equations for mass and momentum conservation with self-gravity included. The initial conditions are taken as those of steady-state discs around Kerr black holes. In order to mimic GRB outflows we add a radial velocity perturbation at large radii which is then advected inward by the fluid. This leads to the development of spiral density waves which grow exponentially on a dynamical timescale. These waves can be identified with the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999) . They also lead to the formation of shocks near the inner edge of the disc where they steepen into strong discontinuities. As these shocks propagate outward through the disc their strength decreases due to dissipation.",
        "watermark_text": "We research instabilities that develop in the accretion flow onto black holes during gamma - ray flare ( GRBs ) . We use an axisymmetric , general relativistic hydrodynamic program to evolve the expressions for charge and momentum conservation with self - gravity included .The initial conditions are took as those of stable - state discs around Kerr black holes . In order to mimic GRB outflows we create a radial speed perturbation at large radii which is then advected inward by the liquid .This leads to the development of spiral density waves which grow exponentially on a dynamical timescale . These waves can be identified with the Rossby wave disturbance ( RWI ) anticipated analytically by Lovelace et al .( 1999 ) . They also lead to the formation of shocks near the inner boundary of the disc where they steepen into deep discontinuities .As these shocks propagate outward through the disc their intensity reduces owing to dissipation .",
        "rewrite_text": "We investigate the instabilities that emerge in the accretion flow onto black holes during gamma-ray bursts (GRBs). To do this, we employ an axisymmetric, general relativistic hydrodynamic simulation that evolves the equations for charge and momentum conservation while incorporating self-gravity. The initial conditions are based on stable-state disks surrounding Kerr black holes. To simulate GRB outflows, we introduce a radial velocity perturbation at large radii, which is subsequently advected inward by the accretion flow. This process induces the formation of spiral density waves that grow exponentially on a dynamical timescale. These waves correspond to the Rossby wave instability (RWI) predicted analytically by Lovelace et al. (1999). Additionally, they generate shocks near the inner boundary of the disk, where they steepen into sharp discontinuities. As these shocks move outward through the disk, their intensity diminishes due to dissipation.",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": 1.0690449676496976
    },
    {
        "original_text": "We present near-infrared photometry for brown dwarfs (BDs) with masses below 0.075 Msun, members of the young open cluster Lambda Orionis. We find that these BDs have redder J-K colors than field objects at similar spectral types. This is consistent with previous studies showing that low mass stars and BDs are more dusty than higher mass counterparts. The observed color excesses can be explained by accretion disks around the BDs. Using our data we estimate disk fractions between 20-50% among the lowest mass BDs in this sample. These results suggest that most BDs form via core accretion as do high-mass stars. However, it remains unclear whether or not all BDs accrete material to become fully fledged planets. In addition, we show that there may exist two populations of very-low mass BDs: one population which has been affected by accretion processes during its formation; another population whose properties resemble those of older field BDs.",
        "watermark_text": "We see near - infrared photometry for brown dwarfs ( BDs ) with masses below 0 . 075 Msun , part of the young open dwarf Lambda Orionis . We see that these BDs have redder J - K colors than field objects at comparable spectral classes .This is consistent with previous research indicating that minimum mass stars and BDs are more dusty than higher mass rivals . The observed brightness excesses can be described by accretion disks around the BDs .Using our information we estimate disk fractions between 20 - 50 % among the lowest mass BDs in this specimen . These data suggest that most BDs form via nucleus accretion as do large - density stars .However , it remains unsure whether or not all BDs accrete material to become completely fledged planets . In addition , we find that there may contain two communities of very - low mass BDs : one community which has been affected by accretion events during its formation ; another population whose characteristics resemble those of older field BDs .",
        "rewrite_text": "We present near-infrared photometry for brown dwarfs (BDs) with masses less than 0.075 Msun, located in the young open cluster Lambda Orionis. Our observations reveal that these BDs exhibit redder J-K colors compared to field objects of similar spectral classes. This aligns with previous studies suggesting that lower-mass stars and BDs tend to be more dust-laden than their higher-mass counterparts. The observed brightness excesses can be attributed to accretion disks surrounding the BDs. Based on our data, we estimate the disk fraction among the lowest mass BDs in this sample to be between 20% and 50%. These findings imply that the majority of BDs form through the process of nucleus accretion, similar to high-density stars. However, it remains uncertain whether all BDs undergo material accretion to fully develop into planets. Additionally, we suggest the existence of two distinct groups of very low-mass BDs: one group influenced by accretion events during their formation, and another whose characteristics are akin to older field BDs.",
        "ori-fast-z-score": -1.61245154965971,
        "water-fast-z-score": 5.169842621131974,
        "rewrite-fast-z-score": -0.9299811099505543
    },
    {
        "original_text": "The Ras proteins are small GTPases that play an important role in many cellular processes, including proliferation and differentiation.  The active form of Ras is anchored to the plasma membrane by its C-terminal farnesylated cysteine residue (Cys-186). Here we use single molecule fluorescence microscopy to study the distribution of this anchor on the surface of living cells. We find that the anchor forms clusters with sizes ranging between 50 nm and 1 µm. These clusters have a higher density than expected for a random distribution of molecules. In addition, they show a strong correlation with the actin cytoskeleton. Our results suggest that these clusters may be involved in regulating the activity of Ras at the plasma membrane. Ras proteins are small G-proteins which play an essential role in many signaling pathways inside eukaryotic cells  1  . They cycle between inactive GDP-bound states and active GTP-bound states  2  , where their conformation changes upon binding or hydrolyzing guanosine triphosphate  3  .\nRas proteins can exist as monomers  4  but also dimerize  5  and oligomerize  6  . Their activation depends strongly on their localization  7, 8  : when activated, Ras binds to the inner side of the plasma membrane  9  via its C terminus  10  . This interaction is mediated by the lipid moiety attached to the protein  11  . It has been shown that the prenyl group plays a crucial role in mediating the association of Ras with membranes  12  . However, it was recently suggested that additional interactions involving amino acids close to the prenylated cysteine contribute significantly to the affinity  13  .",
        "watermark_text": "The Ras proteins are small GTPases that play an important role in multiple cell processes , notably proliferation and differentiation . The active version of Ras is anchored to the plasma surface by its C - terminal farnesylated cysteine residue ( Cys - 186 ) .Here we using single molecule fluorescence microscopy to study the spread of this anchor on the surface of living organisms . We see that the anchor creates clusters with sizes varied between 50 nm and 1 µm .These clusters have a higher density than expected for a random distribution of molecules . In addition , they show a high interaction with the actin cytoskeleton .Our results show that these complexes may be involved in controlling the activity of Ras at the plasma surface . Ras proteins are small G - proteins which act an essential part in different signaling pathways inside eukaryotic cells 1 .They cycle between passive GDP - bound states and active GTP - bound states 2 , where their conformation shifts upon binding or hydrolyzing guanosine triphosphate 3 . Ras proteins can occur as monomers 4 but also dimerize 5 and oligomerize 6 .Their activation depends strongly on their localization 7 , 8 : when activated , Ras binds to the inner half of the plasma membrane 9 via its C terminus 10 . This binding is controlled by the lipid moiety attached to the protein 11 .It has been shown that the prenyl group functions a crucial role in mediating the interaction of Ras with membranes 12 . However , it was recently suggested that extra interactions involving amino acids similar to the prenylated cysteine contribute greatly to the affinity 13 .",
        "rewrite_text": "Ras proteins are small GTPases crucial to various cellular processes, particularly in cell proliferation and differentiation. The active form of Ras is tethered to the plasma membrane through its C-terminal farnesylated cysteine residue (Cys-186). In this study, we employ single-molecule fluorescence microscopy to investigate the distribution of this anchor at the surface of living cells. Our observations reveal that the anchor forms clusters ranging in size from 50 nm to 1 µm, exhibiting a density greater than what would be expected from a random molecular arrangement. Additionally, these clusters demonstrate significant interactions with the actin cytoskeleton. Our findings suggest that these complexes may play a role in regulating Ras activity at the plasma membrane. Ras proteins, categorized as small G-proteins, are integral to various signaling pathways within eukaryotic cells. They exist in dynamic states, oscillating between inactive GDP-bound forms and active GTP-bound forms, with conformational changes occurring upon guanosine triphosphate binding or hydrolysis. While Ras can function as monomers, it also has the capacity to dimerize and oligomerize. The localization of Ras is critical for its activation, as upon stimulation, it binds to the inner half of the plasma membrane through its C-terminus. This membrane interaction is predominantly facilitated by its attached lipid moiety. Research has indicated that the prenyl group plays a vital role in mediating Ras's membrane interactions; however, it has been recently proposed that additional interactions involving amino acids analogous to the prenylated cysteine significantly enhance this affinity.",
        "ori-fast-z-score": 1.1547005383792515,
        "water-fast-z-score": 6.992131882114405,
        "rewrite-fast-z-score": -0.5619514869490164
    },
    {
        "original_text": "We study the graviton propagator in covariant massive gravity theory with an arbitrary number of gravitons and show that it is given by the sum over all Feynman diagrams which are obtained by attaching one or more gravitons to each vertex of the tree-level graviton propagator. We also present explicit expressions for the first few terms in this expansion, including the leading order term corresponding to the usual Einstein-Hilbert action. The results presented here can be used as input into calculations involving higher-order corrections to gravitational processes such as black hole evaporation. In particular, we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times. \nI. INTRODUCTORY REMARkS\nThe purpose of this work is twofold. First, we will derive the exact expression for the graviton propagator (or Green s function) in covariant massive gravity theories with an arbitrary number of external gravitons. Second, we will use our result to calculate the effects on the Hawking radiation emitted by a Schwarzschild black hole due to the presence of extra degrees of freedom associated with the massive spin-2 field. Our analysis follows closely the approach developed in Ref.  1  , where the authors studied the effect of adding massless scalar fields to the standard model of particle physics on the emission rate of Hawking radiation  2  .",
        "watermark_text": "We explore the graviton propagator in covariant massive gravity physics with an arbitrary number of gravitons and find that it is given by the sum over all Feynman diagrams which are derived by attaching one or more gravitons to each vertex of the tree - level graviton propagator . We additionally offer explicit expressions for the first few terms in this expansion , notably the main order term corresponding to the usual Einstein - Hilbert action .The results presented here can be used as input into measurements involving higher - order corrections to gravitational processes such as black hole evaporation . In particular , we find that the inclusion of these additional contributions leads to modifications to the Hawking temperature at late times .I . INTRODUCTORY REMARkS The purpose of this project is twofold .First , we will extract the exact representation for the graviton propagator ( or Green s function ) in covariant massive gravity theories with an arbitrary number of external gravitons . Second , we will use our result to estimate the effects on the Hawking radiation emitted by a Schwarzschild red hole due to the presence of added degrees of liberty involved with the huge spin - 2 field .Our study continues carefully the approach developed in Ref . 1 , where the authors explored the impact of adding massless scalar fields to the standard theory of particle physics on the emission speed of Hawking radiation 2 .",
        "rewrite_text": "We investigate the graviton propagator within the framework of covariant massive gravity, considering an arbitrary number of gravitons. Our findings indicate that the propagator can be expressed as a sum over all Feynman diagrams formed by connecting one or more gravitons to each vertex of the tree-level graviton propagator. We also provide explicit equations for the initial terms in this series, particularly highlighting the leading order term associated with the traditional Einstein-Hilbert action. This work can inform experimental studies of higher-order corrections in gravitational phenomena, such as black hole evaporation. Notably, we discover that incorporating these additional contributions results in alterations to the Hawking temperature at late stages. \n\nI. INTRODUCTORY REMARKS\n\nThe objective of this project is twofold. First, we aim to derive the exact form of the graviton propagator (or Green's function) in covariant massive gravity theories with any number of external gravitons. Second, we intend to utilize our findings to assess the impact on the Hawking radiation emitted by a Schwarzschild black hole, stemming from the additional degrees of freedom associated with the massive spin-2 field. Our research builds upon the approach established in Ref. 1, which investigated the effects of incorporating massless scalar fields into standard particle physics on the rate of Hawking radiation emission.",
        "ori-fast-z-score": 1.2247448713915892,
        "water-fast-z-score": 6.463663618136471,
        "rewrite-fast-z-score": 1.2510864843424487
    },
    {
        "original_text": "We consider the asymptotic behavior of the sample autocovariance function and spectral density functions of stationary processes with regularly varying marginal distributions, which are driven by an infinite order moving average process whose coefficients have regularly varying tails. We show that these quantities can be approximated by those of a finite order autoregressive-moving average (ARMA) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters. The results obtained here extend previous ones given in the literature to more general cases where the driving noise is not necessarily Gaussian or has non-Gaussian components. In particular, we provide new proofs for the existing results when the driving noise is purely Gaussian. Our approach relies heavily on recent developments made in the theory of regular variation and stochastic calculus. As applications, we study two examples of duration-driven LRD models, namely the Cox-Ingersoll-Ross (CIR) model and the Vasicek model.",
        "watermark_text": "We consider the asymptotic activity of the sample autocovariance map and spectral density functions of stationary systems with regularly varying marginal distributions , which are driven by an endless order move average process whose coefficients have regularly varying tails . We see that these quantities can be approximated by those of a finite order autoregressive - moving average ( ARMA ) model in the sense of weak convergence under some conditions on the tail behaviors of the ARMA parameters .The results derived here extend previous ones given in the writings to more general instances where the driving signal is not necessarily Gaussian or has non - Gaussian components . In particular , we provide fresh proofs for the new results when the driving signal is purely Gaussian .Our perspective rely heavily on current developments developed in the theoretical of regular variation and stochastic equations . As applications , we study two examples of duration - based LRD models , namely the Cox - Ingersoll - Ross ( CIR ) model and the Vasicek theory .",
        "rewrite_text": "We analyze the asymptotic behavior of the sample autocovariance map and spectral density functions in stationary systems characterized by regularly varying marginal distributions, influenced by an infinite-order moving average process with coefficients exhibiting regularly varying tails. Our findings indicate that these quantities can be approximated by those of a finite-order autoregressive-moving average (ARMA) model through weak convergence, provided certain conditions on the tail behaviors of the ARMA parameters are met. This research expands upon earlier results by addressing more general scenarios where the driving signal is not necessarily Gaussian or may contain non-Gaussian components. Specifically, we offer new proofs for our findings in cases where the driving signal is purely Gaussian. Our approach draws significantly from recent advancements in the theory of regular variation and stochastic equations. Additionally, we explore two duration-based long-range dependence (LRD) models: the Cox-Ingersoll-Ross (CIR) model and the Vasicek framework.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": 1.3251783128981585
    },
    {
        "original_text": "We present an analysis of the colour-selection criteria for identifying high-redshift (z > 6) galaxies using data from the first public release of the UKIRT Infrared Deep Sky Survey (UKIDSS). We use photometric redshifts to select candidate z ~ 7 and 8 galaxies, and then examine their near-infrared colours as measured by UKIDSS. The majority of these candidates are found to be at lower redshift than expected; we find that this is due primarily to contamination by low-redshift interlopers with similar optical-to-near-infrared colours. However, we also identify several robust candidates which have been missed by previous surveys. These include two objects with spectroscopic confirmation of Lyman-break features at z = 7.071 and z = 7.085 respectively. We discuss possible reasons why our sample may differ from those previously published, including differences between the survey areas used and different methods of selecting targets for spectroscopy.",
        "watermark_text": "We present an assessment of the colour - choice requirements for finding high - redshift ( z > 6 ) galaxies using data from the first public published of the UKIRT Infrared Deep Sky Survey ( UKIDSS ) . We use photometric redshifts to select selected z ~ 7 and 8 galaxies , and then investigate their near - infrared colours as measured by UKIDSS .The majority of these proposals are found to be at lower redshift than expected ; we find that this is due primarily to poisoning by low - redshift interlopers with similar optical - to - near - infrared colours . However , we also identify several robust candidates which have been missed by earlier surveys .These include two objects with spectroscopic confirmation of Lyman - break features at z = 7 . 071 and z = 7 . 085 respectively . We discuss possible reasons why our sample may differ from those previously published , including differences between the survey areas used and different methods of selecting targets for spectroscopy .",
        "rewrite_text": "We provide an evaluation of the color selection criteria necessary for identifying high-redshift galaxies (z > 6) using data from the initial public release of the UKIRT Infrared Deep Sky Survey (UKIDSS). By employing photometric redshifts, we identify potential z ~ 7 and 8 galaxies and examine their near-infrared colors as recorded by UKIDSS. Our findings indicate that most of these candidates are located at lower redshifts than anticipated, a result primarily attributed to contamination from low-redshift interlopers that share similar optical-to-near-infrared colors. Nevertheless, we also pinpoint several strong candidates that previous surveys overlooked, including two objects with spectroscopic confirmation of Lyman-break features at z = 7.071 and z = 7.085, respectively. We explore potential reasons for the discrepancies between our sample and those reported earlier, such as variations in the survey areas and differing methods for spectroscopy target selection.",
        "ori-fast-z-score": 1.6733200530681511,
        "water-fast-z-score": 3.9727331518303837,
        "rewrite-fast-z-score": -1.0834726777719228
    },
    {
        "original_text": "The gravitational lensing effect was first predicted by Einstein in 1915, but it took another 50 years before the first confirmed observation could be made.  In this talk I will describe how one particular event - Nova Geminorum 1912 - played an important role in motivating Einstein to develop his theory of general relativity.   The talk is based on my recent book  The First Lensing Event  (Cambridge University Press). \n\nGravitational lensing occurs when light passes close to a massive object such as a galaxy or cluster of galaxies.  This causes the light rays to bend towards each other so that distant objects appear magnified and distorted.  \n\nIn 1919, Einstein published his famous field equations which described gravity as curvature in space-time caused by matter/energy.  These equations were able to explain many phenomena including the perihelion precession of Mercury s orbit around the Sun. \n\nHowever, there remained some puzzles about these equations.  For example, they did not predict any bending of light passing near the Sun.\n\nEinstein realised he needed to modify his original field equations if he wanted them to account for gravitational lensing effects.  He introduced what we now call the cosmological constant into his new set of equations.  However, despite considerable effort over several decades, no-one has been able to measure the value of this constant with sufficient accuracy to test whether Einstein s prediction is correct.  \n\nThis talk describes how astronomers finally managed to observe gravitational lensing in 1979 using observations of a distant quasar known as Q0957+561A-B.  It also explains why the discovery of gravitational lenses led to the awarding of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997.",
        "watermark_text": "The gravitational lensing effect was first anticipated by Einstein in 1915 , but it takes another 50 centuries before the first proven measurement came be made . In this talk I will explain how one particular event - Nova Geminorum 1912 - played an important role in motivating Einstein to develop his idea of general relativity .The talk is based on my current work The First Lensing Event ( Cambridge University Press ) . Gravitational lensing occurs when light passes close to a huge structure such as a galaxy or cluster of galaxies .This creates the light rays to bend towards each other so that nearby objects look magnified and distorted . In 1919 , Einstein released his important field equations which described gravity as curvature in space - time induced by matter / momentum .These equations were could to explain much processes including the perihelion precession of Mercury s orbit around the Sun . However , there remained some mysteries about these equations .For instance , they did not predict any bending of light moving near the Sun . Einstein realised he needed to modify his existing field equations if he asked them to account for gravitational lensing effects .He proposed what we now call the cosmological steady into his new set of equations . However , despite considerable work over numerous years , no - anyone has been able to measure the value of this constant with sufficient accuracy to test whether Einstein s prediction is accurate .This discussion describes how astronomers last managed to observe gravitational lensing in 1979 utilizing observations of a distant quasar known as Q0957 + 561A - B . It specifically describes why the discovery of gravitational lenses leading to the giving of the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997 .",
        "rewrite_text": "The phenomenon of gravitational lensing was first predicted by Einstein in 1915, but it took another five decades before a definitive measurement could be made. In this presentation, I will discuss how a specific event—Nova Geminorum 1912—significantly inspired Einstein in the development of his theory of general relativity. This discussion is based on my current research, \"The First Lensing Event\" (Cambridge University Press). Gravitational lensing occurs when light passes near a massive object, such as a galaxy or a galaxy cluster, causing the light rays to bend towards one another. As a result, nearby objects appear magnified and distorted. In 1919, Einstein published his pivotal field equations that described gravity as the curvature of space-time caused by matter and momentum. Although these equations helped explain various phenomena—including the precession of Mercury's orbit around the Sun—there were still some unresolved issues. One key issue was that they did not account for the bending of light around the Sun. Einstein recognized the need to revise his equations to include gravitational lensing effects, introducing what we now know as the cosmological constant into his revised equations. However, despite considerable effort over many years, no one has managed to measure this constant accurately enough to verify Einstein's predictions. This discussion also highlights how astronomers finally observed gravitational lensing in 1979 through observations of a distant quasar known as Q0957 + 561A-B. I will explain why the discovery of gravitational lenses was instrumental in awarding the Nobel Prize in Physics to Roger Blandford and Roman Vishniac in 1997.",
        "ori-fast-z-score": 1.0954451150103321,
        "water-fast-z-score": 7.916941713925581,
        "rewrite-fast-z-score": 1.1547005383792515
    },
    {
        "original_text": "We show that the exchange-only optimized potentials (OEPs) are not equivalent to the Kohn-Sham (KS) method in general, even if one uses an exact density functional for the exchangecorrelation energy. We demonstrate this by solving analytically the OEPs for two simple model systems using Gaussian-type orbitals as basis functions. The results obtained within both approaches differ significantly. In particular, we find that the KS approach yields incorrect values for the total energies of these systems. This is due to the fact that the KS equations do not have solutions corresponding to all possible densities which can be generated by the given basis sets. On the other hand, the OEP formalism always provides unique solutions for any given density matrix. Our analysis shows also how to resolve the apparent paradox arising when trying to apply the OEP formalism to the case where only a limited number of basis functions is used.",
        "watermark_text": "We see that the transfer - only optimized potentials ( OEPs ) are not equivalent to the Kohn - Sham ( KS ) method in general , even if one uses an precise density functional for the exchangecorrelation energy . We showed this by solving analytically the OEPs for two simple model models using Gaussian - class orbitals as basis maps .The results derived within both approaches differ significantly . In particular , we find that the KS approach produces unreliable estimates for the total energies of these systems .This is due to the fact that the KS coefficients do not have solutions equivalent to all possible densities which can be generated by the particular basis sets . On the other hand , the OEP formalism certainly presents specific solutions for any certain density matrix .Our study shows also how to overcome the apparent paradox emerging when trying to apply the OEP formalism to the case where only a small number of basis functions is utilized .",
        "rewrite_text": "We observe that transfer-only optimized potentials (OEPs) are generally not equivalent to the Kohn-Sham (KS) method, even when a highly accurate density functional is employed for the exchange-correlation energy. This discrepancy was demonstrated by analytically solving OEPs for two simple model systems using Gaussian-type orbitals as basis functions. The results from both approaches reveal significant differences, particularly indicating that the KS method yields unreliable total energy estimates for these systems. This unreliability arises because the KS coefficients do not correspond to solutions for all possible densities generated by the chosen basis sets. In contrast, the OEP formalism provides specific solutions for any given density matrix. Furthermore, our study addresses how to resolve the apparent paradox encountered when applying the OEP formalism with a limited number of basis functions.",
        "ori-fast-z-score": -1.2309149097933272,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": -1.5
    },
    {
        "original_text": "We report on diffuse X-ray emission in the Carina Nebula observed by Suzaku. The spectrum is well reproduced by thermal plasma models at kT = 0.7-1 keV and nH = (0.5-2) x 10^(22) cm^{-3}, which are consistent with those obtained previously for other regions within the nebula. We find that the total luminosity of this component amounts to Lx ~ 1.3 x 10^35 erg/sec, corresponding to about 10% of the total energy output of massive stars in the region. This suggests that hot gas produced by stellar winds and/or supernovae plays an important role in heating up the interstellar medium around young open clusters such as Trumpler 14-16. \n \n \n \n Keywords: Diffuse X-rays, Hot plasma, Open cluster, Supernova remnant, Stellar wind, Carina Nebula",
        "watermark_text": "We report on diffuse X - ray radiation in the Carina Nebula observed by Suzaku . The spectrum is well illustrated by thermal plasma estimates at kT = 0 . 7 - 1 keV and nH = ( 0 . 5 - 2 ) x 10 ^ ( 22 ) cm ^ { - 3 } , which are compatible with those observed previously for other regions within the nebula .We see that the total luminosity of this constituent amounts to Lx ~ 1 . 3 x 10 ^ 35 erg / sec , equivalent to about 10 % of the total energy produced of large galaxies in the region . This implies that hard gas created by stellar winds and / or supernovae plays an important role in heating up the interstellar medium around young open complexes such as Trumpler 14 - 16 .Keywords : Diffuse X - radiation , Hot plasma , Open core , Supernova remnant , Stellar wind , Carina Nebula",
        "rewrite_text": "We present our findings on diffuse X-ray radiation in the Carina Nebula, as observed by Suzaku. The spectrum can be effectively described by thermal plasma parameters, with kT values ranging from 0.7 to 1 keV and nH values between (0.5 - 2) x 10^(22) cm^(-3), consistent with previous observations in other regions of the nebula. Our calculations indicate a total luminosity of approximately Lx ~ 1.3 x 10^(35) erg/sec, which represents about 10% of the total energy produced by large galaxies in the vicinity. This suggests that the high-energy gas generated by stellar winds and/or supernovae significantly contributes to the heating of the interstellar medium surrounding young open clusters like Trumpler 14-16. \n\nKeywords: Diffuse X-ray radiation, Hot plasma, Open clusters, Supernova remnants, Stellar winds, Carina Nebula.",
        "ori-fast-z-score": -1.3127849234810511,
        "water-fast-z-score": 3.646624787447364,
        "rewrite-fast-z-score": 0.9615239476408232
    },
    {
        "original_text": "We consider the periodic oscillation of an adiabatic piston in contact with two ideal gases at different temperatures and pressures, separated by a rigid wall.  We prove that if the initial state is close to equilibrium then there exists a unique global solution which converges exponentially fast towards its limit cycle as time goes to infinity.  The proof relies on a combination of techniques from nonlinear analysis (Lyapunov functions) and kinetic theory (Boltzmann equation). In this work we study the dynamics of an adiabatic gas-piston system consisting of one-dimensional ideal gases confined between two walls. One of these walls is fixed while the other moves periodically according to some prescribed law. This problem has been studied extensively since the pioneering works of Maxwell  Max  , Boltzmann  Bol1  , and Sackur-Tetrode  Sac  . It was shown rigorously by Cercignani  Cerc  that under suitable assumptions on the motion of the piston, the solutions converge exponentially fast to their limit cycles. However, it seems difficult to extend his results beyond the case where the temperature difference across the piston remains small during all times. Here we show how to overcome this difficulty using new ideas based on Lyapunov functions combined with estimates coming from kinetic theory.",
        "watermark_text": "We consider the periodic oscillation of an adiabatic piston in contact with two ideal gases at different temperatures and pressures , separated by a rigid wall . We assume that if the first state is close to equilibrium then there exists a unique global solution which converges exponentially rapidly towards its limit cycle as time went to infinity .The proof uses on a combination of techniques from nonlinear analysis ( Lyapunov functions ) and kinetic theory ( Boltzmann integral ) . In this research we study the dynamics of an adiabatic gas - cylinder scheme consisting of one - dimensional ideal molecules confined between two walls .One of these barriers is fixed while the other moves periodically due to some prescribed law . This problem has been studied frequently since the pioneering works of Maxwell Max , Boltzmann Bol1 , and Sackur - Tetrode Sac .It was shown rigorously by Cercignani Cerc that under suitable assumptions on the movement of the piston , the systems converge exponentially rapidly to their limit cycles . However , it appears impossible to stretch his results beyond the case where the temperature difference across the piston remains tiny during all periods .Here we give how to overcome this trouble using new concepts relying on Lyapunov numbers coupled with projections come from kinetic theory .",
        "rewrite_text": "We examine the periodic oscillation of an adiabatic piston in contact with two ideal gases at varying temperatures and pressures, separated by a rigid wall. We propose that if the initial state is near equilibrium, there exists a unique global solution that converges exponentially fast to its limit cycle as time approaches infinity. The proof employs a blend of techniques from nonlinear analysis (specifically Lyapunov functions) and kinetic theory (notably the Boltzmann integral). Our research focuses on the dynamics of an adiabatic gas-cylinder setup featuring one-dimensional ideal molecules confined between two walls, one of which is stationary while the other moves periodically according to a defined law. This issue has been extensively studied since the foundational contributions of Maxwell, Boltzmann, and Sackur-Tetrode. Cercignani has rigorously demonstrated that, under specific assumptions regarding the piston’s motion, the system converges exponentially fast to its limit cycles. However, extending these results beyond cases where the temperature difference across the piston remains minimal throughout all cycles has proven challenging. In this paper, we present a solution to this issue by introducing new concepts based on Lyapunov numbers, combined with projections from kinetic theory.",
        "ori-fast-z-score": 0.618852747755276,
        "water-fast-z-score": 5.775958979049243,
        "rewrite-fast-z-score": 1.5554275420956378
    },
    {
        "original_text": "The vapor pressure, solubility in water, and interfacial tension between oil and water are important parameters for understanding the behavior of crude oils during their production or transport through pipelines.  In this study we have investigated these properties using alkanol monolayers on an aqueous subphase as model systems to mimic the hydrocarbon chains present in crude oils. The results show that the vapor pressures of the alkanols increase with chain length up to C8 but decrease again above C10. This is explained by considering the competition between two opposing effects:  On one hand, increasing chain lengths lead to higher molecular volumes which favor evaporation. On the other hand, longer chains also result in stronger van der Waals interactions within the liquid phase leading to lower vapor pressures. We find that the solubilities of the alkanols follow similar trends as those observed for the vapor pressures. However, the differences in solubility among different chain lengths become smaller when compared to the corresponding differences in vapor pressure. Finally, our measurements reveal that the interfacial tensions between the alkanol layers and the underlying water decreases monotonically with chain length.",
        "watermark_text": "The vapor tension , solubility in water , and interfacial tension between petroleum and water are important characteristics for studying the activity of crude oils during their production or delivery through pipelines . In this study we have analyzed these characteristics utilizing alkanol monolayers on an aqueous subphase as simulation structures to mimic the hydrocarbon chains present in crude oils .The results show that the liquid pressures of the alkanols increase with chain length up to C8 but decrease again above C10 . This is explained by using the competition between two conflicting factors : On one hand , increasing chain lengths result to higher molecular volumes which favor evaporation .On the other hand , wider chains also lead in heavier van der Waals bonds within the liquid phase leading to smaller liquid pressures . We see that the solubilities of the alkanols follow similar trends as those observed for the liquid pressures .However , the differences in solubility among different chain lengths become smaller when compared to the associated changes in vapor tension . Finally , our measurements reveal that the interfacial pressures between the alkanol layers and the underlying water reduces monotonically with chain depth .",
        "rewrite_text": "The vapor tension, solubility in water, and interfacial tension between petroleum and water are crucial properties for understanding the behavior of crude oils during production and transportation through pipelines. In this study, we explored these properties by using alkanol monolayers on an aqueous subphase as model systems to simulate the hydrocarbon chains found in crude oils. Our findings indicate that the liquid pressures of the alkanols increase with chain length up to C8 but decline after reaching C10. This pattern can be attributed to the interplay of two opposing factors: longer chains have greater molecular volumes, which promote evaporation, while they also create stronger van der Waals interactions in the liquid phase, resulting in lower liquid pressures. Additionally, we observed that the solubility of the alkanols exhibits similar trends as the liquid pressures; however, the differences in solubility among various chain lengths are less pronounced than those observed in vapor tension. Ultimately, our data show that interfacial pressures between the alkanol layers and the underlying water decrease consistently with increasing chain length.",
        "ori-fast-z-score": 0.21081851067789195,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 0.5488212999484517
    },
    {
        "original_text": "We present new near-infrared (NIR) spectroscopy and photometry for the Herbig Be star HD 98800, which is surrounded by an optically thick dust disk with a radius of ~200 AU. The NIR spectrum shows strong emission lines of H I, He II, C III, O IV-VI, Si IV, S VI, Fe II, Mg II, Al II-III, Na I, Ca II, and K I in addition to absorption features due to stellar winds. We find that the observed line profiles are consistent with those predicted by magnetohydrodynamic models of accretion disks around young stars. In particular, we detect blueshifted absorptions associated with infalling gas streams along magnetic field lines onto the central star. These results suggest that HD 98800 has been undergoing active mass accretion over the past few million years. \n \n Keywords: Accretion, Herbig Be star",
        "watermark_text": "We present new near - infrared ( NIR ) spectroscopy and photometry for the Herbig Be star HD 98800 , which is enclosed by an optically dense dust disk with a diameter of ~ 200 AU . The NIR spectrum displays strong absorption patterns of H I , He II , C III , O IV - VI , Si IV , S VI , Fe II , Mg II , Al II - III , Na I , Ca II , and K I in addition to absorption elements owing to stellar winds .We see that the seen line profiles are compatible with those predicted by magnetohydrodynamic predictions of accretion disks around early stars . In particular , we perceive blueshifted absorptions associated with infalling gas waves along magnetic field lines onto the main star .These data suggest that HD 98800 has been experiencing active mass accretion over the previous few million years . Keywords : Accretion , Herbig Be star",
        "rewrite_text": "We present new near-infrared (NIR) spectroscopy and photometry for the Herbig Be star HD 98800, which is surrounded by a dense dust disk approximately 200 AU in diameter. The NIR spectrum reveals significant absorption features from H I, He II, C III, O IV-VI, Si IV, S VI, Fe II, Mg II, Al II-III, Na I, Ca II, and K I, as well as absorption elements indicative of stellar winds. The observed line profiles align well with the magnetohydrodynamic models of accretion disks around early-type stars. Notably, we detect blueshifted absorptions linked to infalling gas waves along magnetic field lines toward the primary star. This evidence suggests that HD 98800 has undergone active mass accretion over the past few million years. Keywords: Accretion, Herbig Be star.",
        "ori-fast-z-score": -1.3127849234810511,
        "water-fast-z-score": 3.646624787447364,
        "rewrite-fast-z-score": 1.179535649239177
    },
    {
        "original_text": "We report the discovery of two new faint Milky Way open clusters, which we call Palomar 1 and 2 (Palomar 1: RA = 17 h 55 m 00 s , Dec = -28°45 00  ;  Palomar 2: RA = 18 h 04 m 30 s , Dec = -29°00 30 ). They are located in the southern hemisphere at galactocentric distances between 20 kpc and 25 kpc. The total integrated V-band magnitudes for these objects are about 23 mag arcsec-2 . We have obtained deep photometry with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel to study their stellar populations. Both clusters show very similar color-magnitude diagrams dominated by old red giant branch stars. Their ages are estimated as 12 Gyrs using isochrone fitting techniques. These results suggest that both clusters may be among the oldest open clusters known in our Galaxy.",
        "watermark_text": "We report the discovery of two new faint Milky Way open complexes , which we call Palomar 1 and 2 ( Palomar 1 : RA = 17 h 55 m 00 s , Dec = - 28°45 00 ; Palomar 2 : RA = 18 h 04 m 30 s , Dec = - 29°00 30 ) . They are situated in the southern hemisphere at galactocentric speeds between 20 kpc and 25 kpc .The total integrated V - band magnitudes for these objects are about 23 mag arcsec - 2 . We have achieved dark photometry with the Hubble Space Telescope Advanced Camera for Surveys Wide Field Channel to study their stellar populations .Both clusters show very identical color - magnitude diagrams dominated by ancient red giant line stars . Their ages are estimated as 12 Gyrs using isochrone fit techniques .These data suggest that both clusters might be among the earliest open regions known in our Galaxy .",
        "rewrite_text": "We announce the discovery of two new faint open clusters in the Milky Way, designated as Palomar 1 and Palomar 2 (Palomar 1: RA = 17 h 55 m 00 s, Dec = -28° 45' 00\"; Palomar 2: RA = 18 h 04 m 30 s, Dec = -29° 00' 30\"). Located in the southern hemisphere, they have galactocentric distances ranging from 20 kpc to 25 kpc. The total integrated V-band magnitudes of these clusters are approximately 23 mag arcsec^-2. Utilizing dark photometry from the Hubble Space Telescope’s Advanced Camera for Surveys Wide Field Channel, we have analyzed their stellar populations. Both clusters exhibit remarkably similar color-magnitude diagrams, primarily featuring ancient red giant branch stars. Based on isochrone fitting techniques, we estimate their ages to be around 12 billion years, suggesting that these clusters may represent some of the oldest open regions in our Galaxy.",
        "ori-fast-z-score": -0.8846517369293828,
        "water-fast-z-score": 3.8334908600273256,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We report on the detection and analysis of an optical shock front in the supernova remnant (SNR) Tycho using data obtained with Subaru High Dispersion Spectrograph (HDS). The observed spectrum shows strong emission lines of hydrogen, helium, nitrogen, oxygen, sulfur, argon, calcium, magnesium, silicon, iron ions at wavelengths between 3200Å and 9400Å. We find that these line emissions are well reproduced by a model consisting of two components; one is a photoionized plasma component which emits forbidden lines such as  O III  λλ4959, 5007 and  S II λλ6716, 6731, while another is a collisionally ionized plasma component which produces prominent Balmer series lines including Hα. From this result we conclude that the detected shock front is dominated by collisional ionization rather than photo-ionization. \n \n Keywords: Supernova remnants",
        "watermark_text": "We report on the discovery and evaluation of an optical shock front in the supernova remnant ( SNR ) Tycho using data received with Subaru High Dispersion Spectrograph ( HDS ) . The observed spectrum displays large emitted lines of sulfur , helium , nitrogen , hydrogen , sulfur , argon , potassium , magnesium , silicon , iron ions at wavelengths between 3200Å and 9400Å .We see that these line emissions are better illustrated by a theory consisting of two parts ; one is a photoionized plasma component which emits forbidden lines such as O III λλ4959 , 5007 and S II λλ6716 , 6731 , while another is a collisionally ionized plasma component which generates distinctive Balmer series curves including Hα . From this consequence we suppose that the detected shock front is dominated by collisional ionization instead than photo - ionization .Keywords: Supernova remnants",
        "rewrite_text": "We present our findings on the discovery and assessment of an optical shock front in the supernova remnant (SNR) Tycho, utilizing data obtained from the Subaru High Dispersion Spectrograph (HDS). The observed spectrum reveals prominent emissions from sulfur, helium, nitrogen, hydrogen, argon, potassium, magnesium, silicon, and iron ions, spanning wavelengths from 3200 Å to 9400 Å. Our analysis suggests that these emission lines are best explained by a dual-component theory: one component is a photoionized plasma that emits forbidden lines such as O III λλ4959, 5007 and S II λλ6716, 6731, while the other is a collisionally ionized plasma that produces notable Balmer series emissions, including Hα. Consequently, we infer that the shock front we detected is primarily influenced by collisional ionization rather than photoionization. \n\nKeywords: Supernova remnants",
        "ori-fast-z-score": -1.8203641092364127,
        "water-fast-z-score": 3.780756226875626,
        "rewrite-fast-z-score": -0.5547001962252291
    },
    {
        "original_text": "We report on the fabrication and characterization of charge qubits based on self-assembled InAs quantum dots (QDs) embedded in GaAs/AlGaAs heterostructures. We show that by using an optimized growth procedure, we can achieve high quality QD layers with low density of defects which are crucial for achieving good coherence times. The samples were grown by molecular beam epitaxy at 600 °C under As-rich conditions to minimize the formation of threading dislocations. A single layer of self-assembled InAs/GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm thick Al0.3Ga0.7As barrier layer. Finally, a 20 nm thick GaAs capping layer was deposited. The sample structure is shown schematically in Figure 1 . The photoluminescence spectrum shows emission peaks centered around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states associated with charged excitons.",
        "watermark_text": "We report on the fabrication and description of charge qubits based on self - assembled InAs quantum dots ( QDs ) integrated in GaAs / AlGaAs heterostructures . We suggest that by using an optimized growth technique , we can attain high quality QD layers with minimal concentration of flaws which are important for achieving better coherence times .The samples were cultivated by molecular wave epitaxy at 600 °C under As - rich conditions to minimize the formation of threading dislocations . A single mesh of self - assembled InAs / GaAs QDs was formed after annealing at 650 °C for 10 s followed by deposition of a 50 nm wide Al0 . 3Ga0 . 7As barrier layer .Finally , a 20 nm deep GaAs capping layer was extracted . The sample structure is displayed schematically in Figure 1 .The photoluminescence spectrum displays emission levels focused around 1280 nm corresponding to ground state excitonic transitions of individual QDs as well as higher energy states correlated with charged excitons .",
        "rewrite_text": "We present our findings on the fabrication and characterization of charge qubits using self-assembled InAs quantum dots (QDs) integrated within GaAs/AlGaAs heterostructures. Our approach involves utilizing an optimized growth technique to achieve high-quality QD layers with a low density of defects, which are crucial for improving coherence times. The QDs were grown using molecular beam epitaxy at 600 °C under arsenic-rich conditions to reduce the occurrence of threading dislocations. After annealing for 10 seconds at 650 °C, a uniform layer of self-assembled InAs/GaAs QDs was formed, followed by the deposition of a 50 nm Al0.3Ga0.7As barrier layer. Finally, a 20 nm GaAs capping layer was applied. The structural arrangement of the sample is illustrated in Figure 1. The photoluminescence spectrum reveals emission peaks centered around 1280 nm, corresponding to ground state excitonic transitions of individual QDs, as well as higher energy states associated with charged excitons.",
        "ori-fast-z-score": -0.24253562503633297,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": -0.8427009716003844
    },
    {
        "original_text": "The author presents the controversy surrounding the derivation and use of a dispersion relation for magnetohydrodynamic (MHD) waves in plasmas, which is based upon an assumption that the plasma frequency is much greater than the wave frequency.  The author shows how this assumption leads to incorrect results when applied to Alfvén waves propagating parallel or antiparallel to magnetic field lines; he also discusses other problems with using such relations. Finally, he proposes a new method for deriving a dispersion relation for these waves. This article was originally published as part of the series  Perspectives in Plasma Physics  by the American Institute of Physics. It has been reproduced here with permission. For more information about reproducing AIP content visit http://journals.aip.org/authors/rightsandpermissions. The author thanks Dr. J. D. Heymann for his help in preparing this version of the manuscript.",
        "watermark_text": "The author presents the controversy concerning the derivation and use of a dispersion constant for magnetohydrodynamic ( MHD ) currents in plasmas , which is based upon an assume that the plasma rate is much larger than the wave frequency . The author shows how this assumption results to misleading outcomes when applied to Alfvén currents propagating parallel or antiparallel to magnetic field lines ; he also explains other difficulties with employing such relations .Finally , he proposes a new method for deriving a dispersion connection for these waves . This section was originally published as part of the program Perspectives in Plasma Physics by the American Institute of Physics .It has been printed here with authorization . For more information about reproducing AIP material seek http : / / journals . aip . org / authors / rightsandpermissions .The author grateful Dr . J . D . Heymann for his help in preparing this copy of the manuscript .",
        "rewrite_text": "The author discusses the debate surrounding the derivation and application of a dispersion constant for magnetohydrodynamic (MHD) currents in plasmas, which relies on the assumption that the plasma rate is significantly greater than the wave frequency. He demonstrates that this assumption can lead to misleading results, especially when analyzing Alfvén currents that propagate either parallel or antiparallel to magnetic field lines. Additionally, he addresses other challenges associated with using such relations. To remedy these issues, he introduces a new method for deriving a dispersion relation for these waves. This section was originally part of the Perspectives in Plasma Physics program published by the American Institute of Physics and is reproduced here with permission. For further information on reproducing AIP materials, please visit http://journals.aip.org/authors/rightsandpermissions. The author expresses gratitude to Dr. J. D. Heymann for his assistance in preparing this manuscript.",
        "ori-fast-z-score": -0.39735970711951313,
        "water-fast-z-score": 5.514870180108347,
        "rewrite-fast-z-score": 0.9113223768657671
    },
    {
        "original_text": "We have studied how changes in the abundance pattern affect the frequencies and amplitudes of low-degree p-mode oscillations using two different methods, namely forward modelling (with the help of MESA stellar evolution code) and inverse analysis with the help of GYRE pulsation code. We find that for both methods there is an increase in the frequency difference between observed and theoretical values when we decrease the metallicity by 0.1 dex or more. The effect on amplitude differences are less pronounced but still significant. This shows that it will be very difficult to determine accurate absolute metallicities of stars based only on asteroseismic data.  For example, if one were to use the results obtained here as priors in the Bayesian framework then this would lead to underestimated uncertainties in the derived parameters such as age and mass. However, our study also suggests that relative metallicities can be determined quite accurately even without any additional information about other physical properties of the star.",
        "watermark_text": "We have researched how variations in the availability pattern change the frequencies and amplitudes of low - degree p - mode oscillations using two different methods , namely backwards modelling ( with the aid of MESA stars evolution code ) and inverse investigation with the aid of GYRE pulsation code . We see that for both approaches there is an increase in the frequency change between seen and theoretical values when we decrease the metallicity by 0 . 1 dex or more .The impact on amplitude differences are less pronounced but still significant . This shows that it will be very difficult to predict accurate absolute metallicities of stars based only on asteroseismic data .For instance , if one were to use the results derived here as priors in the Bayesian framework then this might lead to underestimated uncertainties in the derived values such as age and mass . However , our research also demonstrates that relative metallicities can be determined quite accurately even without any additional information about other material structures of the star .",
        "rewrite_text": "We investigated how changes in the availability pattern affect the frequencies and amplitudes of low-degree p-mode oscillations using two distinct methods: backward modeling with the MESA stellar evolution code and inverse analysis using the GYRE pulsation code. Our results indicate that both approaches show an increase in the discrepancy between observed and theoretical frequencies when metallicity is reduced by 0.1 dex or more. While the effects on amplitude differences are less pronounced, they remain significant. This suggests that accurately predicting absolute metallicities of stars based solely on asteroseismic data is quite challenging. For example, utilizing the findings from this study as priors in a Bayesian framework could result in underestimating the uncertainties in derived parameters such as age and mass. However, our research also shows that relative metallicities can be determined with considerable accuracy, even in the absence of additional information about the star's other structural materials.",
        "ori-fast-z-score": -0.47140452079103173,
        "water-fast-z-score": 5.34051746187234,
        "rewrite-fast-z-score": -0.7071067811865476
    },
    {
        "original_text": "We study the properties of strong peak points in Banach spaces, which are defined as follows.  Let X be a real or complex normed space with dual space X*. A point x # X is called a strong peak point if there exists an f # S(X) such that |f (x)| = sup{|f (y)| : y # X}. We prove that every separable reflexive Banach space has a dense set of strong peak points. As applications we show that every separable reflexivizable Banach space contains a copy of c 0 , and that every separable superreflexive Banach space contains a subspace isomorphic to l p for some 1 < p < + . In this note we study the properties of strong peaks points in Banach spaces. The concept was introduced by J. Lindenstrauss  L  who proved that every separable reflexible Banach space has a nonempty set of strong peak points; see also  JL1  .\nIn Section 2 we give several equivalent characterizations of strong peak points. In particular, it turns out that a point x # X is a strong peak point if and only if there exist two sequences (a n ) and (b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n &1Â2 b n = 0 such that the sequence (a n b n ) converges weakly to zero but not strongly. This characterization enables us to prove our first main result on the density of strong peak points in separable reflexive BanACH spaces. \nTheorem 3. Every separable reflexive BanACH space has a dense set SP(X) of strong peak points.\n\nAs immediate consequences of Theorem 3 we obtain the following results. (i) Every separable reflexivizable space contains a copy of c0.\n(ii) Every separable superreflexive space contains a subspace isomorphic",
        "watermark_text": "We research the properties of stable peak points in Banach spaces , which are given as follows . Let X be a real or complex normed space with dual collection X * .A point x # X is dubbed a strong peak point if there exists an f # S ( X ) such that | f ( x ) | = sup { | f ( y ) | : y # X } . We say that every separable reflexive Banach space has a dense setting of strengthened peak points .As applications we prove that every separable reflexivizable Banach space includes a copy of c 0 , and that every separable superreflexive Banach space includes a subspace isomorphic to l p for some 1 < p < + . In this note we study the properties of strong peaks points in Banach spaces .The concept was introduced by J . Lindenstrauss L who proved that every separable reflexible Banach space has a nonempty set of strong peak points ; hear also JL1 . In Section 2 we give numerous equivalent characterizations of strong peak points .In particular , it turns out that a point x # X is a strong peak point if and only if there exist two sequences ( a n ) and ( b n ) in R satisfying lim n Ä a n = lim n Ä b n = 1 and lim n Ä a n & 1Â2 b n = 0 such that the sequence ( a n b n ) converges weakly to zero but not strongly . This formulation enables us to prove our first major result on the density of strengthened peak points in separable reflexive BanACH spaces .Theorem 3 . Every separable reflexive BanACH space has a dense setting SP ( X ) of stable peak points .As obvious effects of Theorem 3 we obtain the following results . ( i ) Every separable reflexivizable set contains a copy of c0 .( ii ) Every separable superreflexive space includes a subspace isomorphic",
        "rewrite_text": "We investigate the characteristics of stable peak points in Banach spaces as defined below. Let \\(X\\) be a real or complex normed space with its dual space \\(X^*\\). A point \\(x \\in X\\) is termed a strong peak point if there exists a function \\(f \\in S(X)\\) such that \\(|f(x)| = \\sup \\{ |f(y)| : y \\in X \\}\\). It is established that every separable reflexive Banach space has a dense set of strengthened peak points. Consequently, we demonstrate that every separable reflexivizable Banach space contains a copy of \\(c_0\\), and that every separable superreflexive Banach space has a subspace that is isomorphic to \\(l_p\\) for some \\(1 < p < +\\infty\\). This note focuses on the properties of strong peak points in Banach spaces. The concept was introduced by J. Lindenstrauss, who showed that every separable reflexive Banach space possesses a nonempty set of strong peak points; see also [JL1]. In Section 2, we present various equivalent characterizations of strong peak points. Notably, we find that a point \\(x \\in X\\) is a strong peak point if and only if there exist sequences \\((a_n)\\) and \\((b_n)\\) in \\(\\mathbb{R}\\) such that \\(\\lim_{n \\to \\infty} a_n = \\lim_{n \\to \\infty} b_n = 1\\) and \\(\\lim_{n \\to \\infty} a_n \\cdot b_n = 0\\), with the sequence \\((a_n b_n)\\) converging weakly to zero but not strongly. This characterization allows us to establish our first significant result regarding the density of strengthened peak points in separable reflexive Banach spaces: Theorem 3 states that every separable reflexive Banach space has a dense set of stable peak points, denoted as \\(SP(X)\\). As direct consequences of Theorem 3, we conclude that (i) every separable reflexivizable space contains a copy of \\(c_0\\) and (ii) every separable superreflexive space has a subspace that is isomorphic to \\(l_p\\).",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 3.3093806066996887,
        "rewrite-fast-z-score": -1.4485719366802965
    },
    {
        "original_text": "We present the broad-band spectral evolution of the X-ray binary system Sco-X1, which is one of the brightest persistent sources in our Galaxy and has been observed by many satellites over more than three decades. We use data obtained with Suzaku (0.5-10 keV), Swift/BAT (15-50 keV) and INTEGRAL/IBIS (20-100 keV). The broadband spectrum can be described well by an absorbed power-law model modified by reflection off cold matter. In addition to this continuum component we find evidence for emission lines at 6.4-6.7 keV that are consistent with fluorescent iron Kα line emission produced by photoionization of neutral material surrounding the central source. These results suggest that there may exist a hot accretion disk around the compact object as well as a cool outer disk or torus-like structure. This scenario is supported by recent observations made with Chandra/HETG. \n \n Keywords: Broadband spectroscopy, Reflection features",
        "watermark_text": "We present the broad - band spectral evolution of the X - ray binary system Sco - X1 , which is one of the brightest persistent sources in our Galaxy and has been observed by many satellites over more than three decades . We use data acquired with Suzaku ( 0 . 5 - 10 keV ) , Swift / BAT ( 15 - 50 keV ) and INTEGRAL / IBIS ( 20 - 100 keV ) .The broadband spectrum can be described good by an absorption power - law theory augmented by absorption off cool matter . In addition to this continuum component we find proof for emission lines at 6 . 4 - 6 . 7 keV that are compatible with fluorescent iron Kα line emission generated by photoionization of neutral metal surrounding the main source .These data suggest that there may exist a bright accretion object around the compact body as also as a cool inner disk or torus - like structure . This scenario is endorsed by recent observations made with Chandra / HETG .Keywords: Broadband spectroscopy, Reflection features",
        "rewrite_text": "We present a comprehensive analysis of the broadband spectral evolution of the X-ray binary system Sco X-1, a prominent persistent source in our Galaxy that has been monitored by various satellites for over three decades. Our analysis utilizes data from Suzaku (0.5 - 10 keV), Swift/BAT (15 - 50 keV), and INTEGRAL/IBIS (20 - 100 keV). The broadband spectrum is well described by an absorption power-law model, complemented by absorption effects from cooler matter. Additionally, we detect evidence of emission lines in the range of 6.4 - 6.7 keV, consistent with fluorescent iron Kα line emission caused by the photoionization of neutral metals surrounding the primary source. These findings suggest the presence of a bright accretion object in orbit around the compact component, as well as a cool inner disk or toroidal structure. This interpretation is further supported by recent observations from Chandra/HETG. Keywords: Broadband spectroscopy, Reflection features.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 1.7232808737106582
    },
    {
        "original_text": "We present an algorithm for consistent hypothesis testing in which we consider all possible hypotheses that are compatible with some given set of observations, and select those that maximize their posterior probability according to Bayes  theorem.  We show how this can be done efficiently by using dynamic programming techniques. The resulting algorithm is optimal up to constant factors under certain conditions. Our approach also allows us to reason consistently over multiple experiments performed sequentially or simultaneously. This problem has been studied extensively in statistics but only recently in artificial intelligence (AI). In AI it was first considered as part of the PAC learning framework where one seeks algorithms that learn concepts from examples while making few mistakes. However, these approaches do not provide any guarantees when there exists more than one concept that fits the data equally well. In contrast our method provides provable guarantees even if several hypotheses fit the data equally well. Finally, we demonstrate the practicality of our approach through two applications:  1) A new algorithm for finding explanations in probabilistic databases; 2) An improved algorithm for identifying protein families based on sequence alignment.",
        "watermark_text": "We present an algorithm for consistent hypothesis testing in which we investigate all possible hypotheses that are compatible with some particular set of measurements , and select those that maximize their posterior likelihood according to Bayes principle . We see how this can be performed efficiently by using dynamic programming tools .The resulting algorithm is efficient up to constant factors under certain conditions . Our solution therefore allows us to reason consistently over multiple studies performed sequentially or separately .This problem has been studied frequently in statistics but only lately in artificial intelligence ( AI ) . In AI it was first considered as part of the PAC learning framework where one seeks methods that learn concepts from instances while making few errors .However , these perspectives do not offer any promises when there exists more than one concept that fits the information equally perfectly . In comparison our technique provides provable assurance even if several hypotheses fitted the information equally perfectly .Finally , we prove the practicality of our approach through two applications : 1 ) A modern algorithm for finding explanations in probabilistic libraries ; 2 ) An upgraded method for finding protein families using on sequence alignment .",
        "rewrite_text": "We introduce an algorithm for consistent hypothesis testing that examines all potential hypotheses compatible with a specific set of measurements and selects those that maximize posterior likelihood based on Bayes' principle. We demonstrate that this can be accomplished efficiently using dynamic programming techniques. The resulting algorithm maintains efficiency up to constant factors under certain conditions, enabling consistent reasoning across multiple studies conducted either sequentially or independently. While this problem has been extensively explored in statistics, it has only recently gained attention in the field of artificial intelligence (AI). In AI, it was initially addressed as part of the PAC learning framework, which focuses on developing methods to learn concepts from instances while minimizing errors. However, these approaches do not guarantee effectiveness when multiple concepts perfectly fit the data. In contrast, our method offers provable guarantees, even when several hypotheses align perfectly with the information provided. Finally, we demonstrate the practicality of our approach through two applications: 1) a modern algorithm for deriving explanations in probabilistic libraries; and 2) an enhanced technique for identifying protein families through sequence alignment.",
        "ori-fast-z-score": -2.3849888978799783,
        "water-fast-z-score": 5.848076606885379,
        "rewrite-fast-z-score": -0.8340576562282991
    },
    {
        "original_text": "We present an analysis of gravitational lensing data for the galaxy cluster Abell 1689, which is located at redshift z = 0.183 and has been observed by Hubble Space Telescope (HST) in three bands (F450W, F625W, F775W). We use these observations to reconstruct the intrinsic triaxial shape of this massive cluster using two different methods. First we apply the method developed by Sereno & Umetsu (2006) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios. Second, we employ the technique proposed by Corless et al. (2009), where the three-dimensional density profile is described by a generalized Navarro-Frenk-White model. Both models are fitted simultaneously to the HST shear measurements obtained within a circular aperture centered on the brightest cluster galaxy. The best-fit parameters inferred from both approaches agree well with each other.",
        "watermark_text": "We present an assessment of gravitational lensing data for the galaxy cluster Abell 1689 , which is situated at redshift z = 0 . 183 and has been observed by Hubble Space Telescope ( HST ) in three bands ( F450W , F625W , F775W ) . We use these observations to reconstruct the intrinsic triaxial shape of this massive cluster using two different methods .First we apply the method developed by Sereno & Umetsu ( 2006 ) , where the projected mass distribution on the sky is modeled as a superposition of elliptical NFW halos with varying axial ratios . Second , we utilize the method established by Corless et al .( 2009 ) , where the three - dimensional density profile is modeled by a generalized Navarro - Frenk - White model . Both theories are fit concurrently to the HST shear measurements obtained within a circular aperture located on the brightest cluster galaxy .The best - fitting factors inferred from both approaches agree well with each other .",
        "rewrite_text": "We provide an evaluation of gravitational lensing data for the galaxy cluster Abell 1689, located at a redshift of z = 0.183, which has been observed by the Hubble Space Telescope (HST) across three bands: F450W, F625W, and F775W. Utilizing these observations, we reconstruct the intrinsic triaxial shape of this massive cluster using two distinct methods. First, we employ the approach developed by Sereno & Umetsu (2006), in which the projected mass distribution on the sky is modeled as a combination of elliptical NFW halos with varying axial ratios. Second, we apply the method proposed by Corless et al. (2009), which models the three-dimensional density profile with a generalized Navarro-Frenk-White model. Both methodologies are concurrently fitted to the HST shear measurements acquired within a circular aperture centered on the brightest cluster galaxy. The best-fitting parameters obtained from both methods show strong agreement with one another.",
        "ori-fast-z-score": 1.6035674514745464,
        "water-fast-z-score": 4.27617987059879,
        "rewrite-fast-z-score": 0.9271726499455306
    },
    {
        "original_text": "We study the possibility that CP violation in the Standard Model is induced by several fields with nontrivial transformation properties under flavor and charge conjugation, which we call  Froggatt-Nielsen  (FN) fields.  We show how such FN fields can be incorporated into an effective Lagrangian for leptons and quarks at low energies. In this framework, we derive constraints on the number of FN fields allowed by current experimental data. Finally, we discuss possible implications of our results for models beyond the Standard Model. Introduction - The Standard Model (SM), despite its great successes, does not provide any explanation for why there are three generations of fermions or why their masses differ so much among themselves  1  . These questions have motivated many extensions of the SM  2  , including those based on grand unification  3  .\nIn these theories it has been shown  4  that new sources of CP violation may arise through phases associated with Yukawa couplings between Higgs boson(s) and fermion mass eigenstates. However, since all known particles couple to the same scalar doublet H = (H + , H 0 ) T / √ 2, one expects that the resulting contributions to CP-violating observables will be too small to explain observed phenomena  5  . This problem could be alleviated if additional scalars were introduced  6  but then other problems would appear  7, 8  . Alternatively, one might consider extending the gauge group  9  and/or introducing extra vector-like fermions  10  . Another possibility consists in considering more general transformations than phase rotations when constructing the most general form of the CKM matrix  11  .",
        "watermark_text": "We research the suggestion that CP violation in the Standard Model is caused by many fields with nontrivial transformation properties under flavor and charge conjugation , which we call Froggatt - Nielsen ( FN ) areas . We see how such FN fields can be included into an efficient Lagrangian for leptons and quarks at low energies .In this framework , we derive restrictions on the quantity of FN fields supported by current experimental evidence . Finally , we study possible possibilities of our findings for models beyond the Standard Model .Introduction - The Standard Model ( SM ) , despite its great successes , does not offer any explanation for why there are three generations of fermions or why their masses vary so greatly among themselves 1 . These questions have prompted many extensions of the SM 2 , notably those built on grand unified 3 .In these theories it has been shown 4 that new sources of CP violation may arise through phases related with Yukawa couplings between Higgs boson ( s ) and fermion weight eigenstates . However , since all known ions couple to the same scalar doublet H = ( H + , H 0 ) T / √ 2 , one expects that the resulting contributions to CP - violating observables will be too small to explain predicted events 5 .This problem could be alleviated if added scalars were introduced 6 but then other difficulties would appear 7 , 8 . Alternatively , one might propose extending the gauge group 9 and / or introducing extra vector - like fermions 10 .Another possibility consists in considering more general processes than phase rotations when constructing the most general form of the CKM matrix 11 .",
        "rewrite_text": "In this study, we explore the idea that CP violation within the Standard Model may arise from numerous fields exhibiting complex transformation behaviors under flavor and charge conjugation, which we refer to as Froggatt-Nielsen (FN) sectors. We demonstrate how these FN fields can be incorporated into an effective low-energy Lagrangian for leptons and quarks. Within this framework, we establish constraints on the number of FN fields supported by existing experimental data. Additionally, we investigate the implications of our results for potential models that extend beyond the Standard Model. \n\nIntroduction - While the Standard Model (SM) has achieved significant success, it fails to provide answers to critical questions, such as the existence of three generations of fermions and the substantial variation in their masses. These unresolved issues have led to numerous extensions of the SM, particularly those based on grand unification theories. Recent studies have indicated that new sources of CP violation could emerge from phases connected to the Yukawa couplings between Higgs bosons and fermion mass eigenstates. However, since all known fermions interact with the same scalar doublet, \\( H = ( H^+, H^0 )^T / \\sqrt{2} \\), the resulting contributions to CP-violating observables are anticipated to be too minimal to account for the predicted effects. Introducing additional scalar fields may mitigate this issue, but could introduce other challenges. Alternatively, one could consider expanding the gauge group or incorporating extra vector-like fermions. Another avenue involves examining more general processes than simple phase rotations when formulating the most comprehensive version of the CKM matrix.",
        "ori-fast-z-score": -0.10153461651336192,
        "water-fast-z-score": 6.463663618136471,
        "rewrite-fast-z-score": -0.09667364890456635
    },
    {
        "original_text": "The aim of this thesis is to provide an overview on the state-of-the-art Hartree-Fock Self-Consistent-Field (SCF) methods for solving quantum chemical problems, with special emphasis on their numerical aspects.  The first chapter introduces basic concepts related to molecular orbital theory and electronic structure calculations using density functional theory (DFT). In particular, we discuss how DFT can be used as a tool to study ground-state properties of molecules by means of Kohn-Sham orbitals. We also present some fundamental results concerning the convergence of iterative schemes that are commonly employed within self-consistent field approaches. The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been developed over the last decades to solve the Hartree-Fock equations numerically. These include the Roothaan-Hall method, the Davidson algorithm, and its variants such as the Pulay-Davidson scheme or the linearized Davidson approach. Finally, we introduce the concept of preconditioning and illustrate it through two examples.",
        "watermark_text": "The goal of this dissertation is to provide an overview on the state - of - the - art Hartree - Fock Self - Consistent - Field ( SCF ) techniques for solving quantum chemical problems , with special emphasis on their numerical parts . The first section introduces basic concepts related to molecular orbital theory and electronic stability analysis utilizing density functional theory ( DFT ) .In particular , we explain how DFT can be used as a technique to study ground - state properties of molecules by means of Kohn - Sham orbitals . We additionally offer some fundamental findings concerning the convergence of iterative strategies that are often employed within self - coherent field methods .The second chapter deals with the description of several classes of algorithms based on direct minimization techniques which have been built over the last decades to solve the Hartree - Fock equations numerically . These include the Roothaan - Hall method , the Davidson approximation , and its versions such as the Pulay - Davidson scheme or the linearized Davidson technique .Finally , we present the notion of preconditioning and explain it through two examples .",
        "rewrite_text": "This dissertation aims to provide a comprehensive overview of the latest Hartree-Fock Self-Consistent Field (SCF) techniques used to address quantum chemical challenges, with a particular focus on their numerical aspects. The first section introduces fundamental concepts related to molecular orbital theory and electronic stability analysis through density functional theory (DFT). We detail how DFT can serve as a method for investigating the ground-state properties of molecules using Kohn-Sham orbitals. Additionally, we present key insights into the convergence of iterative methods commonly employed in self-consistent field approaches. The second chapter explores various algorithmic classes based on direct minimization techniques that have been developed over the past few decades to numerically solve the Hartree-Fock equations. This discussion includes methods such as the Roothaan-Hall method, the Davidson approximation, and its derivatives, including the Pulay-Davidson scheme and the linearized Davidson technique. Finally, we introduce the concept of preconditioning and illustrate it with two examples.",
        "ori-fast-z-score": 0.6793662204867574,
        "water-fast-z-score": 6.863036195648746,
        "rewrite-fast-z-score": 1.0256451881367414
    },
    {
        "original_text": "We present the results of an analysis of data on advanced potentials in hadronic collisions at high energies, obtained by the TOTEM experiment at LHC and by the UA7 collaboration at SppS collider. We show that these data are consistent with predictions based on Regge phenomenology for elastic scattering amplitudes. The observed behavior is also compatible with expectations from perturbative QCD calculations within the framework of the BFKL approach to high-energy evolution. \n \n Keywords: High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments \n \n 1 Introduction \n \n In recent years there has been considerable interest in studying the properties of elastic scattering amplitudes at very high energies (see e.g.,  1  ). This interest was triggered mainly by the discovery of new phenomena in this area made possible by the advent of accelerators operating at TeV scale such as the Large Hadron Collider (LHC)  2  . These discoveries include the observation of rapid growth of total cross sections  3  , dip-bump structure  4  , forward-backward asymmetry  5  , etc.. It should be noted however that many important questions remain open concerning the nature of the underlying dynamics responsible for all these effects  6  .\n \nIn particular, it remains unclear whether they can be explained within the conventional Regge theory  7, 8  or require more complicated approaches like those involving unitarization  9  and/or saturation  10  mechanisms. Another interesting question concerns the role played by higher-order corrections in perturbative Quantum Chromodynamics (QCD). Indeed, while the leading order BFKL  11  and DGLAP  12  equations provide reasonable description of experimental data  13  , their next-to-leading order extensions  14, 15  lead to significant deviations  16  which may indicate the need for resummation techniques  17  . \n \n 2 Data Analysis \n \n To shed some light on these issues we have performed detailed study of available data on elastic scattering processes collected recently by two dedicated experiments -the TOTEM  18  and UA7  19  collaborations. Both groups measured differential distributions dσ/d",
        "watermark_text": "We present the conclusion of an assessment of evidence on advanced potentials in hadronic collisions at high energies , obtained by the TOTEM study at LHC and by the UA7 collaboration at SppS collider . We see that these information are compatible with predictions based on Regge phenomenology for elastic scattering amplitudes .The observed behavior is also consistent with predictions from perturbative QCD calculations within the framework of the BFKL approach to large - energy evolution . Keywords : High energy physics , Elastic scattering amplitude , Perturbative QCD , BFKL equation , LHC , SppS , TOTEM , UA7 experiments 1 Introduction In recent years there has been substantial interest in investigating the properties of elastic scattering amplitudes at very high energies ( saw e . g . , 1 ) .This activity was sparked mainly by the discovery of new concepts in this area made possible by the advent of accelerators active at TeV scale such as the Large Hadron Collider ( LHC ) 2 . These finds feature the observation of rapid increase of complete cross sections 3 , dip - bump formation 4 , backwards - backward asymmetry 5 , etc . .It should be mentioned however that several important questions remain open concerning the nature of the fundamental interactions involved for all these influences 6 . In particular , it remains unsure whether they can be described within the standard Regge principle 7 , 8 or use more complicated approaches like those concerning unitarization 9 and / or saturation 10 mechanisms .Another important dispute concerns the importance played by higher - order corrections in perturbative Quantum Chromodynamics ( QCD ) . Indeed , while the led order BFKL 11 and DGLAP 12 equations offer reasonable explanation of theoretical data 13 , their next - to - leading order additions 14 , 15 lead to significant deviations 16 which would indicate the necessity for resummation methods 17 .2 Data Analysis To shed some light on these problems we have done thorough study of available data on elastic scattering systems collected lately by two dedicated studies - the TOTEM 18 and UA7 19 collaborations . Both groups recorded differential functions dσ / d",
        "rewrite_text": "We present the findings of our assessment regarding advanced potentials in hadronic collisions at high energies, as derived from the TOTEM study at the LHC and the UA7 collaboration at the SppS collider. Our analysis indicates that these results align well with the predictions stemming from Regge phenomenology concerning elastic scattering amplitudes. Furthermore, the observed phenomena are consistent with perturbative QCD predictions within the context of the BFKL framework for large-energy evolution.\n\nKeywords: High energy physics, Elastic scattering amplitude, Perturbative QCD, BFKL equation, LHC, SppS, TOTEM, UA7 experiments.\n\n1. Introduction\n\nIn recent years, there has been considerable interest in exploring the characteristics of elastic scattering amplitudes at extremely high energies (see, for instance, Reference 1). This surge of interest was primarily triggered by new insights made possible by the emergence of TeV-scale accelerators, such as the Large Hadron Collider (LHC) (Reference 2). Key discoveries include the observation of a rapid rise in total cross sections (Reference 3), formation of dip-bump structures (Reference 4), and backward-backward asymmetries (Reference 5). However, several critical questions persist regarding the fundamental interactions that govern these effects (Reference 6). In particular, it remains uncertain whether these phenomena can be explained within the standard Regge framework (References 7 and 8) or if they necessitate more complex approaches involving unitarization (Reference 9) or saturation mechanisms (Reference 10). Another significant contention revolves around the role of higher-order corrections in perturbative Quantum Chromodynamics (QCD). Although the leading-order BFKL (Reference 11) and DGLAP (Reference 12) equations provide a plausible explanation for theoretical predictions (Reference 13), their next-to-leading order terms (References 14 and 15) exhibit considerable deviations (Reference 16), suggesting the need for resummation methods (Reference 17).\n\n2. Data Analysis\n\nIn order to gain further insight into these issues, we conducted an extensive study of the elastic scattering data recently collected by the dedicated TOTEM (Reference 18) and UA7 (Reference 19) collaborations. Both groups have recorded differential functions dσ/d...",
        "ori-fast-z-score": -0.16783627165933782,
        "water-fast-z-score": 9.398831212922918,
        "rewrite-fast-z-score": 0.6963106238227914
    },
    {
        "original_text": "We present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that uses fast approximate signatures to detect periodic signals in time series data. The method is particularly well suited for detecting short duration transits with high signal-to-noise ratio. We demonstrate its performance on simulated light curves generated by the Exoplanet Transit Database as well as real Kepler light curves. Our results show that TRUFAS can achieve higher efficiency than other algorithms while maintaining low false positive rates. \n \n Keywords: Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary system characterization \n \n \n \n 1 Introduction \n \n Planets are detected indirectly through their gravitational effects upon their host stars. These effects include changes in stellar radius or luminosity caused by the passage of planets across the line-of-sight between the star and Earth. This phenomenon is known as a transit event. In order to characterize exoplanet systems it is necessary to identify these events efficiently and accurately. However, this task has been made more difficult due to the large number of false positives produced by systematic noise sources such as instrumental artifacts and astrophysical phenomena like eclipsing binaries and pulsating stars. \n \n To date there have been several methods developed specifically for identifying transit-like features within astronomical time series data. Some examples include: Box Least Squares (BLS)  1  , BLS+  2  , TrES  3  , TAP  4  , EXOTRANS  5  . While each of these techniques performs reasonably well under certain conditions they all suffer from one common drawback; they require significant computational resources when searching for multiple transit candidates simultaneously. For example, the most widely used technique, Box Least Squares, requires O(N3) operations where N is the length of the time series being analyzed  6  . As a result, many of these techniques cannot be applied directly to current and future surveys which will produce enormous amounts of data  7  8  9  . \n \n In recent years wavelet transforms have become increasingly popular for analyzing astronomical time series data  10  ",
        "watermark_text": "We introduce TRUFAS ( Transit Recognition Using Fast Approximate Signatures ) , an efficient and strong transit search pipeline that using fast exact signatures to identify continuous patterns in time series information . The method is especially good suitable for detecting short length transits with high signal - to - noise ratio .We showed its effectiveness on simulated light surfaces derived by the Exoplanet Transit Database as also as realistic Kepler light surfaces . Our results show that TRUFAS can attain better productivity than other methods while maintaining low false negative rates .Keywords : Transiting planet , Wavelets , Time - series analysis , False positives reduction , Planetary network characterization 1 Introduction Planets are detected indirectly through their gravitational impacts upon their host stars . These effects include changes in stellar radius or luminosity caused by the travel of stars across the line - of - view between the star and Earth .This phenomenon is known as a transit event . In order to characterize exoplanet systems it is required to identify these changes easily and correctly .However , this job has been turned more challenging due to the huge amount of false positives created by widespread sound sources such as instrumental artifacts and astrophysical processes like eclipsing binaries and pulsating stars . To date there have been numerous algorithms created exclusively for identifying transit - like phenomena within astronomical date sequence data .Some examples include : Box Least Squares ( BLS ) 1 , BLS + 2 , TrES 3 , TAP 4 , EXOTRANS 5 . While each of these procedures works relatively well under certain conditions they all suffer from one common drawback ; they demand significant computational resources when looking for multiple transit alternatives simultaneously .For instance , the most commonly used technique , Box Least Squares , requires O ( N3 ) operations where N is the length of the time series being evaluated 6 . As a result , many of these procedures cannot be applied directly to recent and future surveys which will generate immense sums of data 7 8 9 .In recent years wavelet transforms have developed relatively popular for studying astronomical time cycle information 10",
        "rewrite_text": "We present TRUFAS (Transit Recognition Using Fast Approximate Signatures), an efficient and robust transit search pipeline that utilizes fast exact signatures to detect continuous patterns in time series data. This method is particularly effective in identifying short-duration transits with a high signal-to-noise ratio. Our experiments demonstrate its effectiveness on simulated light curves obtained from the Exoplanet Transit Database, as well as on realistic Kepler light curves. The results indicate that TRUFAS outperforms other methods while maintaining low false negative rates. \n\n**Keywords:** Transiting planet, Wavelets, Time-series analysis, False positives reduction, Planetary network characterization \n\n**1 Introduction**  \nPlanets are identified indirectly through their gravitational influence on their host stars, leading to observable changes in stellar radius or luminosity as the planets transit across the line of sight between the star and Earth. This occurrence is referred to as a transit event. Accurately identifying these changes is essential for characterizing exoplanet systems. However, the task has become increasingly difficult due to a significant number of false positives generated by prevalent noise sources, including instrumental artifacts and astrophysical phenomena, such as eclipsing binaries and pulsating stars. \n\nNumerous algorithms have been developed specifically for detecting transit-like signals within astronomical time series data, including methods like Box Least Squares (BLS), BLS+, TrES, TAP, and EXOTRANS. While these techniques perform well under specific conditions, they share a common limitation: they require substantial computational resources when searching for multiple transit candidates simultaneously. For instance, the widely-used Box Least Squares method demands O(N³) operations, where N represents the length of the time series being analyzed. Consequently, many of these algorithms are not directly applicable to recent and upcoming surveys, which will produce vast amounts of data. In recent years, wavelet transforms have gained popularity for analyzing astronomical time series information.",
        "ori-fast-z-score": -2.5298221281347035,
        "water-fast-z-score": 7.171804696775816,
        "rewrite-fast-z-score": -0.727606875108999
    },
    {
        "original_text": "We present results for the accretion of planetesimals by growing proto-gas-giant planets in circumstellar disks with different masses and compositions, using three-dimensional hydrodynamic simulations coupled to an N-body integrator. We find that the growth rate is strongly dependent on disk mass; more massive disks lead to faster planet formation timescales. The final planetary mass depends primarily on the initial disk surface density profile at large radii (>100 AU), which determines how much material can be transported inward before it dissipates. In addition, we show that the composition of the disk has only minor effects on the resulting planet properties. Our models are able to reproduce observed trends between host star metallicity and giant planet occurrence rates as well as the distribution of orbital periods and eccentricities of known exoplanets. This suggests that our model captures important physical processes involved in forming gas giants. Keywords: Planetary systems; Giant planets",
        "watermark_text": "We present results for the accretion of planetesimals by expanding proto - gas - giant planets in circumstellar disks with various masses and compositions , using three - dimensional hydrodynamic simulations combined to an N - bodies integrator . We see that the development frequency is strongly dependent on disk mass ; more massive disks lead to faster planet development timescales .The final planetary weight depends primarily on the initial disk floor density profile at large radii ( > 100 AU ) , which determines how many material can be transported inward before it dissipates . In addition , we find that the composition of the disk has only minor impacts on the resulting planet properties .Our models are able to capture seen trends between host star metallicity and massive object occurrence periods as also as the distribution of orbital periods and eccentricities of known exoplanets . This implies that our model captures significant physical processes implicated in forming gas giants .Keywords: Planetary systems; Giant planets",
        "rewrite_text": "We present findings on the accretion of planetesimals by expanding proto-gas giants in circumstellar disks of varying masses and compositions. Utilizing three-dimensional hydrodynamic simulations alongside an N-body integrator, we observe that the frequency of development is significantly influenced by disk mass; more massive disks correspond to shorter timescales for planetary development. The final mass of the planets is primarily determined by the initial density profile of the disk at large radii (greater than 100 AU), which dictates the amount of material that can be transported inward before it dissipates. Additionally, our results indicate that the disk's composition has a relatively minor effect on the properties of the resulting planets. Our models successfully capture the observed correlations between the metallicity of host stars and the occurrence rates of massive objects, as well as the distribution of known exoplanets’ orbital periods and eccentricities. This suggests that our model effectively incorporates essential physical processes involved in the formation of gas giants. \nKeywords: Planetary systems; Giant planets.",
        "ori-fast-z-score": 0.7875615306482168,
        "water-fast-z-score": 6.187983455093131,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "We have studied the decay of muons into two neutrinos, one electron-positron pair and one photon by using the Feynman diagram method. We found that there are three types of emitted neutrinos; type I is emitted when the energy of the emitted photon is less than half of the rest mass of the muon; type II is emitted when the energy is greater than or equal to half but less than twice the rest mass of themuon;andtype IIIisemittedwhentheenergyofthespin-1/2particlesissmallerthantwice therespectivestandardmass.Theprobabilitydistributionfunctionsforthese3typesareshown  for different values of the parameter x = Eγ / (2 mμ). The results obtained here agree with those given previously. \n \n Keywords: muon decay, neutrino emission, Feynman diagram technique, probability distribution function, standard model",
        "watermark_text": "We have researched the decay of muons into two neutrinos , one electron - positron pair and one photon by using the Feynman diagram method . We showed that there are three categories of emitted neutrinos ; type I is emitted when the electricity of the emitted photon is greater than quarter of the remainder mass of the muon ; type II is emitted when the power is greater than or equal to quarter but less than times the remainder mass of themuon ; andtype IIIisemittedwhentheenergyofthespin - 1 / 2particlesissmallerthantwice therespectivestandardmass . Theprobabilitydistributionfunctionsforthese3typesareshown for different expressions of the parameter x = Eγ / ( 2 mμ ) .The results derived here agree with those given earlier . Keywords : muon radiation , neutrino emission , Feynman diagram method , probability distribution relation , standard model",
        "rewrite_text": "We conducted a study on the decay of muons into two neutrinos, a pair of electrons and positrons, and a photon using the Feynman diagram technique. Our findings indicate that there are three categories of emitted neutrinos: Type I, which is emitted when the energy of the emitted photon exceeds a quarter of the remaining mass of the muon; Type II, which occurs when the energy is at least a quarter but less than one times the remaining mass of the muon; and Type III, which is emitted when the energy of the spin-1/2 particles is less than twice their respective standard mass. The probability distribution functions for these three types are presented for various expressions of the parameter x = Eγ / (2 mμ). The results obtained in this study are consistent with previous findings. Keywords: muon radiation, neutrino emission, Feynman diagram method, probability distribution relation, standard model.",
        "ori-fast-z-score": 0.6030226891555273,
        "water-fast-z-score": 5.125692857821981,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "We study the gravitational instability (GI) in two differentially rotating, self-gravitating disks with and without magnetic fields using three-dimensional hydrodynamic simulations. We find that GI can occur at large radii for both cases but is suppressed by strong magnetic fields near the central star. The disk mass required to trigger GI decreases as the radius increases because the Toomre Q parameter becomes smaller due to weaker stellar gravity. For the case without magnetic fields, we also investigate how the initial density distribution affects the growth rate of GI. Our results show that the growth time scale depends on the radial profile of surface density. In addition, we examine whether or not GI leads to fragmentation. Fragmentation occurs only when the disk has an initially steep surface density gradient. Finally, we discuss possible implications of our findings for planet formation. Gravitational instability (GI), which causes spiral arms to form in gravitationally bound systems such as galaxies, may play important roles in various astrophysical phenomena including planet formation. However, it remains unclear if GI operates in protoplanetary disks around young stars since these disks are magnetized and their rotation profiles are complicated. Here, we perform 3D hydrodynamical simulations to explore this issue.",
        "watermark_text": "We explore the gravitational instability ( GI ) in two differentially rotating , self - gravitating disks with and without magnetic fields using three - dimensional hydrodynamic simulations . We see that GI can occur at large radii for both cases but is suppressed by intense magnetic waves near the main star .The disk mass needed to stimulate GI decreases as the radius increases because the Toomre Q function decreases smaller due to smaller stellar gravitational . For the case without magnetic fields , we also investigate how the early density function affects the development frequency of GI .Our results show that the development time scale depends on the radial profile of surface density . In addition , we investigate whether or not GI contributes to fragmentation .Fragmentation happens only when the disk has an initially steep surface velocity slope . Finally , we explain possible possibilities of our findings for planet development .Gravitational instability ( GI ) , which causes spiral arms to form in gravitationally locked components such as galaxies , might play important roles in different astrophysical processes including planet development . However , it remains unsure if GI exists in protoplanetary disks around old planets since these disks are magnetized and their rotation relationships are intricate .Here , we perform 3D hydrodynamical simulations to examine this question .",
        "rewrite_text": "We investigate gravitational instability (GI) in two differentially rotating, self-gravitating disks, both with and without magnetic fields, through three-dimensional hydrodynamic simulations. Our findings reveal that GI can occur at larger radii in both scenarios, but is inhibited by strong magnetic waves near the central star. The disk mass required to trigger GI diminishes with increasing radius due to a decrease in the Toomre Q parameter, which is influenced by the weaker gravitational pull of the stars at greater distances. For the case without magnetic fields, we also examine how the initial density distribution affects the frequency of GI development. Our results indicate that the timescale for GI emergence is dependent on the surface density's radial profile. Additionally, we explore whether GI contributes to fragmentation within the disks. Fragmentation is observed only when the disk possesses an initially steep gradient in surface velocity. Lastly, we discuss the implications of our findings for planetary formation. Gravitational instability, which can lead to the formation of spiral structures in gravitationally bound systems like galaxies, is believed to play a significant role in various astrophysical processes, including planet formation. However, it remains unclear whether GI is present in the magnetized protoplanetary disks surrounding older planets, given their complex rotational dynamics. In this study, we conduct 3D hydrodynamic simulations to address this question.",
        "ori-fast-z-score": -0.9901475429766744,
        "water-fast-z-score": 6.534973783646051,
        "rewrite-fast-z-score": 0.6965260331469925
    },
    {
        "original_text": "We present the kinematics and physical properties of strong Mg II absorbers at z = 1.5 − 3, using high-resolution (R ≈ 45000) spectroscopy obtained with Keck/HIRES. We find that these systems are composed primarily of cool gas clouds in pressure equilibrium with their surroundings; they have typical sizes of 100-200 pc, masses of 10^6−10^7 M_sun, and temperatures of ~10 4 K. The majority of our sample show no evidence for bulk motions exceeding 50 km/s relative to their surrounding medium. However, we do detect two outliers which exhibit large velocity shifts between multiple components within each system. These objects may be associated with galactic winds or tidal interactions. Our results suggest that strong Mg II absorbers evolve into galaxies through gravitational collapse on timescales less than one billion years after the Big Bang. This work is based upon observations made with the NASA/ESA Hubble Space Telescope, obtained from the Data Archive at the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS 5-26555.",
        "watermark_text": "We present the kinematics and physical properties of bright Mg II absorbers at z = 1 . 5 − 3 , using high - resolution ( R ≈ 45000 ) spectroscopy derived with Keck / HIRES . We see that these systems are composed primarily of cold gas clouds in pressure equilibrium with their environment ; they have typical sizes of 100 - 200 pc , masses of 10 ^ 6−10 ^ 7 M _ sun , and altitudes of ~ 10 4 K . The majority of our sample report no evidence for bulk motions exceeding 50 km / s relative to their nearby medium .However , we do perceive two outliers which exhibit big momentum changes between multiple components within each system . These bodies may be involved with galactic winds or tidal interactions .Our results propose that strong Mg II absorbers evolve into galaxies through gravity collapse on timescales roughly than one billion years after the Big Bang . This research is based upon measurements made with the NASA / ESA Hubble Space Telescope , obtained from the Data Archive at the Space Telescope Science Institute , which is leased by AURA under NASA contract NAS 5 - 26555 .",
        "rewrite_text": "We present the kinematics and physical characteristics of bright Mg II absorbers at redshifts ranging from 1.5 to 3, utilizing high-resolution spectroscopy (R ≈ 45000) obtained from the Keck Observatory's HIRES instrument. Our findings indicate that these systems are primarily composed of cold gas clouds in pressure equilibrium with their surroundings, exhibiting typical dimensions of 100 to 200 parsecs, masses between 10^6 and 10^7 solar masses, and temperatures around 10^4 K. Most of our sample shows no significant bulk motions, with speeds not exceeding 50 km/s relative to the adjacent medium. However, we identify two outliers that display substantial momentum variations across different components within each system, suggesting potential involvement in galactic winds or tidal interactions. Our results indicate that strong Mg II absorbers likely evolve into galaxies through gravitational collapse on timescales of approximately one billion years following the Big Bang. This study is based on data from the NASA/ESA Hubble Space Telescope, retrieved from the Data Archive at the Space Telescope Science Institute, which is operated by AURA under NASA contract NAS 5-26555.",
        "ori-fast-z-score": 1.3438638879193574,
        "water-fast-z-score": 6.063390625908324,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "The radial distribution of the inner magnetospheric (IM) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two satellites at low altitude in the equatorial plane for an intense geomagnetic storm that occurred between March 1-5, 1982 . The results show that there are significant differences in the IM plasma pressure profiles obtained with different satellites. In particular, the pressure profile derived from GEOS-1 observations shows a sharp peak near L = 3 , while those inferred from ATS-6 and GEOS-2 data exhibit much broader peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space. It is also found that the pressure profiles inferred from the three satellites agree well when they are shifted outward along the L-shell coordinate system. This suggests that the observed difference in the pressure profiles can be attributed mainly to the spatial variation of the pressure rather than temporal changes.",
        "watermark_text": "The radial distribution of the inner magnetospheric ( IM ) plasma pressure is studied by analyzing the magnetic field and particle measurements made onboard two spacecraft at low height in the equatorial plane for an extreme geomagnetic cyclone that occurred between March 1 - 5 , 1982 . The results show that there are significant variations in the IM plasma pressure profiles obtained with various satellites .In particular , the pressure model derived from GEOS - 1 studies shows a sharp peak near L = 3 , while those inferred from ATS - 6 and GEOS - 2 data display considerably wider peaks around L = 4 . These discrepancies may be due to the fact that these satellites have different orbits which sample different regions of space .It is also discovered that the pressure profiles inferred from the three satellites conform well when they are shifted outward along the L - shell coordinate system . This implies that the seen contrast in the pressure profiles can be due mainly to the spatial variation of the pressure rather than temporal changes .",
        "rewrite_text": "The radial distribution of plasma pressure in the inner magnetosphere (IM) is examined by analyzing magnetic field and particle measurements from two spacecraft that operated at low altitude in the equatorial plane during an extreme geomagnetic event from March 1 to March 5, 1982. The findings indicate substantial variations in the IM plasma pressure profiles across different satellites. Notably, the pressure model derived from GEOS-1 data reveals a pronounced peak around L = 3, whereas the profiles derived from ATS-6 and GEOS-2 data exhibit much broader peaks near L = 4. These differences may arise from the distinct orbits of the satellites, sampling different spatial regions. Additionally, it was found that the pressure profiles from the three satellites align well when shifted outward along the L-shell coordinate system. This suggests that the observed differences in pressure profiles are primarily due to spatial variations rather than changes over time.",
        "ori-fast-z-score": 1.4814874939752933,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "We study the exponential growth rate of a typed branching diffusion, which is an extension of the classical Galton-Watson process to allow for multiple types and non-Markovian reproduction laws. We show that this quantity can be expressed as the solution of a fixed point equation involving the Laplace transform of the offspring distribution. This allows us to prove existence and uniqueness results under mild conditions on the offspring distributions. In particular we obtain explicit bounds on the exponential growth rate when all offspring distributions are Poisson or geometric. Finally, we provide some examples illustrating our main result. Keywords: Branching processes; Typed branching diffusions; Exponential growth rate; Fixed-point equations. 1 Introduction Let (Zt) t≥0 denote a continuous-time branching diffusion with Z0 = 0. The population size at time t ≥ 0 is given by Nt := sup{n ∈ N : Zn ≤ t} where (Zn) n∈N denotes the family tree associated with the branching diffusion up to generation n. For each i ∈ {1, . . . , m} let Pi(·), qi(·) and Fi(·) respectively denote the probability generating function, mean number of children and offspring distribution of type-i individuals. Then it follows from  21, Theorem 1  that there exists a unique positive real number λ such that E exp{−λNt}|Ft  < ∞ for every t > 0, where Ft denotes the filtration generated by the branching diffusion up to time t. Moreover, (1 − Pt) −1 , t > 0 converges exponentially fast towards λ, see e.g.,  6, Proposition 3.1  . Here Pt denotes the extinction probability starting from one individual of type i at time zero. It has been shown recently in  4  that if the offspring distributions Fi have finite variance then λ coincides with the Malthusian parameter of the branching diffusion. That is, λ equals the exponential growth rate of the total population size. If additionally the offspring distributions Fi belong to the domain of attraction of a stable law with index αi ∈ (1, 2  then λ also coincides with",
        "watermark_text": "We test the exponential growth speed of a typed branching diffusion , which is an extension of the classical Galton - Watson process to allow for multiple types and non - Markovian reproduction laws . We see that this quantity can be described as the solve of a fixed point equation employing the Laplace transform of the offspring distribution .This enables us to obtain existence and uniqueness results under mild assumptions on the offspring distributions . In particular we obtain exact bounds on the exponential growth velocity when all offspring distributions are Poisson or geometric .Finally , we provide some examples illustrating our major result . Keywords : Branching systems ; Typed branching diffusions ; Exponential growth speed ; Fixed - point variables .1 Introduction Let ( Zt ) t≥0 denote a continuous - time branching diffusion with Z0 = 0 . The population size at time t ≥ 0 is given by Nt : = sup { n ∈ N : Zn ≤ t } where ( Zn ) n∈N denotes the family tree identified with the branching diffusion up to generation n . For each i ∈ { 1 , ..., m } let Pi ( · ) , qi ( · ) and Fi ( · ) respectively represent the probability producing function , mean number of offspring and offspring distribution of type - i people . Then it appears from 21 , Theorem 1 that there exists a unique positive real number μ such that E exp { −λNt } | Ft < ∞ for every t > 0 , where Ft describes the filtration produced by the branching diffusion up to time t . Moreover , ( 1 − Pt ) −1 , t > 0 converges exponentially rapidly towards λ , see e . g . , 6 , Proposition 3 . 1 .Here Pt denotes the extinction probability starting from one individual of type i at time zero . It has been shown lately in 4 that if the offspring distributions Fi have finite variance then λ coincides with the Malthusian variable of the branching diffusion .That is , λ equals the exponential growth speed of the total community size . If additionally the offspring distributions Fi correspond to the domain of attraction of a stable law with index αi ∈ ( 1 , 2 then λ additionally coincides with",
        "rewrite_text": "We investigate the exponential growth rate of a typed branching diffusion, which extends the classical Galton-Watson process to accommodate multiple types and non-Markovian reproductive behaviors. Our analysis reveals that this rate can be characterized by solving a fixed-point equation that utilizes the Laplace transform of the offspring distribution. This approach allows us to establish results regarding existence and uniqueness under mild assumptions concerning the offspring distributions. Notably, we derive precise bounds on the exponential growth rate specifically when all offspring distributions are either Poisson or geometric. Lastly, we present several examples that illustrate our principal findings. \n\n**Keywords:** Branching systems; Typed branching diffusions; Exponential growth rate; Fixed-point variables.\n\n1. **Introduction** \n\nLet \\( (Z_t)_{t\\geq0} \\) represent a continuous-time branching diffusion with \\( Z_0 = 0 \\). The population size at time \\( t \\geq 0 \\) is defined as \\( N_t := \\sup \\{ n \\in \\mathbb{N} : Z_n \\leq t \\} \\), where \\( (Z_n)_{n \\in \\mathbb{N}} \\) corresponds to the family tree associated with the branching diffusion up to generation \\( n \\). For each \\( i \\in \\{ 1, \\ldots, m \\} \\), let \\( P_i(\\cdot) \\), \\( q_i(\\cdot) \\), and \\( F_i(\\cdot) \\) denote the probability generating function, the expected number of offspring, and the offspring distribution for individuals of type \\( i \\), respectively. From Theorem 1 in reference 21, we find that there exists a unique positive real number \\( \\mu \\) such that \\( E[\\exp\\{-\\lambda N_t\\} | \\mathcal{F}_t] < \\infty \\) for all \\( t > 0 \\), where \\( \\mathcal{F}_t \\) represents the filtration generated by the branching diffusion up to time \\( t \\). Moreover, \\( (1 - P_t)^{-1} \\) for \\( t > 0 \\) converges exponentially fast to \\( \\lambda \\) (see, for example, Proposition 3.1 in reference 6). Here, \\( P_t \\) refers to the extinction probability starting from one individual of type \\( i \\) at time zero. Recent work in reference 4 has demonstrated that if the offspring distributions \\( F_i \\) possess finite variance, then \\( \\lambda \\) coincides with the Malthusian variable of the branching diffusion, which signifies the exponential growth rate of the total population size. Furthermore, if the offspring distributions \\( F_i \\) belong to the domain of attraction of a stable law with index \\( \\alpha_i \\in (1, 2) \\), then \\( \\lambda \\) additionally coincides with...",
        "ori-fast-z-score": 1.191759143062248,
        "water-fast-z-score": 6.46954963376649,
        "rewrite-fast-z-score": 1.8970080273373418
    },
    {
        "original_text": "The availability of computer systems is an important aspect in the design, development and operation of any system. The aim of this research was to develop a method for assessing the availability of a large number of SunOS/Solaries systems using syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). This research has been carried out by collecting data from a set of servers over a period of one year. A total of 1,000,000 records were collected during that time frame. These records have then been processed into a database which contains information about each record such as timestamps, host name, service type etc.. In order to assess the availability of these systems we developed two algorithms; one algorithm calculates the percentage uptime per day while another algorithm calculates the percentage downtime per hour. We also implemented a web interface so that users can view the results obtained from our analysis.",
        "watermark_text": "The availability of computer machines is an important element in the development , construction and operation of any program . The goal of this research was to develop a technique for evaluating the availability of a large number of SunOS / Solaries machines combining syslogd ( System Logging Daemon ) and wtmpx ( Windows Time Machine eXtender ) .This research has been carried out by collecting data from a setting of servers over a period of one year . A total of 1 , 000 , 000 data were collected during that time frame .These data have then been processed into a computer which contains information about each record such as timestamps , host title , service size etc . . In order to analyze the availability of these systems we developed two strategies ; one algorithm calculates the percentage uptime per month while another algorithm calculates the percentage downtime per hour .We additionally introduced a web application so that users can view the results derived from our analysis .",
        "rewrite_text": "The availability of computer systems is a crucial factor in the development, construction, and operation of any software program. This research aimed to create a method for assessing the availability of numerous SunOS/Solaris machines by integrating syslogd (System Logging Daemon) and wtmpx (Windows Time Machine eXtender). Data were gathered from a server environment over the course of one year, amassing a total of 1,000,000 records. This information was then processed on a computer, detailing each entry with attributes such as timestamps, host names, and service sizes. To evaluate the systems' availability, we developed two algorithms: one that computes the percentage of uptime per month, and another that calculates the percentage of downtime per hour. We also created a web application, allowing users to access and view the results of our analysis.",
        "ori-fast-z-score": -0.75,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -0.5345224838248488
    },
    {
        "original_text": "We report on the optical investigation of single self-assembled InAs/GaAs quantum dots (QDs) in an external magnetic field applied along their growth direction. The QD emission line splits into two components with opposite circular polarization when the magnetic field is increased to about 1 T, which corresponds to the Zeeman splitting energy of 0.5 meV at 4 K. We observe that this splitting increases linearly as temperature decreases down to 20 mK and then saturates below 10 mK. This behavior can be explained by taking into account both electron-hole exchange interaction and phonon-assisted relaxation processes between different excitonic states within QDs. Our results show that the spin-flip time for electrons confined inside QDs is longer than 100 ns even under high magnetic fields up to 5 T. Quantum dot (QD), also known as semiconductor nanocrystal or artificial atom, has attracted much attention due to its unique physical properties such as size-tunable band gap  1  , strong confinement effect  2  , and large oscillator strength  3  . These features make it possible to use QDs as building blocks for various optoelectronic devices including light-emitting diodes  4  , lasers  5  , solar cells  6  , photodetectors  7  , and so forth  8  .\nIn recent years, there have been many efforts devoted to investigating the spin dynamics of carriers confined in QDs  9  -  11  . It was found that the carrier spins are very stable against decoherence caused by environmental noise  12  -  14  . However, the spin flip times were reported to vary widely depending on experimental conditions  15  -  17  . For example, the spin lifetimes of holes  18  and electrons  19  confined in QDs were measured to be several nanoseconds using pulsed excitation techniques. On the other hand, the spin lifetime of electrons  20  and holes  21  confined in QDs could reach microsecond level if continuous wave laser was used instead.",
        "watermark_text": "We report on the optical study of single self - assembled InAs / GaAs quantum dots ( QDs ) in an external magnetic current applied along their development path . The QD absorption system separates into two parts with opposite spherical polarization when the magnetic field is expanded to about 1 T , which corresponds to the Zeeman splitting energy of 0 . 5 meV at 4 K . We determine that this splitting changes linearly as temperature grows down to 20 mK and then saturates below 10 mK .This phenomenon can be described by take into consideration both electron - hole exchange behavior and phonon - aided vibration mechanisms between various excitonic states within QDs . Our results show that the spin - flip time for electrons trapped inside QDs is longer than 100 ns even under high magnetic waves up to 5 T . Quantum dot ( QD ) , sometimes called as semiconductor nanocrystal or artificial electron , has garnered considerable scrutiny due to its unique physical properties such as size - tunable band gap 1 , large confinement phenomenon 2 , and large oscillator strength 3 .These features make it able to use QDs as building blocks for various optoelectronic applications notably light - emitting diodes 4 , lasers 5 , solar cells 6 , photodetectors 7 , and so forth 8 . In recent seasons , there have been many efforts devoted to investigating the spin behavior of carriers restricted in QDs 9 - 11 .It was shown that the carrier spins are very stable against decoherence caused by environmental noise 12 - 14 . However , the spin flip times were reported to vary widely depending on experimental environments 15 - 17 .For instance , the spin lifetimes of holes 18 and electrons 19 restricted in QDs were calculated to be several nanoseconds using pulsed excitation techniques . On the other hand , the spin lifetime of electrons 20 and holes 21 confined in QDs might reach microsecond level if continuous wave beam was used instead .",
        "rewrite_text": "We present our findings from an optical investigation of individual self-assembled InAs/GaAs quantum dots (QDs) subjected to an external magnetic field aligned with their growth direction. As the magnetic field increases to approximately 1 T, the absorption spectrum of the QDs splits into two components with opposing circular polarizations, corresponding to a Zeeman splitting energy of 0.5 meV at 4 K. We observe that this splitting exhibits a linear dependency on temperature up to 20 mK, after which it plateaus below 10 mK. This behavior can be explained by considering both the exchange interactions between electrons and holes as well as phonon-assisted vibrational mechanisms among various excitonic states present in the QDs. Our results indicate that the spin-flip time for electrons confined within the QDs exceeds 100 ns, even under high magnetic fields of up to 5 T. Quantum dots, often referred to as semiconductor nanocrystals or artificial atoms, have attracted significant attention due to their distinctive physical characteristics, including a size-tunable band gap, strong quantum confinement, and large oscillator strengths. These properties make QDs suitable for a wide range of optoelectronic applications, including light-emitting diodes, lasers, solar cells, and photodetectors. Recently, considerable efforts have been dedicated to exploring the spin dynamics of carriers trapped in QDs. Research has shown that the spin states of carriers are remarkably resilient to decoherence induced by environmental noise. However, reported spin-flip times can vary significantly based on experimental conditions. For example, spin lifetimes for holes and electrons in QDs have been measured to be on the order of several nanoseconds using pulsed excitation techniques. Conversely, when continuous wave excitation is employed, the spin lifetimes for both electrons and holes can extend to the microsecond range.",
        "ori-fast-z-score": 0.9330078226479681,
        "water-fast-z-score": 7.209605902279753,
        "rewrite-fast-z-score": -0.17149858514250882
    },
    {
        "original_text": "We study the electrodynamic properties of Josephson vortices (JVs) in hightemperature superconductors by solving numerically the time-dependent Ginzburg-Landau equations with an external magnetic field and current density. We find that JVs can be driven into motion by applying either a dc or ac electric field, which is consistent with previous experiments on YBa2Cu3O7-δ single crystals. The JV velocity increases linearly as the applied voltage increases for small voltages but saturates at large voltages due to the pinning effect. In addition, we show that the JV velocity decreases when increasing the temperature because of thermal fluctuations. Finally, we demonstrate that the JV dynamics are strongly affected by the anisotropy of the sample. \n \n Introduction \n \n High-temperature superconductivity has been discovered more than 30 years ago  1–3  . Since then, many theoretical models have been proposed to explain this phenomenon  4–9  , among them the so-called two-fluid model  10  . According to this theory, there exist two different types of charge carriers in these materials, namely electrons and holes  11  . These particles interact via attractive Coulomb forces  12  and form Cooper pairs  13  . When the material undergoes a phase transition below its critical temperature Tc, the Cooper pairs condense into a superfluid state  14  . This leads to macroscopic quantum phenomena such as zero resistance  15  and Meissner effect  16  . \n \n However, it was soon realized that the conventional Bardeen-Cooper-Schrieffer (BCS) theory  17  cannot fully account for all experimental observations  18  . For example, the BCS theory predicts that the energy gap between the ground-state and excited states should decrease rapidly near T = 0 K  19  . On the other hand, recent measurements  20  indicate that the energy gap remains almost constant down to very low temperatures  21  . To overcome this problem, several extensions of the original BCS theory were developed  22–24  . Among those theories, one of the most successful ones is the Eliashberg formalism  25  , where the electron-phonon interaction plays an important role  26  . It turns out that",
        "watermark_text": "We research the electrodynamic characteristics of Josephson vortices ( JVs ) in hightemperature superconductors by solving numerically the period - based Ginzburg - Landau coefficients with an external magnetic force and current density . We see that JVs can be pushed into motion by using either a dc or ac electric field , which is compatible with previous research on YBa2Cu3O7 - δ double particles .The JV speed increases linearly as the introduced voltage increases for low voltages but saturates at large voltages due to the locking effect . In addition , we find that the JV speed falls when increasing the temperature because of thermal fluctuations .Finally , we prove that the JV mechanics are strongly altered by the anisotropy of the sample . Introduction High - temperature superconductivity has been detected more than 30 centuries earlier 1 – 3 .Since then , various theoretical theories have been proposed to explain this phenomenon 4 – 9 , among them the so - called two - fluid model 10 . According to this theory , there exist two different kinds of charge carriers in these materials , namely electrons and holes 11 .These particles react via attractive Coulomb forces 12 and form Cooper pairs 13 . When the material undergoes a phase shift below its critical temperature Tc , the Cooper pairs condense into a superfluid state 14 .This leads to macroscopic quantum effects such as zero resistance 15 and Meissner phenomenon 16 . However , it was immediately realized that the standard Bardeen - Cooper - Schrieffer ( BCS ) theory 17 cannot fully account for all observation observations 18 .For instance , the BCS theory predicts that the power gap between the ground - state and excited states should decrease rapidly near T = 0 K 19 . On the other hand , recent observations 20 confirm that the power gap continues almost steady down to very low temperatures 21 .To solve this question , various extensions of the original BCS theory were developed 22 – 24 . Among those theories , one of the most popular ones is the Eliashberg formalism 25 , where the electron - phonon interaction plays an important role 26 .It turns out that",
        "rewrite_text": "We investigate the electrodynamic properties of Josephson vortices (JVs) in high-temperature superconductors by numerically solving the periodic Ginzburg-Landau coefficients under the influence of an external magnetic field and current density. Our findings indicate that JVs can be set in motion using either direct current (dc) or alternating current (ac) electric fields, which aligns with earlier studies of YBa2Cu3O7−δ double particles. At lower voltages, the speed of JVs increases linearly with the applied voltage; however, at higher voltages, this speed levels off due to a locking effect. Furthermore, we observe that increasing the temperature results in a decrease in JV speed due to thermal fluctuations. Lastly, we demonstrate that the mechanical behavior of JVs is significantly influenced by the sample's anisotropy. \n\n**Introduction**  \nHigh-temperature superconductivity has been recognized for over 30 years. Since its discovery, numerous theoretical models have been proposed to explain this phenomenon, including the well-known two-fluid model. This model suggests the existence of two types of charge carriers in these materials: electrons and holes. These carriers interact through attractive Coulomb forces, leading to the formation of Cooper pairs. When the material experiences a phase transition below its critical temperature (Tc), these Cooper pairs condense into a superfluid state, resulting in macroscopic quantum phenomena such as zero electrical resistance and the Meissner effect. However, it quickly became apparent that the traditional Bardeen-Cooper-Schrieffer (BCS) theory could not fully explain all experimental observations. For example, BCS theory predicts that the energy gap between the ground state and excited states should diminish rapidly near 0 K, while recent observations show that this gap remains relatively constant down to very low temperatures. To address this discrepancy, various extensions of the original BCS theory have been developed. Among these, the Eliashberg formalism, which emphasizes electron-phonon interactions, has gained significant attention.",
        "ori-fast-z-score": -1.2815364865751413,
        "water-fast-z-score": 6.920297027505764,
        "rewrite-fast-z-score": 0.2544566789039913
    },
    {
        "original_text": "We propose to use the time evolution of cosmological redshifts in order to probe the nature of dark energy, which is one of the most important problems in modern physics and astronomy. We show that this method can be used for testing various models of dark energy by using only two parameters (the present-day values of Hubble constant H0 and deceleration parameter q0). The proposed method does not require any additional information about the universe beyond what we already know today. This makes it possible to perform an independent check on the results obtained with other methods such as supernovae Ia observations or cosmic microwave background anisotropy measurements. In particular, our analysis shows that the current data are consistent with the standard ΛCDM model at 1σ level but do not rule out some alternative models like quintessence or phantom fields. Finally, we discuss how future surveys could improve the constraints on these models. Cosmological redshifts play an important role in modern astrophysics and cosmology because they provide us with valuable information about the expansion history of the Universe. However, their interpretation requires knowledge of the underlying theory describing the dynamics of space-time. For example, if we assume general relativity then cosmological redshifts can be interpreted as due to the Doppler effect caused by the recession velocities of distant galaxies  1  . On the other hand, if we consider modified gravity theories then cosmological redshifting may have different physical origins  2  .\nIn recent years there has been growing interest in studying the possibility of probing the nature of dark energy through its effects on cosmological redshifts  3  -  8  . Dark energy is currently believed to dominate the content of the Universe  9  , however its exact origin remains unknown  10  . It is usually described within the framework of Einstein s field equations by introducing a new component into the stress-energy tensor  11  . Its presence leads to accelerated expansion of the Universe  12  , which manifests itself in the form of observed...",
        "watermark_text": "We suggest to use the period evolution of cosmological redshifts in order to probe the nature of dark energy , which is one of the most important problems in modern physics and astronomy . We see that this technology can be used for studying several models of bright energy by using only two parameters ( the present - day parameters of Hubble constant H0 and deceleration parameter q0 ) .The proposed approach does not require any additional information about the universe beyond what we already understand today . This lets it convenient to conduct an independent check on the results derived with other methods such as supernovae Ia detection or cosmic microwave background anisotropy observations .In particular , our analysis shows that the present data are compatible with the standard ΛCDM theory at 1σ level but do not leave out some additional models like quintessence or phantom fields . Finally , we explain how potential study could enhance the restrictions on these models .Cosmological redshifts play an important role in modern astrophysics and cosmology because they give us with important information about the expansion history of the Universe . However , their explanation requires knowledge of the fundamental theory explaining the dynamics of space - time .For instance , if we suppose general relativity then cosmological redshifts can be interpreted as owing to the Doppler impact caused by the recession velocities of distant galaxies 1 . On the other hand , if we treat modified gravity theories then cosmological redshifting might have different physical origins 2 .In recent years there has been growing interest in investigating the prospect of probing the nature of dark energy through its consequences on cosmological redshifts 3 - 8 . Dark energy is currently suspected to dominate the content of the Universe 9 , however its exact origin stays unclear 10 .It is usually characterized within the framework of Einstein s field equations by bringing a new constituent into the strain - energy tensor 11 . Its presence contributes to accelerated expansion of the Universe 12 , which manifests itself in the form of observed . . .",
        "rewrite_text": "We propose utilizing the evolution of cosmological redshifts over time to investigate the nature of dark energy, one of the most significant challenges in contemporary physics and astronomy. Our approach demonstrates that this method can be applied to study various dark energy models using just two parameters: the current values of the Hubble constant (H0) and the deceleration parameter (q0). Notably, this method does not necessitate any additional information about the universe beyond our existing understanding, allowing for an independent verification of results obtained through other techniques, such as the detection of Type Ia supernovae or observations of cosmic microwave background anisotropies. Our analysis indicates that current data align with the standard ΛCDM model at the 1σ level, while still permitting consideration of alternative models like quintessence or phantom fields. Furthermore, we discuss how future studies could refine the constraints on these models. Cosmological redshifts are crucial in modern astrophysics and cosmology, as they provide essential insights into the expansion history of the universe. However, interpreting these redshifts requires a foundational understanding of the theory governing space-time dynamics. For example, under the framework of general relativity, cosmological redshifts can be viewed as a result of the Doppler effect due to the recession velocities of distant galaxies. Conversely, in modified gravity theories, the origins of cosmological redshifts could differ. Recently, there has been a growing interest in exploring how cosmological redshifts can be used to probe the nature of dark energy, which is believed to be the dominant component of the universe, yet its true origin remains elusive. Typically, dark energy is characterized within Einstein's field equations by introducing a new element into the stress-energy tensor, which contributes to the accelerated expansion of the universe, as evidenced by observable phenomena.",
        "ori-fast-z-score": 0.4931969619160719,
        "water-fast-z-score": 8.495296818075921,
        "rewrite-fast-z-score": 0.35355339059327373
    },
    {
        "original_text": "We present an overview on the diagnostic potential of emission lines in plasma produced by laser-produced plasmas (LPPs) and Z-pinches for astrophysical applications, with emphasis on He-like ions. We discuss how to obtain reliable results using these diagnostics under laboratory conditions that are relevant to those found in space environments. In particular we focus our attention on the determination of electron temperature T e , density n e , ionization state Z eff , and elemental composition C/H/N/O/Fe/Mg/K/Si/S/Ca. The main advantages and disadvantages of each method are discussed as well as their applicability limits. Finally, we provide examples of recent experimental results obtained at LULI2000 facility in France and NIF facility in USA. Keywords: Astrophysics; Laser-plasma interaction; X-rays; Spectroscopy. 1 Introduction X-ray spectroscopy is one of the most powerful tools available today for studying hot dense plasmas generated in laboratories or in space  1  . It has been used extensively over many decades to diagnose various types of plasmas ranging from solar flares  2  to supernova remnants  3  .\nIn this work, we will review some of the methods currently being developed to measure key physical parameters such as electron temperature T e , electron number density n e , ion charge state distribution Z eff , and chemical composition C/H/N/O/Fe/...",
        "watermark_text": "We present an overview on the clinical ability of emission lines in plasma generated by laser - produced plasmas ( LPPs ) and Z - pinches for astrophysical applications , with emphasis on He - like ions . We discuss how to obtain reliable results using these diagnostics under research circumstances that are applicable to those observed in space environments .In particular we focus our focus on the determination of electron pressure T r , concentration k e , ionization state Z eff , and elemental strength C / H / N / O / Fe / Mg / K / Si / S / Ca . The main benefits and disadvantages of each approach are discussed as well as their applicability restrictions .Finally , we provide details of recent experimental results derived at LULI2000 facility in France and NIF laboratory in USA . Keywords : Astrophysics ; Laser - plasma interaction ; X - radiation ; Spectroscopy .1 Introduction X - ray spectroscopy is one of the most important skills available today for studying warm dense plasmas generated in laboratories or in space 1 . It has been used heavily over numerous years to diagnose various types of plasmas ranging from solar flares 2 to supernova remnants 3 .In this research , we will review some of the methods still being advanced to measure key physical factors such as electron height T e , atom number density n e , ion charge state balance Z eff , and chemical composition C / H / N / O / Fe / . . .",
        "rewrite_text": "We provide an overview of the clinical potential of emission lines from plasmas created by laser-produced plasmas (LPPs) and Z-pinches for astrophysical applications, with a particular focus on He-like ions. Our discussion emphasizes how to obtain reliable results using these diagnostic methods in research settings that mirror conditions found in space environments. Specifically, we concentrate on measuring electron pressure (T_r), electron concentration (k_e), ionization state (Z_eff), and elemental abundances (C, H, N, O, Fe, Mg, K, Si, S, Ca). We evaluate the main advantages and disadvantages of each diagnostic approach, including their limitations in applicability. Furthermore, we present recent experimental findings from the LULI2000 facility in France and the NIF laboratory in the USA. \n\nKeywords: Astrophysics; Laser-plasma interaction; X-radiation; Spectroscopy.\n\n**1 Introduction**  \nX-ray spectroscopy is a crucial technique for investigating warm dense plasmas, whether generated in the lab or in space. It has been extensively utilized over the years to diagnose various plasma types, from solar flares to supernova remnants. In this research, we will review ongoing advancements in methods used to measure essential physical parameters such as electron temperature (T_e), electron number density (n_e), ion charge state balance (Z_eff), and chemical composition (C, H, N, O, Fe, etc.).",
        "ori-fast-z-score": 0.7107423155935334,
        "water-fast-z-score": 6.869037302955033,
        "rewrite-fast-z-score": 3.3005479880281388
    },
    {
        "original_text": "We report on the first detection of Compton scattering of iron K-alpha lines (6.4 keV) by hot plasma in magnetic cataclysmic variables (mCVs). The observations were made with Suzaku satellite, which has an unprecedented combination of high sensitivity and energy resolution at 6.4 keV for mCVs. We detected significant excess emission above continuum around 6.4 keV in three out of four targets observed. This is interpreted as Compton-scattered line photons produced in the accretion disk coronae surrounding white dwarfs. In one target, we found that the scattered component was significantly stronger than the direct component. Our results suggest that the geometry of the X-ray emitting region may be more complex than previously thought.  Keywords: Accreting compact objects, Hot plasma, Iron K-shell lines, Compton scattering, Suzaku, Cataclysmic variable",
        "watermark_text": "We report on the first measurement of Compton diffusion of iron K - alpha bands ( 6 . 4 keV ) by hot plasma in magnetic cataclysmic variables ( mCVs ) . The measurement were made with Suzaku spacecraft , which has an remarkable mix of high sensitivity and energy resolution at 6 . 4 keV for mCVs .We observed significant excess emission above continuum around 6 . 4 keV in three out of four targets observed . This is interpreted as Compton - dispersed line photons generated in the accretion disk coronae surrounding white dwarfs .In one target , we concluded that the scattered component was significantly greater than the direct component . Our results show that the topology of the X - ray emitting area might be more sophisticated than previously thought .Keywords : Accreting rigid surfaces , Hot plasma , Iron K - shell lines , Compton diffusion , Suzaku , Cataclysmic variable",
        "rewrite_text": "We present the first measurement of Compton diffusion of iron K-alpha lines (6.4 keV) in hot plasma within magnetic cataclysmic variables (mCVs). These measurements were conducted using the Suzaku spacecraft, which offers an exceptional combination of high sensitivity and energy resolution at 6.4 keV specifically for mCVs. We detected a significant excess of emission above the continuum around 6.4 keV in three out of the four targets we observed. This phenomenon is interpreted as line photons that have been Compton-scattered, originating from the accretion disk coronae surrounding white dwarfs. In one of the targets, we found that the scattered component was notably greater than the direct component. Our findings suggest that the structure of the X-ray emitting region may be more complex than previously believed. \n\nKeywords: Accreting rigid surfaces, hot plasma, iron K-shell lines, Compton diffusion, Suzaku, cataclysmic variable.",
        "ori-fast-z-score": -0.8017837257372732,
        "water-fast-z-score": 4.810702354423639,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "We present new observations of molecular hydrogen (H_2CO) absorption toward the low-mass protostar IRAS 16293-2422, which is associated with two outflows driven by different components of this binary system. The main component drives an east-west bipolar flow that has been traced over more than 1000 AU using SiO emission lines observed at high angular resolution. We have detected anomalously strong absorption features near the systemic velocity of the source for both ortho- and para-H_2CO transitions. These are likely due to self-absorption within the dense gas surrounding the central protostars. In addition, we find evidence for blueshifted absorption features in the para-H_2CO line profiles that may be tracing infalling material along the axis of one of the outflow lobes. Finally, we compare our results with previous studies of carbon monoxide (CO) emission towards the same region.",
        "watermark_text": "We report new images of molecular hydrogen ( H _ 2CO ) diffusion toward the small - weight protostar IRAS 16293 - 2422 , which is associated with two outflows driven by separate components of this binary system . The main component pushes an eastward - west bipolar flow that has been traced over more than 1000 AU utilizing SiO emission lines observed at high angular resolution .We have discovered anomalously strong absorption properties near the systemic speed of the source for both ortho - and para - H _ 2CO transitions . These are likely due to self - absorption within the deep gas covering the main protostars .In addition , we find proof for blueshifted absorption events in the para - H _ 2CO line profiles that might be tracing infalling matter along the axis of one of the outflow lobes . Finally , we compare our findings with previous investigations of carbon monoxide ( CO ) emission towards the same region .",
        "rewrite_text": "We present new imagery of molecular hydrogen (H₂CO) diffusion towards the low-mass protostar IRAS 16293-2422, which is linked to two outflows originating from different components of this binary system. The primary component generates an east-west bipolar flow, traced over more than 1000 AU using high-angular-resolution observations of SiO emission lines. We have identified unusually strong absorption features near the systemic velocity of the source for both ortho- and para-H₂CO transitions, likely resulting from self-absorption within the dense gas enveloping the main protostars. Additionally, we provide evidence of blueshifted absorption events in the para-H₂CO line profiles, which may indicate infalling material along the axis of one of the outflow lobes. Lastly, we compare our results with earlier studies of carbon monoxide (CO) emission in the same area.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "We present an improved version of our previous work on modelling the ultraviolet upturn in early-type galaxies using binary stars.  We use Monte Carlo simulations to generate synthetic populations with different ages, metallicities and mass ratios between components. The models are compared against observations of nearby galaxies obtained by GALEX. Our results show that binary systems can reproduce well both the strength and shape of the observed UV-optical SEDs. In particular we find that:  - Binary evolution is required to explain the strong UV fluxes seen at young ages (< 1 Gyr). - A large fraction of binaries must be composed of two hot subdwarfs or white dwarfs. - Binaries containing one normal star and one compact object cannot produce enough UV light to match the data. - Mass transfer plays only a minor role in shaping the UV-optical SED. - The best-fit age distribution peaks around 2 Gyr but extends down to younger ages.",
        "watermark_text": "We present an better version of our previous research on modelling the ultraviolet upturn in early - class stars using binary galaxies . We use Monte Carlo simulations to create artificial populations with various ages , metallicities and mass ratios between components .The models are compared against measurements of nearby galaxies collected by GALEX . Our results show that binary structures can predict good both the strength and shape of the seen UV - optical SEDs .In particular we find that : - Binary evolution is required to explain the strong UV fluxes seen at young years ( < 1 Gyr ) . - A wide proportion of binaries may be composed of two hot subdwarfs or white dwarfs .- Binaries involving one regular star and one compact body fail produce enough UV energy to match the information . - Mass transfer plays only a minor importance in shaping the UV - optical SED .- The best - fitting age distribution peaks around 2 Gyr but goes down to younger ages .",
        "rewrite_text": "We present an enhanced version of our previous research on modeling the ultraviolet upturn in early-type stars through binary galaxies. Utilizing Monte Carlo simulations, we generate artificial stellar populations with varying ages, metallicities, and mass ratios among their components. Our models are assessed against measurements from nearby galaxies obtained by GALEX. The results indicate that binary structures can effectively predict both the strength and shape of the observed UV-optical spectral energy distributions (SEDs). Specifically, we find that: - Binary evolution is essential to account for the significant UV fluxes observed in stars younger than 1 Gyr. - A significant proportion of binaries may consist of two hot subdwarfs or white dwarfs. - Binaries featuring one ordinary star and one compact object do not generate sufficient UV energy to align with the observed data. - Mass transfer has a minimal effect on shaping the UV-optical SED. - The optimal age distribution of our model peaks around 2 Gyr, with a decline towards younger ages.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 6.567206798038654,
        "rewrite-fast-z-score": 0.3418817293789138
    },
    {
        "original_text": "We report on the detection and analysis of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 (SOL2010-07-20T17:48) at 17:48 UT on July 20, 2010 using the Nançay Decameter Array (NDA). The event was accompanied by a fast halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. We find that the radio source is located near the center of the CME front as seen in white light images taken by STEREO-Ahead/EUVI 195 Å . The radio flux density shows rapid evolution during the first hour after the onset of the flare, followed by gradual decay over several hours. The radio spectrum has a power-law shape between 1 MHz to 5 GHz. The spectral index decreases rapidly below 100 MHz but remains nearly constant above this frequency.",
        "watermark_text": "We report on the detection and assessment of radio emission associated with an impulsive solar flare that occurred in active region NOAA 10486 ( SOL2010 - 07 - 20T17 : 48 ) at 17 : 48 UT on July 20 , 2010 using the Nançay Decameter Array ( NDA ) . The event was accompanied by a rapid halo coronal mass ejection ( CME ) , which reached Earth at 18 : 20 UT on July 21 .We see that the radio source is situated near the center of the CME front as shown in white light pictures taken by STEREO - Ahead / EUVI 195 Å . The radio flux concentration displays rapid change during the first hour after the outbreak of the flare , followed by rapid fading over numerous weeks .The radio signal has a power - law shape between 1 MHz to 5 GHz . The spectral index drops rapidly below 100 MHz but maintains fairly constant above this signal .",
        "rewrite_text": "We present our findings on the detection and analysis of radio emissions linked to an impulsive solar flare in active region NOAA 10486 (SOL2010-07-20T17:48), which occurred at 17:48 UT on July 20, 2010, utilizing the Nançay Decameter Array (NDA). This event was accompanied by a swift halo coronal mass ejection (CME), which reached Earth at 18:20 UT on July 21. Our observations indicate that the radio source is located near the center of the CME front, as depicted in white light images captured by STEREO-Ahead/EUVI 195 Å. The concentration of radio flux exhibits significant changes during the initial hour following the flare, followed by a rapid decline that persists over several weeks. The radio signal displays a power-law distribution across frequencies from 1 MHz to 5 GHz, with the spectral index rapidly dropping below 100 MHz but remaining relatively stable at higher frequencies.",
        "ori-fast-z-score": -1.1920791213585393,
        "water-fast-z-score": 3.841143835488627,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We introduce the concept of algebraic charge liquids, which are defined as ground states of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators. We show how to construct such models for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions. The resulting model is exactly solvable when G has no non-trivial subgroups. In this case we find that there exists at least one phase transition between different phases characterized by distinct topological orders. For example, if G = Z2 × Z2 then our construction yields two gapped phases distinguished by their chiral central charges c− = 0 or 1. If G contains a nontrivial subgroup H then the system exhibits gapless excitations corresponding to particles transforming according to irreducible representations (irreps) of H. These results provide new insights into the classification problem of quantum many-body systems.",
        "watermark_text": "We introduce the notion of algebraic charge liquids , which are represented as ground regions of Hamiltonians with local interactions that can be written in terms of fermionic creation and annihilation operators . We see how to build such descriptions for any finite group G by using an explicit representation of G on the Hilbert space of spinless fermions .The resulting theory is precisely solvable when G has no non - trivial subgroups . In this situation we find that there exists at least one phase shift between various phases characterized by separate topological orders .For instance , if G = Z2 × Z2 then our construction gives two gapped phases distinguished by their chiral central charges c− = 0 or 1 . If G contains a nontrivial subgroup H then the system displays gapless excitations corresponding to ions transforming according to irreducible representations ( irreps ) of H . These data provide fresh insights into the characterization question of quantum several - bodies systems .",
        "rewrite_text": "We introduce the concept of algebraic charge liquids, which are characterized as ground states of Hamiltonians featuring local interactions expressed through fermionic creation and annihilation operators. We demonstrate how to construct these descriptions for any finite group \\( G \\) by utilizing an explicit representation of \\( G \\) on the Hilbert space of spinless fermions. The resulting theory becomes exactly solvable when \\( G \\) lacks any non-trivial subgroups. In this case, we uncover that there exists at least one phase shift between different phases, each defined by distinct topological orders. For example, with \\( G = \\mathbb{Z}_2 \\times \\mathbb{Z}_2 \\), our construction yields two gapped phases, differentiated by their chiral central charges \\( c^- = 0 \\) or \\( 1 \\). If \\( G \\) includes a nontrivial subgroup \\( H \\), the system exhibits gapless excitations linked to ions that transform according to irreducible representations of \\( H \\). These findings offer new perspectives on the characterization of quantum many-body systems.",
        "ori-fast-z-score": -1.5,
        "water-fast-z-score": 4.75,
        "rewrite-fast-z-score": -1.2309149097933272
    },
    {
        "original_text": "We present the results of our numerical simulations of accreting black holes in which we have included general relativistic effects and radiative transfer using Monte Carlo techniques. We find that for low mass (M < 10 M_sun) black holes, there is an inner edge to the disk at about 3 Schwarzschild radii where the density drops sharply by several orders of magnitude. The temperature profile shows a sharp rise near this radius due to compression as well as heating by viscous dissipation. For higher masses (10 M_sun < M < 100 M_sun), the disks are optically thick throughout their extent with no clear evidence of any inner edge. In these cases, the temperature profiles show a gradual increase towards smaller radii. Finally, for very massive black holes (M > 100 M_sun), we find that the disks become geometrically thin but remain optically thick out to large distances.",
        "watermark_text": "We present the conclusion of our numerical simulations of accreting brown holes in which we have incorporated general relativistic effects and radiative transfer use Monte Carlo methods . We see that for low weight ( M < 10 M _ sun ) white holes , there is an inner boundary to the disk at about 3 Schwarzschild radii where the density decreases sharply by many orders of magnitude .The temperature profile displays a sharp rise near this radius due to compression as well as burning by viscous dissipation . For larger masses ( 10 M _ sun < M < 100 M _ sun ) , the disks are optically dense throughout their depth with no clear proof of any outer edge .In these circumstances , the temperature profiles exhibit a slow increase towards smaller radii . Finally , for very huge black holes ( M > 100 M _ sun ) , we find that the disks become geometrically thin but remain optically dense out to large distances .",
        "rewrite_text": "We present the findings from our numerical simulations of accreting black holes, incorporating general relativistic effects and using Monte Carlo methods for radiative transfer. Our results indicate that for lower mass black holes (M < 10 M_sun), an inner boundary exists in the disk at approximately 3 Schwarzschild radii, where the density decreases dramatically by several orders of magnitude. The temperature profile shows a sharp increase near this radius, resulting from compression and viscous dissipation. For black holes with larger masses (10 M_sun < M < 100 M_sun), the disks remain optically dense throughout, lacking a distinct outer edge. In this regime, the temperature profiles gradually rise towards smaller radii. Lastly, for very massive black holes (M > 100 M_sun), we observe that the disks become geometrically thin while still remaining optically dense even at considerable distances.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.520269441183293,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "We present the results of numerical simulations aimed at studying the formation and evolution of off-center ionized hydrogen (HII) regions around massive stars embedded in an isothermal, self-gravitating gas distribution with a power-law density profile.  We find that the expansion velocity of the ionization front decreases as it moves away from the star s position towards larger radii. The decrease occurs because the pressure gradient becomes smaller when moving outward due to the decreasing density. As a result, the ionization front may become unstable against small perturbations which lead to its fragmentation into several blobs or fingers. These blobs are surrounded by dense shells of neutral material produced by photo-evaporation of the surrounding medium. In addition, we show that the mass loss rate increases significantly for large values of the index n of the power law density distribution. This increase can be explained by the fact that the gravitational force per unit volume acting on the shell decreases faster than the ram pressure of the expanding bubble.",
        "watermark_text": "We present the conclusion of computational simulations aiming at studying the formation and evolution of off - center ionized hydrogen ( HII ) areas around large stars exposed in an isothermal , self - gravitating gas distribution with a power - law density profile . We see that the expansion velocity of the ionization front decreases as it travels away from the star s position towards higher radii .The reduction appears because the pressure gradient becomes lower when moved outward owing to the decreasing volume . As a result , the ionization front may grow unstable against small perturbations which lead to its fragmentation into numerous blobs or fingers .These blobs are surrounded by dense shells of neutral material created by photo - evaporation of the nearby medium . In addition , we find that the mass loss rate grows significantly for large values of the index n of the power law density function .This increase can be described by the fact that the gravitational pressure per unit volume acting on the shell decreases quicker than the ram tension of the expanding bubble .",
        "rewrite_text": "We present the findings from our computational simulations aimed at examining the formation and development of off-center ionized hydrogen (HII) regions around massive stars, set within an isothermal, self-gravitating gas distribution characterized by a power-law density profile. Our results indicate that the expansion velocity of the ionization front diminishes as it moves away from the star toward larger radii. This decrease is attributed to the lowering pressure gradient experienced as it travels outward, a consequence of the decreasing volume. Consequently, the ionization front can become unstable in response to minor perturbations, leading to its fragmentation into numerous blobs or fingers. These blobs are encased in dense shells of neutral material, which are generated by the photo-evaporation of the surrounding medium. Furthermore, our analysis reveals that the mass loss rate significantly increases with higher values of the power-law index, n. This phenomenon can be explained by the fact that the gravitational pressure per unit volume acting on the shell diminishes more rapidly than the ram pressure exerted by the expanding bubble.",
        "ori-fast-z-score": -1.1659976680069961,
        "water-fast-z-score": 4.050813610756598,
        "rewrite-fast-z-score": 0.10846522890932808
    },
    {
        "original_text": "The Sloan Digital Sky Survey (SDSS) is an ambitious project to map one quarter of the sky in five photometric bands, and obtain spectra for over 100 million galaxies, quasars, stars, and other celestial objects.  The survey has been completed on two thirds of its original footprint; it will be complete by 2008. This talk describes some of the science that can be done using this data set. It also discusses how astronomers are working together to make use of these vast resources. In particular I describe my own work on studying galaxy clusters as dark matter halos, measuring their masses through gravitational lensing effects, and determining their evolution across cosmic time. Finally, I discuss plans for future surveys which will build upon SDSS s successes. The Sloan Digital Sky Survey is an ambitious project to measure the positions and redshifts of more than 100 million galaxies, quasARS, stars, and other objects in our Universe.",
        "watermark_text": "The Sloan Digital Sky Survey ( SDSS ) is an ambitious project to map one quarter of the heavens in five photometric bands , and collect spectra for over 100 million galaxies , quasars , stars , and other celestial entities . The survey has been completed on two thirds of its initial footprint ; it will be full by 2008 .This discussion describes some of the science that can be performed using this data set . It especially discusses how astronomers are working united to make using of these enormous resources .In particular I outline my own research on studying galaxy galaxies as dark matter halos , measuring their masses through gravity lensing effects , and determining their development across universe time . Finally , I discuss projects for future surveys which will build upon SDSS s successes .The Sloan Digital Sky Survey is an ambitious project to measure the places and redshifts of more than 100 million galaxies , quasARS , stars , and other objects in our Universe .",
        "rewrite_text": "The Sloan Digital Sky Survey (SDSS) is a groundbreaking initiative aimed at mapping a quarter of the sky across five photometric bands, while also collecting spectra for over 100 million galaxies, quasars, stars, and various other celestial bodies. As of now, two-thirds of the survey's initial area has been completed, with plans for full completion by 2008. This discussion highlights some of the scientific opportunities presented by this extensive dataset, focusing on the collaborative efforts of astronomers to maximize these vast resources. I also present my own research, which involves studying galaxies as dark matter halos, measuring their masses through gravitational lensing, and analyzing their evolution over cosmic time. Furthermore, I will touch upon future survey projects that aim to expand on the achievements of the SDSS. Overall, the Sloan Digital Sky Survey represents a significant endeavor to catalog the locations and redshifts of over 100 million galaxies, quasars, stars, and other objects within our universe.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 6.604006604009906,
        "rewrite-fast-z-score": 0.629940788348712
    },
    {
        "original_text": "The present work is devoted to the investigation of the photothermal properties and dynamics of the Cu2O/CuO nanocomposite films prepared by pulsed laser deposition (PLD) on Si(100). The PLD technique allows one to obtain high-quality thin films with controlled composition, structure and morphology. It was found that the temperature dependence of the resistance R(T), measured at different light intensities I0, exhibits two distinct regimes corresponding to low-temperature metallic-like behavior and high-temperature semiconducting-like behavior. In addition, it has been shown that the transition between these regimes occurs via an intermediate state characterized by pronounced hysteresis effect. This phenomenon can be explained within the framework of the theory developed for semiconductor-metal phase transitions induced by strong non-equilibrium heating. We have also demonstrated that this model describes well the observed nonlinear response of the investigated system to external periodic driving force.",
        "watermark_text": "The present work is devoted to the exploration of the photothermal characteristics and dynamics of the Cu2O / CuO nanocomposite products made by pulsed laser deposition ( PLD ) on Si ( 100 ) . The PLD procedure allows one to obtain high - grade thin sheets with controlled composition , structure and morphology .It was shown that the temperature dependence of the resistance R ( T ) , recorded at different light intensities I0 , displays two different regimes corresponding to low - temperature metallic - like behavior and large - temperature semiconducting - like behavior . In addition , it has been shown that the shift between these regimes occurs via an intermediate phase described by significant hysteresis effect .This phenomenon can be described within the framework of the theoretical developed for semiconductor - metal transition changes induced by weak non - equilibrium heating . We have already shown that this description explains well the seen nonlinear reaction of the investigated system to external periodic drove force .",
        "rewrite_text": "This study focuses on investigating the photothermal properties and dynamics of Cu2O / CuO nanocomposite materials produced through pulsed laser deposition (PLD) on Si (100) substrates. The PLD technique enables the fabrication of high-quality thin films with precisely controlled composition, structure, and morphology. Research findings reveal that the temperature-dependent resistance R(T), measured at various light intensities I0, exhibits two distinct regimes: a metallic-like behavior at low temperatures and a semiconductor-like behavior at high temperatures. Moreover, the transition between these regimes is marked by an intermediate phase characterized by a significant hysteresis effect. This phenomenon can be explained using a theoretical framework developed for semiconductor-metal transitions triggered by weak non-equilibrium heating. Our previous work has demonstrated that this theoretical model effectively accounts for the nonlinear responses observed in the system under external periodic driving forces.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.812381937190964,
        "rewrite-fast-z-score": -1.162476387438193
    },
    {
        "original_text": "We report the observation of dynamic condensation of water vapor on crack tips during slow fracture experiments performed under vacuum conditions (10-6 mbar) and low temperature (77 K). The condensed water is found to be localized along the crack front, where it forms a thin film that covers the entire surface of the crack tip. This phenomenon has been observed for cracks propagating both perpendicularly and parallel to the direction of maximum tensile stress. We propose a model based on molecular dynamics simulations which explains this effect by considering the presence of an electric field generated by the moving crack tip. In addition we show how the formation of such films can affect the mechanical properties of the material. Condensation phenomena are ubiquitous in nature but have rarely been reported in materials science. Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses. These results were obtained using a combination of optical microscopy techniques with environmental scanning electron microscopy (ESEM), Raman spectroscopy and infrared reflection absorption spectroscopy (IRAS).",
        "watermark_text": "We report the observation of dynamic condensation of liquid vapor on break tips during slow fracture studies performed under vacuum environments ( 10 - 6 mbar ) and low heat ( 77 K ) . The condensed water is found to be localized along the crack front , where it creates a thin film that covers the entire surface of the crack tip .This phenomenon has been observed for faults propagating both perpendicularly and parallel to the direction of maximum tensile tension . We suggest a theory based on protein mechanics simulations which explains this effect by assuming the presence of an electric field produced by the moved crack edge .In addition we study how the formation of such films can affect the structural properties of the material . Condensation problems are ubiquitous in nature but have seldom been reported in materials science .Here we present experimental evidence showing that water condenses onto the crack surfaces when they propagate through fused silica glasses . These conclusions were obtained using a combination of optical microscopy methods with environmental scan electron microscopy ( ESEM ) , Raman spectroscopy and infrared reflection detection spectroscopy ( IRAS ) .",
        "rewrite_text": "We present our findings on the dynamic condensation of liquid vapor at crack tips observed during slow fracture experiments conducted in vacuum conditions (10^-6 mbar) and at low temperatures (77 K). The condensed water is localized along the crack front, forming a thin film that encompasses the entire surface of the crack tip. This phenomenon has been noted for faults advancing in both perpendicular and parallel orientations to the direction of maximum tensile stress. We propose a theoretical framework based on simulations of protein mechanics, which posits that the effect arises from an electric field generated by the moving edge of the crack. Furthermore, we investigate how the formation of these films influences the structural properties of the material. While condensation issues are common in nature, they have rarely been documented in materials science. Our experimental evidence demonstrates that water condenses on the crack surfaces during the propagation of cracks in fused silica glasses. These findings were attained using a combination of optical microscopy, environmental scanning electron microscopy (ESEM), Raman spectroscopy, and infrared reflection absorption spectroscopy (IRAS).",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 5.1759731137650435,
        "rewrite-fast-z-score": 1.7253243712550146
    },
    {
        "original_text": "We present an exact hierarchical description of the reduced state and dynamics of open quantum systems in terms of a set of coupled equations, which we call the Hierarchy of Density Tensors (HDT). The HDT is derived by applying the Nakajima-Zwanzig projection operator technique to the von Neumann equation describing the evolution of the total system. We show that this approach allows one to retrieve all relevant information about the environment-induced decoherence process on arbitrary timescales. In particular, it provides access to the full spectrum of relaxation rates characterizing the decay of off-diagonal elements of the reduced density matrix as well as the stationary states reached at late times. As an example, we apply our formalism to study the dissipative spin-boson model with Ohmic dissipation. Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method. \nI. INTRODUCTORY REMARK\nThe understanding of how macroscopic objects behave under the influence of their environments has been a central issue in physics since its very beginning  1, 2  . This problem becomes particularly challenging when dealing with complex many-body systems such as condensed matter or biological ones  3, 4  , where the number of degrees of freedom involved can be extremely large. A powerful theoretical tool to tackle these problems consists in studying the dynamics of the reduced state of the system of interest S conditioned upon some specific measurement performed over the environmental degrees of freedom E  5, 6  .\nIn recent years there have been several attempts to develop efficient methods to describe the time-evolution of the reduced state  7, 8  . Among them, the so-called Hierarchy of Density Matrices (HDM)  9  represents a promising alternative to other approaches  10, 11  due to its ability to capture non-Markovian effects  12  . However, despite being able to provide accurate predictions for short-time evolutions  13  , the HDM fails to reproduce correctly the asymptotic behavior of the system  14  . To overcome this limitation, here we introduce a new formulation of the HDM, called Hierarchy of Density...",
        "watermark_text": "We create an precise hierarchical description of the reduced state and dynamics of open quantum systems in terms of a setting of coupled equations , which we call the Hierarchy of Density Tensors ( HDT ) . The HDT is calculated by using the Nakajima - Zwanzig projection operator technique to the von Neumann equation explaining the evolution of the total system .We see that this methodology allows one to locate all relevant information about the environment - caused decoherence process on arbitrary timescales . In particular , it gives access to the full range of relaxation frequencies characterizing the decay of off - diagonal elements of the reduced density matrix as well as the stationary states reached at late times .As an instance , we apply our formalism to study the dissipative spin - boson theory with Ohmic dissipation . Our results are compared against numerical simulations based on the Quantum Monte Carlo Wavefunction method .I . INTRODUCTORY REMARK The knowledge of how macroscopic objects react under the impact of their environments has been a central topic in physics since its very beginning 1 , 2 .This problem appears particularly challenging when dealing with difficult large - bodies systems such as condensed matter or biological ones 3 , 4 , where the quantity of degrees of freedom employed can be extremely huge . A good experimental tool to tackle these problems involves in examining the dynamics of the reduced state of the system of interest S conditioned upon some specific assessment performed over the environmental degrees of liberty E 5 , 6 .In past decades there have been numerous attempts to develop able methods to explain the period - progression of the reduced state 7 , 8 . Among them , the so - called Hierarchy of Density Matrices ( HDM ) 9 offers a successful approach to other methods 10 , 11 due to its able to capture non - Markovian effects 12 .However , despite being able to provide accurate forecast for short - time evolutions 13 , the HDM fails to predict correctly the asymptotic behavior of the system 14 . To solve this limitation , here we incorporate a new implementation of the HDM , entitled Hierarchy of Density . . .",
        "rewrite_text": "We develop a precise hierarchical framework to describe the reduced state and dynamics of open quantum systems through a set of coupled equations, which we refer to as the Hierarchy of Density Tensors (HDT). This framework is derived using the Nakajima-Zwanzig projection operator technique applied to the von Neumann equation that governs the evolution of the entire system. Our approach enables us to capture all pertinent details regarding the decoherence processes induced by the environment across various timescales. Notably, it provides comprehensive insight into the relaxation frequencies that characterize the decay of off-diagonal elements of the reduced density matrix, as well as the stationary states attained in the long term. We exemplify our formalism by investigating the dissipative spin-boson model with Ohmic dissipation, comparing our findings with numerical simulations obtained through the Quantum Monte Carlo Wavefunction method.\n\n**I. INTRODUCTORY REMARKS**  \nUnderstanding how macroscopic objects respond to their environments has been a fundamental topic in physics since its inception. This challenge is particularly pronounced in complex systems, such as those found in condensed matter and biological contexts, where the number of degrees of freedom can be exceedingly large. A valuable experimental strategy for addressing these challenges is to analyze the dynamics of the reduced state of the system of interest (S) conditioned on specific measurements performed on the environmental degrees of freedom (E). Over the past decades, various methodologies have been developed to elucidate the time evolution of the reduced state. Among these, the Hierarchy of Density Matrices (HDM) has emerged as an effective approach due to its ability to capture non-Markovian effects. However, while the HDM is adept at predicting short-time dynamics, it struggles to accurately determine the asymptotic behavior of the system. To address this shortcoming, we propose an enhanced version of the HDM, which we label the Hierarchy of Density.",
        "ori-fast-z-score": -0.40961596025952024,
        "water-fast-z-score": 7.675067860720625,
        "rewrite-fast-z-score": 0.7071067811865475
    },
    {
        "original_text": "We present new observations and analysis of the neutral hydrogen (HI) disks surrounding isolated galaxies, using data obtained with the Very Large Array (VLA). We have observed 12 nearby galaxies at 21 cm wavelength to determine their total HI mass and distribution within the optical disk. The sample includes both late-type spirals and dwarf irregulars. Our results show that all but one galaxy has an extended HI halo beyond its optical radius; this is true even for the most gas-rich systems such as NGC 4254 and NGC 5253. In addition, we find evidence for significant amounts of atomic gas outside our detection limits which may be associated with tidal features or other interactions between these galaxies and neighboring companions. These findings are consistent with previous studies showing that many isolated spiral galaxies contain large quantities of cold interstellar medium distributed over several kiloparsecs. \n \n Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "watermark_text": "We report new studies and investigation of the neutral hydrogen ( HI ) disks comprising isolated stars , using data acquired with the Very Large Array ( VLA ) . We have noted 12 nearby galaxies at 21 cm wavelength to obtain their total HI mass and distribution within the optical disk .The sample comprises both late - class spirals and dwarf irregulars . Our results show that all but one star has an extended HI halo beyond its optical diameter ; this is true even for the most gas - rich complexes such as NGC 4254 and NGC 5253 .In addition , we find proof for significant amounts of atomic liquid outside our detection limits which may be correlated with tidal features or other relationships between these objects and surrounding companions . These conclusions are compatible with previous research indicating that several isolated spiral clusters hold significant amounts of cold interstellar medium spread over several kiloparsecs .Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure",
        "rewrite_text": "We present new findings from our studies of neutral hydrogen (HI) disks surrounding isolated stars, utilizing data obtained from the Very Large Array (VLA). In our investigation, we observed 12 nearby galaxies at a 21 cm wavelength to assess their total HI mass and distribution within the optical disk. The sample includes late-class spirals and dwarf irregular galaxies. Our results indicate that nearly all the stars have an extensive HI halo that extends beyond their optical boundaries, even in the most gas-rich regions such as NGC 4254 and NGC 5253. Furthermore, we discovered evidence of substantial amounts of atomic hydrogen beyond our detection limits, which may be linked to tidal features or other interactions with surrounding companions. These findings align with previous studies suggesting that several isolated spiral clusters possess significant quantities of cold interstellar medium distributed over several kiloparsecs. Keywords: Atomic Hydrogen, Galaxy Evolution, Spiral Structure.",
        "ori-fast-z-score": -0.5852057359806528,
        "water-fast-z-score": 5.969098507002659,
        "rewrite-fast-z-score": -1.162476387438193
    },
    {
        "original_text": "We present the results of numerical simulations of planetary systems around binary stars, focusing on the effects of orbital migration due to tidal interaction between planets and their host star(s). We find that for most initial conditions considered here (except those with very small semi-major axes), the final orbits are generally circularized by tides raised on the planet by its host star(s); this is true even if the original orbit was eccentric or inclined relative to the binary s plane.  The final mass distribution of planets depends strongly on the initial semimajor axis of the planet. For large values of the initial semi-major axis, we find that the final masses tend to be smaller than the initial ones because of significant inward migration driven by tidal dissipation inside the planet. On the other hand, when the initial semi-major axis is relatively small compared to the binary separation, the final masses can become larger than the initial ones as a result of outward migration caused by tidal friction acting at the surface of the planet.",
        "watermark_text": "We present the conclusion of computational simulations of planetary structures around binary stars , concentrating on the effects of orbital movement owing to tidal interaction between planets and their host star ( s ) . We see that for most initial conditions considered here ( except those with very small semi - major axes ) , the last orbits are typically circularized by tides raised on the planet by its host star ( s ) ; this is true even if the previous orbit was eccentric or tilted relative to the binary s plane .The final mass distribution of planets depends strongly on the first semimajor axis of the planet . For large values of the first semi - major axis , we find that the last masses tend to be less than the first ones because of large inward movement coupled by tidal dissipation inside the planet .On the other hand , when the first semi - major axis is fairly little compared to the binary separation , the last masses can turn larger than the first ones as a due of outward migration caused by tidal tension acted at the surface of the planet .",
        "rewrite_text": "We present the findings from our computational simulations of planetary systems around binary stars, focusing on the impact of orbital dynamics resulting from tidal interactions between planets and their host stars. Our results indicate that for the majority of initial conditions examined (excluding those with very small semi-major axes), the final orbits tend to circularize due to the tides raised on the planets by their host stars. This circularization occurs even if the previous orbits were eccentric or tilted in relation to the binary's orbital plane. The ultimate mass distribution of the planets is significantly influenced by their initial semi-major axis. Specifically, for larger initial semi-major axes, we observe that the final masses typically decrease compared to their initial values, largely due to inward migration and tidal dissipation within the planets. Conversely, when the initial semi-major axis is relatively small compared to the distance between the binary stars, the final masses can increase compared to the initial ones, a result of outward migration driven by tidal forces acting on the planet's surface.",
        "ori-fast-z-score": -2.681695240272863,
        "water-fast-z-score": 4.125684985035173,
        "rewrite-fast-z-score": -0.4975185951049946
    },
    {
        "original_text": "We present an analysis of the broadband (0.5-10 keV) X-ray spectrum of the radio galaxy 3C 445, using data obtained with XMM-Newton and Chandra observatories in 2001-2002 . The observed X-ray emission is dominated by a hard power-law component which can be fitted equally well either by thermal Comptonization or non-thermal inverse Compton scattering models. We find that both models require a large amount of cold matter to produce the soft excess below 1 keV. This suggests that there are two distinct components contributing to the X-ray emission - one associated with hot plasma and another related to cool gas clouds. In addition we detect several narrow absorption lines at energies corresponding to highly ionized species such as O vii , Ne ix , Mg xi and Si xiii . These features may originate in outflows driven by nuclear activity. Finally, we report on the detection of Fe Kα line at 6.4 keV produced by reflection off distant material.",
        "watermark_text": "We present an analysis of the broadband ( 0 . 5 - 10 keV ) X - ray signal of the radio star 3C 445 , using data acquired with XMM - Newton and Chandra observatories in 2001 - 2002 . The observed X - ray radiation is dominated by a hard energy - law component which can be fit equally poorly either by thermal Comptonization or non - thermal integral Compton absorption theories .We see that both models require a large amount of cold matter to produce the soft excess below 1 keV . This implies that there are two different components contributing to the X - ray radiation - one related with hot plasma and another linked to cool gas clouds .In addition we perceive several small absorption patterns at energies corresponding to strongly ionized species such as O vii , Ne ix , Mg xi and Si xiii . These features could occur in outflows driven by nuclear activity .Finally , we publish on the observation of Fe Kα line at 6 . 4 keV generated by absorption off remote material .",
        "rewrite_text": "We provide an analysis of the broadband X-ray signal (0.5 - 10 keV) from the radio star 3C 445, utilizing data collected by the XMM-Newton and Chandra observatories during 2001-2002. The X-ray emissions are predominantly characterized by a hard energy-law component, which is not well modeled by either thermal Comptonization or non-thermal integral Compton absorption theories. Both models indicate the presence of significant cold matter necessary to explain the soft excess observed below 1 keV. This suggests that two distinct components are contributing to the X-ray radiation: one associated with hot plasma and the other with cooler gas clouds. Additionally, we observe several minor absorption features at energies corresponding to highly ionized elements such as O VII, Ne IX, Mg XI, and Si XIII, possibly arising from outflows driven by nuclear activity. Finally, we report the detection of the Fe Kα line at 6.4 keV, which is attributed to absorption from distant material.",
        "ori-fast-z-score": -1.8599622199011085,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "We present the luminosity function (LF) for galaxies in the local universe at rest-frame wavelengths between 3 and 24 microns, using data obtained with Spitzer Space Telescope s Infrared Array Camera (IRAC). We use two different methods to estimate the LF - one based on direct counts of galaxies within bins of absolute magnitude, and another that uses an analytic model fit to these galaxy counts. The results are consistent across both techniques. Our best-fit Schechter parameters are: M* = -19.6 +/- 0.1 mag., log(L/Lsun) * = 10.9 +/- 0.2 dex, and alpha = -1.3 +/- 0.4. These values agree well with previous determinations made by other authors over similar wavelength ranges. However, we find evidence for a significant excess number density of faint sources relative to predictions from our best-fit Schechte functions. This excess is most pronounced at longer wavelengths where it amounts to ~50% more objects than expected.",
        "watermark_text": "We present the luminosity function ( LF ) for galaxies in the local universe at rest - frame wavelengths between 3 and 24 microns , using data acquired with Spitzer Space Telescope s Infrared Array Camera ( IRAC ) . We use two different methods to estimate the LF - one based on direct counts of stars within bins of absolute magnitude , and another that using an analytic method suited to these galaxy counts .The results are compatible across both strategies . Our best - fitting Schechter parameters are : M * = - 19 . 6 + / - 0 . 1 mag . , log ( L / Lsun ) * = 10 . 9 + / - 0 . 2 dex , and alpha = - 1 . 3 + / - 0 . 4 .These quantities agree well with previous determinations made by other researchers over similar frequency bands . However , we find proof for a substantial excess amount density of faint objects relative to observations from our better - fitting Schechte functions .This excess is most pronounced at higher wavelengths where it amounts to ~ 50 % more items than expected .",
        "rewrite_text": "We present the luminosity function (LF) for galaxies in the local universe at rest-frame wavelengths between 3 and 24 microns, utilizing data collected with the Spitzer Space Telescope's Infrared Array Camera (IRAC). To estimate the LF, we employed two distinct methods: one that involves direct counts of galaxies categorized by absolute magnitude bins, and another based on an analytical approach suitable for these galaxy counts. The results obtained from both methods are consistent with each other. Our best-fitting Schechter parameters are: M* = -19.6 ± 0.1 mag, log(L/Lsun)* = 10.9 ± 0.2 dex, and α = -1.3 ± 0.4. These values align well with previous measurements reported by other researchers in comparable frequency ranges. However, we observe a significant excess in the density of faint objects relative to our well-fitting Schechter functions. This excess is particularly pronounced at longer wavelengths, where it represents approximately a 50% increase in the number of objects compared to expectations.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -1.3242443839434612
    },
    {
        "original_text": "We present new calculations for massive star evolution, nucleosynthesis, and yields using updated nuclear reaction rates and solar abundances. We find that our results are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). The sensitivity is particularly strong when we use the most recent recommended value for the 12C(alpha, gamma)16O cross section. This result has important implications for studies of chemical enrichment by supernovae Ia progenitors. \n \n Keywords: Nuclear reactions; Supernovae; Stellar evolution; Yields \n \n 1 Introduction \n \n In this work we study how uncertainties in nuclear physics affect predictions about stellar evolution and nucleosynthesis. Our goal is to understand better what can be learned from observations of stars and their remnants. For example, it is well known that there exist large discrepancies between observed elemental abundance ratios in metal-poor halo stars and those predicted by standard models of galactic chemical evolution  1  . These differences may arise because some key nuclear processes have been poorly understood or not included in current evolutionary codes  2  , but they could also reflect systematic errors in observational data  3  .\n \nIn order to address these issues, we perform detailed numerical simulations of massive star evolution with different sets of input parameters. Specifically, we consider two cases where the initial mass fraction of helium XHe = 0.25 and 0.30 respectively  4  . We evolve each model until its core collapses into a neutron star. During the collapse phase, we follow the hydrodynamics of the explosion as described in  5  . Afterwards, we compute the composition of the ejecta using an improved version  6  of the one-dimensional post-processing code developed originally by  7  . \n \n 2 Input Physics and Numerical Methods",
        "watermark_text": "We report new models for huge galaxy evolution , nucleosynthesis , and yields using updated atomic reaction rates and solar abundances . We see that our findings are susceptible to uncertainties in the helium burning rate at high temperatures ( T > 2 x 10 ^ 9 K ) .The sensitivity is especially powerful when we using the most current recommended estimate for the 12C ( beta , alpha ) 16O cross area . This result has significant implications for research of chemical enrichment by supernovae Ia progenitors .Keywords : Nuclear effects ; Supernovae ; Stellar evolution ; Yields 1 Introduction In this research we study how uncertainties in nuclear science affect assumptions about stellar evolution and nucleosynthesis . Our goal is to study improve what can be learned from measurements of stars and their remnants .For instance , it is well established that there exist large discrepancies between measured elemental availability proportions in metal - scarce halo stars and those predicted by traditional models of galactic chemical evolution 1 . These similarities may arise because some important radioactive processes have been poorly described or not incorporated in current evolutionary codes 2 , but they may also reflect widespread errors in observational data 3 .In order to overcome these problems , we perform comprehensive numerical simulations of large star evolution with various sets of input parameters . Specifically , we solve two situations where the initial mass fraction of helium XHe = 0 . 25 and 0 . 30 respectively 4 .We evolve each model until its core collapses into a neutron star . During the collapse phase , we follow the hydrodynamics of the explosion as described in 5 .Afterwards , we compute the composition of the ejecta using an modified edition 6 of the one - dimensional post - processing code developed originally by 7 . 2 Input Physics and Numerical Methods",
        "rewrite_text": "We present new models for the evolution of massive galaxies, nucleosynthesis, and yields, utilizing updated atomic reaction rates and solar abundances. Our analysis reveals that our results are sensitive to uncertainties in the helium burning rate at high temperatures (T > 2 x 10^9 K). This sensitivity is particularly pronounced when we apply the most up-to-date estimates for the 12C(β, α)16O cross-section. These findings hold significant implications for the study of chemical enrichment from Type Ia supernova progenitors.\n\n**Keywords**: Nuclear effects; Supernovae; Stellar evolution; Yields \n\n**1 Introduction**  \nIn this study, we investigate how uncertainties in nuclear science influence our understanding of stellar evolution and nucleosynthesis. Our aim is to enhance the insights that can be gained from observations of stars and their remnants. For example, significant disparities have been noted between the measured proportions of elements in metal-poor halo stars and the predictions made by conventional models of galactic chemical evolution. These discrepancies may stem from the inadequate representation or omission of certain critical radioactive processes in existing evolutionary codes, or they could indicate widespread inaccuracies in observational data.\n\nTo address these challenges, we conduct comprehensive numerical simulations of massive star evolution using various sets of input parameters. Specifically, we consider two scenarios with initial helium mass fractions of XHe = 0.25 and 0.30, respectively. Each model is evolved until its core undergoes collapse into a neutron star. During the collapse phase, we track the hydrodynamics of the explosion as detailed in the relevant literature. Subsequently, we analyze the composition of the ejecta using a modified version of the one-dimensional post-processing code originally developed by earlier researchers. \n\n**2 Input Physics and Numerical Methods**",
        "ori-fast-z-score": -0.6963106238227914,
        "water-fast-z-score": 7.543856734859843,
        "rewrite-fast-z-score": 0.6115928396627265
    },
    {
        "original_text": "We report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier (CBA). The CBA is used as a dispersive readout device for the quantronium, and we show that it can be operated at high bandwidths without significant back-action noise or added phase noise. We demonstrate measurement times up to 1 ms per data point while maintaining quantum-limited sensitivity. This allows us to perform measurements over many relaxation cycles of the system, which are necessary to extract information about its dynamics. In addition, we use this technique to study the effect of different environmental conditions on the coherence time of our system. Our results indicate that the CBA has great potential for future experiments studying open quantum systems. Quantum mechanics predicts that macroscopic objects such as atoms will exhibit wave-like behavior when they interact with light fields. However, these predictions have not been experimentally verified because measuring the properties of individual particles requires extremely sensitive detectors. Here, we present a new method for detecting single photons based on a nonlinear optical process known as parametric down-conversion. Using this approach, we detect single photons emitted by a trapped atom interacting with a laser field inside a high-finesse optical cavity. By coupling the output of the cavity into a second cavity containing a nonlinear crystal, we generate pairs of entangled photons whose frequencies differ by twice the frequency of the pump beam. These photon pairs are then detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode.",
        "watermark_text": "We report on an experiment to measure decoherence in a quantronium qubit using the cavity bifurcation amplifier ( CBA ) . The CBA is utilized as a dispersive readout device for the quantronium , and we find that it can be run at high bandwidths without significant back - action noise or added phase noise .We demonstrate measurement times up to 1 ms per information point while maintaining quantum - limited accuracy . This enables us to conduct measurements over numerous relaxation cycles of the system , which are necessary to extract information about its dynamics .In addition , we utilize this methodology to study the impact of different climate circumstances on the coherence time of our system . Our results show that the CBA has tremendous possibility for future research studying open quantum systems .Quantum theory predicts that macroscopic objects such as atoms will exhibit wave - like behavior when they interact with light fields . However , these predictions have not been experimentally confirmed because measuring the properties of individual molecules requires exceptionally sensitive detectors .Here , we present a new method for detecting single photons using on a nonlinear optical process known as parametric down - conversion . Using this methodology , we identify single photons generated by a captured molecule interacting with a laser field inside a high - finesse optical cavity .By coupling the output of the cavity into a second tube featuring a nonlinear crystal , we produce pairs of entangled photons whose frequencies vary by twice the frequency of the pump beam . These photon pairs are then discovered concurrently by two silicon avalanche photodiodes running in Geiger mode .",
        "rewrite_text": "We present the results of an experiment aimed at measuring decoherence in a quantronium qubit using a cavity bifurcation amplifier (CBA). The CBA serves as a dispersive readout device for the quantronium, and we demonstrate that it can operate at high bandwidths without introducing significant back-action noise or added phase noise. Our findings show that we can achieve measurement times of up to 1 ms per information point while preserving quantum-limited accuracy. This capability allows us to conduct measurements across multiple relaxation cycles of the system, which is essential for extracting insights into its dynamics. Furthermore, we apply this approach to investigate the influence of varying environmental conditions on the coherence time of the system. Our results indicate that the CBA holds great potential for future investigations into open quantum systems. \n\nAdditionally, quantum theory suggests that macroscopic objects like atoms should exhibit wave-like behavior when interacting with light fields. However, these theoretical predictions have yet to be experimentally validated, as probing the properties of individual molecules necessitates exceptionally sensitive detection methods. In this work, we introduce a novel technique for detecting single photons based on a nonlinear optical process known as parametric down-conversion. Using this approach, we successfully identify single photons produced by a molecule interacting with a laser field within a high-finesse optical cavity. By directing the cavity's output into a second tube containing a nonlinear crystal, we generate pairs of entangled photons whose frequencies differ by twice the frequency of the pump beam. These photon pairs are then detected simultaneously by two silicon avalanche photodiodes operating in Geiger mode.",
        "ori-fast-z-score": 2.209379082955976,
        "water-fast-z-score": 8.28517156108491,
        "rewrite-fast-z-score": 2.2183912735402846
    },
    {
        "original_text": "In this work, we consider the problem of multiuser detection (MUD) for code division multiple access systems with time-varying channels. We propose an algorithm that jointly performs user identification and data detection by using a maximum likelihood criterion. The proposed method is based on the expectation-maximization (EM) algorithm which iteratively estimates both the channel coefficients and transmitted symbols. In order to reduce computational complexity, we also develop a low-complexity suboptimal MUD scheme. Numerical results show that our proposed algorithms outperform existing schemes under various scenarios. Index Terms-Data detection, EM algorithm, Multiuser detection, Time varying channels. 1 Introduction Code-division-multiple-access (CDMA) has been widely used as one of the most promising technologies for next-generation wireless communications due to its high spectral efficiency  1  . However, CDMA suffers from severe interference between users caused by multipath propagation  2  , especially when the number of active users increases  3  .\nTo mitigate inter-user interference, multiuser detectors have been developed  4  -  6  . Among them, linear multiuser detectors are attractive because they can be implemented easily at low cost  7  . Unfortunately, these detectors suffer from performance loss compared to optimal multiuser detectors  8  . To improve their performance, nonlinear multiuser detectors such as successive interference cancellation  9  or parallel interference cancellation  10  were introduced. These detectors require accurate knowledge about the received signals  11  . Therefore, blind multiuser detectors  12  -  14  were proposed to estimate unknown parameters without any training sequence  15  . Although blind multiuser detectors do not need prior information about the received signal, they usually perform worse than conventional multiuser detectors  16  .\nRecently, there has been growing interest in developing multiuser detectors for time-varying channels  17  -  20  . Since the channel varies over time, it becomes more difficult to detect the transmitted symbol accurately  21  . Moreover, if the channel changes rapidly, then the detector may fail completely  22  . Thus, it is important to design robust multiuser detectors against rapid channel variations  23  .",
        "watermark_text": "In this research , we investigate the question of multiuser tracking ( MUD ) for code division multiple entry systems with time - differing channels . We suggest an algorithm that together manages user identification and information detection by using a maximum likelihood threshold .The proposed approach is based on the expectation - maximization ( EM ) algorithm which iteratively generates both the channel coefficients and transmitted symbols . In order to reduce numerical complexity , we also build a small - complexity suboptimal MUD scheme .Numerical results show that our proposed methods outperform established algorithms under various circumstances . Index Terms - Data tracking , EM algorithm , Multiuser tracking , Time varying channels .1 Introduction Code - division - multiple - access ( CDMA ) has been widely adopted as one of the most attractive devices for next - class wireless communications thanks to its high spectral power 1 . However , CDMA suffers from severe interference between clients caused by multipath propagation 2 , particularly when the proportion of active consumers increases 3 .To mitigate inter - customer interference , multiuser detectors have been created 4 - 6 . Among them , continuous multiuser detectors are interesting because they can be deployed easily at low cost 7 .Unfortunately , these detectors suffer from performance loss compared to optimal multiuser detectors 8 . To increase their performance , nonlinear multiuser detectors such as consecutive interference cancellation 9 or parallel interference cancellation 10 were introduced .These detectors need accurate knowledge about the received messages 11 . Therefore , blind multiuser detectors 12 - 14 were developed to estimate unknown parameters without any testing sequence 15 .Although blind multiuser detectors do not require prior information about the received signal , they generally work worse than conventional multiuser detectors 16 . Recently , there has been growing interest in building multiuser detectors for time - differing channels 17 - 20 .Since the channel varies over time , it becomes more impossible to locate the transmitted expression accurately 21 . Moreover , if the channel shifts rapidly , then the sensor may fail entirely 22 .Thus , it is important to build robust multiuser detectors against sudden channel variations 23 .",
        "rewrite_text": "This research explores the issue of multiuser detection (MUD) in code division multiple access (CDMA) systems with time-varying channels. We propose an algorithm that simultaneously handles user identification and information detection using a maximum likelihood threshold. Our approach utilizes the expectation-maximization (EM) algorithm, which iteratively estimates both channel coefficients and transmitted symbols. To minimize numerical complexity, we also develop a low-complexity suboptimal MUD scheme. Numerical simulations demonstrate that our methods outperform existing algorithms in various scenarios. \n\nIndex Terms: Data tracking, EM algorithm, Multiuser detection, Time-varying channels.\n\n1. Introduction  \nCDMA has become a favored technology for next-generation wireless communications due to its high spectral efficiency. However, it faces significant interference from multipath propagation, especially as the number of active users increases. To alleviate inter-user interference, multiuser detectors have been designed. Continuous multiuser detectors, in particular, are appealing because they can be implemented easily and cost-effectively. Unfortunately, these detectors typically experience performance degradation compared to optimal multiuser detectors. To enhance their effectiveness, nonlinear multiuser detectors, such as consecutive and parallel interference cancellation, have been developed. However, these approaches require precise knowledge of the received signals. Consequently, blind multiuser detectors have been introduced to estimate unknown parameters without needing training sequences. While blind MU detectors operate without prior information about the received signals, they generally perform worse than conventional multiuser detectors. Recently, there has been an increasing focus on developing multiuser detectors for time-varying channels. Due to the dynamic nature of these channels, accurately locating transmitted signals becomes more challenging, especially during rapid fluctuations that may cause sensor failures. As a result, developing robust multiuser detectors capable of handling sudden channel variations is essential.",
        "ori-fast-z-score": -1.0309670614335873,
        "water-fast-z-score": 8.751130125933031,
        "rewrite-fast-z-score": -1.7962924780409972
    },
    {
        "original_text": "The evolution of stars is driven by nuclear fusion reactions in their cores, which generate energy that powers the star s luminosity.  The rate at which these reactions occur depends on how much fuel (i.e., helium) there is available to burn.   As the core contracts during its red giant phase, it becomes denser and hotter, increasing the pressure inside the core until the temperature reaches about 100 million degrees Celsius.  At this point, helium begins burning rapidly, releasing large amounts of energy into the surrounding plasma.  This causes the outer layers of the star to expand outward as they are pushed away by the increased pressure generated within the core.  Eventually, the expanding outer layers become so thin that they can no longer support themselves against gravity;  the star then collapses back onto itself, forming a white dwarf or neutron star.   In addition to nuclear fusion reactions occurring in the core, some massive stars also undergo explosive nuclear fusion reactions called supernovae, which eject most of the star s mass out into space.   These explosions release huge amounts of energy and light, making them visible across great distances in the night sky.   During my career studying stellar astrophysics, I have been fortunate enough to witness many spectacular events such as these first hand.   ...",
        "watermark_text": "The evolved of stars is influenced by nuclear fusion compounds in their cores , which generate energy that powers the star s luminosity . The rate at which these reactions occur varies on how many fuel ( i . e . , helium ) there is provided to burn .As the core contracts during its red giant stage , it becomes denser and warmer , increasing the pressure inside the core until the temperature exceeds about 100 million degrees Celsius . At this time , helium enters burning fast , releasing large quantities of electricity into the nearby plasma .This forces the exterior layers of the star to expand outward as they are pushed away by the increased pressure generated within the core . Eventually , the increasing inner layers get so thin that they can no longer support themselves against gravity ; the star then collapses back onto itself , forming a black dwarf or neutron galaxy .In addition to nuclear fusion compounds resulting in the core , some enormous stars also perform intense nuclear fusion events called supernovae , which eject most of the star s mass out into space . These bursts return huge amounts of electricity and light , making them visible across huge distances in the night sky .During my life researching stellar astrophysics , I have been fortunate sufficient to witness several spectacular events such as these first hand . . . .",
        "rewrite_text": "The evolution of stars is driven by nuclear fusion processes occurring in their cores, which produce the energy that generates their luminosity. The rate of these reactions depends on the availability of fuel, such as helium, to sustain the fusion. As a star enters its red giant phase, the core contracts, becoming denser and warmer, which raises the internal pressure until it reaches temperatures exceeding approximately 100 million degrees Celsius. At this point, helium fusion accelerates, releasing significant amounts of energy into the surrounding plasma. This surge in pressure causes the outer layers of the star to expand outward. Eventually, the inner layers become so tenuous that they can no longer withstand the force of gravity, leading to the star collapsing in on itself and potentially forming a black dwarf or neutron star. Additionally, some massive stars undergo dramatic events known as supernovae, which expel much of the star's mass into space. These explosive outbursts emit vast amounts of energy and light, making them visible across great distances in the night sky. Throughout my research in stellar astrophysics, I have been fortunate to witness several of these breathtaking events firsthand.",
        "ori-fast-z-score": -0.0949157995752499,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": 0.8251369970070347
    },
    {
        "original_text": "We present an analysis of pulsar observations to determine the magnetic field strength in the solar corona at heights between 1 and 3 R_Sun . We use data obtained with the Nançay Radio Telescope (NRT) for two different radio frequencies, 327 MHz and 1420 MHz, corresponding to emission heights of about 2 and 5 R_Sun , respectively. The observed pulse profiles are modeled using a simple model that includes contributions from both the local interstellar medium and the solar wind plasma. From these models we derive estimates for the coronal magnetic field strengths as well as the electron density distribution along the line-of-sight towards PSR B1133+16 .\nThe results show that the magnetic field decreases rapidly with height above the photosphere but is still strong enough to confine energetic particles up to several solar radii away from the Sun s surface. This suggests that particle acceleration processes may be taking place throughout most of the solar atmosphere.",
        "watermark_text": "We present an assessment of pulsar observations to estimate the magnetic force size in the solar corona at heights between 1 and 3 R _ Sun . We use data acquired with the Nançay Radio Telescope ( NRT ) for two different radio altitudes , 327 MHz and 1420 MHz , corresponding to emission heights of about 2 and 5 R _ Sun , respectively .The observed pulse profiles are modeled using a simple simulation that contains contributions from both the local interstellar material and the sun breeze plasma . From these models we derive estimates for the coronal magnetic field strengths as also as the electron concentration distribution along the line - of - view towards PSR B1133 + 16 .The results show that the magnetic force reduces rapidly with width above the photosphere but is nevertheless strong enough to confine energetic particles up to several solar radii away from the Sun s surface . This implies that particle acceleration processes possibly be taking place throughout most of the solar atmosphere .",
        "rewrite_text": "We present an evaluation of pulsar observations aimed at estimating the magnitude of the magnetic force in the solar corona at altitudes between 1 and 3 solar radii (R⊙). Our analysis utilizes data from the Nançay Radio Telescope (NRT) collected at two different radio frequencies, 327 MHz and 1420 MHz, which correspond to emission heights of approximately 2 and 5 R⊙, respectively. The observed pulse profiles are simulated using a straightforward model that incorporates contributions from both local interstellar material and solar wind plasma. From these simulations, we derive estimates of the coronal magnetic field strengths as well as the distribution of electron density along the line of sight towards PSR B1133 + 16. Our findings indicate that while the magnetic force decreases rapidly with height above the photosphere, it remains sufficiently strong to confine energetic particles up to several solar radii from the Sun's surface. This suggests that particle acceleration processes may occur throughout much of the solar atmosphere.",
        "ori-fast-z-score": -0.35603449745815596,
        "water-fast-z-score": 4.780914437337574,
        "rewrite-fast-z-score": -0.11704114719613057
    },
    {
        "original_text": "We study the vacant set of random interlacements in Rd, d>1, which is defined as the complement of the union of all open paths starting at 0 up to time 1. We prove that it has Hausdorff dimension equal to d-1 almost surely by showing that its upper Minkowski dimension equals this value with probability one. This result extends previous results on the vacant set of simple random walk obtained by Lawler, Schramm and Werner (Annals of Probability 24(3), 1997) and Benjamini, Kalai and Schramm (Journal of Functional Analysis 157(1), 2000). The proof relies heavily on recent developments concerning the geometry of Brownian motion and the theory of stable processes. In particular we use an estimate for the Green function of the killed Brownian motion due to Bass and Burdzy (Annales de l Institut Henri Poincare 22(2), 1999).\nThe main motivation behind our work comes from percolation theory where the vacant set of random walks plays a crucial role. Indeed, we show how our techniques can be used to obtain new information about the critical behavior of bond-percolation models on Zd.",
        "watermark_text": "We test the empty set of random interlacements in Rd , d > 1 , which is defined as the complement of the union of all open routes starting at 0 up to time 1 . We establish that it has Hausdorff size equal to d - 1 fairly surely by showing that its outer Minkowski dimension equals this value with probability one .This result continues preceding results on the empty set of simple random walk obtained by Lawler , Schramm and Werner ( Annals of Probability 24 ( 3 ) , 1997 ) and Benjamini , Kalai and Schramm ( Journal of Functional Analysis 157 ( 1 ) , 2000 ) . The proof draws highly on current developments concerning the topology of Brownian movement and the notion of stable processes .In particular we using an estimate for the Green function of the dead Brownian moving thanks to Bass and Burdzy ( Annales de l Institut Henri Poincare 22 ( 2 ) , 1999 ) . The main motivation behind our work comes from percolation theory where the empty set of random walks holds a crucial role .Indeed , we prove how our approaches can be used to obtain new information about the important dynamics of bond - percolation models on Zd .",
        "rewrite_text": "We examine the empty set of random interlacements in \\( \\mathbb{R}^d \\) for \\( d > 1 \\), defined as the complement of the union of all open paths originating from the point 0 and extending up to time 1. We show that its Hausdorff size is almost surely \\( d - 1 \\) by demonstrating that its outer Minkowski dimension corresponds to this value with probability one. This finding builds upon earlier work on the empty set of simple random walks, as established by Lawler, Schramm, and Werner (Annals of Probability 24 (3), 1997) and Benjamini, Kalai, and Schramm (Journal of Functional Analysis 157 (1), 2000). Our proof heavily relies on recent advancements in the topology of Brownian motion and the concept of stable processes. Notably, we utilize an estimate for the Green function of the 'dead' Brownian motion, as developed by Bass and Burdzy (Annales de l’Institut Henri Poincaré 22 (2), 1999). Our primary motivation stems from percolation theory, where the empty set of random walks plays a critical role. In fact, we demonstrate that our methods can yield new insights into the significant dynamics of bond-percolation models on \\( \\mathbb{Z}^d \\).",
        "ori-fast-z-score": -1.2874526191574363,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": -1.1952286093343936
    },
    {
        "original_text": "We study the internal state of an isotropic packing of frictionless spherical particles under compression, decompression and cyclic loading by means of numerical simulations using molecular dynamics (MD). We find that the volume fraction increases with increasing applied stress in all cases studied here. The increase of the volume fraction during unloading is smaller than for loading at similar stresses. This hysteresis effect becomes more pronounced as the number of load-unload cycles increases. In addition to this we observe that the distribution function of contact forces changes significantly between different stages of the process. These results are discussed within the framework of elastic-plastic models of granular materials. Granular matter can be found everywhere around us; it forms the basis of many natural phenomena such as avalanches or landslides on mountainsides  1  , mudflow  2  , sedimentation  3  , soil mechanics  4  , earthquakes  5  . It also plays an important role in industrial processes like powder metallurgy  6  , pharmaceutical industry  7  , food processing  8  , etc.. Despite its ubiquity there still exist open questions about how granular systems behave mechanically  9  .\nIn recent years much effort has been devoted to understanding the mechanical behavior of granular media  10  -  12  . One of the most interesting problems concerns the response of granular material to external loads  13  -  16  . For example, one may ask what happens if you compress a sample of sand? What will happen when you release the pressure again?\nThe aim of our work presented below was to investigate these issues numerically  17  . To do so we used Molecular Dynamics (MD)  18  which allows us to simulate large samples consisting of thousands of grains  19  .",
        "watermark_text": "We explore the internal state of an isotropic packing of frictionless spherical objects under compression , decompression and cyclic loading by means of computational simulations using molecular dynamics ( MD ) . We see that the density fraction increases with increasing applied strain in all situations examined here .The expansion of the volume fraction during unloading is smaller than for loading at comparable stresses . This hysteresis effect gets more pronounced as the number of load - unload cycles rises .In addition to this we determine that the distribution relation of touch forces shifts significantly between various phases of the process . These conclusions are discussed within the framework of elastic - plastic models of granular materials .Granular material can be found everywhere around us ; it becomes the foundation of several physical phenomena such as avalanches or landslides on mountainsides 1 , mudflow 2 , sedimentation 3 , soil engineering 4 , earthquakes 5 . It additionally serves an important role in industrial systems like powder metallurgy 6 , pharmaceutical manufacturing 7 , meat mining 8 , etc . .Despite its ubiquity there still appear open questions about how granular structures react mechanically 9 . In recent years much effort has been focused to discovering the mechanical behavior of granular material 10 - 12 .One of the most important problems involves the response of granular material to external loads 13 - 16 . For instance , one may question what happens if you compress a sample of sand ?What will occur when you release the pressure again ? The goal of our work shown below was to examine these problems numerically 17 .To do so we using Molecular Dynamics ( MD ) 18 which allows us to simulate large specimens contained of thousands of grains 19 .",
        "rewrite_text": "We investigate the internal behavior of isotropic packings of frictionless spherical particles under scenarios of compression, decompression, and cyclic loading through computational simulations utilizing molecular dynamics (MD). Our findings reveal that the density fraction consistently increases with applied strain across all examined conditions. Notably, the volume fraction's expansion during unloading is less pronounced than during loading at equivalent stress levels, with this hysteresis effect becoming more evident as the number of load-unload cycles increases. Additionally, we observe a significant shift in the distribution of contact forces throughout different phases of the process. These observations are discussed in relation to elastic-plastic models of granular materials. Granular materials are prevalent in our environment, forming the basis of numerous physical phenomena, including avalanches and landslides on mountainsides, mudflows, sedimentation, soil engineering, and earthquakes. They also play a crucial role in various industrial applications such as powder metallurgy, pharmaceutical manufacturing, and mining. Despite their commonality, questions remain about the mechanical behavior of granular structures. In recent years, considerable effort has been directed towards understanding the mechanics of granular materials. A key area of focus is how these materials respond to external loads. For example, one might wonder what occurs when a sample of sand is compressed, or what happens upon releasing the pressure. The objective of our study, detailed below, was to numerically explore these issues. We employed Molecular Dynamics (MD) simulations, enabling us to model large specimens consisting of thousands of grains.",
        "ori-fast-z-score": -1.1917080461366747,
        "water-fast-z-score": 6.937819061732104,
        "rewrite-fast-z-score": 0.45454545454545453
    },
    {
        "original_text": "We have investigated how different assumptions about the velocity distribution function (VDF) affect the shape of the observed line profile in the solar corona, using an analytical model for the VDF that includes both isotropic thermal motions and anisotropic nonthermal motions. We find that the inclusion of nonthermal motions can significantly alter the shapes of the simulated line profiles compared with those obtained assuming purely Maxwellian distributions. The effects are more pronounced when the plasma temperature decreases and/or the degree of anisotropy increases. \n \n In particular, we show that the presence of nonthermal motions leads to significant asymmetries between the red-and blueshifted wings of the line profiles. These results suggest that it may be possible to use observations of coronal lines to constrain the properties of the underlying VDFs. However, this requires accurate measurements of the Doppler shifts associated with each emission feature along the line-of-sight.",
        "watermark_text": "We have analyzed how various assumptions about the velocity distribution relation ( VDF ) impact the morphology of the seen line profile in the sun corona , using an analytical theory for the VDF that contains both isotropic thermal motions and anisotropic nonthermal movements . We see that the introduction of nonthermal movements can significantly change the shapes of the simulated line profiles compared with those achieved assuming solely Maxwellian distributions .The effects are more pronounced when the plasma pressure drops and / or the degree of anisotropy changes . In particular , we find that the presence of nonthermal movements leads to significant asymmetries between the red - and blueshifted wings of the line profiles .These data suggest that it could be possible to use observations of coronal lines to constrain the properties of the intrinsic VDFs . However , this demands accurate measurements of the Doppler cycles identified with each emission feature along the line - of - view .",
        "rewrite_text": "We have examined how different assumptions regarding the velocity distribution function (VDF) affect the morphology of line profiles observed in the solar corona. Our analysis employs an analytical theory for the VDF that incorporates both isotropic thermal motions and anisotropic nonthermal movements. Our findings indicate that introducing nonthermal movements can significantly alter the shapes of the simulated line profiles compared to those predicted by assuming only Maxwellian distributions. The differences become more pronounced as plasma pressure decreases and/or the level of anisotropy varies. Notably, we observe that the presence of nonthermal movements introduces substantial asymmetries between the redshifted and blueshifted wings of the line profiles. These results imply that observing coronal line profiles may enable us to constrain the characteristics of intrinsic VDFs. However, this requires precise measurements of the Doppler shifts associated with each emission feature along the line of sight.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 6.621221919717306,
        "rewrite-fast-z-score": 0.3611575592573076
    },
    {
        "original_text": "We present an open-source, free software package that computes the one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of heavy quarks into light leptons and pions.  The code is written entirely within the Wolfram Language using only built-in functions.   It can be downloaded at https://github.com/mharrison/harmonic-oscillator-matrix-elements/tree/master/Mathematica_code. We have tested our results against those obtained with FeynArts 3.9  1  , FormCalc 8  2  , LoopTools 2  3  , and QCDLoop  4  . \nThe program has been used to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions  5  .  These are required inputs for the calculation of the branching fractions  6  and CP asymmetries  7, 8  of these processes.  In addition, we provide the LO contribution to the pion form factor  9  .\nOur approach uses the method developed by M. Neubert  10   which exploits the fact that the relevant loop integrals can all be expressed as linear combinations of products of two-point scalar functions.  This allows us to reduce the number of independent integrations needed to evaluate each diagram to three.  Our implementation also includes several improvements over previous calculations  11, 12  :   1)  We use the exact expression for the propagators of massive fermions instead...",
        "watermark_text": "We create an open - source , free software tool that computes the one - loop Feynman diagrams causing to the hadronic weak decay amplitudes of light quarks into light leptons and pions . The language is authored exclusively within the Wolfram Language using only built - in operations .It can be downloaded at https : / / github . com / mharrison / harmonic - oscillator - matrix - elements / tree / master / Mathematica _ code . We have tested our findings against those achieved with FeynArts 3 . 9 1 , FormCalc 8 2 , LoopTools 2 3 , and QCDLoop 4 .The project has been used to compute the led - order ( LO ) contributions to the helicity amplitudes for B meson decays into muons or electrons plus pions 5 . These are required inputs for the determination of the branching fractions 6 and CP asymmetries 7 , 8 of these reactions .In addition , we provide the LO contribution to the pion form parameter 9 . Our solution uses the method developed by M . Neubert 10 which uses the fact that the appropriate loop integrals can all be written as linear combinations of products of two - point scalar functions .This enables us to reduce the total of separate integrations needed to analyze each graph to three . Our formulation also contains several additions over past calculations 11 , 12 : 1 ) We use the exact definition for the propagators of large fermions instead . . .",
        "rewrite_text": "We have developed a free, open-source software tool that calculates one-loop Feynman diagrams contributing to the hadronic weak decay amplitudes of light quarks into light leptons and pions. This tool is implemented solely in the Wolfram Language and relies entirely on built-in functions. It is available for download at https://github.com/mharrison/harmonic-oscillator-matrix-elements/tree/master/Mathematica_code. Our results have been validated against calculations performed with FeynArts 3.91, FormCalc 8.2, LoopTools 2.3, and QCDLoop 4. The project has been utilized to compute the leading-order (LO) contributions to the helicity amplitudes for B meson decays into either muons or electrons plus pions. These contributions are essential for determining the branching fractions and CP asymmetries associated with these processes. Additionally, we offer the LO contribution to the pion form factor. Our approach employs a method developed by M. Neubert, which demonstrates that the relevant loop integrals can be expressed as linear combinations of products of two-point scalar functions. This significantly reduces the number of separate integrations necessary to analyze each diagram to just three. Furthermore, our formulation includes several enhancements over previous calculations: 1) We utilize the exact definition for the propagators of heavy fermions instead...",
        "ori-fast-z-score": -1.9126494315742406,
        "water-fast-z-score": 5.20847435706514,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "We present the relativistic second order perturbation theory for fluids in curved space-time with arbitrary number of components. We derive the general expression for the energy-momentum tensor at first order in perturbations as well as its trace-free part which is responsible for gravitational waves generation. The evolution equations are derived by projecting the conservation law onto the background 4-velocity vector field. In particular we show that the presence of anisotropic stress leads to an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be applied to study different physical situations such as inflationary models or dark matter halos formation. Cosmology has been revolutionized over the past decade thanks to precision measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation  1  . These observations have provided us with detailed information about the early universe and allowed to test fundamental physics on very large scales  2  .\nThe standard model of cosmology assumes that the universe consists of several interacting components including cold dark matter (CDM), baryons, photons, neutrinos etc.. Each component evolves according to some set of hydrodynamical equations describing their dynamics  3  . However, these equations cannot be solved analytically even if one neglects all interactions between particles  4  , so numerical simulations are required  5  . On the other hand, analytical solutions exist only under certain approximations  6  . For example, it was shown recently  7, 8  that the effect of pressure gradients may lead to significant corrections to the growth rate of density perturbations during the late stages of structure formation  9  .",
        "watermark_text": "We introduce the relativistic second order perturbation theory for fluids in curved space - time with arbitrary number of components . We derive the general expression for the power - momentum tensor at first order in perturbations as well as its trace - free portion which is responsible for gravitational waves generation .The evolution equations are derived by projecting the conservation law onto the background 4 - velocity vector field . In particular we find that the presence of anisotropic stress leads to an additional source term in the equation regulating the evolution of scalar modes .Finally , we talk how our formalism can be applied to study various physical conditions such as inflationary theories or black particle halos formation . Cosmology has been revolutionized over the previous decade courtesy to accurate measurements of temperature fluctuations in the cosmic microwave background ( CMB ) radiation 1 .These measurements have provided us with comprehensive information about the early universe and enable to test fundamental theory on very huge scales 2 . The conventional model of cosmology assumes that the universe consists of several interacting components namely cold bright matter ( CDM ) , baryons , photons , neutrinos etc . . Each component evolves due to some setting of hydrodynamical equations explaining their mechanics 3 .However , these equations never be answered analytically even if one neglects all interactions between particles 4 , so numerical simulations are required 5 . On the other hand , analytical solutions arise only under certain approximations 6 .For instance , it was shown ago 7 , 8 that the impact of force gradients might lead to significant corrections to the development time of density perturbations during the late stages of formation formation 9 .",
        "rewrite_text": "We present a relativistic second-order perturbation theory for fluids in curved spacetime, applicable to any number of components. We derive the general expression for the power-momentum tensor at first order in perturbations, including its trace-free part, which is crucial for generating gravitational waves. The evolution equations are formulated by projecting the conservation law onto the background four-velocity vector field. Notably, we discover that the presence of anisotropic stress introduces an additional source term in the equation governing the evolution of scalar modes. Finally, we discuss how our formalism can be utilized to investigate various physical scenarios, such as inflationary theories and the formation of black particle halos. Over the past decade, cosmology has been transformed by precise measurements of temperature fluctuations in the cosmic microwave background (CMB) radiation. These measurements provide valuable insights into the early universe and enable the testing of fundamental theories across vast scales. The standard cosmological model posits that the universe is composed of multiple interacting components, including cold dark matter (CDM), baryons, photons, and neutrinos. Each component evolves according to specific hydrodynamic equations that describe their dynamics. However, these equations cannot be solved analytically, even when ignoring particle interactions, necessitating numerical simulations. In contrast, analytical solutions are only attainable under particular approximations. For example, it has previously been demonstrated that the effects of force gradients can significantly alter the growth rate of density perturbations during the later stages of their formation.",
        "ori-fast-z-score": -2.1213203435596424,
        "water-fast-z-score": 5.1946603473211805,
        "rewrite-fast-z-score": -0.6363636363636364
    },
    {
        "original_text": "We study the transverse momentum dependence of J/ψ and Υ production cross sections at RHIC energies within an effective field theory approach, which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons.  We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV/c while additional contributions are needed to describe data points with larger values of pT . The latter turn out to be dominated by inelastic processes like dissociation into open heavy flavor mesons. In particular we show that the inclusion of these effects leads to a significant reduction of the predicted nuclear modification factor RAA(pT ) compared to previous calculations based on purely elastic interactions. \nPACS numbers: 12.38.Mh, 25.75.-q, 11.10.Kk \nI. INTRODUCTORY REMAR K\nThe measurement of charmonium (J/ψ) and bottomonium (Υ) production is one of the most promising probes to investigate properties of hot and dense matter created in relativistic nucleus-nucleus collisions  1  . It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting  2  , i.e., to a decrease of the bound state masses due to color screening  3  .\nIn this work we present results obtained within an effective field theory framework  4  , where the relevant degrees of freedom are quarks and gluons rather than individual hadronic states. This allows us to calculate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks q = u, d, s and gluons g. These include elastic scattering off quarks and gluon-gluon fusion leading to the formation of quarkonia via the creation of a virtual qq pair  5  . Furthermore, inelastic processes such as quarkonium dissociation into open heavy-flavor hadrons  6  have also been included  7, 8  .",
        "watermark_text": "We study the transverse momentum dependence of J / ψ and [UNK] production cross sections at RHIC energies within an effective field theory approach , which includes both elastic scattering off quarks as well as inelastic processes such as dissociation into open charm or bottom hadrons . We find that the observed suppression pattern can be reproduced by including only elastic scattering for pT < 2 GeV / c while additional contributions are needed to describe data points with larger values of pT .The latter turn out to be dominated by inelastic reactions like dissociation into open heavy flavor mesons . In particular we prove that the introduction of these influences result to a substantial decreased of the expected nuclear modification factor RAA ( pT ) compared to previous analyses based on purely elastic interactions .PACS codes : 12 . 38 . Mh , 25 . 75 . - q , 11 . 10 . Kk I . INTRODUCTORY REMAR K The measurement of charmonium ( J / ψ ) and bottomonium ( [UNK] ) production is one of the most attractive probes to examine properties of hot and dense materials captured in relativistic nucleus - nucleus collisions 1 .It has been proposed that the interaction between the produced quarkonia and the nearby medium may contribute to their partial melting 2 , i . e . , to a reduction of the bound state masses due to color screening 3 . In this research we present results derived within an efficient field model formulation 4 , where the appropriate degrees of liberty are quarks and gluons instead than individual hadronic states .This enables us to estimate the total cross section for quarkonium production in terms of elementary partonic subprocesses involving light quarks g = w , d , s and gluons g . These include elastic scattering off quarks and gluon - gluon fusion led to the formation of quarkonia via the creation of a virtual qq couple 5 . Furthermore , inelastic reactions such as quarkonium dissociation into open heavy - flavor hadrons 6 have also been used 7 , 8 .",
        "rewrite_text": "We investigate the transverse momentum dependence of J/ψ and [UNK] production cross sections at RHIC energies using an effective field theory approach that accounts for both elastic scattering off quarks and inelastic processes, like dissociation into open charm or bottom hadrons. Our findings reveal that the observed suppression pattern can be accurately reproduced by considering only elastic scattering for pT < 2 GeV/c, whereas higher pT values require additional contributions. These higher energy points are primarily influenced by inelastic reactions, specifically the dissociation into open heavy flavor mesons. Our analysis demonstrates that incorporating these inelastic processes significantly lowers the expected nuclear modification factor RAA(pT) compared to earlier studies that relied solely on elastic interactions. \n\nPACS codes: 12.38.Mh, 25.75.-q, 11.10.Kk \n\nI. INTRODUCTORY REMARKS\n\nMeasuring the production of charmonium (J/ψ) and bottomonium ([UNK]) serves as a valuable tool for exploring the properties of hot and dense matter created during relativistic nucleus-nucleus collisions. It has been suggested that the interaction between the produced quarkonia and the surrounding medium may lead to their partial melting, resulting in a reduction of bound state masses due to color screening. In this study, we present results derived from an efficient field theory formulation, which focuses on quarks and gluons instead of individual hadronic states. This framework allows us to estimate the total cross section for quarkonium production through elementary partonic subprocesses that involve light quarks (u, d, s) and gluons. These processes include elastic scattering off quarks as well as gluon-gluon fusion, which leads to quarkonia formation via the creation of a virtual quark-antiquark pair. Additionally, we consider inelastic reactions, such as the dissociation of quarkonium into open heavy-flavor hadrons.",
        "ori-fast-z-score": -0.08606629658238704,
        "water-fast-z-score": 5.115948820192307,
        "rewrite-fast-z-score": 1.310556084991557
    },
    {
        "original_text": "We present dynamical models for the Milky Way halo that include an arbitrary number of spherical shells, each characterized by its own density and velocity dispersion profiles. The model is based on Jeans  theorem applied to the phase-space distribution function (DF) in order to obtain the DF as well as the projected surface mass density along any line-of-sight. We show how this approach can be used to fit observational data such as those obtained from kinematic tracers or gravitational lensing measurements. In particular we apply our method to two different sets of observations: 1) A sample of RR Lyrae stars observed towards the Galactic bulge; 2) Gravitational lensing measurements towards the Bullet Cluster. Our results are compared against previous works using similar techniques but assuming either constant anisotropies across all radii or simple power-law radial dependences. We find that these assumptions lead to significant biases when fitting the data.",
        "watermark_text": "We create dynamical models for the Milky Way halo that include an arbitrary number of spherical shells , each described by its own density and frequency dispersion patterns . The model is based on Jeans theorem applied to the phase - space distribution relation ( DF ) in order to obtain the DF as well as the projected surface weight concentration along any line - of - view .We see how this methodology can be used to fit observational data such as those acquired from kinematic tracers or gravitational lensing observations . In particular we apply our technique to two different sets of measurements : 1 ) A specimen of RR Lyrae stars observed towards the Galactic bulge ; 2 ) Gravitational lensing observations towards the Bullet Cluster .Our results are compared against prior studies use similar techniques but adopting either constant anisotropies across all radii or straightforward power - law longitudinal dependences . We see that these assumptions lead to significant biases when fitting the information .",
        "rewrite_text": "We develop dynamic models of the Milky Way halo that incorporate a flexible number of spherical shells, each characterized by unique density and frequency dispersion profiles. Our approach utilizes Jeans theorem in conjunction with the phase-space distribution relation (DF) to derive the DF as well as the projected surface weight concentration along any specified line of sight. This methodology proves effective in fitting observational data obtained from kinematic tracers and gravitational lensing studies. Specifically, we apply our technique to two distinct sets of measurements: 1) RR Lyrae stars observed in the direction of the Galactic bulge, and 2) gravitational lensing observations of the Bullet Cluster. We compare our findings with previous studies that employed similar techniques but assumed either consistent anisotropies across all radii or simple power-law relationships for longitudinal dependencies. Our analysis reveals that these assumptions introduce considerable biases when fitting the observational data.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.589285179800713,
        "rewrite-fast-z-score": 0.9561828874675149
    },
    {
        "original_text": "We present new results on the properties of diffuse radio emission associated with galaxy clusters, based on observations carried out at 1.4 GHz using the Giant Metrewave Radio Telescope (GMRT). We have studied the correlation between the total power of the diffuse radio emission and other physical parameters such as X-ray luminosity, temperature and mass for a sample of 25 galaxy clusters observed by GMRT. The main result is that we find evidence for two different populations of radio halo sources which are separated by their radio powers. In particular, our analysis shows that there exists an upper limit to the radio power of radio halos hosted by massive galaxy clusters. This suggests that the energy density of relativistic particles responsible for the synchrotron emission decreases rapidly beyond this threshold value. Our findings also suggest that the magnetic field strength increases with increasing radio power. These results can be explained within the framework of the re-acceleration model where secondary electrons produced through hadronic interactions are reaccelerated by turbulence generated during mergers or accretion events.",
        "watermark_text": "We report new data on the properties of diffuse radio emission associated with star clusters , based on observations carried out at 1 . 4 GHz using the Giant Metrewave Radio Telescope ( GMRT ) . We have researched the relationship between the total power of the diffuse radio emission and other physical factors such as X - ray luminosity , temperature and mass for a sample of 25 galaxy galaxies studied by GMRT .The main consequence is that we find proof for two different populations of radio halo sources which are distinct by their radio powers . In particular , our analysis shows that there exists an upper limitation to the radio power of radio halos hosted by massive galaxy regions .This shows that the power concentration of relativistic objects causing for the synchrotron emission decreases quickly beyond this threshold value . Our findings also suggest that the magnetic force capacity increases with expanding radio power .These data can be described within the framework of the re - acceleration hypothesis where secondary electrons produced through hadronic interactions are reaccelerated by turbulence generated during mergers or accretion events .",
        "rewrite_text": "We present new findings on the characteristics of diffuse radio emission linked to star clusters, based on observations conducted at 1.4 GHz using the Giant Metrewave Radio Telescope (GMRT). Our investigation examines the correlation between the total power of the diffuse radio emission and various physical factors, including X-ray luminosity, temperature, and mass, across a sample of 25 galaxies observed by GMRT. Notably, we provide evidence for two distinct populations of radio halo sources that differ in their radio power. Specifically, our analysis indicates an upper limit to the radio power of halos located in massive galaxy regions, suggesting that the concentration of relativistic objects responsible for the synchrotron emission decreases significantly beyond this threshold. Additionally, our results propose that the capacity for magnetic fields increases with rising radio power. These findings can be interpreted within the framework of the re-acceleration hypothesis, whereby secondary electrons generated through hadronic interactions are re-accelerated by turbulence produced during merger or accretion events.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 6.695619859371111,
        "rewrite-fast-z-score": 1.5652475842498528
    }
]